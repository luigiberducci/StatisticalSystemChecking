Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.168s, episode steps: 100, steps per second: 594, episode reward: 52.845, mean reward: 0.528 [0.310, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.634, 10.245], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1596, episode reward: 54.193, mean reward: 0.542 [0.298, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.418, 10.140], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 51.188, mean reward: 0.512 [0.276, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.767, 10.283], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 54.099, mean reward: 0.541 [0.292, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.432, 10.294], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 51.745, mean reward: 0.517 [0.321, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.672, 10.253], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.180s, episode steps: 100, steps per second: 85, episode reward: 56.361, mean reward: 0.564 [0.317, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.647, 10.098], loss: 0.016371, mae: 0.115692, mean_q: -0.482308
   700/100000: episode: 7, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 51.385, mean reward: 0.514 [0.284, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.414, 10.098], loss: 0.005648, mae: 0.079293, mean_q: 0.032738
   800/100000: episode: 8, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 52.737, mean reward: 0.527 [0.264, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.603, 10.175], loss: 0.004392, mae: 0.074084, mean_q: 0.492977
   900/100000: episode: 9, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 53.534, mean reward: 0.535 [0.188, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.957, 10.253], loss: 0.005512, mae: 0.079284, mean_q: 0.979193
  1000/100000: episode: 10, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 50.341, mean reward: 0.503 [0.351, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.433, 10.098], loss: 0.007650, mae: 0.083653, mean_q: 1.468444
  1100/100000: episode: 11, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 54.521, mean reward: 0.545 [0.378, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.484, 10.333], loss: 0.013659, mae: 0.095422, mean_q: 1.921851
  1200/100000: episode: 12, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 50.558, mean reward: 0.506 [0.251, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.933, 10.098], loss: 0.022186, mae: 0.117683, mean_q: 2.367045
  1300/100000: episode: 13, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 55.409, mean reward: 0.554 [0.197, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.799, 10.139], loss: 0.020886, mae: 0.110978, mean_q: 2.821041
  1400/100000: episode: 14, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 54.534, mean reward: 0.545 [0.231, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.163, 10.209], loss: 0.023252, mae: 0.107187, mean_q: 3.278906
  1500/100000: episode: 15, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 54.300, mean reward: 0.543 [0.356, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.982, 10.098], loss: 0.031210, mae: 0.121227, mean_q: 3.726989
  1600/100000: episode: 16, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 54.576, mean reward: 0.546 [0.285, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.601, 10.098], loss: 0.027086, mae: 0.115884, mean_q: 4.167759
  1700/100000: episode: 17, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 53.273, mean reward: 0.533 [0.243, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.712, 10.098], loss: 0.030910, mae: 0.121000, mean_q: 4.605812
  1800/100000: episode: 18, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 55.934, mean reward: 0.559 [0.288, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.660, 10.202], loss: 0.030435, mae: 0.121019, mean_q: 5.058543
  1900/100000: episode: 19, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 55.486, mean reward: 0.555 [0.284, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.386, 10.173], loss: 0.039350, mae: 0.125607, mean_q: 5.442026
  2000/100000: episode: 20, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 47.005, mean reward: 0.470 [0.081, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.950, 10.098], loss: 0.039183, mae: 0.139801, mean_q: 5.865464
  2100/100000: episode: 21, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.145, mean reward: 0.561 [0.352, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.098], loss: 0.033947, mae: 0.125245, mean_q: 6.237003
  2200/100000: episode: 22, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 55.754, mean reward: 0.558 [0.358, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.855, 10.098], loss: 0.037077, mae: 0.138405, mean_q: 6.642278
  2300/100000: episode: 23, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 53.128, mean reward: 0.531 [0.125, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.752, 10.434], loss: 0.028205, mae: 0.120664, mean_q: 7.099795
  2400/100000: episode: 24, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 51.356, mean reward: 0.514 [0.232, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-2.067, 10.098], loss: 0.027119, mae: 0.122964, mean_q: 7.401925
  2500/100000: episode: 25, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 55.466, mean reward: 0.555 [0.353, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.445, 10.110], loss: 0.025589, mae: 0.125375, mean_q: 7.791310
  2600/100000: episode: 26, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 56.009, mean reward: 0.560 [0.307, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.729, 10.100], loss: 0.022608, mae: 0.123493, mean_q: 8.156404
  2700/100000: episode: 27, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 51.869, mean reward: 0.519 [0.335, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.800, 10.264], loss: 0.016938, mae: 0.111990, mean_q: 8.539747
  2800/100000: episode: 28, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 53.623, mean reward: 0.536 [0.133, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.744, 10.098], loss: 0.020487, mae: 0.125630, mean_q: 8.906442
  2900/100000: episode: 29, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 54.294, mean reward: 0.543 [0.238, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.186, 10.179], loss: 0.017008, mae: 0.119666, mean_q: 9.233830
  3000/100000: episode: 30, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 49.989, mean reward: 0.500 [0.267, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.259, 10.276], loss: 0.015475, mae: 0.114394, mean_q: 9.625568
  3100/100000: episode: 31, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 48.197, mean reward: 0.482 [0.308, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.095, 10.271], loss: 0.016863, mae: 0.120802, mean_q: 9.832615
  3200/100000: episode: 32, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 53.964, mean reward: 0.540 [0.223, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.940, 10.346], loss: 0.011989, mae: 0.107655, mean_q: 10.214719
  3300/100000: episode: 33, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 55.776, mean reward: 0.558 [0.346, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.881, 10.098], loss: 0.015338, mae: 0.116739, mean_q: 10.432660
  3400/100000: episode: 34, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 54.134, mean reward: 0.541 [0.359, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.159, 10.179], loss: 0.014578, mae: 0.116581, mean_q: 10.863086
  3500/100000: episode: 35, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 51.901, mean reward: 0.519 [0.223, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.918, 10.098], loss: 0.011541, mae: 0.107270, mean_q: 11.163216
  3600/100000: episode: 36, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 55.197, mean reward: 0.552 [0.337, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.181, 10.098], loss: 0.013154, mae: 0.114876, mean_q: 11.407100
  3700/100000: episode: 37, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 55.330, mean reward: 0.553 [0.261, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.695, 10.098], loss: 0.012445, mae: 0.114171, mean_q: 11.682828
  3800/100000: episode: 38, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.025, mean reward: 0.560 [0.328, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.500, 10.211], loss: 0.013325, mae: 0.118576, mean_q: 11.880898
  3900/100000: episode: 39, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 46.923, mean reward: 0.469 [0.195, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.345, 10.368], loss: 0.012632, mae: 0.113609, mean_q: 12.444162
  4000/100000: episode: 40, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 54.793, mean reward: 0.548 [0.260, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.524, 10.098], loss: 0.012400, mae: 0.117431, mean_q: 12.559936
  4100/100000: episode: 41, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 49.512, mean reward: 0.495 [0.273, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.331, 10.256], loss: 0.012826, mae: 0.118107, mean_q: 12.827306
  4200/100000: episode: 42, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 55.576, mean reward: 0.556 [0.333, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.535, 10.144], loss: 0.012396, mae: 0.116898, mean_q: 13.137836
  4300/100000: episode: 43, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.254, mean reward: 0.573 [0.345, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.696, 10.175], loss: 0.011647, mae: 0.115294, mean_q: 13.227565
  4400/100000: episode: 44, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 49.344, mean reward: 0.493 [0.240, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.389, 10.227], loss: 0.012610, mae: 0.116884, mean_q: 13.632668
  4500/100000: episode: 45, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 54.704, mean reward: 0.547 [0.304, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.031, 10.098], loss: 0.016604, mae: 0.131475, mean_q: 13.723579
  4600/100000: episode: 46, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 55.954, mean reward: 0.560 [0.287, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.204, 10.098], loss: 0.016625, mae: 0.132678, mean_q: 14.023133
  4700/100000: episode: 47, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 54.205, mean reward: 0.542 [0.251, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.433, 10.207], loss: 0.015383, mae: 0.125000, mean_q: 14.197399
  4800/100000: episode: 48, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.954, mean reward: 0.570 [0.273, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.577, 10.098], loss: 0.014599, mae: 0.125837, mean_q: 14.396466
  4900/100000: episode: 49, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 54.712, mean reward: 0.547 [0.250, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.721, 10.098], loss: 0.016371, mae: 0.131434, mean_q: 14.526531
  5000/100000: episode: 50, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 54.631, mean reward: 0.546 [0.286, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.571, 10.098], loss: 0.013949, mae: 0.120918, mean_q: 14.801389
  5100/100000: episode: 51, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 53.586, mean reward: 0.536 [0.244, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.325, 10.098], loss: 0.014709, mae: 0.128649, mean_q: 15.059155
  5200/100000: episode: 52, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 55.349, mean reward: 0.553 [0.279, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.926, 10.122], loss: 0.013574, mae: 0.123365, mean_q: 15.184772
  5300/100000: episode: 53, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 52.584, mean reward: 0.526 [0.327, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.266, 10.098], loss: 0.015108, mae: 0.129357, mean_q: 15.419386
  5400/100000: episode: 54, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 54.588, mean reward: 0.546 [0.291, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.335, 10.145], loss: 0.016237, mae: 0.132481, mean_q: 15.697933
  5500/100000: episode: 55, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 50.709, mean reward: 0.507 [0.197, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.697, 10.228], loss: 0.017610, mae: 0.141753, mean_q: 15.690857
  5600/100000: episode: 56, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 53.886, mean reward: 0.539 [0.297, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.684, 10.098], loss: 0.015376, mae: 0.130645, mean_q: 15.921772
  5700/100000: episode: 57, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 54.265, mean reward: 0.543 [0.319, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.395, 10.342], loss: 0.014889, mae: 0.126236, mean_q: 16.206127
  5800/100000: episode: 58, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 55.234, mean reward: 0.552 [0.378, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.842, 10.098], loss: 0.015103, mae: 0.125875, mean_q: 16.395741
  5900/100000: episode: 59, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 55.047, mean reward: 0.550 [0.325, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.681, 10.316], loss: 0.014298, mae: 0.127930, mean_q: 16.302742
  6000/100000: episode: 60, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 47.843, mean reward: 0.478 [0.185, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.990, 10.098], loss: 0.016797, mae: 0.137958, mean_q: 16.550514
  6100/100000: episode: 61, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 54.214, mean reward: 0.542 [0.246, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.398, 10.409], loss: 0.014488, mae: 0.129961, mean_q: 16.810423
  6200/100000: episode: 62, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 56.139, mean reward: 0.561 [0.315, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.253, 10.104], loss: 0.015800, mae: 0.135268, mean_q: 16.840590
  6300/100000: episode: 63, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 50.855, mean reward: 0.509 [0.316, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.799, 10.342], loss: 0.014585, mae: 0.128331, mean_q: 17.081833
  6400/100000: episode: 64, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 50.510, mean reward: 0.505 [0.265, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.511, 10.342], loss: 0.016580, mae: 0.138770, mean_q: 17.263115
  6500/100000: episode: 65, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 54.209, mean reward: 0.542 [0.118, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.765, 10.098], loss: 0.016877, mae: 0.139251, mean_q: 17.457069
  6600/100000: episode: 66, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 51.852, mean reward: 0.519 [0.312, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.771, 10.098], loss: 0.016099, mae: 0.130325, mean_q: 17.475962
  6700/100000: episode: 67, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 54.288, mean reward: 0.543 [0.284, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.112, 10.110], loss: 0.019912, mae: 0.146489, mean_q: 17.738100
  6800/100000: episode: 68, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 52.992, mean reward: 0.530 [0.339, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.423, 10.260], loss: 0.016836, mae: 0.136413, mean_q: 17.217829
  6900/100000: episode: 69, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 49.612, mean reward: 0.496 [0.282, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.041, 10.444], loss: 0.016795, mae: 0.137374, mean_q: 17.926588
  7000/100000: episode: 70, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 53.614, mean reward: 0.536 [0.240, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.811, 10.098], loss: 0.016705, mae: 0.135390, mean_q: 17.907923
  7100/100000: episode: 71, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 53.859, mean reward: 0.539 [0.310, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.693, 10.098], loss: 0.016980, mae: 0.138119, mean_q: 17.969427
  7200/100000: episode: 72, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.042, mean reward: 0.560 [0.296, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.348, 10.133], loss: 0.015489, mae: 0.134854, mean_q: 18.251534
  7300/100000: episode: 73, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 52.316, mean reward: 0.523 [0.334, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.495, 10.098], loss: 0.017070, mae: 0.137341, mean_q: 18.240822
  7400/100000: episode: 74, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 54.211, mean reward: 0.542 [0.250, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.183, 10.164], loss: 0.017739, mae: 0.145321, mean_q: 18.190794
  7500/100000: episode: 75, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 53.764, mean reward: 0.538 [0.348, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.984, 10.104], loss: 0.014296, mae: 0.130468, mean_q: 18.498638
  7600/100000: episode: 76, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 53.951, mean reward: 0.540 [0.280, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.945, 10.098], loss: 0.017803, mae: 0.135950, mean_q: 18.576691
  7700/100000: episode: 77, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 53.365, mean reward: 0.534 [0.250, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.008, 10.216], loss: 0.015048, mae: 0.133335, mean_q: 18.530470
  7800/100000: episode: 78, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 53.947, mean reward: 0.539 [0.348, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.678, 10.230], loss: 0.017011, mae: 0.135432, mean_q: 18.665464
  7900/100000: episode: 79, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 52.788, mean reward: 0.528 [0.244, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.897, 10.098], loss: 0.016211, mae: 0.131927, mean_q: 19.094872
  8000/100000: episode: 80, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 51.025, mean reward: 0.510 [0.166, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.799, 10.331], loss: 0.015822, mae: 0.132156, mean_q: 18.737394
  8100/100000: episode: 81, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 51.287, mean reward: 0.513 [0.296, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.995, 10.160], loss: 0.015784, mae: 0.130968, mean_q: 18.635046
  8200/100000: episode: 82, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 52.920, mean reward: 0.529 [0.267, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.930, 10.098], loss: 0.020454, mae: 0.152503, mean_q: 18.905573
  8300/100000: episode: 83, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 53.446, mean reward: 0.534 [0.289, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.921, 10.356], loss: 0.015989, mae: 0.129786, mean_q: 19.004244
  8400/100000: episode: 84, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 52.805, mean reward: 0.528 [0.312, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.304, 10.272], loss: 0.017192, mae: 0.134004, mean_q: 18.874348
  8500/100000: episode: 85, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 49.926, mean reward: 0.499 [0.314, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.261, 10.202], loss: 0.016532, mae: 0.132140, mean_q: 18.882195
  8600/100000: episode: 86, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 55.433, mean reward: 0.554 [0.300, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.478, 10.155], loss: 0.015551, mae: 0.131156, mean_q: 19.235903
  8700/100000: episode: 87, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 51.552, mean reward: 0.516 [0.316, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.897, 10.284], loss: 0.017834, mae: 0.141431, mean_q: 19.572680
  8800/100000: episode: 88, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 55.873, mean reward: 0.559 [0.202, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.927, 10.098], loss: 0.016941, mae: 0.138797, mean_q: 19.391302
  8900/100000: episode: 89, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 55.095, mean reward: 0.551 [0.212, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.842, 10.175], loss: 0.014469, mae: 0.127693, mean_q: 19.366873
  9000/100000: episode: 90, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 48.324, mean reward: 0.483 [0.297, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.485, 10.098], loss: 0.018758, mae: 0.146462, mean_q: 19.065374
  9100/100000: episode: 91, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 53.617, mean reward: 0.536 [0.216, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.264, 10.212], loss: 0.017703, mae: 0.140165, mean_q: 19.427965
  9200/100000: episode: 92, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 50.937, mean reward: 0.509 [0.241, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.567, 10.098], loss: 0.015762, mae: 0.130537, mean_q: 19.564445
  9300/100000: episode: 93, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 56.429, mean reward: 0.564 [0.295, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.532, 10.098], loss: 0.015493, mae: 0.130525, mean_q: 19.630344
  9400/100000: episode: 94, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.374, mean reward: 0.574 [0.392, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.517, 10.098], loss: 0.016876, mae: 0.131279, mean_q: 19.593287
  9500/100000: episode: 95, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 54.195, mean reward: 0.542 [0.293, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.048, 10.190], loss: 0.015945, mae: 0.130736, mean_q: 19.622774
  9600/100000: episode: 96, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 52.164, mean reward: 0.522 [0.293, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.021, 10.336], loss: 0.014984, mae: 0.129471, mean_q: 19.663580
  9700/100000: episode: 97, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 48.766, mean reward: 0.488 [0.250, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.583, 10.192], loss: 0.017955, mae: 0.134810, mean_q: 19.549196
  9800/100000: episode: 98, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 52.086, mean reward: 0.521 [0.131, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.084, 10.098], loss: 0.017593, mae: 0.139762, mean_q: 19.794834
  9900/100000: episode: 99, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 49.834, mean reward: 0.498 [0.269, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.830, 10.224], loss: 0.018915, mae: 0.142688, mean_q: 19.624033
 10000/100000: episode: 100, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 45.599, mean reward: 0.456 [0.234, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.976, 10.098], loss: 0.020944, mae: 0.146562, mean_q: 19.547371
 10100/100000: episode: 101, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 49.595, mean reward: 0.496 [0.164, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.256, 10.098], loss: 0.017621, mae: 0.133679, mean_q: 19.739796
 10200/100000: episode: 102, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 50.011, mean reward: 0.500 [0.225, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.951, 10.098], loss: 0.015885, mae: 0.126598, mean_q: 19.638182
 10300/100000: episode: 103, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 50.768, mean reward: 0.508 [0.290, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.727, 10.279], loss: 0.020559, mae: 0.144501, mean_q: 19.310677
 10400/100000: episode: 104, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 53.205, mean reward: 0.532 [0.329, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.644, 10.168], loss: 0.020005, mae: 0.146172, mean_q: 19.866772
 10500/100000: episode: 105, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 56.613, mean reward: 0.566 [0.334, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.616, 10.098], loss: 0.016044, mae: 0.130679, mean_q: 19.579527
 10600/100000: episode: 106, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 52.650, mean reward: 0.526 [0.286, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.846, 10.098], loss: 0.025705, mae: 0.160232, mean_q: 19.471460
 10700/100000: episode: 107, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 53.814, mean reward: 0.538 [0.339, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.189, 10.098], loss: 0.021351, mae: 0.144262, mean_q: 19.600142
 10800/100000: episode: 108, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.422, mean reward: 0.574 [0.376, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.732, 10.325], loss: 0.022776, mae: 0.150146, mean_q: 19.474512
 10900/100000: episode: 109, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 48.272, mean reward: 0.483 [0.232, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.944, 10.299], loss: 0.022315, mae: 0.151549, mean_q: 19.636805
 11000/100000: episode: 110, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 52.300, mean reward: 0.523 [0.250, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.162, 10.125], loss: 0.021434, mae: 0.140142, mean_q: 19.780378
 11100/100000: episode: 111, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 50.083, mean reward: 0.501 [0.282, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.526, 10.098], loss: 0.018271, mae: 0.133210, mean_q: 19.684015
 11200/100000: episode: 112, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 53.651, mean reward: 0.537 [0.303, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.310, 10.232], loss: 0.026664, mae: 0.160404, mean_q: 19.542957
 11300/100000: episode: 113, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 50.959, mean reward: 0.510 [0.294, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.422, 10.098], loss: 0.022244, mae: 0.143862, mean_q: 19.917635
 11400/100000: episode: 114, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 52.280, mean reward: 0.523 [0.285, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.385, 10.121], loss: 0.021324, mae: 0.147843, mean_q: 19.731335
 11500/100000: episode: 115, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 54.196, mean reward: 0.542 [0.291, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.719, 10.098], loss: 0.019456, mae: 0.139833, mean_q: 19.857130
 11600/100000: episode: 116, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 52.470, mean reward: 0.525 [0.234, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.935, 10.098], loss: 0.020680, mae: 0.141624, mean_q: 19.689617
 11700/100000: episode: 117, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.145, mean reward: 0.561 [0.318, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.130, 10.098], loss: 0.020295, mae: 0.139791, mean_q: 19.577578
 11800/100000: episode: 118, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 54.495, mean reward: 0.545 [0.367, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.854, 10.098], loss: 0.019138, mae: 0.134115, mean_q: 19.962591
 11900/100000: episode: 119, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 53.905, mean reward: 0.539 [0.253, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.715, 10.266], loss: 0.017391, mae: 0.130068, mean_q: 19.631771
 12000/100000: episode: 120, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 49.828, mean reward: 0.498 [0.266, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.582, 10.254], loss: 0.019750, mae: 0.143463, mean_q: 19.640488
 12100/100000: episode: 121, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 53.801, mean reward: 0.538 [0.297, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.926, 10.280], loss: 0.021301, mae: 0.148714, mean_q: 19.777979
 12200/100000: episode: 122, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 54.841, mean reward: 0.548 [0.233, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.532, 10.225], loss: 0.017225, mae: 0.135705, mean_q: 19.549414
 12300/100000: episode: 123, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 55.155, mean reward: 0.552 [0.324, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.431, 10.270], loss: 0.017840, mae: 0.134292, mean_q: 19.464018
 12400/100000: episode: 124, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 50.483, mean reward: 0.505 [0.293, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.368, 10.210], loss: 0.017078, mae: 0.129515, mean_q: 19.762836
 12500/100000: episode: 125, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 52.651, mean reward: 0.527 [0.345, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.918, 10.285], loss: 0.017119, mae: 0.133919, mean_q: 19.422966
 12600/100000: episode: 126, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 53.630, mean reward: 0.536 [0.123, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.697, 10.098], loss: 0.016211, mae: 0.129064, mean_q: 19.394554
 12700/100000: episode: 127, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 55.224, mean reward: 0.552 [0.369, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.104, 10.327], loss: 0.015276, mae: 0.131519, mean_q: 19.585407
 12800/100000: episode: 128, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 50.374, mean reward: 0.504 [0.262, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.601, 10.361], loss: 0.013763, mae: 0.124375, mean_q: 19.506445
 12900/100000: episode: 129, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 53.799, mean reward: 0.538 [0.301, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.245, 10.217], loss: 0.015337, mae: 0.129813, mean_q: 19.461233
 13000/100000: episode: 130, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 52.157, mean reward: 0.522 [0.231, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.528, 10.098], loss: 0.012910, mae: 0.119495, mean_q: 19.226561
 13100/100000: episode: 131, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 48.070, mean reward: 0.481 [0.147, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.946, 10.098], loss: 0.016063, mae: 0.130580, mean_q: 19.363720
 13200/100000: episode: 132, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 55.871, mean reward: 0.559 [0.291, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.108, 10.288], loss: 0.014870, mae: 0.127193, mean_q: 19.397419
 13300/100000: episode: 133, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.613, mean reward: 0.556 [0.113, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.036, 10.098], loss: 0.014908, mae: 0.126449, mean_q: 19.442924
 13400/100000: episode: 134, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 55.389, mean reward: 0.554 [0.349, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.338, 10.098], loss: 0.013149, mae: 0.123648, mean_q: 19.417133
 13500/100000: episode: 135, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 53.712, mean reward: 0.537 [0.344, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.781, 10.098], loss: 0.014246, mae: 0.125512, mean_q: 19.351173
 13600/100000: episode: 136, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 52.200, mean reward: 0.522 [0.322, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.593, 10.119], loss: 0.014073, mae: 0.126991, mean_q: 19.583511
 13700/100000: episode: 137, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 51.439, mean reward: 0.514 [0.210, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.671, 10.098], loss: 0.013419, mae: 0.124947, mean_q: 19.490656
 13800/100000: episode: 138, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 45.817, mean reward: 0.458 [0.245, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.969, 10.098], loss: 0.014560, mae: 0.131334, mean_q: 19.425413
 13900/100000: episode: 139, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 52.797, mean reward: 0.528 [0.338, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.275, 10.098], loss: 0.013390, mae: 0.120622, mean_q: 19.613653
 14000/100000: episode: 140, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 55.522, mean reward: 0.555 [0.371, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.827, 10.098], loss: 0.013430, mae: 0.126424, mean_q: 19.505583
 14100/100000: episode: 141, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 52.676, mean reward: 0.527 [0.064, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.262, 10.381], loss: 0.014363, mae: 0.127835, mean_q: 19.401115
 14200/100000: episode: 142, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 54.683, mean reward: 0.547 [0.362, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.999, 10.098], loss: 0.012823, mae: 0.119961, mean_q: 19.443628
 14300/100000: episode: 143, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 54.381, mean reward: 0.544 [0.369, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.926, 10.242], loss: 0.013492, mae: 0.122988, mean_q: 19.745298
 14400/100000: episode: 144, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 53.735, mean reward: 0.537 [0.289, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.928, 10.258], loss: 0.014648, mae: 0.131559, mean_q: 19.468609
 14500/100000: episode: 145, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 52.901, mean reward: 0.529 [0.297, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.958, 10.101], loss: 0.014337, mae: 0.127400, mean_q: 19.114754
 14600/100000: episode: 146, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 48.572, mean reward: 0.486 [0.284, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.659, 10.098], loss: 0.014054, mae: 0.124189, mean_q: 19.399843
 14700/100000: episode: 147, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 55.789, mean reward: 0.558 [0.300, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.681, 10.135], loss: 0.016312, mae: 0.138503, mean_q: 19.679119
 14800/100000: episode: 148, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 51.749, mean reward: 0.517 [0.365, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.567, 10.098], loss: 0.012553, mae: 0.122916, mean_q: 19.447304
 14900/100000: episode: 149, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 52.851, mean reward: 0.529 [0.265, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.554, 10.098], loss: 0.014645, mae: 0.127014, mean_q: 19.481813
[Info] New level: -0.43096768856048584 | Considering 10/90 traces
 15000/100000: episode: 150, duration: 4.520s, episode steps: 100, steps per second: 22, episode reward: 52.774, mean reward: 0.528 [0.293, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.864, 10.098], loss: 0.013984, mae: 0.125437, mean_q: 19.433653
 15002/100000: episode: 151, duration: 0.015s, episode steps: 2, steps per second: 138, episode reward: 0.743, mean reward: 0.371 [0.333, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.281, 10.100], loss: 0.013767, mae: 0.129820, mean_q: 18.083492
 15004/100000: episode: 152, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.709, mean reward: 0.355 [0.354, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.200, 10.100], loss: 0.010807, mae: 0.115394, mean_q: 20.930622
 15006/100000: episode: 153, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.964, mean reward: 0.482 [0.457, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.351, 10.100], loss: 0.021620, mae: 0.136428, mean_q: 20.571985
 15008/100000: episode: 154, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.725, mean reward: 0.362 [0.333, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.338, 10.100], loss: 0.011734, mae: 0.118983, mean_q: 19.110403
 15010/100000: episode: 155, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.582, mean reward: 0.291 [0.288, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.256, 10.100], loss: 0.015230, mae: 0.120188, mean_q: 19.706926
 15012/100000: episode: 156, duration: 0.015s, episode steps: 2, steps per second: 138, episode reward: 1.039, mean reward: 0.520 [0.494, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.330, 10.100], loss: 0.008612, mae: 0.109810, mean_q: 18.372692
 15014/100000: episode: 157, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 1.104, mean reward: 0.552 [0.524, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.232, 10.100], loss: 0.010254, mae: 0.117620, mean_q: 21.078011
 15016/100000: episode: 158, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.550, mean reward: 0.275 [0.267, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.300, 10.100], loss: 0.012607, mae: 0.122105, mean_q: 19.083672
 15018/100000: episode: 159, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 1.095, mean reward: 0.548 [0.523, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.282, 10.100], loss: 0.020622, mae: 0.135432, mean_q: 19.392658
 15020/100000: episode: 160, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.874, mean reward: 0.437 [0.379, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.274, 10.100], loss: 0.016312, mae: 0.151544, mean_q: 19.473671
 15022/100000: episode: 161, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.969, mean reward: 0.484 [0.415, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.279, 10.100], loss: 0.017382, mae: 0.140569, mean_q: 18.918900
 15024/100000: episode: 162, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.805, mean reward: 0.403 [0.402, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.370, 10.100], loss: 0.016618, mae: 0.153769, mean_q: 17.344303
 15026/100000: episode: 163, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.951, mean reward: 0.475 [0.462, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.406, 10.100], loss: 0.013184, mae: 0.125765, mean_q: 18.143719
 15028/100000: episode: 164, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.777, mean reward: 0.389 [0.362, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.357, 10.100], loss: 0.016117, mae: 0.138627, mean_q: 19.997417
 15030/100000: episode: 165, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.971, mean reward: 0.485 [0.471, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.306, 10.100], loss: 0.011779, mae: 0.109073, mean_q: 20.886889
 15032/100000: episode: 166, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.841, mean reward: 0.420 [0.398, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.176, 10.100], loss: 0.011421, mae: 0.110989, mean_q: 19.689404
 15034/100000: episode: 167, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 1.072, mean reward: 0.536 [0.524, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.255, 10.100], loss: 0.021126, mae: 0.123381, mean_q: 19.075333
 15036/100000: episode: 168, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.598, mean reward: 0.299 [0.254, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.266, 10.100], loss: 0.013309, mae: 0.126334, mean_q: 18.873386
 15038/100000: episode: 169, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.856, mean reward: 0.428 [0.427, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.352, 10.100], loss: 0.007865, mae: 0.099142, mean_q: 19.397940
 15040/100000: episode: 170, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.774, mean reward: 0.387 [0.384, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.360, 10.100], loss: 0.010408, mae: 0.110600, mean_q: 18.198637
 15042/100000: episode: 171, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.977, mean reward: 0.488 [0.476, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.206, 10.100], loss: 0.006153, mae: 0.089297, mean_q: 16.486038
 15044/100000: episode: 172, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.036, mean reward: 0.518 [0.504, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.188, 10.100], loss: 0.010743, mae: 0.115457, mean_q: 19.094435
 15046/100000: episode: 173, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.033, mean reward: 0.517 [0.508, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.288, 10.100], loss: 0.014449, mae: 0.121009, mean_q: 19.507381
 15048/100000: episode: 174, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.959, mean reward: 0.480 [0.444, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.152, 10.100], loss: 0.007795, mae: 0.096292, mean_q: 20.651894
 15050/100000: episode: 175, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.045, mean reward: 0.523 [0.519, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.295, 10.100], loss: 0.011827, mae: 0.104341, mean_q: 16.912050
 15052/100000: episode: 176, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.863, mean reward: 0.431 [0.419, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.482, 10.100], loss: 0.010010, mae: 0.112086, mean_q: 19.523302
 15054/100000: episode: 177, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.809, mean reward: 0.404 [0.391, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.230, 10.100], loss: 0.011738, mae: 0.119670, mean_q: 17.904385
 15056/100000: episode: 178, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.577, mean reward: 0.289 [0.275, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.393, 10.100], loss: 0.014196, mae: 0.130306, mean_q: 19.089771
 15058/100000: episode: 179, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.586, mean reward: 0.293 [0.274, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.329, 10.100], loss: 0.019929, mae: 0.147801, mean_q: 19.483614
 15060/100000: episode: 180, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.739, mean reward: 0.369 [0.365, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.292, 10.100], loss: 0.016613, mae: 0.131300, mean_q: 20.801756
 15062/100000: episode: 181, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.598, mean reward: 0.299 [0.256, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.389, 10.100], loss: 0.014087, mae: 0.110988, mean_q: 19.069702
 15064/100000: episode: 182, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.964, mean reward: 0.482 [0.432, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.266, 10.100], loss: 0.021382, mae: 0.134120, mean_q: 17.120144
 15066/100000: episode: 183, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.419, mean reward: 0.209 [0.201, 0.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.396, 10.100], loss: 0.010349, mae: 0.109506, mean_q: 20.458853
 15068/100000: episode: 184, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.636, mean reward: 0.318 [0.304, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.402, 10.100], loss: 0.026144, mae: 0.154318, mean_q: 18.154381
 15070/100000: episode: 185, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 1.102, mean reward: 0.551 [0.531, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.206, 10.100], loss: 0.012336, mae: 0.120681, mean_q: 19.338825
 15072/100000: episode: 186, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.596, mean reward: 0.298 [0.295, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.337, 10.100], loss: 0.011114, mae: 0.121017, mean_q: 19.723883
 15074/100000: episode: 187, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.061, mean reward: 0.530 [0.511, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.289, 10.100], loss: 0.014290, mae: 0.118911, mean_q: 21.715324
 15076/100000: episode: 188, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.714, mean reward: 0.357 [0.305, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.301, 10.100], loss: 0.009647, mae: 0.106158, mean_q: 21.051918
 15078/100000: episode: 189, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.753, mean reward: 0.376 [0.373, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.238, 10.100], loss: 0.009323, mae: 0.109268, mean_q: 19.322226
 15080/100000: episode: 190, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.912, mean reward: 0.456 [0.433, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.153, 10.100], loss: 0.014200, mae: 0.116081, mean_q: 19.672745
 15082/100000: episode: 191, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.507, mean reward: 0.253 [0.247, 0.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.400, 10.100], loss: 0.024910, mae: 0.152901, mean_q: 18.865189
 15084/100000: episode: 192, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.069, mean reward: 0.535 [0.507, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.321, 10.100], loss: 0.036179, mae: 0.200973, mean_q: 18.331829
 15086/100000: episode: 193, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.014, mean reward: 0.507 [0.505, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.185, 10.100], loss: 0.023248, mae: 0.149729, mean_q: 18.293835
 15088/100000: episode: 194, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.668, mean reward: 0.334 [0.325, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.335, 10.100], loss: 0.017688, mae: 0.136676, mean_q: 19.254765
 15090/100000: episode: 195, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.700, mean reward: 0.350 [0.339, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.251, 10.100], loss: 0.015971, mae: 0.144027, mean_q: 16.776320
 15092/100000: episode: 196, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.945, mean reward: 0.473 [0.380, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.316, 10.100], loss: 0.020646, mae: 0.141698, mean_q: 19.487926
 15094/100000: episode: 197, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.784, mean reward: 0.392 [0.386, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.297, 10.100], loss: 0.014258, mae: 0.138179, mean_q: 19.771397
 15096/100000: episode: 198, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.758, mean reward: 0.379 [0.337, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.369, 10.100], loss: 0.020371, mae: 0.164029, mean_q: 20.066547
 15098/100000: episode: 199, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.844, mean reward: 0.422 [0.383, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.332, 10.100], loss: 0.007989, mae: 0.101579, mean_q: 18.224310
 15100/100000: episode: 200, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.035, mean reward: 0.518 [0.471, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.323, 10.100], loss: 0.014565, mae: 0.140335, mean_q: 17.748426
 15102/100000: episode: 201, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.019, mean reward: 0.509 [0.509, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.252, 10.100], loss: 0.012562, mae: 0.117366, mean_q: 20.600964
 15104/100000: episode: 202, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.762, mean reward: 0.381 [0.379, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.332, 10.100], loss: 0.013182, mae: 0.130709, mean_q: 18.301130
 15106/100000: episode: 203, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.027, mean reward: 0.514 [0.487, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.274, 10.100], loss: 0.020710, mae: 0.170045, mean_q: 21.267799
 15108/100000: episode: 204, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.898, mean reward: 0.449 [0.420, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.369, 10.100], loss: 0.015583, mae: 0.118431, mean_q: 20.060093
 15110/100000: episode: 205, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.924, mean reward: 0.462 [0.432, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.178, 10.100], loss: 0.016414, mae: 0.121802, mean_q: 19.864899
 15112/100000: episode: 206, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.070, mean reward: 0.535 [0.517, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.285, 10.100], loss: 0.010160, mae: 0.107805, mean_q: 19.256788
 15114/100000: episode: 207, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.837, mean reward: 0.418 [0.409, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.346, 10.100], loss: 0.011238, mae: 0.119951, mean_q: 19.553814
 15116/100000: episode: 208, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.881, mean reward: 0.440 [0.431, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.323, 10.100], loss: 0.014642, mae: 0.137899, mean_q: 20.127085
 15118/100000: episode: 209, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.984, mean reward: 0.492 [0.459, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.255, 10.100], loss: 0.008460, mae: 0.105769, mean_q: 18.342083
 15120/100000: episode: 210, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.718, mean reward: 0.359 [0.267, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.297, 10.100], loss: 0.009972, mae: 0.117491, mean_q: 19.398243
 15122/100000: episode: 211, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.945, mean reward: 0.472 [0.445, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.232, 10.100], loss: 0.026440, mae: 0.139104, mean_q: 19.866875
 15124/100000: episode: 212, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.025, mean reward: 0.513 [0.499, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.300, 10.100], loss: 0.052316, mae: 0.190525, mean_q: 18.025663
 15126/100000: episode: 213, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.775, mean reward: 0.387 [0.365, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.454, 10.100], loss: 0.018726, mae: 0.138504, mean_q: 18.885487
 15128/100000: episode: 214, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.738, mean reward: 0.369 [0.346, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.232, 10.100], loss: 0.035740, mae: 0.184091, mean_q: 17.968391
 15130/100000: episode: 215, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.938, mean reward: 0.469 [0.462, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.302, 10.100], loss: 0.011722, mae: 0.124793, mean_q: 19.171886
 15132/100000: episode: 216, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.777, mean reward: 0.388 [0.358, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.248, 10.100], loss: 0.017595, mae: 0.150810, mean_q: 19.450531
 15134/100000: episode: 217, duration: 0.016s, episode steps: 2, steps per second: 121, episode reward: 0.990, mean reward: 0.495 [0.428, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.297, 10.100], loss: 0.023647, mae: 0.171043, mean_q: 20.006632
 15136/100000: episode: 218, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.509, mean reward: 0.255 [0.206, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.381, 10.100], loss: 0.013295, mae: 0.119517, mean_q: 17.714283
 15138/100000: episode: 219, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.810, mean reward: 0.405 [0.394, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.388, 10.100], loss: 0.010540, mae: 0.120702, mean_q: 20.123482
 15140/100000: episode: 220, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.087, mean reward: 0.543 [0.505, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.304, 10.100], loss: 0.009456, mae: 0.107139, mean_q: 21.563343
 15142/100000: episode: 221, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.066, mean reward: 0.533 [0.519, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.294, 10.100], loss: 0.013229, mae: 0.115149, mean_q: 17.785513
 15144/100000: episode: 222, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 1.053, mean reward: 0.527 [0.516, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.348, 10.100], loss: 0.012759, mae: 0.108346, mean_q: 19.739630
 15146/100000: episode: 223, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 1.012, mean reward: 0.506 [0.505, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.261, 10.100], loss: 0.016116, mae: 0.130386, mean_q: 20.734264
 15148/100000: episode: 224, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.807, mean reward: 0.404 [0.347, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.317, 10.100], loss: 0.015034, mae: 0.132891, mean_q: 17.223787
 15150/100000: episode: 225, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.720, mean reward: 0.360 [0.357, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.220, 10.100], loss: 0.013559, mae: 0.118045, mean_q: 20.325783
 15152/100000: episode: 226, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.984, mean reward: 0.492 [0.486, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.236, 10.100], loss: 0.019843, mae: 0.169395, mean_q: 20.028105
 15154/100000: episode: 227, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.173, mean reward: 0.586 [0.582, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.306, 10.100], loss: 0.015735, mae: 0.143928, mean_q: 19.508133
 15157/100000: episode: 228, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 1.208, mean reward: 0.403 [0.369, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.431, 10.100], loss: 0.034930, mae: 0.159252, mean_q: 19.242355
 15159/100000: episode: 229, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.884, mean reward: 0.442 [0.437, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.215, 10.100], loss: 0.027189, mae: 0.143223, mean_q: 17.491341
 15161/100000: episode: 230, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.782, mean reward: 0.391 [0.373, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.320, 10.100], loss: 0.015294, mae: 0.128509, mean_q: 21.096785
 15163/100000: episode: 231, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.684, mean reward: 0.342 [0.322, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.350, 10.100], loss: 0.011171, mae: 0.115614, mean_q: 17.622318
 15165/100000: episode: 232, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.691, mean reward: 0.346 [0.292, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.386, 10.100], loss: 0.008271, mae: 0.104283, mean_q: 19.492886
 15167/100000: episode: 233, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.115, mean reward: 0.557 [0.535, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.272, 10.100], loss: 0.019082, mae: 0.126286, mean_q: 19.760942
 15169/100000: episode: 234, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.749, mean reward: 0.375 [0.372, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.227, 10.100], loss: 0.015025, mae: 0.124315, mean_q: 19.915226
 15171/100000: episode: 235, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.949, mean reward: 0.474 [0.437, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.270, 10.100], loss: 0.016242, mae: 0.139673, mean_q: 18.061152
 15173/100000: episode: 236, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.884, mean reward: 0.442 [0.431, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.369, 10.100], loss: 0.010387, mae: 0.113337, mean_q: 19.064917
 15175/100000: episode: 237, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.850, mean reward: 0.425 [0.423, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.348, 10.100], loss: 0.033643, mae: 0.161384, mean_q: 17.390102
 15177/100000: episode: 238, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.691, mean reward: 0.346 [0.340, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.339, 10.100], loss: 0.015365, mae: 0.121198, mean_q: 17.544201
 15179/100000: episode: 239, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.014, mean reward: 0.507 [0.482, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.264, 10.100], loss: 0.014126, mae: 0.130811, mean_q: 18.384727
[Info] New level: -0.8046883344650269 | Considering 10/90 traces
 15181/100000: episode: 240, duration: 4.035s, episode steps: 2, steps per second: 0, episode reward: 1.007, mean reward: 0.503 [0.475, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.233, 10.100], loss: 0.023457, mae: 0.167398, mean_q: 19.559338
 15182/100000: episode: 241, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.266, mean reward: 0.266 [0.266, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.363, 10.100], loss: 0.017110, mae: 0.125164, mean_q: 17.526703
 15183/100000: episode: 242, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.351, mean reward: 0.351 [0.351, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.436, 10.100], loss: 0.017971, mae: 0.153664, mean_q: 20.548595
 15184/100000: episode: 243, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.236, mean reward: 0.236 [0.236, 0.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.341, 10.100], loss: 0.015515, mae: 0.145388, mean_q: 19.658920
 15185/100000: episode: 244, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.405, mean reward: 0.405 [0.405, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.401, 10.100], loss: 0.007318, mae: 0.092590, mean_q: 16.906414
 15186/100000: episode: 245, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.316, mean reward: 0.316 [0.316, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.391, 10.100], loss: 0.021723, mae: 0.171191, mean_q: 19.445505
 15187/100000: episode: 246, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.407, 10.100], loss: 0.025052, mae: 0.181892, mean_q: 18.262486
 15188/100000: episode: 247, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.448, mean reward: 0.448 [0.448, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.400, 10.100], loss: 0.008436, mae: 0.089911, mean_q: 20.201714
 15189/100000: episode: 248, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.429, 10.100], loss: 0.025376, mae: 0.192638, mean_q: 18.876429
 15190/100000: episode: 249, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.340, 10.100], loss: 0.015501, mae: 0.112853, mean_q: 19.668346
 15191/100000: episode: 250, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.427, mean reward: 0.427 [0.427, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.397, 10.100], loss: 0.011859, mae: 0.130356, mean_q: 16.828419
 15192/100000: episode: 251, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.392, mean reward: 0.392 [0.392, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.353, 10.100], loss: 0.023387, mae: 0.182927, mean_q: 19.864178
 15193/100000: episode: 252, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.374, 10.100], loss: 0.013810, mae: 0.105109, mean_q: 19.150806
 15194/100000: episode: 253, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.210, mean reward: 0.210 [0.210, 0.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.404, 10.100], loss: 0.009125, mae: 0.107313, mean_q: 20.336559
 15195/100000: episode: 254, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.380, mean reward: 0.380 [0.380, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.399, 10.100], loss: 0.011205, mae: 0.116487, mean_q: 16.501436
 15196/100000: episode: 255, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.265, mean reward: 0.265 [0.265, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.375, 10.100], loss: 0.012622, mae: 0.123042, mean_q: 17.751945
 15197/100000: episode: 256, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.287, mean reward: 0.287 [0.287, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.438, 10.100], loss: 0.022035, mae: 0.152063, mean_q: 17.738867
 15198/100000: episode: 257, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.421, mean reward: 0.421 [0.421, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.465, 10.100], loss: 0.017009, mae: 0.130343, mean_q: 16.661720
 15199/100000: episode: 258, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.403, 10.100], loss: 0.013248, mae: 0.111839, mean_q: 18.327389
 15200/100000: episode: 259, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.253, mean reward: 0.253 [0.253, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.390, 10.100], loss: 0.068718, mae: 0.158379, mean_q: 18.932812
 15201/100000: episode: 260, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.237, mean reward: 0.237 [0.237, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.434, 10.100], loss: 0.039817, mae: 0.196050, mean_q: 18.453257
 15202/100000: episode: 261, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.248, mean reward: 0.248 [0.248, 0.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.390, 10.100], loss: 0.016428, mae: 0.141511, mean_q: 18.936371
 15203/100000: episode: 262, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.418, mean reward: 0.418 [0.418, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.493, 10.100], loss: 0.015635, mae: 0.142878, mean_q: 19.552662
 15204/100000: episode: 263, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.366, mean reward: 0.366 [0.366, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.398, 10.100], loss: 0.020604, mae: 0.157177, mean_q: 19.039248
 15205/100000: episode: 264, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.392, mean reward: 0.392 [0.392, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.405, 10.100], loss: 0.037684, mae: 0.151608, mean_q: 20.187803
 15206/100000: episode: 265, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.372, 10.100], loss: 0.010780, mae: 0.116497, mean_q: 20.108221
 15207/100000: episode: 266, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.441, 10.100], loss: 0.013443, mae: 0.100303, mean_q: 20.109449
 15208/100000: episode: 267, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.330, 10.100], loss: 0.013391, mae: 0.136452, mean_q: 19.719835
 15209/100000: episode: 268, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.320, mean reward: 0.320 [0.320, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.578, 10.100], loss: 0.013105, mae: 0.108061, mean_q: 20.242363
 15210/100000: episode: 269, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.467, 10.100], loss: 0.009807, mae: 0.114220, mean_q: 17.954584
 15211/100000: episode: 270, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.205, mean reward: 0.205 [0.205, 0.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.387, 10.100], loss: 0.023710, mae: 0.140762, mean_q: 20.513611
 15212/100000: episode: 271, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.433, mean reward: 0.433 [0.433, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.348, 10.100], loss: 0.027329, mae: 0.154302, mean_q: 18.647888
 15213/100000: episode: 272, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.326, mean reward: 0.326 [0.326, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.334, 10.100], loss: 0.016161, mae: 0.117489, mean_q: 17.992804
 15214/100000: episode: 273, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.268, mean reward: 0.268 [0.268, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.330, 10.100], loss: 0.066015, mae: 0.160405, mean_q: 19.007444
 15215/100000: episode: 274, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.227, mean reward: 0.227 [0.227, 0.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.386, 10.100], loss: 0.019301, mae: 0.149330, mean_q: 20.328938
 15216/100000: episode: 275, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.341, mean reward: 0.341 [0.341, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.390, 10.100], loss: 0.019050, mae: 0.164943, mean_q: 18.156340
 15217/100000: episode: 276, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.478, mean reward: 0.478 [0.478, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.357, 10.100], loss: 0.009725, mae: 0.114582, mean_q: 17.428795
 15218/100000: episode: 277, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.366, mean reward: 0.366 [0.366, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.382, 10.100], loss: 0.011848, mae: 0.118812, mean_q: 19.274529
 15219/100000: episode: 278, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.402, mean reward: 0.402 [0.402, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.373, 10.100], loss: 0.023894, mae: 0.172782, mean_q: 15.649488
 15220/100000: episode: 279, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.307, 10.100], loss: 0.013145, mae: 0.135594, mean_q: 21.676796
 15221/100000: episode: 280, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.346, mean reward: 0.346 [0.346, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.389, 10.100], loss: 0.015430, mae: 0.145932, mean_q: 20.274754
 15222/100000: episode: 281, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.114, mean reward: 0.114 [0.114, 0.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.391, 10.100], loss: 0.012812, mae: 0.109337, mean_q: 20.927153
 15223/100000: episode: 282, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.253, mean reward: 0.253 [0.253, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.388, 10.100], loss: 0.013503, mae: 0.130080, mean_q: 19.064938
 15224/100000: episode: 283, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.317, mean reward: 0.317 [0.317, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.403, 10.100], loss: 0.007842, mae: 0.105176, mean_q: 18.252354
 15225/100000: episode: 284, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.269, mean reward: 0.269 [0.269, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.393, 10.100], loss: 0.018412, mae: 0.128396, mean_q: 20.679401
 15226/100000: episode: 285, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.381, mean reward: 0.381 [0.381, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.299, 10.100], loss: 0.014190, mae: 0.133392, mean_q: 18.109806
 15227/100000: episode: 286, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.325, mean reward: 0.325 [0.325, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.348, 10.100], loss: 0.010548, mae: 0.119656, mean_q: 19.791283
 15228/100000: episode: 287, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.267, mean reward: 0.267 [0.267, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.442, 10.100], loss: 0.024555, mae: 0.149191, mean_q: 14.617767
 15229/100000: episode: 288, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.182, mean reward: 0.182 [0.182, 0.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.390, 10.100], loss: 0.015570, mae: 0.135044, mean_q: 17.634705
 15230/100000: episode: 289, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.303, mean reward: 0.303 [0.303, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.301, 10.100], loss: 0.010160, mae: 0.095761, mean_q: 19.451830
 15231/100000: episode: 290, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.348, mean reward: 0.348 [0.348, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.360, 10.100], loss: 0.009408, mae: 0.108628, mean_q: 18.829098
 15232/100000: episode: 291, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.301, mean reward: 0.301 [0.301, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.394, 10.100], loss: 0.023396, mae: 0.157541, mean_q: 19.662188
 15233/100000: episode: 292, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.181, mean reward: 0.181 [0.181, 0.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.407, 10.100], loss: 0.015214, mae: 0.133461, mean_q: 19.176451
 15234/100000: episode: 293, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.266, mean reward: 0.266 [0.266, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.362, 10.100], loss: 0.019039, mae: 0.165309, mean_q: 13.762703
 15235/100000: episode: 294, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.384, mean reward: 0.384 [0.384, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.314, 10.100], loss: 0.021631, mae: 0.148577, mean_q: 18.461636
 15236/100000: episode: 295, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.223, mean reward: 0.223 [0.223, 0.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.366, 10.100], loss: 0.026562, mae: 0.165705, mean_q: 17.101145
 15237/100000: episode: 296, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.423, mean reward: 0.423 [0.423, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.441, 10.100], loss: 0.017332, mae: 0.147596, mean_q: 18.755882
 15238/100000: episode: 297, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.350, mean reward: 0.350 [0.350, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.390, 10.100], loss: 0.024327, mae: 0.141208, mean_q: 19.342884
 15239/100000: episode: 298, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.278, mean reward: 0.278 [0.278, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.387, 10.100], loss: 0.009308, mae: 0.106985, mean_q: 18.424566
 15240/100000: episode: 299, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.354, mean reward: 0.354 [0.354, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.462, 10.100], loss: 0.018519, mae: 0.157655, mean_q: 16.146687
 15241/100000: episode: 300, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.350, mean reward: 0.350 [0.350, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.391, 10.100], loss: 0.007081, mae: 0.100996, mean_q: 17.649780
 15242/100000: episode: 301, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 0.277, mean reward: 0.277 [0.277, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.413, 10.100], loss: 0.065883, mae: 0.195646, mean_q: 15.931408
 15243/100000: episode: 302, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.222, mean reward: 0.222 [0.222, 0.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.367, 10.100], loss: 0.008729, mae: 0.103771, mean_q: 17.334352
 15244/100000: episode: 303, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.457, 10.100], loss: 0.016318, mae: 0.146427, mean_q: 17.380451
 15245/100000: episode: 304, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.406, mean reward: 0.406 [0.406, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.443, 10.100], loss: 0.007202, mae: 0.099397, mean_q: 21.299099
 15246/100000: episode: 305, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.358, mean reward: 0.358 [0.358, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.358, 10.100], loss: 0.013403, mae: 0.116865, mean_q: 19.129314
 15247/100000: episode: 306, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.255, mean reward: 0.255 [0.255, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.379, 10.100], loss: 0.011542, mae: 0.108544, mean_q: 16.860905
 15248/100000: episode: 307, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.361, mean reward: 0.361 [0.361, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.365, 10.100], loss: 0.042551, mae: 0.169107, mean_q: 14.264977
 15249/100000: episode: 308, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.216, mean reward: 0.216 [0.216, 0.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.377, 10.100], loss: 0.012214, mae: 0.121367, mean_q: 15.060446
 15250/100000: episode: 309, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.190, mean reward: 0.190 [0.190, 0.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.386, 10.100], loss: 0.013036, mae: 0.123951, mean_q: 18.676786
 15251/100000: episode: 310, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.321, mean reward: 0.321 [0.321, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.389, 10.100], loss: 0.011209, mae: 0.122495, mean_q: 19.255709
 15252/100000: episode: 311, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.224, mean reward: 0.224 [0.224, 0.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.346, 10.100], loss: 0.007555, mae: 0.095715, mean_q: 17.752069
 15253/100000: episode: 312, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.345, mean reward: 0.345 [0.345, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.364, 10.100], loss: 0.004572, mae: 0.071581, mean_q: 22.076603
 15254/100000: episode: 313, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.378, mean reward: 0.378 [0.378, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.404, 10.100], loss: 0.018991, mae: 0.142974, mean_q: 19.303219
 15255/100000: episode: 314, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.380, mean reward: 0.380 [0.380, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.414, 10.100], loss: 0.024580, mae: 0.160844, mean_q: 17.411879
 15256/100000: episode: 315, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.261, mean reward: 0.261 [0.261, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.411, 10.100], loss: 0.007742, mae: 0.099071, mean_q: 19.603050
 15257/100000: episode: 316, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.400, 10.100], loss: 0.017177, mae: 0.123813, mean_q: 17.355202
 15258/100000: episode: 317, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.392, mean reward: 0.392 [0.392, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.766, 10.100], loss: 0.036987, mae: 0.202218, mean_q: 18.802197
 15259/100000: episode: 318, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.239, mean reward: 0.239 [0.239, 0.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.386, 10.100], loss: 0.012810, mae: 0.124090, mean_q: 21.734947
 15260/100000: episode: 319, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.208, mean reward: 0.208 [0.208, 0.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.401, 10.100], loss: 0.015621, mae: 0.149622, mean_q: 16.626717
 15261/100000: episode: 320, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.378, 10.100], loss: 0.015911, mae: 0.145463, mean_q: 19.407995
 15262/100000: episode: 321, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.410, mean reward: 0.410 [0.410, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.425, 10.100], loss: 0.011522, mae: 0.114683, mean_q: 20.886951
 15263/100000: episode: 322, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.229, mean reward: 0.229 [0.229, 0.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.357, 10.100], loss: 0.022077, mae: 0.181999, mean_q: 19.296446
 15264/100000: episode: 323, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.240, mean reward: 0.240 [0.240, 0.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.349, 10.100], loss: 0.027986, mae: 0.186815, mean_q: 16.990795
 15265/100000: episode: 324, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.287, mean reward: 0.287 [0.287, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.360, 10.100], loss: 0.022775, mae: 0.160044, mean_q: 18.285532
 15266/100000: episode: 325, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.302, mean reward: 0.302 [0.302, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.414, 10.100], loss: 0.065069, mae: 0.203989, mean_q: 21.196455
 15267/100000: episode: 326, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.279, mean reward: 0.279 [0.279, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.372, 10.100], loss: 0.020210, mae: 0.141621, mean_q: 18.558155
 15268/100000: episode: 327, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.425, 10.100], loss: 0.016402, mae: 0.154924, mean_q: 19.668327
 15269/100000: episode: 328, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.405, mean reward: 0.405 [0.405, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.398, 10.100], loss: 0.015425, mae: 0.116504, mean_q: 17.717699
 15270/100000: episode: 329, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.345, mean reward: 0.345 [0.345, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.378, 10.100], loss: 0.021126, mae: 0.143373, mean_q: 20.086540
[Info] Not found new level, current best level reached = -0.8046883344650269
 15271/100000: episode: 330, duration: 3.999s, episode steps: 1, steps per second: 0, episode reward: 0.430, mean reward: 0.430 [0.430, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.342, 10.100], loss: 0.029226, mae: 0.183936, mean_q: 17.195379
 15371/100000: episode: 331, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 54.664, mean reward: 0.547 [0.290, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.267, 10.246], loss: 0.017355, mae: 0.133171, mean_q: 18.417614
 15471/100000: episode: 332, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 41.463, mean reward: 0.415 [0.251, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.093, 10.477], loss: 0.024752, mae: 0.158684, mean_q: 18.041418
 15571/100000: episode: 333, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 56.307, mean reward: 0.563 [0.219, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.075, 10.140], loss: 0.018688, mae: 0.133039, mean_q: 18.638704
 15671/100000: episode: 334, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 46.918, mean reward: 0.469 [0.237, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.872, 10.137], loss: 0.014505, mae: 0.124295, mean_q: 18.067280
 15771/100000: episode: 335, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 56.652, mean reward: 0.567 [0.410, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.114, 10.132], loss: 0.021671, mae: 0.145733, mean_q: 18.451279
 15871/100000: episode: 336, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 55.656, mean reward: 0.557 [0.264, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.484, 10.098], loss: 0.023076, mae: 0.151148, mean_q: 18.653933
 15971/100000: episode: 337, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.782, mean reward: 0.548 [0.299, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.751, 10.098], loss: 0.019315, mae: 0.128976, mean_q: 18.515650
 16071/100000: episode: 338, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 54.597, mean reward: 0.546 [0.206, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.231, 10.106], loss: 0.015482, mae: 0.124468, mean_q: 18.455345
 16171/100000: episode: 339, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.000, mean reward: 0.560 [0.382, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.330, 10.098], loss: 0.025473, mae: 0.156456, mean_q: 18.145576
 16271/100000: episode: 340, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 55.116, mean reward: 0.551 [0.245, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.655, 10.329], loss: 0.020639, mae: 0.146544, mean_q: 18.366100
 16371/100000: episode: 341, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 49.545, mean reward: 0.495 [0.276, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.476, 10.178], loss: 0.023855, mae: 0.153657, mean_q: 18.573553
 16471/100000: episode: 342, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 52.550, mean reward: 0.525 [0.259, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.753, 10.098], loss: 0.024931, mae: 0.150406, mean_q: 18.291666
 16571/100000: episode: 343, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 51.652, mean reward: 0.517 [0.142, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.598, 10.123], loss: 0.016793, mae: 0.126660, mean_q: 18.532301
 16671/100000: episode: 344, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 56.062, mean reward: 0.561 [0.215, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.616, 10.261], loss: 0.019853, mae: 0.139748, mean_q: 18.544743
 16771/100000: episode: 345, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 55.647, mean reward: 0.556 [0.327, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.912, 10.210], loss: 0.024099, mae: 0.151712, mean_q: 18.513199
 16871/100000: episode: 346, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 50.248, mean reward: 0.502 [0.300, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.539, 10.245], loss: 0.024291, mae: 0.147804, mean_q: 18.602655
 16971/100000: episode: 347, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.956, mean reward: 0.590 [0.389, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.629, 10.098], loss: 0.022400, mae: 0.139356, mean_q: 18.566282
 17071/100000: episode: 348, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 52.727, mean reward: 0.527 [0.265, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.305, 10.205], loss: 0.024343, mae: 0.146579, mean_q: 18.420820
 17171/100000: episode: 349, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 49.174, mean reward: 0.492 [0.194, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.208, 10.429], loss: 0.022512, mae: 0.138528, mean_q: 18.492611
 17271/100000: episode: 350, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 52.995, mean reward: 0.530 [0.228, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.914, 10.307], loss: 0.023485, mae: 0.151742, mean_q: 18.582148
 17371/100000: episode: 351, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 55.205, mean reward: 0.552 [0.378, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.683, 10.186], loss: 0.023412, mae: 0.154998, mean_q: 18.584488
 17471/100000: episode: 352, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 53.132, mean reward: 0.531 [0.305, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.460, 10.415], loss: 0.021057, mae: 0.140524, mean_q: 18.634541
 17571/100000: episode: 353, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 53.748, mean reward: 0.537 [0.318, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.577, 10.271], loss: 0.021587, mae: 0.138159, mean_q: 18.590521
 17671/100000: episode: 354, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 52.718, mean reward: 0.527 [0.359, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.354, 10.098], loss: 0.023041, mae: 0.142379, mean_q: 18.478725
 17771/100000: episode: 355, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 55.251, mean reward: 0.553 [0.317, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.222, 10.098], loss: 0.020947, mae: 0.139682, mean_q: 18.680462
 17871/100000: episode: 356, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 58.174, mean reward: 0.582 [0.448, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.528, 10.265], loss: 0.027755, mae: 0.158638, mean_q: 18.017651
 17971/100000: episode: 357, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 55.782, mean reward: 0.558 [0.292, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.174, 10.182], loss: 0.027523, mae: 0.154894, mean_q: 18.612276
 18071/100000: episode: 358, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 54.315, mean reward: 0.543 [0.269, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.515, 10.252], loss: 0.025617, mae: 0.153532, mean_q: 18.320541
 18171/100000: episode: 359, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 52.937, mean reward: 0.529 [0.093, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.684, 10.116], loss: 0.022183, mae: 0.144004, mean_q: 18.735680
 18271/100000: episode: 360, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: 54.234, mean reward: 0.542 [0.334, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.430, 10.098], loss: 0.020733, mae: 0.132396, mean_q: 18.661268
 18371/100000: episode: 361, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 50.636, mean reward: 0.506 [0.192, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.733, 10.416], loss: 0.022476, mae: 0.138207, mean_q: 18.825228
 18471/100000: episode: 362, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 52.442, mean reward: 0.524 [0.273, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.679, 10.098], loss: 0.022437, mae: 0.140953, mean_q: 18.832170
 18571/100000: episode: 363, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 54.559, mean reward: 0.546 [0.331, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.433, 10.098], loss: 0.018661, mae: 0.128421, mean_q: 18.337170
 18671/100000: episode: 364, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 49.944, mean reward: 0.499 [0.274, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.662, 10.098], loss: 0.021726, mae: 0.135053, mean_q: 18.677490
 18771/100000: episode: 365, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 54.410, mean reward: 0.544 [0.257, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.707, 10.098], loss: 0.024181, mae: 0.141655, mean_q: 18.358269
 18871/100000: episode: 366, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 43.395, mean reward: 0.434 [0.046, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.830, 10.194], loss: 0.018726, mae: 0.130083, mean_q: 18.353636
 18971/100000: episode: 367, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 51.066, mean reward: 0.511 [0.217, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.300, 10.098], loss: 0.020071, mae: 0.125475, mean_q: 18.714031
 19071/100000: episode: 368, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 47.616, mean reward: 0.476 [0.262, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.530, 10.522], loss: 0.021349, mae: 0.139842, mean_q: 18.694641
 19171/100000: episode: 369, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 52.627, mean reward: 0.526 [0.345, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.403, 10.098], loss: 0.024574, mae: 0.145922, mean_q: 18.612778
 19271/100000: episode: 370, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 53.229, mean reward: 0.532 [0.272, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.742, 10.098], loss: 0.016109, mae: 0.125453, mean_q: 19.008757
 19371/100000: episode: 371, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 53.055, mean reward: 0.531 [0.198, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.962, 10.312], loss: 0.020267, mae: 0.134433, mean_q: 18.830853
 19471/100000: episode: 372, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 52.467, mean reward: 0.525 [0.281, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.390, 10.098], loss: 0.017461, mae: 0.126107, mean_q: 18.780729
 19571/100000: episode: 373, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 52.390, mean reward: 0.524 [0.317, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.894, 10.175], loss: 0.024574, mae: 0.149508, mean_q: 18.467798
 19671/100000: episode: 374, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 53.475, mean reward: 0.535 [0.379, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.269, 10.098], loss: 0.017854, mae: 0.126839, mean_q: 18.488319
 19771/100000: episode: 375, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 54.120, mean reward: 0.541 [0.351, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.662, 10.196], loss: 0.022812, mae: 0.137362, mean_q: 18.333712
 19871/100000: episode: 376, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 52.868, mean reward: 0.529 [0.285, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.134, 10.110], loss: 0.018085, mae: 0.125189, mean_q: 18.933010
 19971/100000: episode: 377, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 53.526, mean reward: 0.535 [0.359, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.866, 10.098], loss: 0.019010, mae: 0.129499, mean_q: 19.162453
 20071/100000: episode: 378, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 55.729, mean reward: 0.557 [0.360, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.571, 10.098], loss: 0.020389, mae: 0.133481, mean_q: 19.568306
 20171/100000: episode: 379, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 53.222, mean reward: 0.532 [0.313, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.639, 10.098], loss: 0.015873, mae: 0.125764, mean_q: 19.749989
 20271/100000: episode: 380, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 51.687, mean reward: 0.517 [0.295, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.440, 10.098], loss: 0.020503, mae: 0.137466, mean_q: 19.830381
 20371/100000: episode: 381, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 50.706, mean reward: 0.507 [0.166, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.216, 10.395], loss: 0.020524, mae: 0.141798, mean_q: 19.818619
 20471/100000: episode: 382, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 53.139, mean reward: 0.531 [0.301, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.691, 10.274], loss: 0.016071, mae: 0.128421, mean_q: 19.965700
 20571/100000: episode: 383, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 56.162, mean reward: 0.562 [0.356, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.527, 10.192], loss: 0.013500, mae: 0.113886, mean_q: 19.746494
 20671/100000: episode: 384, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 52.327, mean reward: 0.523 [0.323, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.315, 10.232], loss: 0.014845, mae: 0.124400, mean_q: 19.907682
 20771/100000: episode: 385, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 48.090, mean reward: 0.481 [0.249, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.625, 10.115], loss: 0.014630, mae: 0.123462, mean_q: 19.661585
 20871/100000: episode: 386, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 51.813, mean reward: 0.518 [0.173, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.308, 10.098], loss: 0.014990, mae: 0.127694, mean_q: 19.498039
 20971/100000: episode: 387, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 53.635, mean reward: 0.536 [0.228, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.790, 10.173], loss: 0.012304, mae: 0.114982, mean_q: 19.709990
 21071/100000: episode: 388, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 55.221, mean reward: 0.552 [0.359, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.419, 10.098], loss: 0.013089, mae: 0.121216, mean_q: 19.877571
 21171/100000: episode: 389, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 51.689, mean reward: 0.517 [0.148, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.842, 10.098], loss: 0.012648, mae: 0.118315, mean_q: 19.799322
 21271/100000: episode: 390, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 55.241, mean reward: 0.552 [0.292, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.072, 10.098], loss: 0.013043, mae: 0.122727, mean_q: 19.902122
 21371/100000: episode: 391, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 56.295, mean reward: 0.563 [0.284, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.934, 10.098], loss: 0.010613, mae: 0.109624, mean_q: 19.758678
 21471/100000: episode: 392, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 50.842, mean reward: 0.508 [0.321, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.544, 10.098], loss: 0.011418, mae: 0.115247, mean_q: 19.429522
 21571/100000: episode: 393, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 53.232, mean reward: 0.532 [0.331, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.602, 10.098], loss: 0.011987, mae: 0.116426, mean_q: 19.891180
 21671/100000: episode: 394, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 52.973, mean reward: 0.530 [0.248, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.404, 10.120], loss: 0.011549, mae: 0.115523, mean_q: 19.865070
 21771/100000: episode: 395, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 52.746, mean reward: 0.527 [0.338, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.487, 10.276], loss: 0.012069, mae: 0.117055, mean_q: 19.771242
 21871/100000: episode: 396, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 53.103, mean reward: 0.531 [0.309, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.018, 10.203], loss: 0.012092, mae: 0.118401, mean_q: 20.156685
 21971/100000: episode: 397, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 51.017, mean reward: 0.510 [0.238, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.568, 10.544], loss: 0.012769, mae: 0.122966, mean_q: 19.871990
 22071/100000: episode: 398, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 52.695, mean reward: 0.527 [0.225, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.255, 10.156], loss: 0.010735, mae: 0.109541, mean_q: 19.840176
 22171/100000: episode: 399, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 56.143, mean reward: 0.561 [0.318, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.244, 10.098], loss: 0.012392, mae: 0.117844, mean_q: 20.010601
 22271/100000: episode: 400, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 53.031, mean reward: 0.530 [0.324, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.079, 10.176], loss: 0.010591, mae: 0.111484, mean_q: 19.935261
 22371/100000: episode: 401, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 52.652, mean reward: 0.527 [0.227, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.810, 10.247], loss: 0.010748, mae: 0.112427, mean_q: 19.805458
 22471/100000: episode: 402, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 52.708, mean reward: 0.527 [0.277, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.501, 10.098], loss: 0.011707, mae: 0.117221, mean_q: 19.526791
 22571/100000: episode: 403, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 49.318, mean reward: 0.493 [0.127, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.550, 10.279], loss: 0.011869, mae: 0.117587, mean_q: 19.917793
 22671/100000: episode: 404, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 52.408, mean reward: 0.524 [0.212, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.421, 10.159], loss: 0.009957, mae: 0.107028, mean_q: 19.722612
 22771/100000: episode: 405, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 54.329, mean reward: 0.543 [0.200, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.218, 10.098], loss: 0.010055, mae: 0.108461, mean_q: 19.810980
 22871/100000: episode: 406, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 52.367, mean reward: 0.524 [0.261, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.218, 10.151], loss: 0.011846, mae: 0.118951, mean_q: 19.467333
 22971/100000: episode: 407, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 53.573, mean reward: 0.536 [0.357, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.248, 10.098], loss: 0.010676, mae: 0.112178, mean_q: 19.621557
 23071/100000: episode: 408, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 54.582, mean reward: 0.546 [0.352, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.529, 10.221], loss: 0.011855, mae: 0.118866, mean_q: 20.024294
 23171/100000: episode: 409, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 55.077, mean reward: 0.551 [0.401, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.105, 10.098], loss: 0.012739, mae: 0.123726, mean_q: 19.539976
 23271/100000: episode: 410, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 51.360, mean reward: 0.514 [0.220, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.448, 10.461], loss: 0.010733, mae: 0.112168, mean_q: 19.671019
 23371/100000: episode: 411, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 53.190, mean reward: 0.532 [0.284, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.674, 10.098], loss: 0.008513, mae: 0.100677, mean_q: 19.706430
 23471/100000: episode: 412, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 52.306, mean reward: 0.523 [0.243, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.203, 10.098], loss: 0.009033, mae: 0.101760, mean_q: 19.629030
 23571/100000: episode: 413, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 54.494, mean reward: 0.545 [0.306, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.423, 10.133], loss: 0.009448, mae: 0.105285, mean_q: 19.430941
 23671/100000: episode: 414, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 54.334, mean reward: 0.543 [0.326, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.757, 10.098], loss: 0.008666, mae: 0.101788, mean_q: 19.639677
 23771/100000: episode: 415, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 55.520, mean reward: 0.555 [0.309, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.086, 10.098], loss: 0.009111, mae: 0.104128, mean_q: 19.884773
 23871/100000: episode: 416, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 48.609, mean reward: 0.486 [0.294, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.481, 10.098], loss: 0.010893, mae: 0.112366, mean_q: 19.518227
 23971/100000: episode: 417, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 54.054, mean reward: 0.541 [0.327, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.195, 10.221], loss: 0.008643, mae: 0.100437, mean_q: 19.478209
 24071/100000: episode: 418, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 52.094, mean reward: 0.521 [0.279, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.623, 10.098], loss: 0.007195, mae: 0.093176, mean_q: 19.528946
 24171/100000: episode: 419, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.262, mean reward: 0.533 [0.278, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.862, 10.239], loss: 0.008900, mae: 0.104649, mean_q: 20.063278
 24271/100000: episode: 420, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 50.887, mean reward: 0.509 [0.189, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.534, 10.149], loss: 0.009002, mae: 0.101506, mean_q: 19.778223
 24371/100000: episode: 421, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 52.463, mean reward: 0.525 [0.345, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.653, 10.107], loss: 0.010769, mae: 0.113449, mean_q: 19.857882
 24471/100000: episode: 422, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 53.933, mean reward: 0.539 [0.302, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.587, 10.098], loss: 0.009337, mae: 0.104173, mean_q: 19.554552
 24571/100000: episode: 423, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 56.471, mean reward: 0.565 [0.343, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.380, 10.245], loss: 0.007391, mae: 0.093534, mean_q: 19.975157
 24671/100000: episode: 424, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 48.768, mean reward: 0.488 [0.235, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.101, 10.098], loss: 0.008129, mae: 0.096781, mean_q: 19.698694
 24771/100000: episode: 425, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 53.796, mean reward: 0.538 [0.312, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.781, 10.196], loss: 0.008095, mae: 0.098741, mean_q: 19.753511
 24871/100000: episode: 426, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 53.977, mean reward: 0.540 [0.235, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.643, 10.274], loss: 0.007351, mae: 0.094095, mean_q: 19.472185
 24971/100000: episode: 427, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 38.138, mean reward: 0.381 [0.155, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.413, 10.287], loss: 0.009092, mae: 0.102728, mean_q: 19.655685
 25071/100000: episode: 428, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 50.695, mean reward: 0.507 [0.286, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.404, 10.098], loss: 0.009427, mae: 0.107506, mean_q: 19.683962
 25171/100000: episode: 429, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 53.003, mean reward: 0.530 [0.327, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.415, 10.119], loss: 0.008338, mae: 0.101877, mean_q: 19.700607
[Info] New level: -0.059600651264190674 | Considering 10/90 traces
 25271/100000: episode: 430, duration: 4.446s, episode steps: 100, steps per second: 22, episode reward: 55.362, mean reward: 0.554 [0.261, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.447, 10.098], loss: 0.010415, mae: 0.111538, mean_q: 19.672031
 25273/100000: episode: 431, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.844, mean reward: 0.422 [0.393, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.262, 10.100], loss: 0.004665, mae: 0.075442, mean_q: 21.509039
 25275/100000: episode: 432, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 1.026, mean reward: 0.513 [0.448, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.258, 10.100], loss: 0.007353, mae: 0.082758, mean_q: 20.328577
 25277/100000: episode: 433, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.799, mean reward: 0.399 [0.382, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.118, 10.100], loss: 0.005568, mae: 0.082844, mean_q: 18.823645
 25279/100000: episode: 434, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.859, mean reward: 0.429 [0.427, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.187, 10.100], loss: 0.004240, mae: 0.070492, mean_q: 21.783825
 25281/100000: episode: 435, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 0.993, mean reward: 0.497 [0.452, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.214, 10.100], loss: 0.007671, mae: 0.093915, mean_q: 23.143490
 25283/100000: episode: 436, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.017, mean reward: 0.508 [0.463, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.205, 10.100], loss: 0.008991, mae: 0.104173, mean_q: 18.015419
 25285/100000: episode: 437, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.036, mean reward: 0.518 [0.509, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.225, 10.100], loss: 0.007428, mae: 0.101333, mean_q: 21.287922
 25288/100000: episode: 438, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.591, mean reward: 0.197 [0.181, 0.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.404, 10.100], loss: 0.009228, mae: 0.096906, mean_q: 16.513433
 25290/100000: episode: 439, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.854, mean reward: 0.427 [0.421, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.232, 10.100], loss: 0.010910, mae: 0.113695, mean_q: 20.276421
 25292/100000: episode: 440, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 1.026, mean reward: 0.513 [0.510, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.369, 10.100], loss: 0.013570, mae: 0.108298, mean_q: 19.854023
 25294/100000: episode: 441, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.788, mean reward: 0.394 [0.373, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.246, 10.100], loss: 0.009123, mae: 0.113815, mean_q: 20.846210
 25296/100000: episode: 442, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.836, mean reward: 0.418 [0.416, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.245, 10.100], loss: 0.008767, mae: 0.107314, mean_q: 19.523594
 25299/100000: episode: 443, duration: 0.021s, episode steps: 3, steps per second: 143, episode reward: 0.846, mean reward: 0.282 [0.234, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.401, 10.100], loss: 0.008209, mae: 0.105688, mean_q: 20.034174
 25302/100000: episode: 444, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.874, mean reward: 0.291 [0.278, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.388, 10.100], loss: 0.016057, mae: 0.129948, mean_q: 19.986708
 25304/100000: episode: 445, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.999, mean reward: 0.499 [0.475, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.163, 10.100], loss: 0.013603, mae: 0.123444, mean_q: 22.600063
 25306/100000: episode: 446, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.058, mean reward: 0.529 [0.528, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.247, 10.100], loss: 0.022461, mae: 0.154563, mean_q: 18.710823
 25308/100000: episode: 447, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.610, mean reward: 0.305 [0.226, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.346, 10.100], loss: 0.004622, mae: 0.075071, mean_q: 20.251238
 25310/100000: episode: 448, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.651, mean reward: 0.326 [0.223, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.215, 10.100], loss: 0.015343, mae: 0.127227, mean_q: 18.361748
 25312/100000: episode: 449, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.118, mean reward: 0.559 [0.547, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.085, 10.100], loss: 0.005448, mae: 0.082281, mean_q: 18.095720
 25314/100000: episode: 450, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 0.972, mean reward: 0.486 [0.454, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.194, 10.100], loss: 0.007619, mae: 0.092993, mean_q: 20.227077
 25316/100000: episode: 451, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.937, mean reward: 0.468 [0.358, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.212, 10.100], loss: 0.009999, mae: 0.110292, mean_q: 17.352362
 25318/100000: episode: 452, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.692, mean reward: 0.346 [0.329, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.252, 10.100], loss: 0.009531, mae: 0.115223, mean_q: 17.766470
 25320/100000: episode: 453, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.936, mean reward: 0.468 [0.461, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.203, 10.100], loss: 0.008973, mae: 0.096162, mean_q: 18.553568
 25322/100000: episode: 454, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.218, mean reward: 0.609 [0.608, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.147, 10.100], loss: 0.019410, mae: 0.130811, mean_q: 21.210781
 25324/100000: episode: 455, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.865, mean reward: 0.432 [0.418, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.264, 10.100], loss: 0.007505, mae: 0.098216, mean_q: 17.079712
 25326/100000: episode: 456, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.892, mean reward: 0.446 [0.434, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.246, 10.100], loss: 0.007934, mae: 0.103360, mean_q: 19.469780
 25328/100000: episode: 457, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.040, mean reward: 0.520 [0.515, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.236, 10.100], loss: 0.013906, mae: 0.137673, mean_q: 18.585884
 25330/100000: episode: 458, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.945, mean reward: 0.473 [0.441, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.331, 10.100], loss: 0.007136, mae: 0.088909, mean_q: 19.896002
 25333/100000: episode: 459, duration: 0.027s, episode steps: 3, steps per second: 112, episode reward: 0.913, mean reward: 0.304 [0.289, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.307, 10.100], loss: 0.005839, mae: 0.086539, mean_q: 20.671255
 25335/100000: episode: 460, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 1.081, mean reward: 0.540 [0.507, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.240, 10.100], loss: 0.014408, mae: 0.102807, mean_q: 17.242868
 25337/100000: episode: 461, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 1.152, mean reward: 0.576 [0.566, 0.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.289, 10.100], loss: 0.006863, mae: 0.087527, mean_q: 19.289207
 25339/100000: episode: 462, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.863, mean reward: 0.432 [0.423, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.267, 10.100], loss: 0.007951, mae: 0.085524, mean_q: 17.629364
 25341/100000: episode: 463, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.755, mean reward: 0.378 [0.365, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.209, 10.100], loss: 0.004222, mae: 0.072678, mean_q: 18.918388
 25344/100000: episode: 464, duration: 0.025s, episode steps: 3, steps per second: 121, episode reward: 0.897, mean reward: 0.299 [0.289, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.333, 10.100], loss: 0.008638, mae: 0.098597, mean_q: 19.379709
 25346/100000: episode: 465, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.062, mean reward: 0.531 [0.516, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.190, 10.100], loss: 0.006970, mae: 0.095583, mean_q: 16.723705
 25348/100000: episode: 466, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.333, mean reward: 0.666 [0.652, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.242, 10.100], loss: 0.010825, mae: 0.097149, mean_q: 20.156567
 25350/100000: episode: 467, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.079, mean reward: 0.539 [0.529, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.388, 10.100], loss: 0.013802, mae: 0.128132, mean_q: 19.511002
 25352/100000: episode: 468, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.883, mean reward: 0.441 [0.421, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.226, 10.100], loss: 0.007453, mae: 0.091731, mean_q: 17.511639
 25354/100000: episode: 469, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.781, mean reward: 0.391 [0.382, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.364, 10.100], loss: 0.007088, mae: 0.089096, mean_q: 19.161196
 25356/100000: episode: 470, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.934, mean reward: 0.467 [0.466, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.213, 10.100], loss: 0.008546, mae: 0.101469, mean_q: 21.011028
 25358/100000: episode: 471, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.829, mean reward: 0.414 [0.359, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.229, 10.100], loss: 0.006536, mae: 0.082226, mean_q: 20.119781
 25360/100000: episode: 472, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.806, mean reward: 0.403 [0.387, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.128, 10.100], loss: 0.008429, mae: 0.102079, mean_q: 20.262156
 25362/100000: episode: 473, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.766, mean reward: 0.383 [0.340, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.736, 10.100], loss: 0.009231, mae: 0.098259, mean_q: 20.089005
 25364/100000: episode: 474, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.762, mean reward: 0.381 [0.353, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.254, 10.100], loss: 0.010697, mae: 0.106855, mean_q: 19.548252
 25366/100000: episode: 475, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.931, mean reward: 0.465 [0.460, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.204, 10.100], loss: 0.006622, mae: 0.086026, mean_q: 19.043465
 25368/100000: episode: 476, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 1.132, mean reward: 0.566 [0.549, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.152, 10.100], loss: 0.006820, mae: 0.091422, mean_q: 18.898342
 25370/100000: episode: 477, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.756, mean reward: 0.378 [0.312, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.273, 10.100], loss: 0.017952, mae: 0.140825, mean_q: 19.284672
 25372/100000: episode: 478, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.708, mean reward: 0.354 [0.250, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.145, 10.100], loss: 0.011733, mae: 0.113274, mean_q: 17.622318
 25374/100000: episode: 479, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.934, mean reward: 0.467 [0.459, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.277, 10.100], loss: 0.011737, mae: 0.113440, mean_q: 20.430378
 25376/100000: episode: 480, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.817, mean reward: 0.408 [0.285, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.172, 10.100], loss: 0.009993, mae: 0.107450, mean_q: 20.892159
 25378/100000: episode: 481, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.854, mean reward: 0.427 [0.420, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.197, 10.100], loss: 0.013303, mae: 0.127786, mean_q: 20.762192
 25380/100000: episode: 482, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.880, mean reward: 0.440 [0.389, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.222, 10.100], loss: 0.006830, mae: 0.093029, mean_q: 19.752899
 25382/100000: episode: 483, duration: 0.021s, episode steps: 2, steps per second: 94, episode reward: 0.849, mean reward: 0.425 [0.411, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.320, 10.100], loss: 0.006297, mae: 0.090283, mean_q: 19.814201
 25384/100000: episode: 484, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 1.021, mean reward: 0.510 [0.509, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.242, 10.100], loss: 0.008952, mae: 0.107551, mean_q: 19.260895
 25387/100000: episode: 485, duration: 0.020s, episode steps: 3, steps per second: 149, episode reward: 0.890, mean reward: 0.297 [0.274, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.412, 10.100], loss: 0.009053, mae: 0.101361, mean_q: 19.110781
 25389/100000: episode: 486, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 1.164, mean reward: 0.582 [0.538, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.206, 10.100], loss: 0.009952, mae: 0.106288, mean_q: 17.969860
 25391/100000: episode: 487, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.991, mean reward: 0.495 [0.485, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.647, 10.100], loss: 0.009484, mae: 0.105312, mean_q: 20.128643
 25394/100000: episode: 488, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 0.837, mean reward: 0.279 [0.271, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.381, 10.100], loss: 0.013507, mae: 0.122916, mean_q: 20.491072
 25396/100000: episode: 489, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.475, mean reward: 0.238 [0.172, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.221, 10.100], loss: 0.004513, mae: 0.071679, mean_q: 18.199692
 25398/100000: episode: 490, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.849, mean reward: 0.425 [0.365, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.267, 10.100], loss: 0.007796, mae: 0.084457, mean_q: 19.036419
 25400/100000: episode: 491, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.815, mean reward: 0.407 [0.407, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.234, 10.100], loss: 0.015830, mae: 0.136644, mean_q: 16.374474
 25402/100000: episode: 492, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.802, mean reward: 0.401 [0.392, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.229, 10.100], loss: 0.015766, mae: 0.137906, mean_q: 18.226246
 25405/100000: episode: 493, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 0.817, mean reward: 0.272 [0.237, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.360, 10.100], loss: 0.017394, mae: 0.146075, mean_q: 19.630968
 25407/100000: episode: 494, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.967, mean reward: 0.484 [0.466, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.243, 10.100], loss: 0.013999, mae: 0.129252, mean_q: 20.386280
 25409/100000: episode: 495, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.896, mean reward: 0.448 [0.378, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.255, 10.100], loss: 0.011018, mae: 0.118158, mean_q: 17.285212
 25411/100000: episode: 496, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.919, mean reward: 0.460 [0.459, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.188, 10.100], loss: 0.010042, mae: 0.106324, mean_q: 17.214550
 25413/100000: episode: 497, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 1.163, mean reward: 0.582 [0.540, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.111, 10.100], loss: 0.012786, mae: 0.111332, mean_q: 18.478653
 25416/100000: episode: 498, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 0.835, mean reward: 0.278 [0.261, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.389, 10.100], loss: 0.010456, mae: 0.111968, mean_q: 17.046354
 25418/100000: episode: 499, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.756, mean reward: 0.378 [0.302, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.245, 10.100], loss: 0.012961, mae: 0.127845, mean_q: 22.058180
 25420/100000: episode: 500, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.838, mean reward: 0.419 [0.398, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.206, 10.100], loss: 0.009626, mae: 0.105719, mean_q: 16.669670
 25422/100000: episode: 501, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 1.056, mean reward: 0.528 [0.492, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.210, 10.100], loss: 0.007187, mae: 0.091318, mean_q: 19.509823
 25424/100000: episode: 502, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.070, mean reward: 0.535 [0.513, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.190, 10.100], loss: 0.006497, mae: 0.085979, mean_q: 18.444700
 25426/100000: episode: 503, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.901, mean reward: 0.451 [0.363, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.180, 10.100], loss: 0.005715, mae: 0.081913, mean_q: 18.303335
 25428/100000: episode: 504, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.638, mean reward: 0.319 [0.147, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.307, 10.100], loss: 0.010666, mae: 0.111665, mean_q: 18.283190
 25430/100000: episode: 505, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 1.077, mean reward: 0.538 [0.535, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.290, 10.100], loss: 0.023218, mae: 0.158580, mean_q: 16.176315
 25432/100000: episode: 506, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.700, mean reward: 0.350 [0.333, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.470, 10.100], loss: 0.008988, mae: 0.107893, mean_q: 18.634832
 25434/100000: episode: 507, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 0.959, mean reward: 0.479 [0.462, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.246, 10.100], loss: 0.012483, mae: 0.128439, mean_q: 19.175106
 25436/100000: episode: 508, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 1.130, mean reward: 0.565 [0.555, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.128, 10.100], loss: 0.011596, mae: 0.122854, mean_q: 18.884121
 25438/100000: episode: 509, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.724, mean reward: 0.362 [0.308, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.263, 10.100], loss: 0.013435, mae: 0.114142, mean_q: 19.234528
 25440/100000: episode: 510, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.748, mean reward: 0.374 [0.365, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.284, 10.100], loss: 0.012932, mae: 0.118280, mean_q: 19.551340
 25442/100000: episode: 511, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.184, mean reward: 0.592 [0.585, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.126, 10.100], loss: 0.010272, mae: 0.115746, mean_q: 18.905106
 25444/100000: episode: 512, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.041, mean reward: 0.520 [0.488, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.225, 10.100], loss: 0.008295, mae: 0.104883, mean_q: 20.332939
 25446/100000: episode: 513, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.776, mean reward: 0.388 [0.369, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.283, 10.100], loss: 0.006709, mae: 0.095651, mean_q: 19.089710
 25448/100000: episode: 514, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.015, mean reward: 0.508 [0.447, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.231, 10.100], loss: 0.007375, mae: 0.101280, mean_q: 20.137363
 25451/100000: episode: 515, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 0.810, mean reward: 0.270 [0.258, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.376, 10.100], loss: 0.012724, mae: 0.121948, mean_q: 20.788404
 25453/100000: episode: 516, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 1.039, mean reward: 0.520 [0.515, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.394, 10.100], loss: 0.015702, mae: 0.141451, mean_q: 17.581535
 25456/100000: episode: 517, duration: 0.025s, episode steps: 3, steps per second: 122, episode reward: 0.573, mean reward: 0.191 [0.184, 0.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.367, 10.100], loss: 0.011407, mae: 0.117430, mean_q: 18.361082
 25458/100000: episode: 518, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.866, mean reward: 0.433 [0.362, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.261, 10.100], loss: 0.009092, mae: 0.099828, mean_q: 19.479710
 25460/100000: episode: 519, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.751, mean reward: 0.375 [0.325, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.163, 10.100], loss: 0.016555, mae: 0.139092, mean_q: 18.441141
[Info] New level: -0.7685321569442749 | Considering 10/90 traces
 25462/100000: episode: 520, duration: 3.988s, episode steps: 2, steps per second: 1, episode reward: 0.779, mean reward: 0.389 [0.248, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.413, 10.100], loss: 0.009823, mae: 0.099508, mean_q: 18.347691
 25463/100000: episode: 521, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.239, mean reward: 0.239 [0.239, 0.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.347, 10.100], loss: 0.006305, mae: 0.094555, mean_q: 19.344601
 25464/100000: episode: 522, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.178, mean reward: 0.178 [0.178, 0.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.361, 10.100], loss: 0.007891, mae: 0.094069, mean_q: 17.593439
 25465/100000: episode: 523, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.244, mean reward: 0.244 [0.244, 0.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.414, 10.100], loss: 0.007663, mae: 0.089773, mean_q: 18.815248
 25466/100000: episode: 524, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.249, mean reward: 0.249 [0.249, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.438, 10.100], loss: 0.025197, mae: 0.168905, mean_q: 16.081478
 25467/100000: episode: 525, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.297, mean reward: 0.297 [0.297, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.436, 10.100], loss: 0.010629, mae: 0.119490, mean_q: 15.689286
 25468/100000: episode: 526, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.274, mean reward: 0.274 [0.274, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.344, 10.100], loss: 0.016432, mae: 0.147393, mean_q: 18.264668
 25469/100000: episode: 527, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.253, mean reward: 0.253 [0.253, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.386, 10.100], loss: 0.004445, mae: 0.080013, mean_q: 19.309093
 25470/100000: episode: 528, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.263, mean reward: 0.263 [0.263, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.382, 10.100], loss: 0.007284, mae: 0.106162, mean_q: 21.528214
 25471/100000: episode: 529, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.220, mean reward: 0.220 [0.220, 0.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.357, 10.100], loss: 0.009676, mae: 0.105867, mean_q: 15.328335
 25472/100000: episode: 530, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.261, mean reward: 0.261 [0.261, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.482, 10.100], loss: 0.010240, mae: 0.115965, mean_q: 21.728226
 25473/100000: episode: 531, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.284, mean reward: 0.284 [0.284, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.457, 10.100], loss: 0.004309, mae: 0.076869, mean_q: 19.163931
 25474/100000: episode: 532, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.265, mean reward: 0.265 [0.265, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.449, 10.100], loss: 0.008581, mae: 0.104151, mean_q: 18.834717
 25475/100000: episode: 533, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.293, mean reward: 0.293 [0.293, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.361, 10.100], loss: 0.012656, mae: 0.120908, mean_q: 18.916977
 25476/100000: episode: 534, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.237, mean reward: 0.237 [0.237, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.353, 10.100], loss: 0.019877, mae: 0.151102, mean_q: 16.182825
 25477/100000: episode: 535, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.299, mean reward: 0.299 [0.299, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.420, 10.100], loss: 0.009211, mae: 0.110181, mean_q: 18.341988
 25478/100000: episode: 536, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.444, 10.100], loss: 0.011346, mae: 0.109848, mean_q: 19.867493
 25479/100000: episode: 537, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.280, mean reward: 0.280 [0.280, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.376, 10.100], loss: 0.006471, mae: 0.089828, mean_q: 20.544149
 25480/100000: episode: 538, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.301, mean reward: 0.301 [0.301, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.304, 10.100], loss: 0.009726, mae: 0.106978, mean_q: 17.834784
 25481/100000: episode: 539, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.258, mean reward: 0.258 [0.258, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.380, 10.100], loss: 0.007254, mae: 0.103350, mean_q: 17.086578
 25482/100000: episode: 540, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.302, mean reward: 0.302 [0.302, 0.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.376, 10.100], loss: 0.009366, mae: 0.096436, mean_q: 19.196449
 25483/100000: episode: 541, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.273, mean reward: 0.273 [0.273, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.401, 10.100], loss: 0.014697, mae: 0.134194, mean_q: 18.809151
 25484/100000: episode: 542, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.261, mean reward: 0.261 [0.261, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.420, 10.100], loss: 0.008106, mae: 0.104553, mean_q: 17.815147
 25485/100000: episode: 543, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.294, mean reward: 0.294 [0.294, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.342, 10.100], loss: 0.015459, mae: 0.139087, mean_q: 23.023399
 25486/100000: episode: 544, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.265, mean reward: 0.265 [0.265, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.405, 10.100], loss: 0.009937, mae: 0.117494, mean_q: 20.958675
 25487/100000: episode: 545, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.172, mean reward: 0.172 [0.172, 0.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.441, 10.100], loss: 0.015365, mae: 0.132779, mean_q: 18.224075
 25488/100000: episode: 546, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.110, mean reward: 0.110 [0.110, 0.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.377, 10.100], loss: 0.014446, mae: 0.087631, mean_q: 18.754757
 25489/100000: episode: 547, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.311, mean reward: 0.311 [0.311, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.353, 10.100], loss: 0.003812, mae: 0.072470, mean_q: 19.827385
 25490/100000: episode: 548, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.175, mean reward: 0.175 [0.175, 0.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.450, 10.100], loss: 0.006756, mae: 0.098527, mean_q: 19.366653
 25491/100000: episode: 549, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.198, mean reward: 0.198 [0.198, 0.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.387, 10.100], loss: 0.009997, mae: 0.114928, mean_q: 16.495922
 25492/100000: episode: 550, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.301, mean reward: 0.301 [0.301, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.419, 10.100], loss: 0.012426, mae: 0.097874, mean_q: 20.194950
 25493/100000: episode: 551, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.223, mean reward: 0.223 [0.223, 0.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.368, 10.100], loss: 0.007238, mae: 0.087159, mean_q: 19.419668
 25494/100000: episode: 552, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.256, mean reward: 0.256 [0.256, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.399, 10.100], loss: 0.009256, mae: 0.116150, mean_q: 19.073725
 25495/100000: episode: 553, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.270, mean reward: 0.270 [0.270, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.414, 10.100], loss: 0.007214, mae: 0.089935, mean_q: 19.251818
 25496/100000: episode: 554, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.438, 10.100], loss: 0.011420, mae: 0.108053, mean_q: 18.566509
 25497/100000: episode: 555, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.202, mean reward: 0.202 [0.202, 0.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.402, 10.100], loss: 0.021052, mae: 0.151745, mean_q: 15.814494
 25498/100000: episode: 556, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.279, mean reward: 0.279 [0.279, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.426, 10.100], loss: 0.012199, mae: 0.103948, mean_q: 18.202888
 25499/100000: episode: 557, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.303, mean reward: 0.303 [0.303, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.373, 10.100], loss: 0.005278, mae: 0.076821, mean_q: 20.656710
 25500/100000: episode: 558, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.258, mean reward: 0.258 [0.258, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.381, 10.100], loss: 0.016853, mae: 0.125628, mean_q: 20.967793
 25501/100000: episode: 559, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.276, mean reward: 0.276 [0.276, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.400, 10.100], loss: 0.019789, mae: 0.149068, mean_q: 15.646974
 25502/100000: episode: 560, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.250, mean reward: 0.250 [0.250, 0.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.380, 10.100], loss: 0.024837, mae: 0.178705, mean_q: 18.746017
 25503/100000: episode: 561, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.284, mean reward: 0.284 [0.284, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.452, 10.100], loss: 0.010114, mae: 0.112332, mean_q: 19.817417
 25504/100000: episode: 562, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.249, mean reward: 0.249 [0.249, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.393, 10.100], loss: 0.010979, mae: 0.112642, mean_q: 19.266750
 25505/100000: episode: 563, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.204, mean reward: 0.204 [0.204, 0.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.370, 10.100], loss: 0.014616, mae: 0.117703, mean_q: 15.425713
 25506/100000: episode: 564, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.320, mean reward: 0.320 [0.320, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.392, 10.100], loss: 0.005880, mae: 0.087536, mean_q: 19.182402
 25507/100000: episode: 565, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.255, mean reward: 0.255 [0.255, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.326, 10.100], loss: 0.017786, mae: 0.139908, mean_q: 19.833303
 25508/100000: episode: 566, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.261, mean reward: 0.261 [0.261, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.401, 10.100], loss: 0.018374, mae: 0.167733, mean_q: 20.469769
 25509/100000: episode: 567, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.319, mean reward: 0.319 [0.319, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.373, 10.100], loss: 0.019912, mae: 0.139012, mean_q: 15.883654
 25510/100000: episode: 568, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.281, mean reward: 0.281 [0.281, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.434, 10.100], loss: 0.019664, mae: 0.162844, mean_q: 16.214293
 25511/100000: episode: 569, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.297, mean reward: 0.297 [0.297, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.385, 10.100], loss: 0.017232, mae: 0.129630, mean_q: 17.151798
 25512/100000: episode: 570, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.125, mean reward: 0.125 [0.125, 0.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.327, 10.100], loss: 0.014279, mae: 0.120109, mean_q: 19.896088
 25513/100000: episode: 571, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.263, mean reward: 0.263 [0.263, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.438, 10.100], loss: 0.015666, mae: 0.146563, mean_q: 18.180367
 25514/100000: episode: 572, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.295, mean reward: 0.295 [0.295, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.395, 10.100], loss: 0.032468, mae: 0.229304, mean_q: 19.858330
 25515/100000: episode: 573, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.254, mean reward: 0.254 [0.254, 0.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.419, 10.100], loss: 0.033866, mae: 0.218340, mean_q: 18.226797
 25516/100000: episode: 574, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.222, mean reward: 0.222 [0.222, 0.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.403, 10.100], loss: 0.012253, mae: 0.120575, mean_q: 17.843925
 25517/100000: episode: 575, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.284, mean reward: 0.284 [0.284, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.420, 10.100], loss: 0.015935, mae: 0.143549, mean_q: 17.939295
 25518/100000: episode: 576, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.287, mean reward: 0.287 [0.287, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.370, 10.100], loss: 0.012756, mae: 0.131979, mean_q: 17.485886
 25519/100000: episode: 577, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.244, mean reward: 0.244 [0.244, 0.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.401, 10.100], loss: 0.013129, mae: 0.132422, mean_q: 18.913418
 25520/100000: episode: 578, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.236, mean reward: 0.236 [0.236, 0.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.381, 10.100], loss: 0.009007, mae: 0.116531, mean_q: 15.945744
 25521/100000: episode: 579, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.179, mean reward: 0.179 [0.179, 0.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.385, 10.100], loss: 0.006106, mae: 0.087721, mean_q: 18.264877
 25522/100000: episode: 580, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.213, mean reward: 0.213 [0.213, 0.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.362, 10.100], loss: 0.015762, mae: 0.121169, mean_q: 19.419453
 25523/100000: episode: 581, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.251, mean reward: 0.251 [0.251, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.406, 10.100], loss: 0.015039, mae: 0.138822, mean_q: 17.064707
 25524/100000: episode: 582, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.257, mean reward: 0.257 [0.257, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.353, 10.100], loss: 0.009452, mae: 0.095020, mean_q: 19.977123
 25525/100000: episode: 583, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.256, mean reward: 0.256 [0.256, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.458, 10.100], loss: 0.008582, mae: 0.106601, mean_q: 18.510590
 25526/100000: episode: 584, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.244, mean reward: 0.244 [0.244, 0.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.411, 10.100], loss: 0.006269, mae: 0.090645, mean_q: 18.103003
 25527/100000: episode: 585, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.418, 10.100], loss: 0.005978, mae: 0.091381, mean_q: 18.139585
 25528/100000: episode: 586, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.258, mean reward: 0.258 [0.258, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.455, 10.100], loss: 0.010755, mae: 0.113237, mean_q: 18.200092
 25529/100000: episode: 587, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.245, mean reward: 0.245 [0.245, 0.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.385, 10.100], loss: 0.009942, mae: 0.115146, mean_q: 17.248005
 25530/100000: episode: 588, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.259, mean reward: 0.259 [0.259, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.449, 10.100], loss: 0.008223, mae: 0.101852, mean_q: 21.274754
 25531/100000: episode: 589, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.290, mean reward: 0.290 [0.290, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.409, 10.100], loss: 0.011304, mae: 0.125092, mean_q: 19.170370
 25532/100000: episode: 590, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.186, mean reward: 0.186 [0.186, 0.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.359, 10.100], loss: 0.012620, mae: 0.110159, mean_q: 21.717300
 25533/100000: episode: 591, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.259, mean reward: 0.259 [0.259, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.349, 10.100], loss: 0.013486, mae: 0.126596, mean_q: 17.739052
 25534/100000: episode: 592, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.263, mean reward: 0.263 [0.263, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.457, 10.100], loss: 0.011077, mae: 0.105254, mean_q: 17.278332
 25535/100000: episode: 593, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.232, mean reward: 0.232 [0.232, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.371, 10.100], loss: 0.020206, mae: 0.140012, mean_q: 17.553253
 25536/100000: episode: 594, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.259, mean reward: 0.259 [0.259, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.359, 10.100], loss: 0.011392, mae: 0.109793, mean_q: 17.073973
 25537/100000: episode: 595, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.238, mean reward: 0.238 [0.238, 0.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.360, 10.100], loss: 0.037717, mae: 0.171488, mean_q: 19.789438
 25538/100000: episode: 596, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.321, mean reward: 0.321 [0.321, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.340, 10.100], loss: 0.019900, mae: 0.167723, mean_q: 18.385607
 25539/100000: episode: 597, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.188, mean reward: 0.188 [0.188, 0.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.422, 10.100], loss: 0.041951, mae: 0.234576, mean_q: 14.211432
 25540/100000: episode: 598, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.268, mean reward: 0.268 [0.268, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.425, 10.100], loss: 0.018403, mae: 0.168929, mean_q: 20.156958
 25541/100000: episode: 599, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.240, mean reward: 0.240 [0.240, 0.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.380, 10.100], loss: 0.012619, mae: 0.128970, mean_q: 19.136356
 25542/100000: episode: 600, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.269, mean reward: 0.269 [0.269, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.347, 10.100], loss: 0.009624, mae: 0.103981, mean_q: 18.422441
 25543/100000: episode: 601, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.224, mean reward: 0.224 [0.224, 0.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.340, 10.100], loss: 0.024079, mae: 0.183060, mean_q: 16.945507
 25544/100000: episode: 602, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.277, mean reward: 0.277 [0.277, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.426, 10.100], loss: 0.010502, mae: 0.114806, mean_q: 18.645144
 25545/100000: episode: 603, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.256, mean reward: 0.256 [0.256, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.366, 10.100], loss: 0.013338, mae: 0.123504, mean_q: 22.087685
 25546/100000: episode: 604, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.261, mean reward: 0.261 [0.261, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.410, 10.100], loss: 0.017225, mae: 0.148268, mean_q: 16.770096
 25547/100000: episode: 605, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.321, mean reward: 0.321 [0.321, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.422, 10.100], loss: 0.013286, mae: 0.121157, mean_q: 22.308083
 25548/100000: episode: 606, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.269, mean reward: 0.269 [0.269, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.401, 10.100], loss: 0.017057, mae: 0.144956, mean_q: 17.575859
 25549/100000: episode: 607, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.261, mean reward: 0.261 [0.261, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.443, 10.100], loss: 0.015961, mae: 0.128931, mean_q: 16.161179
 25550/100000: episode: 608, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.254, mean reward: 0.254 [0.254, 0.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.358, 10.100], loss: 0.010575, mae: 0.109011, mean_q: 18.539961
 25551/100000: episode: 609, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.255, mean reward: 0.255 [0.255, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.422, 10.100], loss: 0.020951, mae: 0.172323, mean_q: 21.728214
[Info] Not found new level, current best level reached = -0.7685321569442749
 25552/100000: episode: 610, duration: 4.007s, episode steps: 1, steps per second: 0, episode reward: 0.266, mean reward: 0.266 [0.266, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.391, 10.100], loss: 0.010746, mae: 0.124180, mean_q: 20.460585
 25652/100000: episode: 611, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 50.131, mean reward: 0.501 [0.266, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.933, 10.098], loss: 0.012191, mae: 0.114934, mean_q: 18.190392
 25752/100000: episode: 612, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 51.318, mean reward: 0.513 [0.306, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.357, 10.098], loss: 0.012217, mae: 0.114524, mean_q: 18.666931
 25852/100000: episode: 613, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 52.861, mean reward: 0.529 [0.157, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.842, 10.329], loss: 0.011343, mae: 0.110903, mean_q: 18.582899
 25952/100000: episode: 614, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 56.147, mean reward: 0.561 [0.237, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.100, 10.098], loss: 0.011991, mae: 0.112944, mean_q: 18.212534
 26052/100000: episode: 615, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 53.786, mean reward: 0.538 [0.300, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.845, 10.098], loss: 0.011927, mae: 0.115868, mean_q: 18.739103
 26152/100000: episode: 616, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 46.731, mean reward: 0.467 [0.070, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.936, 10.098], loss: 0.010921, mae: 0.108097, mean_q: 18.339243
 26252/100000: episode: 617, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 54.663, mean reward: 0.547 [0.409, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.105, 10.098], loss: 0.012050, mae: 0.115804, mean_q: 18.498631
 26352/100000: episode: 618, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 49.883, mean reward: 0.499 [0.279, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.856, 10.263], loss: 0.009601, mae: 0.101166, mean_q: 18.234385
 26452/100000: episode: 619, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 52.309, mean reward: 0.523 [0.198, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.259, 10.380], loss: 0.012955, mae: 0.118454, mean_q: 18.615332
 26552/100000: episode: 620, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 52.482, mean reward: 0.525 [0.282, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.289, 10.098], loss: 0.012267, mae: 0.115445, mean_q: 18.657232
 26652/100000: episode: 621, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 49.429, mean reward: 0.494 [0.115, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.336, 10.098], loss: 0.010539, mae: 0.105933, mean_q: 18.656221
 26752/100000: episode: 622, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 53.250, mean reward: 0.532 [0.250, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.697, 10.275], loss: 0.013132, mae: 0.121226, mean_q: 18.714058
 26852/100000: episode: 623, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 54.760, mean reward: 0.548 [0.344, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.705, 10.098], loss: 0.010213, mae: 0.102708, mean_q: 18.381750
 26952/100000: episode: 624, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 53.903, mean reward: 0.539 [0.280, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.271, 10.098], loss: 0.010024, mae: 0.103795, mean_q: 18.385521
 27052/100000: episode: 625, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 51.082, mean reward: 0.511 [0.162, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.301, 10.113], loss: 0.010700, mae: 0.105439, mean_q: 18.570530
 27152/100000: episode: 626, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 52.966, mean reward: 0.530 [0.359, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.640, 10.098], loss: 0.011333, mae: 0.112260, mean_q: 18.307039
 27252/100000: episode: 627, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 50.821, mean reward: 0.508 [0.282, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.649, 10.277], loss: 0.010844, mae: 0.107519, mean_q: 18.734585
 27352/100000: episode: 628, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 49.611, mean reward: 0.496 [0.170, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.103, 10.098], loss: 0.009590, mae: 0.103769, mean_q: 18.923271
 27452/100000: episode: 629, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 53.623, mean reward: 0.536 [0.202, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.442, 10.208], loss: 0.011922, mae: 0.114309, mean_q: 18.269220
 27552/100000: episode: 630, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 53.194, mean reward: 0.532 [0.310, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.054, 10.298], loss: 0.010311, mae: 0.104677, mean_q: 18.454351
 27652/100000: episode: 631, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 57.166, mean reward: 0.572 [0.307, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.370, 10.226], loss: 0.010545, mae: 0.107667, mean_q: 18.486195
 27752/100000: episode: 632, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.640, mean reward: 0.546 [0.255, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.630, 10.243], loss: 0.010857, mae: 0.109347, mean_q: 18.647062
 27852/100000: episode: 633, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 54.668, mean reward: 0.547 [0.368, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.629, 10.253], loss: 0.012195, mae: 0.114661, mean_q: 18.350555
 27952/100000: episode: 634, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 48.813, mean reward: 0.488 [0.139, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.485, 10.229], loss: 0.010019, mae: 0.104880, mean_q: 18.678913
 28052/100000: episode: 635, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.655, mean reward: 0.547 [0.207, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.526, 10.098], loss: 0.010477, mae: 0.105613, mean_q: 18.696924
 28152/100000: episode: 636, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 55.681, mean reward: 0.557 [0.303, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.447, 10.098], loss: 0.014644, mae: 0.127550, mean_q: 18.366886
 28252/100000: episode: 637, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 54.447, mean reward: 0.544 [0.221, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.495, 10.127], loss: 0.011407, mae: 0.111419, mean_q: 18.505089
 28352/100000: episode: 638, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 50.045, mean reward: 0.500 [0.238, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.415, 10.235], loss: 0.010460, mae: 0.107903, mean_q: 18.555084
 28452/100000: episode: 639, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 52.591, mean reward: 0.526 [0.258, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.896, 10.148], loss: 0.011246, mae: 0.110327, mean_q: 18.657864
 28552/100000: episode: 640, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.977, mean reward: 0.540 [0.293, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.193, 10.190], loss: 0.012292, mae: 0.117063, mean_q: 18.030823
 28652/100000: episode: 641, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 54.871, mean reward: 0.549 [0.307, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.522, 10.102], loss: 0.011299, mae: 0.110498, mean_q: 18.390375
 28752/100000: episode: 642, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 51.839, mean reward: 0.518 [0.334, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.858, 10.098], loss: 0.010612, mae: 0.108115, mean_q: 18.791981
 28852/100000: episode: 643, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 54.376, mean reward: 0.544 [0.246, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.796, 10.098], loss: 0.010546, mae: 0.105315, mean_q: 18.447447
 28952/100000: episode: 644, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 54.604, mean reward: 0.546 [0.351, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.086, 10.316], loss: 0.010411, mae: 0.105552, mean_q: 18.446308
 29052/100000: episode: 645, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.495, mean reward: 0.575 [0.213, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.467, 10.098], loss: 0.013274, mae: 0.123184, mean_q: 18.344397
 29152/100000: episode: 646, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 55.176, mean reward: 0.552 [0.343, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.921, 10.098], loss: 0.010129, mae: 0.106274, mean_q: 18.370155
 29252/100000: episode: 647, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 51.332, mean reward: 0.513 [0.263, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.871, 10.193], loss: 0.011256, mae: 0.110589, mean_q: 18.053583
 29352/100000: episode: 648, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 52.294, mean reward: 0.523 [0.196, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.735, 10.382], loss: 0.014228, mae: 0.124225, mean_q: 18.207237
 29452/100000: episode: 649, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 50.590, mean reward: 0.506 [0.208, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.068, 10.098], loss: 0.012433, mae: 0.117671, mean_q: 18.489824
 29552/100000: episode: 650, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 52.931, mean reward: 0.529 [0.272, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.870, 10.098], loss: 0.010680, mae: 0.109166, mean_q: 18.522850
 29652/100000: episode: 651, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 52.853, mean reward: 0.529 [0.266, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.350, 10.146], loss: 0.010677, mae: 0.105534, mean_q: 18.284681
 29752/100000: episode: 652, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 49.825, mean reward: 0.498 [0.194, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.265, 10.117], loss: 0.011138, mae: 0.111339, mean_q: 18.430174
 29852/100000: episode: 653, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 56.068, mean reward: 0.561 [0.292, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.025, 10.098], loss: 0.011370, mae: 0.110099, mean_q: 18.255013
 29952/100000: episode: 654, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 48.431, mean reward: 0.484 [0.188, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.758, 10.423], loss: 0.011768, mae: 0.113174, mean_q: 18.539326
 30052/100000: episode: 655, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 49.440, mean reward: 0.494 [0.311, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.631, 10.098], loss: 0.010333, mae: 0.106773, mean_q: 18.096033
 30152/100000: episode: 656, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 54.183, mean reward: 0.542 [0.349, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.849, 10.098], loss: 0.011993, mae: 0.114540, mean_q: 18.240076
 30252/100000: episode: 657, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.924, mean reward: 0.569 [0.376, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.702, 10.098], loss: 0.009850, mae: 0.103685, mean_q: 19.193007
 30352/100000: episode: 658, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 52.558, mean reward: 0.526 [0.290, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.671, 10.098], loss: 0.011312, mae: 0.113762, mean_q: 19.183952
 30452/100000: episode: 659, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 53.783, mean reward: 0.538 [0.275, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.026, 10.238], loss: 0.009172, mae: 0.100231, mean_q: 19.552065
 30552/100000: episode: 660, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 53.646, mean reward: 0.536 [0.288, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.759, 10.098], loss: 0.010389, mae: 0.106943, mean_q: 19.474451
 30652/100000: episode: 661, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 54.865, mean reward: 0.549 [0.372, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.386, 10.098], loss: 0.011024, mae: 0.110643, mean_q: 19.559814
 30752/100000: episode: 662, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 54.604, mean reward: 0.546 [0.370, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.948, 10.247], loss: 0.009163, mae: 0.098782, mean_q: 19.727528
 30852/100000: episode: 663, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.523, mean reward: 0.565 [0.348, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.864, 10.099], loss: 0.010406, mae: 0.109077, mean_q: 19.329838
 30952/100000: episode: 664, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 51.654, mean reward: 0.517 [0.291, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.612, 10.098], loss: 0.010138, mae: 0.107757, mean_q: 19.572403
 31052/100000: episode: 665, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 49.063, mean reward: 0.491 [0.245, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.631, 10.420], loss: 0.010377, mae: 0.109547, mean_q: 19.719236
 31152/100000: episode: 666, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 53.964, mean reward: 0.540 [0.338, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.397, 10.281], loss: 0.010390, mae: 0.107532, mean_q: 19.369642
 31252/100000: episode: 667, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 52.645, mean reward: 0.526 [0.279, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.397, 10.300], loss: 0.009514, mae: 0.104183, mean_q: 19.508900
 31352/100000: episode: 668, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 52.177, mean reward: 0.522 [0.342, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.599, 10.206], loss: 0.008765, mae: 0.101238, mean_q: 19.615950
 31452/100000: episode: 669, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 47.685, mean reward: 0.477 [0.196, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.377, 10.098], loss: 0.008048, mae: 0.096667, mean_q: 19.133768
 31552/100000: episode: 670, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 47.303, mean reward: 0.473 [0.133, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.019, 10.427], loss: 0.008806, mae: 0.102038, mean_q: 19.446430
 31652/100000: episode: 671, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 55.864, mean reward: 0.559 [0.398, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.393, 10.214], loss: 0.010287, mae: 0.109293, mean_q: 19.449587
 31752/100000: episode: 672, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 47.711, mean reward: 0.477 [0.177, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.810, 10.098], loss: 0.009363, mae: 0.103672, mean_q: 19.524946
 31852/100000: episode: 673, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 51.414, mean reward: 0.514 [0.231, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.498, 10.098], loss: 0.008888, mae: 0.102779, mean_q: 19.480495
 31952/100000: episode: 674, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 50.920, mean reward: 0.509 [0.266, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.054, 10.098], loss: 0.007849, mae: 0.097038, mean_q: 19.367355
 32052/100000: episode: 675, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 53.975, mean reward: 0.540 [0.321, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.433, 10.098], loss: 0.009673, mae: 0.107213, mean_q: 19.937248
 32152/100000: episode: 676, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 55.405, mean reward: 0.554 [0.242, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.826, 10.279], loss: 0.009339, mae: 0.105045, mean_q: 19.382429
 32252/100000: episode: 677, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 52.901, mean reward: 0.529 [0.296, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.647, 10.098], loss: 0.009367, mae: 0.105819, mean_q: 19.756397
 32352/100000: episode: 678, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 54.062, mean reward: 0.541 [0.180, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.245, 10.098], loss: 0.009896, mae: 0.110210, mean_q: 19.637539
 32452/100000: episode: 679, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 56.317, mean reward: 0.563 [0.286, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.550, 10.098], loss: 0.008640, mae: 0.100481, mean_q: 19.487850
 32552/100000: episode: 680, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 53.664, mean reward: 0.537 [0.327, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.081, 10.174], loss: 0.009181, mae: 0.103680, mean_q: 19.821844
 32652/100000: episode: 681, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 53.224, mean reward: 0.532 [0.345, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.505, 10.098], loss: 0.009672, mae: 0.104016, mean_q: 19.371479
 32752/100000: episode: 682, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 52.238, mean reward: 0.522 [0.300, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.239, 10.247], loss: 0.010158, mae: 0.110237, mean_q: 19.591385
 32852/100000: episode: 683, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 52.883, mean reward: 0.529 [0.265, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.297, 10.098], loss: 0.009386, mae: 0.106532, mean_q: 19.212313
 32952/100000: episode: 684, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 51.565, mean reward: 0.516 [0.222, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.479, 10.098], loss: 0.009343, mae: 0.105790, mean_q: 19.973766
 33052/100000: episode: 685, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 52.890, mean reward: 0.529 [0.257, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.423, 10.125], loss: 0.008585, mae: 0.102120, mean_q: 19.537447
 33152/100000: episode: 686, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 56.308, mean reward: 0.563 [0.250, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.814, 10.098], loss: 0.007922, mae: 0.096524, mean_q: 19.600964
 33252/100000: episode: 687, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 55.825, mean reward: 0.558 [0.296, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.605, 10.098], loss: 0.009100, mae: 0.103300, mean_q: 19.525990
 33352/100000: episode: 688, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 54.851, mean reward: 0.549 [0.314, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.633, 10.117], loss: 0.007476, mae: 0.093842, mean_q: 19.475279
 33452/100000: episode: 689, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 53.863, mean reward: 0.539 [0.280, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.559, 10.098], loss: 0.008412, mae: 0.099516, mean_q: 19.462702
 33552/100000: episode: 690, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 55.921, mean reward: 0.559 [0.339, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.433, 10.174], loss: 0.009470, mae: 0.106196, mean_q: 19.583725
 33652/100000: episode: 691, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 54.173, mean reward: 0.542 [0.189, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.751, 10.191], loss: 0.008125, mae: 0.099824, mean_q: 19.781277
 33752/100000: episode: 692, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 52.542, mean reward: 0.525 [0.337, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.631, 10.405], loss: 0.007518, mae: 0.094189, mean_q: 19.494820
 33852/100000: episode: 693, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 53.170, mean reward: 0.532 [0.263, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.099, 10.098], loss: 0.009025, mae: 0.103087, mean_q: 19.752573
 33952/100000: episode: 694, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 52.986, mean reward: 0.530 [0.335, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.733, 10.188], loss: 0.011768, mae: 0.121070, mean_q: 19.543655
 34052/100000: episode: 695, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 52.872, mean reward: 0.529 [0.316, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.754, 10.361], loss: 0.007914, mae: 0.097445, mean_q: 19.556738
 34152/100000: episode: 696, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 54.915, mean reward: 0.549 [0.187, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.046, 10.144], loss: 0.009997, mae: 0.109499, mean_q: 19.668974
 34252/100000: episode: 697, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 49.896, mean reward: 0.499 [0.198, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.582, 10.194], loss: 0.009766, mae: 0.108750, mean_q: 19.659958
 34352/100000: episode: 698, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.337, mean reward: 0.573 [0.444, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.051, 10.098], loss: 0.010611, mae: 0.112492, mean_q: 19.754274
 34452/100000: episode: 699, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 52.863, mean reward: 0.529 [0.338, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.468, 10.376], loss: 0.007797, mae: 0.096886, mean_q: 19.548626
 34552/100000: episode: 700, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 52.956, mean reward: 0.530 [0.317, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.510, 10.197], loss: 0.008526, mae: 0.101061, mean_q: 19.526901
 34652/100000: episode: 701, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 51.579, mean reward: 0.516 [0.276, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.322, 10.375], loss: 0.008829, mae: 0.102114, mean_q: 19.867994
 34752/100000: episode: 702, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 53.745, mean reward: 0.537 [0.251, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.932, 10.228], loss: 0.009659, mae: 0.107748, mean_q: 19.403685
 34852/100000: episode: 703, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 52.692, mean reward: 0.527 [0.278, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.524, 10.277], loss: 0.008671, mae: 0.098823, mean_q: 19.178984
 34952/100000: episode: 704, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 54.639, mean reward: 0.546 [0.306, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.762, 10.302], loss: 0.008097, mae: 0.097435, mean_q: 19.704657
 35052/100000: episode: 705, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 49.781, mean reward: 0.498 [0.326, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.740, 10.098], loss: 0.010238, mae: 0.110417, mean_q: 19.289192
 35152/100000: episode: 706, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 52.294, mean reward: 0.523 [0.340, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.498, 10.186], loss: 0.009364, mae: 0.101756, mean_q: 19.763556
 35252/100000: episode: 707, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 54.446, mean reward: 0.544 [0.237, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.625, 10.149], loss: 0.007898, mae: 0.096940, mean_q: 19.300087
 35352/100000: episode: 708, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 55.100, mean reward: 0.551 [0.302, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.747, 10.098], loss: 0.010311, mae: 0.111782, mean_q: 19.293507
 35452/100000: episode: 709, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 53.976, mean reward: 0.540 [0.262, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.058, 10.138], loss: 0.009391, mae: 0.105022, mean_q: 19.674374
[Info] New level: 0.08597135543823242 | Considering 10/90 traces
 35552/100000: episode: 710, duration: 4.434s, episode steps: 100, steps per second: 23, episode reward: 51.497, mean reward: 0.515 [0.177, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.634, 10.261], loss: 0.009027, mae: 0.102093, mean_q: 19.526522
 35554/100000: episode: 711, duration: 0.024s, episode steps: 2, steps per second: 85, episode reward: 0.970, mean reward: 0.485 [0.484, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.267, 10.100], loss: 0.011346, mae: 0.109540, mean_q: 20.459045
 35556/100000: episode: 712, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.780, mean reward: 0.390 [0.319, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.212, 10.100], loss: 0.006699, mae: 0.082299, mean_q: 16.973194
 35558/100000: episode: 713, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.824, mean reward: 0.412 [0.405, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.330, 10.100], loss: 0.004829, mae: 0.075075, mean_q: 19.251022
 35560/100000: episode: 714, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.082, mean reward: 0.541 [0.538, 0.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.292, 10.100], loss: 0.006996, mae: 0.085752, mean_q: 18.044552
 35562/100000: episode: 715, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.709, mean reward: 0.355 [0.346, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.255, 10.100], loss: 0.006316, mae: 0.093410, mean_q: 19.794622
 35564/100000: episode: 716, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.818, mean reward: 0.409 [0.375, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.252, 10.100], loss: 0.005704, mae: 0.086989, mean_q: 19.711647
 35566/100000: episode: 717, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.501, mean reward: 0.251 [0.233, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.418, 10.100], loss: 0.007281, mae: 0.096997, mean_q: 19.404642
 35568/100000: episode: 718, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.866, mean reward: 0.433 [0.385, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.177, 10.100], loss: 0.005154, mae: 0.081613, mean_q: 19.123989
 35570/100000: episode: 719, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.533, mean reward: 0.267 [0.262, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.426, 10.100], loss: 0.005268, mae: 0.082883, mean_q: 20.055878
 35572/100000: episode: 720, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.862, mean reward: 0.431 [0.415, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.237, 10.100], loss: 0.016504, mae: 0.130698, mean_q: 21.907581
 35574/100000: episode: 721, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 1.059, mean reward: 0.530 [0.503, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.304, 10.100], loss: 0.017377, mae: 0.145860, mean_q: 20.744267
 35576/100000: episode: 722, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.698, mean reward: 0.349 [0.301, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.297, 10.100], loss: 0.020656, mae: 0.176194, mean_q: 19.264631
 35578/100000: episode: 723, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.664, mean reward: 0.332 [0.310, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.330, 10.100], loss: 0.020321, mae: 0.157256, mean_q: 18.866529
 35580/100000: episode: 724, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.868, mean reward: 0.434 [0.387, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.079, 10.100], loss: 0.021838, mae: 0.156097, mean_q: 18.493710
 35582/100000: episode: 725, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 0.574, mean reward: 0.287 [0.274, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.357, 10.100], loss: 0.007706, mae: 0.099009, mean_q: 17.944338
 35584/100000: episode: 726, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.854, mean reward: 0.427 [0.403, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.227, 10.100], loss: 0.008459, mae: 0.101885, mean_q: 18.354099
 35586/100000: episode: 727, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.883, mean reward: 0.441 [0.438, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.268, 10.100], loss: 0.010655, mae: 0.119451, mean_q: 19.712782
 35588/100000: episode: 728, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.924, mean reward: 0.462 [0.450, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.117, 10.100], loss: 0.009054, mae: 0.103715, mean_q: 21.667789
 35590/100000: episode: 729, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.940, mean reward: 0.470 [0.468, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.194, 10.100], loss: 0.010005, mae: 0.108192, mean_q: 19.147900
 35592/100000: episode: 730, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 1.027, mean reward: 0.514 [0.513, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.239, 10.100], loss: 0.010602, mae: 0.109517, mean_q: 19.202362
 35594/100000: episode: 731, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 1.069, mean reward: 0.535 [0.480, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.220, 10.100], loss: 0.008117, mae: 0.103589, mean_q: 19.596992
 35596/100000: episode: 732, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.973, mean reward: 0.487 [0.474, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.191, 10.100], loss: 0.011996, mae: 0.128906, mean_q: 20.608850
 35598/100000: episode: 733, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.545, mean reward: 0.272 [0.218, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.291, 10.100], loss: 0.006944, mae: 0.092358, mean_q: 20.398560
 35600/100000: episode: 734, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.008, mean reward: 0.504 [0.460, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.221, 10.100], loss: 0.008979, mae: 0.108060, mean_q: 21.359180
 35602/100000: episode: 735, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.872, mean reward: 0.436 [0.421, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.212, 10.100], loss: 0.007461, mae: 0.088045, mean_q: 20.154282
 35604/100000: episode: 736, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.816, mean reward: 0.408 [0.369, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.340, 10.100], loss: 0.005405, mae: 0.087314, mean_q: 19.448179
 35606/100000: episode: 737, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.500, mean reward: 0.250 [0.217, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.351, 10.100], loss: 0.004184, mae: 0.071094, mean_q: 20.355703
 35608/100000: episode: 738, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.910, mean reward: 0.455 [0.454, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.282, 10.100], loss: 0.010061, mae: 0.100053, mean_q: 20.056511
 35610/100000: episode: 739, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.820, mean reward: 0.410 [0.397, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.293, 10.100], loss: 0.005616, mae: 0.078882, mean_q: 18.131733
 35612/100000: episode: 740, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.899, mean reward: 0.450 [0.428, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.244, 10.100], loss: 0.013146, mae: 0.095891, mean_q: 20.075104
 35614/100000: episode: 741, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.680, mean reward: 0.340 [0.333, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.456, 10.100], loss: 0.006085, mae: 0.093123, mean_q: 19.217882
 35616/100000: episode: 742, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 0.829, mean reward: 0.415 [0.409, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.223, 10.100], loss: 0.007114, mae: 0.096092, mean_q: 19.037123
 35618/100000: episode: 743, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.108, mean reward: 0.554 [0.520, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.209, 10.100], loss: 0.006096, mae: 0.082902, mean_q: 19.326185
 35620/100000: episode: 744, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.929, mean reward: 0.465 [0.460, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.181, 10.100], loss: 0.003948, mae: 0.076335, mean_q: 21.392078
 35622/100000: episode: 745, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.034, mean reward: 0.517 [0.508, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.229, 10.100], loss: 0.007689, mae: 0.098458, mean_q: 20.222534
 35624/100000: episode: 746, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.820, mean reward: 0.410 [0.334, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.208, 10.100], loss: 0.011787, mae: 0.114335, mean_q: 19.643652
 35626/100000: episode: 747, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.762, mean reward: 0.381 [0.370, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.329, 10.100], loss: 0.005360, mae: 0.080314, mean_q: 17.111345
 35628/100000: episode: 748, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.849, mean reward: 0.424 [0.411, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.302, 10.100], loss: 0.011151, mae: 0.114220, mean_q: 19.791000
 35630/100000: episode: 749, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.916, mean reward: 0.458 [0.428, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.152, 10.100], loss: 0.006452, mae: 0.086586, mean_q: 19.781651
 35632/100000: episode: 750, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.154, mean reward: 0.577 [0.566, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.231, 10.100], loss: 0.010851, mae: 0.116161, mean_q: 18.833172
 35634/100000: episode: 751, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.843, mean reward: 0.422 [0.396, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.300, 10.100], loss: 0.011191, mae: 0.121458, mean_q: 17.392052
 35636/100000: episode: 752, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.782, mean reward: 0.391 [0.366, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.947, 10.100], loss: 0.009201, mae: 0.102704, mean_q: 17.551025
 35638/100000: episode: 753, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.979, mean reward: 0.489 [0.451, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.282, 10.100], loss: 0.005531, mae: 0.088967, mean_q: 18.415903
 35640/100000: episode: 754, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.787, mean reward: 0.393 [0.352, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.251, 10.100], loss: 0.008985, mae: 0.101230, mean_q: 19.648720
 35642/100000: episode: 755, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.773, mean reward: 0.387 [0.358, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.347, 10.100], loss: 0.004073, mae: 0.074179, mean_q: 18.311918
 35644/100000: episode: 756, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.780, mean reward: 0.390 [0.385, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.262, 10.100], loss: 0.010304, mae: 0.088487, mean_q: 18.776882
 35646/100000: episode: 757, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 0.840, mean reward: 0.420 [0.401, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.239, 10.100], loss: 0.006451, mae: 0.085956, mean_q: 18.925966
 35648/100000: episode: 758, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 0.874, mean reward: 0.437 [0.429, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.351, 10.100], loss: 0.009196, mae: 0.101481, mean_q: 17.181345
 35650/100000: episode: 759, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.939, mean reward: 0.469 [0.454, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.194, 10.100], loss: 0.010143, mae: 0.102404, mean_q: 19.662558
 35652/100000: episode: 760, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.776, mean reward: 0.388 [0.335, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.304, 10.100], loss: 0.012238, mae: 0.121336, mean_q: 19.003353
 35654/100000: episode: 761, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 1.096, mean reward: 0.548 [0.520, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.588, 10.100], loss: 0.011774, mae: 0.112112, mean_q: 19.122953
 35656/100000: episode: 762, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.019, mean reward: 0.510 [0.507, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.181, 10.100], loss: 0.009556, mae: 0.108932, mean_q: 19.500216
 35658/100000: episode: 763, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.076, mean reward: 0.538 [0.517, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.260, 10.100], loss: 0.005976, mae: 0.088596, mean_q: 19.737761
 35660/100000: episode: 764, duration: 0.016s, episode steps: 2, steps per second: 121, episode reward: 0.826, mean reward: 0.413 [0.399, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.294, 10.100], loss: 0.006419, mae: 0.088508, mean_q: 18.121971
 35662/100000: episode: 765, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.540, mean reward: 0.270 [0.253, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.411, 10.100], loss: 0.009494, mae: 0.102235, mean_q: 19.823042
 35664/100000: episode: 766, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.798, mean reward: 0.399 [0.364, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.264, 10.100], loss: 0.006915, mae: 0.088665, mean_q: 19.485882
 35666/100000: episode: 767, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 1.042, mean reward: 0.521 [0.473, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.311, 10.100], loss: 0.006557, mae: 0.088928, mean_q: 17.743290
 35668/100000: episode: 768, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 1.013, mean reward: 0.507 [0.498, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.256, 10.100], loss: 0.006107, mae: 0.083248, mean_q: 18.949123
 35670/100000: episode: 769, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.033, mean reward: 0.516 [0.505, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.277, 10.100], loss: 0.008612, mae: 0.085379, mean_q: 18.877388
 35672/100000: episode: 770, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.927, mean reward: 0.464 [0.460, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.307, 10.100], loss: 0.006713, mae: 0.087759, mean_q: 19.734509
 35674/100000: episode: 771, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 1.047, mean reward: 0.524 [0.502, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.212, 10.100], loss: 0.016237, mae: 0.124672, mean_q: 17.756296
 35676/100000: episode: 772, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 0.775, mean reward: 0.387 [0.348, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.241, 10.100], loss: 0.008945, mae: 0.100614, mean_q: 22.001474
 35678/100000: episode: 773, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.623, mean reward: 0.312 [0.301, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.366, 10.100], loss: 0.006178, mae: 0.089705, mean_q: 17.812050
 35680/100000: episode: 774, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.989, mean reward: 0.495 [0.489, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.236, 10.100], loss: 0.009372, mae: 0.101829, mean_q: 18.641150
 35682/100000: episode: 775, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.215, mean reward: 0.608 [0.597, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.264, 10.100], loss: 0.008363, mae: 0.107344, mean_q: 20.902267
 35684/100000: episode: 776, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.708, mean reward: 0.354 [0.330, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.342, 10.100], loss: 0.007865, mae: 0.090404, mean_q: 17.786524
 35686/100000: episode: 777, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.885, mean reward: 0.443 [0.435, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.189, 10.100], loss: 0.005214, mae: 0.084687, mean_q: 20.340717
 35688/100000: episode: 778, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.878, mean reward: 0.439 [0.414, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.215, 10.100], loss: 0.008036, mae: 0.092860, mean_q: 18.483463
 35690/100000: episode: 779, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.803, mean reward: 0.402 [0.388, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.168, 10.100], loss: 0.010475, mae: 0.109739, mean_q: 16.842133
 35692/100000: episode: 780, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 0.747, mean reward: 0.373 [0.344, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.368, 10.100], loss: 0.008546, mae: 0.099282, mean_q: 17.691107
 35694/100000: episode: 781, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.817, mean reward: 0.409 [0.408, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.289, 10.100], loss: 0.008300, mae: 0.094692, mean_q: 18.540648
 35696/100000: episode: 782, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.894, mean reward: 0.447 [0.355, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.646, 10.100], loss: 0.009841, mae: 0.104591, mean_q: 16.925539
 35698/100000: episode: 783, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.898, mean reward: 0.449 [0.396, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.528, 10.100], loss: 0.007341, mae: 0.091789, mean_q: 18.632267
 35700/100000: episode: 784, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 1.083, mean reward: 0.542 [0.535, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.187, 10.100], loss: 0.012208, mae: 0.120840, mean_q: 18.959991
 35702/100000: episode: 785, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.835, mean reward: 0.418 [0.415, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.125, 10.100], loss: 0.009517, mae: 0.102907, mean_q: 16.879305
 35704/100000: episode: 786, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 1.148, mean reward: 0.574 [0.541, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.244, 10.100], loss: 0.010793, mae: 0.109681, mean_q: 18.081751
 35706/100000: episode: 787, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.730, mean reward: 0.365 [0.364, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.227, 10.100], loss: 0.012132, mae: 0.124898, mean_q: 19.197723
 35708/100000: episode: 788, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.990, mean reward: 0.495 [0.493, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.196, 10.100], loss: 0.007027, mae: 0.088012, mean_q: 17.608501
 35710/100000: episode: 789, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.988, mean reward: 0.494 [0.465, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.218, 10.100], loss: 0.008505, mae: 0.094616, mean_q: 18.749050
 35712/100000: episode: 790, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.497, mean reward: 0.249 [0.244, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.356, 10.100], loss: 0.010126, mae: 0.111352, mean_q: 18.666735
 35714/100000: episode: 791, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.978, mean reward: 0.489 [0.473, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.305, 10.100], loss: 0.007404, mae: 0.092858, mean_q: 18.330690
 35716/100000: episode: 792, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.775, mean reward: 0.387 [0.379, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.345, 10.100], loss: 0.008044, mae: 0.089645, mean_q: 19.491341
 35718/100000: episode: 793, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.923, mean reward: 0.462 [0.431, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.203, 10.100], loss: 0.007085, mae: 0.095254, mean_q: 17.793983
 35720/100000: episode: 794, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 0.754, mean reward: 0.377 [0.364, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.200, 10.100], loss: 0.006480, mae: 0.083693, mean_q: 19.353817
 35722/100000: episode: 795, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.937, mean reward: 0.469 [0.375, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.279, 10.100], loss: 0.006251, mae: 0.092679, mean_q: 21.382397
 35724/100000: episode: 796, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.965, mean reward: 0.482 [0.464, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.205, 10.100], loss: 0.006258, mae: 0.090645, mean_q: 19.399998
 35726/100000: episode: 797, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.620, mean reward: 0.310 [0.218, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.214, 10.100], loss: 0.012055, mae: 0.111798, mean_q: 17.910324
 35728/100000: episode: 798, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.939, mean reward: 0.470 [0.452, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.184, 10.100], loss: 0.008783, mae: 0.101556, mean_q: 19.466042
 35730/100000: episode: 799, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 1.002, mean reward: 0.501 [0.496, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.131, 10.100], loss: 0.015320, mae: 0.122627, mean_q: 18.912035
[Info] New level: -0.34198224544525146 | Considering 10/90 traces
 35732/100000: episode: 800, duration: 4.097s, episode steps: 2, steps per second: 0, episode reward: 0.507, mean reward: 0.254 [0.245, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.426, 10.100], loss: 0.006177, mae: 0.086703, mean_q: 18.785419
 35733/100000: episode: 801, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.248, mean reward: 0.248 [0.248, 0.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.356, 10.100], loss: 0.010551, mae: 0.115957, mean_q: 19.290876
 35734/100000: episode: 802, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.329, mean reward: 0.329 [0.329, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.364, 10.100], loss: 0.005595, mae: 0.084359, mean_q: 17.046597
 35735/100000: episode: 803, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.223, mean reward: 0.223 [0.223, 0.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.365, 10.100], loss: 0.007361, mae: 0.084752, mean_q: 20.735920
 35736/100000: episode: 804, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.235, mean reward: 0.235 [0.235, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.393, 10.100], loss: 0.007020, mae: 0.092396, mean_q: 18.987255
 35737/100000: episode: 805, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.321, mean reward: 0.321 [0.321, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.370, 10.100], loss: 0.009427, mae: 0.107311, mean_q: 17.474070
 35738/100000: episode: 806, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.355, mean reward: 0.355 [0.355, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.353, 10.100], loss: 0.005316, mae: 0.085502, mean_q: 19.378445
 35739/100000: episode: 807, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.275, mean reward: 0.275 [0.275, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.319, 10.100], loss: 0.004031, mae: 0.074549, mean_q: 19.237877
 35740/100000: episode: 808, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.263, mean reward: 0.263 [0.263, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.366, 10.100], loss: 0.005746, mae: 0.085028, mean_q: 19.319145
 35741/100000: episode: 809, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.153, mean reward: 0.153 [0.153, 0.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.374, 10.100], loss: 0.009177, mae: 0.106467, mean_q: 19.593349
 35742/100000: episode: 810, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.231, mean reward: 0.231 [0.231, 0.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.352, 10.100], loss: 0.006816, mae: 0.091451, mean_q: 16.973112
 35743/100000: episode: 811, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.186, mean reward: 0.186 [0.186, 0.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.427, 10.100], loss: 0.011403, mae: 0.119199, mean_q: 16.999737
 35744/100000: episode: 812, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.287, mean reward: 0.287 [0.287, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.602, 10.100], loss: 0.013706, mae: 0.117854, mean_q: 17.830669
 35745/100000: episode: 813, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.233, mean reward: 0.233 [0.233, 0.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.395, 10.100], loss: 0.006819, mae: 0.099465, mean_q: 17.958183
 35746/100000: episode: 814, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.347, 10.100], loss: 0.005552, mae: 0.087102, mean_q: 20.100086
 35747/100000: episode: 815, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.297, mean reward: 0.297 [0.297, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.374, 10.100], loss: 0.011983, mae: 0.115377, mean_q: 20.282951
 35748/100000: episode: 816, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.340, mean reward: 0.340 [0.340, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.348, 10.100], loss: 0.021056, mae: 0.136657, mean_q: 16.121082
 35749/100000: episode: 817, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.256, mean reward: 0.256 [0.256, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.331, 10.100], loss: 0.015769, mae: 0.151840, mean_q: 17.483492
 35750/100000: episode: 818, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.271, mean reward: 0.271 [0.271, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.403, 10.100], loss: 0.017972, mae: 0.163717, mean_q: 19.474945
 35751/100000: episode: 819, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.236, mean reward: 0.236 [0.236, 0.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.435, 10.100], loss: 0.010196, mae: 0.118375, mean_q: 19.105511
 35752/100000: episode: 820, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.298, mean reward: 0.298 [0.298, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.331, 10.100], loss: 0.014472, mae: 0.107916, mean_q: 20.095406
 35753/100000: episode: 821, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.217, mean reward: 0.217 [0.217, 0.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.332, 10.100], loss: 0.014199, mae: 0.141872, mean_q: 14.615973
 35754/100000: episode: 822, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.341, 10.100], loss: 0.019643, mae: 0.167082, mean_q: 19.842743
 35755/100000: episode: 823, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.249, mean reward: 0.249 [0.249, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.351, 10.100], loss: 0.011795, mae: 0.119047, mean_q: 21.455910
 35756/100000: episode: 824, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.375, mean reward: 0.375 [0.375, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.403, 10.100], loss: 0.005806, mae: 0.089916, mean_q: 21.906713
 35757/100000: episode: 825, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.337, mean reward: 0.337 [0.337, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.367, 10.100], loss: 0.006282, mae: 0.086502, mean_q: 22.051865
 35758/100000: episode: 826, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.283, mean reward: 0.283 [0.283, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.362, 10.100], loss: 0.003919, mae: 0.073005, mean_q: 16.929943
 35759/100000: episode: 827, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.251, mean reward: 0.251 [0.251, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.347, 10.100], loss: 0.004742, mae: 0.078653, mean_q: 19.112318
 35760/100000: episode: 828, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.315, mean reward: 0.315 [0.315, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.308, 10.100], loss: 0.005890, mae: 0.090942, mean_q: 17.594679
 35761/100000: episode: 829, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.238, mean reward: 0.238 [0.238, 0.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.354, 10.100], loss: 0.007199, mae: 0.090531, mean_q: 19.907019
 35762/100000: episode: 830, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.276, mean reward: 0.276 [0.276, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.352, 10.100], loss: 0.008656, mae: 0.106295, mean_q: 21.286734
 35763/100000: episode: 831, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.280, mean reward: 0.280 [0.280, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.281, 10.100], loss: 0.003771, mae: 0.070057, mean_q: 19.530073
 35764/100000: episode: 832, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.414, 10.100], loss: 0.005323, mae: 0.081632, mean_q: 18.436527
 35765/100000: episode: 833, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.235, mean reward: 0.235 [0.235, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.810, 10.100], loss: 0.006492, mae: 0.087830, mean_q: 17.619038
 35766/100000: episode: 834, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.346, 10.100], loss: 0.011639, mae: 0.115759, mean_q: 14.973570
 35767/100000: episode: 835, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.231, mean reward: 0.231 [0.231, 0.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.491, 10.100], loss: 0.006052, mae: 0.078862, mean_q: 19.263756
 35768/100000: episode: 836, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.224, mean reward: 0.224 [0.224, 0.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.341, 10.100], loss: 0.023447, mae: 0.115679, mean_q: 19.762959
 35769/100000: episode: 837, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.216, mean reward: 0.216 [0.216, 0.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.402, 10.100], loss: 0.003966, mae: 0.074359, mean_q: 18.298933
 35770/100000: episode: 838, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.271, mean reward: 0.271 [0.271, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.385, 10.100], loss: 0.004055, mae: 0.076889, mean_q: 17.364079
 35771/100000: episode: 839, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.289, mean reward: 0.289 [0.289, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.360, 10.100], loss: 0.010765, mae: 0.112859, mean_q: 17.816101
 35772/100000: episode: 840, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.212, mean reward: 0.212 [0.212, 0.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.332, 10.100], loss: 0.013337, mae: 0.133026, mean_q: 20.590200
 35773/100000: episode: 841, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.377, mean reward: 0.377 [0.377, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.353, 10.100], loss: 0.017402, mae: 0.150764, mean_q: 16.500025
 35774/100000: episode: 842, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.252, mean reward: 0.252 [0.252, 0.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.331, 10.100], loss: 0.012440, mae: 0.130063, mean_q: 18.509018
 35775/100000: episode: 843, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.261, mean reward: 0.261 [0.261, 0.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.344, 10.100], loss: 0.008514, mae: 0.104973, mean_q: 19.584969
 35776/100000: episode: 844, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.229, mean reward: 0.229 [0.229, 0.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.373, 10.100], loss: 0.012982, mae: 0.121417, mean_q: 18.192327
 35777/100000: episode: 845, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.364, 10.100], loss: 0.015659, mae: 0.144169, mean_q: 16.202932
 35778/100000: episode: 846, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.362, 10.100], loss: 0.017960, mae: 0.135737, mean_q: 15.792957
 35779/100000: episode: 847, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.339, mean reward: 0.339 [0.339, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.347, 10.100], loss: 0.011378, mae: 0.119644, mean_q: 17.460117
 35780/100000: episode: 848, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.258, mean reward: 0.258 [0.258, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.439, 10.100], loss: 0.005439, mae: 0.083065, mean_q: 20.199043
 35781/100000: episode: 849, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.237, mean reward: 0.237 [0.237, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.341, 10.100], loss: 0.017441, mae: 0.147132, mean_q: 16.821869
 35782/100000: episode: 850, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.287, mean reward: 0.287 [0.287, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.322, 10.100], loss: 0.016722, mae: 0.156253, mean_q: 16.830341
 35783/100000: episode: 851, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.285, mean reward: 0.285 [0.285, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.311, 10.100], loss: 0.009725, mae: 0.111376, mean_q: 19.157040
 35784/100000: episode: 852, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.259, mean reward: 0.259 [0.259, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.313, 10.100], loss: 0.006755, mae: 0.100332, mean_q: 18.498405
 35785/100000: episode: 853, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.375, mean reward: 0.375 [0.375, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.350, 10.100], loss: 0.005424, mae: 0.081584, mean_q: 21.236225
 35786/100000: episode: 854, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.287, mean reward: 0.287 [0.287, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.315, 10.100], loss: 0.008602, mae: 0.091644, mean_q: 21.309792
 35787/100000: episode: 855, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.238, mean reward: 0.238 [0.238, 0.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.359, 10.100], loss: 0.010940, mae: 0.113942, mean_q: 18.971035
 35788/100000: episode: 856, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.235, mean reward: 0.235 [0.235, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.398, 10.100], loss: 0.006178, mae: 0.087674, mean_q: 19.184429
 35789/100000: episode: 857, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.125, mean reward: 0.125 [0.125, 0.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.464, 10.100], loss: 0.005771, mae: 0.084574, mean_q: 16.844870
 35790/100000: episode: 858, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.206, mean reward: 0.206 [0.206, 0.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.349, 10.100], loss: 0.013754, mae: 0.107821, mean_q: 19.252045
 35791/100000: episode: 859, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.166, mean reward: 0.166 [0.166, 0.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.424, 10.100], loss: 0.006661, mae: 0.097143, mean_q: 17.620827
 35792/100000: episode: 860, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.312, mean reward: 0.312 [0.312, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.389, 10.100], loss: 0.005750, mae: 0.077913, mean_q: 19.572472
 35793/100000: episode: 861, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.348, 10.100], loss: 0.008037, mae: 0.094258, mean_q: 18.911041
 35794/100000: episode: 862, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.264, mean reward: 0.264 [0.264, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.483, 10.100], loss: 0.007974, mae: 0.097056, mean_q: 16.174702
 35795/100000: episode: 863, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.252, mean reward: 0.252 [0.252, 0.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.333, 10.100], loss: 0.005113, mae: 0.084932, mean_q: 18.607635
 35796/100000: episode: 864, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.366, 10.100], loss: 0.004005, mae: 0.068726, mean_q: 17.024504
 35797/100000: episode: 865, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.332, mean reward: 0.332 [0.332, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.363, 10.100], loss: 0.016238, mae: 0.107366, mean_q: 18.852955
 35798/100000: episode: 866, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.368, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.270, 10.100], loss: 0.012230, mae: 0.127926, mean_q: 18.216009
 35799/100000: episode: 867, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.303, mean reward: 0.303 [0.303, 0.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.432, 10.100], loss: 0.008723, mae: 0.102864, mean_q: 20.397417
 35800/100000: episode: 868, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.294, mean reward: 0.294 [0.294, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.355, 10.100], loss: 0.005497, mae: 0.088643, mean_q: 17.428864
 35801/100000: episode: 869, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.314, mean reward: 0.314 [0.314, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.367, 10.100], loss: 0.017052, mae: 0.108622, mean_q: 20.426050
 35802/100000: episode: 870, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.297, mean reward: 0.297 [0.297, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.004, 10.100], loss: 0.005707, mae: 0.087320, mean_q: 16.736340
 35803/100000: episode: 871, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.359, mean reward: 0.359 [0.359, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.395, 10.100], loss: 0.008944, mae: 0.106600, mean_q: 19.271461
 35804/100000: episode: 872, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.249, mean reward: 0.249 [0.249, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.369, 10.100], loss: 0.008693, mae: 0.112620, mean_q: 16.177631
 35805/100000: episode: 873, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.243, mean reward: 0.243 [0.243, 0.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.338, 10.100], loss: 0.014931, mae: 0.091416, mean_q: 14.695934
 35806/100000: episode: 874, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.384, mean reward: 0.384 [0.384, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.345, 10.100], loss: 0.014100, mae: 0.110579, mean_q: 15.838682
 35807/100000: episode: 875, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.366, mean reward: 0.366 [0.366, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.401, 10.100], loss: 0.005470, mae: 0.080549, mean_q: 21.155037
 35808/100000: episode: 876, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.270, mean reward: 0.270 [0.270, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.355, 10.100], loss: 0.009059, mae: 0.108071, mean_q: 17.040377
 35809/100000: episode: 877, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.288, mean reward: 0.288 [0.288, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.306, 10.100], loss: 0.004507, mae: 0.079806, mean_q: 18.093191
 35810/100000: episode: 878, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.301, 10.100], loss: 0.003826, mae: 0.069297, mean_q: 17.432354
 35811/100000: episode: 879, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.277, mean reward: 0.277 [0.277, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.390, 10.100], loss: 0.004467, mae: 0.073529, mean_q: 17.215858
 35812/100000: episode: 880, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.234, mean reward: 0.234 [0.234, 0.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.380, 10.100], loss: 0.003965, mae: 0.075547, mean_q: 16.677006
 35813/100000: episode: 881, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.138, mean reward: 0.138 [0.138, 0.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.749, 10.100], loss: 0.004342, mae: 0.080225, mean_q: 20.124361
 35814/100000: episode: 882, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.243, mean reward: 0.243 [0.243, 0.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.473, 10.100], loss: 0.007590, mae: 0.102202, mean_q: 19.485697
 35815/100000: episode: 883, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.334, mean reward: 0.334 [0.334, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.357, 10.100], loss: 0.018660, mae: 0.138761, mean_q: 19.132633
 35816/100000: episode: 884, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.264, mean reward: 0.264 [0.264, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.344, 10.100], loss: 0.007046, mae: 0.099975, mean_q: 16.710512
 35817/100000: episode: 885, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.214, mean reward: 0.214 [0.214, 0.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.313, 10.100], loss: 0.012053, mae: 0.120897, mean_q: 17.288282
 35818/100000: episode: 886, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.243, mean reward: 0.243 [0.243, 0.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.354, 10.100], loss: 0.008767, mae: 0.098402, mean_q: 18.970369
 35819/100000: episode: 887, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.263, mean reward: 0.263 [0.263, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.414, 10.100], loss: 0.005257, mae: 0.079198, mean_q: 18.843416
 35820/100000: episode: 888, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.280, mean reward: 0.280 [0.280, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.373, 10.100], loss: 0.006071, mae: 0.091684, mean_q: 19.767834
 35821/100000: episode: 889, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.289, mean reward: 0.289 [0.289, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.369, 10.100], loss: 0.005723, mae: 0.091409, mean_q: 17.215771
[Info] New level: -0.3645530939102173 | Considering 10/90 traces
 35822/100000: episode: 890, duration: 4.044s, episode steps: 1, steps per second: 0, episode reward: 0.251, mean reward: 0.251 [0.251, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.334, 10.100], loss: 0.006761, mae: 0.083994, mean_q: 18.173512
 35823/100000: episode: 891, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.173, mean reward: 0.173 [0.173, 0.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.456, 10.100], loss: 0.004606, mae: 0.076062, mean_q: 19.688311
 35824/100000: episode: 892, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.197, mean reward: 0.197 [0.197, 0.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.449, 10.100], loss: 0.004615, mae: 0.079440, mean_q: 18.148075
 35825/100000: episode: 893, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.227, mean reward: 0.227 [0.227, 0.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.427, 10.100], loss: 0.013875, mae: 0.119610, mean_q: 17.925367
 35826/100000: episode: 894, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.246, mean reward: 0.246 [0.246, 0.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.359, 10.100], loss: 0.009060, mae: 0.102108, mean_q: 17.070515
 35827/100000: episode: 895, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.390, 10.100], loss: 0.008014, mae: 0.099229, mean_q: 18.622713
 35828/100000: episode: 896, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.275, mean reward: 0.275 [0.275, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.342, 10.100], loss: 0.005156, mae: 0.080283, mean_q: 21.053425
 35829/100000: episode: 897, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.146, mean reward: 0.146 [0.146, 0.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.371, 10.100], loss: 0.007713, mae: 0.085226, mean_q: 18.275211
 35830/100000: episode: 898, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.259, mean reward: 0.259 [0.259, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.442, 10.100], loss: 0.006457, mae: 0.093559, mean_q: 18.852840
 35831/100000: episode: 899, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.266, mean reward: 0.266 [0.266, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.432, 10.100], loss: 0.007010, mae: 0.106742, mean_q: 19.737425
 35832/100000: episode: 900, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.159, mean reward: 0.159 [0.159, 0.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.441, 10.100], loss: 0.009784, mae: 0.114106, mean_q: 17.043873
 35833/100000: episode: 901, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.235, mean reward: 0.235 [0.235, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.376, 10.100], loss: 0.022032, mae: 0.191360, mean_q: 19.569391
 35834/100000: episode: 902, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.310, mean reward: 0.310 [0.310, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.370 [-0.387, 10.100], loss: 0.008541, mae: 0.099029, mean_q: 19.137104
 35835/100000: episode: 903, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.268, mean reward: 0.268 [0.268, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.327, 10.100], loss: 0.007471, mae: 0.093575, mean_q: 19.557850
 35836/100000: episode: 904, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.205, mean reward: 0.205 [0.205, 0.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.365, 10.100], loss: 0.012369, mae: 0.127370, mean_q: 19.646051
 35837/100000: episode: 905, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.275, mean reward: 0.275 [0.275, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.413, 10.100], loss: 0.018563, mae: 0.157142, mean_q: 16.593298
 35838/100000: episode: 906, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.259, mean reward: 0.259 [0.259, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.367, 10.100], loss: 0.010039, mae: 0.111117, mean_q: 19.753597
 35839/100000: episode: 907, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.407, 10.100], loss: 0.006953, mae: 0.097898, mean_q: 17.017469
 35840/100000: episode: 908, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.200, mean reward: 0.200 [0.200, 0.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.425, 10.100], loss: 0.018015, mae: 0.155508, mean_q: 18.284336
 35841/100000: episode: 909, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.244, mean reward: 0.244 [0.244, 0.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.444, 10.100], loss: 0.043098, mae: 0.246595, mean_q: 15.992636
 35842/100000: episode: 910, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.259, mean reward: 0.259 [0.259, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.404, 10.100], loss: 0.015287, mae: 0.137905, mean_q: 17.830437
 35843/100000: episode: 911, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.296, mean reward: 0.296 [0.296, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.511, 10.100], loss: 0.009884, mae: 0.117212, mean_q: 17.402588
 35844/100000: episode: 912, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.243, mean reward: 0.243 [0.243, 0.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.408, 10.100], loss: 0.019972, mae: 0.171900, mean_q: 16.792274
 35845/100000: episode: 913, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.235, mean reward: 0.235 [0.235, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.380, 10.100], loss: 0.018597, mae: 0.164110, mean_q: 16.057087
 35846/100000: episode: 914, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.257, mean reward: 0.257 [0.257, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.495, 10.100], loss: 0.014808, mae: 0.147443, mean_q: 20.912701
 35847/100000: episode: 915, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.253, mean reward: 0.253 [0.253, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.384, 10.100], loss: 0.013649, mae: 0.134296, mean_q: 19.323442
 35848/100000: episode: 916, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.256, mean reward: 0.256 [0.256, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.382, 10.100], loss: 0.007488, mae: 0.088488, mean_q: 18.779728
 35849/100000: episode: 917, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.262, mean reward: 0.262 [0.262, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.447, 10.100], loss: 0.006975, mae: 0.101101, mean_q: 20.694763
 35850/100000: episode: 918, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.193, mean reward: 0.193 [0.193, 0.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.432, 10.100], loss: 0.008004, mae: 0.103113, mean_q: 19.344166
 35851/100000: episode: 919, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.231, mean reward: 0.231 [0.231, 0.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.380, 10.100], loss: 0.004218, mae: 0.076262, mean_q: 19.731028
 35852/100000: episode: 920, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.295, mean reward: 0.295 [0.295, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.377, 10.100], loss: 0.008392, mae: 0.096605, mean_q: 17.670527
 35853/100000: episode: 921, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.256, mean reward: 0.256 [0.256, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.389, 10.100], loss: 0.003359, mae: 0.066594, mean_q: 18.937555
 35854/100000: episode: 922, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.177, mean reward: 0.177 [0.177, 0.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.468, 10.100], loss: 0.010411, mae: 0.118956, mean_q: 20.039669
 35855/100000: episode: 923, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.231, mean reward: 0.231 [0.231, 0.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.361, 10.100], loss: 0.010205, mae: 0.118969, mean_q: 16.736324
 35856/100000: episode: 924, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.204, mean reward: 0.204 [0.204, 0.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.417, 10.100], loss: 0.006545, mae: 0.087138, mean_q: 17.647438
 35857/100000: episode: 925, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.441, 10.100], loss: 0.009096, mae: 0.115401, mean_q: 21.355619
 35858/100000: episode: 926, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.220, mean reward: 0.220 [0.220, 0.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.393, 10.100], loss: 0.008285, mae: 0.109638, mean_q: 20.188683
 35859/100000: episode: 927, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.198, mean reward: 0.198 [0.198, 0.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.433, 10.100], loss: 0.009675, mae: 0.105218, mean_q: 20.047344
 35860/100000: episode: 928, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.232, mean reward: 0.232 [0.232, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.404, 10.100], loss: 0.013777, mae: 0.139405, mean_q: 19.020269
 35861/100000: episode: 929, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.264, mean reward: 0.264 [0.264, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.413, 10.100], loss: 0.011121, mae: 0.110364, mean_q: 21.478466
 35862/100000: episode: 930, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.250, mean reward: 0.250 [0.250, 0.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.453, 10.100], loss: 0.007299, mae: 0.091202, mean_q: 18.017603
 35863/100000: episode: 931, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.231, mean reward: 0.231 [0.231, 0.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.439, 10.100], loss: 0.010630, mae: 0.115126, mean_q: 17.000320
 35864/100000: episode: 932, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.194, mean reward: 0.194 [0.194, 0.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.453, 10.100], loss: 0.004937, mae: 0.076944, mean_q: 16.303205
 35865/100000: episode: 933, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.264, mean reward: 0.264 [0.264, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.437, 10.100], loss: 0.004919, mae: 0.080524, mean_q: 17.031101
 35866/100000: episode: 934, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.211, mean reward: 0.211 [0.211, 0.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.433, 10.100], loss: 0.012618, mae: 0.117927, mean_q: 17.897762
 35867/100000: episode: 935, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.215, mean reward: 0.215 [0.215, 0.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.423, 10.100], loss: 0.009460, mae: 0.092566, mean_q: 20.882011
 35868/100000: episode: 936, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.181, mean reward: 0.181 [0.181, 0.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.483, 10.100], loss: 0.007652, mae: 0.100619, mean_q: 17.751347
 35869/100000: episode: 937, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.323, mean reward: 0.323 [0.323, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.355, 10.100], loss: 0.004817, mae: 0.077235, mean_q: 16.828728
 35870/100000: episode: 938, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.266, mean reward: 0.266 [0.266, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.394, 10.100], loss: 0.006255, mae: 0.085593, mean_q: 17.084389
 35871/100000: episode: 939, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.245, mean reward: 0.245 [0.245, 0.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.387, 10.100], loss: 0.013662, mae: 0.134595, mean_q: 16.104208
 35872/100000: episode: 940, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.255, mean reward: 0.255 [0.255, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.416, 10.100], loss: 0.009703, mae: 0.111852, mean_q: 19.611469
 35873/100000: episode: 941, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.258, mean reward: 0.258 [0.258, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.423, 10.100], loss: 0.012850, mae: 0.140910, mean_q: 22.488333
 35874/100000: episode: 942, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.218, mean reward: 0.218 [0.218, 0.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.398, 10.100], loss: 0.011100, mae: 0.112628, mean_q: 18.209015
 35875/100000: episode: 943, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.410, 10.100], loss: 0.006180, mae: 0.076330, mean_q: 17.719831
 35876/100000: episode: 944, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.184, mean reward: 0.184 [0.184, 0.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.426, 10.100], loss: 0.006434, mae: 0.095176, mean_q: 18.210842
 35877/100000: episode: 945, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.280, mean reward: 0.280 [0.280, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.408, 10.100], loss: 0.007995, mae: 0.096577, mean_q: 16.920340
 35878/100000: episode: 946, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.361, 10.100], loss: 0.011876, mae: 0.119724, mean_q: 17.913567
 35879/100000: episode: 947, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.284, mean reward: 0.284 [0.284, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.406, 10.100], loss: 0.012277, mae: 0.119442, mean_q: 17.751545
 35880/100000: episode: 948, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.454, 10.100], loss: 0.008849, mae: 0.105146, mean_q: 18.986111
 35881/100000: episode: 949, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.387, 10.100], loss: 0.006120, mae: 0.082899, mean_q: 22.107830
 35882/100000: episode: 950, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.264, mean reward: 0.264 [0.264, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.404, 10.100], loss: 0.011863, mae: 0.093249, mean_q: 16.841991
 35883/100000: episode: 951, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.233, mean reward: 0.233 [0.233, 0.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.410, 10.100], loss: 0.009007, mae: 0.110710, mean_q: 21.803261
 35884/100000: episode: 952, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.282, mean reward: 0.282 [0.282, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.393, 10.100], loss: 0.008598, mae: 0.110819, mean_q: 17.642853
 35885/100000: episode: 953, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.240, mean reward: 0.240 [0.240, 0.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.367, 10.100], loss: 0.010318, mae: 0.100375, mean_q: 19.905128
 35886/100000: episode: 954, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.209, mean reward: 0.209 [0.209, 0.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.441, 10.100], loss: 0.008713, mae: 0.089217, mean_q: 20.447357
 35887/100000: episode: 955, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.232, mean reward: 0.232 [0.232, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.343, 10.100], loss: 0.008142, mae: 0.094148, mean_q: 16.679995
 35888/100000: episode: 956, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.270, mean reward: 0.270 [0.270, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.385, 10.100], loss: 0.005664, mae: 0.087627, mean_q: 17.615135
 35889/100000: episode: 957, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.122, mean reward: 0.122 [0.122, 0.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.470, 10.100], loss: 0.003483, mae: 0.070227, mean_q: 17.283390
 35890/100000: episode: 958, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.250, mean reward: 0.250 [0.250, 0.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.454, 10.100], loss: 0.005304, mae: 0.078221, mean_q: 18.072752
 35891/100000: episode: 959, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.281, mean reward: 0.281 [0.281, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.424, 10.100], loss: 0.007797, mae: 0.098518, mean_q: 17.030014
 35892/100000: episode: 960, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.253, mean reward: 0.253 [0.253, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.417, 10.100], loss: 0.011298, mae: 0.129693, mean_q: 14.704068
 35893/100000: episode: 961, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.281, mean reward: 0.281 [0.281, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.338, 10.100], loss: 0.006597, mae: 0.090546, mean_q: 17.383324
 35894/100000: episode: 962, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.183, mean reward: 0.183 [0.183, 0.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.384, 10.100], loss: 0.003924, mae: 0.076658, mean_q: 16.962860
 35895/100000: episode: 963, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.245, mean reward: 0.245 [0.245, 0.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.405, 10.100], loss: 0.011208, mae: 0.085869, mean_q: 19.761135
 35896/100000: episode: 964, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.235, mean reward: 0.235 [0.235, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.437, 10.100], loss: 0.006079, mae: 0.086524, mean_q: 18.068684
 35897/100000: episode: 965, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.162, mean reward: 0.162 [0.162, 0.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.445, 10.100], loss: 0.005326, mae: 0.078026, mean_q: 17.434660
 35898/100000: episode: 966, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.248, mean reward: 0.248 [0.248, 0.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.455, 10.100], loss: 0.006359, mae: 0.102329, mean_q: 18.293484
 35899/100000: episode: 967, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.182, mean reward: 0.182 [0.182, 0.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.457, 10.100], loss: 0.010209, mae: 0.128263, mean_q: 16.184643
 35900/100000: episode: 968, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.318, mean reward: 0.318 [0.318, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.407, 10.100], loss: 0.010231, mae: 0.103759, mean_q: 18.101295
 35901/100000: episode: 969, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.256, mean reward: 0.256 [0.256, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.396, 10.100], loss: 0.010981, mae: 0.122277, mean_q: 16.974403
 35902/100000: episode: 970, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.496, 10.100], loss: 0.018867, mae: 0.119243, mean_q: 17.124733
 35903/100000: episode: 971, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.232, mean reward: 0.232 [0.232, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.434, 10.100], loss: 0.008944, mae: 0.110018, mean_q: 16.456318
 35904/100000: episode: 972, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.232, mean reward: 0.232 [0.232, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.407, 10.100], loss: 0.009025, mae: 0.120692, mean_q: 19.665033
 35905/100000: episode: 973, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.225, mean reward: 0.225 [0.225, 0.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.383, 10.100], loss: 0.008596, mae: 0.102244, mean_q: 18.567389
 35906/100000: episode: 974, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.185, mean reward: 0.185 [0.185, 0.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.412, 10.100], loss: 0.005792, mae: 0.090507, mean_q: 20.311705
 35907/100000: episode: 975, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.215, mean reward: 0.215 [0.215, 0.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.458, 10.100], loss: 0.003997, mae: 0.074634, mean_q: 17.849932
 35908/100000: episode: 976, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.217, mean reward: 0.217 [0.217, 0.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.399, 10.100], loss: 0.009199, mae: 0.100479, mean_q: 19.096729
 35909/100000: episode: 977, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.249, mean reward: 0.249 [0.249, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.368, 10.100], loss: 0.008757, mae: 0.113027, mean_q: 18.141121
 35910/100000: episode: 978, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.229, mean reward: 0.229 [0.229, 0.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.407, 10.100], loss: 0.020991, mae: 0.138868, mean_q: 20.050983
 35911/100000: episode: 979, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.203, mean reward: 0.203 [0.203, 0.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.435, 10.100], loss: 0.009828, mae: 0.101115, mean_q: 19.175850
[Info] Not found new level, current best level reached = -0.3645530939102173
 35912/100000: episode: 980, duration: 4.002s, episode steps: 1, steps per second: 0, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.390, 10.100], loss: 0.005429, mae: 0.085863, mean_q: 16.533377
 36012/100000: episode: 981, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 50.450, mean reward: 0.504 [0.186, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.854, 10.331], loss: 0.010218, mae: 0.104661, mean_q: 18.192165
 36112/100000: episode: 982, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 49.561, mean reward: 0.496 [0.265, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.663, 10.332], loss: 0.009044, mae: 0.100756, mean_q: 17.788824
 36212/100000: episode: 983, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 50.693, mean reward: 0.507 [0.220, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.467, 10.352], loss: 0.008610, mae: 0.096398, mean_q: 18.298029
 36312/100000: episode: 984, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 53.686, mean reward: 0.537 [0.282, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.008, 10.098], loss: 0.008688, mae: 0.098345, mean_q: 18.610285
 36412/100000: episode: 985, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 51.311, mean reward: 0.513 [0.219, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.360, 10.217], loss: 0.009404, mae: 0.099422, mean_q: 17.965740
 36512/100000: episode: 986, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 53.286, mean reward: 0.533 [0.333, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.815, 10.098], loss: 0.011761, mae: 0.115802, mean_q: 17.789982
 36612/100000: episode: 987, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: 53.780, mean reward: 0.538 [0.282, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.672, 10.111], loss: 0.009969, mae: 0.103788, mean_q: 17.941967
 36712/100000: episode: 988, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 51.379, mean reward: 0.514 [0.290, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.517, 10.182], loss: 0.011231, mae: 0.111013, mean_q: 18.287712
 36812/100000: episode: 989, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 56.712, mean reward: 0.567 [0.383, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.700, 10.098], loss: 0.009931, mae: 0.100753, mean_q: 18.050610
 36912/100000: episode: 990, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 53.592, mean reward: 0.536 [0.298, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.404, 10.105], loss: 0.009136, mae: 0.100823, mean_q: 18.191422
 37012/100000: episode: 991, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 49.312, mean reward: 0.493 [0.173, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.724, 10.521], loss: 0.009703, mae: 0.102750, mean_q: 18.174688
 37112/100000: episode: 992, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 53.202, mean reward: 0.532 [0.318, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.974, 10.098], loss: 0.009634, mae: 0.102016, mean_q: 18.249653
 37212/100000: episode: 993, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 53.490, mean reward: 0.535 [0.308, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.218, 10.270], loss: 0.010495, mae: 0.105751, mean_q: 18.138819
 37312/100000: episode: 994, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 53.665, mean reward: 0.537 [0.294, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.314, 10.143], loss: 0.009413, mae: 0.102248, mean_q: 17.863396
 37412/100000: episode: 995, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 50.232, mean reward: 0.502 [0.237, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.656, 10.098], loss: 0.009242, mae: 0.101126, mean_q: 17.999636
 37512/100000: episode: 996, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 55.745, mean reward: 0.557 [0.291, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.915, 10.098], loss: 0.011456, mae: 0.113320, mean_q: 17.888351
 37612/100000: episode: 997, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 56.506, mean reward: 0.565 [0.301, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.588, 10.115], loss: 0.012503, mae: 0.116842, mean_q: 17.827288
 37712/100000: episode: 998, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 49.909, mean reward: 0.499 [0.268, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.152, 10.346], loss: 0.012056, mae: 0.116087, mean_q: 18.028786
 37812/100000: episode: 999, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 51.372, mean reward: 0.514 [0.335, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.207, 10.118], loss: 0.009980, mae: 0.104984, mean_q: 18.475422
 37912/100000: episode: 1000, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 44.613, mean reward: 0.446 [0.144, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.499, 10.098], loss: 0.012386, mae: 0.116666, mean_q: 18.186342
 38012/100000: episode: 1001, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 54.243, mean reward: 0.542 [0.259, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.569, 10.112], loss: 0.008860, mae: 0.098492, mean_q: 18.178513
 38112/100000: episode: 1002, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 53.717, mean reward: 0.537 [0.278, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.777, 10.219], loss: 0.011088, mae: 0.111435, mean_q: 17.990969
 38212/100000: episode: 1003, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 52.551, mean reward: 0.526 [0.272, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.708, 10.098], loss: 0.009940, mae: 0.101965, mean_q: 18.093208
 38312/100000: episode: 1004, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 53.306, mean reward: 0.533 [0.237, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.043, 10.098], loss: 0.012308, mae: 0.115193, mean_q: 18.044815
 38412/100000: episode: 1005, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 42.734, mean reward: 0.427 [0.118, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.162, 10.098], loss: 0.010883, mae: 0.109715, mean_q: 18.239851
 38512/100000: episode: 1006, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 54.993, mean reward: 0.550 [0.270, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.581, 10.098], loss: 0.009814, mae: 0.102715, mean_q: 18.233595
 38612/100000: episode: 1007, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 53.397, mean reward: 0.534 [0.258, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.460, 10.281], loss: 0.010706, mae: 0.112319, mean_q: 18.182699
 38712/100000: episode: 1008, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 49.625, mean reward: 0.496 [0.264, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.400, 10.246], loss: 0.010832, mae: 0.112837, mean_q: 18.057367
 38812/100000: episode: 1009, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.822, mean reward: 0.578 [0.403, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.900, 10.098], loss: 0.011242, mae: 0.111476, mean_q: 17.988794
 38912/100000: episode: 1010, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 49.552, mean reward: 0.496 [0.273, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.824, 10.098], loss: 0.009921, mae: 0.105462, mean_q: 17.959518
 39012/100000: episode: 1011, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 54.662, mean reward: 0.547 [0.301, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.991, 10.098], loss: 0.008765, mae: 0.098145, mean_q: 18.046230
 39112/100000: episode: 1012, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 55.773, mean reward: 0.558 [0.257, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.415, 10.151], loss: 0.009870, mae: 0.106254, mean_q: 18.230532
 39212/100000: episode: 1013, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 51.459, mean reward: 0.515 [0.310, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.288, 10.225], loss: 0.008317, mae: 0.097227, mean_q: 17.783161
 39312/100000: episode: 1014, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 54.447, mean reward: 0.544 [0.294, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.542, 10.357], loss: 0.009503, mae: 0.103487, mean_q: 17.831562
 39412/100000: episode: 1015, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.855, mean reward: 0.579 [0.387, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.841, 10.098], loss: 0.009278, mae: 0.101350, mean_q: 18.148897
 39512/100000: episode: 1016, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 50.057, mean reward: 0.501 [0.301, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.314, 10.332], loss: 0.010263, mae: 0.106158, mean_q: 17.941072
 39612/100000: episode: 1017, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 53.809, mean reward: 0.538 [0.126, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.322, 10.098], loss: 0.013223, mae: 0.123386, mean_q: 17.747585
 39712/100000: episode: 1018, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 56.053, mean reward: 0.561 [0.329, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.585, 10.098], loss: 0.008566, mae: 0.097764, mean_q: 18.055870
 39812/100000: episode: 1019, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 49.423, mean reward: 0.494 [0.047, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.921, 10.098], loss: 0.010172, mae: 0.103785, mean_q: 17.806747
 39912/100000: episode: 1020, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 54.710, mean reward: 0.547 [0.210, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.171, 10.164], loss: 0.011336, mae: 0.112866, mean_q: 17.951540
 40012/100000: episode: 1021, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 51.668, mean reward: 0.517 [0.252, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.003, 10.385], loss: 0.012068, mae: 0.115023, mean_q: 17.983891
 40112/100000: episode: 1022, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 53.728, mean reward: 0.537 [0.289, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.402, 10.278], loss: 0.011535, mae: 0.112862, mean_q: 18.052086
 40212/100000: episode: 1023, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 54.723, mean reward: 0.547 [0.306, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.883, 10.098], loss: 0.010943, mae: 0.109382, mean_q: 18.042234
 40312/100000: episode: 1024, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 52.788, mean reward: 0.528 [0.278, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.687, 10.098], loss: 0.010120, mae: 0.105269, mean_q: 18.154543
 40412/100000: episode: 1025, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 53.333, mean reward: 0.533 [0.295, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.875, 10.234], loss: 0.012077, mae: 0.116611, mean_q: 18.386549
 40512/100000: episode: 1026, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.411, mean reward: 0.554 [0.338, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.685, 10.098], loss: 0.010902, mae: 0.107631, mean_q: 19.085957
 40612/100000: episode: 1027, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 56.266, mean reward: 0.563 [0.322, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.602, 10.190], loss: 0.009521, mae: 0.103334, mean_q: 19.051222
 40712/100000: episode: 1028, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 53.789, mean reward: 0.538 [0.309, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.480, 10.190], loss: 0.010184, mae: 0.106092, mean_q: 19.322033
 40812/100000: episode: 1029, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 55.991, mean reward: 0.560 [0.356, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.809, 10.223], loss: 0.011502, mae: 0.109698, mean_q: 19.343271
 40912/100000: episode: 1030, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 54.688, mean reward: 0.547 [0.342, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.478, 10.098], loss: 0.009782, mae: 0.105252, mean_q: 19.400623
 41012/100000: episode: 1031, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 48.033, mean reward: 0.480 [0.188, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.859, 10.098], loss: 0.009577, mae: 0.106403, mean_q: 19.635061
 41112/100000: episode: 1032, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 48.813, mean reward: 0.488 [0.291, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.176, 10.098], loss: 0.009754, mae: 0.106623, mean_q: 19.391043
 41212/100000: episode: 1033, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.414, mean reward: 0.534 [0.248, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.456, 10.098], loss: 0.010561, mae: 0.106951, mean_q: 19.418577
 41312/100000: episode: 1034, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 51.769, mean reward: 0.518 [0.129, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.248, 10.098], loss: 0.011759, mae: 0.117545, mean_q: 19.298386
 41412/100000: episode: 1035, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 54.127, mean reward: 0.541 [0.262, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.818, 10.183], loss: 0.008548, mae: 0.099528, mean_q: 19.286226
 41512/100000: episode: 1036, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 56.382, mean reward: 0.564 [0.265, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.586, 10.142], loss: 0.010836, mae: 0.116501, mean_q: 19.308287
 41612/100000: episode: 1037, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 50.074, mean reward: 0.501 [0.325, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.654, 10.098], loss: 0.010204, mae: 0.110726, mean_q: 19.260506
 41712/100000: episode: 1038, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 56.160, mean reward: 0.562 [0.363, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.951, 10.117], loss: 0.008693, mae: 0.099637, mean_q: 19.572670
 41812/100000: episode: 1039, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 50.538, mean reward: 0.505 [0.251, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.215, 10.187], loss: 0.008424, mae: 0.100093, mean_q: 19.392189
 41912/100000: episode: 1040, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 55.459, mean reward: 0.555 [0.199, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.204, 10.218], loss: 0.007834, mae: 0.098058, mean_q: 19.489923
 42012/100000: episode: 1041, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 51.124, mean reward: 0.511 [0.325, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.888, 10.405], loss: 0.008603, mae: 0.103024, mean_q: 19.569548
 42112/100000: episode: 1042, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 48.160, mean reward: 0.482 [0.298, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.129, 10.126], loss: 0.007496, mae: 0.094966, mean_q: 19.624151
 42212/100000: episode: 1043, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 50.229, mean reward: 0.502 [0.330, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.577, 10.098], loss: 0.009567, mae: 0.106159, mean_q: 19.489182
 42312/100000: episode: 1044, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 48.818, mean reward: 0.488 [0.165, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.283, 10.130], loss: 0.008205, mae: 0.099196, mean_q: 19.553669
 42412/100000: episode: 1045, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 48.840, mean reward: 0.488 [0.305, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.720, 10.261], loss: 0.007880, mae: 0.095616, mean_q: 19.425606
 42512/100000: episode: 1046, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 51.681, mean reward: 0.517 [0.190, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.693, 10.098], loss: 0.009986, mae: 0.110542, mean_q: 19.393881
 42612/100000: episode: 1047, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 53.677, mean reward: 0.537 [0.308, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.098], loss: 0.008737, mae: 0.100078, mean_q: 19.234135
 42712/100000: episode: 1048, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 52.819, mean reward: 0.528 [0.320, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.562, 10.170], loss: 0.007929, mae: 0.096938, mean_q: 19.263105
 42812/100000: episode: 1049, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 54.490, mean reward: 0.545 [0.313, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.250, 10.251], loss: 0.007884, mae: 0.096869, mean_q: 19.740446
 42912/100000: episode: 1050, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 48.942, mean reward: 0.489 [0.263, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.944, 10.098], loss: 0.007912, mae: 0.095771, mean_q: 19.533169
 43012/100000: episode: 1051, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 52.849, mean reward: 0.528 [0.099, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.624, 10.174], loss: 0.009887, mae: 0.109024, mean_q: 19.685154
 43112/100000: episode: 1052, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 50.905, mean reward: 0.509 [0.255, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.402, 10.098], loss: 0.008953, mae: 0.102379, mean_q: 19.410738
 43212/100000: episode: 1053, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 56.678, mean reward: 0.567 [0.339, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.348, 10.208], loss: 0.008516, mae: 0.100831, mean_q: 19.712746
 43312/100000: episode: 1054, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 47.623, mean reward: 0.476 [0.234, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.516, 10.308], loss: 0.010411, mae: 0.112664, mean_q: 19.445406
 43412/100000: episode: 1055, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 53.609, mean reward: 0.536 [0.143, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.925, 10.098], loss: 0.009348, mae: 0.106401, mean_q: 19.387524
 43512/100000: episode: 1056, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 53.265, mean reward: 0.533 [0.349, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.481, 10.098], loss: 0.006925, mae: 0.090649, mean_q: 19.350693
 43612/100000: episode: 1057, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 56.894, mean reward: 0.569 [0.237, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.647, 10.098], loss: 0.009427, mae: 0.106387, mean_q: 19.348715
 43712/100000: episode: 1058, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 53.468, mean reward: 0.535 [0.275, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.098], loss: 0.008813, mae: 0.098658, mean_q: 19.555361
 43812/100000: episode: 1059, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 50.919, mean reward: 0.509 [0.287, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.724, 10.109], loss: 0.008502, mae: 0.098859, mean_q: 19.426720
 43912/100000: episode: 1060, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 54.914, mean reward: 0.549 [0.302, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.522, 10.277], loss: 0.008086, mae: 0.098006, mean_q: 19.557789
 44012/100000: episode: 1061, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 55.567, mean reward: 0.556 [0.309, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.635, 10.212], loss: 0.008903, mae: 0.102607, mean_q: 19.261293
 44112/100000: episode: 1062, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 55.140, mean reward: 0.551 [0.316, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.389, 10.098], loss: 0.008911, mae: 0.099799, mean_q: 19.521357
 44212/100000: episode: 1063, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 54.670, mean reward: 0.547 [0.320, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.460, 10.098], loss: 0.008903, mae: 0.102626, mean_q: 19.459841
 44312/100000: episode: 1064, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 52.407, mean reward: 0.524 [0.266, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.259, 10.182], loss: 0.009449, mae: 0.104594, mean_q: 19.469231
 44412/100000: episode: 1065, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 53.871, mean reward: 0.539 [0.168, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.887, 10.098], loss: 0.008517, mae: 0.101439, mean_q: 19.454945
 44512/100000: episode: 1066, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 50.363, mean reward: 0.504 [0.305, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.155, 10.098], loss: 0.010387, mae: 0.109321, mean_q: 19.317608
 44612/100000: episode: 1067, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 55.380, mean reward: 0.554 [0.355, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.077, 10.312], loss: 0.008183, mae: 0.099661, mean_q: 19.543652
 44712/100000: episode: 1068, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 50.642, mean reward: 0.506 [0.235, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.595, 10.098], loss: 0.007684, mae: 0.095269, mean_q: 19.347420
 44812/100000: episode: 1069, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 56.232, mean reward: 0.562 [0.381, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.198, 10.098], loss: 0.007571, mae: 0.094044, mean_q: 19.759716
 44912/100000: episode: 1070, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 53.636, mean reward: 0.536 [0.328, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.354, 10.197], loss: 0.008519, mae: 0.100932, mean_q: 19.393196
 45012/100000: episode: 1071, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 51.334, mean reward: 0.513 [0.287, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.372, 10.098], loss: 0.008831, mae: 0.098774, mean_q: 19.537083
 45112/100000: episode: 1072, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 48.184, mean reward: 0.482 [0.201, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.010, 10.098], loss: 0.011062, mae: 0.114942, mean_q: 19.372217
 45212/100000: episode: 1073, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 58.608, mean reward: 0.586 [0.340, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.927, 10.191], loss: 0.007553, mae: 0.093422, mean_q: 19.223671
 45312/100000: episode: 1074, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: 53.044, mean reward: 0.530 [0.298, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.778, 10.098], loss: 0.008390, mae: 0.096660, mean_q: 19.332273
 45412/100000: episode: 1075, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 49.621, mean reward: 0.496 [0.207, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.589, 10.430], loss: 0.010192, mae: 0.109217, mean_q: 19.597828
 45512/100000: episode: 1076, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 49.384, mean reward: 0.494 [0.263, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.660, 10.289], loss: 0.007619, mae: 0.093606, mean_q: 19.680992
 45612/100000: episode: 1077, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 53.517, mean reward: 0.535 [0.346, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.814, 10.261], loss: 0.010589, mae: 0.112010, mean_q: 19.493254
 45712/100000: episode: 1078, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.779, mean reward: 0.568 [0.303, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.254, 10.208], loss: 0.011126, mae: 0.111378, mean_q: 19.631231
 45812/100000: episode: 1079, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 56.023, mean reward: 0.560 [0.274, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.651, 10.117], loss: 0.009622, mae: 0.103422, mean_q: 19.316002
[Info] New level: -0.19560033082962036 | Considering 10/90 traces
 45912/100000: episode: 1080, duration: 4.504s, episode steps: 100, steps per second: 22, episode reward: 51.495, mean reward: 0.515 [0.306, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.368, 10.098], loss: 0.008392, mae: 0.098580, mean_q: 19.372774
 45914/100000: episode: 1081, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.974, mean reward: 0.487 [0.428, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.263, 10.100], loss: 0.007070, mae: 0.094970, mean_q: 19.658428
 45916/100000: episode: 1082, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.676, mean reward: 0.338 [0.334, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.398, 10.100], loss: 0.006936, mae: 0.088179, mean_q: 17.792727
 45918/100000: episode: 1083, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.747, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.479, 10.100], loss: 0.004642, mae: 0.079331, mean_q: 21.493828
 45920/100000: episode: 1084, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.635, mean reward: 0.318 [0.313, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.414, 10.100], loss: 0.004113, mae: 0.066999, mean_q: 17.497746
 45922/100000: episode: 1085, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.687, mean reward: 0.344 [0.331, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.322, 10.100], loss: 0.004720, mae: 0.081266, mean_q: 20.014919
 45924/100000: episode: 1086, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.807, mean reward: 0.404 [0.403, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.402, 10.100], loss: 0.004213, mae: 0.075071, mean_q: 17.614868
 45926/100000: episode: 1087, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.917, mean reward: 0.459 [0.429, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.371, 10.100], loss: 0.006784, mae: 0.092479, mean_q: 20.052261
 45928/100000: episode: 1088, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 0.774, mean reward: 0.387 [0.365, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.354, 10.100], loss: 0.006204, mae: 0.087965, mean_q: 19.485180
 45930/100000: episode: 1089, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.707, mean reward: 0.354 [0.336, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.421, 10.100], loss: 0.007896, mae: 0.098747, mean_q: 20.520590
 45932/100000: episode: 1090, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.753, mean reward: 0.377 [0.369, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.468, 10.100], loss: 0.011739, mae: 0.124835, mean_q: 22.096329
 45934/100000: episode: 1091, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.729, mean reward: 0.365 [0.310, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.354, 10.100], loss: 0.015867, mae: 0.145807, mean_q: 20.260798
 45936/100000: episode: 1092, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.368, mean reward: 0.184 [0.180, 0.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.412, 10.100], loss: 0.015559, mae: 0.133160, mean_q: 18.785408
 45938/100000: episode: 1093, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.685, mean reward: 0.343 [0.342, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.321, 10.100], loss: 0.020095, mae: 0.125907, mean_q: 17.465527
 45940/100000: episode: 1094, duration: 0.018s, episode steps: 2, steps per second: 108, episode reward: 0.988, mean reward: 0.494 [0.478, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.315, 10.100], loss: 0.008159, mae: 0.105034, mean_q: 18.067474
 45942/100000: episode: 1095, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.692, mean reward: 0.346 [0.296, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.340, 10.100], loss: 0.007347, mae: 0.089426, mean_q: 21.085701
 45944/100000: episode: 1096, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.627, mean reward: 0.313 [0.307, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.430, 10.100], loss: 0.005739, mae: 0.082384, mean_q: 20.582926
 45946/100000: episode: 1097, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.954, mean reward: 0.477 [0.405, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.287, 10.100], loss: 0.006212, mae: 0.085095, mean_q: 19.561371
 45948/100000: episode: 1098, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.892, mean reward: 0.446 [0.437, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.254, 10.100], loss: 0.018561, mae: 0.119187, mean_q: 18.705025
 45950/100000: episode: 1099, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.840, mean reward: 0.420 [0.375, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.247, 10.100], loss: 0.008535, mae: 0.094828, mean_q: 17.708866
 45952/100000: episode: 1100, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.184, mean reward: 0.592 [0.584, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.297, 10.100], loss: 0.007940, mae: 0.090174, mean_q: 18.084827
 45954/100000: episode: 1101, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.965, mean reward: 0.482 [0.415, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.341, 10.100], loss: 0.008268, mae: 0.091272, mean_q: 17.965569
 45957/100000: episode: 1102, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 0.806, mean reward: 0.269 [0.240, 0.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.478, 10.100], loss: 0.008079, mae: 0.105154, mean_q: 18.461615
 45959/100000: episode: 1103, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.801, mean reward: 0.400 [0.390, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.363, 10.100], loss: 0.005450, mae: 0.081810, mean_q: 15.545793
 45961/100000: episode: 1104, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.809, mean reward: 0.404 [0.392, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.340, 10.100], loss: 0.009262, mae: 0.093046, mean_q: 20.946127
 45963/100000: episode: 1105, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.902, mean reward: 0.451 [0.428, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.302, 10.100], loss: 0.007317, mae: 0.099197, mean_q: 19.614265
 45966/100000: episode: 1106, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 0.391, mean reward: 0.130 [0.111, 0.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.567, 10.100], loss: 0.007195, mae: 0.091380, mean_q: 18.150839
 45968/100000: episode: 1107, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.891, mean reward: 0.446 [0.410, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.303, 10.100], loss: 0.009379, mae: 0.102690, mean_q: 19.471596
 45971/100000: episode: 1108, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 0.788, mean reward: 0.263 [0.255, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.465, 10.100], loss: 0.006448, mae: 0.087078, mean_q: 18.896193
 45973/100000: episode: 1109, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.798, mean reward: 0.399 [0.380, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.413, 10.100], loss: 0.007592, mae: 0.088951, mean_q: 22.461983
 45976/100000: episode: 1110, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 0.571, mean reward: 0.190 [0.150, 0.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.561, 10.100], loss: 0.005983, mae: 0.083575, mean_q: 19.251411
 45978/100000: episode: 1111, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.685, mean reward: 0.343 [0.342, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.365, 10.100], loss: 0.022169, mae: 0.115748, mean_q: 19.443783
 45980/100000: episode: 1112, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.624, mean reward: 0.312 [0.274, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.044, 10.100], loss: 0.011862, mae: 0.108359, mean_q: 18.377544
 45982/100000: episode: 1113, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.630, mean reward: 0.315 [0.281, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.319, 10.100], loss: 0.007417, mae: 0.093318, mean_q: 18.140244
 45985/100000: episode: 1114, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 0.453, mean reward: 0.151 [0.094, 0.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.484, 10.100], loss: 0.009774, mae: 0.099736, mean_q: 17.465830
 45987/100000: episode: 1115, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.689, mean reward: 0.344 [0.323, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.362, 10.100], loss: 0.006005, mae: 0.086199, mean_q: 21.067909
 45989/100000: episode: 1116, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.366, mean reward: 0.183 [0.168, 0.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.405, 10.100], loss: 0.013169, mae: 0.111731, mean_q: 20.962601
 45991/100000: episode: 1117, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.685, mean reward: 0.342 [0.332, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.398, 10.100], loss: 0.011825, mae: 0.112467, mean_q: 18.385761
 45993/100000: episode: 1118, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.318, mean reward: 0.159 [0.158, 0.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.416, 10.100], loss: 0.009805, mae: 0.093539, mean_q: 20.127064
 45996/100000: episode: 1119, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 0.713, mean reward: 0.238 [0.219, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.534, 10.100], loss: 0.010976, mae: 0.124079, mean_q: 17.922075
 45998/100000: episode: 1120, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.330, mean reward: 0.165 [0.152, 0.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.782, 10.100], loss: 0.008365, mae: 0.099635, mean_q: 19.965752
 46000/100000: episode: 1121, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.118, mean reward: 0.559 [0.553, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.307, 10.100], loss: 0.019536, mae: 0.149647, mean_q: 16.489204
 46002/100000: episode: 1122, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.476, mean reward: 0.238 [0.132, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.401, 10.100], loss: 0.013540, mae: 0.130404, mean_q: 17.267147
 46004/100000: episode: 1123, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 1.076, mean reward: 0.538 [0.514, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.269, 10.100], loss: 0.009978, mae: 0.117451, mean_q: 20.137154
 46006/100000: episode: 1124, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.015, mean reward: 0.508 [0.460, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.246, 10.100], loss: 0.007093, mae: 0.095693, mean_q: 19.951065
 46008/100000: episode: 1125, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.075, mean reward: 0.538 [0.531, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.293, 10.100], loss: 0.012528, mae: 0.125678, mean_q: 19.053570
 46010/100000: episode: 1126, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.028, mean reward: 0.514 [0.427, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.197, 10.100], loss: 0.006528, mae: 0.092700, mean_q: 18.745722
 46012/100000: episode: 1127, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.722, mean reward: 0.361 [0.358, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.415, 10.100], loss: 0.008024, mae: 0.099619, mean_q: 19.393385
 46014/100000: episode: 1128, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.665, mean reward: 0.332 [0.283, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.423, 10.100], loss: 0.012155, mae: 0.123111, mean_q: 19.623217
 46017/100000: episode: 1129, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 0.694, mean reward: 0.231 [0.217, 0.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.431, 10.100], loss: 0.010921, mae: 0.107342, mean_q: 17.635761
 46019/100000: episode: 1130, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.884, mean reward: 0.442 [0.433, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.319, 10.100], loss: 0.018949, mae: 0.122842, mean_q: 18.361605
 46021/100000: episode: 1131, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 1.132, mean reward: 0.566 [0.549, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.267, 10.100], loss: 0.009511, mae: 0.104612, mean_q: 19.361219
 46023/100000: episode: 1132, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.661, mean reward: 0.330 [0.325, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.432, 10.100], loss: 0.009627, mae: 0.115781, mean_q: 19.836163
 46025/100000: episode: 1133, duration: 0.017s, episode steps: 2, steps per second: 114, episode reward: 0.911, mean reward: 0.455 [0.368, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.243, 10.100], loss: 0.013015, mae: 0.096774, mean_q: 19.578985
 46027/100000: episode: 1134, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.906, mean reward: 0.453 [0.444, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.298, 10.100], loss: 0.012997, mae: 0.131010, mean_q: 19.422935
 46029/100000: episode: 1135, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.102, mean reward: 0.551 [0.549, 0.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.332, 10.100], loss: 0.011623, mae: 0.103005, mean_q: 17.062820
 46031/100000: episode: 1136, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.577, mean reward: 0.289 [0.258, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.284, 10.100], loss: 0.011501, mae: 0.100212, mean_q: 19.855362
 46033/100000: episode: 1137, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.786, mean reward: 0.393 [0.367, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.701, 10.100], loss: 0.006626, mae: 0.089654, mean_q: 17.925318
 46035/100000: episode: 1138, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.704, mean reward: 0.352 [0.347, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.426, 10.100], loss: 0.010448, mae: 0.103119, mean_q: 18.514828
 46037/100000: episode: 1139, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.362, mean reward: 0.181 [0.170, 0.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.400, 10.100], loss: 0.007285, mae: 0.099204, mean_q: 17.792406
 46039/100000: episode: 1140, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.900, mean reward: 0.450 [0.426, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.431, 10.100], loss: 0.012868, mae: 0.105669, mean_q: 17.970505
 46041/100000: episode: 1141, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.740, mean reward: 0.370 [0.364, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.408, 10.100], loss: 0.009371, mae: 0.109578, mean_q: 20.382996
 46043/100000: episode: 1142, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.751, mean reward: 0.376 [0.307, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.346, 10.100], loss: 0.008654, mae: 0.098186, mean_q: 17.939396
 46045/100000: episode: 1143, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.976, mean reward: 0.488 [0.471, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.333, 10.100], loss: 0.011607, mae: 0.107398, mean_q: 20.005806
 46047/100000: episode: 1144, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.867, mean reward: 0.433 [0.431, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.851, 10.100], loss: 0.010451, mae: 0.116058, mean_q: 18.075623
 46049/100000: episode: 1145, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.941, mean reward: 0.471 [0.365, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.590, 10.100], loss: 0.016389, mae: 0.113932, mean_q: 17.921885
 46051/100000: episode: 1146, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.781, mean reward: 0.391 [0.336, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.199, 10.100], loss: 0.008096, mae: 0.089827, mean_q: 18.573526
 46054/100000: episode: 1147, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 0.489, mean reward: 0.163 [0.158, 0.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.494, 10.100], loss: 0.006346, mae: 0.088304, mean_q: 18.944532
 46056/100000: episode: 1148, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.782, mean reward: 0.391 [0.369, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.383, 10.100], loss: 0.010092, mae: 0.103246, mean_q: 18.655476
 46058/100000: episode: 1149, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.719, mean reward: 0.359 [0.356, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.365, 10.100], loss: 0.008876, mae: 0.106033, mean_q: 18.218977
 46061/100000: episode: 1150, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 0.568, mean reward: 0.189 [0.170, 0.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.486, 10.100], loss: 0.009074, mae: 0.103631, mean_q: 18.806292
 46064/100000: episode: 1151, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 0.553, mean reward: 0.184 [0.165, 0.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.570, 10.100], loss: 0.008691, mae: 0.093270, mean_q: 21.174833
 46067/100000: episode: 1152, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 0.527, mean reward: 0.176 [0.158, 0.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.523, 10.100], loss: 0.009885, mae: 0.108485, mean_q: 19.355490
 46069/100000: episode: 1153, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.305, mean reward: 0.152 [0.144, 0.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.428, 10.100], loss: 0.013091, mae: 0.124655, mean_q: 19.135832
 46071/100000: episode: 1154, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.698, mean reward: 0.349 [0.292, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.262, 10.100], loss: 0.008885, mae: 0.104810, mean_q: 19.022743
 46073/100000: episode: 1155, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.968, mean reward: 0.484 [0.482, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.291, 10.100], loss: 0.026918, mae: 0.144362, mean_q: 16.729012
 46075/100000: episode: 1156, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.751, mean reward: 0.375 [0.366, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.259, 10.100], loss: 0.013909, mae: 0.099439, mean_q: 19.056946
 46078/100000: episode: 1157, duration: 0.025s, episode steps: 3, steps per second: 122, episode reward: 0.571, mean reward: 0.190 [0.158, 0.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.513, 10.100], loss: 0.007690, mae: 0.097925, mean_q: 17.304842
 46080/100000: episode: 1158, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.780, mean reward: 0.390 [0.350, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.276, 10.100], loss: 0.009053, mae: 0.109099, mean_q: 20.252625
 46083/100000: episode: 1159, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 0.623, mean reward: 0.208 [0.194, 0.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.602, 10.100], loss: 0.011206, mae: 0.119457, mean_q: 17.133169
 46085/100000: episode: 1160, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.864, mean reward: 0.432 [0.358, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.304, 10.100], loss: 0.008315, mae: 0.100351, mean_q: 19.474949
 46087/100000: episode: 1161, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.769, mean reward: 0.385 [0.364, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.389, 10.100], loss: 0.011747, mae: 0.116619, mean_q: 19.523170
 46089/100000: episode: 1162, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 0.875, mean reward: 0.437 [0.435, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.229, 10.100], loss: 0.010817, mae: 0.117341, mean_q: 17.351774
 46091/100000: episode: 1163, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.898, mean reward: 0.449 [0.394, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.257, 10.100], loss: 0.011785, mae: 0.119956, mean_q: 20.071383
 46093/100000: episode: 1164, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.420, mean reward: 0.210 [0.205, 0.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.415, 10.100], loss: 0.011569, mae: 0.115607, mean_q: 18.711363
 46095/100000: episode: 1165, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.772, mean reward: 0.386 [0.346, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.370, 10.100], loss: 0.008821, mae: 0.105340, mean_q: 16.687523
 46097/100000: episode: 1166, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.736, mean reward: 0.368 [0.344, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.437, 10.100], loss: 0.009818, mae: 0.107944, mean_q: 20.482620
 46099/100000: episode: 1167, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.359, mean reward: 0.180 [0.175, 0.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.469, 10.100], loss: 0.009292, mae: 0.103780, mean_q: 18.414709
 46102/100000: episode: 1168, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 0.423, mean reward: 0.141 [0.112, 0.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.577, 10.100], loss: 0.008979, mae: 0.105342, mean_q: 17.461252
 46104/100000: episode: 1169, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.408, mean reward: 0.204 [0.191, 0.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.425, 10.100], loss: 0.009803, mae: 0.106892, mean_q: 19.052553
[Info] New level: -0.5548295974731445 | Considering 10/90 traces
 46106/100000: episode: 1170, duration: 4.024s, episode steps: 2, steps per second: 0, episode reward: 0.813, mean reward: 0.407 [0.398, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.308, 10.100], loss: 0.009220, mae: 0.107938, mean_q: 18.916777
 46107/100000: episode: 1171, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.290, mean reward: 0.290 [0.290, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.456, 10.100], loss: 0.011145, mae: 0.102143, mean_q: 16.560997
 46108/100000: episode: 1172, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.151, mean reward: 0.151 [0.151, 0.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.523, 10.100], loss: 0.027111, mae: 0.164740, mean_q: 14.377865
 46109/100000: episode: 1173, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.297, mean reward: 0.297 [0.297, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.402, 10.100], loss: 0.008372, mae: 0.111231, mean_q: 20.093744
 46110/100000: episode: 1174, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.201, mean reward: 0.201 [0.201, 0.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.555, 10.100], loss: 0.008270, mae: 0.098853, mean_q: 16.258146
 46111/100000: episode: 1175, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.222, mean reward: 0.222 [0.222, 0.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.492, 10.100], loss: 0.007657, mae: 0.104613, mean_q: 19.695333
 46112/100000: episode: 1176, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.218, mean reward: 0.218 [0.218, 0.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.451, 10.100], loss: 0.013257, mae: 0.119019, mean_q: 16.394419
 46113/100000: episode: 1177, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.182, mean reward: 0.182 [0.182, 0.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.534, 10.100], loss: 0.012995, mae: 0.112450, mean_q: 20.996786
 46114/100000: episode: 1178, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.204, mean reward: 0.204 [0.204, 0.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.494, 10.100], loss: 0.007769, mae: 0.100604, mean_q: 17.298882
 46115/100000: episode: 1179, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.186, mean reward: 0.186 [0.186, 0.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.496, 10.100], loss: 0.009474, mae: 0.102605, mean_q: 18.898748
 46116/100000: episode: 1180, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.149, mean reward: 0.149 [0.149, 0.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.532, 10.100], loss: 0.010303, mae: 0.106036, mean_q: 16.523193
 46117/100000: episode: 1181, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.186, mean reward: 0.186 [0.186, 0.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.464, 10.100], loss: 0.011474, mae: 0.117329, mean_q: 18.175423
 46118/100000: episode: 1182, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.263, mean reward: 0.263 [0.263, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.517, 10.100], loss: 0.006993, mae: 0.100914, mean_q: 19.530380
 46119/100000: episode: 1183, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.128, mean reward: 0.128 [0.128, 0.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.483, 10.100], loss: 0.010975, mae: 0.106983, mean_q: 18.423948
 46120/100000: episode: 1184, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.204, mean reward: 0.204 [0.204, 0.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.516, 10.100], loss: 0.007680, mae: 0.092087, mean_q: 19.445902
 46121/100000: episode: 1185, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.277, mean reward: 0.277 [0.277, 0.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.525, 10.100], loss: 0.007979, mae: 0.100082, mean_q: 22.231932
 46122/100000: episode: 1186, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.161, mean reward: 0.161 [0.161, 0.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.474, 10.100], loss: 0.006766, mae: 0.089685, mean_q: 19.804340
 46123/100000: episode: 1187, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.146, mean reward: 0.146 [0.146, 0.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.451, 10.100], loss: 0.007877, mae: 0.098889, mean_q: 18.951099
 46124/100000: episode: 1188, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.330, mean reward: 0.330 [0.330, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.417, 10.100], loss: 0.018348, mae: 0.134498, mean_q: 18.844778
 46125/100000: episode: 1189, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.383, mean reward: 0.383 [0.383, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.404, 10.100], loss: 0.010405, mae: 0.103234, mean_q: 18.140991
 46126/100000: episode: 1190, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.120, mean reward: 0.120 [0.120, 0.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.517, 10.100], loss: 0.018088, mae: 0.157792, mean_q: 16.103985
 46127/100000: episode: 1191, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.156, mean reward: 0.156 [0.156, 0.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.526, 10.100], loss: 0.017068, mae: 0.135668, mean_q: 17.655233
 46128/100000: episode: 1192, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.226, mean reward: 0.226 [0.226, 0.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.465, 10.100], loss: 0.009089, mae: 0.097771, mean_q: 16.647121
 46129/100000: episode: 1193, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.205, mean reward: 0.205 [0.205, 0.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.564, 10.100], loss: 0.011985, mae: 0.118250, mean_q: 18.110823
 46130/100000: episode: 1194, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.194, mean reward: 0.194 [0.194, 0.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.541, 10.100], loss: 0.017052, mae: 0.138436, mean_q: 17.565563
 46131/100000: episode: 1195, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.320, mean reward: 0.320 [0.320, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.423, 10.100], loss: 0.009779, mae: 0.119858, mean_q: 17.818314
 46132/100000: episode: 1196, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.190, mean reward: 0.190 [0.190, 0.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.550, 10.100], loss: 0.016351, mae: 0.111559, mean_q: 19.655779
 46133/100000: episode: 1197, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.208, mean reward: 0.208 [0.208, 0.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.460, 10.100], loss: 0.010489, mae: 0.101001, mean_q: 17.044312
 46134/100000: episode: 1198, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.194, mean reward: 0.194 [0.194, 0.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.614, 10.100], loss: 0.007746, mae: 0.098460, mean_q: 14.967154
 46135/100000: episode: 1199, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.259, mean reward: 0.259 [0.259, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.549, 10.100], loss: 0.020862, mae: 0.123642, mean_q: 15.315728
 46136/100000: episode: 1200, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.164, mean reward: 0.164 [0.164, 0.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.500, 10.100], loss: 0.008449, mae: 0.112904, mean_q: 19.266201
 46137/100000: episode: 1201, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.222, mean reward: 0.222 [0.222, 0.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.575, 10.100], loss: 0.006543, mae: 0.095719, mean_q: 20.670044
 46138/100000: episode: 1202, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.120, mean reward: 0.120 [0.120, 0.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.545, 10.100], loss: 0.008252, mae: 0.101107, mean_q: 17.303196
 46139/100000: episode: 1203, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.149, mean reward: 0.149 [0.149, 0.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.484, 10.100], loss: 0.007987, mae: 0.098213, mean_q: 16.988560
 46140/100000: episode: 1204, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.200, mean reward: 0.200 [0.200, 0.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.476, 10.100], loss: 0.009914, mae: 0.102641, mean_q: 19.287640
 46141/100000: episode: 1205, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.219, mean reward: 0.219 [0.219, 0.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.597, 10.100], loss: 0.009126, mae: 0.107973, mean_q: 18.652969
 46142/100000: episode: 1206, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.521, 10.100], loss: 0.005498, mae: 0.075694, mean_q: 17.156734
 46143/100000: episode: 1207, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.313, mean reward: 0.313 [0.313, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.466, 10.100], loss: 0.010105, mae: 0.099621, mean_q: 19.433392
 46144/100000: episode: 1208, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.209, mean reward: 0.209 [0.209, 0.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.499, 10.100], loss: 0.009914, mae: 0.121708, mean_q: 18.512211
 46145/100000: episode: 1209, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.143, mean reward: 0.143 [0.143, 0.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.821, 10.100], loss: 0.011090, mae: 0.114635, mean_q: 17.479286
 46146/100000: episode: 1210, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.192, mean reward: 0.192 [0.192, 0.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.398, 10.100], loss: 0.011401, mae: 0.128640, mean_q: 16.080963
 46147/100000: episode: 1211, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.193, mean reward: 0.193 [0.193, 0.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.586, 10.100], loss: 0.013225, mae: 0.128852, mean_q: 18.834126
 46148/100000: episode: 1212, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.431, mean reward: 0.431 [0.431, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.401, 10.100], loss: 0.011622, mae: 0.125560, mean_q: 18.866196
 46149/100000: episode: 1213, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.262, mean reward: 0.262 [0.262, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.512, 10.100], loss: 0.011388, mae: 0.113219, mean_q: 18.098820
 46150/100000: episode: 1214, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.164, mean reward: 0.164 [0.164, 0.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.522, 10.100], loss: 0.006665, mae: 0.086689, mean_q: 19.008270
 46151/100000: episode: 1215, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.185, mean reward: 0.185 [0.185, 0.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.548, 10.100], loss: 0.006802, mae: 0.090774, mean_q: 18.389256
 46152/100000: episode: 1216, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.248, mean reward: 0.248 [0.248, 0.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.475, 10.100], loss: 0.005135, mae: 0.081293, mean_q: 21.592268
 46153/100000: episode: 1217, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.349, mean reward: 0.349 [0.349, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.431, 10.100], loss: 0.015610, mae: 0.137920, mean_q: 20.689495
 46154/100000: episode: 1218, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.142, mean reward: 0.142 [0.142, 0.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.647, 10.100], loss: 0.010076, mae: 0.091329, mean_q: 20.158150
 46155/100000: episode: 1219, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.160, mean reward: 0.160 [0.160, 0.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.554, 10.100], loss: 0.005208, mae: 0.085771, mean_q: 20.008108
 46156/100000: episode: 1220, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.157, mean reward: 0.157 [0.157, 0.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.563, 10.100], loss: 0.007461, mae: 0.089374, mean_q: 21.042244
 46157/100000: episode: 1221, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.207, mean reward: 0.207 [0.207, 0.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.479, 10.100], loss: 0.006987, mae: 0.090750, mean_q: 17.472042
 46158/100000: episode: 1222, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.185, mean reward: 0.185 [0.185, 0.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.580, 10.100], loss: 0.016582, mae: 0.129838, mean_q: 15.253846
 46159/100000: episode: 1223, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.235, mean reward: 0.235 [0.235, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.564, 10.100], loss: 0.027307, mae: 0.144486, mean_q: 18.704559
 46160/100000: episode: 1224, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.041, mean reward: 0.041 [0.041, 0.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.517, 10.100], loss: 0.019497, mae: 0.140613, mean_q: 17.775085
 46161/100000: episode: 1225, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.185, mean reward: 0.185 [0.185, 0.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.393, 10.100], loss: 0.014556, mae: 0.120629, mean_q: 19.101818
 46162/100000: episode: 1226, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.269, mean reward: 0.269 [0.269, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.583, 10.100], loss: 0.008411, mae: 0.109655, mean_q: 16.009750
 46163/100000: episode: 1227, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.193, mean reward: 0.193 [0.193, 0.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.512, 10.100], loss: 0.014969, mae: 0.143597, mean_q: 17.147015
 46164/100000: episode: 1228, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.278, mean reward: 0.278 [0.278, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.510, 10.100], loss: 0.006095, mae: 0.083277, mean_q: 17.660519
 46165/100000: episode: 1229, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.253, mean reward: 0.253 [0.253, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.376, 10.100], loss: 0.006985, mae: 0.089948, mean_q: 17.255238
 46166/100000: episode: 1230, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.202, mean reward: 0.202 [0.202, 0.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.519, 10.100], loss: 0.010417, mae: 0.123861, mean_q: 20.161297
 46167/100000: episode: 1231, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.164, mean reward: 0.164 [0.164, 0.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.472, 10.100], loss: 0.005513, mae: 0.079290, mean_q: 16.476601
 46168/100000: episode: 1232, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.210, mean reward: 0.210 [0.210, 0.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.462, 10.100], loss: 0.007547, mae: 0.093065, mean_q: 16.437681
 46169/100000: episode: 1233, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.309, mean reward: 0.309 [0.309, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.438, 10.100], loss: 0.010203, mae: 0.107566, mean_q: 19.645840
 46170/100000: episode: 1234, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.399, 10.100], loss: 0.006090, mae: 0.091958, mean_q: 17.560225
 46171/100000: episode: 1235, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.223, mean reward: 0.223 [0.223, 0.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.622, 10.100], loss: 0.008522, mae: 0.097429, mean_q: 20.921867
 46172/100000: episode: 1236, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.326, mean reward: 0.326 [0.326, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.448, 10.100], loss: 0.006925, mae: 0.098987, mean_q: 19.934769
 46173/100000: episode: 1237, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.232, mean reward: 0.232 [0.232, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.483, 10.100], loss: 0.017691, mae: 0.136398, mean_q: 20.560631
 46174/100000: episode: 1238, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.167, mean reward: 0.167 [0.167, 0.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.460, 10.100], loss: 0.006988, mae: 0.098724, mean_q: 17.773663
 46175/100000: episode: 1239, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.438, 10.100], loss: 0.023747, mae: 0.125739, mean_q: 16.873203
 46176/100000: episode: 1240, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.162, mean reward: 0.162 [0.162, 0.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.475, 10.100], loss: 0.008607, mae: 0.102564, mean_q: 17.660549
 46177/100000: episode: 1241, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.185, mean reward: 0.185 [0.185, 0.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.636, 10.100], loss: 0.007723, mae: 0.095433, mean_q: 16.921696
 46178/100000: episode: 1242, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.149, mean reward: 0.149 [0.149, 0.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.693, 10.100], loss: 0.010231, mae: 0.122293, mean_q: 19.456999
 46179/100000: episode: 1243, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.318, mean reward: 0.318 [0.318, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.437, 10.100], loss: 0.017472, mae: 0.127797, mean_q: 16.687534
 46180/100000: episode: 1244, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.294, mean reward: 0.294 [0.294, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.512, 10.100], loss: 0.009077, mae: 0.104717, mean_q: 16.603462
 46181/100000: episode: 1245, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.082, mean reward: 0.082 [0.082, 0.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.515, 10.100], loss: 0.006845, mae: 0.094595, mean_q: 22.996067
 46182/100000: episode: 1246, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.255, mean reward: 0.255 [0.255, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.470, 10.100], loss: 0.013066, mae: 0.104392, mean_q: 19.836323
 46183/100000: episode: 1247, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.095, mean reward: 0.095 [0.095, 0.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.803, 10.100], loss: 0.008416, mae: 0.100765, mean_q: 18.517607
 46184/100000: episode: 1248, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.767, 10.100], loss: 0.008664, mae: 0.114449, mean_q: 18.080734
 46185/100000: episode: 1249, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.034, mean reward: 0.034 [0.034, 0.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.501, 10.100], loss: 0.010450, mae: 0.107440, mean_q: 21.742535
 46186/100000: episode: 1250, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.258, mean reward: 0.258 [0.258, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.484, 10.100], loss: 0.010450, mae: 0.113019, mean_q: 18.065697
 46187/100000: episode: 1251, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.098, mean reward: 0.098 [0.098, 0.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.491, 10.100], loss: 0.006920, mae: 0.094767, mean_q: 15.443447
 46188/100000: episode: 1252, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.189, mean reward: 0.189 [0.189, 0.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.522, 10.100], loss: 0.007590, mae: 0.094562, mean_q: 18.940327
 46189/100000: episode: 1253, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.170, mean reward: 0.170 [0.170, 0.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.454, 10.100], loss: 0.009591, mae: 0.120020, mean_q: 19.588446
 46190/100000: episode: 1254, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.214, mean reward: 0.214 [0.214, 0.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.431, 10.100], loss: 0.012147, mae: 0.120119, mean_q: 14.833521
 46191/100000: episode: 1255, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.262, mean reward: 0.262 [0.262, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.472, 10.100], loss: 0.006470, mae: 0.093453, mean_q: 18.032001
 46192/100000: episode: 1256, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.128, mean reward: 0.128 [0.128, 0.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.559, 10.100], loss: 0.005388, mae: 0.087483, mean_q: 18.417633
 46193/100000: episode: 1257, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.207, mean reward: 0.207 [0.207, 0.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.426, 10.100], loss: 0.008738, mae: 0.104765, mean_q: 16.817924
 46194/100000: episode: 1258, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.223, mean reward: 0.223 [0.223, 0.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.596, 10.100], loss: 0.009863, mae: 0.117393, mean_q: 17.431515
 46195/100000: episode: 1259, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.239, mean reward: 0.239 [0.239, 0.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.486, 10.100], loss: 0.009440, mae: 0.094225, mean_q: 18.151804
[Info] New level: -0.596084713935852 | Considering 10/90 traces
 46196/100000: episode: 1260, duration: 4.025s, episode steps: 1, steps per second: 0, episode reward: 0.139, mean reward: 0.139 [0.139, 0.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.519, 10.100], loss: 0.006619, mae: 0.094904, mean_q: 16.564770
 46197/100000: episode: 1261, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.218, mean reward: 0.218 [0.218, 0.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.560, 10.100], loss: 0.007601, mae: 0.093229, mean_q: 19.409359
 46198/100000: episode: 1262, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.249, mean reward: 0.249 [0.249, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.568, 10.100], loss: 0.005561, mae: 0.081698, mean_q: 18.758970
 46199/100000: episode: 1263, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.125, mean reward: 0.125 [0.125, 0.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.572, 10.100], loss: 0.014486, mae: 0.110899, mean_q: 20.809658
 46200/100000: episode: 1264, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.195, mean reward: 0.195 [0.195, 0.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.596, 10.100], loss: 0.019236, mae: 0.146197, mean_q: 18.347290
 46201/100000: episode: 1265, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.268, mean reward: 0.268 [0.268, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.438, 10.100], loss: 0.007057, mae: 0.093572, mean_q: 17.161152
 46202/100000: episode: 1266, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.143, mean reward: 0.143 [0.143, 0.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.593, 10.100], loss: 0.005872, mae: 0.082674, mean_q: 18.252796
 46203/100000: episode: 1267, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.239, mean reward: 0.239 [0.239, 0.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.442, 10.100], loss: 0.011153, mae: 0.104854, mean_q: 16.279568
 46204/100000: episode: 1268, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.213, mean reward: 0.213 [0.213, 0.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.505, 10.100], loss: 0.008376, mae: 0.096326, mean_q: 17.298077
 46205/100000: episode: 1269, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 0.389, mean reward: 0.389 [0.389, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.418, 10.100], loss: 0.006568, mae: 0.096321, mean_q: 19.190153
 46206/100000: episode: 1270, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.173, mean reward: 0.173 [0.173, 0.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.542, 10.100], loss: 0.011014, mae: 0.107564, mean_q: 18.590748
 46207/100000: episode: 1271, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.328, mean reward: 0.328 [0.328, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.434, 10.100], loss: 0.005840, mae: 0.086866, mean_q: 17.143833
 46208/100000: episode: 1272, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.047, mean reward: 0.047 [0.047, 0.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.552, 10.100], loss: 0.006647, mae: 0.091330, mean_q: 18.684183
 46209/100000: episode: 1273, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.096, mean reward: 0.096 [0.096, 0.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.527, 10.100], loss: 0.012453, mae: 0.117711, mean_q: 18.358498
 46210/100000: episode: 1274, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.321, mean reward: 0.321 [0.321, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.336, 10.100], loss: 0.008458, mae: 0.099129, mean_q: 19.042152
 46211/100000: episode: 1275, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.179, mean reward: 0.179 [0.179, 0.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.627, 10.100], loss: 0.005999, mae: 0.085275, mean_q: 17.605082
 46212/100000: episode: 1276, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.204, mean reward: 0.204 [0.204, 0.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.601, 10.100], loss: 0.004856, mae: 0.082504, mean_q: 18.309368
 46213/100000: episode: 1277, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.166, mean reward: 0.166 [0.166, 0.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.568, 10.100], loss: 0.008938, mae: 0.108749, mean_q: 17.756304
 46214/100000: episode: 1278, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.207, mean reward: 0.207 [0.207, 0.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.050, 10.100], loss: 0.005862, mae: 0.082549, mean_q: 20.523314
 46215/100000: episode: 1279, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.156, mean reward: 0.156 [0.156, 0.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.520, 10.100], loss: 0.006228, mae: 0.088451, mean_q: 20.317038
 46216/100000: episode: 1280, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.237, mean reward: 0.237 [0.237, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.575, 10.100], loss: 0.007629, mae: 0.099819, mean_q: 16.582191
 46217/100000: episode: 1281, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.230, mean reward: 0.230 [0.230, 0.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.585, 10.100], loss: 0.010421, mae: 0.130129, mean_q: 17.749123
 46218/100000: episode: 1282, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.178, mean reward: 0.178 [0.178, 0.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.484, 10.100], loss: 0.005913, mae: 0.087961, mean_q: 17.941525
 46219/100000: episode: 1283, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.122, mean reward: 0.122 [0.122, 0.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.516, 10.100], loss: 0.008301, mae: 0.085976, mean_q: 19.803761
 46220/100000: episode: 1284, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.155, mean reward: 0.155 [0.155, 0.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.506, 10.100], loss: 0.010570, mae: 0.107773, mean_q: 15.880022
 46221/100000: episode: 1285, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.220, mean reward: 0.220 [0.220, 0.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.568, 10.100], loss: 0.010416, mae: 0.124812, mean_q: 13.590712
 46222/100000: episode: 1286, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.144, mean reward: 0.144 [0.144, 0.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.525, 10.100], loss: 0.021085, mae: 0.169774, mean_q: 18.285219
 46223/100000: episode: 1287, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.228, mean reward: 0.228 [0.228, 0.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.525, 10.100], loss: 0.013656, mae: 0.139220, mean_q: 19.689051
 46224/100000: episode: 1288, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.165, mean reward: 0.165 [0.165, 0.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.452, 10.100], loss: 0.008918, mae: 0.093366, mean_q: 16.512674
 46225/100000: episode: 1289, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.193, mean reward: 0.193 [0.193, 0.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.557, 10.100], loss: 0.015532, mae: 0.109204, mean_q: 18.503866
 46226/100000: episode: 1290, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.197, mean reward: 0.197 [0.197, 0.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.473, 10.100], loss: 0.015580, mae: 0.154330, mean_q: 19.220533
 46227/100000: episode: 1291, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.231, mean reward: 0.231 [0.231, 0.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.506, 10.100], loss: 0.034028, mae: 0.201996, mean_q: 19.582201
 46228/100000: episode: 1292, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.332, mean reward: 0.332 [0.332, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.397, 10.100], loss: 0.016428, mae: 0.145647, mean_q: 19.976528
 46229/100000: episode: 1293, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.340, mean reward: 0.340 [0.340, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.443, 10.100], loss: 0.004592, mae: 0.078464, mean_q: 21.168007
 46230/100000: episode: 1294, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.182, mean reward: 0.182 [0.182, 0.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.509, 10.100], loss: 0.008950, mae: 0.109021, mean_q: 18.164091
 46231/100000: episode: 1295, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.212, mean reward: 0.212 [0.212, 0.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.544, 10.100], loss: 0.012378, mae: 0.129875, mean_q: 18.332506
 46232/100000: episode: 1296, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.189, mean reward: 0.189 [0.189, 0.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.508, 10.100], loss: 0.011605, mae: 0.110385, mean_q: 19.020475
 46233/100000: episode: 1297, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.199, mean reward: 0.199 [0.199, 0.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.509, 10.100], loss: 0.007698, mae: 0.101494, mean_q: 17.905861
 46234/100000: episode: 1298, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.221, mean reward: 0.221 [0.221, 0.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.500, 10.100], loss: 0.019532, mae: 0.134569, mean_q: 16.881042
 46235/100000: episode: 1299, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.181, mean reward: 0.181 [0.181, 0.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.609, 10.100], loss: 0.013608, mae: 0.135465, mean_q: 15.507170
 46236/100000: episode: 1300, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.116, mean reward: 0.116 [0.116, 0.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.505, 10.100], loss: 0.008832, mae: 0.115814, mean_q: 19.104258
 46237/100000: episode: 1301, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.175, mean reward: 0.175 [0.175, 0.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.503, 10.100], loss: 0.015484, mae: 0.108662, mean_q: 17.069836
 46238/100000: episode: 1302, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.208, mean reward: 0.208 [0.208, 0.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.480, 10.100], loss: 0.021683, mae: 0.130405, mean_q: 19.497826
 46239/100000: episode: 1303, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.399, 10.100], loss: 0.009089, mae: 0.111586, mean_q: 17.421076
 46240/100000: episode: 1304, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.265, mean reward: 0.265 [0.265, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.501, 10.100], loss: 0.009495, mae: 0.103145, mean_q: 17.995552
 46241/100000: episode: 1305, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.131, mean reward: 0.131 [0.131, 0.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.510, 10.100], loss: 0.021784, mae: 0.111796, mean_q: 16.816631
 46242/100000: episode: 1306, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.229, mean reward: 0.229 [0.229, 0.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.477, 10.100], loss: 0.010549, mae: 0.107148, mean_q: 19.075359
 46243/100000: episode: 1307, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.341, mean reward: 0.341 [0.341, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.415, 10.100], loss: 0.004591, mae: 0.076457, mean_q: 19.688236
 46244/100000: episode: 1308, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.167, mean reward: 0.167 [0.167, 0.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.532, 10.100], loss: 0.007063, mae: 0.089988, mean_q: 18.019070
 46245/100000: episode: 1309, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.206, mean reward: 0.206 [0.206, 0.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.541, 10.100], loss: 0.010939, mae: 0.115984, mean_q: 19.617189
 46246/100000: episode: 1310, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.251, mean reward: 0.251 [0.251, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.463, 10.100], loss: 0.022313, mae: 0.187133, mean_q: 20.390049
 46247/100000: episode: 1311, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.217, mean reward: 0.217 [0.217, 0.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.528, 10.100], loss: 0.021197, mae: 0.162817, mean_q: 20.359621
 46248/100000: episode: 1312, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.242, mean reward: 0.242 [0.242, 0.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.599, 10.100], loss: 0.017049, mae: 0.154871, mean_q: 16.324198
 46249/100000: episode: 1313, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.172, mean reward: 0.172 [0.172, 0.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.543, 10.100], loss: 0.010734, mae: 0.109477, mean_q: 17.496277
 46250/100000: episode: 1314, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.184, mean reward: 0.184 [0.184, 0.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.596, 10.100], loss: 0.010030, mae: 0.107119, mean_q: 14.508996
 46251/100000: episode: 1315, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.189, mean reward: 0.189 [0.189, 0.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.578, 10.100], loss: 0.007125, mae: 0.101865, mean_q: 19.232882
 46252/100000: episode: 1316, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.196, mean reward: 0.196 [0.196, 0.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.646, 10.100], loss: 0.008608, mae: 0.099625, mean_q: 21.266027
 46253/100000: episode: 1317, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.196, mean reward: 0.196 [0.196, 0.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.615, 10.100], loss: 0.010572, mae: 0.117587, mean_q: 16.640190
 46254/100000: episode: 1318, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.204, mean reward: 0.204 [0.204, 0.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.573, 10.100], loss: 0.019734, mae: 0.110986, mean_q: 17.902548
 46255/100000: episode: 1319, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.140, mean reward: 0.140 [0.140, 0.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.550, 10.100], loss: 0.007047, mae: 0.089222, mean_q: 15.678634
 46256/100000: episode: 1320, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.171, mean reward: 0.171 [0.171, 0.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.595, 10.100], loss: 0.004514, mae: 0.074330, mean_q: 17.644711
 46257/100000: episode: 1321, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.126, mean reward: 0.126 [0.126, 0.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.495, 10.100], loss: 0.020263, mae: 0.153992, mean_q: 19.068481
 46258/100000: episode: 1322, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.183, mean reward: 0.183 [0.183, 0.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.538, 10.100], loss: 0.011890, mae: 0.128061, mean_q: 17.870085
 46259/100000: episode: 1323, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.216, mean reward: 0.216 [0.216, 0.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.619, 10.100], loss: 0.009131, mae: 0.111404, mean_q: 17.676231
 46260/100000: episode: 1324, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.310, mean reward: 0.310 [0.310, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.544, 10.100], loss: 0.007894, mae: 0.081428, mean_q: 18.841209
 46261/100000: episode: 1325, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.135, mean reward: 0.135 [0.135, 0.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.472, 10.100], loss: 0.007997, mae: 0.111222, mean_q: 18.088251
 46262/100000: episode: 1326, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.214, mean reward: 0.214 [0.214, 0.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.568, 10.100], loss: 0.032317, mae: 0.175282, mean_q: 18.530132
 46263/100000: episode: 1327, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.189, mean reward: 0.189 [0.189, 0.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.495, 10.100], loss: 0.011180, mae: 0.124986, mean_q: 22.125198
 46264/100000: episode: 1328, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.192, mean reward: 0.192 [0.192, 0.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.566, 10.100], loss: 0.007328, mae: 0.097387, mean_q: 16.000360
 46265/100000: episode: 1329, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.159, mean reward: 0.159 [0.159, 0.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.546, 10.100], loss: 0.006830, mae: 0.090966, mean_q: 18.902370
 46266/100000: episode: 1330, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.145, mean reward: 0.145 [0.145, 0.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.602, 10.100], loss: 0.011559, mae: 0.127266, mean_q: 15.854368
 46267/100000: episode: 1331, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.310, mean reward: 0.310 [0.310, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.448, 10.100], loss: 0.017295, mae: 0.145832, mean_q: 15.953421
 46268/100000: episode: 1332, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.120, mean reward: 0.120 [0.120, 0.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.505, 10.100], loss: 0.010678, mae: 0.116906, mean_q: 16.988163
 46269/100000: episode: 1333, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.240, mean reward: 0.240 [0.240, 0.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.521, 10.100], loss: 0.005932, mae: 0.085115, mean_q: 19.693306
 46270/100000: episode: 1334, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.340, mean reward: 0.340 [0.340, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.453, 10.100], loss: 0.005712, mae: 0.081699, mean_q: 18.950947
 46271/100000: episode: 1335, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.192, mean reward: 0.192 [0.192, 0.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.547, 10.100], loss: 0.010549, mae: 0.120819, mean_q: 19.687723
 46272/100000: episode: 1336, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.199, mean reward: 0.199 [0.199, 0.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.534, 10.100], loss: 0.018388, mae: 0.139944, mean_q: 15.742605
 46273/100000: episode: 1337, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.221, mean reward: 0.221 [0.221, 0.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.537, 10.100], loss: 0.009416, mae: 0.096622, mean_q: 17.095295
 46274/100000: episode: 1338, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.235, mean reward: 0.235 [0.235, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.510, 10.100], loss: 0.009778, mae: 0.102671, mean_q: 22.590662
 46275/100000: episode: 1339, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.196, mean reward: 0.196 [0.196, 0.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.505, 10.100], loss: 0.034619, mae: 0.178083, mean_q: 16.376284
 46276/100000: episode: 1340, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.240, mean reward: 0.240 [0.240, 0.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.563, 10.100], loss: 0.004013, mae: 0.069841, mean_q: 18.112911
 46277/100000: episode: 1341, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.237, mean reward: 0.237 [0.237, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.547, 10.100], loss: 0.014042, mae: 0.142140, mean_q: 17.360809
 46278/100000: episode: 1342, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.184, mean reward: 0.184 [0.184, 0.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.554, 10.100], loss: 0.017133, mae: 0.145208, mean_q: 18.809977
 46279/100000: episode: 1343, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.263, mean reward: 0.263 [0.263, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.509, 10.100], loss: 0.016648, mae: 0.156648, mean_q: 18.512772
 46280/100000: episode: 1344, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.249, mean reward: 0.249 [0.249, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.512, 10.100], loss: 0.005282, mae: 0.077306, mean_q: 16.987013
 46281/100000: episode: 1345, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.293, mean reward: 0.293 [0.293, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.450, 10.100], loss: 0.013038, mae: 0.119080, mean_q: 19.186100
 46282/100000: episode: 1346, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.184, mean reward: 0.184 [0.184, 0.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.971, 10.100], loss: 0.012138, mae: 0.128001, mean_q: 19.933556
 46283/100000: episode: 1347, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.202, mean reward: 0.202 [0.202, 0.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.559, 10.100], loss: 0.006714, mae: 0.085418, mean_q: 17.884087
 46284/100000: episode: 1348, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.256, mean reward: 0.256 [0.256, 0.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.490, 10.100], loss: 0.016026, mae: 0.120868, mean_q: 18.900576
 46285/100000: episode: 1349, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.171, mean reward: 0.171 [0.171, 0.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.536, 10.100], loss: 0.020267, mae: 0.175412, mean_q: 18.841328
[Info] Not found new level, current best level reached = -0.596084713935852
 46286/100000: episode: 1350, duration: 4.013s, episode steps: 1, steps per second: 0, episode reward: 0.332, mean reward: 0.332 [0.332, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.423, 10.100], loss: 0.009850, mae: 0.111741, mean_q: 18.879103
 46386/100000: episode: 1351, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 48.528, mean reward: 0.485 [0.255, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.240, 10.098], loss: 0.010930, mae: 0.110059, mean_q: 17.900276
 46486/100000: episode: 1352, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 46.493, mean reward: 0.465 [0.294, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.564, 10.337], loss: 0.014360, mae: 0.126043, mean_q: 18.002224
 46586/100000: episode: 1353, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 55.416, mean reward: 0.554 [0.312, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.138, 10.291], loss: 0.011103, mae: 0.111157, mean_q: 17.813425
 46686/100000: episode: 1354, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 52.308, mean reward: 0.523 [0.236, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.149, 10.098], loss: 0.011147, mae: 0.108501, mean_q: 18.044815
 46786/100000: episode: 1355, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 50.152, mean reward: 0.502 [0.207, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.922, 10.098], loss: 0.011034, mae: 0.109452, mean_q: 17.899004
 46886/100000: episode: 1356, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 48.342, mean reward: 0.483 [0.272, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.747, 10.272], loss: 0.011464, mae: 0.111329, mean_q: 17.694159
 46986/100000: episode: 1357, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 50.775, mean reward: 0.508 [0.246, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.935, 10.323], loss: 0.012197, mae: 0.115799, mean_q: 17.887487
 47086/100000: episode: 1358, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 56.076, mean reward: 0.561 [0.338, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.364, 10.218], loss: 0.011382, mae: 0.109219, mean_q: 18.338503
 47186/100000: episode: 1359, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 48.369, mean reward: 0.484 [0.288, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.589, 10.098], loss: 0.008897, mae: 0.097742, mean_q: 17.612995
 47286/100000: episode: 1360, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 54.929, mean reward: 0.549 [0.331, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.307, 10.139], loss: 0.010108, mae: 0.103980, mean_q: 18.187288
 47386/100000: episode: 1361, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 54.346, mean reward: 0.543 [0.228, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.930, 10.138], loss: 0.010151, mae: 0.104024, mean_q: 18.146421
 47486/100000: episode: 1362, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 55.158, mean reward: 0.552 [0.370, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.670, 10.098], loss: 0.010113, mae: 0.104828, mean_q: 17.936352
 47586/100000: episode: 1363, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 54.371, mean reward: 0.544 [0.380, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.681, 10.098], loss: 0.011933, mae: 0.117729, mean_q: 18.277786
 47686/100000: episode: 1364, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 55.453, mean reward: 0.555 [0.369, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.523, 10.278], loss: 0.014759, mae: 0.129273, mean_q: 17.972353
 47786/100000: episode: 1365, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 54.057, mean reward: 0.541 [0.285, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.125, 10.098], loss: 0.010431, mae: 0.107085, mean_q: 17.941748
 47886/100000: episode: 1366, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 55.930, mean reward: 0.559 [0.335, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.910, 10.131], loss: 0.011162, mae: 0.110302, mean_q: 17.791546
 47986/100000: episode: 1367, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 55.317, mean reward: 0.553 [0.341, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.792, 10.189], loss: 0.011152, mae: 0.108934, mean_q: 17.779779
 48086/100000: episode: 1368, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 55.137, mean reward: 0.551 [0.336, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.591, 10.098], loss: 0.009687, mae: 0.103860, mean_q: 17.881933
 48186/100000: episode: 1369, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 52.850, mean reward: 0.529 [0.319, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.855, 10.098], loss: 0.009562, mae: 0.101141, mean_q: 18.086451
 48286/100000: episode: 1370, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 52.802, mean reward: 0.528 [0.191, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.254, 10.098], loss: 0.008847, mae: 0.099452, mean_q: 17.853537
 48386/100000: episode: 1371, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 52.353, mean reward: 0.524 [0.275, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.887, 10.098], loss: 0.009446, mae: 0.100268, mean_q: 17.966202
 48486/100000: episode: 1372, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 53.948, mean reward: 0.539 [0.370, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.764, 10.098], loss: 0.009488, mae: 0.102468, mean_q: 18.171841
 48586/100000: episode: 1373, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 56.517, mean reward: 0.565 [0.308, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.603, 10.159], loss: 0.010003, mae: 0.104403, mean_q: 17.930635
 48686/100000: episode: 1374, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 54.023, mean reward: 0.540 [0.291, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.268, 10.098], loss: 0.009715, mae: 0.103838, mean_q: 17.848593
 48786/100000: episode: 1375, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 53.699, mean reward: 0.537 [0.404, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.904, 10.193], loss: 0.009492, mae: 0.102451, mean_q: 18.152737
 48886/100000: episode: 1376, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 54.460, mean reward: 0.545 [0.371, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.907, 10.267], loss: 0.011187, mae: 0.116134, mean_q: 18.161724
 48986/100000: episode: 1377, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 56.081, mean reward: 0.561 [0.310, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.470, 10.098], loss: 0.008394, mae: 0.096069, mean_q: 18.239426
 49086/100000: episode: 1378, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 54.360, mean reward: 0.544 [0.204, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.272, 10.191], loss: 0.008717, mae: 0.100835, mean_q: 18.003929
 49186/100000: episode: 1379, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 52.190, mean reward: 0.522 [0.231, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.920, 10.098], loss: 0.008556, mae: 0.096059, mean_q: 18.231125
 49286/100000: episode: 1380, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 47.192, mean reward: 0.472 [0.291, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.511, 10.098], loss: 0.010570, mae: 0.109439, mean_q: 18.499716
 49386/100000: episode: 1381, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 54.926, mean reward: 0.549 [0.292, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.821, 10.098], loss: 0.008864, mae: 0.099635, mean_q: 18.136986
 49486/100000: episode: 1382, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.973, mean reward: 0.540 [0.285, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.424, 10.098], loss: 0.008826, mae: 0.102028, mean_q: 18.032028
 49586/100000: episode: 1383, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 50.176, mean reward: 0.502 [0.267, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.884, 10.098], loss: 0.009798, mae: 0.105458, mean_q: 18.262846
 49686/100000: episode: 1384, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 56.954, mean reward: 0.570 [0.259, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.090, 10.098], loss: 0.009277, mae: 0.104035, mean_q: 17.984188
 49786/100000: episode: 1385, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 53.074, mean reward: 0.531 [0.286, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.879, 10.289], loss: 0.007935, mae: 0.095416, mean_q: 17.883341
 49886/100000: episode: 1386, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 49.028, mean reward: 0.490 [0.274, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.442, 10.098], loss: 0.010146, mae: 0.104820, mean_q: 17.907154
 49986/100000: episode: 1387, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 55.037, mean reward: 0.550 [0.311, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.635, 10.303], loss: 0.012257, mae: 0.119101, mean_q: 18.158106
 50086/100000: episode: 1388, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 56.801, mean reward: 0.568 [0.290, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.404, 10.102], loss: 0.010902, mae: 0.110135, mean_q: 18.059107
 50186/100000: episode: 1389, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 55.138, mean reward: 0.551 [0.328, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.567, 10.169], loss: 0.009086, mae: 0.104065, mean_q: 17.827394
 50286/100000: episode: 1390, duration: 0.475s, episode steps: 100, steps per second: 211, episode reward: 48.489, mean reward: 0.485 [0.288, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.774, 10.211], loss: 0.008595, mae: 0.098519, mean_q: 18.067743
 50386/100000: episode: 1391, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 55.418, mean reward: 0.554 [0.351, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.797, 10.098], loss: 0.009012, mae: 0.101042, mean_q: 18.185287
 50486/100000: episode: 1392, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 55.065, mean reward: 0.551 [0.314, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.685, 10.140], loss: 0.008964, mae: 0.103435, mean_q: 18.360107
 50586/100000: episode: 1393, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 55.814, mean reward: 0.558 [0.216, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.546, 10.098], loss: 0.009173, mae: 0.100985, mean_q: 18.107063
 50686/100000: episode: 1394, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 56.366, mean reward: 0.564 [0.264, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.073, 10.167], loss: 0.009723, mae: 0.101406, mean_q: 18.210606
 50786/100000: episode: 1395, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.376, mean reward: 0.574 [0.332, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.474, 10.147], loss: 0.009768, mae: 0.104960, mean_q: 18.622818
 50886/100000: episode: 1396, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.886, mean reward: 0.539 [0.335, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.468, 10.098], loss: 0.009047, mae: 0.100612, mean_q: 18.964968
 50986/100000: episode: 1397, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 51.902, mean reward: 0.519 [0.268, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.201, 10.098], loss: 0.009462, mae: 0.102723, mean_q: 19.235758
 51086/100000: episode: 1398, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 50.129, mean reward: 0.501 [0.273, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.056, 10.274], loss: 0.012282, mae: 0.118716, mean_q: 19.344383
 51186/100000: episode: 1399, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 54.350, mean reward: 0.543 [0.280, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.379, 10.098], loss: 0.010700, mae: 0.109991, mean_q: 19.393696
 51286/100000: episode: 1400, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 54.990, mean reward: 0.550 [0.337, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.571, 10.098], loss: 0.011254, mae: 0.112765, mean_q: 19.845695
 51386/100000: episode: 1401, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 53.036, mean reward: 0.530 [0.344, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.506, 10.300], loss: 0.010034, mae: 0.106965, mean_q: 19.519466
 51486/100000: episode: 1402, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 55.826, mean reward: 0.558 [0.387, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.418, 10.177], loss: 0.009667, mae: 0.104208, mean_q: 19.866674
 51586/100000: episode: 1403, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 52.366, mean reward: 0.524 [0.298, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.803, 10.305], loss: 0.010252, mae: 0.110207, mean_q: 19.983498
 51686/100000: episode: 1404, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 53.841, mean reward: 0.538 [0.270, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.607, 10.152], loss: 0.008693, mae: 0.098446, mean_q: 19.675732
 51786/100000: episode: 1405, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 55.049, mean reward: 0.550 [0.308, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.635, 10.098], loss: 0.008659, mae: 0.099371, mean_q: 19.892187
 51886/100000: episode: 1406, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 48.750, mean reward: 0.487 [0.243, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.586, 10.200], loss: 0.008646, mae: 0.098265, mean_q: 19.927753
 51986/100000: episode: 1407, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 54.714, mean reward: 0.547 [0.319, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.560, 10.227], loss: 0.008751, mae: 0.099772, mean_q: 19.742357
 52086/100000: episode: 1408, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 56.521, mean reward: 0.565 [0.337, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.873, 10.146], loss: 0.008708, mae: 0.099726, mean_q: 19.722549
 52186/100000: episode: 1409, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 55.250, mean reward: 0.553 [0.357, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.898, 10.098], loss: 0.009167, mae: 0.100789, mean_q: 19.950336
 52286/100000: episode: 1410, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 54.067, mean reward: 0.541 [0.252, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.489, 10.098], loss: 0.009332, mae: 0.103624, mean_q: 19.662834
 52386/100000: episode: 1411, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 55.819, mean reward: 0.558 [0.292, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.788, 10.112], loss: 0.008126, mae: 0.096635, mean_q: 19.756041
 52486/100000: episode: 1412, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 55.252, mean reward: 0.553 [0.276, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.976, 10.323], loss: 0.008320, mae: 0.097238, mean_q: 19.870560
 52586/100000: episode: 1413, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 47.687, mean reward: 0.477 [0.278, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.285, 10.508], loss: 0.007561, mae: 0.093522, mean_q: 19.684809
 52686/100000: episode: 1414, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 52.124, mean reward: 0.521 [0.271, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.643, 10.315], loss: 0.007987, mae: 0.094001, mean_q: 19.870245
 52786/100000: episode: 1415, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 55.176, mean reward: 0.552 [0.350, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.605, 10.098], loss: 0.008132, mae: 0.097286, mean_q: 19.561356
 52886/100000: episode: 1416, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 51.319, mean reward: 0.513 [0.317, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.467, 10.312], loss: 0.006905, mae: 0.090336, mean_q: 19.883919
 52986/100000: episode: 1417, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 52.591, mean reward: 0.526 [0.280, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.507, 10.211], loss: 0.006847, mae: 0.090856, mean_q: 19.430336
 53086/100000: episode: 1418, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 54.296, mean reward: 0.543 [0.337, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.360, 10.098], loss: 0.007914, mae: 0.094398, mean_q: 19.747166
 53186/100000: episode: 1419, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.215, mean reward: 0.572 [0.328, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.701, 10.214], loss: 0.008671, mae: 0.099308, mean_q: 19.598770
 53286/100000: episode: 1420, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 48.387, mean reward: 0.484 [0.224, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.240, 10.225], loss: 0.007797, mae: 0.094674, mean_q: 19.786940
 53386/100000: episode: 1421, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 51.645, mean reward: 0.516 [0.260, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.935, 10.344], loss: 0.007685, mae: 0.094169, mean_q: 19.908653
 53486/100000: episode: 1422, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 51.221, mean reward: 0.512 [0.217, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.869, 10.164], loss: 0.007195, mae: 0.091870, mean_q: 20.102404
 53586/100000: episode: 1423, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 56.379, mean reward: 0.564 [0.356, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.338, 10.291], loss: 0.006999, mae: 0.090977, mean_q: 20.097723
 53686/100000: episode: 1424, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 47.724, mean reward: 0.477 [0.272, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.566, 10.106], loss: 0.008614, mae: 0.100377, mean_q: 19.691046
 53786/100000: episode: 1425, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 47.978, mean reward: 0.480 [0.231, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.765, 10.098], loss: 0.008293, mae: 0.098906, mean_q: 20.033417
 53886/100000: episode: 1426, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 53.066, mean reward: 0.531 [0.262, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.512, 10.098], loss: 0.008201, mae: 0.099122, mean_q: 19.801626
 53986/100000: episode: 1427, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 55.067, mean reward: 0.551 [0.363, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.823, 10.149], loss: 0.007029, mae: 0.091947, mean_q: 19.857649
 54086/100000: episode: 1428, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 47.214, mean reward: 0.472 [0.074, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.770, 10.313], loss: 0.008837, mae: 0.103322, mean_q: 19.974888
 54186/100000: episode: 1429, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 53.497, mean reward: 0.535 [0.305, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.568, 10.312], loss: 0.008154, mae: 0.098859, mean_q: 19.627968
 54286/100000: episode: 1430, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 53.137, mean reward: 0.531 [0.273, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.180, 10.274], loss: 0.008651, mae: 0.101180, mean_q: 19.939163
 54386/100000: episode: 1431, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 53.130, mean reward: 0.531 [0.130, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.496, 10.098], loss: 0.008440, mae: 0.098672, mean_q: 19.734306
 54486/100000: episode: 1432, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 54.733, mean reward: 0.547 [0.296, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.474, 10.144], loss: 0.008327, mae: 0.099169, mean_q: 19.830120
 54586/100000: episode: 1433, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 55.448, mean reward: 0.554 [0.287, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.940, 10.373], loss: 0.007651, mae: 0.096172, mean_q: 19.757336
 54686/100000: episode: 1434, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 53.003, mean reward: 0.530 [0.174, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.666, 10.178], loss: 0.007798, mae: 0.093788, mean_q: 19.640329
 54786/100000: episode: 1435, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 55.846, mean reward: 0.558 [0.288, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.653, 10.139], loss: 0.007547, mae: 0.093431, mean_q: 19.651993
 54886/100000: episode: 1436, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 52.781, mean reward: 0.528 [0.229, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.848, 10.098], loss: 0.008897, mae: 0.098042, mean_q: 19.724514
 54986/100000: episode: 1437, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 55.411, mean reward: 0.554 [0.321, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.587, 10.109], loss: 0.007413, mae: 0.094993, mean_q: 19.795965
 55086/100000: episode: 1438, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 51.450, mean reward: 0.514 [0.319, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.860, 10.098], loss: 0.007798, mae: 0.095946, mean_q: 19.892859
 55186/100000: episode: 1439, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 54.347, mean reward: 0.543 [0.287, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.756, 10.302], loss: 0.006685, mae: 0.088314, mean_q: 19.334604
 55286/100000: episode: 1440, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 53.071, mean reward: 0.531 [0.180, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.223, 10.340], loss: 0.008870, mae: 0.098799, mean_q: 19.812449
 55386/100000: episode: 1441, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 48.730, mean reward: 0.487 [0.291, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.148, 10.098], loss: 0.008808, mae: 0.101635, mean_q: 19.684881
 55486/100000: episode: 1442, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 48.354, mean reward: 0.484 [0.164, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.130, 10.286], loss: 0.009085, mae: 0.101929, mean_q: 19.806303
 55586/100000: episode: 1443, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 55.104, mean reward: 0.551 [0.293, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.304, 10.098], loss: 0.008366, mae: 0.099003, mean_q: 19.867823
 55686/100000: episode: 1444, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 48.971, mean reward: 0.490 [0.264, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.130, 10.341], loss: 0.008362, mae: 0.098723, mean_q: 19.729374
 55786/100000: episode: 1445, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 53.396, mean reward: 0.534 [0.284, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.111, 10.341], loss: 0.008877, mae: 0.101441, mean_q: 19.755058
 55886/100000: episode: 1446, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 53.216, mean reward: 0.532 [0.273, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.163, 10.296], loss: 0.008352, mae: 0.098187, mean_q: 19.921492
 55986/100000: episode: 1447, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 52.888, mean reward: 0.529 [0.358, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.915, 10.098], loss: 0.008386, mae: 0.098653, mean_q: 19.686546
 56086/100000: episode: 1448, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 50.681, mean reward: 0.507 [0.318, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.292, 10.127], loss: 0.008879, mae: 0.101848, mean_q: 19.787975
 56186/100000: episode: 1449, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 48.857, mean reward: 0.489 [0.164, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.865, 10.098], loss: 0.008524, mae: 0.098522, mean_q: 19.577644
[Info] New level: 0.24068409204483032 | Considering 10/90 traces
 56286/100000: episode: 1450, duration: 4.523s, episode steps: 100, steps per second: 22, episode reward: 40.101, mean reward: 0.401 [0.126, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.997, 10.449], loss: 0.007794, mae: 0.097823, mean_q: 19.891359
 56288/100000: episode: 1451, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.823, mean reward: 0.412 [0.389, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.218, 10.100], loss: 0.005079, mae: 0.084988, mean_q: 19.344040
 56290/100000: episode: 1452, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.274, mean reward: 0.637 [0.627, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.257, 10.100], loss: 0.013308, mae: 0.130522, mean_q: 20.220268
 56292/100000: episode: 1453, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.165, mean reward: 0.582 [0.524, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.148, 10.115], loss: 0.004224, mae: 0.073926, mean_q: 19.351685
 56294/100000: episode: 1454, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.911, mean reward: 0.456 [0.363, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.180, 10.100], loss: 0.009579, mae: 0.110255, mean_q: 20.089483
 56296/100000: episode: 1455, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.707, mean reward: 0.354 [0.350, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.312, 10.100], loss: 0.007314, mae: 0.088423, mean_q: 19.303900
 56298/100000: episode: 1456, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 0.931, mean reward: 0.466 [0.377, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.152, 10.100], loss: 0.005738, mae: 0.085837, mean_q: 20.006662
 56300/100000: episode: 1457, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 0.920, mean reward: 0.460 [0.458, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.213, 10.100], loss: 0.008228, mae: 0.090517, mean_q: 17.321520
 56302/100000: episode: 1458, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.259, mean reward: 0.629 [0.579, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.143, 10.100], loss: 0.008273, mae: 0.101962, mean_q: 20.600063
 56304/100000: episode: 1459, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 1.218, mean reward: 0.609 [0.570, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.122, 10.100], loss: 0.010096, mae: 0.109059, mean_q: 18.100943
 56306/100000: episode: 1460, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.831, mean reward: 0.415 [0.398, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.212, 10.100], loss: 0.009475, mae: 0.112489, mean_q: 20.296236
 56308/100000: episode: 1461, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.847, mean reward: 0.423 [0.392, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.149, 10.100], loss: 0.006004, mae: 0.085771, mean_q: 20.593216
 56310/100000: episode: 1462, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.825, mean reward: 0.413 [0.394, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.287, 10.100], loss: 0.006470, mae: 0.091985, mean_q: 19.699078
 56312/100000: episode: 1463, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.804, mean reward: 0.402 [0.402, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.331, 10.100], loss: 0.004701, mae: 0.082623, mean_q: 17.104630
 56314/100000: episode: 1464, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.799, mean reward: 0.399 [0.388, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.218, 10.100], loss: 0.006869, mae: 0.090633, mean_q: 21.305080
 56316/100000: episode: 1465, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.701, mean reward: 0.350 [0.316, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.338, 10.100], loss: 0.010980, mae: 0.121184, mean_q: 19.811056
 56318/100000: episode: 1466, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.874, mean reward: 0.437 [0.434, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.420, 10.100], loss: 0.012918, mae: 0.124368, mean_q: 18.527832
 56320/100000: episode: 1467, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 1.162, mean reward: 0.581 [0.562, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.181, 10.100], loss: 0.009086, mae: 0.099173, mean_q: 19.943529
 56322/100000: episode: 1468, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.918, mean reward: 0.459 [0.391, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.182, 10.100], loss: 0.008315, mae: 0.102940, mean_q: 18.542675
 56324/100000: episode: 1469, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.704, mean reward: 0.352 [0.314, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.133, 10.100], loss: 0.007560, mae: 0.099573, mean_q: 19.870676
 56326/100000: episode: 1470, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.910, mean reward: 0.455 [0.434, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.227, 10.100], loss: 0.010242, mae: 0.120777, mean_q: 20.553617
 56328/100000: episode: 1471, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.946, mean reward: 0.473 [0.444, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.174, 10.100], loss: 0.008117, mae: 0.102652, mean_q: 21.131443
 56330/100000: episode: 1472, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 1.119, mean reward: 0.559 [0.511, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.221, 10.100], loss: 0.006018, mae: 0.088551, mean_q: 21.048798
 56332/100000: episode: 1473, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 1.053, mean reward: 0.527 [0.413, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.192, 10.118], loss: 0.008596, mae: 0.110150, mean_q: 16.313650
 56334/100000: episode: 1474, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.995, mean reward: 0.497 [0.478, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.192, 10.100], loss: 0.006076, mae: 0.089083, mean_q: 20.908993
 56336/100000: episode: 1475, duration: 0.023s, episode steps: 2, steps per second: 88, episode reward: 1.168, mean reward: 0.584 [0.552, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.202, 10.100], loss: 0.006882, mae: 0.090325, mean_q: 19.487041
 56338/100000: episode: 1476, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.706, mean reward: 0.353 [0.338, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.315, 10.100], loss: 0.010225, mae: 0.103835, mean_q: 18.450798
 56340/100000: episode: 1477, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 0.911, mean reward: 0.455 [0.436, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.280, 10.100], loss: 0.007276, mae: 0.101690, mean_q: 17.645329
 56342/100000: episode: 1478, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.191, mean reward: 0.596 [0.557, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.208, 10.100], loss: 0.008746, mae: 0.105634, mean_q: 18.946281
 56344/100000: episode: 1479, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.098, mean reward: 0.549 [0.547, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.136, 10.100], loss: 0.006080, mae: 0.087762, mean_q: 18.927322
 56346/100000: episode: 1480, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.242, mean reward: 0.621 [0.600, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.251, 10.122], loss: 0.006540, mae: 0.090982, mean_q: 18.987400
 56348/100000: episode: 1481, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.910, mean reward: 0.455 [0.451, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.107, 10.100], loss: 0.007449, mae: 0.089467, mean_q: 19.973648
 56350/100000: episode: 1482, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.249, mean reward: 0.625 [0.564, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.183, 10.100], loss: 0.005971, mae: 0.083818, mean_q: 19.758823
 56352/100000: episode: 1483, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.798, mean reward: 0.399 [0.396, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.286, 10.100], loss: 0.009194, mae: 0.105808, mean_q: 20.620615
 56354/100000: episode: 1484, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.937, mean reward: 0.468 [0.460, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.283, 10.100], loss: 0.008063, mae: 0.108463, mean_q: 18.089827
 56356/100000: episode: 1485, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.859, mean reward: 0.430 [0.367, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.152, 10.100], loss: 0.012816, mae: 0.121873, mean_q: 19.599373
 56358/100000: episode: 1486, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.051, mean reward: 0.525 [0.521, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.178, 10.100], loss: 0.008136, mae: 0.098762, mean_q: 18.725096
 56360/100000: episode: 1487, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.984, mean reward: 0.492 [0.483, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.175, 10.100], loss: 0.005308, mae: 0.084012, mean_q: 20.172813
 56362/100000: episode: 1488, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.822, mean reward: 0.411 [0.370, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.270, 10.100], loss: 0.004453, mae: 0.079986, mean_q: 18.289272
 56364/100000: episode: 1489, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.895, mean reward: 0.447 [0.444, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.268, 10.100], loss: 0.007399, mae: 0.096680, mean_q: 19.951992
 56366/100000: episode: 1490, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.948, mean reward: 0.474 [0.450, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.159, 10.100], loss: 0.004602, mae: 0.077127, mean_q: 20.093147
 56368/100000: episode: 1491, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.137, mean reward: 0.568 [0.561, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.149, 10.100], loss: 0.009084, mae: 0.105727, mean_q: 21.114794
 56370/100000: episode: 1492, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.927, mean reward: 0.463 [0.461, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.219, 10.100], loss: 0.006182, mae: 0.082573, mean_q: 20.244328
 56372/100000: episode: 1493, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.841, mean reward: 0.420 [0.387, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.205, 10.100], loss: 0.008960, mae: 0.101440, mean_q: 20.237707
 56374/100000: episode: 1494, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.145, mean reward: 0.573 [0.540, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.137, 10.100], loss: 0.015002, mae: 0.123793, mean_q: 19.516426
 56376/100000: episode: 1495, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 1.051, mean reward: 0.526 [0.505, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.184, 10.100], loss: 0.007131, mae: 0.096489, mean_q: 19.762745
 56378/100000: episode: 1496, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.011, mean reward: 0.505 [0.449, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.235, 10.100], loss: 0.011403, mae: 0.102962, mean_q: 19.660406
 56380/100000: episode: 1497, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 0.737, mean reward: 0.368 [0.347, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.294, 10.100], loss: 0.007498, mae: 0.102938, mean_q: 18.508654
 56382/100000: episode: 1498, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.933, mean reward: 0.466 [0.350, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.081, 10.100], loss: 0.007849, mae: 0.099921, mean_q: 20.786697
 56384/100000: episode: 1499, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.245, mean reward: 0.623 [0.607, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.239, 10.100], loss: 0.007226, mae: 0.094627, mean_q: 18.411232
 56386/100000: episode: 1500, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.818, mean reward: 0.409 [0.397, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.244, 10.100], loss: 0.012161, mae: 0.129446, mean_q: 20.340172
 56388/100000: episode: 1501, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.214, mean reward: 0.607 [0.555, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.198, 10.100], loss: 0.009000, mae: 0.106475, mean_q: 21.066948
 56390/100000: episode: 1502, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.955, mean reward: 0.478 [0.364, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.126, 10.100], loss: 0.006821, mae: 0.094397, mean_q: 18.330437
 56392/100000: episode: 1503, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 1.014, mean reward: 0.507 [0.491, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.152, 10.100], loss: 0.008303, mae: 0.095360, mean_q: 19.741993
 56394/100000: episode: 1504, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.826, mean reward: 0.413 [0.410, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.265, 10.100], loss: 0.010862, mae: 0.115263, mean_q: 18.774570
 56396/100000: episode: 1505, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.776, mean reward: 0.388 [0.375, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.335, 10.100], loss: 0.008566, mae: 0.098179, mean_q: 17.031073
 56398/100000: episode: 1506, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.882, mean reward: 0.441 [0.421, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.328, 10.100], loss: 0.006811, mae: 0.093692, mean_q: 19.503166
 56400/100000: episode: 1507, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.195, mean reward: 0.598 [0.579, 0.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.248, 10.100], loss: 0.005161, mae: 0.082339, mean_q: 20.081747
 56402/100000: episode: 1508, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.177, mean reward: 0.588 [0.580, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.198, 10.100], loss: 0.007814, mae: 0.096171, mean_q: 19.755655
 56404/100000: episode: 1509, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.905, mean reward: 0.452 [0.450, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.333, 10.100], loss: 0.010943, mae: 0.122584, mean_q: 20.456089
 56406/100000: episode: 1510, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.215, mean reward: 0.608 [0.606, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.256, 10.100], loss: 0.007555, mae: 0.090829, mean_q: 20.158737
 56408/100000: episode: 1511, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.801, mean reward: 0.401 [0.383, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.164, 10.100], loss: 0.006810, mae: 0.094191, mean_q: 18.409496
 56410/100000: episode: 1512, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.959, mean reward: 0.479 [0.475, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.106, 10.100], loss: 0.007234, mae: 0.095292, mean_q: 18.752888
 56412/100000: episode: 1513, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.111, mean reward: 0.555 [0.530, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.223, 10.100], loss: 0.010066, mae: 0.117870, mean_q: 19.599815
 56414/100000: episode: 1514, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.990, mean reward: 0.495 [0.485, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.217, 10.100], loss: 0.007581, mae: 0.095134, mean_q: 18.799473
 56416/100000: episode: 1515, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.870, mean reward: 0.435 [0.402, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.142, 10.100], loss: 0.008849, mae: 0.092584, mean_q: 18.519112
 56418/100000: episode: 1516, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.626, mean reward: 0.313 [0.267, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.259, 10.100], loss: 0.008598, mae: 0.104730, mean_q: 17.845638
 56420/100000: episode: 1517, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 1.101, mean reward: 0.551 [0.467, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.142, 10.100], loss: 0.006444, mae: 0.090297, mean_q: 18.439983
 56422/100000: episode: 1518, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.928, mean reward: 0.464 [0.393, 0.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.181, 10.100], loss: 0.005991, mae: 0.087704, mean_q: 19.632460
 56424/100000: episode: 1519, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.230, mean reward: 0.615 [0.598, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.162, 10.100], loss: 0.006433, mae: 0.091897, mean_q: 19.535248
 56426/100000: episode: 1520, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.833, mean reward: 0.417 [0.414, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.318, 10.100], loss: 0.011284, mae: 0.101980, mean_q: 19.271168
 56428/100000: episode: 1521, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.811, mean reward: 0.406 [0.397, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.230, 10.100], loss: 0.006619, mae: 0.094772, mean_q: 19.462486
 56430/100000: episode: 1522, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.963, mean reward: 0.481 [0.462, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.183, 10.100], loss: 0.007841, mae: 0.101952, mean_q: 19.996899
 56432/100000: episode: 1523, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.697, mean reward: 0.348 [0.343, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.186, 10.100], loss: 0.013447, mae: 0.110770, mean_q: 18.851669
 56434/100000: episode: 1524, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.790, mean reward: 0.395 [0.368, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.304, 10.100], loss: 0.005918, mae: 0.089563, mean_q: 15.604391
 56436/100000: episode: 1525, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.914, mean reward: 0.457 [0.449, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.265, 10.100], loss: 0.008383, mae: 0.105872, mean_q: 21.169758
 56438/100000: episode: 1526, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.782, mean reward: 0.391 [0.363, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.299, 10.100], loss: 0.006724, mae: 0.087758, mean_q: 20.808189
 56440/100000: episode: 1527, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.772, mean reward: 0.386 [0.313, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.296, 10.100], loss: 0.010750, mae: 0.109142, mean_q: 18.589182
 56442/100000: episode: 1528, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 1.218, mean reward: 0.609 [0.553, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.231, 10.130], loss: 0.006373, mae: 0.086047, mean_q: 18.166784
 56444/100000: episode: 1529, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.887, mean reward: 0.443 [0.372, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.162, 10.100], loss: 0.009391, mae: 0.110202, mean_q: 18.283024
 56446/100000: episode: 1530, duration: 0.017s, episode steps: 2, steps per second: 114, episode reward: 1.101, mean reward: 0.550 [0.546, 0.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.156, 10.100], loss: 0.009868, mae: 0.102407, mean_q: 19.559845
 56448/100000: episode: 1531, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.259, mean reward: 0.630 [0.586, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.224, 10.100], loss: 0.008429, mae: 0.109330, mean_q: 19.832775
 56450/100000: episode: 1532, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.833, mean reward: 0.417 [0.389, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.377, 10.100], loss: 0.011386, mae: 0.111139, mean_q: 17.931446
 56452/100000: episode: 1533, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.891, mean reward: 0.445 [0.403, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.240, 10.100], loss: 0.013442, mae: 0.124808, mean_q: 19.303749
 56454/100000: episode: 1534, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.847, mean reward: 0.423 [0.396, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.132, 10.100], loss: 0.009456, mae: 0.107090, mean_q: 18.388340
 56456/100000: episode: 1535, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.806, mean reward: 0.403 [0.377, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.210, 10.100], loss: 0.004982, mae: 0.079289, mean_q: 20.457413
 56458/100000: episode: 1536, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.952, mean reward: 0.476 [0.435, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.105, 10.100], loss: 0.005686, mae: 0.084582, mean_q: 19.070900
 56460/100000: episode: 1537, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.827, mean reward: 0.414 [0.408, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.304, 10.100], loss: 0.006586, mae: 0.089455, mean_q: 19.112684
 56462/100000: episode: 1538, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.796, mean reward: 0.398 [0.379, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.262, 10.100], loss: 0.005214, mae: 0.083498, mean_q: 18.186817
 56464/100000: episode: 1539, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.778, mean reward: 0.389 [0.384, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.346, 10.100], loss: 0.013893, mae: 0.145624, mean_q: 20.360855
[Info] Not found new level, current best level reached = 0.24068409204483032
 56466/100000: episode: 1540, duration: 4.027s, episode steps: 2, steps per second: 0, episode reward: 1.021, mean reward: 0.511 [0.407, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.195, 10.100], loss: 0.009611, mae: 0.111865, mean_q: 18.487116
 56566/100000: episode: 1541, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 52.594, mean reward: 0.526 [0.270, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.189, 10.098], loss: 0.009310, mae: 0.103515, mean_q: 18.963158
 56666/100000: episode: 1542, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 55.837, mean reward: 0.558 [0.297, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.285, 10.246], loss: 0.008939, mae: 0.102488, mean_q: 19.172497
 56766/100000: episode: 1543, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 52.124, mean reward: 0.521 [0.289, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.080, 10.098], loss: 0.010434, mae: 0.109850, mean_q: 18.766560
 56866/100000: episode: 1544, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 52.515, mean reward: 0.525 [0.222, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.878, 10.130], loss: 0.010983, mae: 0.109514, mean_q: 19.071238
 56966/100000: episode: 1545, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 52.812, mean reward: 0.528 [0.236, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.267, 10.127], loss: 0.010180, mae: 0.105569, mean_q: 18.973827
 57066/100000: episode: 1546, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 53.785, mean reward: 0.538 [0.313, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.932, 10.098], loss: 0.011614, mae: 0.116089, mean_q: 18.822695
 57166/100000: episode: 1547, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 53.116, mean reward: 0.531 [0.291, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.909, 10.370], loss: 0.010825, mae: 0.110010, mean_q: 18.734831
 57266/100000: episode: 1548, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 51.627, mean reward: 0.516 [0.258, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.096, 10.402], loss: 0.010036, mae: 0.106253, mean_q: 19.364788
 57366/100000: episode: 1549, duration: 0.512s, episode steps: 100, steps per second: 196, episode reward: 48.698, mean reward: 0.487 [0.178, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.016, 10.098], loss: 0.010941, mae: 0.111456, mean_q: 18.891619
 57466/100000: episode: 1550, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 53.469, mean reward: 0.535 [0.275, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.878, 10.098], loss: 0.010345, mae: 0.105717, mean_q: 19.215002
 57566/100000: episode: 1551, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 55.898, mean reward: 0.559 [0.258, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.778, 10.098], loss: 0.012211, mae: 0.115578, mean_q: 18.840818
 57666/100000: episode: 1552, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 53.286, mean reward: 0.533 [0.240, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.466, 10.098], loss: 0.012100, mae: 0.116824, mean_q: 19.147289
 57766/100000: episode: 1553, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 53.400, mean reward: 0.534 [0.302, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.835, 10.182], loss: 0.011010, mae: 0.109840, mean_q: 18.451679
 57866/100000: episode: 1554, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 55.598, mean reward: 0.556 [0.220, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.799, 10.098], loss: 0.011395, mae: 0.112738, mean_q: 18.631012
 57966/100000: episode: 1555, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 54.397, mean reward: 0.544 [0.343, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.727, 10.214], loss: 0.013110, mae: 0.120743, mean_q: 18.611933
 58066/100000: episode: 1556, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 55.026, mean reward: 0.550 [0.359, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.863, 10.145], loss: 0.010706, mae: 0.106487, mean_q: 19.058762
 58166/100000: episode: 1557, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 51.796, mean reward: 0.518 [0.231, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.096, 10.098], loss: 0.009920, mae: 0.105831, mean_q: 18.933447
 58266/100000: episode: 1558, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 50.869, mean reward: 0.509 [0.234, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.510, 10.098], loss: 0.011087, mae: 0.110230, mean_q: 18.835081
 58366/100000: episode: 1559, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 56.008, mean reward: 0.560 [0.181, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.096, 10.098], loss: 0.011834, mae: 0.114193, mean_q: 18.631882
 58466/100000: episode: 1560, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 45.256, mean reward: 0.453 [0.202, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.590, 10.336], loss: 0.010850, mae: 0.106013, mean_q: 18.935186
 58566/100000: episode: 1561, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 54.041, mean reward: 0.540 [0.321, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.787, 10.098], loss: 0.010366, mae: 0.104720, mean_q: 18.953354
 58666/100000: episode: 1562, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 53.035, mean reward: 0.530 [0.323, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.357, 10.098], loss: 0.010075, mae: 0.103859, mean_q: 19.028210
 58766/100000: episode: 1563, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 52.786, mean reward: 0.528 [0.312, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.517, 10.098], loss: 0.010809, mae: 0.109481, mean_q: 18.800703
 58866/100000: episode: 1564, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.035, mean reward: 0.570 [0.328, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.108, 10.098], loss: 0.009612, mae: 0.101410, mean_q: 18.811270
 58966/100000: episode: 1565, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 53.410, mean reward: 0.534 [0.331, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.008, 10.147], loss: 0.010550, mae: 0.106007, mean_q: 18.799845
 59066/100000: episode: 1566, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 53.740, mean reward: 0.537 [0.339, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.925, 10.170], loss: 0.012456, mae: 0.116389, mean_q: 18.871244
 59166/100000: episode: 1567, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 53.184, mean reward: 0.532 [0.241, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.724, 10.167], loss: 0.012492, mae: 0.118422, mean_q: 19.072903
 59266/100000: episode: 1568, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 53.070, mean reward: 0.531 [0.318, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.976, 10.098], loss: 0.010529, mae: 0.106877, mean_q: 18.704615
 59366/100000: episode: 1569, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 54.535, mean reward: 0.545 [0.297, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.172, 10.210], loss: 0.009546, mae: 0.100222, mean_q: 18.887825
 59466/100000: episode: 1570, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 51.025, mean reward: 0.510 [0.210, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.565, 10.111], loss: 0.010359, mae: 0.106013, mean_q: 18.489399
 59566/100000: episode: 1571, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 56.079, mean reward: 0.561 [0.333, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.630, 10.227], loss: 0.010003, mae: 0.105034, mean_q: 18.767136
 59666/100000: episode: 1572, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 55.301, mean reward: 0.553 [0.334, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.671, 10.098], loss: 0.010293, mae: 0.105843, mean_q: 19.068657
 59766/100000: episode: 1573, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 56.312, mean reward: 0.563 [0.324, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.000, 10.193], loss: 0.012236, mae: 0.114942, mean_q: 18.817057
 59866/100000: episode: 1574, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 55.463, mean reward: 0.555 [0.201, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.164, 10.098], loss: 0.010280, mae: 0.101375, mean_q: 18.874952
 59966/100000: episode: 1575, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 51.917, mean reward: 0.519 [0.135, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.220, 10.167], loss: 0.010351, mae: 0.103969, mean_q: 18.727692
 60066/100000: episode: 1576, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 49.633, mean reward: 0.496 [0.265, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.857, 10.441], loss: 0.009190, mae: 0.099321, mean_q: 18.908873
 60166/100000: episode: 1577, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 55.196, mean reward: 0.552 [0.251, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.347, 10.098], loss: 0.011716, mae: 0.111810, mean_q: 18.817223
 60266/100000: episode: 1578, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 54.554, mean reward: 0.546 [0.327, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.479, 10.339], loss: 0.011494, mae: 0.110958, mean_q: 18.827684
 60366/100000: episode: 1579, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 51.833, mean reward: 0.518 [0.247, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.993, 10.158], loss: 0.010855, mae: 0.108375, mean_q: 18.582401
 60466/100000: episode: 1580, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 53.780, mean reward: 0.538 [0.281, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.934, 10.273], loss: 0.011400, mae: 0.107221, mean_q: 18.714956
 60566/100000: episode: 1581, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 54.808, mean reward: 0.548 [0.324, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.357, 10.288], loss: 0.011442, mae: 0.111968, mean_q: 18.978012
 60666/100000: episode: 1582, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 54.769, mean reward: 0.548 [0.257, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.162, 10.180], loss: 0.009696, mae: 0.103141, mean_q: 18.911039
 60766/100000: episode: 1583, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 50.453, mean reward: 0.505 [0.113, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.214, 10.257], loss: 0.010344, mae: 0.106467, mean_q: 18.885115
 60866/100000: episode: 1584, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 54.287, mean reward: 0.543 [0.290, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.143, 10.098], loss: 0.012160, mae: 0.114545, mean_q: 18.802801
 60966/100000: episode: 1585, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 51.597, mean reward: 0.516 [0.285, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.937, 10.098], loss: 0.010106, mae: 0.106012, mean_q: 18.841696
 61066/100000: episode: 1586, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 53.553, mean reward: 0.536 [0.230, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.979, 10.174], loss: 0.010651, mae: 0.104849, mean_q: 18.613012
 61166/100000: episode: 1587, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 54.442, mean reward: 0.544 [0.321, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.231, 10.100], loss: 0.012340, mae: 0.113823, mean_q: 18.786680
 61266/100000: episode: 1588, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 53.931, mean reward: 0.539 [0.339, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.709, 10.126], loss: 0.010952, mae: 0.110529, mean_q: 19.131727
 61366/100000: episode: 1589, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 56.044, mean reward: 0.560 [0.357, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.533, 10.175], loss: 0.010253, mae: 0.103110, mean_q: 19.068275
 61466/100000: episode: 1590, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 50.176, mean reward: 0.502 [0.271, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.827, 10.100], loss: 0.010443, mae: 0.103511, mean_q: 19.699774
 61566/100000: episode: 1591, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 51.352, mean reward: 0.514 [0.291, 0.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.231], loss: 0.012318, mae: 0.112908, mean_q: 19.633263
 61666/100000: episode: 1592, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 53.132, mean reward: 0.531 [0.289, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.299, 10.123], loss: 0.009063, mae: 0.099768, mean_q: 19.396818
 61766/100000: episode: 1593, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 57.004, mean reward: 0.570 [0.294, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.399, 10.161], loss: 0.009603, mae: 0.097428, mean_q: 19.273712
 61866/100000: episode: 1594, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 51.351, mean reward: 0.514 [0.161, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.005, 10.098], loss: 0.012026, mae: 0.114565, mean_q: 19.389759
 61966/100000: episode: 1595, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 53.569, mean reward: 0.536 [0.174, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.581, 10.429], loss: 0.009433, mae: 0.099161, mean_q: 19.504139
 62066/100000: episode: 1596, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 54.409, mean reward: 0.544 [0.332, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.660, 10.121], loss: 0.008984, mae: 0.094927, mean_q: 19.581343
 62166/100000: episode: 1597, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 47.497, mean reward: 0.475 [0.319, 0.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.947, 10.274], loss: 0.010790, mae: 0.104572, mean_q: 19.460392
 62266/100000: episode: 1598, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 55.741, mean reward: 0.557 [0.400, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.865, 10.098], loss: 0.008997, mae: 0.095419, mean_q: 19.599754
 62366/100000: episode: 1599, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 51.542, mean reward: 0.515 [0.198, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.110, 10.123], loss: 0.010229, mae: 0.102371, mean_q: 19.716032
 62466/100000: episode: 1600, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 52.684, mean reward: 0.527 [0.246, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.651, 10.098], loss: 0.010673, mae: 0.106345, mean_q: 19.929201
 62566/100000: episode: 1601, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 53.090, mean reward: 0.531 [0.267, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.503, 10.259], loss: 0.010895, mae: 0.106853, mean_q: 19.704145
 62666/100000: episode: 1602, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 54.788, mean reward: 0.548 [0.280, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.525, 10.098], loss: 0.010888, mae: 0.106729, mean_q: 19.578798
 62766/100000: episode: 1603, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 54.468, mean reward: 0.545 [0.330, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.065, 10.098], loss: 0.010589, mae: 0.104822, mean_q: 19.484301
 62866/100000: episode: 1604, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 54.713, mean reward: 0.547 [0.343, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.507, 10.098], loss: 0.010744, mae: 0.106308, mean_q: 19.387354
 62966/100000: episode: 1605, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 49.271, mean reward: 0.493 [0.266, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.791, 10.098], loss: 0.010586, mae: 0.103877, mean_q: 19.336809
 63066/100000: episode: 1606, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.521, mean reward: 0.535 [0.320, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.763, 10.098], loss: 0.009999, mae: 0.105060, mean_q: 19.574228
 63166/100000: episode: 1607, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 51.558, mean reward: 0.516 [0.226, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.793, 10.098], loss: 0.008580, mae: 0.097255, mean_q: 19.638926
 63266/100000: episode: 1608, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 53.849, mean reward: 0.538 [0.232, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.191, 10.203], loss: 0.011119, mae: 0.108576, mean_q: 19.536186
 63366/100000: episode: 1609, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 53.812, mean reward: 0.538 [0.285, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.716, 10.110], loss: 0.012791, mae: 0.119323, mean_q: 19.502741
 63466/100000: episode: 1610, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 55.715, mean reward: 0.557 [0.346, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.483, 10.098], loss: 0.010775, mae: 0.107604, mean_q: 19.310287
 63566/100000: episode: 1611, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 52.581, mean reward: 0.526 [0.333, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.689, 10.287], loss: 0.008169, mae: 0.092800, mean_q: 19.798721
 63666/100000: episode: 1612, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 51.855, mean reward: 0.519 [0.268, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.573, 10.122], loss: 0.009972, mae: 0.102410, mean_q: 19.341223
 63766/100000: episode: 1613, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 50.896, mean reward: 0.509 [0.285, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.986, 10.193], loss: 0.010688, mae: 0.105132, mean_q: 19.765173
 63866/100000: episode: 1614, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 48.348, mean reward: 0.483 [0.190, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.826, 10.098], loss: 0.010277, mae: 0.108533, mean_q: 19.613144
 63966/100000: episode: 1615, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 49.308, mean reward: 0.493 [0.274, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.734, 10.486], loss: 0.010140, mae: 0.104403, mean_q: 19.649845
 64066/100000: episode: 1616, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 52.257, mean reward: 0.523 [0.225, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.707, 10.098], loss: 0.011724, mae: 0.117521, mean_q: 19.527916
 64166/100000: episode: 1617, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 55.841, mean reward: 0.558 [0.307, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.662, 10.098], loss: 0.010555, mae: 0.108183, mean_q: 19.668373
 64266/100000: episode: 1618, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 53.774, mean reward: 0.538 [0.309, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.518, 10.098], loss: 0.008820, mae: 0.095476, mean_q: 19.275007
 64366/100000: episode: 1619, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 42.038, mean reward: 0.420 [0.201, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.909, 10.098], loss: 0.009146, mae: 0.100355, mean_q: 19.582500
 64466/100000: episode: 1620, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 51.088, mean reward: 0.511 [0.237, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-2.271, 10.334], loss: 0.009331, mae: 0.103086, mean_q: 19.934303
 64566/100000: episode: 1621, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 55.346, mean reward: 0.553 [0.225, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.429, 10.098], loss: 0.008753, mae: 0.099185, mean_q: 19.676981
 64666/100000: episode: 1622, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 53.639, mean reward: 0.536 [0.339, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.238, 10.120], loss: 0.009216, mae: 0.105826, mean_q: 19.343760
 64766/100000: episode: 1623, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 53.575, mean reward: 0.536 [0.337, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.769, 10.098], loss: 0.009161, mae: 0.101208, mean_q: 19.464272
 64866/100000: episode: 1624, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 50.072, mean reward: 0.501 [0.248, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.686, 10.369], loss: 0.009449, mae: 0.104752, mean_q: 19.453863
 64966/100000: episode: 1625, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 51.261, mean reward: 0.513 [0.212, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.550, 10.572], loss: 0.009007, mae: 0.101645, mean_q: 19.718561
 65066/100000: episode: 1626, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 52.138, mean reward: 0.521 [0.238, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.514, 10.227], loss: 0.008335, mae: 0.097143, mean_q: 19.752522
 65166/100000: episode: 1627, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 52.290, mean reward: 0.523 [0.336, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.063, 10.098], loss: 0.009419, mae: 0.104035, mean_q: 19.791611
 65266/100000: episode: 1628, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 52.152, mean reward: 0.522 [0.306, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.420, 10.098], loss: 0.010442, mae: 0.110733, mean_q: 19.553143
 65366/100000: episode: 1629, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 54.258, mean reward: 0.543 [0.241, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.394, 10.170], loss: 0.007701, mae: 0.096974, mean_q: 19.675087
 65466/100000: episode: 1630, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 54.378, mean reward: 0.544 [0.315, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.763, 10.098], loss: 0.009017, mae: 0.102344, mean_q: 19.607363
 65566/100000: episode: 1631, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 54.510, mean reward: 0.545 [0.233, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.171, 10.212], loss: 0.007652, mae: 0.094541, mean_q: 19.473312
 65666/100000: episode: 1632, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 55.214, mean reward: 0.552 [0.286, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.155, 10.098], loss: 0.008028, mae: 0.097457, mean_q: 19.498486
 65766/100000: episode: 1633, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 55.960, mean reward: 0.560 [0.336, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.722, 10.121], loss: 0.007665, mae: 0.095823, mean_q: 19.796400
 65866/100000: episode: 1634, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 54.321, mean reward: 0.543 [0.295, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.003, 10.266], loss: 0.008982, mae: 0.100991, mean_q: 19.711723
 65966/100000: episode: 1635, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 47.117, mean reward: 0.471 [0.195, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.772, 10.098], loss: 0.008149, mae: 0.097938, mean_q: 19.854237
 66066/100000: episode: 1636, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 53.288, mean reward: 0.533 [0.348, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.582, 10.098], loss: 0.007353, mae: 0.093740, mean_q: 19.557632
 66166/100000: episode: 1637, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 52.328, mean reward: 0.523 [0.168, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.375, 10.176], loss: 0.008306, mae: 0.098945, mean_q: 19.624266
 66266/100000: episode: 1638, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 45.503, mean reward: 0.455 [0.288, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.244, 10.314], loss: 0.009049, mae: 0.104965, mean_q: 19.709904
 66366/100000: episode: 1639, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 51.131, mean reward: 0.511 [0.250, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.343], loss: 0.008105, mae: 0.097449, mean_q: 19.837109
[Info] New level: 0.1543029248714447 | Considering 10/90 traces
 66466/100000: episode: 1640, duration: 4.482s, episode steps: 100, steps per second: 22, episode reward: 55.958, mean reward: 0.560 [0.266, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.827, 10.124], loss: 0.006642, mae: 0.089609, mean_q: 19.307154
 66468/100000: episode: 1641, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.672, mean reward: 0.336 [0.311, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.230, 10.100], loss: 0.004306, mae: 0.076216, mean_q: 19.821278
 66470/100000: episode: 1642, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.881, mean reward: 0.441 [0.431, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.284, 10.100], loss: 0.006756, mae: 0.089283, mean_q: 19.076456
 66472/100000: episode: 1643, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.570, mean reward: 0.285 [0.252, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.230, 10.100], loss: 0.006020, mae: 0.080917, mean_q: 19.964565
 66474/100000: episode: 1644, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.602, mean reward: 0.301 [0.279, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.521, 10.100], loss: 0.009430, mae: 0.112434, mean_q: 17.580162
 66476/100000: episode: 1645, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.667, mean reward: 0.333 [0.327, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.284, 10.100], loss: 0.007200, mae: 0.088115, mean_q: 20.936630
 66478/100000: episode: 1646, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.887, mean reward: 0.443 [0.440, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.330, 10.100], loss: 0.006485, mae: 0.081116, mean_q: 20.935732
 66480/100000: episode: 1647, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.671, mean reward: 0.336 [0.327, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.261, 10.100], loss: 0.006141, mae: 0.082836, mean_q: 19.362328
 66482/100000: episode: 1648, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.985, mean reward: 0.493 [0.475, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.285, 10.100], loss: 0.007055, mae: 0.095348, mean_q: 19.659563
 66484/100000: episode: 1649, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.894, mean reward: 0.447 [0.431, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.151, 10.100], loss: 0.010972, mae: 0.120073, mean_q: 19.995457
 66486/100000: episode: 1650, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.668, mean reward: 0.334 [0.326, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.248, 10.100], loss: 0.005058, mae: 0.077972, mean_q: 19.194164
 66488/100000: episode: 1651, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.811, mean reward: 0.405 [0.396, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.364, 10.100], loss: 0.007497, mae: 0.104279, mean_q: 19.953665
 66490/100000: episode: 1652, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.600, mean reward: 0.300 [0.282, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.240, 10.100], loss: 0.005920, mae: 0.083662, mean_q: 19.018349
 66492/100000: episode: 1653, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.508, mean reward: 0.254 [0.227, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.302, 10.100], loss: 0.009544, mae: 0.114310, mean_q: 20.869396
 66494/100000: episode: 1654, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.650, mean reward: 0.325 [0.323, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.247, 10.100], loss: 0.006498, mae: 0.086514, mean_q: 20.241432
 66496/100000: episode: 1655, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.964, mean reward: 0.482 [0.460, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.541, 10.100], loss: 0.009707, mae: 0.110582, mean_q: 19.162491
 66498/100000: episode: 1656, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.887, mean reward: 0.443 [0.402, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.223, 10.100], loss: 0.017521, mae: 0.125729, mean_q: 18.903557
 66500/100000: episode: 1657, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.998, mean reward: 0.499 [0.469, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.197, 10.100], loss: 0.014705, mae: 0.139545, mean_q: 19.915512
 66502/100000: episode: 1658, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.867, mean reward: 0.433 [0.423, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.251, 10.100], loss: 0.011877, mae: 0.119393, mean_q: 19.475147
 66504/100000: episode: 1659, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.817, mean reward: 0.409 [0.403, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.319, 10.100], loss: 0.004754, mae: 0.077353, mean_q: 19.587051
 66506/100000: episode: 1660, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.924, mean reward: 0.462 [0.432, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.252, 10.100], loss: 0.006905, mae: 0.095448, mean_q: 18.564407
 66508/100000: episode: 1661, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.116, mean reward: 0.558 [0.534, 0.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.252, 10.100], loss: 0.011090, mae: 0.113678, mean_q: 21.708912
 66510/100000: episode: 1662, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.982, mean reward: 0.491 [0.473, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.250, 10.100], loss: 0.006564, mae: 0.095182, mean_q: 21.191980
 66512/100000: episode: 1663, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.570, mean reward: 0.285 [0.278, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.207, 10.100], loss: 0.007370, mae: 0.083557, mean_q: 18.826710
 66514/100000: episode: 1664, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.021, mean reward: 0.511 [0.468, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.216, 10.100], loss: 0.008908, mae: 0.105898, mean_q: 20.267052
 66516/100000: episode: 1665, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.763, mean reward: 0.381 [0.298, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.343, 10.100], loss: 0.007929, mae: 0.094993, mean_q: 17.651932
 66518/100000: episode: 1666, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.073, mean reward: 0.537 [0.535, 0.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.279, 10.100], loss: 0.006727, mae: 0.092720, mean_q: 19.573898
 66520/100000: episode: 1667, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.761, mean reward: 0.380 [0.330, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.199, 10.100], loss: 0.006324, mae: 0.090373, mean_q: 21.351582
 66522/100000: episode: 1668, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.799, mean reward: 0.400 [0.370, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.276, 10.100], loss: 0.007152, mae: 0.092743, mean_q: 20.055580
 66524/100000: episode: 1669, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.611, mean reward: 0.305 [0.280, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.197, 10.100], loss: 0.011812, mae: 0.128463, mean_q: 18.755573
 66526/100000: episode: 1670, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.505, mean reward: 0.253 [0.220, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.264, 10.100], loss: 0.011354, mae: 0.113608, mean_q: 18.757641
 66528/100000: episode: 1671, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.640, mean reward: 0.320 [0.316, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.221, 10.100], loss: 0.010062, mae: 0.118478, mean_q: 18.041824
 66530/100000: episode: 1672, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.609, mean reward: 0.305 [0.277, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.288, 10.100], loss: 0.011938, mae: 0.114744, mean_q: 20.468594
 66532/100000: episode: 1673, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.920, mean reward: 0.460 [0.422, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.321, 10.100], loss: 0.008039, mae: 0.098251, mean_q: 18.819313
 66534/100000: episode: 1674, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.646, mean reward: 0.323 [0.319, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.264, 10.100], loss: 0.007261, mae: 0.091504, mean_q: 18.855572
 66536/100000: episode: 1675, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.618, mean reward: 0.309 [0.303, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.242, 10.100], loss: 0.011907, mae: 0.113383, mean_q: 20.573193
 66538/100000: episode: 1676, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.139, mean reward: 0.569 [0.539, 0.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.333, 10.100], loss: 0.008153, mae: 0.101441, mean_q: 16.346600
 66540/100000: episode: 1677, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.749, mean reward: 0.375 [0.338, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.262, 10.100], loss: 0.009112, mae: 0.099254, mean_q: 19.381561
 66542/100000: episode: 1678, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.835, mean reward: 0.417 [0.413, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.261, 10.100], loss: 0.010817, mae: 0.116485, mean_q: 18.927696
 66544/100000: episode: 1679, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.844, mean reward: 0.422 [0.394, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.342, 10.100], loss: 0.008992, mae: 0.098586, mean_q: 21.138092
 66546/100000: episode: 1680, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.694, mean reward: 0.347 [0.326, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.321, 10.100], loss: 0.009798, mae: 0.106669, mean_q: 18.800037
 66548/100000: episode: 1681, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.539, mean reward: 0.270 [0.265, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.226, 10.100], loss: 0.008765, mae: 0.104516, mean_q: 19.317312
 66550/100000: episode: 1682, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 1.088, mean reward: 0.544 [0.525, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.262, 10.100], loss: 0.010121, mae: 0.117519, mean_q: 16.874756
 66552/100000: episode: 1683, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.675, mean reward: 0.337 [0.321, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.361, 10.100], loss: 0.012030, mae: 0.119757, mean_q: 21.645550
 66554/100000: episode: 1684, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.831, mean reward: 0.416 [0.361, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.274, 10.100], loss: 0.009522, mae: 0.113314, mean_q: 17.559317
 66556/100000: episode: 1685, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.696, mean reward: 0.348 [0.329, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.294, 10.100], loss: 0.008584, mae: 0.099405, mean_q: 20.278875
 66558/100000: episode: 1686, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.857, mean reward: 0.429 [0.390, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.322, 10.100], loss: 0.010051, mae: 0.112196, mean_q: 21.069878
 66560/100000: episode: 1687, duration: 0.022s, episode steps: 2, steps per second: 93, episode reward: 0.638, mean reward: 0.319 [0.307, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.227, 10.100], loss: 0.011055, mae: 0.112809, mean_q: 16.906742
 66562/100000: episode: 1688, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.555, mean reward: 0.278 [0.249, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.754, 10.100], loss: 0.014014, mae: 0.128556, mean_q: 21.118082
 66564/100000: episode: 1689, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.890, mean reward: 0.445 [0.430, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.277, 10.100], loss: 0.012197, mae: 0.134566, mean_q: 18.245026
 66566/100000: episode: 1690, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.967, mean reward: 0.483 [0.474, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.354, 10.100], loss: 0.011182, mae: 0.125319, mean_q: 19.676296
 66568/100000: episode: 1691, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 1.051, mean reward: 0.525 [0.439, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.291, 10.100], loss: 0.007355, mae: 0.096411, mean_q: 18.454075
 66570/100000: episode: 1692, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.997, mean reward: 0.498 [0.475, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.190, 10.100], loss: 0.010629, mae: 0.115626, mean_q: 18.314074
 66572/100000: episode: 1693, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.731, mean reward: 0.366 [0.324, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.324, 10.100], loss: 0.008243, mae: 0.104761, mean_q: 16.614410
 66574/100000: episode: 1694, duration: 0.022s, episode steps: 2, steps per second: 90, episode reward: 0.590, mean reward: 0.295 [0.285, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.259, 10.100], loss: 0.008128, mae: 0.102919, mean_q: 19.347965
 66576/100000: episode: 1695, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 1.129, mean reward: 0.565 [0.518, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.217, 10.100], loss: 0.006652, mae: 0.090307, mean_q: 18.961754
 66578/100000: episode: 1696, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.704, mean reward: 0.352 [0.349, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.216, 10.100], loss: 0.007571, mae: 0.099455, mean_q: 19.038925
 66580/100000: episode: 1697, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.032, mean reward: 0.516 [0.493, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.195, 10.100], loss: 0.010056, mae: 0.111778, mean_q: 20.115662
 66582/100000: episode: 1698, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.929, mean reward: 0.465 [0.445, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.222, 10.100], loss: 0.015899, mae: 0.148732, mean_q: 21.257860
 66584/100000: episode: 1699, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.498, mean reward: 0.249 [0.226, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.256, 10.100], loss: 0.008678, mae: 0.107543, mean_q: 18.142120
 66586/100000: episode: 1700, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.495, mean reward: 0.247 [0.238, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.252, 10.100], loss: 0.010380, mae: 0.116246, mean_q: 19.298683
 66588/100000: episode: 1701, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.933, mean reward: 0.466 [0.442, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.218, 10.100], loss: 0.008626, mae: 0.091976, mean_q: 19.318470
 66590/100000: episode: 1702, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.864, mean reward: 0.432 [0.379, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.326, 10.100], loss: 0.008628, mae: 0.101825, mean_q: 18.400219
 66592/100000: episode: 1703, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.701, mean reward: 0.351 [0.283, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.242, 10.100], loss: 0.007500, mae: 0.093840, mean_q: 20.813158
 66594/100000: episode: 1704, duration: 0.022s, episode steps: 2, steps per second: 93, episode reward: 0.973, mean reward: 0.486 [0.463, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.278, 10.100], loss: 0.005540, mae: 0.084858, mean_q: 20.110151
 66596/100000: episode: 1705, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.932, mean reward: 0.466 [0.440, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.136, 10.100], loss: 0.006691, mae: 0.097741, mean_q: 18.255589
 66598/100000: episode: 1706, duration: 0.021s, episode steps: 2, steps per second: 98, episode reward: 1.009, mean reward: 0.505 [0.494, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.224, 10.100], loss: 0.009952, mae: 0.092632, mean_q: 18.149271
 66600/100000: episode: 1707, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.663, mean reward: 0.332 [0.208, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.321, 10.100], loss: 0.008795, mae: 0.102843, mean_q: 19.220730
 66602/100000: episode: 1708, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.847, mean reward: 0.424 [0.410, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.343, 10.100], loss: 0.005977, mae: 0.085320, mean_q: 19.182318
 66604/100000: episode: 1709, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.897, mean reward: 0.449 [0.439, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.152, 10.100], loss: 0.007969, mae: 0.093810, mean_q: 19.229069
 66606/100000: episode: 1710, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 1.138, mean reward: 0.569 [0.555, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.189, 10.100], loss: 0.009504, mae: 0.093348, mean_q: 20.177788
 66608/100000: episode: 1711, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.892, mean reward: 0.446 [0.441, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.335, 10.100], loss: 0.006481, mae: 0.085649, mean_q: 20.218985
 66610/100000: episode: 1712, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.729, mean reward: 0.364 [0.363, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.295, 10.100], loss: 0.007248, mae: 0.101644, mean_q: 21.292568
 66612/100000: episode: 1713, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.695, mean reward: 0.347 [0.333, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.207, 10.100], loss: 0.005820, mae: 0.088354, mean_q: 19.140644
 66614/100000: episode: 1714, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.676, mean reward: 0.338 [0.334, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.275, 10.100], loss: 0.009250, mae: 0.103416, mean_q: 18.832176
 66616/100000: episode: 1715, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.600, mean reward: 0.300 [0.296, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.669, 10.100], loss: 0.010047, mae: 0.101797, mean_q: 18.172636
 66618/100000: episode: 1716, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.877, mean reward: 0.438 [0.409, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.298, 10.100], loss: 0.012588, mae: 0.114278, mean_q: 18.078323
 66620/100000: episode: 1717, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.150, mean reward: 0.575 [0.573, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.213, 10.100], loss: 0.010563, mae: 0.114714, mean_q: 19.671204
 66622/100000: episode: 1718, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.743, mean reward: 0.371 [0.340, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.347, 10.100], loss: 0.012640, mae: 0.120351, mean_q: 19.919876
 66624/100000: episode: 1719, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 1.088, mean reward: 0.544 [0.518, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.190, 10.100], loss: 0.006778, mae: 0.095381, mean_q: 17.068132
 66626/100000: episode: 1720, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.698, mean reward: 0.349 [0.321, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.286, 10.100], loss: 0.008291, mae: 0.100403, mean_q: 20.121191
 66628/100000: episode: 1721, duration: 0.023s, episode steps: 2, steps per second: 88, episode reward: 1.237, mean reward: 0.619 [0.609, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.268, 10.100], loss: 0.005534, mae: 0.085600, mean_q: 18.895277
 66630/100000: episode: 1722, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.729, mean reward: 0.365 [0.349, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.227, 10.100], loss: 0.006951, mae: 0.090684, mean_q: 17.439007
 66632/100000: episode: 1723, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.892, mean reward: 0.446 [0.422, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.299, 10.100], loss: 0.006329, mae: 0.089040, mean_q: 17.237801
 66634/100000: episode: 1724, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.791, mean reward: 0.396 [0.347, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.352, 10.100], loss: 0.008396, mae: 0.103704, mean_q: 19.710247
 66636/100000: episode: 1725, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.979, mean reward: 0.490 [0.470, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.229, 10.100], loss: 0.008044, mae: 0.098585, mean_q: 18.801949
 66638/100000: episode: 1726, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.692, mean reward: 0.346 [0.334, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.239, 10.100], loss: 0.013270, mae: 0.130443, mean_q: 19.245483
 66640/100000: episode: 1727, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.662, mean reward: 0.331 [0.330, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.218, 10.100], loss: 0.021670, mae: 0.129251, mean_q: 18.839870
 66642/100000: episode: 1728, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.697, mean reward: 0.349 [0.339, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.290, 10.100], loss: 0.011029, mae: 0.124139, mean_q: 18.235167
 66644/100000: episode: 1729, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 1.344, mean reward: 0.672 [0.670, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.454, 10.128], loss: 0.005891, mae: 0.082651, mean_q: 22.785408
[Info] New level: 0.14971467852592468 | Considering 10/90 traces
 66646/100000: episode: 1730, duration: 4.004s, episode steps: 2, steps per second: 0, episode reward: 0.692, mean reward: 0.346 [0.329, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.296, 10.100], loss: 0.007591, mae: 0.099618, mean_q: 19.557735
 66647/100000: episode: 1731, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.494, mean reward: 0.494 [0.494, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.336, 10.100], loss: 0.006948, mae: 0.088504, mean_q: 17.621422
 66648/100000: episode: 1732, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.363, mean reward: 0.363 [0.363, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.242, 10.100], loss: 0.004468, mae: 0.079109, mean_q: 19.077721
 66649/100000: episode: 1733, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.297, mean reward: 0.297 [0.297, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.240, 10.100], loss: 0.003547, mae: 0.070022, mean_q: 19.874481
 66650/100000: episode: 1734, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.373, mean reward: 0.373 [0.373, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.398, 10.100], loss: 0.008989, mae: 0.105210, mean_q: 18.686687
 66651/100000: episode: 1735, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.367, mean reward: 0.367 [0.367, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.292, 10.100], loss: 0.012511, mae: 0.119233, mean_q: 23.266640
 66652/100000: episode: 1736, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.201, mean reward: 0.201 [0.201, 0.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.246, 10.100], loss: 0.016291, mae: 0.134636, mean_q: 17.463625
 66653/100000: episode: 1737, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.406, 10.100], loss: 0.005471, mae: 0.085834, mean_q: 18.696037
 66654/100000: episode: 1738, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.392, mean reward: 0.392 [0.392, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.360, 10.100], loss: 0.015507, mae: 0.127133, mean_q: 20.488892
 66655/100000: episode: 1739, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.334, mean reward: 0.334 [0.334, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.343, 10.100], loss: 0.005979, mae: 0.080486, mean_q: 17.420019
 66656/100000: episode: 1740, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.534, mean reward: 0.534 [0.534, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.285, 10.100], loss: 0.008000, mae: 0.106398, mean_q: 22.314728
 66657/100000: episode: 1741, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.399, mean reward: 0.399 [0.399, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.344, 10.100], loss: 0.009613, mae: 0.103393, mean_q: 18.360407
 66658/100000: episode: 1742, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.318, mean reward: 0.318 [0.318, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.219, 10.100], loss: 0.005437, mae: 0.083136, mean_q: 17.172426
 66659/100000: episode: 1743, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.425, mean reward: 0.425 [0.425, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.354, 10.100], loss: 0.007557, mae: 0.094127, mean_q: 13.063246
 66660/100000: episode: 1744, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.492, mean reward: 0.492 [0.492, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.246, 10.100], loss: 0.008106, mae: 0.109491, mean_q: 20.308441
 66661/100000: episode: 1745, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.360, mean reward: 0.360 [0.360, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.285, 10.100], loss: 0.006739, mae: 0.093554, mean_q: 18.865278
 66662/100000: episode: 1746, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.301, mean reward: 0.301 [0.301, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.266, 10.100], loss: 0.006474, mae: 0.087904, mean_q: 20.504751
 66663/100000: episode: 1747, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.513, mean reward: 0.513 [0.513, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.236, 10.100], loss: 0.014136, mae: 0.122750, mean_q: 16.050129
 66664/100000: episode: 1748, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.385, mean reward: 0.385 [0.385, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.245, 10.100], loss: 0.005108, mae: 0.073794, mean_q: 19.166462
 66665/100000: episode: 1749, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.340, mean reward: 0.340 [0.340, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.402, 10.100], loss: 0.008247, mae: 0.102802, mean_q: 18.203920
 66666/100000: episode: 1750, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.506, mean reward: 0.506 [0.506, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.351, 10.100], loss: 0.009181, mae: 0.098770, mean_q: 19.921791
 66667/100000: episode: 1751, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.415, mean reward: 0.415 [0.415, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.320, 10.100], loss: 0.009087, mae: 0.095720, mean_q: 19.127501
 66668/100000: episode: 1752, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.284, 10.100], loss: 0.018695, mae: 0.096675, mean_q: 15.231447
 66669/100000: episode: 1753, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.267, mean reward: 0.267 [0.267, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.333, 10.100], loss: 0.006329, mae: 0.088627, mean_q: 20.266090
 66670/100000: episode: 1754, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.218, mean reward: 0.218 [0.218, 0.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.232, 10.100], loss: 0.007847, mae: 0.097949, mean_q: 18.906378
 66671/100000: episode: 1755, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.405, mean reward: 0.405 [0.405, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.286, 10.100], loss: 0.015717, mae: 0.152249, mean_q: 15.807436
 66672/100000: episode: 1756, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.417, mean reward: 0.417 [0.417, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.282, 10.100], loss: 0.007132, mae: 0.090445, mean_q: 15.741640
 66673/100000: episode: 1757, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.363, mean reward: 0.363 [0.363, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.295, 10.100], loss: 0.007816, mae: 0.094626, mean_q: 18.450840
 66674/100000: episode: 1758, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.461, mean reward: 0.461 [0.461, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.367, 10.100], loss: 0.008991, mae: 0.107638, mean_q: 19.748859
 66675/100000: episode: 1759, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.321, 10.100], loss: 0.012371, mae: 0.107819, mean_q: 17.841110
 66676/100000: episode: 1760, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.528, mean reward: 0.528 [0.528, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.001, 10.100], loss: 0.003905, mae: 0.068560, mean_q: 19.531630
 66677/100000: episode: 1761, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.399, mean reward: 0.399 [0.399, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.313, 10.100], loss: 0.014982, mae: 0.143931, mean_q: 20.835335
 66678/100000: episode: 1762, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.377, mean reward: 0.377 [0.377, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.285, 10.100], loss: 0.015860, mae: 0.134400, mean_q: 19.096884
 66679/100000: episode: 1763, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.488, mean reward: 0.488 [0.488, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.351, 10.100], loss: 0.007442, mae: 0.098136, mean_q: 15.201768
 66680/100000: episode: 1764, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.329, mean reward: 0.329 [0.329, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.270, 10.100], loss: 0.007853, mae: 0.101325, mean_q: 19.438816
 66681/100000: episode: 1765, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.406, mean reward: 0.406 [0.406, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.241, 10.100], loss: 0.007353, mae: 0.098358, mean_q: 20.844305
 66682/100000: episode: 1766, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.314, 10.100], loss: 0.008553, mae: 0.093616, mean_q: 18.031361
 66683/100000: episode: 1767, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.381, mean reward: 0.381 [0.381, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.337, 10.100], loss: 0.010723, mae: 0.122129, mean_q: 18.726185
 66684/100000: episode: 1768, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.298, 10.100], loss: 0.016358, mae: 0.142655, mean_q: 19.855751
 66685/100000: episode: 1769, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.422, mean reward: 0.422 [0.422, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.317, 10.100], loss: 0.005928, mae: 0.087422, mean_q: 16.574959
 66686/100000: episode: 1770, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.382, mean reward: 0.382 [0.382, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.300, 10.100], loss: 0.007657, mae: 0.106102, mean_q: 17.474245
 66687/100000: episode: 1771, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.351, mean reward: 0.351 [0.351, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.270, 10.100], loss: 0.014690, mae: 0.122486, mean_q: 17.422836
 66688/100000: episode: 1772, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.407, mean reward: 0.407 [0.407, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.261, 10.100], loss: 0.006861, mae: 0.101229, mean_q: 21.079811
 66689/100000: episode: 1773, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.385, mean reward: 0.385 [0.385, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.266, 10.100], loss: 0.008341, mae: 0.103628, mean_q: 19.406334
 66690/100000: episode: 1774, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.215, mean reward: 0.215 [0.215, 0.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.375 [-0.351, 10.100], loss: 0.009481, mae: 0.105015, mean_q: 15.796459
 66691/100000: episode: 1775, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.222, mean reward: 0.222 [0.222, 0.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.287, 10.100], loss: 0.008201, mae: 0.107947, mean_q: 18.545387
 66692/100000: episode: 1776, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.436, mean reward: 0.436 [0.436, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.270, 10.100], loss: 0.010006, mae: 0.112720, mean_q: 17.474003
 66693/100000: episode: 1777, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.310, mean reward: 0.310 [0.310, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.279, 10.100], loss: 0.013666, mae: 0.130253, mean_q: 16.720686
 66694/100000: episode: 1778, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.374, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.305, 10.100], loss: 0.004592, mae: 0.075408, mean_q: 14.941322
 66695/100000: episode: 1779, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.456, mean reward: 0.456 [0.456, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.270, 10.100], loss: 0.010330, mae: 0.106781, mean_q: 17.398474
 66696/100000: episode: 1780, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.432, mean reward: 0.432 [0.432, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.325, 10.100], loss: 0.005599, mae: 0.081597, mean_q: 18.782913
 66697/100000: episode: 1781, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.286, 10.100], loss: 0.008062, mae: 0.095620, mean_q: 17.945347
 66698/100000: episode: 1782, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.287, mean reward: 0.287 [0.287, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.188, 10.100], loss: 0.003377, mae: 0.064295, mean_q: 16.856014
 66699/100000: episode: 1783, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.325, 10.100], loss: 0.014703, mae: 0.119864, mean_q: 20.693335
 66700/100000: episode: 1784, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.452, mean reward: 0.452 [0.452, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.390, 10.100], loss: 0.010086, mae: 0.122213, mean_q: 19.174715
 66701/100000: episode: 1785, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.299, mean reward: 0.299 [0.299, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.374, 10.100], loss: 0.022838, mae: 0.143817, mean_q: 17.513983
 66702/100000: episode: 1786, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.381, mean reward: 0.381 [0.381, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.232, 10.100], loss: 0.009058, mae: 0.100688, mean_q: 21.329235
 66703/100000: episode: 1787, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.459, mean reward: 0.459 [0.459, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.332, 10.100], loss: 0.011142, mae: 0.112988, mean_q: 17.646873
 66704/100000: episode: 1788, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.373, mean reward: 0.373 [0.373, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.358, 10.100], loss: 0.004306, mae: 0.077446, mean_q: 20.455864
 66705/100000: episode: 1789, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.481, mean reward: 0.481 [0.481, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.047, 10.100], loss: 0.013759, mae: 0.116746, mean_q: 18.468357
 66706/100000: episode: 1790, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.341, mean reward: 0.341 [0.341, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.266, 10.100], loss: 0.011248, mae: 0.126725, mean_q: 22.901119
 66707/100000: episode: 1791, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.352, 10.100], loss: 0.015121, mae: 0.105653, mean_q: 16.114395
 66708/100000: episode: 1792, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.316, mean reward: 0.316 [0.316, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.312, 10.100], loss: 0.007784, mae: 0.097017, mean_q: 19.459341
 66709/100000: episode: 1793, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.329, mean reward: 0.329 [0.329, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.265, 10.100], loss: 0.008435, mae: 0.098262, mean_q: 18.142218
 66710/100000: episode: 1794, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.525, mean reward: 0.525 [0.525, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.240, 10.100], loss: 0.008704, mae: 0.102473, mean_q: 16.371832
 66711/100000: episode: 1795, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.309, mean reward: 0.309 [0.309, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.267, 10.100], loss: 0.021113, mae: 0.121519, mean_q: 18.158634
 66712/100000: episode: 1796, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.327, 10.100], loss: 0.007052, mae: 0.104188, mean_q: 18.776436
 66713/100000: episode: 1797, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.551, mean reward: 0.551 [0.551, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.268, 10.100], loss: 0.011671, mae: 0.107263, mean_q: 17.528450
 66714/100000: episode: 1798, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.319, 10.100], loss: 0.019822, mae: 0.126235, mean_q: 19.712687
 66715/100000: episode: 1799, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.476, mean reward: 0.476 [0.476, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.283, 10.100], loss: 0.008946, mae: 0.104112, mean_q: 20.892710
 66716/100000: episode: 1800, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.368, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.294, 10.100], loss: 0.008901, mae: 0.106243, mean_q: 16.925982
 66717/100000: episode: 1801, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.497, mean reward: 0.497 [0.497, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.340, 10.100], loss: 0.016798, mae: 0.112109, mean_q: 14.695243
 66718/100000: episode: 1802, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.547, mean reward: 0.547 [0.547, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.550, 10.100], loss: 0.009512, mae: 0.108313, mean_q: 17.996258
 66719/100000: episode: 1803, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.423, mean reward: 0.423 [0.423, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.375, 10.100], loss: 0.013180, mae: 0.117691, mean_q: 16.736919
 66720/100000: episode: 1804, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.556, mean reward: 0.556 [0.556, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.302, 10.100], loss: 0.007364, mae: 0.096316, mean_q: 18.821825
 66721/100000: episode: 1805, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.328, 10.100], loss: 0.009942, mae: 0.101078, mean_q: 19.958532
 66722/100000: episode: 1806, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.381, mean reward: 0.381 [0.381, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.294, 10.100], loss: 0.005347, mae: 0.074565, mean_q: 20.441500
 66723/100000: episode: 1807, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.294, mean reward: 0.294 [0.294, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.177, 10.100], loss: 0.010411, mae: 0.117745, mean_q: 22.041372
 66724/100000: episode: 1808, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.324, mean reward: 0.324 [0.324, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.359, 10.100], loss: 0.008996, mae: 0.114638, mean_q: 19.321556
 66725/100000: episode: 1809, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.469, mean reward: 0.469 [0.469, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.361, 10.100], loss: 0.021831, mae: 0.120114, mean_q: 17.828651
 66726/100000: episode: 1810, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.428, mean reward: 0.428 [0.428, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.332, 10.100], loss: 0.008933, mae: 0.117402, mean_q: 17.404371
 66727/100000: episode: 1811, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.430, mean reward: 0.430 [0.430, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.269, 10.100], loss: 0.012968, mae: 0.117197, mean_q: 20.178360
 66728/100000: episode: 1812, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.337, mean reward: 0.337 [0.337, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.342, 10.100], loss: 0.010642, mae: 0.113176, mean_q: 15.664704
 66729/100000: episode: 1813, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.409, mean reward: 0.409 [0.409, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.294, 10.100], loss: 0.005622, mae: 0.085007, mean_q: 17.810547
 66730/100000: episode: 1814, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.409, mean reward: 0.409 [0.409, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.281, 10.100], loss: 0.012132, mae: 0.107020, mean_q: 19.250885
 66731/100000: episode: 1815, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.520, mean reward: 0.520 [0.520, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.311, 10.100], loss: 0.008772, mae: 0.108200, mean_q: 17.661444
 66732/100000: episode: 1816, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.415, mean reward: 0.415 [0.415, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.281, 10.100], loss: 0.005786, mae: 0.081772, mean_q: 19.586973
 66733/100000: episode: 1817, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.419, mean reward: 0.419 [0.419, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.310, 10.100], loss: 0.009161, mae: 0.088099, mean_q: 19.659309
 66734/100000: episode: 1818, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.511, mean reward: 0.511 [0.511, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.346, 10.100], loss: 0.010213, mae: 0.118590, mean_q: 19.601429
 66735/100000: episode: 1819, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.496, mean reward: 0.496 [0.496, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.378, 10.100], loss: 0.009210, mae: 0.107033, mean_q: 16.872799
[Info] New level: 0.13260257244110107 | Considering 10/90 traces
 66736/100000: episode: 1820, duration: 4.071s, episode steps: 1, steps per second: 0, episode reward: 0.376, mean reward: 0.376 [0.376, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.230, 10.100], loss: 0.003792, mae: 0.065534, mean_q: 16.517765
 66737/100000: episode: 1821, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.274, mean reward: 0.274 [0.274, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.338, 10.100], loss: 0.009269, mae: 0.107954, mean_q: 19.287510
 66738/100000: episode: 1822, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.451, mean reward: 0.451 [0.451, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.365, 10.100], loss: 0.010718, mae: 0.123113, mean_q: 23.155680
 66739/100000: episode: 1823, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.562, mean reward: 0.562 [0.562, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.274, 10.100], loss: 0.008050, mae: 0.094833, mean_q: 18.068768
 66740/100000: episode: 1824, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.294, 10.100], loss: 0.019826, mae: 0.177806, mean_q: 21.391327
 66741/100000: episode: 1825, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.473, mean reward: 0.473 [0.473, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.362, 10.100], loss: 0.016706, mae: 0.153410, mean_q: 16.191427
 66742/100000: episode: 1826, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.435, mean reward: 0.435 [0.435, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.340, 10.100], loss: 0.005304, mae: 0.090426, mean_q: 17.580681
 66743/100000: episode: 1827, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.452, mean reward: 0.452 [0.452, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.431, 10.100], loss: 0.006885, mae: 0.091362, mean_q: 17.916885
 66744/100000: episode: 1828, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.285, mean reward: 0.285 [0.285, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.275, 10.100], loss: 0.009215, mae: 0.106074, mean_q: 15.617781
 66745/100000: episode: 1829, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.090, 10.100], loss: 0.008769, mae: 0.106990, mean_q: 18.993736
 66746/100000: episode: 1830, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.399, mean reward: 0.399 [0.399, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.296, 10.100], loss: 0.004833, mae: 0.081337, mean_q: 17.880924
 66747/100000: episode: 1831, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.477, mean reward: 0.477 [0.477, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.372, 10.100], loss: 0.008434, mae: 0.094954, mean_q: 19.692152
 66748/100000: episode: 1832, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.458, mean reward: 0.458 [0.458, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.324, 10.100], loss: 0.011030, mae: 0.108561, mean_q: 17.446732
 66749/100000: episode: 1833, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.292, 10.100], loss: 0.007991, mae: 0.094365, mean_q: 17.703100
 66750/100000: episode: 1834, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.504, mean reward: 0.504 [0.504, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.340, 10.100], loss: 0.006498, mae: 0.093196, mean_q: 16.273169
 66751/100000: episode: 1835, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.442, mean reward: 0.442 [0.442, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.383, 10.100], loss: 0.006669, mae: 0.091298, mean_q: 21.195742
 66752/100000: episode: 1836, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.358, mean reward: 0.358 [0.358, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.240, 10.100], loss: 0.006042, mae: 0.092073, mean_q: 17.963678
 66753/100000: episode: 1837, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.299, mean reward: 0.299 [0.299, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.909, 10.100], loss: 0.008404, mae: 0.088665, mean_q: 17.250315
 66754/100000: episode: 1838, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.235, 10.100], loss: 0.011569, mae: 0.114183, mean_q: 19.203711
 66755/100000: episode: 1839, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.326, mean reward: 0.326 [0.326, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.239, 10.100], loss: 0.005946, mae: 0.082984, mean_q: 17.725845
 66756/100000: episode: 1840, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.287, mean reward: 0.287 [0.287, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.249, 10.100], loss: 0.006339, mae: 0.086245, mean_q: 20.518877
 66757/100000: episode: 1841, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.357, mean reward: 0.357 [0.357, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.246, 10.100], loss: 0.007815, mae: 0.100035, mean_q: 20.498398
 66758/100000: episode: 1842, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.427, mean reward: 0.427 [0.427, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.367, 10.100], loss: 0.013605, mae: 0.142394, mean_q: 17.693581
 66759/100000: episode: 1843, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.448, mean reward: 0.448 [0.448, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.345, 10.100], loss: 0.022111, mae: 0.146284, mean_q: 17.560116
 66760/100000: episode: 1844, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.315, mean reward: 0.315 [0.315, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.257, 10.100], loss: 0.004661, mae: 0.080220, mean_q: 17.082058
 66761/100000: episode: 1845, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.551, mean reward: 0.551 [0.551, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.263, 10.100], loss: 0.003564, mae: 0.072511, mean_q: 16.701279
 66762/100000: episode: 1846, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.479, mean reward: 0.479 [0.479, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.269, 10.100], loss: 0.008277, mae: 0.106143, mean_q: 16.691841
 66763/100000: episode: 1847, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.490, mean reward: 0.490 [0.490, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.227, 10.100], loss: 0.007105, mae: 0.090920, mean_q: 21.133652
 66764/100000: episode: 1848, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.281, 10.100], loss: 0.010845, mae: 0.126322, mean_q: 18.641975
 66765/100000: episode: 1849, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.370, mean reward: 0.370 [0.370, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.273, 10.100], loss: 0.010676, mae: 0.111530, mean_q: 17.906826
 66766/100000: episode: 1850, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.433, mean reward: 0.433 [0.433, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.353, 10.100], loss: 0.008797, mae: 0.099088, mean_q: 20.813114
 66767/100000: episode: 1851, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.337, mean reward: 0.337 [0.337, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.220, 10.100], loss: 0.008507, mae: 0.110939, mean_q: 18.306915
 66768/100000: episode: 1852, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.277, 10.100], loss: 0.009742, mae: 0.104388, mean_q: 19.143257
 66769/100000: episode: 1853, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.381, 10.100], loss: 0.006850, mae: 0.095704, mean_q: 18.959700
 66770/100000: episode: 1854, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.457, mean reward: 0.457 [0.457, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.266, 10.100], loss: 0.005542, mae: 0.090572, mean_q: 21.600922
 66771/100000: episode: 1855, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.349, mean reward: 0.349 [0.349, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.240, 10.100], loss: 0.007419, mae: 0.101314, mean_q: 16.988140
 66772/100000: episode: 1856, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.367, mean reward: 0.367 [0.367, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.294, 10.100], loss: 0.008525, mae: 0.101878, mean_q: 19.468317
 66773/100000: episode: 1857, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.266, mean reward: 0.266 [0.266, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.351, 10.100], loss: 0.006145, mae: 0.086424, mean_q: 18.974651
 66774/100000: episode: 1858, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.515, mean reward: 0.515 [0.515, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.207, 10.100], loss: 0.010707, mae: 0.113374, mean_q: 16.842661
 66775/100000: episode: 1859, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.492, mean reward: 0.492 [0.492, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.319, 10.100], loss: 0.004081, mae: 0.072735, mean_q: 21.380165
 66776/100000: episode: 1860, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.346, mean reward: 0.346 [0.346, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.250, 10.100], loss: 0.005640, mae: 0.083914, mean_q: 19.301380
 66777/100000: episode: 1861, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.521, mean reward: 0.521 [0.521, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.259, 10.100], loss: 0.009476, mae: 0.109532, mean_q: 14.568221
 66778/100000: episode: 1862, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.349, mean reward: 0.349 [0.349, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.582, 10.100], loss: 0.006520, mae: 0.093249, mean_q: 19.740376
 66779/100000: episode: 1863, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.354, 10.100], loss: 0.009840, mae: 0.111109, mean_q: 21.037998
 66780/100000: episode: 1864, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.389, mean reward: 0.389 [0.389, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.321, 10.100], loss: 0.006556, mae: 0.083504, mean_q: 18.795315
 66781/100000: episode: 1865, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.343, 10.100], loss: 0.012228, mae: 0.106218, mean_q: 17.961525
 66782/100000: episode: 1866, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.274, mean reward: 0.274 [0.274, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.233, 10.100], loss: 0.008968, mae: 0.112654, mean_q: 19.502178
 66783/100000: episode: 1867, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.468, mean reward: 0.468 [0.468, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.331, 10.100], loss: 0.014136, mae: 0.135311, mean_q: 19.302042
 66784/100000: episode: 1868, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.479, mean reward: 0.479 [0.479, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.351, 10.100], loss: 0.007093, mae: 0.088449, mean_q: 20.022419
 66785/100000: episode: 1869, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.240, 10.100], loss: 0.011094, mae: 0.119724, mean_q: 18.943480
 66786/100000: episode: 1870, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.513, mean reward: 0.513 [0.513, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.283, 10.100], loss: 0.002971, mae: 0.066407, mean_q: 18.488541
 66787/100000: episode: 1871, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.439, mean reward: 0.439 [0.439, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.275, 10.100], loss: 0.008040, mae: 0.107319, mean_q: 17.586428
 66788/100000: episode: 1872, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.372, 10.100], loss: 0.006770, mae: 0.095110, mean_q: 16.520096
 66789/100000: episode: 1873, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.503, mean reward: 0.503 [0.503, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.279, 10.100], loss: 0.014127, mae: 0.135170, mean_q: 21.616867
 66790/100000: episode: 1874, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.299, mean reward: 0.299 [0.299, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.312, 10.100], loss: 0.014144, mae: 0.145971, mean_q: 17.989689
 66791/100000: episode: 1875, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.476, mean reward: 0.476 [0.476, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.359, 10.100], loss: 0.006881, mae: 0.088440, mean_q: 20.265806
 66792/100000: episode: 1876, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.514, mean reward: 0.514 [0.514, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.213, 10.100], loss: 0.019889, mae: 0.153690, mean_q: 18.381815
 66793/100000: episode: 1877, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.516, mean reward: 0.516 [0.516, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.292, 10.100], loss: 0.015152, mae: 0.133586, mean_q: 18.288052
 66794/100000: episode: 1878, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.507, mean reward: 0.507 [0.507, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.282, 10.100], loss: 0.006607, mae: 0.100205, mean_q: 18.607815
 66795/100000: episode: 1879, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.409, mean reward: 0.409 [0.409, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.306, 10.100], loss: 0.007076, mae: 0.085660, mean_q: 19.072411
 66796/100000: episode: 1880, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.367, mean reward: 0.367 [0.367, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.272, 10.100], loss: 0.011712, mae: 0.120192, mean_q: 16.095165
 66797/100000: episode: 1881, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.430, mean reward: 0.430 [0.430, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.316, 10.100], loss: 0.013813, mae: 0.129619, mean_q: 18.764378
 66798/100000: episode: 1882, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.403, mean reward: 0.403 [0.403, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.344, 10.100], loss: 0.013477, mae: 0.147012, mean_q: 20.091690
 66799/100000: episode: 1883, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.343, mean reward: 0.343 [0.343, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.214, 10.100], loss: 0.013495, mae: 0.115123, mean_q: 14.975607
 66800/100000: episode: 1884, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.483, mean reward: 0.483 [0.483, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.299, 10.100], loss: 0.018097, mae: 0.156472, mean_q: 16.969828
 66801/100000: episode: 1885, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.323, mean reward: 0.323 [0.323, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.595, 10.100], loss: 0.007146, mae: 0.100821, mean_q: 19.383690
 66802/100000: episode: 1886, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.334, mean reward: 0.334 [0.334, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.250, 10.100], loss: 0.007183, mae: 0.098180, mean_q: 18.794302
 66803/100000: episode: 1887, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 0.462, mean reward: 0.462 [0.462, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.333, 10.100], loss: 0.016007, mae: 0.130709, mean_q: 17.356413
 66804/100000: episode: 1888, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.567, mean reward: 0.567 [0.567, 0.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.206, 10.100], loss: 0.011451, mae: 0.127240, mean_q: 20.389317
 66805/100000: episode: 1889, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.540, mean reward: 0.540 [0.540, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.325, 10.100], loss: 0.017632, mae: 0.145030, mean_q: 19.394386
 66806/100000: episode: 1890, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.346, mean reward: 0.346 [0.346, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.346, 10.100], loss: 0.008532, mae: 0.103131, mean_q: 18.926605
 66807/100000: episode: 1891, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.383, mean reward: 0.383 [0.383, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.355, 10.100], loss: 0.006900, mae: 0.094441, mean_q: 14.382658
 66808/100000: episode: 1892, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.426, mean reward: 0.426 [0.426, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.352, 10.100], loss: 0.008818, mae: 0.109874, mean_q: 17.575962
 66809/100000: episode: 1893, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.459, mean reward: 0.459 [0.459, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.360, 10.100], loss: 0.006708, mae: 0.096466, mean_q: 20.502598
 66810/100000: episode: 1894, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.528, mean reward: 0.528 [0.528, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.520, 10.100], loss: 0.006851, mae: 0.099167, mean_q: 16.611820
 66811/100000: episode: 1895, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.515, mean reward: 0.515 [0.515, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.294, 10.100], loss: 0.007792, mae: 0.101558, mean_q: 19.549637
 66812/100000: episode: 1896, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.518, mean reward: 0.518 [0.518, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.223, 10.100], loss: 0.003622, mae: 0.067904, mean_q: 16.109838
 66813/100000: episode: 1897, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.473, mean reward: 0.473 [0.473, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.212, 10.100], loss: 0.006435, mae: 0.081493, mean_q: 17.233694
 66814/100000: episode: 1898, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.339, mean reward: 0.339 [0.339, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.269, 10.100], loss: 0.004466, mae: 0.074750, mean_q: 17.019051
 66815/100000: episode: 1899, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.432, mean reward: 0.432 [0.432, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.234, 10.100], loss: 0.012623, mae: 0.102994, mean_q: 18.436838
 66816/100000: episode: 1900, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.480, mean reward: 0.480 [0.480, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.321, 10.100], loss: 0.017129, mae: 0.165216, mean_q: 18.638802
 66817/100000: episode: 1901, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.288, mean reward: 0.288 [0.288, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.257, 10.100], loss: 0.009731, mae: 0.113250, mean_q: 17.225904
 66818/100000: episode: 1902, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.506, mean reward: 0.506 [0.506, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.306, 10.100], loss: 0.006845, mae: 0.092111, mean_q: 19.991961
 66819/100000: episode: 1903, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.487, mean reward: 0.487 [0.487, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.325, 10.100], loss: 0.008269, mae: 0.109073, mean_q: 17.826519
 66820/100000: episode: 1904, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.281, mean reward: 0.281 [0.281, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.283, 10.100], loss: 0.007515, mae: 0.093936, mean_q: 18.929264
 66821/100000: episode: 1905, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.370, mean reward: 0.370 [0.370, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.362, 10.100], loss: 0.006248, mae: 0.085337, mean_q: 18.452637
 66822/100000: episode: 1906, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.460, mean reward: 0.460 [0.460, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.378, 10.100], loss: 0.006365, mae: 0.089504, mean_q: 18.873856
 66823/100000: episode: 1907, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.513, mean reward: 0.513 [0.513, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.341, 10.100], loss: 0.007454, mae: 0.089768, mean_q: 19.569170
 66824/100000: episode: 1908, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.364, mean reward: 0.364 [0.364, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.297, 10.100], loss: 0.012941, mae: 0.129437, mean_q: 17.326199
 66825/100000: episode: 1909, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.490, mean reward: 0.490 [0.490, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.262, 10.100], loss: 0.017414, mae: 0.113481, mean_q: 18.058016
[Info] Not found new level, current best level reached = 0.13260257244110107
 66826/100000: episode: 1910, duration: 3.986s, episode steps: 1, steps per second: 0, episode reward: 0.310, mean reward: 0.310 [0.310, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.241, 10.100], loss: 0.005072, mae: 0.087040, mean_q: 19.698362
 66926/100000: episode: 1911, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 49.158, mean reward: 0.492 [0.244, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.033, 10.098], loss: 0.008914, mae: 0.100580, mean_q: 17.941023
 67026/100000: episode: 1912, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 53.428, mean reward: 0.534 [0.289, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.584, 10.098], loss: 0.007222, mae: 0.091861, mean_q: 18.025318
 67126/100000: episode: 1913, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 52.659, mean reward: 0.527 [0.201, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.388, 10.209], loss: 0.010520, mae: 0.113279, mean_q: 18.105726
 67226/100000: episode: 1914, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 51.493, mean reward: 0.515 [0.263, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.381, 10.098], loss: 0.007837, mae: 0.096416, mean_q: 18.107079
 67326/100000: episode: 1915, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 46.221, mean reward: 0.462 [0.225, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.480, 10.392], loss: 0.009202, mae: 0.104612, mean_q: 18.033346
 67426/100000: episode: 1916, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 52.797, mean reward: 0.528 [0.265, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.210], loss: 0.009289, mae: 0.104138, mean_q: 17.928923
 67526/100000: episode: 1917, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 52.006, mean reward: 0.520 [0.262, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.500, 10.381], loss: 0.007727, mae: 0.094583, mean_q: 18.193048
 67626/100000: episode: 1918, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 51.132, mean reward: 0.511 [0.216, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.137], loss: 0.009885, mae: 0.107252, mean_q: 18.154736
 67726/100000: episode: 1919, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 54.141, mean reward: 0.541 [0.269, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.798, 10.098], loss: 0.008506, mae: 0.099060, mean_q: 18.061403
 67826/100000: episode: 1920, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 55.477, mean reward: 0.555 [0.292, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.416, 10.098], loss: 0.009323, mae: 0.103817, mean_q: 18.171995
 67926/100000: episode: 1921, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 52.271, mean reward: 0.523 [0.247, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.635, 10.394], loss: 0.008316, mae: 0.098956, mean_q: 18.071684
 68026/100000: episode: 1922, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 55.300, mean reward: 0.553 [0.234, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.619, 10.098], loss: 0.009494, mae: 0.104092, mean_q: 17.961973
 68126/100000: episode: 1923, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 52.295, mean reward: 0.523 [0.275, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.758, 10.304], loss: 0.009717, mae: 0.109083, mean_q: 17.756319
 68226/100000: episode: 1924, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 51.002, mean reward: 0.510 [0.176, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.131, 10.098], loss: 0.010207, mae: 0.109745, mean_q: 17.864992
 68326/100000: episode: 1925, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 53.823, mean reward: 0.538 [0.165, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.571, 10.248], loss: 0.008974, mae: 0.102658, mean_q: 17.942995
 68426/100000: episode: 1926, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 50.509, mean reward: 0.505 [0.235, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.747, 10.098], loss: 0.009218, mae: 0.104767, mean_q: 18.136156
 68526/100000: episode: 1927, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 48.680, mean reward: 0.487 [0.292, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.256, 10.184], loss: 0.008519, mae: 0.100312, mean_q: 18.220589
 68626/100000: episode: 1928, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.642, mean reward: 0.536 [0.279, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.644, 10.098], loss: 0.010441, mae: 0.110015, mean_q: 18.164389
 68726/100000: episode: 1929, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 46.491, mean reward: 0.465 [0.167, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.113, 10.293], loss: 0.008898, mae: 0.103638, mean_q: 17.972654
 68826/100000: episode: 1930, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 49.600, mean reward: 0.496 [0.191, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.563, 10.098], loss: 0.009711, mae: 0.109353, mean_q: 18.103828
 68926/100000: episode: 1931, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 52.670, mean reward: 0.527 [0.247, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.833, 10.098], loss: 0.007537, mae: 0.093632, mean_q: 17.863117
 69026/100000: episode: 1932, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 49.246, mean reward: 0.492 [0.206, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.003, 10.180], loss: 0.008737, mae: 0.101955, mean_q: 17.765690
 69126/100000: episode: 1933, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 53.366, mean reward: 0.534 [0.334, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.281, 10.161], loss: 0.009498, mae: 0.107243, mean_q: 17.901369
 69226/100000: episode: 1934, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: 47.644, mean reward: 0.476 [0.141, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.861, 10.215], loss: 0.007097, mae: 0.092151, mean_q: 18.001638
 69326/100000: episode: 1935, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 51.929, mean reward: 0.519 [0.309, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.506, 10.098], loss: 0.008078, mae: 0.097630, mean_q: 17.706244
 69426/100000: episode: 1936, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 55.158, mean reward: 0.552 [0.346, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.330, 10.098], loss: 0.009470, mae: 0.107147, mean_q: 17.885023
 69526/100000: episode: 1937, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 50.167, mean reward: 0.502 [0.316, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.691, 10.342], loss: 0.009026, mae: 0.105316, mean_q: 18.097923
 69626/100000: episode: 1938, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.288, mean reward: 0.573 [0.426, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.546, 10.183], loss: 0.007684, mae: 0.095598, mean_q: 17.722980
 69726/100000: episode: 1939, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 48.767, mean reward: 0.488 [0.263, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.896, 10.098], loss: 0.007852, mae: 0.097177, mean_q: 18.227606
 69826/100000: episode: 1940, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 51.283, mean reward: 0.513 [0.218, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.157, 10.098], loss: 0.007477, mae: 0.093837, mean_q: 18.075651
 69926/100000: episode: 1941, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 53.536, mean reward: 0.535 [0.290, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.290, 10.098], loss: 0.007455, mae: 0.094762, mean_q: 17.905533
 70026/100000: episode: 1942, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 57.151, mean reward: 0.572 [0.317, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.289, 10.121], loss: 0.009206, mae: 0.106539, mean_q: 18.104307
 70126/100000: episode: 1943, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 50.698, mean reward: 0.507 [0.319, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.120, 10.098], loss: 0.007378, mae: 0.093848, mean_q: 17.635592
 70226/100000: episode: 1944, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 51.430, mean reward: 0.514 [0.204, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.661, 10.241], loss: 0.007666, mae: 0.095377, mean_q: 18.072205
 70326/100000: episode: 1945, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 53.501, mean reward: 0.535 [0.300, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.931, 10.340], loss: 0.007281, mae: 0.092793, mean_q: 17.627508
 70426/100000: episode: 1946, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 45.996, mean reward: 0.460 [0.261, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.518, 10.098], loss: 0.008998, mae: 0.105101, mean_q: 18.002722
 70526/100000: episode: 1947, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 49.933, mean reward: 0.499 [0.177, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.714, 10.245], loss: 0.008977, mae: 0.103845, mean_q: 17.839727
 70626/100000: episode: 1948, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 50.193, mean reward: 0.502 [0.197, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.139, 10.098], loss: 0.008384, mae: 0.100225, mean_q: 17.958855
 70726/100000: episode: 1949, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 53.710, mean reward: 0.537 [0.309, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.380, 10.233], loss: 0.011098, mae: 0.116021, mean_q: 18.179810
 70826/100000: episode: 1950, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 53.263, mean reward: 0.533 [0.294, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.098], loss: 0.010340, mae: 0.111368, mean_q: 18.153267
 70926/100000: episode: 1951, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 54.865, mean reward: 0.549 [0.313, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.082, 10.133], loss: 0.008389, mae: 0.099266, mean_q: 17.576862
 71026/100000: episode: 1952, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 55.037, mean reward: 0.550 [0.359, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.930, 10.199], loss: 0.008115, mae: 0.096970, mean_q: 17.772703
 71126/100000: episode: 1953, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 45.403, mean reward: 0.454 [0.147, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.084, 10.098], loss: 0.008160, mae: 0.098720, mean_q: 17.812933
 71226/100000: episode: 1954, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 52.628, mean reward: 0.526 [0.258, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.875, 10.203], loss: 0.008421, mae: 0.099105, mean_q: 18.166492
 71326/100000: episode: 1955, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 46.466, mean reward: 0.465 [0.228, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.427, 10.098], loss: 0.008786, mae: 0.101776, mean_q: 18.441259
 71426/100000: episode: 1956, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 53.327, mean reward: 0.533 [0.339, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.741, 10.284], loss: 0.009456, mae: 0.105007, mean_q: 18.712292
 71526/100000: episode: 1957, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 54.461, mean reward: 0.545 [0.275, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.429, 10.145], loss: 0.008398, mae: 0.099100, mean_q: 18.659571
 71626/100000: episode: 1958, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: 50.001, mean reward: 0.500 [0.150, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.795, 10.098], loss: 0.007620, mae: 0.097221, mean_q: 18.876575
 71726/100000: episode: 1959, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 56.344, mean reward: 0.563 [0.335, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.252, 10.098], loss: 0.009655, mae: 0.105458, mean_q: 19.061665
 71826/100000: episode: 1960, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 55.678, mean reward: 0.557 [0.346, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.816, 10.140], loss: 0.009750, mae: 0.109250, mean_q: 19.145472
 71926/100000: episode: 1961, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 53.834, mean reward: 0.538 [0.202, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.585, 10.098], loss: 0.008913, mae: 0.102571, mean_q: 19.097157
 72026/100000: episode: 1962, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 54.881, mean reward: 0.549 [0.313, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.638, 10.212], loss: 0.008839, mae: 0.101607, mean_q: 19.103386
 72126/100000: episode: 1963, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 55.914, mean reward: 0.559 [0.258, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.918, 10.184], loss: 0.008315, mae: 0.099387, mean_q: 19.368355
 72226/100000: episode: 1964, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.477, mean reward: 0.565 [0.217, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.668, 10.141], loss: 0.008504, mae: 0.099138, mean_q: 19.300360
 72326/100000: episode: 1965, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 52.481, mean reward: 0.525 [0.310, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.159, 10.144], loss: 0.008850, mae: 0.102332, mean_q: 19.258240
 72426/100000: episode: 1966, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 54.557, mean reward: 0.546 [0.272, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.785, 10.272], loss: 0.010062, mae: 0.108452, mean_q: 19.394272
 72526/100000: episode: 1967, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 53.367, mean reward: 0.534 [0.336, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.006, 10.098], loss: 0.009679, mae: 0.108109, mean_q: 19.157774
 72626/100000: episode: 1968, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 54.531, mean reward: 0.545 [0.336, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.418, 10.098], loss: 0.007691, mae: 0.095790, mean_q: 19.231012
 72726/100000: episode: 1969, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 55.294, mean reward: 0.553 [0.369, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.874, 10.157], loss: 0.008462, mae: 0.099425, mean_q: 19.350754
 72826/100000: episode: 1970, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 53.201, mean reward: 0.532 [0.242, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.572, 10.098], loss: 0.008764, mae: 0.103231, mean_q: 19.236046
 72926/100000: episode: 1971, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 53.768, mean reward: 0.538 [0.297, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.530, 10.251], loss: 0.008909, mae: 0.102582, mean_q: 19.116175
 73026/100000: episode: 1972, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 56.529, mean reward: 0.565 [0.350, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.269, 10.162], loss: 0.010308, mae: 0.112444, mean_q: 19.378376
 73126/100000: episode: 1973, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 53.173, mean reward: 0.532 [0.293, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.836, 10.113], loss: 0.009311, mae: 0.105779, mean_q: 19.045235
 73226/100000: episode: 1974, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 55.166, mean reward: 0.552 [0.341, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.095, 10.109], loss: 0.008525, mae: 0.101237, mean_q: 19.390736
 73326/100000: episode: 1975, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 55.225, mean reward: 0.552 [0.266, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.836, 10.135], loss: 0.009162, mae: 0.104694, mean_q: 19.162300
 73426/100000: episode: 1976, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.225, mean reward: 0.582 [0.364, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.535, 10.165], loss: 0.008954, mae: 0.102231, mean_q: 19.072910
 73526/100000: episode: 1977, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 54.273, mean reward: 0.543 [0.272, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.062, 10.098], loss: 0.009268, mae: 0.104267, mean_q: 19.401545
 73626/100000: episode: 1978, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 55.279, mean reward: 0.553 [0.286, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.834, 10.481], loss: 0.008758, mae: 0.100407, mean_q: 19.364012
 73726/100000: episode: 1979, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 55.610, mean reward: 0.556 [0.325, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.989, 10.128], loss: 0.008396, mae: 0.099739, mean_q: 19.129778
 73826/100000: episode: 1980, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 46.444, mean reward: 0.464 [0.265, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.922, 10.391], loss: 0.008704, mae: 0.101803, mean_q: 19.115877
 73926/100000: episode: 1981, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 54.338, mean reward: 0.543 [0.175, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.369, 10.216], loss: 0.008369, mae: 0.100718, mean_q: 19.491932
 74026/100000: episode: 1982, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 54.561, mean reward: 0.546 [0.248, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.527, 10.098], loss: 0.008611, mae: 0.100756, mean_q: 19.396688
 74126/100000: episode: 1983, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 56.084, mean reward: 0.561 [0.354, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.669, 10.098], loss: 0.009949, mae: 0.109195, mean_q: 19.265165
 74226/100000: episode: 1984, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 51.453, mean reward: 0.515 [0.323, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.255, 10.260], loss: 0.008409, mae: 0.098588, mean_q: 19.090670
 74326/100000: episode: 1985, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 53.745, mean reward: 0.537 [0.339, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.451, 10.098], loss: 0.007827, mae: 0.096764, mean_q: 19.216965
 74426/100000: episode: 1986, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 55.199, mean reward: 0.552 [0.322, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.538, 10.098], loss: 0.007364, mae: 0.092637, mean_q: 19.564957
 74526/100000: episode: 1987, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 54.503, mean reward: 0.545 [0.315, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.840, 10.098], loss: 0.007781, mae: 0.096366, mean_q: 19.152250
 74626/100000: episode: 1988, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 53.249, mean reward: 0.532 [0.297, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.738, 10.263], loss: 0.008799, mae: 0.099471, mean_q: 19.530872
 74726/100000: episode: 1989, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 53.253, mean reward: 0.533 [0.353, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.405, 10.339], loss: 0.006906, mae: 0.088897, mean_q: 19.527458
 74826/100000: episode: 1990, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 55.412, mean reward: 0.554 [0.331, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.804, 10.111], loss: 0.007339, mae: 0.094039, mean_q: 19.092073
 74926/100000: episode: 1991, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 54.142, mean reward: 0.541 [0.276, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.104, 10.098], loss: 0.007021, mae: 0.090632, mean_q: 19.054815
 75026/100000: episode: 1992, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 52.833, mean reward: 0.528 [0.304, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.431, 10.098], loss: 0.008215, mae: 0.097567, mean_q: 19.113476
 75126/100000: episode: 1993, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 54.303, mean reward: 0.543 [0.314, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.603, 10.098], loss: 0.008028, mae: 0.098098, mean_q: 19.017479
 75226/100000: episode: 1994, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 52.181, mean reward: 0.522 [0.232, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.136, 10.367], loss: 0.007411, mae: 0.092824, mean_q: 19.194094
 75326/100000: episode: 1995, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 53.915, mean reward: 0.539 [0.313, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.952, 10.098], loss: 0.009392, mae: 0.105281, mean_q: 19.398209
 75426/100000: episode: 1996, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 55.814, mean reward: 0.558 [0.349, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.737, 10.098], loss: 0.007138, mae: 0.091700, mean_q: 19.390003
 75526/100000: episode: 1997, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 55.033, mean reward: 0.550 [0.370, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.132, 10.207], loss: 0.008562, mae: 0.099623, mean_q: 19.709692
 75626/100000: episode: 1998, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 52.165, mean reward: 0.522 [0.297, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.998, 10.257], loss: 0.007040, mae: 0.092134, mean_q: 19.330944
 75726/100000: episode: 1999, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 49.710, mean reward: 0.497 [0.203, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.787, 10.098], loss: 0.006908, mae: 0.089615, mean_q: 19.410864
 75826/100000: episode: 2000, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 52.309, mean reward: 0.523 [0.300, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.391, 10.278], loss: 0.007517, mae: 0.094519, mean_q: 19.508505
 75926/100000: episode: 2001, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 54.995, mean reward: 0.550 [0.225, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.577, 10.098], loss: 0.006916, mae: 0.090781, mean_q: 19.438515
 76026/100000: episode: 2002, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 54.031, mean reward: 0.540 [0.346, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.036, 10.252], loss: 0.007484, mae: 0.094484, mean_q: 19.274090
 76126/100000: episode: 2003, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 55.749, mean reward: 0.557 [0.247, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.934, 10.205], loss: 0.006880, mae: 0.090057, mean_q: 19.434637
 76226/100000: episode: 2004, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 51.220, mean reward: 0.512 [0.316, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.991, 10.098], loss: 0.008033, mae: 0.098206, mean_q: 19.503080
 76326/100000: episode: 2005, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 53.972, mean reward: 0.540 [0.316, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.823, 10.174], loss: 0.007989, mae: 0.096646, mean_q: 19.712164
 76426/100000: episode: 2006, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 55.938, mean reward: 0.559 [0.374, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.160, 10.118], loss: 0.006635, mae: 0.089133, mean_q: 19.550037
 76526/100000: episode: 2007, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 52.009, mean reward: 0.520 [0.316, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.146, 10.204], loss: 0.007303, mae: 0.093277, mean_q: 19.353436
 76626/100000: episode: 2008, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 55.289, mean reward: 0.553 [0.318, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.540, 10.177], loss: 0.007618, mae: 0.091183, mean_q: 19.624199
 76726/100000: episode: 2009, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 49.039, mean reward: 0.490 [0.159, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.038, 10.141], loss: 0.007480, mae: 0.091522, mean_q: 19.465538
[Info] New level: 0.18640786409378052 | Considering 10/90 traces
 76826/100000: episode: 2010, duration: 4.492s, episode steps: 100, steps per second: 22, episode reward: 56.166, mean reward: 0.562 [0.383, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.894, 10.098], loss: 0.007494, mae: 0.095328, mean_q: 19.710381
 76828/100000: episode: 2011, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.530, mean reward: 0.265 [0.250, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.293, 10.100], loss: 0.004835, mae: 0.078981, mean_q: 21.103630
 76830/100000: episode: 2012, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.633, mean reward: 0.316 [0.309, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.345, 10.100], loss: 0.007952, mae: 0.098597, mean_q: 19.884615
 76832/100000: episode: 2013, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.094, mean reward: 0.547 [0.500, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.252, 10.100], loss: 0.007624, mae: 0.101797, mean_q: 21.431334
 76834/100000: episode: 2014, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.813, mean reward: 0.407 [0.385, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.300, 10.100], loss: 0.005799, mae: 0.084149, mean_q: 19.383635
 76836/100000: episode: 2015, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.832, mean reward: 0.416 [0.407, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.318, 10.100], loss: 0.006428, mae: 0.087435, mean_q: 20.249809
 76838/100000: episode: 2016, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.143, mean reward: 0.571 [0.550, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.188, 10.100], loss: 0.004812, mae: 0.081376, mean_q: 21.762238
 76840/100000: episode: 2017, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.640, mean reward: 0.320 [0.311, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.201, 10.100], loss: 0.005481, mae: 0.081764, mean_q: 20.626772
 76842/100000: episode: 2018, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.831, mean reward: 0.415 [0.401, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.239, 10.100], loss: 0.005785, mae: 0.085439, mean_q: 18.418001
 76844/100000: episode: 2019, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.904, mean reward: 0.452 [0.437, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.232, 10.100], loss: 0.006481, mae: 0.094321, mean_q: 19.473473
 76846/100000: episode: 2020, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.814, mean reward: 0.407 [0.400, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.334, 10.100], loss: 0.006739, mae: 0.090056, mean_q: 19.588255
 76848/100000: episode: 2021, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.850, mean reward: 0.425 [0.409, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.231, 10.100], loss: 0.006655, mae: 0.090302, mean_q: 19.034418
 76850/100000: episode: 2022, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.734, mean reward: 0.367 [0.361, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.320, 10.100], loss: 0.006611, mae: 0.091125, mean_q: 19.653114
 76852/100000: episode: 2023, duration: 0.015s, episode steps: 2, steps per second: 129, episode reward: 1.052, mean reward: 0.526 [0.506, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.124, 10.100], loss: 0.014840, mae: 0.117750, mean_q: 18.122061
 76854/100000: episode: 2024, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.754, mean reward: 0.377 [0.375, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.268, 10.100], loss: 0.004919, mae: 0.078205, mean_q: 18.870724
 76856/100000: episode: 2025, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.968, mean reward: 0.484 [0.436, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.319, 10.100], loss: 0.004562, mae: 0.076711, mean_q: 19.916130
 76858/100000: episode: 2026, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.885, mean reward: 0.443 [0.407, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.225, 10.100], loss: 0.010520, mae: 0.099454, mean_q: 18.452366
 76860/100000: episode: 2027, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.937, mean reward: 0.468 [0.412, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.341, 10.100], loss: 0.009135, mae: 0.108681, mean_q: 18.092617
 76862/100000: episode: 2028, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.978, mean reward: 0.489 [0.450, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.295, 10.100], loss: 0.007741, mae: 0.103393, mean_q: 18.977863
 76864/100000: episode: 2029, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.985, mean reward: 0.492 [0.476, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.213, 10.100], loss: 0.009668, mae: 0.119412, mean_q: 17.813065
 76866/100000: episode: 2030, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.758, mean reward: 0.379 [0.354, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.238, 10.100], loss: 0.010426, mae: 0.102455, mean_q: 18.708096
 76868/100000: episode: 2031, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.891, mean reward: 0.446 [0.444, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.188, 10.100], loss: 0.010176, mae: 0.124589, mean_q: 20.423065
 76870/100000: episode: 2032, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.421, mean reward: 0.211 [0.171, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.332, 10.100], loss: 0.012377, mae: 0.110002, mean_q: 17.569168
 76872/100000: episode: 2033, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.704, mean reward: 0.352 [0.287, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.926, 10.100], loss: 0.009390, mae: 0.103317, mean_q: 16.469564
 76874/100000: episode: 2034, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.577, mean reward: 0.288 [0.259, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.659, 10.100], loss: 0.012337, mae: 0.125772, mean_q: 17.487217
 76876/100000: episode: 2035, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.792, mean reward: 0.396 [0.386, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.147, 10.100], loss: 0.006631, mae: 0.090265, mean_q: 21.039406
 76878/100000: episode: 2036, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.702, mean reward: 0.351 [0.346, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.381, 10.100], loss: 0.009089, mae: 0.106461, mean_q: 20.754190
 76880/100000: episode: 2037, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.816, mean reward: 0.408 [0.401, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.163, 10.100], loss: 0.009629, mae: 0.091633, mean_q: 21.161119
 76882/100000: episode: 2038, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.540, mean reward: 0.270 [0.262, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.349, 10.100], loss: 0.008819, mae: 0.103397, mean_q: 16.361080
 76884/100000: episode: 2039, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.483, mean reward: 0.241 [0.229, 0.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.282, 10.100], loss: 0.007272, mae: 0.095412, mean_q: 15.597702
 76886/100000: episode: 2040, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.736, mean reward: 0.368 [0.365, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.217, 10.100], loss: 0.005186, mae: 0.081783, mean_q: 18.454708
 76888/100000: episode: 2041, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.506, mean reward: 0.253 [0.185, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.277, 10.100], loss: 0.006082, mae: 0.090514, mean_q: 20.027615
 76890/100000: episode: 2042, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.770, mean reward: 0.385 [0.338, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.230, 10.100], loss: 0.008482, mae: 0.095515, mean_q: 17.827139
 76892/100000: episode: 2043, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.854, mean reward: 0.427 [0.409, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.275, 10.100], loss: 0.006388, mae: 0.098371, mean_q: 21.696573
 76894/100000: episode: 2044, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.713, mean reward: 0.357 [0.342, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.257, 10.100], loss: 0.005001, mae: 0.078575, mean_q: 21.028408
 76896/100000: episode: 2045, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.798, mean reward: 0.399 [0.390, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.186, 10.100], loss: 0.004433, mae: 0.077258, mean_q: 19.744432
 76898/100000: episode: 2046, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.740, mean reward: 0.370 [0.343, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.202, 10.100], loss: 0.006005, mae: 0.086851, mean_q: 18.015347
 76900/100000: episode: 2047, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.684, mean reward: 0.342 [0.311, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.278, 10.100], loss: 0.006060, mae: 0.090159, mean_q: 17.183231
 76902/100000: episode: 2048, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.457, mean reward: 0.228 [0.226, 0.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.242, 10.100], loss: 0.004175, mae: 0.077071, mean_q: 21.129448
 76904/100000: episode: 2049, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.658, mean reward: 0.329 [0.299, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.297, 10.100], loss: 0.003762, mae: 0.069230, mean_q: 19.589191
 76906/100000: episode: 2050, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.008, mean reward: 0.504 [0.497, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.285, 10.100], loss: 0.006348, mae: 0.089430, mean_q: 19.428829
 76908/100000: episode: 2051, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.942, mean reward: 0.471 [0.461, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.255, 10.100], loss: 0.004588, mae: 0.078258, mean_q: 18.523655
 76910/100000: episode: 2052, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.909, mean reward: 0.455 [0.437, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.703, 10.100], loss: 0.006153, mae: 0.087833, mean_q: 19.150986
 76912/100000: episode: 2053, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.801, mean reward: 0.400 [0.376, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.275, 10.100], loss: 0.005419, mae: 0.088882, mean_q: 21.912827
 76914/100000: episode: 2054, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.612, mean reward: 0.306 [0.287, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.341, 10.100], loss: 0.004383, mae: 0.074053, mean_q: 19.835579
 76916/100000: episode: 2055, duration: 0.015s, episode steps: 2, steps per second: 129, episode reward: 0.802, mean reward: 0.401 [0.395, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.306, 10.100], loss: 0.004645, mae: 0.080073, mean_q: 20.240551
 76918/100000: episode: 2056, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.670, mean reward: 0.335 [0.331, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.230, 10.100], loss: 0.009614, mae: 0.091989, mean_q: 20.578007
 76920/100000: episode: 2057, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.884, mean reward: 0.442 [0.417, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.195, 10.100], loss: 0.006627, mae: 0.088323, mean_q: 18.826265
 76922/100000: episode: 2058, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.663, mean reward: 0.332 [0.288, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.192, 10.100], loss: 0.005895, mae: 0.089625, mean_q: 17.736782
 76924/100000: episode: 2059, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 1.099, mean reward: 0.550 [0.522, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.178, 10.100], loss: 0.005603, mae: 0.079239, mean_q: 20.285130
 76926/100000: episode: 2060, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.730, mean reward: 0.365 [0.339, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.255, 10.100], loss: 0.012745, mae: 0.118694, mean_q: 18.053619
 76928/100000: episode: 2061, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.842, mean reward: 0.421 [0.417, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.288, 10.100], loss: 0.009721, mae: 0.100163, mean_q: 19.538757
 76930/100000: episode: 2062, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.322, mean reward: 0.161 [0.155, 0.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.298, 10.100], loss: 0.005555, mae: 0.089572, mean_q: 19.614788
 76932/100000: episode: 2063, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.630, mean reward: 0.315 [0.302, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.330, 10.100], loss: 0.007525, mae: 0.096112, mean_q: 20.911873
 76934/100000: episode: 2064, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.686, mean reward: 0.343 [0.337, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.316, 10.100], loss: 0.003174, mae: 0.065262, mean_q: 18.858459
 76936/100000: episode: 2065, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.924, mean reward: 0.462 [0.461, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.142, 10.100], loss: 0.007884, mae: 0.096760, mean_q: 20.081406
 76938/100000: episode: 2066, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.910, mean reward: 0.455 [0.441, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-1.497, 10.100], loss: 0.012861, mae: 0.109493, mean_q: 18.678028
 76940/100000: episode: 2067, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.908, mean reward: 0.454 [0.453, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.303, 10.100], loss: 0.013029, mae: 0.136323, mean_q: 18.470057
 76942/100000: episode: 2068, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.592, mean reward: 0.296 [0.149, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.239, 10.100], loss: 0.013915, mae: 0.116104, mean_q: 18.789719
 76944/100000: episode: 2069, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.766, mean reward: 0.383 [0.381, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.210, 10.100], loss: 0.008237, mae: 0.102062, mean_q: 16.972919
 76946/100000: episode: 2070, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.533, mean reward: 0.266 [0.229, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.269, 10.100], loss: 0.011488, mae: 0.125706, mean_q: 18.270733
 76948/100000: episode: 2071, duration: 0.021s, episode steps: 2, steps per second: 97, episode reward: 0.899, mean reward: 0.449 [0.438, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.279, 10.100], loss: 0.011612, mae: 0.132223, mean_q: 15.692996
 76950/100000: episode: 2072, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.828, mean reward: 0.414 [0.365, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.226, 10.100], loss: 0.006150, mae: 0.087823, mean_q: 18.026886
 76952/100000: episode: 2073, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.874, mean reward: 0.437 [0.427, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.269, 10.100], loss: 0.007962, mae: 0.098179, mean_q: 19.639708
 76954/100000: episode: 2074, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.616, mean reward: 0.308 [0.269, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.290, 10.100], loss: 0.007762, mae: 0.098322, mean_q: 17.409548
 76956/100000: episode: 2075, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.831, mean reward: 0.415 [0.399, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.252, 10.100], loss: 0.009656, mae: 0.103591, mean_q: 18.571949
 76958/100000: episode: 2076, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.792, mean reward: 0.396 [0.372, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.284, 10.100], loss: 0.012112, mae: 0.118429, mean_q: 17.754477
 76960/100000: episode: 2077, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.956, mean reward: 0.478 [0.413, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.264, 10.100], loss: 0.009592, mae: 0.115773, mean_q: 19.894369
 76962/100000: episode: 2078, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.925, mean reward: 0.462 [0.439, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.217, 10.100], loss: 0.007242, mae: 0.096477, mean_q: 19.330114
 76964/100000: episode: 2079, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.843, mean reward: 0.422 [0.379, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.278, 10.100], loss: 0.006823, mae: 0.087192, mean_q: 19.120571
 76966/100000: episode: 2080, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.771, mean reward: 0.385 [0.382, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.314, 10.100], loss: 0.005584, mae: 0.086497, mean_q: 19.966274
 76968/100000: episode: 2081, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.589, mean reward: 0.295 [0.282, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.214, 10.100], loss: 0.005449, mae: 0.083117, mean_q: 19.380375
 76970/100000: episode: 2082, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 1.094, mean reward: 0.547 [0.522, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.147, 10.100], loss: 0.014388, mae: 0.134478, mean_q: 19.040154
 76972/100000: episode: 2083, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.690, mean reward: 0.345 [0.272, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.246, 10.100], loss: 0.009304, mae: 0.105359, mean_q: 20.506165
 76974/100000: episode: 2084, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.734, mean reward: 0.367 [0.350, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.287, 10.100], loss: 0.011227, mae: 0.125587, mean_q: 18.250797
 76976/100000: episode: 2085, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.758, mean reward: 0.379 [0.369, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.147, 10.100], loss: 0.012047, mae: 0.114804, mean_q: 18.638958
 76978/100000: episode: 2086, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.816, mean reward: 0.408 [0.397, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.256, 10.100], loss: 0.011505, mae: 0.118219, mean_q: 19.858288
 76980/100000: episode: 2087, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.849, mean reward: 0.425 [0.371, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.249, 10.100], loss: 0.006275, mae: 0.088203, mean_q: 18.444521
 76982/100000: episode: 2088, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.836, mean reward: 0.418 [0.408, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.301, 10.100], loss: 0.008547, mae: 0.100466, mean_q: 21.179323
 76984/100000: episode: 2089, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.932, mean reward: 0.466 [0.464, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.156, 10.100], loss: 0.013763, mae: 0.133610, mean_q: 18.610691
 76986/100000: episode: 2090, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.909, mean reward: 0.455 [0.406, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.233, 10.100], loss: 0.011138, mae: 0.117705, mean_q: 17.319836
 76988/100000: episode: 2091, duration: 0.023s, episode steps: 2, steps per second: 89, episode reward: 0.966, mean reward: 0.483 [0.463, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.309, 10.100], loss: 0.009274, mae: 0.120618, mean_q: 18.202234
 76990/100000: episode: 2092, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.853, mean reward: 0.426 [0.426, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.447, 10.100], loss: 0.008779, mae: 0.103738, mean_q: 19.380402
 76992/100000: episode: 2093, duration: 0.021s, episode steps: 2, steps per second: 96, episode reward: 0.647, mean reward: 0.324 [0.292, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.273, 10.100], loss: 0.005770, mae: 0.084793, mean_q: 21.130899
 76994/100000: episode: 2094, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.840, mean reward: 0.420 [0.419, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.624, 10.100], loss: 0.010813, mae: 0.117943, mean_q: 17.639679
 76996/100000: episode: 2095, duration: 0.014s, episode steps: 2, steps per second: 138, episode reward: 0.676, mean reward: 0.338 [0.304, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.286, 10.100], loss: 0.005039, mae: 0.082037, mean_q: 19.004557
 76998/100000: episode: 2096, duration: 0.023s, episode steps: 2, steps per second: 89, episode reward: 0.832, mean reward: 0.416 [0.409, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.245, 10.100], loss: 0.006486, mae: 0.086128, mean_q: 19.810890
 77000/100000: episode: 2097, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 0.891, mean reward: 0.446 [0.433, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.432, 10.100], loss: 0.007500, mae: 0.101365, mean_q: 18.326981
 77002/100000: episode: 2098, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.766, mean reward: 0.383 [0.332, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.570, 10.100], loss: 0.008980, mae: 0.107569, mean_q: 18.743813
 77004/100000: episode: 2099, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.661, mean reward: 0.330 [0.310, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.194, 10.100], loss: 0.008484, mae: 0.100879, mean_q: 18.938644
[Info] New level: 0.05348485708236694 | Considering 10/90 traces
 77006/100000: episode: 2100, duration: 4.042s, episode steps: 2, steps per second: 0, episode reward: 0.677, mean reward: 0.338 [0.329, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.196, 10.100], loss: 0.011211, mae: 0.113905, mean_q: 19.310001
 77007/100000: episode: 2101, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.236, mean reward: 0.236 [0.236, 0.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.276, 10.100], loss: 0.010025, mae: 0.112337, mean_q: 22.357361
 77008/100000: episode: 2102, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.331, mean reward: 0.331 [0.331, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.324, 10.100], loss: 0.004303, mae: 0.076729, mean_q: 20.179665
 77009/100000: episode: 2103, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.414, mean reward: 0.414 [0.414, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.277, 10.100], loss: 0.009732, mae: 0.102867, mean_q: 21.561035
 77010/100000: episode: 2104, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.207, mean reward: 0.207 [0.207, 0.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.324, 10.100], loss: 0.010562, mae: 0.120305, mean_q: 20.915611
 77011/100000: episode: 2105, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.307, mean reward: 0.307 [0.307, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.311, 10.100], loss: 0.008005, mae: 0.102618, mean_q: 17.882179
 77012/100000: episode: 2106, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.374, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.199, 10.100], loss: 0.005450, mae: 0.084605, mean_q: 18.306007
 77013/100000: episode: 2107, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.369, mean reward: 0.369 [0.369, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.272, 10.100], loss: 0.004695, mae: 0.080744, mean_q: 18.893911
 77014/100000: episode: 2108, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.337, mean reward: 0.337 [0.337, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.357, 10.100], loss: 0.008851, mae: 0.104542, mean_q: 17.700825
 77015/100000: episode: 2109, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.292, mean reward: 0.292 [0.292, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.346, 10.100], loss: 0.004881, mae: 0.088997, mean_q: 18.320133
 77016/100000: episode: 2110, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.218, mean reward: 0.218 [0.218, 0.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.293, 10.100], loss: 0.004925, mae: 0.076033, mean_q: 17.391737
 77017/100000: episode: 2111, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.423, mean reward: 0.423 [0.423, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.209, 10.100], loss: 0.003694, mae: 0.070259, mean_q: 20.156555
 77018/100000: episode: 2112, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.176, mean reward: 0.176 [0.176, 0.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.227, 10.100], loss: 0.012341, mae: 0.128719, mean_q: 19.477547
 77019/100000: episode: 2113, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.334, mean reward: 0.334 [0.334, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.226, 10.100], loss: 0.013107, mae: 0.087988, mean_q: 16.820990
 77020/100000: episode: 2114, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.304, 10.100], loss: 0.008809, mae: 0.108132, mean_q: 18.409573
 77021/100000: episode: 2115, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.227, 10.100], loss: 0.007613, mae: 0.097200, mean_q: 18.589436
 77022/100000: episode: 2116, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.406, mean reward: 0.406 [0.406, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.341, 10.100], loss: 0.005255, mae: 0.080149, mean_q: 19.415001
 77023/100000: episode: 2117, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.260, 10.100], loss: 0.006533, mae: 0.086643, mean_q: 17.830116
 77024/100000: episode: 2118, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 0.263, mean reward: 0.263 [0.263, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.287, 10.100], loss: 0.008516, mae: 0.110720, mean_q: 20.471027
 77025/100000: episode: 2119, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.334, mean reward: 0.334 [0.334, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.305, 10.100], loss: 0.008560, mae: 0.103636, mean_q: 19.812111
 77026/100000: episode: 2120, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.300, mean reward: 0.300 [0.300, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.311, 10.100], loss: 0.005036, mae: 0.080534, mean_q: 19.312704
 77027/100000: episode: 2121, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.436, mean reward: 0.436 [0.436, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.291, 10.100], loss: 0.007797, mae: 0.105443, mean_q: 16.227772
 77028/100000: episode: 2122, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.259, 10.100], loss: 0.019043, mae: 0.099279, mean_q: 16.966612
 77029/100000: episode: 2123, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.343, mean reward: 0.343 [0.343, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.246, 10.100], loss: 0.007734, mae: 0.095565, mean_q: 16.531265
 77030/100000: episode: 2124, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.421, mean reward: 0.421 [0.421, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.199, 10.100], loss: 0.010475, mae: 0.119627, mean_q: 19.491722
 77031/100000: episode: 2125, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.237, 10.100], loss: 0.005182, mae: 0.086230, mean_q: 18.354000
 77032/100000: episode: 2126, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.234, 10.100], loss: 0.006135, mae: 0.089184, mean_q: 19.682487
 77033/100000: episode: 2127, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.306, mean reward: 0.306 [0.306, 0.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.209, 10.100], loss: 0.018670, mae: 0.160986, mean_q: 20.573524
 77034/100000: episode: 2128, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.230, mean reward: 0.230 [0.230, 0.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.311, 10.100], loss: 0.009264, mae: 0.104497, mean_q: 18.193993
 77035/100000: episode: 2129, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.287, mean reward: 0.287 [0.287, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.315, 10.100], loss: 0.008394, mae: 0.108781, mean_q: 19.082588
 77036/100000: episode: 2130, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.364, mean reward: 0.364 [0.364, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.296, 10.100], loss: 0.023523, mae: 0.193998, mean_q: 19.014826
 77037/100000: episode: 2131, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.336, mean reward: 0.336 [0.336, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.299, 10.100], loss: 0.028666, mae: 0.212852, mean_q: 18.188677
 77038/100000: episode: 2132, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.430, mean reward: 0.430 [0.430, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.320, 10.100], loss: 0.004540, mae: 0.077730, mean_q: 20.814423
 77039/100000: episode: 2133, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.274, mean reward: 0.274 [0.274, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.301, 10.100], loss: 0.008228, mae: 0.107144, mean_q: 17.829317
 77040/100000: episode: 2134, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.330, mean reward: 0.330 [0.330, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.221, 10.100], loss: 0.022358, mae: 0.188872, mean_q: 15.236042
 77041/100000: episode: 2135, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.242, mean reward: 0.242 [0.242, 0.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.236, 10.100], loss: 0.017212, mae: 0.155341, mean_q: 20.550652
 77042/100000: episode: 2136, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.399, mean reward: 0.399 [0.399, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.203, 10.100], loss: 0.007426, mae: 0.087891, mean_q: 18.507133
 77043/100000: episode: 2137, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.304, 10.100], loss: 0.011063, mae: 0.118346, mean_q: 17.606720
 77044/100000: episode: 2138, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.347, mean reward: 0.347 [0.347, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.316, 10.100], loss: 0.022795, mae: 0.112740, mean_q: 19.837334
 77045/100000: episode: 2139, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.217, mean reward: 0.217 [0.217, 0.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.295, 10.100], loss: 0.004562, mae: 0.076945, mean_q: 20.615383
 77046/100000: episode: 2140, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.451, mean reward: 0.451 [0.451, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.317, 10.100], loss: 0.010166, mae: 0.104130, mean_q: 19.282913
 77047/100000: episode: 2141, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.412, mean reward: 0.412 [0.412, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.238, 10.100], loss: 0.006962, mae: 0.088715, mean_q: 22.108160
 77048/100000: episode: 2142, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.341, 10.100], loss: 0.005609, mae: 0.082541, mean_q: 17.847191
 77049/100000: episode: 2143, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.278, mean reward: 0.278 [0.278, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.373, 10.100], loss: 0.006791, mae: 0.099364, mean_q: 19.372702
 77050/100000: episode: 2144, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.245, 10.100], loss: 0.010164, mae: 0.108180, mean_q: 14.243513
 77051/100000: episode: 2145, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.442, mean reward: 0.442 [0.442, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.353, 10.100], loss: 0.004617, mae: 0.077967, mean_q: 16.306599
 77052/100000: episode: 2146, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.414, mean reward: 0.414 [0.414, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.300, 10.100], loss: 0.005944, mae: 0.091105, mean_q: 17.995361
 77053/100000: episode: 2147, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.368, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.279, 10.100], loss: 0.005240, mae: 0.076222, mean_q: 19.743862
 77054/100000: episode: 2148, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.225, mean reward: 0.225 [0.225, 0.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.292, 10.100], loss: 0.022593, mae: 0.096151, mean_q: 17.703707
 77055/100000: episode: 2149, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.253, 10.100], loss: 0.005842, mae: 0.086611, mean_q: 20.577324
 77056/100000: episode: 2150, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.322, mean reward: 0.322 [0.322, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.317, 10.100], loss: 0.002355, mae: 0.054836, mean_q: 19.326645
 77057/100000: episode: 2151, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.304, 10.100], loss: 0.006574, mae: 0.091284, mean_q: 18.565142
 77058/100000: episode: 2152, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.370, mean reward: 0.370 [0.370, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.233, 10.100], loss: 0.008520, mae: 0.098193, mean_q: 18.687666
 77059/100000: episode: 2153, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.282, mean reward: 0.282 [0.282, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.337, 10.100], loss: 0.010189, mae: 0.112707, mean_q: 17.993473
 77060/100000: episode: 2154, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.292, mean reward: 0.292 [0.292, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.304, 10.100], loss: 0.007658, mae: 0.099645, mean_q: 17.644705
 77061/100000: episode: 2155, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.365, mean reward: 0.365 [0.365, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.260, 10.100], loss: 0.005876, mae: 0.094358, mean_q: 19.848625
 77062/100000: episode: 2156, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.402, mean reward: 0.402 [0.402, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.280, 10.100], loss: 0.005798, mae: 0.088232, mean_q: 17.672157
 77063/100000: episode: 2157, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.274, mean reward: 0.274 [0.274, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.355, 10.100], loss: 0.016786, mae: 0.111096, mean_q: 18.033482
 77064/100000: episode: 2158, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.254, mean reward: 0.254 [0.254, 0.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.269, 10.100], loss: 0.003851, mae: 0.074287, mean_q: 18.777426
 77065/100000: episode: 2159, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.334, mean reward: 0.334 [0.334, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.282, 10.100], loss: 0.008950, mae: 0.104703, mean_q: 19.844383
 77066/100000: episode: 2160, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.188, mean reward: 0.188 [0.188, 0.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.348, 10.100], loss: 0.009408, mae: 0.106690, mean_q: 20.504333
 77067/100000: episode: 2161, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.234, mean reward: 0.234 [0.234, 0.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.365, 10.100], loss: 0.007723, mae: 0.100573, mean_q: 17.843735
 77068/100000: episode: 2162, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.203, mean reward: 0.203 [0.203, 0.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.300, 10.100], loss: 0.005188, mae: 0.082654, mean_q: 19.044491
 77069/100000: episode: 2163, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.283, mean reward: 0.283 [0.283, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.381, 10.100], loss: 0.005616, mae: 0.088241, mean_q: 18.928299
 77070/100000: episode: 2164, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.470, mean reward: 0.470 [0.470, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.339, 10.100], loss: 0.006205, mae: 0.082768, mean_q: 18.586117
 77071/100000: episode: 2165, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.200, 10.100], loss: 0.004012, mae: 0.072953, mean_q: 22.903475
 77072/100000: episode: 2166, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.421, mean reward: 0.421 [0.421, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.265, 10.100], loss: 0.004056, mae: 0.076611, mean_q: 15.818663
 77073/100000: episode: 2167, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.357, mean reward: 0.357 [0.357, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.333, 10.100], loss: 0.006229, mae: 0.087077, mean_q: 14.693299
 77074/100000: episode: 2168, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.376, mean reward: 0.376 [0.376, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.207, 10.100], loss: 0.006898, mae: 0.090988, mean_q: 17.476822
 77075/100000: episode: 2169, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.292, mean reward: 0.292 [0.292, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.327, 10.100], loss: 0.006143, mae: 0.094512, mean_q: 21.408035
 77076/100000: episode: 2170, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.257, mean reward: 0.257 [0.257, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.255, 10.100], loss: 0.004624, mae: 0.071209, mean_q: 20.204775
 77077/100000: episode: 2171, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.265, mean reward: 0.265 [0.265, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.324, 10.100], loss: 0.003266, mae: 0.060831, mean_q: 20.513493
 77078/100000: episode: 2172, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.229, mean reward: 0.229 [0.229, 0.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.292, 10.100], loss: 0.006859, mae: 0.087747, mean_q: 16.380264
 77079/100000: episode: 2173, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.335, mean reward: 0.335 [0.335, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.315, 10.100], loss: 0.009494, mae: 0.105185, mean_q: 17.740137
 77080/100000: episode: 2174, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.237, mean reward: 0.237 [0.237, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.370, 10.100], loss: 0.004416, mae: 0.076114, mean_q: 17.188501
 77081/100000: episode: 2175, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.328, mean reward: 0.328 [0.328, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.318, 10.100], loss: 0.004723, mae: 0.078473, mean_q: 17.495161
 77082/100000: episode: 2176, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.344, mean reward: 0.344 [0.344, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.223, 10.100], loss: 0.005281, mae: 0.086011, mean_q: 20.450905
 77083/100000: episode: 2177, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.315, mean reward: 0.315 [0.315, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.267, 10.100], loss: 0.006993, mae: 0.104647, mean_q: 19.111683
 77084/100000: episode: 2178, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.342, mean reward: 0.342 [0.342, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.256, 10.100], loss: 0.003623, mae: 0.065695, mean_q: 18.309563
 77085/100000: episode: 2179, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.398, mean reward: 0.398 [0.398, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.232, 10.100], loss: 0.008029, mae: 0.099823, mean_q: 19.079987
 77086/100000: episode: 2180, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.234, mean reward: 0.234 [0.234, 0.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.251, 10.100], loss: 0.008216, mae: 0.107585, mean_q: 17.717382
 77087/100000: episode: 2181, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.251, 10.100], loss: 0.003502, mae: 0.068534, mean_q: 19.569969
 77088/100000: episode: 2182, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.458, mean reward: 0.458 [0.458, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.298, 10.100], loss: 0.007426, mae: 0.090237, mean_q: 18.153263
 77089/100000: episode: 2183, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.431, mean reward: 0.431 [0.431, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.252, 10.100], loss: 0.006918, mae: 0.094814, mean_q: 19.054203
 77090/100000: episode: 2184, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.271, mean reward: 0.271 [0.271, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.268, 10.100], loss: 0.006283, mae: 0.092071, mean_q: 20.413593
 77091/100000: episode: 2185, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.345, mean reward: 0.345 [0.345, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.282, 10.100], loss: 0.007616, mae: 0.096181, mean_q: 23.417753
 77092/100000: episode: 2186, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.250, mean reward: 0.250 [0.250, 0.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.217, 10.100], loss: 0.007638, mae: 0.085200, mean_q: 19.797607
 77093/100000: episode: 2187, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.245, mean reward: 0.245 [0.245, 0.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.321, 10.100], loss: 0.004355, mae: 0.070468, mean_q: 19.578510
 77094/100000: episode: 2188, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.274, 10.100], loss: 0.009022, mae: 0.104226, mean_q: 19.687346
 77095/100000: episode: 2189, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.315, mean reward: 0.315 [0.315, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.348, 10.100], loss: 0.015508, mae: 0.105519, mean_q: 19.503805
[Info] Not found new level, current best level reached = 0.05348485708236694
 77096/100000: episode: 2190, duration: 4.028s, episode steps: 1, steps per second: 0, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.367, 10.100], loss: 0.006391, mae: 0.088871, mean_q: 22.161493
 77196/100000: episode: 2191, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 45.937, mean reward: 0.459 [0.181, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.548, 10.098], loss: 0.007441, mae: 0.093963, mean_q: 18.578636
 77296/100000: episode: 2192, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 56.354, mean reward: 0.564 [0.315, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.450, 10.098], loss: 0.007207, mae: 0.091653, mean_q: 18.789583
 77396/100000: episode: 2193, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 52.080, mean reward: 0.521 [0.284, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.068, 10.347], loss: 0.007168, mae: 0.091353, mean_q: 19.046949
 77496/100000: episode: 2194, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 55.537, mean reward: 0.555 [0.232, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.502, 10.268], loss: 0.008282, mae: 0.098941, mean_q: 18.843500
 77596/100000: episode: 2195, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 50.597, mean reward: 0.506 [0.215, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.988, 10.154], loss: 0.007430, mae: 0.090110, mean_q: 18.618959
 77696/100000: episode: 2196, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 53.853, mean reward: 0.539 [0.314, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.438, 10.098], loss: 0.008127, mae: 0.094664, mean_q: 18.649553
 77796/100000: episode: 2197, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 55.477, mean reward: 0.555 [0.277, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.475, 10.098], loss: 0.007269, mae: 0.089706, mean_q: 18.788446
 77896/100000: episode: 2198, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 52.491, mean reward: 0.525 [0.292, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.055, 10.098], loss: 0.008061, mae: 0.094130, mean_q: 18.948225
 77996/100000: episode: 2199, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 54.130, mean reward: 0.541 [0.299, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.337, 10.098], loss: 0.007954, mae: 0.094197, mean_q: 18.577839
 78096/100000: episode: 2200, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.816, mean reward: 0.588 [0.425, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.513, 10.251], loss: 0.008002, mae: 0.094546, mean_q: 18.609507
 78196/100000: episode: 2201, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 50.444, mean reward: 0.504 [0.309, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.341, 10.098], loss: 0.008017, mae: 0.094265, mean_q: 18.466768
 78296/100000: episode: 2202, duration: 0.473s, episode steps: 100, steps per second: 211, episode reward: 54.838, mean reward: 0.548 [0.201, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.379, 10.152], loss: 0.007410, mae: 0.090902, mean_q: 18.475286
 78396/100000: episode: 2203, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 54.204, mean reward: 0.542 [0.257, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.912, 10.098], loss: 0.010528, mae: 0.110815, mean_q: 19.077988
 78496/100000: episode: 2204, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 53.182, mean reward: 0.532 [0.356, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.787, 10.098], loss: 0.008953, mae: 0.102668, mean_q: 18.444944
 78596/100000: episode: 2205, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 49.358, mean reward: 0.494 [0.236, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.011, 10.270], loss: 0.008352, mae: 0.095398, mean_q: 18.635624
 78696/100000: episode: 2206, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 53.213, mean reward: 0.532 [0.322, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.690, 10.098], loss: 0.008330, mae: 0.096225, mean_q: 18.970142
 78796/100000: episode: 2207, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 52.130, mean reward: 0.521 [0.272, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.275, 10.098], loss: 0.010603, mae: 0.110067, mean_q: 18.807581
 78896/100000: episode: 2208, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 52.640, mean reward: 0.526 [0.297, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.568, 10.111], loss: 0.006827, mae: 0.087538, mean_q: 18.580410
 78996/100000: episode: 2209, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 51.930, mean reward: 0.519 [0.214, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.156, 10.195], loss: 0.008327, mae: 0.095566, mean_q: 19.080614
 79096/100000: episode: 2210, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 52.044, mean reward: 0.520 [0.191, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.610, 10.188], loss: 0.008643, mae: 0.096552, mean_q: 18.761034
 79196/100000: episode: 2211, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 54.375, mean reward: 0.544 [0.209, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.127, 10.140], loss: 0.008992, mae: 0.097183, mean_q: 18.563990
 79296/100000: episode: 2212, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 58.110, mean reward: 0.581 [0.318, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.774, 10.098], loss: 0.009663, mae: 0.101615, mean_q: 18.853741
 79396/100000: episode: 2213, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 52.028, mean reward: 0.520 [0.319, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.622, 10.139], loss: 0.008525, mae: 0.096484, mean_q: 18.578083
 79496/100000: episode: 2214, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 49.429, mean reward: 0.494 [0.243, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.044, 10.422], loss: 0.009306, mae: 0.098206, mean_q: 18.694826
 79596/100000: episode: 2215, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 52.133, mean reward: 0.521 [0.232, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.181, 10.098], loss: 0.008680, mae: 0.096818, mean_q: 18.920984
 79696/100000: episode: 2216, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 54.701, mean reward: 0.547 [0.298, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.713, 10.166], loss: 0.008315, mae: 0.096585, mean_q: 18.681782
 79796/100000: episode: 2217, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 53.606, mean reward: 0.536 [0.305, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.092, 10.167], loss: 0.009568, mae: 0.100522, mean_q: 18.725744
 79896/100000: episode: 2218, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 52.310, mean reward: 0.523 [0.302, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.897, 10.150], loss: 0.009110, mae: 0.099501, mean_q: 18.995298
 79996/100000: episode: 2219, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.516, mean reward: 0.575 [0.342, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.549, 10.098], loss: 0.009837, mae: 0.101612, mean_q: 18.974480
 80096/100000: episode: 2220, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 54.133, mean reward: 0.541 [0.270, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.609, 10.169], loss: 0.008285, mae: 0.097616, mean_q: 18.926615
 80196/100000: episode: 2221, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: 43.547, mean reward: 0.435 [0.203, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.002, 10.412], loss: 0.009579, mae: 0.100708, mean_q: 18.974049
 80296/100000: episode: 2222, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 53.964, mean reward: 0.540 [0.375, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.065, 10.189], loss: 0.010865, mae: 0.106593, mean_q: 18.827833
 80396/100000: episode: 2223, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 52.560, mean reward: 0.526 [0.331, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.432, 10.098], loss: 0.011865, mae: 0.110179, mean_q: 18.773052
 80496/100000: episode: 2224, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 49.098, mean reward: 0.491 [0.296, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.623, 10.218], loss: 0.009873, mae: 0.101049, mean_q: 18.779417
 80596/100000: episode: 2225, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 49.220, mean reward: 0.492 [0.198, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.969, 10.098], loss: 0.012050, mae: 0.114209, mean_q: 18.665705
 80696/100000: episode: 2226, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 56.844, mean reward: 0.568 [0.328, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.420, 10.165], loss: 0.009083, mae: 0.097276, mean_q: 18.834473
 80796/100000: episode: 2227, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 53.038, mean reward: 0.530 [0.217, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.356, 10.211], loss: 0.009985, mae: 0.107118, mean_q: 18.745516
 80896/100000: episode: 2228, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 52.577, mean reward: 0.526 [0.315, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.815, 10.137], loss: 0.010870, mae: 0.104759, mean_q: 18.708149
 80996/100000: episode: 2229, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 53.283, mean reward: 0.533 [0.279, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.649, 10.118], loss: 0.010456, mae: 0.104792, mean_q: 18.721403
 81096/100000: episode: 2230, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 55.652, mean reward: 0.557 [0.243, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.331, 10.098], loss: 0.008659, mae: 0.094119, mean_q: 18.728306
 81196/100000: episode: 2231, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 50.577, mean reward: 0.506 [0.220, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.206, 10.098], loss: 0.009428, mae: 0.102787, mean_q: 18.534845
 81296/100000: episode: 2232, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 53.142, mean reward: 0.531 [0.252, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.096, 10.098], loss: 0.012809, mae: 0.116220, mean_q: 18.541973
 81396/100000: episode: 2233, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 53.891, mean reward: 0.539 [0.319, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.756, 10.098], loss: 0.010835, mae: 0.106035, mean_q: 18.527275
 81496/100000: episode: 2234, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 54.115, mean reward: 0.541 [0.288, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.609, 10.098], loss: 0.009229, mae: 0.097387, mean_q: 18.821348
 81596/100000: episode: 2235, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 55.879, mean reward: 0.559 [0.363, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.881, 10.324], loss: 0.009081, mae: 0.098593, mean_q: 18.638090
 81696/100000: episode: 2236, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 54.332, mean reward: 0.543 [0.348, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.641, 10.270], loss: 0.009958, mae: 0.100149, mean_q: 18.988438
 81796/100000: episode: 2237, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 52.126, mean reward: 0.521 [0.312, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.988, 10.183], loss: 0.008436, mae: 0.097717, mean_q: 19.226862
 81896/100000: episode: 2238, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 53.203, mean reward: 0.532 [0.256, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.233, 10.197], loss: 0.010674, mae: 0.102365, mean_q: 19.570644
 81996/100000: episode: 2239, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 52.980, mean reward: 0.530 [0.303, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.804, 10.098], loss: 0.013611, mae: 0.121187, mean_q: 19.168009
 82096/100000: episode: 2240, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 51.836, mean reward: 0.518 [0.265, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.614, 10.098], loss: 0.011087, mae: 0.112449, mean_q: 19.717365
 82196/100000: episode: 2241, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 51.687, mean reward: 0.517 [0.366, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.088, 10.236], loss: 0.009840, mae: 0.099338, mean_q: 19.699547
 82296/100000: episode: 2242, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 50.627, mean reward: 0.506 [0.179, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.698, 10.117], loss: 0.008848, mae: 0.096749, mean_q: 19.835772
 82396/100000: episode: 2243, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 44.683, mean reward: 0.447 [0.072, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.798, 10.192], loss: 0.009124, mae: 0.099528, mean_q: 19.613094
 82496/100000: episode: 2244, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 55.035, mean reward: 0.550 [0.365, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.655, 10.307], loss: 0.009821, mae: 0.101504, mean_q: 19.590940
 82596/100000: episode: 2245, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 57.267, mean reward: 0.573 [0.380, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.586, 10.106], loss: 0.008670, mae: 0.097519, mean_q: 19.873066
 82696/100000: episode: 2246, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 55.110, mean reward: 0.551 [0.208, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.700, 10.269], loss: 0.011968, mae: 0.114873, mean_q: 19.751520
 82796/100000: episode: 2247, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 55.690, mean reward: 0.557 [0.354, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.743, 10.179], loss: 0.010549, mae: 0.106197, mean_q: 19.824646
 82896/100000: episode: 2248, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 53.879, mean reward: 0.539 [0.278, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.330, 10.244], loss: 0.010135, mae: 0.103678, mean_q: 19.491850
 82996/100000: episode: 2249, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 52.231, mean reward: 0.522 [0.171, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.068, 10.231], loss: 0.011036, mae: 0.107893, mean_q: 19.724777
 83096/100000: episode: 2250, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 53.138, mean reward: 0.531 [0.182, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.883, 10.098], loss: 0.014870, mae: 0.131132, mean_q: 19.549074
 83196/100000: episode: 2251, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 55.808, mean reward: 0.558 [0.296, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.284, 10.098], loss: 0.009450, mae: 0.103119, mean_q: 20.051481
 83296/100000: episode: 2252, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 54.142, mean reward: 0.541 [0.144, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.509, 10.117], loss: 0.011166, mae: 0.109539, mean_q: 19.796423
 83396/100000: episode: 2253, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 49.769, mean reward: 0.498 [0.118, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.132, 10.403], loss: 0.010669, mae: 0.106472, mean_q: 20.070538
 83496/100000: episode: 2254, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 55.108, mean reward: 0.551 [0.308, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.760, 10.107], loss: 0.010544, mae: 0.103525, mean_q: 19.440191
 83596/100000: episode: 2255, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 50.016, mean reward: 0.500 [0.361, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.788, 10.098], loss: 0.010595, mae: 0.104670, mean_q: 19.512323
 83696/100000: episode: 2256, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 55.375, mean reward: 0.554 [0.299, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.443, 10.232], loss: 0.010727, mae: 0.108025, mean_q: 19.727066
 83796/100000: episode: 2257, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.805, mean reward: 0.538 [0.251, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.285, 10.098], loss: 0.012080, mae: 0.111970, mean_q: 19.694286
 83896/100000: episode: 2258, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 52.890, mean reward: 0.529 [0.192, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.625, 10.186], loss: 0.011326, mae: 0.110156, mean_q: 19.422094
 83996/100000: episode: 2259, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 51.876, mean reward: 0.519 [0.216, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.479, 10.236], loss: 0.010423, mae: 0.107137, mean_q: 19.597204
 84096/100000: episode: 2260, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 53.025, mean reward: 0.530 [0.249, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.795, 10.098], loss: 0.011964, mae: 0.115212, mean_q: 19.551062
 84196/100000: episode: 2261, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 54.780, mean reward: 0.548 [0.288, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.844, 10.325], loss: 0.009921, mae: 0.103971, mean_q: 20.006248
 84296/100000: episode: 2262, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 55.457, mean reward: 0.555 [0.280, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.129, 10.128], loss: 0.010225, mae: 0.110333, mean_q: 19.871307
 84396/100000: episode: 2263, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 51.651, mean reward: 0.517 [0.337, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.381, 10.098], loss: 0.009882, mae: 0.103826, mean_q: 19.597214
 84496/100000: episode: 2264, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 54.604, mean reward: 0.546 [0.273, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.098], loss: 0.010603, mae: 0.111525, mean_q: 19.696985
 84596/100000: episode: 2265, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 54.351, mean reward: 0.544 [0.318, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.428, 10.165], loss: 0.009165, mae: 0.101399, mean_q: 19.748310
 84696/100000: episode: 2266, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 53.836, mean reward: 0.538 [0.255, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.098], loss: 0.009439, mae: 0.106094, mean_q: 19.575289
 84796/100000: episode: 2267, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 55.255, mean reward: 0.553 [0.358, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.234, 10.265], loss: 0.010318, mae: 0.107353, mean_q: 19.792521
 84896/100000: episode: 2268, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 49.012, mean reward: 0.490 [0.172, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.203, 10.231], loss: 0.010218, mae: 0.108144, mean_q: 19.738470
 84996/100000: episode: 2269, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 50.916, mean reward: 0.509 [0.265, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.945, 10.098], loss: 0.008381, mae: 0.100228, mean_q: 19.580690
 85096/100000: episode: 2270, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 50.405, mean reward: 0.504 [0.149, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.017, 10.098], loss: 0.011324, mae: 0.114670, mean_q: 20.133310
 85196/100000: episode: 2271, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 52.981, mean reward: 0.530 [0.224, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.697, 10.199], loss: 0.011375, mae: 0.113964, mean_q: 19.741484
 85296/100000: episode: 2272, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 55.271, mean reward: 0.553 [0.335, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.524, 10.098], loss: 0.009375, mae: 0.103683, mean_q: 19.531004
 85396/100000: episode: 2273, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 54.008, mean reward: 0.540 [0.357, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.012, 10.211], loss: 0.009661, mae: 0.104478, mean_q: 19.594526
 85496/100000: episode: 2274, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 53.232, mean reward: 0.532 [0.281, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.784, 10.352], loss: 0.011034, mae: 0.113530, mean_q: 19.638157
 85596/100000: episode: 2275, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 51.342, mean reward: 0.513 [0.262, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.236, 10.098], loss: 0.010007, mae: 0.105575, mean_q: 19.599867
 85696/100000: episode: 2276, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 53.563, mean reward: 0.536 [0.259, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.724, 10.098], loss: 0.008728, mae: 0.100593, mean_q: 19.439840
 85796/100000: episode: 2277, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 52.319, mean reward: 0.523 [0.254, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.615, 10.098], loss: 0.010010, mae: 0.105296, mean_q: 19.767628
 85896/100000: episode: 2278, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 51.389, mean reward: 0.514 [0.295, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.052, 10.098], loss: 0.009590, mae: 0.105387, mean_q: 19.670622
 85996/100000: episode: 2279, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 53.499, mean reward: 0.535 [0.247, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.801, 10.171], loss: 0.011385, mae: 0.114217, mean_q: 19.728804
 86096/100000: episode: 2280, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 56.007, mean reward: 0.560 [0.317, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.511, 10.166], loss: 0.009754, mae: 0.104441, mean_q: 19.615765
 86196/100000: episode: 2281, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 52.000, mean reward: 0.520 [0.272, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.719, 10.280], loss: 0.009110, mae: 0.103720, mean_q: 19.658796
 86296/100000: episode: 2282, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 55.953, mean reward: 0.560 [0.198, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.095, 10.189], loss: 0.008797, mae: 0.101126, mean_q: 19.500200
 86396/100000: episode: 2283, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.721, mean reward: 0.577 [0.329, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.644, 10.098], loss: 0.009984, mae: 0.106599, mean_q: 19.932713
 86496/100000: episode: 2284, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 50.205, mean reward: 0.502 [0.322, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.906, 10.098], loss: 0.009779, mae: 0.104873, mean_q: 19.737692
 86596/100000: episode: 2285, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 55.401, mean reward: 0.554 [0.350, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.018, 10.115], loss: 0.010321, mae: 0.108545, mean_q: 19.682222
 86696/100000: episode: 2286, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 56.085, mean reward: 0.561 [0.321, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.505, 10.264], loss: 0.008724, mae: 0.100085, mean_q: 19.866650
 86796/100000: episode: 2287, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 52.363, mean reward: 0.524 [0.345, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.396, 10.098], loss: 0.011347, mae: 0.114055, mean_q: 19.721981
 86896/100000: episode: 2288, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 52.928, mean reward: 0.529 [0.204, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.355, 10.098], loss: 0.011593, mae: 0.116815, mean_q: 19.764032
 86996/100000: episode: 2289, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 51.025, mean reward: 0.510 [0.277, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.842, 10.098], loss: 0.009973, mae: 0.105543, mean_q: 19.697586
[Info] New level: 0.29635119438171387 | Considering 10/90 traces
 87096/100000: episode: 2290, duration: 4.492s, episode steps: 100, steps per second: 22, episode reward: 47.367, mean reward: 0.474 [0.201, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.497, 10.098], loss: 0.010023, mae: 0.106208, mean_q: 20.023270
 87098/100000: episode: 2291, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.821, mean reward: 0.411 [0.376, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.698, 10.100], loss: 0.006884, mae: 0.093640, mean_q: 20.457138
 87100/100000: episode: 2292, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.973, mean reward: 0.487 [0.448, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.264, 10.100], loss: 0.013183, mae: 0.120283, mean_q: 19.376650
 87102/100000: episode: 2293, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.767, mean reward: 0.384 [0.354, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.311, 10.100], loss: 0.010068, mae: 0.113652, mean_q: 16.831146
 87104/100000: episode: 2294, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.988, mean reward: 0.494 [0.466, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.286, 10.100], loss: 0.006331, mae: 0.089575, mean_q: 19.570705
 87106/100000: episode: 2295, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.964, mean reward: 0.482 [0.467, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-1.026, 10.100], loss: 0.010520, mae: 0.100650, mean_q: 19.497044
 87108/100000: episode: 2296, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.843, mean reward: 0.421 [0.418, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.197, 10.100], loss: 0.008723, mae: 0.107278, mean_q: 18.820171
 87110/100000: episode: 2297, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.654, mean reward: 0.327 [0.265, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.235, 10.100], loss: 0.009358, mae: 0.103761, mean_q: 17.945259
 87113/100000: episode: 2298, duration: 0.028s, episode steps: 3, steps per second: 107, episode reward: 1.125, mean reward: 0.375 [0.324, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.405, 10.100], loss: 0.009235, mae: 0.107506, mean_q: 22.036554
 87115/100000: episode: 2299, duration: 0.022s, episode steps: 2, steps per second: 91, episode reward: 0.903, mean reward: 0.451 [0.448, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.246, 10.100], loss: 0.011588, mae: 0.121123, mean_q: 20.468904
 87117/100000: episode: 2300, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.700, mean reward: 0.350 [0.322, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.139, 10.100], loss: 0.008172, mae: 0.102289, mean_q: 19.145304
 87120/100000: episode: 2301, duration: 0.024s, episode steps: 3, steps per second: 127, episode reward: 1.156, mean reward: 0.385 [0.361, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.388, 10.100], loss: 0.007844, mae: 0.098956, mean_q: 20.003029
 87122/100000: episode: 2302, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.902, mean reward: 0.451 [0.416, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.366, 10.100], loss: 0.007055, mae: 0.095543, mean_q: 19.846287
 87124/100000: episode: 2303, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.824, mean reward: 0.412 [0.393, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.161, 10.100], loss: 0.009962, mae: 0.102323, mean_q: 20.573919
 87126/100000: episode: 2304, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 1.040, mean reward: 0.520 [0.515, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.768, 10.100], loss: 0.006343, mae: 0.091158, mean_q: 20.423780
 87128/100000: episode: 2305, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.738, mean reward: 0.369 [0.304, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.129, 10.100], loss: 0.009591, mae: 0.106307, mean_q: 18.757383
 87130/100000: episode: 2306, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.857, mean reward: 0.429 [0.422, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.133, 10.100], loss: 0.006984, mae: 0.092968, mean_q: 20.068300
 87132/100000: episode: 2307, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.606, mean reward: 0.303 [0.299, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.304, 10.100], loss: 0.010472, mae: 0.108833, mean_q: 21.163664
 87134/100000: episode: 2308, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.902, mean reward: 0.451 [0.409, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.339, 10.100], loss: 0.013130, mae: 0.133711, mean_q: 18.728624
 87137/100000: episode: 2309, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 0.890, mean reward: 0.297 [0.270, 0.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.811, 10.100], loss: 0.010887, mae: 0.121297, mean_q: 18.453217
 87139/100000: episode: 2310, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.900, mean reward: 0.450 [0.429, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.277, 10.100], loss: 0.011573, mae: 0.123474, mean_q: 17.343325
 87141/100000: episode: 2311, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.896, mean reward: 0.448 [0.445, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.189, 10.100], loss: 0.007130, mae: 0.094027, mean_q: 20.654366
 87143/100000: episode: 2312, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.773, mean reward: 0.386 [0.369, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.332, 10.100], loss: 0.008667, mae: 0.103461, mean_q: 19.664701
 87145/100000: episode: 2313, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.741, mean reward: 0.370 [0.346, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.485, 10.100], loss: 0.010651, mae: 0.117308, mean_q: 19.912281
 87147/100000: episode: 2314, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.671, mean reward: 0.336 [0.295, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.252, 10.100], loss: 0.013360, mae: 0.124960, mean_q: 19.946909
 87149/100000: episode: 2315, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.726, mean reward: 0.363 [0.295, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.230, 10.100], loss: 0.009995, mae: 0.117099, mean_q: 19.022705
 87151/100000: episode: 2316, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.040, mean reward: 0.520 [0.501, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.384, 10.100], loss: 0.007893, mae: 0.091950, mean_q: 20.571404
 87153/100000: episode: 2317, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.133, mean reward: 0.566 [0.549, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.314, 10.100], loss: 0.015046, mae: 0.143601, mean_q: 17.705172
 87155/100000: episode: 2318, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.932, mean reward: 0.466 [0.403, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.251, 10.100], loss: 0.006489, mae: 0.087869, mean_q: 19.516731
 87157/100000: episode: 2319, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.668, mean reward: 0.334 [0.309, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.206, 10.100], loss: 0.008518, mae: 0.106774, mean_q: 18.630993
 87159/100000: episode: 2320, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 1.086, mean reward: 0.543 [0.535, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.314, 10.100], loss: 0.006759, mae: 0.088266, mean_q: 21.043221
 87161/100000: episode: 2321, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.865, mean reward: 0.433 [0.428, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.317, 10.100], loss: 0.006066, mae: 0.090891, mean_q: 20.378321
 87164/100000: episode: 2322, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 1.169, mean reward: 0.390 [0.350, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.455, 10.100], loss: 0.005966, mae: 0.092763, mean_q: 18.741240
 87166/100000: episode: 2323, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.075, mean reward: 0.538 [0.523, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.225, 10.100], loss: 0.010225, mae: 0.106578, mean_q: 21.726925
 87168/100000: episode: 2324, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.930, mean reward: 0.465 [0.439, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.255, 10.100], loss: 0.010285, mae: 0.100476, mean_q: 19.468088
 87170/100000: episode: 2325, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.838, mean reward: 0.419 [0.412, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.133, 10.100], loss: 0.007328, mae: 0.094717, mean_q: 17.839493
 87172/100000: episode: 2326, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.896, mean reward: 0.448 [0.438, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.241, 10.100], loss: 0.008808, mae: 0.101914, mean_q: 20.281994
 87174/100000: episode: 2327, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.924, mean reward: 0.462 [0.460, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.339, 10.100], loss: 0.008538, mae: 0.099767, mean_q: 18.987291
 87176/100000: episode: 2328, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.019, mean reward: 0.510 [0.491, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.189, 10.100], loss: 0.008517, mae: 0.103282, mean_q: 19.747509
 87178/100000: episode: 2329, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.852, mean reward: 0.426 [0.417, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.776, 10.100], loss: 0.008103, mae: 0.097700, mean_q: 17.394026
 87180/100000: episode: 2330, duration: 0.021s, episode steps: 2, steps per second: 94, episode reward: 1.122, mean reward: 0.561 [0.546, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.224, 10.100], loss: 0.009206, mae: 0.111881, mean_q: 20.240135
 87182/100000: episode: 2331, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.979, mean reward: 0.490 [0.484, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.192, 10.100], loss: 0.006402, mae: 0.092225, mean_q: 21.078745
 87184/100000: episode: 2332, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.930, mean reward: 0.465 [0.435, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.276, 10.100], loss: 0.008498, mae: 0.107103, mean_q: 19.255756
 87187/100000: episode: 2333, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 1.310, mean reward: 0.437 [0.419, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.373, 10.100], loss: 0.012737, mae: 0.125058, mean_q: 18.272270
 87189/100000: episode: 2334, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.693, mean reward: 0.346 [0.336, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.204, 10.100], loss: 0.007101, mae: 0.089036, mean_q: 19.409962
 87191/100000: episode: 2335, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.810, mean reward: 0.405 [0.374, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.183, 10.100], loss: 0.013242, mae: 0.112318, mean_q: 18.793892
 87193/100000: episode: 2336, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 1.102, mean reward: 0.551 [0.551, 0.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.314, 10.100], loss: 0.010386, mae: 0.099586, mean_q: 21.533230
 87195/100000: episode: 2337, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.769, mean reward: 0.384 [0.364, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.290, 10.100], loss: 0.008815, mae: 0.104607, mean_q: 19.764221
 87197/100000: episode: 2338, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.030, mean reward: 0.515 [0.512, 0.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.120, 10.100], loss: 0.011432, mae: 0.119908, mean_q: 17.961716
 87199/100000: episode: 2339, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.035, mean reward: 0.517 [0.517, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.320, 10.100], loss: 0.010832, mae: 0.121554, mean_q: 20.033051
 87201/100000: episode: 2340, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.683, mean reward: 0.341 [0.313, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.238, 10.100], loss: 0.011725, mae: 0.110466, mean_q: 19.792385
 87203/100000: episode: 2341, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.766, mean reward: 0.383 [0.368, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.299, 10.100], loss: 0.009763, mae: 0.116789, mean_q: 20.202261
 87205/100000: episode: 2342, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.863, mean reward: 0.432 [0.426, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.278, 10.100], loss: 0.010711, mae: 0.105127, mean_q: 19.062258
 87207/100000: episode: 2343, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.882, mean reward: 0.441 [0.411, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.252, 10.100], loss: 0.013010, mae: 0.131790, mean_q: 18.132231
 87210/100000: episode: 2344, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 1.075, mean reward: 0.358 [0.335, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.668, 10.100], loss: 0.011331, mae: 0.112493, mean_q: 17.765148
 87212/100000: episode: 2345, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.817, mean reward: 0.409 [0.395, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.272, 10.100], loss: 0.011450, mae: 0.118043, mean_q: 19.394737
 87214/100000: episode: 2346, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.966, mean reward: 0.483 [0.458, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-1.023, 10.100], loss: 0.008546, mae: 0.096652, mean_q: 20.432007
 87216/100000: episode: 2347, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.848, mean reward: 0.424 [0.386, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.285, 10.100], loss: 0.017255, mae: 0.145316, mean_q: 18.498486
 87218/100000: episode: 2348, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.994, mean reward: 0.497 [0.465, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.285, 10.100], loss: 0.008383, mae: 0.093528, mean_q: 18.258579
 87220/100000: episode: 2349, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.719, mean reward: 0.359 [0.312, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.221, 10.100], loss: 0.009771, mae: 0.115056, mean_q: 20.451553
 87222/100000: episode: 2350, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.855, mean reward: 0.428 [0.412, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.291, 10.100], loss: 0.010310, mae: 0.104697, mean_q: 19.715828
 87224/100000: episode: 2351, duration: 0.016s, episode steps: 2, steps per second: 121, episode reward: 0.993, mean reward: 0.497 [0.483, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.189, 10.100], loss: 0.010569, mae: 0.103403, mean_q: 20.285713
 87226/100000: episode: 2352, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.667, mean reward: 0.334 [0.284, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.236, 10.100], loss: 0.009922, mae: 0.109723, mean_q: 21.363529
 87228/100000: episode: 2353, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.800, mean reward: 0.400 [0.395, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.293, 10.100], loss: 0.007295, mae: 0.088604, mean_q: 20.049767
 87230/100000: episode: 2354, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.909, mean reward: 0.455 [0.442, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.241, 10.100], loss: 0.009470, mae: 0.098404, mean_q: 21.594000
 87232/100000: episode: 2355, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.802, mean reward: 0.401 [0.345, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.196, 10.100], loss: 0.009931, mae: 0.101818, mean_q: 19.124317
 87234/100000: episode: 2356, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.840, mean reward: 0.420 [0.405, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.250, 10.100], loss: 0.007606, mae: 0.097977, mean_q: 17.244904
 87236/100000: episode: 2357, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 0.696, mean reward: 0.348 [0.327, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.281, 10.100], loss: 0.012721, mae: 0.119332, mean_q: 18.751339
 87238/100000: episode: 2358, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.013, mean reward: 0.506 [0.429, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.225, 10.100], loss: 0.013602, mae: 0.139609, mean_q: 17.498066
 87240/100000: episode: 2359, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.759, mean reward: 0.380 [0.339, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.305, 10.100], loss: 0.010940, mae: 0.113342, mean_q: 19.308475
 87242/100000: episode: 2360, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.814, mean reward: 0.407 [0.398, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.250, 10.100], loss: 0.011934, mae: 0.122105, mean_q: 19.358665
 87244/100000: episode: 2361, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.873, mean reward: 0.437 [0.400, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.278, 10.100], loss: 0.011675, mae: 0.105505, mean_q: 19.388130
 87246/100000: episode: 2362, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.835, mean reward: 0.417 [0.406, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.192, 10.100], loss: 0.011730, mae: 0.129566, mean_q: 18.792576
 87248/100000: episode: 2363, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 0.955, mean reward: 0.477 [0.468, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.259, 10.100], loss: 0.014588, mae: 0.129258, mean_q: 22.022083
 87250/100000: episode: 2364, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.752, mean reward: 0.376 [0.355, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.203, 10.100], loss: 0.014926, mae: 0.127892, mean_q: 18.253601
 87252/100000: episode: 2365, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.887, mean reward: 0.443 [0.440, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.100, 10.100], loss: 0.007299, mae: 0.099679, mean_q: 19.812141
 87254/100000: episode: 2366, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.846, mean reward: 0.423 [0.421, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.324, 10.100], loss: 0.006829, mae: 0.098284, mean_q: 17.104649
 87256/100000: episode: 2367, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.851, mean reward: 0.425 [0.384, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.315, 10.100], loss: 0.011500, mae: 0.114133, mean_q: 19.777237
 87258/100000: episode: 2368, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.085, mean reward: 0.543 [0.515, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.240, 10.100], loss: 0.006389, mae: 0.093638, mean_q: 19.366226
 87261/100000: episode: 2369, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 0.922, mean reward: 0.307 [0.283, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.405, 10.100], loss: 0.011903, mae: 0.108631, mean_q: 19.149824
 87263/100000: episode: 2370, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.052, mean reward: 0.526 [0.454, 0.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.162, 10.100], loss: 0.007048, mae: 0.094512, mean_q: 20.579643
 87266/100000: episode: 2371, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 1.070, mean reward: 0.357 [0.332, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.293, 10.100], loss: 0.010554, mae: 0.112183, mean_q: 19.866480
 87268/100000: episode: 2372, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.738, mean reward: 0.369 [0.347, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.366, 10.100], loss: 0.010189, mae: 0.100764, mean_q: 17.334414
 87271/100000: episode: 2373, duration: 0.026s, episode steps: 3, steps per second: 114, episode reward: 1.109, mean reward: 0.370 [0.341, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.516, 10.100], loss: 0.012372, mae: 0.108550, mean_q: 17.972836
 87273/100000: episode: 2374, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.823, mean reward: 0.412 [0.370, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.936, 10.100], loss: 0.007718, mae: 0.097415, mean_q: 19.994036
 87275/100000: episode: 2375, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.943, mean reward: 0.472 [0.436, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.235, 10.100], loss: 0.009073, mae: 0.102615, mean_q: 18.159611
 87277/100000: episode: 2376, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.659, mean reward: 0.330 [0.324, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.320, 10.100], loss: 0.007341, mae: 0.089323, mean_q: 19.969543
 87279/100000: episode: 2377, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 1.054, mean reward: 0.527 [0.453, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.188, 10.100], loss: 0.005179, mae: 0.074488, mean_q: 20.531948
 87281/100000: episode: 2378, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.803, mean reward: 0.402 [0.401, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.186, 10.100], loss: 0.006707, mae: 0.093874, mean_q: 17.334641
 87284/100000: episode: 2379, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 0.926, mean reward: 0.309 [0.266, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.396, 10.100], loss: 0.008364, mae: 0.100846, mean_q: 18.300926
[Info] New level: 0.0989983081817627 | Considering 10/90 traces
 87286/100000: episode: 2380, duration: 4.041s, episode steps: 2, steps per second: 0, episode reward: 0.934, mean reward: 0.467 [0.458, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.272, 10.100], loss: 0.005535, mae: 0.080175, mean_q: 20.608156
 87287/100000: episode: 2381, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.466, mean reward: 0.466 [0.466, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.302, 10.100], loss: 0.009071, mae: 0.104723, mean_q: 18.063128
 87288/100000: episode: 2382, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.416, 10.100], loss: 0.004951, mae: 0.080266, mean_q: 18.424316
 87289/100000: episode: 2383, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.414, mean reward: 0.414 [0.414, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.392, 10.100], loss: 0.007639, mae: 0.088639, mean_q: 16.215317
 87290/100000: episode: 2384, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.324, mean reward: 0.324 [0.324, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.375, 10.100], loss: 0.008468, mae: 0.095571, mean_q: 20.596565
 87291/100000: episode: 2385, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.285, mean reward: 0.285 [0.285, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.346, 10.100], loss: 0.004834, mae: 0.081303, mean_q: 17.958929
 87292/100000: episode: 2386, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.310, mean reward: 0.310 [0.310, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.408, 10.100], loss: 0.004672, mae: 0.079524, mean_q: 15.740382
 87293/100000: episode: 2387, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.232, mean reward: 0.232 [0.232, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.391, 10.100], loss: 0.007345, mae: 0.086455, mean_q: 20.632435
 87294/100000: episode: 2388, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.464, mean reward: 0.464 [0.464, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.328, 10.100], loss: 0.011807, mae: 0.104980, mean_q: 19.338802
 87295/100000: episode: 2389, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.464, mean reward: 0.464 [0.464, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.380, 10.100], loss: 0.006413, mae: 0.091627, mean_q: 16.263424
 87296/100000: episode: 2390, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.299, mean reward: 0.299 [0.299, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.395, 10.100], loss: 0.014642, mae: 0.133966, mean_q: 18.868893
 87297/100000: episode: 2391, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.413, 10.100], loss: 0.010397, mae: 0.119169, mean_q: 17.825687
 87298/100000: episode: 2392, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.212, mean reward: 0.212 [0.212, 0.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.346, 10.100], loss: 0.004602, mae: 0.077318, mean_q: 20.336246
 87299/100000: episode: 2393, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.296, mean reward: 0.296 [0.296, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.401, 10.100], loss: 0.008643, mae: 0.109708, mean_q: 20.199244
 87300/100000: episode: 2394, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.199, mean reward: 0.199 [0.199, 0.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.406, 10.100], loss: 0.009039, mae: 0.109966, mean_q: 19.148039
 87301/100000: episode: 2395, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.334, mean reward: 0.334 [0.334, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.416, 10.100], loss: 0.010318, mae: 0.106193, mean_q: 18.638533
 87302/100000: episode: 2396, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.314, mean reward: 0.314 [0.314, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.361, 10.100], loss: 0.009320, mae: 0.096844, mean_q: 20.577751
 87303/100000: episode: 2397, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.275, mean reward: 0.275 [0.275, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.400, 10.100], loss: 0.013431, mae: 0.141755, mean_q: 20.134474
 87304/100000: episode: 2398, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.368, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.388, 10.100], loss: 0.011690, mae: 0.122424, mean_q: 20.117395
 87305/100000: episode: 2399, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.264, mean reward: 0.264 [0.264, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.394, 10.100], loss: 0.003993, mae: 0.077930, mean_q: 19.462120
 87306/100000: episode: 2400, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.341, mean reward: 0.341 [0.341, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.309, 10.100], loss: 0.014231, mae: 0.138424, mean_q: 19.620409
 87307/100000: episode: 2401, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.526, mean reward: 0.526 [0.526, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.347, 10.100], loss: 0.011076, mae: 0.105656, mean_q: 18.729223
 87308/100000: episode: 2402, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.353, 10.100], loss: 0.006251, mae: 0.082311, mean_q: 19.081470
 87309/100000: episode: 2403, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.315, 10.100], loss: 0.019280, mae: 0.174608, mean_q: 16.759775
 87310/100000: episode: 2404, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.345, 10.100], loss: 0.006487, mae: 0.092131, mean_q: 17.739388
 87311/100000: episode: 2405, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.294, mean reward: 0.294 [0.294, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.334, 10.100], loss: 0.006129, mae: 0.095224, mean_q: 19.042734
 87312/100000: episode: 2406, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.341, mean reward: 0.341 [0.341, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.322, 10.100], loss: 0.010650, mae: 0.109308, mean_q: 19.457790
 87313/100000: episode: 2407, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.305, mean reward: 0.305 [0.305, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.382, 10.100], loss: 0.006702, mae: 0.098116, mean_q: 19.815727
 87314/100000: episode: 2408, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.328, mean reward: 0.328 [0.328, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.385, 10.100], loss: 0.004723, mae: 0.081181, mean_q: 17.219280
 87315/100000: episode: 2409, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.223, mean reward: 0.223 [0.223, 0.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.432, 10.100], loss: 0.019894, mae: 0.159406, mean_q: 20.683620
 87316/100000: episode: 2410, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.399, mean reward: 0.399 [0.399, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.411, 10.100], loss: 0.010278, mae: 0.096835, mean_q: 16.896864
 87317/100000: episode: 2411, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.271, mean reward: 0.271 [0.271, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.384, 10.100], loss: 0.007470, mae: 0.095268, mean_q: 18.934948
 87318/100000: episode: 2412, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.299, mean reward: 0.299 [0.299, 0.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.377, 10.100], loss: 0.011415, mae: 0.129392, mean_q: 17.690723
 87319/100000: episode: 2413, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.465, 10.100], loss: 0.004359, mae: 0.076399, mean_q: 18.795551
 87320/100000: episode: 2414, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.369, 10.100], loss: 0.014161, mae: 0.124431, mean_q: 19.253283
 87321/100000: episode: 2415, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.363, mean reward: 0.363 [0.363, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.367, 10.100], loss: 0.014098, mae: 0.146658, mean_q: 16.610777
 87322/100000: episode: 2416, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.275, mean reward: 0.275 [0.275, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.390, 10.100], loss: 0.017789, mae: 0.093566, mean_q: 16.268112
 87323/100000: episode: 2417, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.255, mean reward: 0.255 [0.255, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.423, 10.100], loss: 0.008773, mae: 0.097171, mean_q: 19.462402
 87324/100000: episode: 2418, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.318, mean reward: 0.318 [0.318, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.371, 10.100], loss: 0.004520, mae: 0.075694, mean_q: 22.036400
 87325/100000: episode: 2419, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.347, mean reward: 0.347 [0.347, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.339, 10.100], loss: 0.008804, mae: 0.106966, mean_q: 19.319483
 87326/100000: episode: 2420, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.265, mean reward: 0.265 [0.265, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.379, 10.100], loss: 0.018924, mae: 0.143723, mean_q: 19.991285
 87327/100000: episode: 2421, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.523, mean reward: 0.523 [0.523, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.287, 10.100], loss: 0.012309, mae: 0.125695, mean_q: 16.516621
 87328/100000: episode: 2422, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.482, mean reward: 0.482 [0.482, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.305, 10.100], loss: 0.007184, mae: 0.096302, mean_q: 16.263622
 87329/100000: episode: 2423, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.360, mean reward: 0.360 [0.360, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.379, 10.100], loss: 0.012362, mae: 0.121153, mean_q: 20.312832
 87330/100000: episode: 2424, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.424, mean reward: 0.424 [0.424, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.412, 10.100], loss: 0.009724, mae: 0.124762, mean_q: 19.878862
 87331/100000: episode: 2425, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.500, mean reward: 0.500 [0.500, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.339, 10.100], loss: 0.006498, mae: 0.087450, mean_q: 16.929924
 87332/100000: episode: 2426, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.481, mean reward: 0.481 [0.481, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.305, 10.100], loss: 0.009942, mae: 0.097720, mean_q: 20.267939
 87333/100000: episode: 2427, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.437, mean reward: 0.437 [0.437, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.416, 10.100], loss: 0.009938, mae: 0.115133, mean_q: 18.791945
 87334/100000: episode: 2428, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.512, 10.100], loss: 0.006116, mae: 0.089320, mean_q: 18.259083
 87335/100000: episode: 2429, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.435, mean reward: 0.435 [0.435, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.393, 10.100], loss: 0.013570, mae: 0.136272, mean_q: 19.744801
 87336/100000: episode: 2430, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.300, mean reward: 0.300 [0.300, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.379, 10.100], loss: 0.012223, mae: 0.131436, mean_q: 19.813026
 87337/100000: episode: 2431, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.237, mean reward: 0.237 [0.237, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.406, 10.100], loss: 0.037889, mae: 0.141814, mean_q: 20.535110
 87338/100000: episode: 2432, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.236, mean reward: 0.236 [0.236, 0.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.435, 10.100], loss: 0.006376, mae: 0.085941, mean_q: 19.601307
 87339/100000: episode: 2433, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.410, 10.100], loss: 0.005837, mae: 0.086117, mean_q: 22.560749
 87340/100000: episode: 2434, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.295, mean reward: 0.295 [0.295, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.363, 10.100], loss: 0.014006, mae: 0.114930, mean_q: 15.681723
 87341/100000: episode: 2435, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.307, mean reward: 0.307 [0.307, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.455, 10.100], loss: 0.013117, mae: 0.091172, mean_q: 20.023018
 87342/100000: episode: 2436, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.335, mean reward: 0.335 [0.335, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.388, 10.100], loss: 0.013107, mae: 0.092920, mean_q: 17.450079
 87343/100000: episode: 2437, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.355, mean reward: 0.355 [0.355, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.395, 10.100], loss: 0.006453, mae: 0.090531, mean_q: 17.597754
 87344/100000: episode: 2438, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.358, mean reward: 0.358 [0.358, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.339, 10.100], loss: 0.007840, mae: 0.101691, mean_q: 20.640036
 87345/100000: episode: 2439, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.516, mean reward: 0.516 [0.516, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.272, 10.100], loss: 0.011319, mae: 0.113942, mean_q: 18.412098
 87346/100000: episode: 2440, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.326, mean reward: 0.326 [0.326, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.393, 10.100], loss: 0.010017, mae: 0.113845, mean_q: 18.427517
 87347/100000: episode: 2441, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.318, mean reward: 0.318 [0.318, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.391, 10.100], loss: 0.011463, mae: 0.121423, mean_q: 16.946125
 87348/100000: episode: 2442, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.367, mean reward: 0.367 [0.367, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.420, 10.100], loss: 0.008201, mae: 0.095090, mean_q: 21.124630
 87349/100000: episode: 2443, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.318, mean reward: 0.318 [0.318, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.385, 10.100], loss: 0.014199, mae: 0.123145, mean_q: 18.447418
 87350/100000: episode: 2444, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.418, 10.100], loss: 0.006924, mae: 0.089173, mean_q: 17.810741
 87351/100000: episode: 2445, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.331, mean reward: 0.331 [0.331, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.450, 10.100], loss: 0.005914, mae: 0.086194, mean_q: 19.420969
 87352/100000: episode: 2446, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.228, mean reward: 0.228 [0.228, 0.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.395, 10.100], loss: 0.013715, mae: 0.147673, mean_q: 16.689732
 87353/100000: episode: 2447, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.271, mean reward: 0.271 [0.271, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.387, 10.100], loss: 0.006221, mae: 0.090266, mean_q: 17.731306
 87354/100000: episode: 2448, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.469, mean reward: 0.469 [0.469, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.324, 10.100], loss: 0.014247, mae: 0.131410, mean_q: 21.539989
 87355/100000: episode: 2449, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.327, mean reward: 0.327 [0.327, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.365, 10.100], loss: 0.010528, mae: 0.120747, mean_q: 18.069565
 87356/100000: episode: 2450, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 0.295, mean reward: 0.295 [0.295, 0.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.408, 10.100], loss: 0.012020, mae: 0.126922, mean_q: 20.349159
 87357/100000: episode: 2451, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.322, mean reward: 0.322 [0.322, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.408, 10.100], loss: 0.008737, mae: 0.100338, mean_q: 17.423225
 87358/100000: episode: 2452, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.384, 10.100], loss: 0.009745, mae: 0.113549, mean_q: 19.772577
 87359/100000: episode: 2453, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.241, mean reward: 0.241 [0.241, 0.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.417, 10.100], loss: 0.007825, mae: 0.102570, mean_q: 18.004925
 87360/100000: episode: 2454, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.389, mean reward: 0.389 [0.389, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.383, 10.100], loss: 0.006182, mae: 0.091324, mean_q: 19.225075
 87361/100000: episode: 2455, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.365, mean reward: 0.365 [0.365, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.417, 10.100], loss: 0.009895, mae: 0.098655, mean_q: 18.380978
 87362/100000: episode: 2456, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.316, mean reward: 0.316 [0.316, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.353, 10.100], loss: 0.005572, mae: 0.087106, mean_q: 19.475536
 87363/100000: episode: 2457, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.311, mean reward: 0.311 [0.311, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.405, 10.100], loss: 0.005813, mae: 0.074878, mean_q: 17.907349
 87364/100000: episode: 2458, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.357, mean reward: 0.357 [0.357, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.375, 10.100], loss: 0.006595, mae: 0.082528, mean_q: 18.756691
 87365/100000: episode: 2459, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.353, 10.100], loss: 0.009449, mae: 0.089696, mean_q: 18.833345
 87366/100000: episode: 2460, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.489, mean reward: 0.489 [0.489, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.379, 10.100], loss: 0.007660, mae: 0.098168, mean_q: 16.839676
 87367/100000: episode: 2461, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.369, mean reward: 0.369 [0.369, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.344, 10.100], loss: 0.008349, mae: 0.088956, mean_q: 17.273287
 87368/100000: episode: 2462, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.332, mean reward: 0.332 [0.332, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.354, 10.100], loss: 0.009944, mae: 0.111292, mean_q: 20.803684
 87369/100000: episode: 2463, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.311, mean reward: 0.311 [0.311, 0.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.358, 10.100], loss: 0.003360, mae: 0.061766, mean_q: 18.158276
 87370/100000: episode: 2464, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.554, mean reward: 0.554 [0.554, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.358, 10.100], loss: 0.005231, mae: 0.083864, mean_q: 17.699032
 87371/100000: episode: 2465, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.385, 10.100], loss: 0.005574, mae: 0.084607, mean_q: 19.960278
 87372/100000: episode: 2466, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.374, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.380, 10.100], loss: 0.006775, mae: 0.091986, mean_q: 16.950108
 87373/100000: episode: 2467, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.317, mean reward: 0.317 [0.317, 0.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.369, 10.100], loss: 0.007035, mae: 0.086684, mean_q: 18.498631
 87374/100000: episode: 2468, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.273, mean reward: 0.273 [0.273, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.373, 10.100], loss: 0.005538, mae: 0.081636, mean_q: 19.560221
 87375/100000: episode: 2469, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.309, mean reward: 0.309 [0.309, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.428, 10.100], loss: 0.009266, mae: 0.093010, mean_q: 18.864603
[Info] New level: -0.01732611656188965 | Considering 10/90 traces
 87376/100000: episode: 2470, duration: 4.007s, episode steps: 1, steps per second: 0, episode reward: 0.416, mean reward: 0.416 [0.416, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.371, 10.100], loss: 0.008371, mae: 0.107877, mean_q: 18.258804
 87377/100000: episode: 2471, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.417, mean reward: 0.417 [0.417, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.528, 10.100], loss: 0.007604, mae: 0.091854, mean_q: 17.808594
 87378/100000: episode: 2472, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.368, 10.100], loss: 0.008387, mae: 0.101287, mean_q: 17.501747
 87379/100000: episode: 2473, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.370, 10.100], loss: 0.011185, mae: 0.126984, mean_q: 20.330566
 87380/100000: episode: 2474, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.223, mean reward: 0.223 [0.223, 0.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.411, 10.100], loss: 0.008090, mae: 0.084236, mean_q: 19.186028
 87381/100000: episode: 2475, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.329, mean reward: 0.329 [0.329, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.418, 10.100], loss: 0.005116, mae: 0.081115, mean_q: 20.074663
 87382/100000: episode: 2476, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.409, 10.100], loss: 0.007890, mae: 0.102206, mean_q: 20.724689
 87383/100000: episode: 2477, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.345, mean reward: 0.345 [0.345, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.400, 10.100], loss: 0.009661, mae: 0.105044, mean_q: 17.409645
 87384/100000: episode: 2478, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.368, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.327, 10.100], loss: 0.008187, mae: 0.105118, mean_q: 18.243254
 87385/100000: episode: 2479, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.412, mean reward: 0.412 [0.412, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.358, 10.100], loss: 0.006317, mae: 0.090531, mean_q: 17.338751
 87386/100000: episode: 2480, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.347, mean reward: 0.347 [0.347, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.367, 10.100], loss: 0.008832, mae: 0.109926, mean_q: 17.299784
 87387/100000: episode: 2481, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.456, mean reward: 0.456 [0.456, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.378, 10.100], loss: 0.008575, mae: 0.105566, mean_q: 19.693466
 87388/100000: episode: 2482, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.335, mean reward: 0.335 [0.335, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.353, 10.100], loss: 0.005457, mae: 0.083880, mean_q: 19.370972
 87389/100000: episode: 2483, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.368, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.370, 10.100], loss: 0.016935, mae: 0.128298, mean_q: 18.144157
 87390/100000: episode: 2484, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.300, mean reward: 0.300 [0.300, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.349, 10.100], loss: 0.010972, mae: 0.104377, mean_q: 17.904171
 87391/100000: episode: 2485, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.283, mean reward: 0.283 [0.283, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.394, 10.100], loss: 0.007532, mae: 0.097028, mean_q: 16.876972
 87392/100000: episode: 2486, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.334, mean reward: 0.334 [0.334, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.356, 10.100], loss: 0.008719, mae: 0.078737, mean_q: 16.002388
 87393/100000: episode: 2487, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.244, mean reward: 0.244 [0.244, 0.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.390, 10.100], loss: 0.007976, mae: 0.094449, mean_q: 16.017511
 87394/100000: episode: 2488, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.359, 10.100], loss: 0.008126, mae: 0.108645, mean_q: 18.187302
 87395/100000: episode: 2489, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.411, 10.100], loss: 0.005060, mae: 0.077528, mean_q: 17.243984
 87396/100000: episode: 2490, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.362, 10.100], loss: 0.007097, mae: 0.096641, mean_q: 20.785027
 87397/100000: episode: 2491, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.215, mean reward: 0.215 [0.215, 0.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.147, 10.100], loss: 0.011793, mae: 0.130128, mean_q: 16.562893
 87398/100000: episode: 2492, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.385, mean reward: 0.385 [0.385, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.381, 10.100], loss: 0.014408, mae: 0.138743, mean_q: 19.060793
 87399/100000: episode: 2493, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.343, mean reward: 0.343 [0.343, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.389, 10.100], loss: 0.008920, mae: 0.106686, mean_q: 14.725273
 87400/100000: episode: 2494, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.389, mean reward: 0.389 [0.389, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.421, 10.100], loss: 0.016339, mae: 0.131958, mean_q: 16.980946
 87401/100000: episode: 2495, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.376, mean reward: 0.376 [0.376, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.422, 10.100], loss: 0.016140, mae: 0.146062, mean_q: 17.890360
 87402/100000: episode: 2496, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.427, mean reward: 0.427 [0.427, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.378, 10.100], loss: 0.011800, mae: 0.133291, mean_q: 17.078693
 87403/100000: episode: 2497, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.298, mean reward: 0.298 [0.298, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.433, 10.100], loss: 0.005561, mae: 0.089924, mean_q: 19.259735
 87404/100000: episode: 2498, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.377, mean reward: 0.377 [0.377, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.407, 10.100], loss: 0.016383, mae: 0.141427, mean_q: 17.004330
 87405/100000: episode: 2499, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.330, mean reward: 0.330 [0.330, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.381, 10.100], loss: 0.026543, mae: 0.205891, mean_q: 16.717442
 87406/100000: episode: 2500, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.372, 10.100], loss: 0.008345, mae: 0.103425, mean_q: 19.698868
 87407/100000: episode: 2501, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.312, mean reward: 0.312 [0.312, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.397, 10.100], loss: 0.010833, mae: 0.109734, mean_q: 16.541540
 87408/100000: episode: 2502, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.406, mean reward: 0.406 [0.406, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.383, 10.100], loss: 0.013119, mae: 0.138087, mean_q: 19.618540
 87409/100000: episode: 2503, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.232, mean reward: 0.232 [0.232, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.398, 10.100], loss: 0.007422, mae: 0.097438, mean_q: 15.582357
 87410/100000: episode: 2504, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.394, 10.100], loss: 0.014264, mae: 0.099578, mean_q: 16.584461
 87411/100000: episode: 2505, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.338, mean reward: 0.338 [0.338, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.366, 10.100], loss: 0.010602, mae: 0.127587, mean_q: 15.608240
 87412/100000: episode: 2506, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.314, mean reward: 0.314 [0.314, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.394, 10.100], loss: 0.007833, mae: 0.090818, mean_q: 17.710960
 87413/100000: episode: 2507, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.425, mean reward: 0.425 [0.425, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.384, 10.100], loss: 0.006784, mae: 0.087376, mean_q: 20.466751
 87414/100000: episode: 2508, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.336, mean reward: 0.336 [0.336, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.397, 10.100], loss: 0.006767, mae: 0.080383, mean_q: 18.239538
 87415/100000: episode: 2509, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.354, mean reward: 0.354 [0.354, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.434, 10.100], loss: 0.004087, mae: 0.072207, mean_q: 19.674931
 87416/100000: episode: 2510, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.344, mean reward: 0.344 [0.344, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.376, 10.100], loss: 0.006670, mae: 0.087596, mean_q: 19.565313
 87417/100000: episode: 2511, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.385, mean reward: 0.385 [0.385, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.380, 10.100], loss: 0.005518, mae: 0.089137, mean_q: 17.135593
 87418/100000: episode: 2512, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.233, mean reward: 0.233 [0.233, 0.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.402, 10.100], loss: 0.003823, mae: 0.076312, mean_q: 17.848301
 87419/100000: episode: 2513, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.409, 10.100], loss: 0.004239, mae: 0.075481, mean_q: 19.222015
 87420/100000: episode: 2514, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.347, mean reward: 0.347 [0.347, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.391, 10.100], loss: 0.005154, mae: 0.082470, mean_q: 22.016251
 87421/100000: episode: 2515, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.263, mean reward: 0.263 [0.263, 0.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.380, 10.100], loss: 0.015242, mae: 0.132882, mean_q: 18.884714
 87422/100000: episode: 2516, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.328, mean reward: 0.328 [0.328, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.411, 10.100], loss: 0.007022, mae: 0.091226, mean_q: 18.258171
 87423/100000: episode: 2517, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.312, mean reward: 0.312 [0.312, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.372, 10.100], loss: 0.005897, mae: 0.088203, mean_q: 19.968155
 87424/100000: episode: 2518, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.418, mean reward: 0.418 [0.418, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.443, 10.100], loss: 0.013996, mae: 0.135291, mean_q: 18.781811
 87425/100000: episode: 2519, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.375, 10.100], loss: 0.023322, mae: 0.126113, mean_q: 13.707537
 87426/100000: episode: 2520, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.370, mean reward: 0.370 [0.370, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.379, 10.100], loss: 0.012736, mae: 0.130628, mean_q: 19.773636
 87427/100000: episode: 2521, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.313, mean reward: 0.313 [0.313, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.433, 10.100], loss: 0.012712, mae: 0.110225, mean_q: 19.333691
 87428/100000: episode: 2522, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.388, 10.100], loss: 0.005718, mae: 0.082923, mean_q: 17.631958
 87429/100000: episode: 2523, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.418, mean reward: 0.418 [0.418, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.307, 10.100], loss: 0.012003, mae: 0.129203, mean_q: 16.222061
 87430/100000: episode: 2524, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.390, mean reward: 0.390 [0.390, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.397, 10.100], loss: 0.006642, mae: 0.092141, mean_q: 19.405735
 87431/100000: episode: 2525, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.374, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.428, 10.100], loss: 0.007526, mae: 0.088707, mean_q: 15.902630
 87432/100000: episode: 2526, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.365, mean reward: 0.365 [0.365, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.362, 10.100], loss: 0.009941, mae: 0.114401, mean_q: 19.893166
 87433/100000: episode: 2527, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.381, mean reward: 0.381 [0.381, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.443, 10.100], loss: 0.003683, mae: 0.061635, mean_q: 17.581963
 87434/100000: episode: 2528, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.314, mean reward: 0.314 [0.314, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.356, 10.100], loss: 0.007944, mae: 0.095649, mean_q: 16.914803
 87435/100000: episode: 2529, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.358, mean reward: 0.358 [0.358, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.410, 10.100], loss: 0.006304, mae: 0.101209, mean_q: 19.441689
 87436/100000: episode: 2530, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.326, mean reward: 0.326 [0.326, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.383, 10.100], loss: 0.005432, mae: 0.083596, mean_q: 17.915314
 87437/100000: episode: 2531, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.383, 10.100], loss: 0.012432, mae: 0.126045, mean_q: 21.656067
 87438/100000: episode: 2532, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.463, mean reward: 0.463 [0.463, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.381, 10.100], loss: 0.006643, mae: 0.092884, mean_q: 15.160568
 87439/100000: episode: 2533, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.324, mean reward: 0.324 [0.324, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.416, 10.100], loss: 0.013328, mae: 0.099522, mean_q: 13.055559
 87440/100000: episode: 2534, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.357, 10.100], loss: 0.009151, mae: 0.101789, mean_q: 18.867176
 87441/100000: episode: 2535, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.262, mean reward: 0.262 [0.262, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.462, 10.100], loss: 0.008696, mae: 0.104193, mean_q: 21.609573
 87442/100000: episode: 2536, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.374, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.377, 10.100], loss: 0.006101, mae: 0.089733, mean_q: 17.266037
 87443/100000: episode: 2537, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.335, mean reward: 0.335 [0.335, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.357, 10.100], loss: 0.005687, mae: 0.088026, mean_q: 19.410755
 87444/100000: episode: 2538, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.330, mean reward: 0.330 [0.330, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.398, 10.100], loss: 0.007740, mae: 0.091463, mean_q: 19.293127
 87445/100000: episode: 2539, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.377, mean reward: 0.377 [0.377, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.424, 10.100], loss: 0.012700, mae: 0.108716, mean_q: 18.218948
 87446/100000: episode: 2540, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.327, mean reward: 0.327 [0.327, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.358, 10.100], loss: 0.005731, mae: 0.088559, mean_q: 16.048916
 87447/100000: episode: 2541, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.365, mean reward: 0.365 [0.365, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.447, 10.100], loss: 0.009387, mae: 0.102298, mean_q: 16.921013
 87448/100000: episode: 2542, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.379, 10.100], loss: 0.007205, mae: 0.086116, mean_q: 20.311451
 87449/100000: episode: 2543, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.368, 10.100], loss: 0.004105, mae: 0.074810, mean_q: 22.468946
 87450/100000: episode: 2544, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.342, mean reward: 0.342 [0.342, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.393, 10.100], loss: 0.012619, mae: 0.130721, mean_q: 17.079201
 87451/100000: episode: 2545, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 0.286, mean reward: 0.286 [0.286, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.425, 10.100], loss: 0.005459, mae: 0.087741, mean_q: 19.519176
 87452/100000: episode: 2546, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.375, 10.100], loss: 0.004918, mae: 0.077751, mean_q: 14.235731
 87453/100000: episode: 2547, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.384, mean reward: 0.384 [0.384, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.365, 10.100], loss: 0.008204, mae: 0.083473, mean_q: 16.035896
 87454/100000: episode: 2548, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.312, mean reward: 0.312 [0.312, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.377, 10.100], loss: 0.004682, mae: 0.077307, mean_q: 18.600101
 87455/100000: episode: 2549, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.359, mean reward: 0.359 [0.359, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.423, 10.100], loss: 0.008351, mae: 0.101155, mean_q: 18.823965
 87456/100000: episode: 2550, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.388, mean reward: 0.388 [0.388, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.422, 10.100], loss: 0.012730, mae: 0.122652, mean_q: 17.446915
 87457/100000: episode: 2551, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.228, mean reward: 0.228 [0.228, 0.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.449, 10.100], loss: 0.011342, mae: 0.113545, mean_q: 16.827610
 87458/100000: episode: 2552, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.367, 10.100], loss: 0.024492, mae: 0.135856, mean_q: 18.124830
 87459/100000: episode: 2553, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.424, mean reward: 0.424 [0.424, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.402, 10.100], loss: 0.005268, mae: 0.082149, mean_q: 18.352139
 87460/100000: episode: 2554, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.364, mean reward: 0.364 [0.364, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.407, 10.100], loss: 0.005259, mae: 0.079874, mean_q: 18.346329
 87461/100000: episode: 2555, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.305, mean reward: 0.305 [0.305, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.406, 10.100], loss: 0.012241, mae: 0.114837, mean_q: 17.855316
 87462/100000: episode: 2556, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.403, 10.100], loss: 0.008657, mae: 0.105234, mean_q: 21.170979
 87463/100000: episode: 2557, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.390, mean reward: 0.390 [0.390, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.383, 10.100], loss: 0.011545, mae: 0.110555, mean_q: 18.789177
 87464/100000: episode: 2558, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.376, mean reward: 0.376 [0.376, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.433, 10.100], loss: 0.007640, mae: 0.100085, mean_q: 13.351242
 87465/100000: episode: 2559, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.358, mean reward: 0.358 [0.358, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.393, 10.100], loss: 0.020729, mae: 0.140717, mean_q: 18.639114
[Info] New level: -0.048601388931274414 | Considering 10/90 traces
 87466/100000: episode: 2560, duration: 4.024s, episode steps: 1, steps per second: 0, episode reward: 0.308, mean reward: 0.308 [0.308, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.370, 10.100], loss: 0.016325, mae: 0.109561, mean_q: 18.965097
 87467/100000: episode: 2561, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.399, mean reward: 0.399 [0.399, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.360, 10.100], loss: 0.007991, mae: 0.104873, mean_q: 15.245266
 87468/100000: episode: 2562, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.397, mean reward: 0.397 [0.397, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.397, 10.100], loss: 0.005450, mae: 0.078394, mean_q: 16.140669
 87469/100000: episode: 2563, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.342, mean reward: 0.342 [0.342, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.378, 10.100], loss: 0.005030, mae: 0.082990, mean_q: 21.637682
 87470/100000: episode: 2564, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.436, mean reward: 0.436 [0.436, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.410, 10.100], loss: 0.006815, mae: 0.105591, mean_q: 19.111855
 87471/100000: episode: 2565, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.403, mean reward: 0.403 [0.403, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.377, 10.100], loss: 0.014280, mae: 0.109168, mean_q: 19.421516
 87472/100000: episode: 2566, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.407, mean reward: 0.407 [0.407, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.389, 10.100], loss: 0.009870, mae: 0.118377, mean_q: 21.701584
 87473/100000: episode: 2567, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.394, 10.100], loss: 0.005835, mae: 0.075706, mean_q: 20.806898
 87474/100000: episode: 2568, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.427, mean reward: 0.427 [0.427, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.377, 10.100], loss: 0.009961, mae: 0.092766, mean_q: 19.915731
 87475/100000: episode: 2569, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.366, mean reward: 0.366 [0.366, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.405, 10.100], loss: 0.008496, mae: 0.109042, mean_q: 17.936214
 87476/100000: episode: 2570, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.412, mean reward: 0.412 [0.412, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.402, 10.100], loss: 0.011420, mae: 0.099825, mean_q: 19.822981
 87477/100000: episode: 2571, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.314, mean reward: 0.314 [0.314, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.384, 10.100], loss: 0.004925, mae: 0.075225, mean_q: 17.395281
 87478/100000: episode: 2572, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.288, mean reward: 0.288 [0.288, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.430, 10.100], loss: 0.010118, mae: 0.115037, mean_q: 16.307850
 87479/100000: episode: 2573, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.365, 10.100], loss: 0.008660, mae: 0.096687, mean_q: 17.502964
 87480/100000: episode: 2574, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.345, mean reward: 0.345 [0.345, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.443, 10.100], loss: 0.011710, mae: 0.117309, mean_q: 20.174850
 87481/100000: episode: 2575, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.355, mean reward: 0.355 [0.355, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.389, 10.100], loss: 0.010576, mae: 0.121509, mean_q: 15.108762
 87482/100000: episode: 2576, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.446, 10.100], loss: 0.013086, mae: 0.134560, mean_q: 18.648701
 87483/100000: episode: 2577, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.375, mean reward: 0.375 [0.375, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.402, 10.100], loss: 0.009269, mae: 0.105520, mean_q: 13.602641
 87484/100000: episode: 2578, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.336, mean reward: 0.336 [0.336, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.411, 10.100], loss: 0.009855, mae: 0.105857, mean_q: 13.936623
 87485/100000: episode: 2579, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.359, mean reward: 0.359 [0.359, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.443, 10.100], loss: 0.007816, mae: 0.095380, mean_q: 15.412822
 87486/100000: episode: 2580, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.363, 10.100], loss: 0.012646, mae: 0.131596, mean_q: 16.715914
 87487/100000: episode: 2581, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.355, 10.100], loss: 0.006149, mae: 0.083648, mean_q: 18.209900
 87488/100000: episode: 2582, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.408, 10.100], loss: 0.007041, mae: 0.105325, mean_q: 18.743843
 87489/100000: episode: 2583, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.425, 10.100], loss: 0.019648, mae: 0.136426, mean_q: 17.748451
 87490/100000: episode: 2584, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.375, mean reward: 0.375 [0.375, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.412, 10.100], loss: 0.008820, mae: 0.098896, mean_q: 19.700783
 87491/100000: episode: 2585, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.390, 10.100], loss: 0.016014, mae: 0.134019, mean_q: 16.772724
 87492/100000: episode: 2586, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.315, mean reward: 0.315 [0.315, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.389, 10.100], loss: 0.007928, mae: 0.095224, mean_q: 18.752146
 87493/100000: episode: 2587, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.399, mean reward: 0.399 [0.399, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.356, 10.100], loss: 0.011042, mae: 0.112497, mean_q: 17.290554
 87494/100000: episode: 2588, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.427, 10.100], loss: 0.013081, mae: 0.129098, mean_q: 18.928301
 87495/100000: episode: 2589, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.292, mean reward: 0.292 [0.292, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.415, 10.100], loss: 0.011524, mae: 0.109841, mean_q: 19.338837
 87496/100000: episode: 2590, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.393, 10.100], loss: 0.023528, mae: 0.110059, mean_q: 17.466135
 87497/100000: episode: 2591, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.385, 10.100], loss: 0.014664, mae: 0.135782, mean_q: 18.252491
 87498/100000: episode: 2592, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 0.350, mean reward: 0.350 [0.350, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.441, 10.100], loss: 0.008108, mae: 0.108345, mean_q: 15.529153
 87499/100000: episode: 2593, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.313, mean reward: 0.313 [0.313, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.423, 10.100], loss: 0.005480, mae: 0.082274, mean_q: 19.936495
 87500/100000: episode: 2594, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.373, mean reward: 0.373 [0.373, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.424, 10.100], loss: 0.021054, mae: 0.147253, mean_q: 17.594040
 87501/100000: episode: 2595, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.385, mean reward: 0.385 [0.385, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.407, 10.100], loss: 0.010977, mae: 0.115407, mean_q: 14.791834
 87502/100000: episode: 2596, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.446, mean reward: 0.446 [0.446, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.423, 10.100], loss: 0.006446, mae: 0.081492, mean_q: 20.596306
 87503/100000: episode: 2597, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.441, 10.100], loss: 0.006958, mae: 0.098018, mean_q: 19.751751
 87504/100000: episode: 2598, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.314, mean reward: 0.314 [0.314, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.431, 10.100], loss: 0.007296, mae: 0.091234, mean_q: 16.329914
 87505/100000: episode: 2599, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.374, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.361, 10.100], loss: 0.008269, mae: 0.105145, mean_q: 19.175270
 87506/100000: episode: 2600, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.347, mean reward: 0.347 [0.347, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.424, 10.100], loss: 0.006016, mae: 0.088610, mean_q: 16.449865
 87507/100000: episode: 2601, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.354, mean reward: 0.354 [0.354, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.402, 10.100], loss: 0.005342, mae: 0.084678, mean_q: 15.912656
 87508/100000: episode: 2602, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.419, mean reward: 0.419 [0.419, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.394, 10.100], loss: 0.010183, mae: 0.112364, mean_q: 19.682804
 87509/100000: episode: 2603, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.346, mean reward: 0.346 [0.346, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.395, 10.100], loss: 0.005123, mae: 0.078807, mean_q: 17.208569
 87510/100000: episode: 2604, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.427, 10.100], loss: 0.005833, mae: 0.088777, mean_q: 16.707848
 87511/100000: episode: 2605, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.421, mean reward: 0.421 [0.421, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.409, 10.100], loss: 0.011212, mae: 0.114115, mean_q: 16.689785
 87512/100000: episode: 2606, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.416, mean reward: 0.416 [0.416, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.372, 10.100], loss: 0.014654, mae: 0.131043, mean_q: 19.003471
 87513/100000: episode: 2607, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.363, mean reward: 0.363 [0.363, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.410, 10.100], loss: 0.007335, mae: 0.095196, mean_q: 17.925228
 87514/100000: episode: 2608, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.361, mean reward: 0.361 [0.361, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.384, 10.100], loss: 0.005799, mae: 0.087545, mean_q: 14.963467
 87515/100000: episode: 2609, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.357, mean reward: 0.357 [0.357, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.401, 10.100], loss: 0.006048, mae: 0.082340, mean_q: 16.519201
 87516/100000: episode: 2610, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.385, 10.100], loss: 0.009312, mae: 0.109009, mean_q: 19.340210
 87517/100000: episode: 2611, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.389, 10.100], loss: 0.010289, mae: 0.092698, mean_q: 17.110008
 87518/100000: episode: 2612, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.382, mean reward: 0.382 [0.382, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.411, 10.100], loss: 0.009944, mae: 0.112718, mean_q: 17.871044
 87519/100000: episode: 2613, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.361, mean reward: 0.361 [0.361, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.373, 10.100], loss: 0.009753, mae: 0.112891, mean_q: 16.652742
 87520/100000: episode: 2614, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.372, 10.100], loss: 0.005738, mae: 0.089032, mean_q: 19.219006
 87521/100000: episode: 2615, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.359, mean reward: 0.359 [0.359, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.386, 10.100], loss: 0.007750, mae: 0.104816, mean_q: 17.445375
 87522/100000: episode: 2616, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.372, 10.100], loss: 0.006404, mae: 0.088736, mean_q: 19.613136
 87523/100000: episode: 2617, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.436, mean reward: 0.436 [0.436, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.422, 10.100], loss: 0.007445, mae: 0.093778, mean_q: 19.413576
 87524/100000: episode: 2618, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.442, mean reward: 0.442 [0.442, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.384, 10.100], loss: 0.006687, mae: 0.086688, mean_q: 17.442921
 87525/100000: episode: 2619, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.368, mean reward: 0.368 [0.368, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.419, 10.100], loss: 0.005104, mae: 0.076190, mean_q: 17.497723
 87526/100000: episode: 2620, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.338, mean reward: 0.338 [0.338, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.425, 10.100], loss: 0.008148, mae: 0.102664, mean_q: 21.126919
 87527/100000: episode: 2621, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.366, mean reward: 0.366 [0.366, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.409, 10.100], loss: 0.005020, mae: 0.085060, mean_q: 15.964354
 87528/100000: episode: 2622, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.379, 10.100], loss: 0.005940, mae: 0.083274, mean_q: 16.889259
 87529/100000: episode: 2623, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.388, mean reward: 0.388 [0.388, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.416, 10.100], loss: 0.007560, mae: 0.102380, mean_q: 16.408192
 87530/100000: episode: 2624, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.380, 10.100], loss: 0.003779, mae: 0.067783, mean_q: 17.843443
 87531/100000: episode: 2625, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.338, mean reward: 0.338 [0.338, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.364, 10.100], loss: 0.009064, mae: 0.101945, mean_q: 18.058758
 87532/100000: episode: 2626, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.364, mean reward: 0.364 [0.364, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.402, 10.100], loss: 0.008925, mae: 0.097199, mean_q: 16.519464
 87533/100000: episode: 2627, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.309, mean reward: 0.309 [0.309, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.392, 10.100], loss: 0.008039, mae: 0.088988, mean_q: 17.262932
 87534/100000: episode: 2628, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.414, mean reward: 0.414 [0.414, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.370, 10.100], loss: 0.006850, mae: 0.094442, mean_q: 17.986454
 87535/100000: episode: 2629, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.427, mean reward: 0.427 [0.427, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.391, 10.100], loss: 0.012514, mae: 0.101132, mean_q: 20.134497
 87536/100000: episode: 2630, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.377, 10.100], loss: 0.010080, mae: 0.114035, mean_q: 18.735998
 87537/100000: episode: 2631, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.380, mean reward: 0.380 [0.380, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.370, 10.100], loss: 0.005942, mae: 0.084809, mean_q: 18.955894
 87538/100000: episode: 2632, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.392, mean reward: 0.392 [0.392, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.388, 10.100], loss: 0.005121, mae: 0.076840, mean_q: 20.257786
 87539/100000: episode: 2633, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.370, mean reward: 0.370 [0.370, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.426, 10.100], loss: 0.008400, mae: 0.102620, mean_q: 18.585205
 87540/100000: episode: 2634, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.378, 10.100], loss: 0.010435, mae: 0.117897, mean_q: 17.476513
 87541/100000: episode: 2635, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.381, mean reward: 0.381 [0.381, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.403, 10.100], loss: 0.011976, mae: 0.116767, mean_q: 20.720745
 87542/100000: episode: 2636, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.406, mean reward: 0.406 [0.406, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.394, 10.100], loss: 0.013762, mae: 0.126450, mean_q: 15.249103
 87543/100000: episode: 2637, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.379, mean reward: 0.379 [0.379, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.401, 10.100], loss: 0.031794, mae: 0.182236, mean_q: 17.333717
 87544/100000: episode: 2638, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.397, mean reward: 0.397 [0.397, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.395, 10.100], loss: 0.008649, mae: 0.104613, mean_q: 15.360081
 87545/100000: episode: 2639, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.320, mean reward: 0.320 [0.320, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.406, 10.100], loss: 0.020979, mae: 0.148751, mean_q: 16.856752
 87546/100000: episode: 2640, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.388, mean reward: 0.388 [0.388, 0.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.406, 10.100], loss: 0.020313, mae: 0.160446, mean_q: 18.657230
 87547/100000: episode: 2641, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.412, mean reward: 0.412 [0.412, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.456, 10.100], loss: 0.006225, mae: 0.093901, mean_q: 17.737150
 87548/100000: episode: 2642, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.392, mean reward: 0.392 [0.392, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.393, 10.100], loss: 0.010242, mae: 0.107401, mean_q: 18.167013
 87549/100000: episode: 2643, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.377, mean reward: 0.377 [0.377, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.427, 10.100], loss: 0.021966, mae: 0.176604, mean_q: 17.578512
 87550/100000: episode: 2644, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.348, mean reward: 0.348 [0.348, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.373, 10.100], loss: 0.012702, mae: 0.115438, mean_q: 21.547892
 87551/100000: episode: 2645, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.402, 10.100], loss: 0.007020, mae: 0.085853, mean_q: 16.557901
 87552/100000: episode: 2646, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.416, mean reward: 0.416 [0.416, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.381, 10.100], loss: 0.014503, mae: 0.138956, mean_q: 18.296078
 87553/100000: episode: 2647, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.382, 10.100], loss: 0.013191, mae: 0.133612, mean_q: 14.991819
 87554/100000: episode: 2648, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.340, mean reward: 0.340 [0.340, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.430, 10.100], loss: 0.007724, mae: 0.107162, mean_q: 18.189991
 87555/100000: episode: 2649, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.406, mean reward: 0.406 [0.406, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.445, 10.100], loss: 0.007845, mae: 0.098613, mean_q: 19.770554
[Info] Not found new level, current best level reached = -0.048601388931274414
 87556/100000: episode: 2650, duration: 4.044s, episode steps: 1, steps per second: 0, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.348, 10.100], loss: 0.004133, mae: 0.069388, mean_q: 18.528679
 87656/100000: episode: 2651, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 51.091, mean reward: 0.511 [0.263, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.616, 10.098], loss: 0.009527, mae: 0.103039, mean_q: 18.229488
 87756/100000: episode: 2652, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 53.391, mean reward: 0.534 [0.328, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.735, 10.165], loss: 0.009480, mae: 0.104733, mean_q: 17.968744
 87856/100000: episode: 2653, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: 54.628, mean reward: 0.546 [0.351, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.333, 10.098], loss: 0.010617, mae: 0.110591, mean_q: 17.655174
 87956/100000: episode: 2654, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 54.141, mean reward: 0.541 [0.302, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.463, 10.117], loss: 0.009435, mae: 0.104835, mean_q: 17.868328
 88056/100000: episode: 2655, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 51.889, mean reward: 0.519 [0.296, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.928, 10.098], loss: 0.008633, mae: 0.101215, mean_q: 18.088791
 88156/100000: episode: 2656, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.382, mean reward: 0.574 [0.368, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.501, 10.098], loss: 0.010315, mae: 0.108062, mean_q: 17.616114
 88256/100000: episode: 2657, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 51.672, mean reward: 0.517 [0.274, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.568, 10.098], loss: 0.009456, mae: 0.103883, mean_q: 17.984735
 88356/100000: episode: 2658, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 55.035, mean reward: 0.550 [0.365, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.554, 10.098], loss: 0.010308, mae: 0.111722, mean_q: 17.984417
 88456/100000: episode: 2659, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 53.967, mean reward: 0.540 [0.352, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.579, 10.098], loss: 0.009064, mae: 0.101658, mean_q: 17.509666
 88556/100000: episode: 2660, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 50.121, mean reward: 0.501 [0.209, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.503, 10.129], loss: 0.009100, mae: 0.102852, mean_q: 17.983608
 88656/100000: episode: 2661, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 53.922, mean reward: 0.539 [0.343, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.301, 10.266], loss: 0.008263, mae: 0.098647, mean_q: 17.851686
 88756/100000: episode: 2662, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 54.338, mean reward: 0.543 [0.347, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.811, 10.203], loss: 0.009488, mae: 0.105015, mean_q: 17.951004
 88856/100000: episode: 2663, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 54.573, mean reward: 0.546 [0.313, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.779, 10.265], loss: 0.009815, mae: 0.106003, mean_q: 17.706371
 88956/100000: episode: 2664, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 52.179, mean reward: 0.522 [0.238, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.350, 10.275], loss: 0.008747, mae: 0.100996, mean_q: 17.532627
 89056/100000: episode: 2665, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 55.198, mean reward: 0.552 [0.257, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.010, 10.098], loss: 0.009219, mae: 0.101825, mean_q: 17.981543
 89156/100000: episode: 2666, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 53.818, mean reward: 0.538 [0.328, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.898, 10.098], loss: 0.008654, mae: 0.099370, mean_q: 17.905869
 89256/100000: episode: 2667, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 50.636, mean reward: 0.506 [0.291, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.816, 10.234], loss: 0.009605, mae: 0.106383, mean_q: 17.909365
 89356/100000: episode: 2668, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 53.206, mean reward: 0.532 [0.301, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.886, 10.098], loss: 0.009783, mae: 0.106688, mean_q: 17.729145
 89456/100000: episode: 2669, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.830, mean reward: 0.568 [0.371, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.485, 10.277], loss: 0.008196, mae: 0.099195, mean_q: 17.864927
 89556/100000: episode: 2670, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 52.519, mean reward: 0.525 [0.219, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.645, 10.289], loss: 0.008901, mae: 0.100591, mean_q: 17.878359
 89656/100000: episode: 2671, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 55.142, mean reward: 0.551 [0.357, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.439, 10.218], loss: 0.009514, mae: 0.104849, mean_q: 17.539139
 89756/100000: episode: 2672, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 52.833, mean reward: 0.528 [0.139, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.687, 10.098], loss: 0.008054, mae: 0.096198, mean_q: 17.864443
 89856/100000: episode: 2673, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 53.570, mean reward: 0.536 [0.363, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.382, 10.153], loss: 0.008659, mae: 0.100071, mean_q: 17.944679
 89956/100000: episode: 2674, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 55.844, mean reward: 0.558 [0.341, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.898, 10.247], loss: 0.009592, mae: 0.105607, mean_q: 17.833122
 90056/100000: episode: 2675, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 51.474, mean reward: 0.515 [0.308, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.650, 10.239], loss: 0.010456, mae: 0.110486, mean_q: 17.856918
 90156/100000: episode: 2676, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 53.790, mean reward: 0.538 [0.206, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.963, 10.098], loss: 0.010772, mae: 0.109855, mean_q: 17.583397
 90256/100000: episode: 2677, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 50.592, mean reward: 0.506 [0.305, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.167, 10.098], loss: 0.009902, mae: 0.108401, mean_q: 18.301493
 90356/100000: episode: 2678, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 56.057, mean reward: 0.561 [0.329, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.077, 10.098], loss: 0.010379, mae: 0.108882, mean_q: 17.777542
 90456/100000: episode: 2679, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 50.552, mean reward: 0.506 [0.277, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.889, 10.098], loss: 0.010061, mae: 0.107537, mean_q: 17.753300
 90556/100000: episode: 2680, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 52.693, mean reward: 0.527 [0.304, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.544, 10.333], loss: 0.010073, mae: 0.106850, mean_q: 17.801888
 90656/100000: episode: 2681, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 51.507, mean reward: 0.515 [0.317, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.200, 10.098], loss: 0.009950, mae: 0.106874, mean_q: 17.816360
 90756/100000: episode: 2682, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 53.892, mean reward: 0.539 [0.288, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.011, 10.256], loss: 0.010789, mae: 0.110074, mean_q: 17.780859
 90856/100000: episode: 2683, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 54.245, mean reward: 0.542 [0.238, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.303, 10.216], loss: 0.010725, mae: 0.110081, mean_q: 17.975243
 90956/100000: episode: 2684, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 49.961, mean reward: 0.500 [0.211, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.458, 10.098], loss: 0.011576, mae: 0.115411, mean_q: 17.673601
 91056/100000: episode: 2685, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 56.654, mean reward: 0.567 [0.383, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.578, 10.098], loss: 0.009456, mae: 0.105171, mean_q: 17.594526
 91156/100000: episode: 2686, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 49.988, mean reward: 0.500 [0.261, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.506, 10.098], loss: 0.010120, mae: 0.106404, mean_q: 17.741398
 91256/100000: episode: 2687, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 54.348, mean reward: 0.543 [0.354, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.738, 10.138], loss: 0.010644, mae: 0.112796, mean_q: 17.723146
 91356/100000: episode: 2688, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 56.038, mean reward: 0.560 [0.360, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.499, 10.112], loss: 0.010576, mae: 0.111397, mean_q: 17.736877
 91456/100000: episode: 2689, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 54.377, mean reward: 0.544 [0.361, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.422, 10.169], loss: 0.010712, mae: 0.109712, mean_q: 18.052214
 91556/100000: episode: 2690, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 55.226, mean reward: 0.552 [0.226, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.852, 10.098], loss: 0.009621, mae: 0.103800, mean_q: 17.677242
 91656/100000: episode: 2691, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 53.168, mean reward: 0.532 [0.360, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.330, 10.098], loss: 0.009687, mae: 0.105172, mean_q: 17.861809
 91756/100000: episode: 2692, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 53.376, mean reward: 0.534 [0.341, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.141, 10.118], loss: 0.010240, mae: 0.108004, mean_q: 17.925940
 91856/100000: episode: 2693, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 54.728, mean reward: 0.547 [0.353, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.538, 10.286], loss: 0.009875, mae: 0.101530, mean_q: 17.983244
 91956/100000: episode: 2694, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 52.998, mean reward: 0.530 [0.303, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.043, 10.169], loss: 0.012878, mae: 0.120933, mean_q: 18.462091
 92056/100000: episode: 2695, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 52.865, mean reward: 0.529 [0.258, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.790, 10.098], loss: 0.010281, mae: 0.106803, mean_q: 19.027851
 92156/100000: episode: 2696, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 52.293, mean reward: 0.523 [0.252, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.106, 10.098], loss: 0.010519, mae: 0.105561, mean_q: 18.913368
 92256/100000: episode: 2697, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 52.859, mean reward: 0.529 [0.374, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.489, 10.177], loss: 0.011664, mae: 0.111966, mean_q: 19.447145
 92356/100000: episode: 2698, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 53.244, mean reward: 0.532 [0.276, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.569, 10.287], loss: 0.011733, mae: 0.115101, mean_q: 19.519861
 92456/100000: episode: 2699, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 54.257, mean reward: 0.543 [0.261, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.568, 10.098], loss: 0.012371, mae: 0.115795, mean_q: 19.619684
 92556/100000: episode: 2700, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 53.319, mean reward: 0.533 [0.284, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.006, 10.258], loss: 0.010130, mae: 0.107294, mean_q: 20.051683
 92656/100000: episode: 2701, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 55.997, mean reward: 0.560 [0.362, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.632, 10.128], loss: 0.010068, mae: 0.105498, mean_q: 19.992004
 92756/100000: episode: 2702, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 49.050, mean reward: 0.491 [0.190, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.748, 10.098], loss: 0.011209, mae: 0.107162, mean_q: 20.020452
 92856/100000: episode: 2703, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 54.013, mean reward: 0.540 [0.259, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.554, 10.098], loss: 0.010549, mae: 0.106275, mean_q: 19.749235
 92956/100000: episode: 2704, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 56.157, mean reward: 0.562 [0.334, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.900, 10.124], loss: 0.012174, mae: 0.115536, mean_q: 20.044790
 93056/100000: episode: 2705, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 52.075, mean reward: 0.521 [0.322, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.663, 10.269], loss: 0.013473, mae: 0.122181, mean_q: 19.868006
 93156/100000: episode: 2706, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 53.359, mean reward: 0.534 [0.220, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.761, 10.341], loss: 0.009530, mae: 0.103589, mean_q: 19.858801
 93256/100000: episode: 2707, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 55.948, mean reward: 0.559 [0.286, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.973, 10.223], loss: 0.011270, mae: 0.111672, mean_q: 19.780434
 93356/100000: episode: 2708, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 50.179, mean reward: 0.502 [0.230, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.688, 10.098], loss: 0.012523, mae: 0.117644, mean_q: 20.197121
 93456/100000: episode: 2709, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 55.096, mean reward: 0.551 [0.363, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.512, 10.098], loss: 0.010523, mae: 0.108660, mean_q: 19.860516
 93556/100000: episode: 2710, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 51.006, mean reward: 0.510 [0.255, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.295, 10.098], loss: 0.009890, mae: 0.102576, mean_q: 19.850946
 93656/100000: episode: 2711, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.680, mean reward: 0.547 [0.331, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.646, 10.105], loss: 0.010857, mae: 0.108979, mean_q: 19.857363
 93756/100000: episode: 2712, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 53.847, mean reward: 0.538 [0.244, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.520, 10.098], loss: 0.011325, mae: 0.112015, mean_q: 19.698317
 93856/100000: episode: 2713, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.081, mean reward: 0.561 [0.332, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.204, 10.220], loss: 0.010913, mae: 0.110449, mean_q: 19.792551
 93956/100000: episode: 2714, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 51.208, mean reward: 0.512 [0.257, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.327, 10.098], loss: 0.008899, mae: 0.100536, mean_q: 19.944233
 94056/100000: episode: 2715, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 53.544, mean reward: 0.535 [0.345, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.976, 10.138], loss: 0.010691, mae: 0.111697, mean_q: 19.809700
 94156/100000: episode: 2716, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 55.650, mean reward: 0.557 [0.291, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.538, 10.098], loss: 0.009546, mae: 0.103403, mean_q: 19.916227
 94256/100000: episode: 2717, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 53.463, mean reward: 0.535 [0.303, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.835, 10.113], loss: 0.009292, mae: 0.103577, mean_q: 19.911928
 94356/100000: episode: 2718, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 56.269, mean reward: 0.563 [0.345, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.322, 10.098], loss: 0.008670, mae: 0.099783, mean_q: 20.170664
 94456/100000: episode: 2719, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 52.417, mean reward: 0.524 [0.376, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.995, 10.098], loss: 0.009335, mae: 0.104801, mean_q: 20.001310
 94556/100000: episode: 2720, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 56.129, mean reward: 0.561 [0.293, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.515, 10.098], loss: 0.008719, mae: 0.103003, mean_q: 19.880905
 94656/100000: episode: 2721, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 52.128, mean reward: 0.521 [0.295, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.295, 10.098], loss: 0.009381, mae: 0.104107, mean_q: 19.619576
 94756/100000: episode: 2722, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.055, mean reward: 0.561 [0.335, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.414, 10.234], loss: 0.008507, mae: 0.101305, mean_q: 19.918213
 94856/100000: episode: 2723, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 53.578, mean reward: 0.536 [0.326, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.451, 10.104], loss: 0.007499, mae: 0.095296, mean_q: 20.287537
 94956/100000: episode: 2724, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 55.398, mean reward: 0.554 [0.380, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.131, 10.098], loss: 0.009760, mae: 0.107530, mean_q: 19.983103
 95056/100000: episode: 2725, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 47.250, mean reward: 0.473 [0.146, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.131, 10.368], loss: 0.009332, mae: 0.105498, mean_q: 19.628778
 95156/100000: episode: 2726, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 52.294, mean reward: 0.523 [0.247, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.276, 10.380], loss: 0.008920, mae: 0.102397, mean_q: 19.967125
 95256/100000: episode: 2727, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 52.550, mean reward: 0.525 [0.314, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.897, 10.098], loss: 0.008296, mae: 0.100017, mean_q: 19.992393
 95356/100000: episode: 2728, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 55.407, mean reward: 0.554 [0.355, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.578, 10.142], loss: 0.009097, mae: 0.104436, mean_q: 19.790068
 95456/100000: episode: 2729, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 51.163, mean reward: 0.512 [0.145, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.446, 10.098], loss: 0.008316, mae: 0.100324, mean_q: 20.288630
 95556/100000: episode: 2730, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 52.971, mean reward: 0.530 [0.274, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.942, 10.098], loss: 0.009810, mae: 0.109072, mean_q: 20.120787
 95656/100000: episode: 2731, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 51.864, mean reward: 0.519 [0.262, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.295, 10.121], loss: 0.008368, mae: 0.099542, mean_q: 19.772093
 95756/100000: episode: 2732, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 55.938, mean reward: 0.559 [0.291, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.300, 10.098], loss: 0.009233, mae: 0.105151, mean_q: 19.988754
 95856/100000: episode: 2733, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 51.522, mean reward: 0.515 [0.311, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.230, 10.415], loss: 0.008283, mae: 0.099703, mean_q: 19.799833
 95956/100000: episode: 2734, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 51.012, mean reward: 0.510 [0.194, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.546, 10.296], loss: 0.009006, mae: 0.104162, mean_q: 19.495056
 96056/100000: episode: 2735, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 49.121, mean reward: 0.491 [0.212, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.407, 10.098], loss: 0.007581, mae: 0.095878, mean_q: 19.964718
 96156/100000: episode: 2736, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 55.236, mean reward: 0.552 [0.369, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.997, 10.098], loss: 0.009223, mae: 0.104204, mean_q: 19.681097
 96256/100000: episode: 2737, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 55.189, mean reward: 0.552 [0.258, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.612, 10.098], loss: 0.009295, mae: 0.105377, mean_q: 19.805618
 96356/100000: episode: 2738, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 53.026, mean reward: 0.530 [0.324, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.233, 10.098], loss: 0.008749, mae: 0.103046, mean_q: 19.865875
 96456/100000: episode: 2739, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 50.729, mean reward: 0.507 [0.272, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.407, 10.098], loss: 0.010214, mae: 0.111027, mean_q: 19.745115
 96556/100000: episode: 2740, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 54.994, mean reward: 0.550 [0.273, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.896, 10.098], loss: 0.009763, mae: 0.108138, mean_q: 19.737839
 96656/100000: episode: 2741, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 56.019, mean reward: 0.560 [0.357, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.545, 10.154], loss: 0.009102, mae: 0.104823, mean_q: 19.707634
 96756/100000: episode: 2742, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 54.159, mean reward: 0.542 [0.314, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.537, 10.098], loss: 0.009295, mae: 0.106437, mean_q: 19.975985
 96856/100000: episode: 2743, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.254, mean reward: 0.583 [0.321, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.914, 10.205], loss: 0.008425, mae: 0.100780, mean_q: 19.950512
 96956/100000: episode: 2744, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 53.350, mean reward: 0.534 [0.252, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.353, 10.098], loss: 0.008965, mae: 0.105195, mean_q: 19.368361
 97056/100000: episode: 2745, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 54.436, mean reward: 0.544 [0.304, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.210, 10.098], loss: 0.011094, mae: 0.115371, mean_q: 20.215837
 97156/100000: episode: 2746, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 52.429, mean reward: 0.524 [0.241, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.652, 10.098], loss: 0.008946, mae: 0.103139, mean_q: 19.631683
 97256/100000: episode: 2747, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 53.341, mean reward: 0.533 [0.193, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.065, 10.162], loss: 0.009599, mae: 0.105414, mean_q: 20.113373
 97356/100000: episode: 2748, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 53.231, mean reward: 0.532 [0.360, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.007, 10.259], loss: 0.010481, mae: 0.111193, mean_q: 19.820511
 97456/100000: episode: 2749, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 53.870, mean reward: 0.539 [0.291, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.814, 10.098], loss: 0.010281, mae: 0.110810, mean_q: 19.811729
[Info] New level: 0.12740212678909302 | Considering 10/90 traces
 97556/100000: episode: 2750, duration: 4.508s, episode steps: 100, steps per second: 22, episode reward: 50.044, mean reward: 0.500 [0.185, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.703, 10.098], loss: 0.009703, mae: 0.106788, mean_q: 19.832569
 97558/100000: episode: 2751, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.553, mean reward: 0.277 [0.224, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.209, 10.100], loss: 0.011929, mae: 0.108668, mean_q: 21.335430
 97560/100000: episode: 2752, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.028, mean reward: 0.514 [0.483, 0.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.124, 10.100], loss: 0.011012, mae: 0.111414, mean_q: 20.259665
 97562/100000: episode: 2753, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.687, mean reward: 0.344 [0.340, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.240, 10.100], loss: 0.012533, mae: 0.123308, mean_q: 19.867115
 97564/100000: episode: 2754, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.005, mean reward: 0.502 [0.497, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.310, 10.100], loss: 0.012967, mae: 0.125059, mean_q: 17.067942
 97566/100000: episode: 2755, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 1.329, mean reward: 0.664 [0.641, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.178, 10.105], loss: 0.008408, mae: 0.100519, mean_q: 19.494381
 97568/100000: episode: 2756, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.212, mean reward: 0.606 [0.575, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.268, 10.119], loss: 0.008836, mae: 0.103788, mean_q: 18.087006
 97570/100000: episode: 2757, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.134, mean reward: 0.567 [0.552, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.251, 10.100], loss: 0.009831, mae: 0.115025, mean_q: 20.552340
 97572/100000: episode: 2758, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.699, mean reward: 0.349 [0.284, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.286, 10.100], loss: 0.009065, mae: 0.103364, mean_q: 16.116360
 97574/100000: episode: 2759, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 1.193, mean reward: 0.597 [0.596, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.445, 10.100], loss: 0.010287, mae: 0.116024, mean_q: 21.245991
 97576/100000: episode: 2760, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.535, mean reward: 0.267 [0.261, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.293, 10.100], loss: 0.011111, mae: 0.100729, mean_q: 19.539577
 97578/100000: episode: 2761, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.922, mean reward: 0.461 [0.458, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.679, 10.100], loss: 0.006830, mae: 0.094246, mean_q: 18.231539
 97580/100000: episode: 2762, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.663, mean reward: 0.331 [0.266, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.240, 10.100], loss: 0.012054, mae: 0.123683, mean_q: 18.156937
 97582/100000: episode: 2763, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.892, mean reward: 0.446 [0.424, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.187, 10.100], loss: 0.013500, mae: 0.125735, mean_q: 19.972229
 97584/100000: episode: 2764, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.216, mean reward: 0.608 [0.567, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.228, 10.100], loss: 0.005194, mae: 0.081757, mean_q: 18.670391
 97586/100000: episode: 2765, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.599, mean reward: 0.299 [0.240, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.306, 10.100], loss: 0.005065, mae: 0.076134, mean_q: 20.701435
 97588/100000: episode: 2766, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.928, mean reward: 0.464 [0.419, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.328, 10.100], loss: 0.007113, mae: 0.095692, mean_q: 20.600531
 97590/100000: episode: 2767, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.496, mean reward: 0.248 [0.205, 0.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.145, 10.100], loss: 0.008790, mae: 0.105815, mean_q: 21.093567
 97592/100000: episode: 2768, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.685, mean reward: 0.343 [0.336, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.191, 10.100], loss: 0.005398, mae: 0.081983, mean_q: 21.314968
 97594/100000: episode: 2769, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.629, mean reward: 0.315 [0.291, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.293, 10.100], loss: 0.008697, mae: 0.101469, mean_q: 20.234501
 97596/100000: episode: 2770, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.114, mean reward: 0.557 [0.537, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.223, 10.119], loss: 0.006399, mae: 0.091138, mean_q: 19.722786
 97598/100000: episode: 2771, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.772, mean reward: 0.386 [0.369, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.291, 10.100], loss: 0.007155, mae: 0.082818, mean_q: 19.444036
 97600/100000: episode: 2772, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.787, mean reward: 0.393 [0.364, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.352, 10.100], loss: 0.013990, mae: 0.124464, mean_q: 19.496305
 97602/100000: episode: 2773, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.250, mean reward: 0.625 [0.612, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.240, 10.156], loss: 0.016662, mae: 0.147796, mean_q: 19.275490
 97604/100000: episode: 2774, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.698, mean reward: 0.349 [0.345, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.238, 10.100], loss: 0.010879, mae: 0.119983, mean_q: 21.848284
 97606/100000: episode: 2775, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.036, mean reward: 0.518 [0.494, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.167, 10.136], loss: 0.013844, mae: 0.138191, mean_q: 18.552546
 97608/100000: episode: 2776, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.879, mean reward: 0.439 [0.362, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.142, 10.100], loss: 0.009551, mae: 0.099237, mean_q: 22.750830
 97610/100000: episode: 2777, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 1.132, mean reward: 0.566 [0.559, 0.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.803, 10.100], loss: 0.012350, mae: 0.124794, mean_q: 20.351467
 97612/100000: episode: 2778, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.737, mean reward: 0.368 [0.361, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.338, 10.100], loss: 0.012455, mae: 0.113212, mean_q: 19.794788
 97614/100000: episode: 2779, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.494, mean reward: 0.247 [0.242, 0.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.259, 10.100], loss: 0.011092, mae: 0.118779, mean_q: 19.625355
 97616/100000: episode: 2780, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.873, mean reward: 0.437 [0.344, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.320, 10.100], loss: 0.018381, mae: 0.146491, mean_q: 18.526100
 97618/100000: episode: 2781, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.590, mean reward: 0.295 [0.265, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.214, 10.100], loss: 0.006528, mae: 0.087221, mean_q: 20.369766
 97620/100000: episode: 2782, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.614, mean reward: 0.307 [0.288, 0.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.242, 10.100], loss: 0.010124, mae: 0.110869, mean_q: 17.351057
 97622/100000: episode: 2783, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.190, mean reward: 0.595 [0.565, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.207, 10.100], loss: 0.010292, mae: 0.116157, mean_q: 19.566835
 97624/100000: episode: 2784, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.122, mean reward: 0.561 [0.531, 0.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.222, 10.100], loss: 0.011230, mae: 0.115732, mean_q: 19.006996
 97626/100000: episode: 2785, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.741, mean reward: 0.370 [0.363, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.211, 10.100], loss: 0.012348, mae: 0.117496, mean_q: 18.651516
 97628/100000: episode: 2786, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.806, mean reward: 0.403 [0.383, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.252, 10.100], loss: 0.014525, mae: 0.127294, mean_q: 16.875622
 97630/100000: episode: 2787, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 1.351, mean reward: 0.676 [0.674, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.192, 10.108], loss: 0.010121, mae: 0.106093, mean_q: 20.621555
 97632/100000: episode: 2788, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.154, mean reward: 0.577 [0.535, 0.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.230, 10.100], loss: 0.006986, mae: 0.094507, mean_q: 20.711098
 97634/100000: episode: 2789, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.720, mean reward: 0.360 [0.351, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.227, 10.100], loss: 0.009166, mae: 0.100096, mean_q: 19.594166
 97636/100000: episode: 2790, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.629, mean reward: 0.314 [0.301, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.298, 10.100], loss: 0.008803, mae: 0.094946, mean_q: 20.721527
 97638/100000: episode: 2791, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.826, mean reward: 0.413 [0.318, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.177, 10.100], loss: 0.005642, mae: 0.080705, mean_q: 20.406973
 97640/100000: episode: 2792, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.202, mean reward: 0.601 [0.560, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.169, 10.100], loss: 0.012508, mae: 0.109027, mean_q: 18.900784
 97642/100000: episode: 2793, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.043, mean reward: 0.521 [0.509, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.306, 10.100], loss: 0.009775, mae: 0.112280, mean_q: 19.020256
 97644/100000: episode: 2794, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.496, mean reward: 0.248 [0.189, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.240, 10.100], loss: 0.012477, mae: 0.118776, mean_q: 17.575352
 97646/100000: episode: 2795, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 1.083, mean reward: 0.542 [0.490, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.188, 10.143], loss: 0.011726, mae: 0.122766, mean_q: 18.981110
 97648/100000: episode: 2796, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 1.158, mean reward: 0.579 [0.495, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.232, 10.100], loss: 0.013116, mae: 0.125199, mean_q: 20.091251
 97650/100000: episode: 2797, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.970, mean reward: 0.485 [0.448, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.257, 10.100], loss: 0.006300, mae: 0.080410, mean_q: 19.927105
 97652/100000: episode: 2798, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.002, mean reward: 0.501 [0.499, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.187, 10.100], loss: 0.010619, mae: 0.100501, mean_q: 20.190441
 97654/100000: episode: 2799, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.083, mean reward: 0.541 [0.493, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.249, 10.108], loss: 0.008876, mae: 0.105375, mean_q: 20.576923
 97656/100000: episode: 2800, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 0.822, mean reward: 0.411 [0.395, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.349, 10.100], loss: 0.008130, mae: 0.093171, mean_q: 20.857159
 97658/100000: episode: 2801, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 1.209, mean reward: 0.605 [0.593, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.186, 10.100], loss: 0.007277, mae: 0.094715, mean_q: 19.206284
 97660/100000: episode: 2802, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.691, mean reward: 0.345 [0.334, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.153, 10.100], loss: 0.011646, mae: 0.115097, mean_q: 17.782093
 97662/100000: episode: 2803, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 1.310, mean reward: 0.655 [0.627, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.149, 10.148], loss: 0.014809, mae: 0.134263, mean_q: 20.849792
 97664/100000: episode: 2804, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 1.008, mean reward: 0.504 [0.449, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.200, 10.100], loss: 0.010634, mae: 0.118090, mean_q: 20.167416
 97666/100000: episode: 2805, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.252, mean reward: 0.626 [0.604, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.228, 10.100], loss: 0.010225, mae: 0.111662, mean_q: 18.612705
 97668/100000: episode: 2806, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.822, mean reward: 0.411 [0.360, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.182, 10.100], loss: 0.008975, mae: 0.101921, mean_q: 17.757689
 97670/100000: episode: 2807, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.475, mean reward: 0.238 [0.143, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.254, 10.100], loss: 0.013331, mae: 0.117616, mean_q: 17.868023
 97672/100000: episode: 2808, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.535, mean reward: 0.267 [0.233, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.215, 10.100], loss: 0.011968, mae: 0.132693, mean_q: 19.489513
 97674/100000: episode: 2809, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.871, mean reward: 0.435 [0.404, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.325, 10.100], loss: 0.015002, mae: 0.141558, mean_q: 18.364780
 97676/100000: episode: 2810, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.601, mean reward: 0.301 [0.297, 0.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.210, 10.100], loss: 0.019013, mae: 0.149046, mean_q: 18.067017
 97678/100000: episode: 2811, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.027, mean reward: 0.513 [0.368, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.206, 10.196], loss: 0.012370, mae: 0.124582, mean_q: 18.932190
 97680/100000: episode: 2812, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.695, mean reward: 0.348 [0.304, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.286, 10.100], loss: 0.016130, mae: 0.120344, mean_q: 19.480919
 97682/100000: episode: 2813, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.980, mean reward: 0.490 [0.486, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.149, 10.100], loss: 0.012331, mae: 0.125342, mean_q: 18.451220
 97684/100000: episode: 2814, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.673, mean reward: 0.337 [0.281, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.189, 10.100], loss: 0.009911, mae: 0.110733, mean_q: 22.389025
 97686/100000: episode: 2815, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.503, mean reward: 0.252 [0.238, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.295, 10.100], loss: 0.008026, mae: 0.099117, mean_q: 18.891714
 97688/100000: episode: 2816, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.849, mean reward: 0.425 [0.419, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.160, 10.100], loss: 0.011919, mae: 0.113619, mean_q: 17.463541
 97690/100000: episode: 2817, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 1.303, mean reward: 0.652 [0.642, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.133, 10.100], loss: 0.007780, mae: 0.096115, mean_q: 18.952103
 97692/100000: episode: 2818, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.820, mean reward: 0.410 [0.397, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.310, 10.100], loss: 0.007710, mae: 0.097979, mean_q: 20.665375
 97694/100000: episode: 2819, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.043, mean reward: 0.522 [0.495, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.353, 10.100], loss: 0.009806, mae: 0.104649, mean_q: 19.820217
 97696/100000: episode: 2820, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.132, mean reward: 0.566 [0.543, 0.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.212, 10.100], loss: 0.005862, mae: 0.085689, mean_q: 18.957096
 97698/100000: episode: 2821, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.954, mean reward: 0.477 [0.476, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.332, 10.100], loss: 0.010845, mae: 0.103294, mean_q: 18.888195
 97700/100000: episode: 2822, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 1.248, mean reward: 0.624 [0.621, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.204, 10.130], loss: 0.011935, mae: 0.115434, mean_q: 17.829029
 97702/100000: episode: 2823, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.988, mean reward: 0.494 [0.471, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.162, 10.100], loss: 0.007662, mae: 0.091392, mean_q: 18.774408
 97704/100000: episode: 2824, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.158, mean reward: 0.579 [0.571, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.091, 10.100], loss: 0.008266, mae: 0.102545, mean_q: 18.886086
 97706/100000: episode: 2825, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.722, mean reward: 0.361 [0.352, 0.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.330, 10.100], loss: 0.008065, mae: 0.101853, mean_q: 18.636814
 97708/100000: episode: 2826, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.676, mean reward: 0.338 [0.303, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.236, 10.100], loss: 0.008775, mae: 0.096910, mean_q: 20.165709
 97710/100000: episode: 2827, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 1.173, mean reward: 0.587 [0.570, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.200, 10.100], loss: 0.009957, mae: 0.113888, mean_q: 20.923052
 97712/100000: episode: 2828, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.783, mean reward: 0.391 [0.391, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.255, 10.100], loss: 0.012585, mae: 0.119776, mean_q: 22.163544
 97714/100000: episode: 2829, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.178, mean reward: 0.589 [0.574, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.223, 10.100], loss: 0.008293, mae: 0.103061, mean_q: 19.204853
 97716/100000: episode: 2830, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.096, mean reward: 0.548 [0.512, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.167, 10.100], loss: 0.013548, mae: 0.117985, mean_q: 19.125393
 97718/100000: episode: 2831, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.748, mean reward: 0.374 [0.279, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.229, 10.100], loss: 0.014389, mae: 0.134979, mean_q: 18.449245
 97720/100000: episode: 2832, duration: 0.022s, episode steps: 2, steps per second: 93, episode reward: 0.857, mean reward: 0.428 [0.426, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.252, 10.100], loss: 0.015641, mae: 0.141846, mean_q: 18.315187
 97722/100000: episode: 2833, duration: 0.022s, episode steps: 2, steps per second: 93, episode reward: 1.062, mean reward: 0.531 [0.512, 0.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.312, 10.100], loss: 0.012626, mae: 0.122024, mean_q: 19.247890
 97724/100000: episode: 2834, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.687, mean reward: 0.344 [0.320, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.354, 10.100], loss: 0.011270, mae: 0.107950, mean_q: 18.494904
 97726/100000: episode: 2835, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.797, mean reward: 0.398 [0.345, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.298, 10.100], loss: 0.009975, mae: 0.113434, mean_q: 15.949402
 97728/100000: episode: 2836, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.072, mean reward: 0.536 [0.507, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.144, 10.100], loss: 0.012116, mae: 0.118383, mean_q: 18.868685
 97730/100000: episode: 2837, duration: 0.021s, episode steps: 2, steps per second: 96, episode reward: 1.057, mean reward: 0.529 [0.471, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.138, 10.145], loss: 0.010234, mae: 0.115012, mean_q: 17.835869
 97732/100000: episode: 2838, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.918, mean reward: 0.459 [0.412, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.208, 10.100], loss: 0.010554, mae: 0.118061, mean_q: 20.763449
 97734/100000: episode: 2839, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.918, mean reward: 0.459 [0.382, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.247, 10.100], loss: 0.013006, mae: 0.130964, mean_q: 18.638966
[Info] New level: 0.03414130210876465 | Considering 10/90 traces
 97736/100000: episode: 2840, duration: 4.061s, episode steps: 2, steps per second: 0, episode reward: 1.291, mean reward: 0.646 [0.637, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.193, 10.100], loss: 0.012317, mae: 0.128984, mean_q: 20.919819
 97737/100000: episode: 2841, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.335, 10.100], loss: 0.007966, mae: 0.096033, mean_q: 18.802801
 97738/100000: episode: 2842, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.425, mean reward: 0.425 [0.425, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.373, 10.100], loss: 0.013345, mae: 0.125002, mean_q: 18.620159
 97739/100000: episode: 2843, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.303, 10.100], loss: 0.017729, mae: 0.144812, mean_q: 19.665300
 97740/100000: episode: 2844, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.308, mean reward: 0.308 [0.308, 0.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.287, 10.100], loss: 0.010023, mae: 0.119074, mean_q: 19.467733
 97741/100000: episode: 2845, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.451, mean reward: 0.451 [0.451, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.305, 10.100], loss: 0.008208, mae: 0.095607, mean_q: 20.122601
 97742/100000: episode: 2846, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.490, mean reward: 0.490 [0.490, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.310, 10.100], loss: 0.021162, mae: 0.171528, mean_q: 17.343510
 97743/100000: episode: 2847, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.327, 10.100], loss: 0.018009, mae: 0.155463, mean_q: 16.645992
 97744/100000: episode: 2848, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.512, mean reward: 0.512 [0.512, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.312, 10.100], loss: 0.013552, mae: 0.130143, mean_q: 16.062038
 97745/100000: episode: 2849, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.436, mean reward: 0.436 [0.436, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.320, 10.100], loss: 0.016520, mae: 0.136645, mean_q: 16.341169
 97746/100000: episode: 2850, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.492, mean reward: 0.492 [0.492, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.350, 10.100], loss: 0.007134, mae: 0.082133, mean_q: 17.425041
 97747/100000: episode: 2851, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.377, mean reward: 0.377 [0.377, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.298, 10.100], loss: 0.008232, mae: 0.100670, mean_q: 19.382044
 97748/100000: episode: 2852, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.445, mean reward: 0.445 [0.445, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.305, 10.100], loss: 0.007030, mae: 0.102339, mean_q: 20.740780
 97749/100000: episode: 2853, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.361, mean reward: 0.361 [0.361, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.314, 10.100], loss: 0.014928, mae: 0.129363, mean_q: 17.915064
 97750/100000: episode: 2854, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.507, mean reward: 0.507 [0.507, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.320, 10.100], loss: 0.011897, mae: 0.114152, mean_q: 15.069284
 97751/100000: episode: 2855, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.439, mean reward: 0.439 [0.439, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.345, 10.100], loss: 0.008987, mae: 0.091646, mean_q: 15.694420
 97752/100000: episode: 2856, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.505, mean reward: 0.505 [0.505, 0.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.307, 10.100], loss: 0.013030, mae: 0.124042, mean_q: 20.487293
 97753/100000: episode: 2857, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.484, mean reward: 0.484 [0.484, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.294, 10.100], loss: 0.014473, mae: 0.126637, mean_q: 21.559868
 97754/100000: episode: 2858, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.322, 10.100], loss: 0.006386, mae: 0.093937, mean_q: 18.242022
 97755/100000: episode: 2859, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.360, 10.100], loss: 0.012064, mae: 0.126046, mean_q: 16.476879
 97756/100000: episode: 2860, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.422, mean reward: 0.422 [0.422, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.321, 10.100], loss: 0.015752, mae: 0.141742, mean_q: 15.131388
 97757/100000: episode: 2861, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.464, mean reward: 0.464 [0.464, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.299, 10.100], loss: 0.010658, mae: 0.098758, mean_q: 17.511635
 97758/100000: episode: 2862, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.474, mean reward: 0.474 [0.474, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.266, 10.100], loss: 0.007137, mae: 0.090676, mean_q: 18.378462
 97759/100000: episode: 2863, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.330, 10.100], loss: 0.012807, mae: 0.128343, mean_q: 17.613972
 97760/100000: episode: 2864, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.349, mean reward: 0.349 [0.349, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.333, 10.100], loss: 0.006133, mae: 0.087365, mean_q: 20.705582
 97761/100000: episode: 2865, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.295, 10.100], loss: 0.018479, mae: 0.151834, mean_q: 19.782034
 97762/100000: episode: 2866, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.468, mean reward: 0.468 [0.468, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.339, 10.100], loss: 0.011157, mae: 0.120617, mean_q: 15.187872
 97763/100000: episode: 2867, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.303, 10.100], loss: 0.012542, mae: 0.112811, mean_q: 20.474682
 97764/100000: episode: 2868, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.418, mean reward: 0.418 [0.418, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.316, 10.100], loss: 0.007483, mae: 0.104183, mean_q: 17.513861
 97765/100000: episode: 2869, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.417, mean reward: 0.417 [0.417, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.313, 10.100], loss: 0.008446, mae: 0.116776, mean_q: 18.788486
 97766/100000: episode: 2870, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.486, mean reward: 0.486 [0.486, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.269, 10.100], loss: 0.008792, mae: 0.109879, mean_q: 18.705509
 97767/100000: episode: 2871, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.359, mean reward: 0.359 [0.359, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.305, 10.100], loss: 0.004901, mae: 0.076439, mean_q: 19.582802
 97768/100000: episode: 2872, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.503, mean reward: 0.503 [0.503, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.331, 10.100], loss: 0.013139, mae: 0.132474, mean_q: 19.349205
 97769/100000: episode: 2873, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.302, 10.100], loss: 0.015676, mae: 0.142008, mean_q: 19.684971
 97770/100000: episode: 2874, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.493, mean reward: 0.493 [0.493, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.319, 10.100], loss: 0.019088, mae: 0.147792, mean_q: 20.252071
 97771/100000: episode: 2875, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.503, mean reward: 0.503 [0.503, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.295, 10.100], loss: 0.005501, mae: 0.087328, mean_q: 15.154601
 97772/100000: episode: 2876, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.360, mean reward: 0.360 [0.360, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.358, 10.100], loss: 0.005898, mae: 0.095304, mean_q: 18.115232
 97773/100000: episode: 2877, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.320, mean reward: 0.320 [0.320, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.345, 10.100], loss: 0.005333, mae: 0.083818, mean_q: 18.309723
 97774/100000: episode: 2878, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.412, mean reward: 0.412 [0.412, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.299, 10.100], loss: 0.010204, mae: 0.115796, mean_q: 20.560970
 97775/100000: episode: 2879, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.359, 10.100], loss: 0.010039, mae: 0.096508, mean_q: 18.073898
 97776/100000: episode: 2880, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.526, mean reward: 0.526 [0.526, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.250, 10.100], loss: 0.010357, mae: 0.110728, mean_q: 15.406242
 97777/100000: episode: 2881, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.428, mean reward: 0.428 [0.428, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.332, 10.100], loss: 0.007817, mae: 0.098811, mean_q: 17.017982
 97778/100000: episode: 2882, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.332, mean reward: 0.332 [0.332, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.359, 10.100], loss: 0.006853, mae: 0.089022, mean_q: 18.983215
 97779/100000: episode: 2883, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.286, mean reward: 0.286 [0.286, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.597, 10.100], loss: 0.013279, mae: 0.130894, mean_q: 20.123779
 97780/100000: episode: 2884, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.533, mean reward: 0.533 [0.533, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.326, 10.100], loss: 0.010701, mae: 0.113880, mean_q: 20.208059
 97781/100000: episode: 2885, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.350, mean reward: 0.350 [0.350, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.306, 10.100], loss: 0.009489, mae: 0.113212, mean_q: 16.557617
 97782/100000: episode: 2886, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.489, mean reward: 0.489 [0.489, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.279, 10.100], loss: 0.007932, mae: 0.099591, mean_q: 20.320322
 97783/100000: episode: 2887, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.442, mean reward: 0.442 [0.442, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.309, 10.100], loss: 0.010995, mae: 0.109962, mean_q: 19.883972
 97784/100000: episode: 2888, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.457, mean reward: 0.457 [0.457, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.345, 10.100], loss: 0.008594, mae: 0.103046, mean_q: 18.562737
 97785/100000: episode: 2889, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.303, 10.100], loss: 0.008711, mae: 0.092607, mean_q: 20.479239
 97786/100000: episode: 2890, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.454, mean reward: 0.454 [0.454, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.343, 10.100], loss: 0.006690, mae: 0.092565, mean_q: 19.058849
 97787/100000: episode: 2891, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.351, mean reward: 0.351 [0.351, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.335, 10.100], loss: 0.006752, mae: 0.095744, mean_q: 18.890846
 97788/100000: episode: 2892, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.479, mean reward: 0.479 [0.479, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.287, 10.100], loss: 0.011955, mae: 0.116356, mean_q: 21.970367
 97789/100000: episode: 2893, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.517, mean reward: 0.517 [0.517, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.272, 10.100], loss: 0.006874, mae: 0.096579, mean_q: 20.779797
 97790/100000: episode: 2894, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.503, mean reward: 0.503 [0.503, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.288, 10.100], loss: 0.012156, mae: 0.129070, mean_q: 16.831745
 97791/100000: episode: 2895, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.522, mean reward: 0.522 [0.522, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.313, 10.100], loss: 0.007360, mae: 0.110438, mean_q: 19.248005
 97792/100000: episode: 2896, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.367, 10.100], loss: 0.007125, mae: 0.101334, mean_q: 19.062527
 97793/100000: episode: 2897, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.495, mean reward: 0.495 [0.495, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.264, 10.100], loss: 0.009939, mae: 0.105919, mean_q: 17.626492
 97794/100000: episode: 2898, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.460, mean reward: 0.460 [0.460, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.303, 10.100], loss: 0.010043, mae: 0.122566, mean_q: 15.054532
 97795/100000: episode: 2899, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.458, mean reward: 0.458 [0.458, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.371, 10.100], loss: 0.015468, mae: 0.139349, mean_q: 23.216682
 97796/100000: episode: 2900, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.321, mean reward: 0.321 [0.321, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.326, 10.100], loss: 0.015039, mae: 0.121107, mean_q: 20.356747
 97797/100000: episode: 2901, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.328, 10.100], loss: 0.009306, mae: 0.110174, mean_q: 16.527525
 97798/100000: episode: 2902, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.424, mean reward: 0.424 [0.424, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.376, 10.100], loss: 0.019316, mae: 0.159772, mean_q: 18.798069
 97799/100000: episode: 2903, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.491, mean reward: 0.491 [0.491, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.595, 10.100], loss: 0.022353, mae: 0.172593, mean_q: 17.999523
 97800/100000: episode: 2904, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.531, mean reward: 0.531 [0.531, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.310, 10.100], loss: 0.015407, mae: 0.116749, mean_q: 20.489567
 97801/100000: episode: 2905, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.425, mean reward: 0.425 [0.425, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.650, 10.100], loss: 0.040176, mae: 0.248711, mean_q: 17.607506
 97802/100000: episode: 2906, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.363, mean reward: 0.363 [0.363, 0.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.320, 10.100], loss: 0.026182, mae: 0.199865, mean_q: 15.891285
 97803/100000: episode: 2907, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.331, mean reward: 0.331 [0.331, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.837, 10.100], loss: 0.014322, mae: 0.131415, mean_q: 15.215368
 97804/100000: episode: 2908, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.556, mean reward: 0.556 [0.556, 0.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.262, 10.100], loss: 0.023534, mae: 0.174651, mean_q: 19.063004
 97805/100000: episode: 2909, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.307, 10.100], loss: 0.035860, mae: 0.225615, mean_q: 18.666058
 97806/100000: episode: 2910, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.424, mean reward: 0.424 [0.424, 0.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.308, 10.100], loss: 0.025399, mae: 0.173062, mean_q: 14.557313
 97807/100000: episode: 2911, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.303, 10.100], loss: 0.010225, mae: 0.117823, mean_q: 23.659184
 97808/100000: episode: 2912, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.504, mean reward: 0.504 [0.504, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.313, 10.100], loss: 0.010034, mae: 0.102762, mean_q: 18.264437
 97809/100000: episode: 2913, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.409, mean reward: 0.409 [0.409, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.325, 10.100], loss: 0.015718, mae: 0.141979, mean_q: 15.777109
 97810/100000: episode: 2914, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.467, mean reward: 0.467 [0.467, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.337, 10.100], loss: 0.019238, mae: 0.139717, mean_q: 18.168484
 97811/100000: episode: 2915, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.354, mean reward: 0.354 [0.354, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.301, 10.100], loss: 0.007617, mae: 0.099687, mean_q: 16.329693
 97812/100000: episode: 2916, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.397, mean reward: 0.397 [0.397, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.347, 10.100], loss: 0.008903, mae: 0.099141, mean_q: 17.334599
 97813/100000: episode: 2917, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.417, mean reward: 0.417 [0.417, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.336, 10.100], loss: 0.007041, mae: 0.101580, mean_q: 16.994591
 97814/100000: episode: 2918, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.382, mean reward: 0.382 [0.382, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.332, 10.100], loss: 0.011510, mae: 0.109047, mean_q: 21.333385
 97815/100000: episode: 2919, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.450, mean reward: 0.450 [0.450, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.338, 10.100], loss: 0.008800, mae: 0.107847, mean_q: 17.507330
 97816/100000: episode: 2920, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.368, 10.100], loss: 0.013489, mae: 0.134430, mean_q: 18.437506
 97817/100000: episode: 2921, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.459, mean reward: 0.459 [0.459, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.307, 10.100], loss: 0.009047, mae: 0.110703, mean_q: 21.488152
 97818/100000: episode: 2922, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.492, mean reward: 0.492 [0.492, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.262, 10.100], loss: 0.005908, mae: 0.087958, mean_q: 19.174246
 97819/100000: episode: 2923, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.422, mean reward: 0.422 [0.422, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.295, 10.100], loss: 0.012993, mae: 0.114349, mean_q: 20.175043
 97820/100000: episode: 2924, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.511, mean reward: 0.511 [0.511, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.314, 10.100], loss: 0.020380, mae: 0.139598, mean_q: 17.661926
 97821/100000: episode: 2925, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 0.449, mean reward: 0.449 [0.449, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.308, 10.100], loss: 0.005503, mae: 0.084238, mean_q: 18.538088
 97822/100000: episode: 2926, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.493, mean reward: 0.493 [0.493, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.347, 10.100], loss: 0.006778, mae: 0.099684, mean_q: 19.844168
 97823/100000: episode: 2927, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.333, mean reward: 0.333 [0.333, 0.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.349, 10.100], loss: 0.006766, mae: 0.093707, mean_q: 19.791073
 97824/100000: episode: 2928, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.279, 10.100], loss: 0.005861, mae: 0.089192, mean_q: 19.568119
 97825/100000: episode: 2929, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.506, mean reward: 0.506 [0.506, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.301, 10.100], loss: 0.012654, mae: 0.136729, mean_q: 17.097691
[Info] Not found new level, current best level reached = 0.03414130210876465
 97826/100000: episode: 2930, duration: 4.024s, episode steps: 1, steps per second: 0, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.292, 10.100], loss: 0.011264, mae: 0.121165, mean_q: 18.737114
 97926/100000: episode: 2931, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 54.755, mean reward: 0.548 [0.311, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.470, 10.115], loss: 0.014819, mae: 0.129871, mean_q: 18.957783
 98026/100000: episode: 2932, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 56.461, mean reward: 0.565 [0.375, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.719, 10.302], loss: 0.013824, mae: 0.126577, mean_q: 19.028950
 98126/100000: episode: 2933, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.133, mean reward: 0.571 [0.378, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.157, 10.098], loss: 0.012691, mae: 0.118681, mean_q: 18.831854
 98226/100000: episode: 2934, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 53.894, mean reward: 0.539 [0.284, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.968, 10.292], loss: 0.014571, mae: 0.127593, mean_q: 19.120228
 98326/100000: episode: 2935, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 51.116, mean reward: 0.511 [0.243, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.021, 10.098], loss: 0.014752, mae: 0.129096, mean_q: 19.109438
 98426/100000: episode: 2936, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 53.012, mean reward: 0.530 [0.318, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.953, 10.238], loss: 0.014415, mae: 0.126237, mean_q: 18.575024
 98526/100000: episode: 2937, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 53.244, mean reward: 0.532 [0.343, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.207, 10.271], loss: 0.015013, mae: 0.129105, mean_q: 18.897768
 98626/100000: episode: 2938, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 52.757, mean reward: 0.528 [0.347, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.264, 10.098], loss: 0.016114, mae: 0.132602, mean_q: 18.731350
 98726/100000: episode: 2939, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 53.389, mean reward: 0.534 [0.289, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.092, 10.098], loss: 0.016231, mae: 0.134575, mean_q: 18.370373
 98826/100000: episode: 2940, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 50.868, mean reward: 0.509 [0.310, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.365, 10.098], loss: 0.016001, mae: 0.131536, mean_q: 18.590042
 98926/100000: episode: 2941, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 52.107, mean reward: 0.521 [0.288, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.592, 10.106], loss: 0.016950, mae: 0.136188, mean_q: 18.644012
 99026/100000: episode: 2942, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 54.025, mean reward: 0.540 [0.354, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.161, 10.223], loss: 0.017445, mae: 0.139975, mean_q: 18.900070
 99126/100000: episode: 2943, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.386, mean reward: 0.584 [0.348, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.048, 10.161], loss: 0.016819, mae: 0.134869, mean_q: 18.561768
 99226/100000: episode: 2944, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 53.204, mean reward: 0.532 [0.172, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.260, 10.153], loss: 0.017332, mae: 0.135319, mean_q: 18.597548
 99326/100000: episode: 2945, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 55.704, mean reward: 0.557 [0.319, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.439, 10.155], loss: 0.017157, mae: 0.140147, mean_q: 18.893225
 99426/100000: episode: 2946, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 48.918, mean reward: 0.489 [0.246, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.031, 10.098], loss: 0.017732, mae: 0.141561, mean_q: 18.554167
 99526/100000: episode: 2947, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 47.986, mean reward: 0.480 [0.283, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.720, 10.098], loss: 0.016493, mae: 0.135140, mean_q: 18.608767
 99626/100000: episode: 2948, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 53.573, mean reward: 0.536 [0.329, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.482, 10.211], loss: 0.018228, mae: 0.141437, mean_q: 18.592257
 99726/100000: episode: 2949, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 52.406, mean reward: 0.524 [0.349, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.631, 10.387], loss: 0.017872, mae: 0.137250, mean_q: 18.373947
 99826/100000: episode: 2950, duration: 0.471s, episode steps: 100, steps per second: 212, episode reward: 48.857, mean reward: 0.489 [0.224, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.712, 10.450], loss: 0.018900, mae: 0.137725, mean_q: 18.476076
 99926/100000: episode: 2951, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 51.391, mean reward: 0.514 [0.254, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.861, 10.196], loss: 0.015596, mae: 0.127649, mean_q: 18.850254
done, took 643.513 seconds
[Info] End Importance Splitting.
