Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.162s, episode steps: 100, steps per second: 616, episode reward: 56.489, mean reward: 0.565 [0.401, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.856, 10.127], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.075s, episode steps: 100, steps per second: 1329, episode reward: 55.793, mean reward: 0.558 [0.336, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.785, 10.176], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.090s, episode steps: 100, steps per second: 1113, episode reward: 50.964, mean reward: 0.510 [0.331, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.832, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.063s, episode steps: 100, steps per second: 1587, episode reward: 46.330, mean reward: 0.463 [0.187, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.006, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.071s, episode steps: 100, steps per second: 1403, episode reward: 51.723, mean reward: 0.517 [0.310, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.237, 10.351], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.252s, episode steps: 100, steps per second: 80, episode reward: 51.043, mean reward: 0.510 [0.300, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.623, 10.172], loss: 0.038378, mae: 0.156456, mean_q: 1.457441
   700/100000: episode: 7, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 50.625, mean reward: 0.506 [0.234, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.543, 10.373], loss: 0.026170, mae: 0.125411, mean_q: 2.001359
   800/100000: episode: 8, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 54.558, mean reward: 0.546 [0.322, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.825, 10.266], loss: 0.026664, mae: 0.121816, mean_q: 2.464546
   900/100000: episode: 9, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 51.472, mean reward: 0.515 [0.250, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.554, 10.098], loss: 0.025526, mae: 0.118926, mean_q: 2.868318
  1000/100000: episode: 10, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 47.375, mean reward: 0.474 [0.281, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.155, 10.098], loss: 0.035112, mae: 0.131080, mean_q: 3.350158
  1100/100000: episode: 11, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 53.350, mean reward: 0.534 [0.148, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.177, 10.098], loss: 0.021757, mae: 0.110731, mean_q: 3.786234
  1200/100000: episode: 12, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 51.293, mean reward: 0.513 [0.291, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-2.478, 10.352], loss: 0.027475, mae: 0.133601, mean_q: 4.187002
  1300/100000: episode: 13, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 53.880, mean reward: 0.539 [0.256, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.905, 10.098], loss: 0.028496, mae: 0.122257, mean_q: 4.599453
  1400/100000: episode: 14, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 51.474, mean reward: 0.515 [0.315, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.550, 10.269], loss: 0.019556, mae: 0.111353, mean_q: 5.033174
  1500/100000: episode: 15, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 51.823, mean reward: 0.518 [0.235, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.756, 10.331], loss: 0.027595, mae: 0.132992, mean_q: 5.424136
  1600/100000: episode: 16, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 52.691, mean reward: 0.527 [0.321, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.392, 10.267], loss: 0.016935, mae: 0.110648, mean_q: 5.887944
  1700/100000: episode: 17, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 54.954, mean reward: 0.550 [0.264, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.473, 10.119], loss: 0.019165, mae: 0.121819, mean_q: 6.303473
  1800/100000: episode: 18, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 50.232, mean reward: 0.502 [0.294, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.825, 10.411], loss: 0.017758, mae: 0.115394, mean_q: 6.712348
  1900/100000: episode: 19, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 50.571, mean reward: 0.506 [0.304, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.059, 10.400], loss: 0.016571, mae: 0.112967, mean_q: 7.058608
  2000/100000: episode: 20, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 55.452, mean reward: 0.555 [0.301, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.623, 10.364], loss: 0.015225, mae: 0.113475, mean_q: 7.503190
  2100/100000: episode: 21, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 52.379, mean reward: 0.524 [0.331, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.406, 10.098], loss: 0.016159, mae: 0.120493, mean_q: 7.825106
  2200/100000: episode: 22, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 54.178, mean reward: 0.542 [0.269, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.611, 10.098], loss: 0.011117, mae: 0.104997, mean_q: 8.164195
  2300/100000: episode: 23, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 55.644, mean reward: 0.556 [0.224, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.212, 10.135], loss: 0.011458, mae: 0.110464, mean_q: 8.475569
  2400/100000: episode: 24, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 53.126, mean reward: 0.531 [0.283, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.134, 10.098], loss: 0.012295, mae: 0.111586, mean_q: 8.865868
  2500/100000: episode: 25, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 53.249, mean reward: 0.532 [0.211, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.237, 10.195], loss: 0.011193, mae: 0.108717, mean_q: 9.279013
  2600/100000: episode: 26, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 55.265, mean reward: 0.553 [0.348, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.671, 10.098], loss: 0.010705, mae: 0.109248, mean_q: 9.572147
  2700/100000: episode: 27, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 53.423, mean reward: 0.534 [0.298, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.878, 10.132], loss: 0.011566, mae: 0.113560, mean_q: 9.914852
  2800/100000: episode: 28, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 43.328, mean reward: 0.433 [0.064, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.921, 10.098], loss: 0.011074, mae: 0.110629, mean_q: 10.161339
  2900/100000: episode: 29, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 54.009, mean reward: 0.540 [0.309, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.199, 10.138], loss: 0.010840, mae: 0.107639, mean_q: 10.475180
  3000/100000: episode: 30, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 53.812, mean reward: 0.538 [0.272, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.197, 10.165], loss: 0.011150, mae: 0.110353, mean_q: 10.865089
  3100/100000: episode: 31, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 54.244, mean reward: 0.542 [0.236, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.911, 10.140], loss: 0.013352, mae: 0.119601, mean_q: 11.009905
  3200/100000: episode: 32, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 47.208, mean reward: 0.472 [0.277, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.626, 10.168], loss: 0.012722, mae: 0.115214, mean_q: 11.372808
  3300/100000: episode: 33, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 52.262, mean reward: 0.523 [0.305, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.359, 10.098], loss: 0.012982, mae: 0.118522, mean_q: 11.565451
  3400/100000: episode: 34, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 55.575, mean reward: 0.556 [0.297, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.444, 10.098], loss: 0.014026, mae: 0.117250, mean_q: 11.989130
  3500/100000: episode: 35, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 54.560, mean reward: 0.546 [0.347, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.767, 10.098], loss: 0.013034, mae: 0.118580, mean_q: 12.228473
  3600/100000: episode: 36, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 54.995, mean reward: 0.550 [0.341, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.867, 10.290], loss: 0.019365, mae: 0.144485, mean_q: 12.468110
  3700/100000: episode: 37, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.137, mean reward: 0.571 [0.413, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.315, 10.098], loss: 0.012546, mae: 0.118729, mean_q: 12.788505
  3800/100000: episode: 38, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 47.985, mean reward: 0.480 [0.175, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.759, 10.098], loss: 0.015446, mae: 0.131462, mean_q: 12.938343
  3900/100000: episode: 39, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 54.489, mean reward: 0.545 [0.325, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.554, 10.331], loss: 0.015235, mae: 0.129551, mean_q: 13.183867
  4000/100000: episode: 40, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 54.832, mean reward: 0.548 [0.303, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.452, 10.098], loss: 0.015574, mae: 0.130878, mean_q: 13.421191
  4100/100000: episode: 41, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 53.791, mean reward: 0.538 [0.357, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.036, 10.128], loss: 0.014458, mae: 0.123768, mean_q: 13.742779
  4200/100000: episode: 42, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.055, mean reward: 0.571 [0.272, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.834, 10.182], loss: 0.013551, mae: 0.124545, mean_q: 13.943229
  4300/100000: episode: 43, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 53.660, mean reward: 0.537 [0.298, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.177], loss: 0.014907, mae: 0.129587, mean_q: 14.109035
  4400/100000: episode: 44, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 54.833, mean reward: 0.548 [0.256, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.417, 10.233], loss: 0.015301, mae: 0.133015, mean_q: 14.518128
  4500/100000: episode: 45, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.407, mean reward: 0.534 [0.338, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.563, 10.098], loss: 0.014689, mae: 0.127071, mean_q: 14.561082
  4600/100000: episode: 46, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 51.474, mean reward: 0.515 [0.288, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.120, 10.098], loss: 0.019333, mae: 0.147955, mean_q: 14.933216
  4700/100000: episode: 47, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 52.432, mean reward: 0.524 [0.173, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.280, 10.128], loss: 0.015374, mae: 0.132175, mean_q: 14.950588
  4800/100000: episode: 48, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 56.711, mean reward: 0.567 [0.369, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.710, 10.098], loss: 0.016769, mae: 0.140230, mean_q: 15.137603
  4900/100000: episode: 49, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 48.141, mean reward: 0.481 [0.274, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.740, 10.098], loss: 0.016136, mae: 0.136004, mean_q: 15.341394
  5000/100000: episode: 50, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 56.296, mean reward: 0.563 [0.373, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.112, 10.178], loss: 0.013266, mae: 0.124531, mean_q: 15.476773
  5100/100000: episode: 51, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 53.199, mean reward: 0.532 [0.223, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.961, 10.098], loss: 0.017686, mae: 0.142903, mean_q: 15.641109
  5200/100000: episode: 52, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 53.073, mean reward: 0.531 [0.328, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.849, 10.098], loss: 0.014873, mae: 0.130800, mean_q: 15.737735
  5300/100000: episode: 53, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 56.134, mean reward: 0.561 [0.296, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.736, 10.217], loss: 0.014365, mae: 0.127118, mean_q: 16.092682
  5400/100000: episode: 54, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 53.285, mean reward: 0.533 [0.297, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.423, 10.098], loss: 0.015334, mae: 0.132978, mean_q: 16.326933
  5500/100000: episode: 55, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 47.666, mean reward: 0.477 [0.288, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.733, 10.098], loss: 0.016269, mae: 0.137319, mean_q: 16.221172
  5600/100000: episode: 56, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 54.634, mean reward: 0.546 [0.366, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.902, 10.098], loss: 0.016225, mae: 0.137030, mean_q: 16.352489
  5700/100000: episode: 57, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 49.846, mean reward: 0.498 [0.248, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.498, 10.221], loss: 0.017679, mae: 0.146921, mean_q: 16.470676
  5800/100000: episode: 58, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 54.533, mean reward: 0.545 [0.309, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.120, 10.098], loss: 0.014604, mae: 0.131983, mean_q: 16.915268
  5900/100000: episode: 59, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 55.971, mean reward: 0.560 [0.331, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.834, 10.124], loss: 0.015667, mae: 0.136712, mean_q: 17.052210
  6000/100000: episode: 60, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 48.895, mean reward: 0.489 [0.272, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.113, 10.098], loss: 0.013830, mae: 0.127689, mean_q: 17.223106
  6100/100000: episode: 61, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 57.793, mean reward: 0.578 [0.329, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.784, 10.186], loss: 0.015782, mae: 0.138599, mean_q: 17.221827
  6200/100000: episode: 62, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 47.447, mean reward: 0.474 [0.239, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.849, 10.098], loss: 0.016458, mae: 0.138654, mean_q: 17.315298
  6300/100000: episode: 63, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 54.846, mean reward: 0.548 [0.296, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.509, 10.098], loss: 0.017214, mae: 0.144469, mean_q: 17.373276
  6400/100000: episode: 64, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 52.050, mean reward: 0.520 [0.261, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.328, 10.098], loss: 0.015561, mae: 0.135644, mean_q: 17.573816
  6500/100000: episode: 65, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 55.001, mean reward: 0.550 [0.340, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.944, 10.157], loss: 0.014781, mae: 0.133493, mean_q: 17.487877
  6600/100000: episode: 66, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 50.870, mean reward: 0.509 [0.319, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.146, 10.378], loss: 0.017317, mae: 0.143996, mean_q: 17.741871
  6700/100000: episode: 67, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 53.672, mean reward: 0.537 [0.295, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.449, 10.331], loss: 0.016606, mae: 0.141675, mean_q: 17.757786
  6800/100000: episode: 68, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 56.464, mean reward: 0.565 [0.384, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.366, 10.098], loss: 0.015500, mae: 0.136959, mean_q: 18.129337
  6900/100000: episode: 69, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 53.650, mean reward: 0.536 [0.282, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.680, 10.178], loss: 0.018292, mae: 0.148462, mean_q: 17.795488
  7000/100000: episode: 70, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.798, mean reward: 0.578 [0.325, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.186, 10.130], loss: 0.017017, mae: 0.142107, mean_q: 18.149632
  7100/100000: episode: 71, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 53.222, mean reward: 0.532 [0.230, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.495, 10.098], loss: 0.017988, mae: 0.147573, mean_q: 18.287500
  7200/100000: episode: 72, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 53.775, mean reward: 0.538 [0.358, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.478, 10.098], loss: 0.016641, mae: 0.140655, mean_q: 18.426064
  7300/100000: episode: 73, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 52.593, mean reward: 0.526 [0.335, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.087, 10.393], loss: 0.017955, mae: 0.145123, mean_q: 18.658913
  7400/100000: episode: 74, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 50.480, mean reward: 0.505 [0.296, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.911, 10.098], loss: 0.017320, mae: 0.141553, mean_q: 18.465971
  7500/100000: episode: 75, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 54.111, mean reward: 0.541 [0.199, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.262, 10.098], loss: 0.016843, mae: 0.139644, mean_q: 18.366592
  7600/100000: episode: 76, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 50.908, mean reward: 0.509 [0.238, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.932, 10.098], loss: 0.016918, mae: 0.141919, mean_q: 18.667885
  7700/100000: episode: 77, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 56.877, mean reward: 0.569 [0.386, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.182], loss: 0.016550, mae: 0.141240, mean_q: 18.783903
  7800/100000: episode: 78, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 50.972, mean reward: 0.510 [0.245, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.350, 10.118], loss: 0.016829, mae: 0.142721, mean_q: 18.663546
  7900/100000: episode: 79, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 55.901, mean reward: 0.559 [0.320, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.315, 10.098], loss: 0.018858, mae: 0.151674, mean_q: 18.846991
  8000/100000: episode: 80, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 54.427, mean reward: 0.544 [0.298, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.269, 10.323], loss: 0.016014, mae: 0.140111, mean_q: 18.776304
  8100/100000: episode: 81, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 53.421, mean reward: 0.534 [0.284, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.468, 10.255], loss: 0.016390, mae: 0.141368, mean_q: 18.893881
  8200/100000: episode: 82, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 56.644, mean reward: 0.566 [0.372, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.391, 10.405], loss: 0.016338, mae: 0.141177, mean_q: 19.141640
  8300/100000: episode: 83, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 47.460, mean reward: 0.475 [0.230, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.802, 10.098], loss: 0.017277, mae: 0.147722, mean_q: 18.760418
  8400/100000: episode: 84, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 48.707, mean reward: 0.487 [0.211, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.420, 10.108], loss: 0.015506, mae: 0.137602, mean_q: 19.208399
  8500/100000: episode: 85, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 48.346, mean reward: 0.483 [0.245, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.193, 10.098], loss: 0.017711, mae: 0.147602, mean_q: 18.859922
  8600/100000: episode: 86, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 52.834, mean reward: 0.528 [0.290, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.624, 10.162], loss: 0.016465, mae: 0.142602, mean_q: 19.325960
  8700/100000: episode: 87, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 52.918, mean reward: 0.529 [0.248, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.015, 10.117], loss: 0.016602, mae: 0.142736, mean_q: 18.887680
  8800/100000: episode: 88, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 54.830, mean reward: 0.548 [0.216, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.269, 10.288], loss: 0.015727, mae: 0.140630, mean_q: 19.151423
  8900/100000: episode: 89, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 47.586, mean reward: 0.476 [0.196, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.884, 10.377], loss: 0.016858, mae: 0.143460, mean_q: 19.397884
  9000/100000: episode: 90, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 46.688, mean reward: 0.467 [0.266, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.576, 10.098], loss: 0.018715, mae: 0.153119, mean_q: 19.144855
  9100/100000: episode: 91, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 51.689, mean reward: 0.517 [0.178, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.477, 10.098], loss: 0.016806, mae: 0.142931, mean_q: 19.586126
  9200/100000: episode: 92, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 54.387, mean reward: 0.544 [0.300, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.940, 10.187], loss: 0.019238, mae: 0.152753, mean_q: 19.312054
  9300/100000: episode: 93, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 56.552, mean reward: 0.566 [0.260, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.945, 10.098], loss: 0.015991, mae: 0.138646, mean_q: 19.141876
  9400/100000: episode: 94, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 50.358, mean reward: 0.504 [0.282, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.861, 10.345], loss: 0.017593, mae: 0.146355, mean_q: 19.621286
  9500/100000: episode: 95, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 53.023, mean reward: 0.530 [0.218, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.207, 10.098], loss: 0.017503, mae: 0.147428, mean_q: 19.325682
  9600/100000: episode: 96, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 54.417, mean reward: 0.544 [0.341, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.998, 10.170], loss: 0.016831, mae: 0.144585, mean_q: 19.434481
  9700/100000: episode: 97, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.875, mean reward: 0.539 [0.335, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.312, 10.098], loss: 0.016802, mae: 0.141416, mean_q: 19.385796
  9800/100000: episode: 98, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 52.545, mean reward: 0.525 [0.335, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.690, 10.098], loss: 0.016552, mae: 0.143730, mean_q: 19.389704
  9900/100000: episode: 99, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 52.576, mean reward: 0.526 [0.311, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.614, 10.215], loss: 0.015773, mae: 0.137560, mean_q: 19.604721
 10000/100000: episode: 100, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 54.213, mean reward: 0.542 [0.313, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.416, 10.150], loss: 0.017643, mae: 0.147940, mean_q: 19.529955
 10100/100000: episode: 101, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 56.786, mean reward: 0.568 [0.377, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.155, 10.199], loss: 0.018941, mae: 0.151023, mean_q: 19.554022
 10200/100000: episode: 102, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 54.685, mean reward: 0.547 [0.310, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.703, 10.179], loss: 0.017100, mae: 0.143132, mean_q: 19.562069
 10300/100000: episode: 103, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 51.220, mean reward: 0.512 [0.273, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.358, 10.451], loss: 0.018265, mae: 0.149358, mean_q: 19.509920
 10400/100000: episode: 104, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 52.768, mean reward: 0.528 [0.333, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.940, 10.098], loss: 0.018577, mae: 0.150566, mean_q: 19.394585
 10500/100000: episode: 105, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 54.955, mean reward: 0.550 [0.321, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.886, 10.184], loss: 0.016299, mae: 0.142092, mean_q: 19.359589
 10600/100000: episode: 106, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 52.587, mean reward: 0.526 [0.315, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.662, 10.098], loss: 0.016639, mae: 0.142099, mean_q: 19.433668
 10700/100000: episode: 107, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 52.770, mean reward: 0.528 [0.291, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.850, 10.101], loss: 0.015881, mae: 0.137758, mean_q: 19.603529
 10800/100000: episode: 108, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 56.471, mean reward: 0.565 [0.397, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.454, 10.136], loss: 0.018220, mae: 0.148434, mean_q: 19.488239
 10900/100000: episode: 109, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 56.788, mean reward: 0.568 [0.350, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.015, 10.098], loss: 0.018752, mae: 0.149166, mean_q: 19.562759
 11000/100000: episode: 110, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 50.796, mean reward: 0.508 [0.278, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.708, 10.098], loss: 0.018831, mae: 0.148101, mean_q: 19.612394
 11100/100000: episode: 111, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 53.938, mean reward: 0.539 [0.327, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.354, 10.098], loss: 0.020021, mae: 0.155146, mean_q: 19.564199
 11200/100000: episode: 112, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 51.564, mean reward: 0.516 [0.112, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.003, 10.098], loss: 0.016685, mae: 0.140836, mean_q: 19.701982
 11300/100000: episode: 113, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 47.886, mean reward: 0.479 [0.286, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.860, 10.098], loss: 0.016861, mae: 0.142233, mean_q: 19.518787
 11400/100000: episode: 114, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 55.296, mean reward: 0.553 [0.332, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.506, 10.215], loss: 0.016739, mae: 0.140426, mean_q: 19.491901
 11500/100000: episode: 115, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 52.690, mean reward: 0.527 [0.331, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.777, 10.098], loss: 0.018474, mae: 0.149716, mean_q: 19.488049
 11600/100000: episode: 116, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 57.232, mean reward: 0.572 [0.338, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.374, 10.261], loss: 0.018793, mae: 0.148534, mean_q: 19.450930
 11700/100000: episode: 117, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 53.707, mean reward: 0.537 [0.261, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.870, 10.098], loss: 0.016582, mae: 0.140833, mean_q: 19.529728
 11800/100000: episode: 118, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 54.501, mean reward: 0.545 [0.192, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.152, 10.098], loss: 0.016729, mae: 0.140589, mean_q: 19.404137
 11900/100000: episode: 119, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 51.178, mean reward: 0.512 [0.252, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.261, 10.098], loss: 0.020159, mae: 0.153645, mean_q: 19.737089
 12000/100000: episode: 120, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 54.729, mean reward: 0.547 [0.165, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.777, 10.377], loss: 0.019047, mae: 0.153634, mean_q: 19.548359
 12100/100000: episode: 121, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 54.155, mean reward: 0.542 [0.351, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.464, 10.225], loss: 0.017041, mae: 0.141624, mean_q: 19.640537
 12200/100000: episode: 122, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 50.232, mean reward: 0.502 [0.281, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.904, 10.336], loss: 0.018081, mae: 0.146949, mean_q: 19.526642
 12300/100000: episode: 123, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 50.916, mean reward: 0.509 [0.232, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.417, 10.098], loss: 0.018812, mae: 0.149631, mean_q: 19.408422
 12400/100000: episode: 124, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 51.935, mean reward: 0.519 [0.248, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.441, 10.244], loss: 0.017846, mae: 0.145613, mean_q: 19.408361
 12500/100000: episode: 125, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 54.033, mean reward: 0.540 [0.373, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.566, 10.361], loss: 0.019487, mae: 0.151275, mean_q: 19.509293
 12600/100000: episode: 126, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 53.120, mean reward: 0.531 [0.211, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.177, 10.098], loss: 0.017304, mae: 0.142229, mean_q: 19.707924
 12700/100000: episode: 127, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 54.155, mean reward: 0.542 [0.314, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.669, 10.247], loss: 0.017459, mae: 0.145367, mean_q: 19.554480
 12800/100000: episode: 128, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 51.876, mean reward: 0.519 [0.239, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.547, 10.282], loss: 0.016651, mae: 0.142532, mean_q: 19.654690
 12900/100000: episode: 129, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 54.743, mean reward: 0.547 [0.347, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.322, 10.212], loss: 0.015038, mae: 0.135700, mean_q: 19.594572
 13000/100000: episode: 130, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 54.338, mean reward: 0.543 [0.322, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.377, 10.213], loss: 0.015576, mae: 0.139474, mean_q: 19.567472
 13100/100000: episode: 131, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 53.979, mean reward: 0.540 [0.347, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.333, 10.307], loss: 0.015958, mae: 0.138715, mean_q: 19.581411
 13200/100000: episode: 132, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 55.806, mean reward: 0.558 [0.338, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.844, 10.257], loss: 0.015303, mae: 0.137298, mean_q: 19.556410
 13300/100000: episode: 133, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.272, mean reward: 0.573 [0.322, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.907, 10.098], loss: 0.014992, mae: 0.136937, mean_q: 20.081785
 13400/100000: episode: 134, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 56.537, mean reward: 0.565 [0.304, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.385, 10.098], loss: 0.015125, mae: 0.137639, mean_q: 19.717699
 13500/100000: episode: 135, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 57.624, mean reward: 0.576 [0.346, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.608, 10.119], loss: 0.017058, mae: 0.145101, mean_q: 19.451313
 13600/100000: episode: 136, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 53.372, mean reward: 0.534 [0.322, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.812, 10.098], loss: 0.014974, mae: 0.135288, mean_q: 19.261715
 13700/100000: episode: 137, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 52.117, mean reward: 0.521 [0.280, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.768, 10.305], loss: 0.015125, mae: 0.137244, mean_q: 19.698355
 13800/100000: episode: 138, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 50.892, mean reward: 0.509 [0.247, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.243, 10.489], loss: 0.015320, mae: 0.136638, mean_q: 19.848753
 13900/100000: episode: 139, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 53.702, mean reward: 0.537 [0.335, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.943, 10.178], loss: 0.015548, mae: 0.140132, mean_q: 19.465443
 14000/100000: episode: 140, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 53.936, mean reward: 0.539 [0.299, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.618, 10.275], loss: 0.013640, mae: 0.128818, mean_q: 19.808800
 14100/100000: episode: 141, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 52.563, mean reward: 0.526 [0.283, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.996, 10.274], loss: 0.015284, mae: 0.138143, mean_q: 19.942305
 14200/100000: episode: 142, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 51.540, mean reward: 0.515 [0.253, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.852, 10.295], loss: 0.015626, mae: 0.139802, mean_q: 19.378260
 14300/100000: episode: 143, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 54.443, mean reward: 0.544 [0.245, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.937, 10.104], loss: 0.015540, mae: 0.137962, mean_q: 19.704424
 14400/100000: episode: 144, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 53.394, mean reward: 0.534 [0.248, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.857, 10.118], loss: 0.014739, mae: 0.135153, mean_q: 19.769461
 14500/100000: episode: 145, duration: 1.235s, episode steps: 100, steps per second: 81, episode reward: 49.090, mean reward: 0.491 [0.337, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.034, 10.098], loss: 0.015280, mae: 0.138003, mean_q: 19.634771
 14600/100000: episode: 146, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 49.360, mean reward: 0.494 [0.132, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.618, 10.098], loss: 0.013362, mae: 0.128541, mean_q: 19.807959
 14700/100000: episode: 147, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 50.008, mean reward: 0.500 [0.262, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.021, 10.098], loss: 0.015205, mae: 0.137399, mean_q: 19.770746
 14800/100000: episode: 148, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 56.183, mean reward: 0.562 [0.365, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.305, 10.098], loss: 0.014790, mae: 0.135169, mean_q: 19.575369
 14900/100000: episode: 149, duration: 0.922s, episode steps: 100, steps per second: 108, episode reward: 53.902, mean reward: 0.539 [0.299, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.931, 10.352], loss: 0.015642, mae: 0.138637, mean_q: 19.688387
 15000/100000: episode: 150, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: 52.750, mean reward: 0.527 [0.323, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.177, 10.128], loss: 0.015070, mae: 0.134019, mean_q: 19.613714
 15100/100000: episode: 151, duration: 0.736s, episode steps: 100, steps per second: 136, episode reward: 49.677, mean reward: 0.497 [0.307, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.596, 10.364], loss: 0.012936, mae: 0.124961, mean_q: 19.793024
 15200/100000: episode: 152, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 52.267, mean reward: 0.523 [0.249, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.591, 10.098], loss: 0.013338, mae: 0.127822, mean_q: 19.737780
 15300/100000: episode: 153, duration: 0.889s, episode steps: 100, steps per second: 113, episode reward: 51.869, mean reward: 0.519 [0.356, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.195, 10.098], loss: 0.013415, mae: 0.127958, mean_q: 19.658506
 15400/100000: episode: 154, duration: 1.066s, episode steps: 100, steps per second: 94, episode reward: 48.882, mean reward: 0.489 [0.273, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.756, 10.233], loss: 0.014091, mae: 0.131103, mean_q: 19.826401
 15500/100000: episode: 155, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: 54.282, mean reward: 0.543 [0.232, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.037, 10.203], loss: 0.016455, mae: 0.141051, mean_q: 19.913837
 15600/100000: episode: 156, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 54.131, mean reward: 0.541 [0.265, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.296, 10.098], loss: 0.016508, mae: 0.142270, mean_q: 19.714191
 15700/100000: episode: 157, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: 54.956, mean reward: 0.550 [0.221, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.218, 10.098], loss: 0.013301, mae: 0.127913, mean_q: 19.875586
 15800/100000: episode: 158, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: 52.498, mean reward: 0.525 [0.280, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.151, 10.265], loss: 0.018436, mae: 0.151504, mean_q: 19.671751
 15900/100000: episode: 159, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 56.233, mean reward: 0.562 [0.248, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.472, 10.152], loss: 0.015688, mae: 0.139694, mean_q: 19.958525
 16000/100000: episode: 160, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 53.257, mean reward: 0.533 [0.260, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.517, 10.290], loss: 0.013247, mae: 0.127940, mean_q: 20.000269
 16100/100000: episode: 161, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 56.575, mean reward: 0.566 [0.423, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.463, 10.192], loss: 0.013653, mae: 0.130108, mean_q: 19.686146
 16200/100000: episode: 162, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 55.275, mean reward: 0.553 [0.310, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.696, 10.175], loss: 0.014700, mae: 0.135748, mean_q: 19.686682
 16300/100000: episode: 163, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 51.563, mean reward: 0.516 [0.295, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.682, 10.301], loss: 0.013837, mae: 0.131257, mean_q: 19.513996
 16400/100000: episode: 164, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: 52.048, mean reward: 0.520 [0.341, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.608, 10.098], loss: 0.013573, mae: 0.129021, mean_q: 19.763456
 16500/100000: episode: 165, duration: 0.993s, episode steps: 100, steps per second: 101, episode reward: 54.531, mean reward: 0.545 [0.220, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.879, 10.098], loss: 0.016196, mae: 0.142569, mean_q: 19.614119
 16600/100000: episode: 166, duration: 1.071s, episode steps: 100, steps per second: 93, episode reward: 53.005, mean reward: 0.530 [0.180, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.143, 10.428], loss: 0.014876, mae: 0.135348, mean_q: 19.667587
 16700/100000: episode: 167, duration: 1.015s, episode steps: 100, steps per second: 99, episode reward: 51.712, mean reward: 0.517 [0.270, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.181, 10.135], loss: 0.013776, mae: 0.131149, mean_q: 19.762169
 16800/100000: episode: 168, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 53.365, mean reward: 0.534 [0.366, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.859, 10.098], loss: 0.014402, mae: 0.132881, mean_q: 19.826462
 16900/100000: episode: 169, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 54.681, mean reward: 0.547 [0.306, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.112, 10.098], loss: 0.013089, mae: 0.127337, mean_q: 19.772802
 17000/100000: episode: 170, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 55.917, mean reward: 0.559 [0.339, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.039, 10.197], loss: 0.013217, mae: 0.129006, mean_q: 19.564802
 17100/100000: episode: 171, duration: 0.660s, episode steps: 100, steps per second: 152, episode reward: 51.730, mean reward: 0.517 [0.304, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.380, 10.323], loss: 0.016200, mae: 0.142114, mean_q: 20.038309
 17200/100000: episode: 172, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 51.531, mean reward: 0.515 [0.258, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.895, 10.156], loss: 0.015192, mae: 0.137136, mean_q: 19.622295
 17300/100000: episode: 173, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 49.099, mean reward: 0.491 [0.269, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.620, 10.494], loss: 0.014487, mae: 0.136973, mean_q: 19.701332
 17400/100000: episode: 174, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 55.052, mean reward: 0.551 [0.254, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.851, 10.098], loss: 0.013843, mae: 0.129588, mean_q: 19.505400
 17500/100000: episode: 175, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: 55.051, mean reward: 0.551 [0.309, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.154, 10.233], loss: 0.012032, mae: 0.120897, mean_q: 19.483416
 17600/100000: episode: 176, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 52.802, mean reward: 0.528 [0.314, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.187, 10.120], loss: 0.013667, mae: 0.130880, mean_q: 19.477148
 17700/100000: episode: 177, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: 46.013, mean reward: 0.460 [0.214, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.880, 10.545], loss: 0.013122, mae: 0.126225, mean_q: 19.929276
 17800/100000: episode: 178, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 53.331, mean reward: 0.533 [0.331, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.030, 10.098], loss: 0.014437, mae: 0.133816, mean_q: 19.725594
 17900/100000: episode: 179, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 54.883, mean reward: 0.549 [0.348, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.845, 10.170], loss: 0.012815, mae: 0.124831, mean_q: 19.562216
 18000/100000: episode: 180, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 52.425, mean reward: 0.524 [0.287, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.693, 10.401], loss: 0.011950, mae: 0.121571, mean_q: 19.909348
 18100/100000: episode: 181, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 54.227, mean reward: 0.542 [0.311, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.170, 10.287], loss: 0.012562, mae: 0.124398, mean_q: 19.745724
 18200/100000: episode: 182, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: 57.225, mean reward: 0.572 [0.329, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.989, 10.098], loss: 0.014848, mae: 0.135272, mean_q: 19.786655
 18300/100000: episode: 183, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 53.798, mean reward: 0.538 [0.303, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.409, 10.098], loss: 0.014845, mae: 0.134655, mean_q: 19.929317
 18400/100000: episode: 184, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 53.766, mean reward: 0.538 [0.305, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.390, 10.137], loss: 0.012940, mae: 0.126336, mean_q: 19.379602
 18500/100000: episode: 185, duration: 0.844s, episode steps: 100, steps per second: 118, episode reward: 55.400, mean reward: 0.554 [0.281, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.351, 10.150], loss: 0.012008, mae: 0.120717, mean_q: 19.678318
 18600/100000: episode: 186, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: 55.868, mean reward: 0.559 [0.317, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.151], loss: 0.015484, mae: 0.138444, mean_q: 19.774622
 18700/100000: episode: 187, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: 44.856, mean reward: 0.449 [0.253, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.932, 10.540], loss: 0.017817, mae: 0.145966, mean_q: 19.750063
 18800/100000: episode: 188, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 54.466, mean reward: 0.545 [0.306, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.577, 10.257], loss: 0.014800, mae: 0.134343, mean_q: 19.730413
 18900/100000: episode: 189, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: 55.431, mean reward: 0.554 [0.240, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.483, 10.204], loss: 0.016072, mae: 0.139301, mean_q: 19.679333
 19000/100000: episode: 190, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 57.430, mean reward: 0.574 [0.376, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.187, 10.242], loss: 0.013769, mae: 0.128798, mean_q: 19.465940
 19100/100000: episode: 191, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: 50.246, mean reward: 0.502 [0.282, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.197, 10.098], loss: 0.014044, mae: 0.127352, mean_q: 19.238537
 19200/100000: episode: 192, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 51.765, mean reward: 0.518 [0.279, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.107, 10.352], loss: 0.017001, mae: 0.142930, mean_q: 19.702650
 19300/100000: episode: 193, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 52.461, mean reward: 0.525 [0.372, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.808, 10.251], loss: 0.013824, mae: 0.129989, mean_q: 19.633654
 19400/100000: episode: 194, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 54.320, mean reward: 0.543 [0.383, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.635, 10.146], loss: 0.014084, mae: 0.130703, mean_q: 19.531313
 19500/100000: episode: 195, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 48.076, mean reward: 0.481 [0.203, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.888, 10.395], loss: 0.015157, mae: 0.136589, mean_q: 19.570030
 19600/100000: episode: 196, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 51.705, mean reward: 0.517 [0.192, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.906, 10.112], loss: 0.016684, mae: 0.142656, mean_q: 19.846115
 19700/100000: episode: 197, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 54.940, mean reward: 0.549 [0.278, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.798, 10.098], loss: 0.012839, mae: 0.124040, mean_q: 19.571661
 19800/100000: episode: 198, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 52.197, mean reward: 0.522 [0.192, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.992, 10.098], loss: 0.013033, mae: 0.124778, mean_q: 19.812338
 19900/100000: episode: 199, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 51.337, mean reward: 0.513 [0.236, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.385, 10.334], loss: 0.014627, mae: 0.133234, mean_q: 19.550982
 20000/100000: episode: 200, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 54.380, mean reward: 0.544 [0.254, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.831, 10.098], loss: 0.016325, mae: 0.138966, mean_q: 19.560179
 20100/100000: episode: 201, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: 48.202, mean reward: 0.482 [0.284, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.959, 10.098], loss: 0.014176, mae: 0.129948, mean_q: 19.532665
 20200/100000: episode: 202, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 49.563, mean reward: 0.496 [0.223, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.256, 10.210], loss: 0.015101, mae: 0.135237, mean_q: 19.614363
 20300/100000: episode: 203, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 52.615, mean reward: 0.526 [0.266, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.278, 10.098], loss: 0.013676, mae: 0.128415, mean_q: 19.626560
 20400/100000: episode: 204, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 49.066, mean reward: 0.491 [0.195, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.209, 10.448], loss: 0.013662, mae: 0.126886, mean_q: 19.578711
 20500/100000: episode: 205, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 50.765, mean reward: 0.508 [0.249, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.711, 10.496], loss: 0.014044, mae: 0.130295, mean_q: 19.637127
 20600/100000: episode: 206, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 54.956, mean reward: 0.550 [0.299, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.677, 10.098], loss: 0.012603, mae: 0.123260, mean_q: 19.654339
 20700/100000: episode: 207, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: 52.564, mean reward: 0.526 [0.273, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.770, 10.098], loss: 0.015186, mae: 0.133952, mean_q: 19.310684
 20800/100000: episode: 208, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 56.807, mean reward: 0.568 [0.342, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.787, 10.219], loss: 0.013314, mae: 0.123873, mean_q: 19.373896
 20900/100000: episode: 209, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 49.764, mean reward: 0.498 [0.272, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.478, 10.403], loss: 0.016103, mae: 0.137202, mean_q: 19.710443
 21000/100000: episode: 210, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 56.031, mean reward: 0.560 [0.359, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.478, 10.120], loss: 0.019742, mae: 0.151474, mean_q: 19.688740
 21100/100000: episode: 211, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 54.260, mean reward: 0.543 [0.333, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.790, 10.098], loss: 0.017539, mae: 0.142153, mean_q: 19.383717
 21200/100000: episode: 212, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 54.511, mean reward: 0.545 [0.392, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.729, 10.098], loss: 0.017063, mae: 0.140303, mean_q: 19.434340
 21300/100000: episode: 213, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 52.426, mean reward: 0.524 [0.331, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.530, 10.355], loss: 0.015687, mae: 0.136103, mean_q: 19.742727
 21400/100000: episode: 214, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 46.428, mean reward: 0.464 [0.169, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.035, 10.098], loss: 0.016492, mae: 0.138260, mean_q: 19.199303
 21500/100000: episode: 215, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 53.373, mean reward: 0.534 [0.248, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.522, 10.194], loss: 0.016411, mae: 0.137794, mean_q: 19.672932
 21600/100000: episode: 216, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 54.665, mean reward: 0.547 [0.224, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.665, 10.098], loss: 0.015303, mae: 0.134797, mean_q: 19.722157
 21700/100000: episode: 217, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 49.843, mean reward: 0.498 [0.237, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.974, 10.444], loss: 0.021249, mae: 0.157396, mean_q: 19.776520
 21800/100000: episode: 218, duration: 0.678s, episode steps: 100, steps per second: 147, episode reward: 51.970, mean reward: 0.520 [0.310, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.774, 10.114], loss: 0.016674, mae: 0.138972, mean_q: 19.722847
 21900/100000: episode: 219, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: 48.186, mean reward: 0.482 [0.229, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.350, 10.527], loss: 0.018423, mae: 0.142430, mean_q: 19.538956
 22000/100000: episode: 220, duration: 0.864s, episode steps: 100, steps per second: 116, episode reward: 50.266, mean reward: 0.503 [0.276, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.416, 10.447], loss: 0.015535, mae: 0.132360, mean_q: 19.613649
 22100/100000: episode: 221, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 53.956, mean reward: 0.540 [0.167, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.014, 10.098], loss: 0.017270, mae: 0.138225, mean_q: 19.531540
 22200/100000: episode: 222, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 56.183, mean reward: 0.562 [0.419, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.386, 10.098], loss: 0.020229, mae: 0.152902, mean_q: 19.586531
 22300/100000: episode: 223, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 52.758, mean reward: 0.528 [0.294, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.204, 10.098], loss: 0.015054, mae: 0.133414, mean_q: 19.444220
 22400/100000: episode: 224, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 54.416, mean reward: 0.544 [0.355, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.366, 10.190], loss: 0.021231, mae: 0.150960, mean_q: 19.241318
 22500/100000: episode: 225, duration: 0.825s, episode steps: 100, steps per second: 121, episode reward: 53.649, mean reward: 0.536 [0.288, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.858, 10.098], loss: 0.016316, mae: 0.135771, mean_q: 19.487604
 22600/100000: episode: 226, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 55.830, mean reward: 0.558 [0.372, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.014, 10.183], loss: 0.016017, mae: 0.134566, mean_q: 19.539236
 22700/100000: episode: 227, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 50.692, mean reward: 0.507 [0.188, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.707, 10.334], loss: 0.016774, mae: 0.135850, mean_q: 19.536432
 22800/100000: episode: 228, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 56.092, mean reward: 0.561 [0.258, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.537, 10.098], loss: 0.017954, mae: 0.142502, mean_q: 19.648056
 22900/100000: episode: 229, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 54.705, mean reward: 0.547 [0.296, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.244, 10.183], loss: 0.016822, mae: 0.139015, mean_q: 19.386129
 23000/100000: episode: 230, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 55.764, mean reward: 0.558 [0.347, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.781, 10.121], loss: 0.016007, mae: 0.131488, mean_q: 19.352842
 23100/100000: episode: 231, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 48.289, mean reward: 0.483 [0.198, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.525, 10.316], loss: 0.021837, mae: 0.154837, mean_q: 19.240202
 23200/100000: episode: 232, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 48.467, mean reward: 0.485 [0.273, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.704, 10.098], loss: 0.017130, mae: 0.142527, mean_q: 19.494112
 23300/100000: episode: 233, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 55.040, mean reward: 0.550 [0.285, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.610, 10.390], loss: 0.018475, mae: 0.146343, mean_q: 19.623554
 23400/100000: episode: 234, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 53.431, mean reward: 0.534 [0.268, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.197, 10.098], loss: 0.018661, mae: 0.141998, mean_q: 19.553873
 23500/100000: episode: 235, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 54.036, mean reward: 0.540 [0.297, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.000, 10.098], loss: 0.015073, mae: 0.130977, mean_q: 19.584972
 23600/100000: episode: 236, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 53.733, mean reward: 0.537 [0.257, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.651, 10.258], loss: 0.019102, mae: 0.148188, mean_q: 19.310898
 23700/100000: episode: 237, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 53.892, mean reward: 0.539 [0.258, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.481, 10.098], loss: 0.018739, mae: 0.141549, mean_q: 19.333271
 23800/100000: episode: 238, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 53.435, mean reward: 0.534 [0.320, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.019, 10.258], loss: 0.016468, mae: 0.139441, mean_q: 19.646799
 23900/100000: episode: 239, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 55.398, mean reward: 0.554 [0.235, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.943, 10.102], loss: 0.017817, mae: 0.139328, mean_q: 19.300016
 24000/100000: episode: 240, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 53.188, mean reward: 0.532 [0.307, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.811, 10.098], loss: 0.016963, mae: 0.138273, mean_q: 19.543715
 24100/100000: episode: 241, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 51.342, mean reward: 0.513 [0.318, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.691, 10.098], loss: 0.016419, mae: 0.137374, mean_q: 19.505590
 24200/100000: episode: 242, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 55.039, mean reward: 0.550 [0.263, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.962, 10.098], loss: 0.017503, mae: 0.139452, mean_q: 19.431154
 24300/100000: episode: 243, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 54.452, mean reward: 0.545 [0.245, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.675, 10.098], loss: 0.018905, mae: 0.146101, mean_q: 19.685095
 24400/100000: episode: 244, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 51.628, mean reward: 0.516 [0.304, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.765, 10.098], loss: 0.020744, mae: 0.156361, mean_q: 19.268387
 24500/100000: episode: 245, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 55.445, mean reward: 0.554 [0.176, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.087, 10.197], loss: 0.017072, mae: 0.141254, mean_q: 19.568733
 24600/100000: episode: 246, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 46.894, mean reward: 0.469 [0.148, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.402, 10.098], loss: 0.018104, mae: 0.144753, mean_q: 19.299145
 24700/100000: episode: 247, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 55.292, mean reward: 0.553 [0.333, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.340, 10.131], loss: 0.016837, mae: 0.139148, mean_q: 19.685741
 24800/100000: episode: 248, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: 55.301, mean reward: 0.553 [0.345, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.957, 10.098], loss: 0.019710, mae: 0.143013, mean_q: 19.691204
 24900/100000: episode: 249, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 55.611, mean reward: 0.556 [0.304, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.156, 10.173], loss: 0.017716, mae: 0.142433, mean_q: 19.401909
 25000/100000: episode: 250, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 52.633, mean reward: 0.526 [0.237, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.348, 10.098], loss: 0.016210, mae: 0.138090, mean_q: 19.526525
 25100/100000: episode: 251, duration: 0.849s, episode steps: 100, steps per second: 118, episode reward: 54.628, mean reward: 0.546 [0.273, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.513, 10.256], loss: 0.018619, mae: 0.147971, mean_q: 19.675169
 25200/100000: episode: 252, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 53.581, mean reward: 0.536 [0.314, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.865, 10.126], loss: 0.017628, mae: 0.139118, mean_q: 19.460646
 25300/100000: episode: 253, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 54.161, mean reward: 0.542 [0.296, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.299, 10.138], loss: 0.019626, mae: 0.148972, mean_q: 19.702108
 25400/100000: episode: 254, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 54.686, mean reward: 0.547 [0.256, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.612, 10.255], loss: 0.017374, mae: 0.142019, mean_q: 19.454353
 25500/100000: episode: 255, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 52.080, mean reward: 0.521 [0.244, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.392, 10.098], loss: 0.018267, mae: 0.147064, mean_q: 19.603510
 25600/100000: episode: 256, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 52.919, mean reward: 0.529 [0.273, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.574, 10.098], loss: 0.017574, mae: 0.144012, mean_q: 19.532982
 25700/100000: episode: 257, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 53.598, mean reward: 0.536 [0.256, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.697, 10.098], loss: 0.020016, mae: 0.154903, mean_q: 19.298096
 25800/100000: episode: 258, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 52.434, mean reward: 0.524 [0.252, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.640, 10.174], loss: 0.018238, mae: 0.144235, mean_q: 19.877188
 25900/100000: episode: 259, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 52.029, mean reward: 0.520 [0.175, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.639, 10.098], loss: 0.014423, mae: 0.129471, mean_q: 19.514473
 26000/100000: episode: 260, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 52.643, mean reward: 0.526 [0.235, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.186, 10.098], loss: 0.020809, mae: 0.155285, mean_q: 19.757215
 26100/100000: episode: 261, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 53.863, mean reward: 0.539 [0.345, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.389, 10.098], loss: 0.016817, mae: 0.140301, mean_q: 19.412235
 26200/100000: episode: 262, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 54.438, mean reward: 0.544 [0.372, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.884, 10.098], loss: 0.017335, mae: 0.142747, mean_q: 19.639744
 26300/100000: episode: 263, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 50.753, mean reward: 0.508 [0.295, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.053, 10.201], loss: 0.015448, mae: 0.133087, mean_q: 19.664133
 26400/100000: episode: 264, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 51.393, mean reward: 0.514 [0.196, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.463, 10.202], loss: 0.018236, mae: 0.144470, mean_q: 19.757874
 26500/100000: episode: 265, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 52.709, mean reward: 0.527 [0.262, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.175, 10.248], loss: 0.016045, mae: 0.137494, mean_q: 19.470745
 26600/100000: episode: 266, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 52.777, mean reward: 0.528 [0.265, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.617, 10.127], loss: 0.017696, mae: 0.141561, mean_q: 19.575027
 26700/100000: episode: 267, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 50.611, mean reward: 0.506 [0.265, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.841, 10.098], loss: 0.018111, mae: 0.144213, mean_q: 19.835375
 26800/100000: episode: 268, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 54.830, mean reward: 0.548 [0.360, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.472, 10.098], loss: 0.015496, mae: 0.133156, mean_q: 19.406061
 26900/100000: episode: 269, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 53.661, mean reward: 0.537 [0.319, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.183, 10.242], loss: 0.016591, mae: 0.139535, mean_q: 19.184212
 27000/100000: episode: 270, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: 53.029, mean reward: 0.530 [0.145, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.496, 10.341], loss: 0.015431, mae: 0.135174, mean_q: 19.599504
 27100/100000: episode: 271, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 56.726, mean reward: 0.567 [0.266, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.414, 10.153], loss: 0.016222, mae: 0.137615, mean_q: 19.742985
 27200/100000: episode: 272, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 53.962, mean reward: 0.540 [0.250, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.742, 10.247], loss: 0.017230, mae: 0.142434, mean_q: 19.828814
 27300/100000: episode: 273, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 53.623, mean reward: 0.536 [0.143, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.002, 10.227], loss: 0.015424, mae: 0.133816, mean_q: 19.478973
 27400/100000: episode: 274, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 47.561, mean reward: 0.476 [0.279, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.066, 10.098], loss: 0.015504, mae: 0.136984, mean_q: 19.747019
 27500/100000: episode: 275, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 57.199, mean reward: 0.572 [0.416, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.966, 10.263], loss: 0.012788, mae: 0.124264, mean_q: 19.363567
 27600/100000: episode: 276, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 52.907, mean reward: 0.529 [0.235, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.017, 10.098], loss: 0.014864, mae: 0.133029, mean_q: 19.641493
 27700/100000: episode: 277, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 55.504, mean reward: 0.555 [0.239, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.677, 10.098], loss: 0.013663, mae: 0.127502, mean_q: 19.740976
 27800/100000: episode: 278, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 53.785, mean reward: 0.538 [0.351, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.342, 10.121], loss: 0.014837, mae: 0.133529, mean_q: 19.638519
 27900/100000: episode: 279, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 52.420, mean reward: 0.524 [0.308, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.626, 10.112], loss: 0.015314, mae: 0.136765, mean_q: 19.543337
 28000/100000: episode: 280, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 52.610, mean reward: 0.526 [0.294, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.520, 10.098], loss: 0.014017, mae: 0.127521, mean_q: 19.540987
 28100/100000: episode: 281, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 49.926, mean reward: 0.499 [0.293, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.284, 10.386], loss: 0.014272, mae: 0.131428, mean_q: 19.884792
 28200/100000: episode: 282, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 56.070, mean reward: 0.561 [0.342, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.963, 10.142], loss: 0.015962, mae: 0.140174, mean_q: 19.542387
 28300/100000: episode: 283, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 54.279, mean reward: 0.543 [0.333, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.167, 10.098], loss: 0.013975, mae: 0.131655, mean_q: 19.437193
 28400/100000: episode: 284, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 52.742, mean reward: 0.527 [0.291, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.768, 10.149], loss: 0.013965, mae: 0.129993, mean_q: 19.469681
 28500/100000: episode: 285, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 50.587, mean reward: 0.506 [0.283, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.188, 10.098], loss: 0.015672, mae: 0.139391, mean_q: 19.755110
 28600/100000: episode: 286, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 55.812, mean reward: 0.558 [0.288, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.403, 10.098], loss: 0.015949, mae: 0.137956, mean_q: 19.660793
 28700/100000: episode: 287, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: 52.229, mean reward: 0.522 [0.232, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.666, 10.120], loss: 0.013108, mae: 0.126565, mean_q: 19.352413
 28800/100000: episode: 288, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 53.173, mean reward: 0.532 [0.355, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.414, 10.442], loss: 0.015517, mae: 0.136618, mean_q: 19.365351
 28900/100000: episode: 289, duration: 0.883s, episode steps: 100, steps per second: 113, episode reward: 55.811, mean reward: 0.558 [0.361, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.389, 10.177], loss: 0.014763, mae: 0.133887, mean_q: 19.742983
 29000/100000: episode: 290, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 55.005, mean reward: 0.550 [0.325, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.605, 10.098], loss: 0.013203, mae: 0.126347, mean_q: 19.788219
 29100/100000: episode: 291, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 54.642, mean reward: 0.546 [0.292, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.965, 10.279], loss: 0.013454, mae: 0.128778, mean_q: 19.615618
 29200/100000: episode: 292, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 47.266, mean reward: 0.473 [0.234, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.065, 10.176], loss: 0.013821, mae: 0.131146, mean_q: 20.070726
 29300/100000: episode: 293, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 45.832, mean reward: 0.458 [0.131, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-1.774, 10.098], loss: 0.013393, mae: 0.126530, mean_q: 19.502363
 29400/100000: episode: 294, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 52.713, mean reward: 0.527 [0.284, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.236, 10.505], loss: 0.013342, mae: 0.126430, mean_q: 19.856819
 29500/100000: episode: 295, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 55.788, mean reward: 0.558 [0.287, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.983, 10.099], loss: 0.014452, mae: 0.128680, mean_q: 19.673372
 29600/100000: episode: 296, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 50.779, mean reward: 0.508 [0.285, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.052, 10.326], loss: 0.013866, mae: 0.129125, mean_q: 19.698404
 29700/100000: episode: 297, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 50.959, mean reward: 0.510 [0.192, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.757, 10.098], loss: 0.012437, mae: 0.123192, mean_q: 19.644512
 29800/100000: episode: 298, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: 55.963, mean reward: 0.560 [0.223, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.892, 10.098], loss: 0.013912, mae: 0.129931, mean_q: 19.736914
 29900/100000: episode: 299, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 53.402, mean reward: 0.534 [0.305, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.371, 10.098], loss: 0.012427, mae: 0.124355, mean_q: 19.939817
 30000/100000: episode: 300, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 55.191, mean reward: 0.552 [0.332, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.209, 10.191], loss: 0.014295, mae: 0.128929, mean_q: 19.719587
 30100/100000: episode: 301, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 54.897, mean reward: 0.549 [0.270, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.823, 10.281], loss: 0.014075, mae: 0.130837, mean_q: 19.721321
 30200/100000: episode: 302, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 52.531, mean reward: 0.525 [0.212, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.380, 10.098], loss: 0.013726, mae: 0.128916, mean_q: 19.330595
 30300/100000: episode: 303, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 48.369, mean reward: 0.484 [0.225, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.257, 10.399], loss: 0.012290, mae: 0.121694, mean_q: 19.545052
 30400/100000: episode: 304, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: 51.764, mean reward: 0.518 [0.314, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.314, 10.098], loss: 0.014712, mae: 0.130164, mean_q: 19.661140
 30500/100000: episode: 305, duration: 1.041s, episode steps: 100, steps per second: 96, episode reward: 54.194, mean reward: 0.542 [0.296, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.764, 10.098], loss: 0.014241, mae: 0.130483, mean_q: 19.555861
 30600/100000: episode: 306, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 53.593, mean reward: 0.536 [0.116, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.493, 10.098], loss: 0.014761, mae: 0.132834, mean_q: 19.932949
 30700/100000: episode: 307, duration: 0.894s, episode steps: 100, steps per second: 112, episode reward: 55.268, mean reward: 0.553 [0.230, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.822, 10.218], loss: 0.014102, mae: 0.129781, mean_q: 19.653820
 30800/100000: episode: 308, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 51.174, mean reward: 0.512 [0.242, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.029, 10.358], loss: 0.016167, mae: 0.140055, mean_q: 19.751915
 30900/100000: episode: 309, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 51.777, mean reward: 0.518 [0.200, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.350, 10.186], loss: 0.013931, mae: 0.128903, mean_q: 19.338585
 31000/100000: episode: 310, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 55.048, mean reward: 0.550 [0.308, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.142, 10.098], loss: 0.015163, mae: 0.135107, mean_q: 19.607719
 31100/100000: episode: 311, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: 51.631, mean reward: 0.516 [0.333, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.659, 10.117], loss: 0.014743, mae: 0.134752, mean_q: 19.653393
 31200/100000: episode: 312, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 54.811, mean reward: 0.548 [0.300, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.106, 10.198], loss: 0.016082, mae: 0.139029, mean_q: 19.625357
 31300/100000: episode: 313, duration: 0.818s, episode steps: 100, steps per second: 122, episode reward: 53.200, mean reward: 0.532 [0.243, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.209, 10.177], loss: 0.015110, mae: 0.135243, mean_q: 19.816126
 31400/100000: episode: 314, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 53.220, mean reward: 0.532 [0.286, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.563, 10.098], loss: 0.015117, mae: 0.135875, mean_q: 19.589540
 31500/100000: episode: 315, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 56.374, mean reward: 0.564 [0.353, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.116, 10.291], loss: 0.014994, mae: 0.134930, mean_q: 19.485764
 31600/100000: episode: 316, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 55.198, mean reward: 0.552 [0.349, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.568, 10.098], loss: 0.013386, mae: 0.127077, mean_q: 19.416248
 31700/100000: episode: 317, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 53.857, mean reward: 0.539 [0.337, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.795, 10.098], loss: 0.012973, mae: 0.124654, mean_q: 19.599720
 31800/100000: episode: 318, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 56.040, mean reward: 0.560 [0.414, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.723, 10.098], loss: 0.013106, mae: 0.125952, mean_q: 20.091059
 31900/100000: episode: 319, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 56.928, mean reward: 0.569 [0.345, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.010, 10.216], loss: 0.014931, mae: 0.134032, mean_q: 19.335308
 32000/100000: episode: 320, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 53.212, mean reward: 0.532 [0.249, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.194, 10.098], loss: 0.012564, mae: 0.123685, mean_q: 19.643633
 32100/100000: episode: 321, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 49.837, mean reward: 0.498 [0.255, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.192, 10.098], loss: 0.013813, mae: 0.129791, mean_q: 19.462055
 32200/100000: episode: 322, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 55.664, mean reward: 0.557 [0.320, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.674, 10.207], loss: 0.014677, mae: 0.133646, mean_q: 19.447212
 32300/100000: episode: 323, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 53.262, mean reward: 0.533 [0.343, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.976, 10.098], loss: 0.015584, mae: 0.137040, mean_q: 19.508085
 32400/100000: episode: 324, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 54.953, mean reward: 0.550 [0.195, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.710, 10.265], loss: 0.014575, mae: 0.132064, mean_q: 19.295275
 32500/100000: episode: 325, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: 55.825, mean reward: 0.558 [0.374, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.335, 10.151], loss: 0.014194, mae: 0.130399, mean_q: 19.596403
 32600/100000: episode: 326, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 52.798, mean reward: 0.528 [0.251, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.605, 10.212], loss: 0.014802, mae: 0.133059, mean_q: 19.540520
 32700/100000: episode: 327, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 53.410, mean reward: 0.534 [0.292, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.641, 10.241], loss: 0.012972, mae: 0.125245, mean_q: 19.718258
 32800/100000: episode: 328, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 51.472, mean reward: 0.515 [0.199, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.235, 10.098], loss: 0.014873, mae: 0.132912, mean_q: 19.437214
 32900/100000: episode: 329, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 54.569, mean reward: 0.546 [0.274, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.457, 10.098], loss: 0.014322, mae: 0.130538, mean_q: 19.677788
 33000/100000: episode: 330, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 50.934, mean reward: 0.509 [0.226, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.528, 10.098], loss: 0.013790, mae: 0.129328, mean_q: 19.333933
 33100/100000: episode: 331, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 53.792, mean reward: 0.538 [0.315, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.885, 10.196], loss: 0.013242, mae: 0.126098, mean_q: 19.984032
 33200/100000: episode: 332, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 53.717, mean reward: 0.537 [0.205, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.748, 10.237], loss: 0.013557, mae: 0.128662, mean_q: 19.926807
 33300/100000: episode: 333, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: 50.468, mean reward: 0.505 [0.296, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.863, 10.342], loss: 0.013891, mae: 0.128694, mean_q: 19.361116
 33400/100000: episode: 334, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 52.965, mean reward: 0.530 [0.228, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.355, 10.098], loss: 0.012589, mae: 0.124189, mean_q: 19.516325
 33500/100000: episode: 335, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 52.882, mean reward: 0.529 [0.367, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.874, 10.158], loss: 0.013716, mae: 0.128283, mean_q: 19.391356
 33600/100000: episode: 336, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 45.240, mean reward: 0.452 [0.269, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.674, 10.458], loss: 0.015798, mae: 0.139084, mean_q: 19.600138
 33700/100000: episode: 337, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 54.764, mean reward: 0.548 [0.192, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.176, 10.098], loss: 0.015066, mae: 0.134173, mean_q: 19.529505
 33800/100000: episode: 338, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 52.671, mean reward: 0.527 [0.255, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.291, 10.222], loss: 0.015509, mae: 0.136186, mean_q: 19.608608
 33900/100000: episode: 339, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 55.446, mean reward: 0.554 [0.300, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.674, 10.156], loss: 0.014320, mae: 0.131887, mean_q: 19.316872
 34000/100000: episode: 340, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 53.759, mean reward: 0.538 [0.300, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.713, 10.356], loss: 0.013823, mae: 0.129351, mean_q: 19.687099
 34100/100000: episode: 341, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 53.186, mean reward: 0.532 [0.179, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.027, 10.098], loss: 0.014708, mae: 0.133422, mean_q: 19.519753
 34200/100000: episode: 342, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 53.742, mean reward: 0.537 [0.332, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.729, 10.233], loss: 0.014734, mae: 0.132863, mean_q: 19.686022
 34300/100000: episode: 343, duration: 0.601s, episode steps: 100, steps per second: 167, episode reward: 54.182, mean reward: 0.542 [0.343, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.560, 10.143], loss: 0.014375, mae: 0.130116, mean_q: 19.557806
 34400/100000: episode: 344, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 53.549, mean reward: 0.535 [0.266, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.437, 10.183], loss: 0.013228, mae: 0.125653, mean_q: 19.730000
 34500/100000: episode: 345, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 48.688, mean reward: 0.487 [0.271, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.028, 10.098], loss: 0.013891, mae: 0.130730, mean_q: 19.750439
 34600/100000: episode: 346, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 56.216, mean reward: 0.562 [0.285, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.415, 10.138], loss: 0.016122, mae: 0.139161, mean_q: 19.963518
 34700/100000: episode: 347, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 55.084, mean reward: 0.551 [0.332, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.529, 10.098], loss: 0.012814, mae: 0.122354, mean_q: 19.464275
 34800/100000: episode: 348, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 54.362, mean reward: 0.544 [0.292, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.749, 10.098], loss: 0.012560, mae: 0.123510, mean_q: 19.699034
 34900/100000: episode: 349, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 49.548, mean reward: 0.495 [0.267, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.968, 10.180], loss: 0.013288, mae: 0.128551, mean_q: 19.745472
 35000/100000: episode: 350, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 50.126, mean reward: 0.501 [0.231, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.856, 10.367], loss: 0.015139, mae: 0.134893, mean_q: 19.412107
 35100/100000: episode: 351, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 49.032, mean reward: 0.490 [0.256, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.221, 10.306], loss: 0.014978, mae: 0.136092, mean_q: 19.718361
 35200/100000: episode: 352, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 52.445, mean reward: 0.524 [0.222, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.754, 10.344], loss: 0.016737, mae: 0.143471, mean_q: 19.344316
 35300/100000: episode: 353, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 51.807, mean reward: 0.518 [0.266, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.944, 10.098], loss: 0.014616, mae: 0.133921, mean_q: 19.687794
 35400/100000: episode: 354, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 49.609, mean reward: 0.496 [0.233, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.449, 10.288], loss: 0.012929, mae: 0.126871, mean_q: 19.502314
 35500/100000: episode: 355, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 56.704, mean reward: 0.567 [0.219, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.557, 10.098], loss: 0.014699, mae: 0.132023, mean_q: 19.730528
 35600/100000: episode: 356, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 51.573, mean reward: 0.516 [0.140, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.320, 10.258], loss: 0.014041, mae: 0.128549, mean_q: 19.668411
 35700/100000: episode: 357, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 52.510, mean reward: 0.525 [0.345, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.512, 10.098], loss: 0.012888, mae: 0.124478, mean_q: 19.701218
 35800/100000: episode: 358, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 52.157, mean reward: 0.522 [0.294, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.301, 10.297], loss: 0.013302, mae: 0.128788, mean_q: 20.074247
 35900/100000: episode: 359, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 54.433, mean reward: 0.544 [0.369, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.782, 10.234], loss: 0.012983, mae: 0.124302, mean_q: 19.814587
 36000/100000: episode: 360, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 52.153, mean reward: 0.522 [0.345, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.328, 10.265], loss: 0.015499, mae: 0.138176, mean_q: 19.717091
 36100/100000: episode: 361, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 51.932, mean reward: 0.519 [0.301, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.203, 10.281], loss: 0.012915, mae: 0.127114, mean_q: 19.521292
 36200/100000: episode: 362, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 54.769, mean reward: 0.548 [0.362, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.189, 10.098], loss: 0.014664, mae: 0.133863, mean_q: 19.644506
 36300/100000: episode: 363, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 52.789, mean reward: 0.528 [0.342, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.992, 10.098], loss: 0.013070, mae: 0.125421, mean_q: 19.801546
 36400/100000: episode: 364, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 53.860, mean reward: 0.539 [0.320, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.908, 10.098], loss: 0.015407, mae: 0.137804, mean_q: 19.769804
 36500/100000: episode: 365, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 51.876, mean reward: 0.519 [0.265, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.294, 10.216], loss: 0.013728, mae: 0.129001, mean_q: 19.964289
 36600/100000: episode: 366, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: 50.850, mean reward: 0.509 [0.195, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.756, 10.098], loss: 0.013078, mae: 0.127257, mean_q: 19.922924
 36700/100000: episode: 367, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 53.436, mean reward: 0.534 [0.340, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.320, 10.098], loss: 0.012044, mae: 0.121649, mean_q: 19.721708
 36800/100000: episode: 368, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 52.989, mean reward: 0.530 [0.368, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.071, 10.242], loss: 0.014101, mae: 0.132388, mean_q: 19.800945
 36900/100000: episode: 369, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 54.500, mean reward: 0.545 [0.305, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.114, 10.283], loss: 0.013330, mae: 0.128494, mean_q: 19.691002
 37000/100000: episode: 370, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 52.475, mean reward: 0.525 [0.194, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.591, 10.346], loss: 0.012027, mae: 0.121093, mean_q: 19.605877
 37100/100000: episode: 371, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 53.132, mean reward: 0.531 [0.337, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.330, 10.206], loss: 0.013321, mae: 0.127823, mean_q: 20.065479
 37200/100000: episode: 372, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 54.197, mean reward: 0.542 [0.311, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.157, 10.219], loss: 0.014179, mae: 0.132199, mean_q: 19.667532
 37300/100000: episode: 373, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 51.736, mean reward: 0.517 [0.125, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.956, 10.373], loss: 0.012708, mae: 0.125104, mean_q: 19.662918
 37400/100000: episode: 374, duration: 0.734s, episode steps: 100, steps per second: 136, episode reward: 47.917, mean reward: 0.479 [0.261, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.251, 10.173], loss: 0.013915, mae: 0.131079, mean_q: 19.831450
 37500/100000: episode: 375, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: 54.290, mean reward: 0.543 [0.305, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.839, 10.098], loss: 0.012901, mae: 0.124929, mean_q: 19.588411
 37600/100000: episode: 376, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 55.379, mean reward: 0.554 [0.349, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.936, 10.098], loss: 0.012982, mae: 0.126745, mean_q: 19.476608
 37700/100000: episode: 377, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 54.598, mean reward: 0.546 [0.334, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.622, 10.098], loss: 0.012761, mae: 0.124346, mean_q: 19.877958
 37800/100000: episode: 378, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 52.643, mean reward: 0.526 [0.301, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.947, 10.174], loss: 0.011779, mae: 0.119602, mean_q: 19.870590
 37900/100000: episode: 379, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 52.570, mean reward: 0.526 [0.266, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.756, 10.107], loss: 0.013965, mae: 0.130484, mean_q: 19.646652
 38000/100000: episode: 380, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 52.519, mean reward: 0.525 [0.342, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.989, 10.174], loss: 0.015770, mae: 0.138930, mean_q: 19.933804
 38100/100000: episode: 381, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 54.911, mean reward: 0.549 [0.290, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.770, 10.223], loss: 0.014194, mae: 0.131417, mean_q: 19.677174
 38200/100000: episode: 382, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 54.127, mean reward: 0.541 [0.193, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.742, 10.104], loss: 0.012996, mae: 0.127969, mean_q: 19.670923
 38300/100000: episode: 383, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 50.275, mean reward: 0.503 [0.301, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.460, 10.098], loss: 0.013244, mae: 0.128116, mean_q: 19.419724
 38400/100000: episode: 384, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 53.383, mean reward: 0.534 [0.253, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.099, 10.279], loss: 0.012497, mae: 0.124577, mean_q: 19.671101
 38500/100000: episode: 385, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 54.380, mean reward: 0.544 [0.357, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.470, 10.098], loss: 0.012557, mae: 0.125413, mean_q: 19.473764
 38600/100000: episode: 386, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 51.538, mean reward: 0.515 [0.322, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.096, 10.098], loss: 0.014495, mae: 0.132676, mean_q: 19.636879
 38700/100000: episode: 387, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 51.494, mean reward: 0.515 [0.304, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.013, 10.262], loss: 0.013327, mae: 0.129304, mean_q: 19.860973
 38800/100000: episode: 388, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 52.951, mean reward: 0.530 [0.322, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.671, 10.098], loss: 0.013577, mae: 0.130886, mean_q: 19.800684
 38900/100000: episode: 389, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 53.058, mean reward: 0.531 [0.395, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.980, 10.098], loss: 0.012407, mae: 0.124591, mean_q: 19.486946
 39000/100000: episode: 390, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 53.452, mean reward: 0.535 [0.138, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.717, 10.199], loss: 0.013514, mae: 0.129694, mean_q: 19.674244
 39100/100000: episode: 391, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 52.137, mean reward: 0.521 [0.351, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.303, 10.098], loss: 0.013184, mae: 0.128878, mean_q: 19.684578
 39200/100000: episode: 392, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 54.681, mean reward: 0.547 [0.330, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.970, 10.328], loss: 0.014019, mae: 0.130676, mean_q: 19.790119
 39300/100000: episode: 393, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 52.959, mean reward: 0.530 [0.258, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.930, 10.156], loss: 0.014834, mae: 0.136989, mean_q: 19.706930
 39400/100000: episode: 394, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 55.971, mean reward: 0.560 [0.257, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.427, 10.154], loss: 0.012934, mae: 0.126491, mean_q: 19.554970
 39500/100000: episode: 395, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 48.423, mean reward: 0.484 [0.195, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.942, 10.098], loss: 0.012951, mae: 0.126466, mean_q: 19.865063
 39600/100000: episode: 396, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 47.564, mean reward: 0.476 [0.322, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.331, 10.297], loss: 0.014284, mae: 0.133896, mean_q: 19.509289
 39700/100000: episode: 397, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 50.922, mean reward: 0.509 [0.275, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.818, 10.200], loss: 0.012707, mae: 0.125581, mean_q: 19.602148
 39800/100000: episode: 398, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 51.127, mean reward: 0.511 [0.278, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.053, 10.098], loss: 0.014226, mae: 0.131173, mean_q: 19.652971
 39900/100000: episode: 399, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 55.027, mean reward: 0.550 [0.361, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.749, 10.098], loss: 0.013193, mae: 0.127365, mean_q: 19.699083
 40000/100000: episode: 400, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 54.610, mean reward: 0.546 [0.275, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.839, 10.189], loss: 0.014025, mae: 0.132709, mean_q: 19.654839
 40100/100000: episode: 401, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 56.605, mean reward: 0.566 [0.310, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.632, 10.098], loss: 0.011727, mae: 0.119100, mean_q: 19.709766
 40200/100000: episode: 402, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 53.477, mean reward: 0.535 [0.354, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.577, 10.194], loss: 0.014062, mae: 0.131340, mean_q: 19.693623
 40300/100000: episode: 403, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 52.379, mean reward: 0.524 [0.279, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.482, 10.098], loss: 0.013051, mae: 0.126376, mean_q: 19.562891
 40400/100000: episode: 404, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 56.369, mean reward: 0.564 [0.401, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.146, 10.098], loss: 0.014620, mae: 0.134961, mean_q: 19.429859
 40500/100000: episode: 405, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 51.565, mean reward: 0.516 [0.278, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.275, 10.283], loss: 0.014051, mae: 0.130586, mean_q: 19.853878
 40600/100000: episode: 406, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 46.949, mean reward: 0.469 [0.225, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.439, 10.338], loss: 0.013885, mae: 0.131004, mean_q: 19.572205
 40700/100000: episode: 407, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 54.333, mean reward: 0.543 [0.315, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.727, 10.098], loss: 0.013869, mae: 0.128340, mean_q: 19.580006
 40800/100000: episode: 408, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 50.955, mean reward: 0.510 [0.248, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.342, 10.152], loss: 0.012572, mae: 0.122676, mean_q: 19.527042
 40900/100000: episode: 409, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 53.607, mean reward: 0.536 [0.313, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.796, 10.182], loss: 0.012961, mae: 0.123558, mean_q: 19.519020
 41000/100000: episode: 410, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 54.445, mean reward: 0.544 [0.318, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.582, 10.098], loss: 0.013857, mae: 0.128908, mean_q: 19.763145
 41100/100000: episode: 411, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: 55.151, mean reward: 0.552 [0.290, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.689, 10.098], loss: 0.013788, mae: 0.128322, mean_q: 19.516390
 41200/100000: episode: 412, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 52.043, mean reward: 0.520 [0.279, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.206, 10.446], loss: 0.014441, mae: 0.132097, mean_q: 19.406403
 41300/100000: episode: 413, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 54.532, mean reward: 0.545 [0.371, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.518, 10.285], loss: 0.015624, mae: 0.137225, mean_q: 19.542337
 41400/100000: episode: 414, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 56.505, mean reward: 0.565 [0.301, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.005, 10.098], loss: 0.014017, mae: 0.127854, mean_q: 19.333332
 41500/100000: episode: 415, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 52.233, mean reward: 0.522 [0.276, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.910, 10.151], loss: 0.015567, mae: 0.134341, mean_q: 19.325804
 41600/100000: episode: 416, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 53.654, mean reward: 0.537 [0.160, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.584, 10.098], loss: 0.013379, mae: 0.125068, mean_q: 19.616844
 41700/100000: episode: 417, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 56.845, mean reward: 0.568 [0.356, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.446, 10.098], loss: 0.014365, mae: 0.130117, mean_q: 19.842640
 41800/100000: episode: 418, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 55.561, mean reward: 0.556 [0.291, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.306, 10.098], loss: 0.015627, mae: 0.133867, mean_q: 19.751070
 41900/100000: episode: 419, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 48.517, mean reward: 0.485 [0.181, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.946, 10.409], loss: 0.015605, mae: 0.136318, mean_q: 19.609419
 42000/100000: episode: 420, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 51.517, mean reward: 0.515 [0.266, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.339, 10.098], loss: 0.015532, mae: 0.135782, mean_q: 19.574112
 42100/100000: episode: 421, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 55.848, mean reward: 0.558 [0.321, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.996, 10.197], loss: 0.015963, mae: 0.135443, mean_q: 19.807215
 42200/100000: episode: 422, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 51.287, mean reward: 0.513 [0.300, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.604, 10.314], loss: 0.014763, mae: 0.130158, mean_q: 19.500015
 42300/100000: episode: 423, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 53.651, mean reward: 0.537 [0.318, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.958, 10.098], loss: 0.018521, mae: 0.149201, mean_q: 19.502111
 42400/100000: episode: 424, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 55.048, mean reward: 0.550 [0.339, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.497, 10.098], loss: 0.015180, mae: 0.129441, mean_q: 19.254044
 42500/100000: episode: 425, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 48.888, mean reward: 0.489 [0.316, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.633, 10.276], loss: 0.016647, mae: 0.137910, mean_q: 19.967422
 42600/100000: episode: 426, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 55.115, mean reward: 0.551 [0.343, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.056, 10.098], loss: 0.015729, mae: 0.133111, mean_q: 19.878668
 42700/100000: episode: 427, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 53.898, mean reward: 0.539 [0.348, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.700, 10.408], loss: 0.015163, mae: 0.132171, mean_q: 19.724005
 42800/100000: episode: 428, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 53.404, mean reward: 0.534 [0.291, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.158, 10.173], loss: 0.015685, mae: 0.129531, mean_q: 19.415176
 42900/100000: episode: 429, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: 53.450, mean reward: 0.534 [0.244, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.544, 10.098], loss: 0.013769, mae: 0.125480, mean_q: 19.561884
 43000/100000: episode: 430, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 48.113, mean reward: 0.481 [0.209, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.300, 10.098], loss: 0.013956, mae: 0.127689, mean_q: 19.703043
 43100/100000: episode: 431, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 53.014, mean reward: 0.530 [0.288, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.618, 10.297], loss: 0.016172, mae: 0.136945, mean_q: 19.584003
 43200/100000: episode: 432, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 53.647, mean reward: 0.536 [0.352, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.266, 10.098], loss: 0.015436, mae: 0.133263, mean_q: 19.433704
 43300/100000: episode: 433, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 49.707, mean reward: 0.497 [0.184, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.407, 10.098], loss: 0.013358, mae: 0.125974, mean_q: 19.440865
 43400/100000: episode: 434, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 52.123, mean reward: 0.521 [0.307, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.406, 10.159], loss: 0.014390, mae: 0.129701, mean_q: 19.657385
 43500/100000: episode: 435, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 46.973, mean reward: 0.470 [0.233, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.084, 10.141], loss: 0.015788, mae: 0.136567, mean_q: 19.707483
 43600/100000: episode: 436, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 47.713, mean reward: 0.477 [0.259, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.165, 10.098], loss: 0.015145, mae: 0.133713, mean_q: 19.501732
 43700/100000: episode: 437, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 53.378, mean reward: 0.534 [0.314, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.887, 10.098], loss: 0.015261, mae: 0.135219, mean_q: 19.769873
 43800/100000: episode: 438, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: 48.316, mean reward: 0.483 [0.281, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.169, 10.195], loss: 0.016497, mae: 0.138755, mean_q: 19.755177
 43900/100000: episode: 439, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 53.697, mean reward: 0.537 [0.354, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.300, 10.098], loss: 0.015315, mae: 0.133156, mean_q: 19.867424
 44000/100000: episode: 440, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 48.259, mean reward: 0.483 [0.220, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.686, 10.098], loss: 0.014447, mae: 0.130091, mean_q: 19.689743
 44100/100000: episode: 441, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 48.446, mean reward: 0.484 [0.248, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.852, 10.386], loss: 0.015620, mae: 0.137163, mean_q: 19.706272
 44200/100000: episode: 442, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 52.409, mean reward: 0.524 [0.307, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.970, 10.224], loss: 0.015113, mae: 0.136519, mean_q: 19.463818
 44300/100000: episode: 443, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: 53.695, mean reward: 0.537 [0.296, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.901, 10.098], loss: 0.013626, mae: 0.127663, mean_q: 19.292818
 44400/100000: episode: 444, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 52.171, mean reward: 0.522 [0.321, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.807, 10.116], loss: 0.016133, mae: 0.141049, mean_q: 19.601509
 44500/100000: episode: 445, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 53.145, mean reward: 0.531 [0.282, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.761, 10.098], loss: 0.013892, mae: 0.129158, mean_q: 19.588028
 44600/100000: episode: 446, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 43.567, mean reward: 0.436 [0.099, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.114, 10.357], loss: 0.015594, mae: 0.137085, mean_q: 19.572210
 44700/100000: episode: 447, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 54.665, mean reward: 0.547 [0.317, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.300, 10.098], loss: 0.015974, mae: 0.139778, mean_q: 19.517868
 44800/100000: episode: 448, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 52.978, mean reward: 0.530 [0.336, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.503, 10.098], loss: 0.014206, mae: 0.133179, mean_q: 19.673695
 44900/100000: episode: 449, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 54.093, mean reward: 0.541 [0.304, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.565, 10.211], loss: 0.016509, mae: 0.142408, mean_q: 19.637966
 45000/100000: episode: 450, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 52.503, mean reward: 0.525 [0.309, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.832, 10.098], loss: 0.014189, mae: 0.131343, mean_q: 19.480145
 45100/100000: episode: 451, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 49.212, mean reward: 0.492 [0.240, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.071, 10.098], loss: 0.013780, mae: 0.129502, mean_q: 19.180628
 45200/100000: episode: 452, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 57.398, mean reward: 0.574 [0.412, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.414, 10.140], loss: 0.014608, mae: 0.131455, mean_q: 19.363424
 45300/100000: episode: 453, duration: 0.612s, episode steps: 100, steps per second: 164, episode reward: 52.072, mean reward: 0.521 [0.263, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.706, 10.126], loss: 0.015837, mae: 0.139685, mean_q: 19.588943
 45400/100000: episode: 454, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: 53.646, mean reward: 0.536 [0.277, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.600, 10.115], loss: 0.015600, mae: 0.138689, mean_q: 19.767689
 45500/100000: episode: 455, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 53.309, mean reward: 0.533 [0.219, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.979, 10.167], loss: 0.017966, mae: 0.149581, mean_q: 19.425798
 45600/100000: episode: 456, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 49.890, mean reward: 0.499 [0.206, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.499, 10.098], loss: 0.014416, mae: 0.132391, mean_q: 19.396864
 45700/100000: episode: 457, duration: 0.660s, episode steps: 100, steps per second: 152, episode reward: 46.253, mean reward: 0.463 [0.331, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.668, 10.098], loss: 0.016536, mae: 0.140944, mean_q: 19.466906
 45800/100000: episode: 458, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 55.608, mean reward: 0.556 [0.276, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.407, 10.098], loss: 0.017575, mae: 0.146226, mean_q: 19.688629
 45900/100000: episode: 459, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: 54.461, mean reward: 0.545 [0.244, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.226, 10.146], loss: 0.014628, mae: 0.133899, mean_q: 19.156103
 46000/100000: episode: 460, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 54.154, mean reward: 0.542 [0.252, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.515, 10.116], loss: 0.014616, mae: 0.133033, mean_q: 19.623554
 46100/100000: episode: 461, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 52.532, mean reward: 0.525 [0.202, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.953, 10.098], loss: 0.016723, mae: 0.144675, mean_q: 19.296328
 46200/100000: episode: 462, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 46.857, mean reward: 0.469 [0.251, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.507, 10.098], loss: 0.016885, mae: 0.144588, mean_q: 19.312477
 46300/100000: episode: 463, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 54.520, mean reward: 0.545 [0.365, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.417, 10.098], loss: 0.015247, mae: 0.136629, mean_q: 19.597425
 46400/100000: episode: 464, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: 54.240, mean reward: 0.542 [0.286, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.264, 10.098], loss: 0.017593, mae: 0.148265, mean_q: 19.670691
 46500/100000: episode: 465, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 57.239, mean reward: 0.572 [0.303, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.408, 10.172], loss: 0.017086, mae: 0.144691, mean_q: 19.528135
 46600/100000: episode: 466, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 54.926, mean reward: 0.549 [0.300, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.669, 10.098], loss: 0.014787, mae: 0.134846, mean_q: 19.524864
 46700/100000: episode: 467, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 54.419, mean reward: 0.544 [0.327, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.771, 10.209], loss: 0.016066, mae: 0.140298, mean_q: 19.561939
 46800/100000: episode: 468, duration: 1.052s, episode steps: 100, steps per second: 95, episode reward: 57.435, mean reward: 0.574 [0.344, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.610, 10.191], loss: 0.015951, mae: 0.141129, mean_q: 19.368486
 46900/100000: episode: 469, duration: 0.965s, episode steps: 100, steps per second: 104, episode reward: 55.090, mean reward: 0.551 [0.336, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.135, 10.271], loss: 0.017738, mae: 0.146663, mean_q: 19.425264
 47000/100000: episode: 470, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 55.546, mean reward: 0.555 [0.298, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.990, 10.159], loss: 0.015514, mae: 0.137426, mean_q: 19.418861
 47100/100000: episode: 471, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: 56.945, mean reward: 0.569 [0.423, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.304, 10.152], loss: 0.015096, mae: 0.136550, mean_q: 19.246876
 47200/100000: episode: 472, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 52.580, mean reward: 0.526 [0.265, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.364, 10.214], loss: 0.014742, mae: 0.134491, mean_q: 19.032085
 47300/100000: episode: 473, duration: 0.735s, episode steps: 100, steps per second: 136, episode reward: 42.876, mean reward: 0.429 [0.176, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.750, 10.098], loss: 0.014046, mae: 0.131534, mean_q: 19.544703
 47400/100000: episode: 474, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 51.350, mean reward: 0.514 [0.303, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.481, 10.098], loss: 0.015382, mae: 0.137549, mean_q: 19.144281
 47500/100000: episode: 475, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 52.850, mean reward: 0.529 [0.295, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.135, 10.157], loss: 0.016566, mae: 0.142643, mean_q: 19.358179
 47600/100000: episode: 476, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 53.542, mean reward: 0.535 [0.226, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.755, 10.098], loss: 0.016266, mae: 0.140331, mean_q: 19.280907
 47700/100000: episode: 477, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 54.538, mean reward: 0.545 [0.355, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.412, 10.142], loss: 0.017271, mae: 0.145485, mean_q: 19.696325
 47800/100000: episode: 478, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 56.274, mean reward: 0.563 [0.383, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.993, 10.207], loss: 0.014334, mae: 0.132389, mean_q: 19.358776
 47900/100000: episode: 479, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 54.597, mean reward: 0.546 [0.304, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.545, 10.335], loss: 0.018106, mae: 0.150590, mean_q: 19.624100
 48000/100000: episode: 480, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 53.257, mean reward: 0.533 [0.182, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.776, 10.174], loss: 0.016880, mae: 0.143524, mean_q: 19.352934
 48100/100000: episode: 481, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 51.976, mean reward: 0.520 [0.262, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.101, 10.134], loss: 0.016048, mae: 0.139903, mean_q: 19.424791
 48200/100000: episode: 482, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 55.466, mean reward: 0.555 [0.208, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.649, 10.123], loss: 0.019228, mae: 0.156247, mean_q: 19.607878
 48300/100000: episode: 483, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 54.624, mean reward: 0.546 [0.272, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.744, 10.098], loss: 0.015943, mae: 0.140281, mean_q: 19.591946
 48400/100000: episode: 484, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: 54.350, mean reward: 0.544 [0.269, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.854, 10.136], loss: 0.016530, mae: 0.140627, mean_q: 19.257082
 48500/100000: episode: 485, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 52.348, mean reward: 0.523 [0.317, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.439, 10.162], loss: 0.015835, mae: 0.139613, mean_q: 19.317415
 48600/100000: episode: 486, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 51.608, mean reward: 0.516 [0.300, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.154, 10.263], loss: 0.015536, mae: 0.137855, mean_q: 19.427864
 48700/100000: episode: 487, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 47.287, mean reward: 0.473 [0.075, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.808, 10.381], loss: 0.015222, mae: 0.137235, mean_q: 19.520281
 48800/100000: episode: 488, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 54.331, mean reward: 0.543 [0.332, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.977, 10.098], loss: 0.015407, mae: 0.138078, mean_q: 19.413387
 48900/100000: episode: 489, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 54.356, mean reward: 0.544 [0.209, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.408, 10.203], loss: 0.019851, mae: 0.157862, mean_q: 19.369642
 49000/100000: episode: 490, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 51.853, mean reward: 0.519 [0.345, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.593, 10.098], loss: 0.017520, mae: 0.146690, mean_q: 19.601387
 49100/100000: episode: 491, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 53.048, mean reward: 0.530 [0.288, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.043, 10.141], loss: 0.018474, mae: 0.150412, mean_q: 19.190237
 49200/100000: episode: 492, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 48.357, mean reward: 0.484 [0.169, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.228, 10.346], loss: 0.015810, mae: 0.139929, mean_q: 19.545980
 49300/100000: episode: 493, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 55.921, mean reward: 0.559 [0.365, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.357, 10.098], loss: 0.015149, mae: 0.136662, mean_q: 19.465870
 49400/100000: episode: 494, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 51.941, mean reward: 0.519 [0.251, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.613, 10.098], loss: 0.015940, mae: 0.140070, mean_q: 19.422886
 49500/100000: episode: 495, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 51.752, mean reward: 0.518 [0.214, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.731, 10.113], loss: 0.020238, mae: 0.159392, mean_q: 19.277901
 49600/100000: episode: 496, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: 53.749, mean reward: 0.537 [0.316, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.280, 10.098], loss: 0.016725, mae: 0.142393, mean_q: 19.313515
 49700/100000: episode: 497, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 51.007, mean reward: 0.510 [0.264, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.962, 10.163], loss: 0.014712, mae: 0.132668, mean_q: 19.432108
 49800/100000: episode: 498, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: 52.622, mean reward: 0.526 [0.295, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.926, 10.239], loss: 0.016555, mae: 0.142864, mean_q: 19.845446
 49900/100000: episode: 499, duration: 1.185s, episode steps: 100, steps per second: 84, episode reward: 53.729, mean reward: 0.537 [0.322, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.365, 10.098], loss: 0.015934, mae: 0.140762, mean_q: 19.545910
 50000/100000: episode: 500, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: 52.828, mean reward: 0.528 [0.162, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.503, 10.098], loss: 0.019883, mae: 0.157222, mean_q: 19.582220
 50100/100000: episode: 501, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 51.585, mean reward: 0.516 [0.329, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.800, 10.216], loss: 0.016343, mae: 0.142087, mean_q: 19.525623
 50200/100000: episode: 502, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 55.242, mean reward: 0.552 [0.319, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.292, 10.344], loss: 0.014803, mae: 0.135052, mean_q: 19.537848
 50300/100000: episode: 503, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 53.615, mean reward: 0.536 [0.301, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.725, 10.098], loss: 0.015449, mae: 0.137161, mean_q: 19.266817
 50400/100000: episode: 504, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 56.899, mean reward: 0.569 [0.340, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.515, 10.227], loss: 0.015056, mae: 0.135601, mean_q: 19.199944
 50500/100000: episode: 505, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 52.996, mean reward: 0.530 [0.266, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.110, 10.098], loss: 0.016901, mae: 0.144267, mean_q: 19.359577
 50600/100000: episode: 506, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 54.952, mean reward: 0.550 [0.308, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.687, 10.121], loss: 0.013344, mae: 0.128279, mean_q: 19.427078
 50700/100000: episode: 507, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 57.124, mean reward: 0.571 [0.354, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.493, 10.155], loss: 0.014544, mae: 0.134150, mean_q: 19.248184
 50800/100000: episode: 508, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 51.693, mean reward: 0.517 [0.304, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.836, 10.272], loss: 0.013581, mae: 0.129253, mean_q: 19.410761
 50900/100000: episode: 509, duration: 0.657s, episode steps: 100, steps per second: 152, episode reward: 51.968, mean reward: 0.520 [0.295, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.869, 10.196], loss: 0.017814, mae: 0.148022, mean_q: 19.594339
 51000/100000: episode: 510, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 51.698, mean reward: 0.517 [0.147, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.533, 10.269], loss: 0.014745, mae: 0.134653, mean_q: 19.322691
 51100/100000: episode: 511, duration: 0.643s, episode steps: 100, steps per second: 155, episode reward: 51.266, mean reward: 0.513 [0.304, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.755, 10.098], loss: 0.013821, mae: 0.129485, mean_q: 19.413820
 51200/100000: episode: 512, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: 53.320, mean reward: 0.533 [0.292, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.863, 10.120], loss: 0.014289, mae: 0.131678, mean_q: 19.276476
 51300/100000: episode: 513, duration: 0.722s, episode steps: 100, steps per second: 139, episode reward: 56.875, mean reward: 0.569 [0.395, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.807, 10.098], loss: 0.014513, mae: 0.131938, mean_q: 19.746191
 51400/100000: episode: 514, duration: 1.035s, episode steps: 100, steps per second: 97, episode reward: 57.018, mean reward: 0.570 [0.315, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.125, 10.111], loss: 0.017862, mae: 0.147345, mean_q: 19.434933
 51500/100000: episode: 515, duration: 1.146s, episode steps: 100, steps per second: 87, episode reward: 47.745, mean reward: 0.477 [0.144, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.299, 10.098], loss: 0.013390, mae: 0.127745, mean_q: 19.268650
 51600/100000: episode: 516, duration: 1.007s, episode steps: 100, steps per second: 99, episode reward: 47.560, mean reward: 0.476 [0.239, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.411, 10.098], loss: 0.016595, mae: 0.143153, mean_q: 19.766006
 51700/100000: episode: 517, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 55.050, mean reward: 0.551 [0.394, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.553, 10.329], loss: 0.015860, mae: 0.140369, mean_q: 19.608322
 51800/100000: episode: 518, duration: 0.843s, episode steps: 100, steps per second: 119, episode reward: 49.766, mean reward: 0.498 [0.291, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.308, 10.098], loss: 0.017514, mae: 0.146171, mean_q: 19.479172
 51900/100000: episode: 519, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 55.634, mean reward: 0.556 [0.355, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.756, 10.195], loss: 0.014364, mae: 0.133989, mean_q: 19.398146
 52000/100000: episode: 520, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 51.227, mean reward: 0.512 [0.309, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.955, 10.098], loss: 0.015613, mae: 0.138648, mean_q: 19.369713
 52100/100000: episode: 521, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 55.542, mean reward: 0.555 [0.300, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.184, 10.098], loss: 0.015989, mae: 0.141448, mean_q: 19.414246
 52200/100000: episode: 522, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: 47.530, mean reward: 0.475 [0.285, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.933, 10.098], loss: 0.015046, mae: 0.135537, mean_q: 19.358978
 52300/100000: episode: 523, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: 55.144, mean reward: 0.551 [0.336, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.793, 10.098], loss: 0.014581, mae: 0.133337, mean_q: 19.648390
 52400/100000: episode: 524, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 50.076, mean reward: 0.501 [0.221, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.315, 10.254], loss: 0.013873, mae: 0.129293, mean_q: 19.434841
 52500/100000: episode: 525, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 55.901, mean reward: 0.559 [0.375, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.431, 10.098], loss: 0.015149, mae: 0.136704, mean_q: 19.569002
 52600/100000: episode: 526, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 54.627, mean reward: 0.546 [0.299, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.578, 10.193], loss: 0.019653, mae: 0.155002, mean_q: 19.246096
 52700/100000: episode: 527, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 53.795, mean reward: 0.538 [0.195, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.739, 10.197], loss: 0.015645, mae: 0.138702, mean_q: 19.240681
 52800/100000: episode: 528, duration: 0.772s, episode steps: 100, steps per second: 129, episode reward: 47.793, mean reward: 0.478 [0.276, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.096, 10.492], loss: 0.014208, mae: 0.131922, mean_q: 19.735149
 52900/100000: episode: 529, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 53.484, mean reward: 0.535 [0.312, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.800, 10.166], loss: 0.014659, mae: 0.133096, mean_q: 19.660791
 53000/100000: episode: 530, duration: 1.050s, episode steps: 100, steps per second: 95, episode reward: 53.776, mean reward: 0.538 [0.360, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.016, 10.098], loss: 0.020013, mae: 0.156785, mean_q: 19.570734
 53100/100000: episode: 531, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 54.972, mean reward: 0.550 [0.279, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.203, 10.188], loss: 0.018306, mae: 0.150521, mean_q: 19.305840
 53200/100000: episode: 532, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 55.770, mean reward: 0.558 [0.370, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.487, 10.098], loss: 0.017434, mae: 0.147023, mean_q: 19.595102
 53300/100000: episode: 533, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 55.547, mean reward: 0.555 [0.226, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.993, 10.253], loss: 0.014547, mae: 0.132667, mean_q: 19.597990
 53400/100000: episode: 534, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 54.892, mean reward: 0.549 [0.376, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.097, 10.217], loss: 0.015732, mae: 0.139419, mean_q: 19.722269
 53500/100000: episode: 535, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: 56.238, mean reward: 0.562 [0.302, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.428, 10.181], loss: 0.014117, mae: 0.130246, mean_q: 19.709457
 53600/100000: episode: 536, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 52.394, mean reward: 0.524 [0.266, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.098], loss: 0.015384, mae: 0.137111, mean_q: 19.632299
 53700/100000: episode: 537, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 48.130, mean reward: 0.481 [0.164, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.448, 10.164], loss: 0.015579, mae: 0.137136, mean_q: 19.350063
 53800/100000: episode: 538, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 52.348, mean reward: 0.523 [0.326, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.796, 10.315], loss: 0.013769, mae: 0.130312, mean_q: 19.874195
 53900/100000: episode: 539, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 51.786, mean reward: 0.518 [0.317, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.154, 10.412], loss: 0.017476, mae: 0.145690, mean_q: 19.578327
 54000/100000: episode: 540, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 50.792, mean reward: 0.508 [0.174, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.629, 10.356], loss: 0.013924, mae: 0.129787, mean_q: 19.085091
 54100/100000: episode: 541, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 53.479, mean reward: 0.535 [0.243, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.434, 10.312], loss: 0.014426, mae: 0.130958, mean_q: 19.503567
 54200/100000: episode: 542, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 51.981, mean reward: 0.520 [0.314, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.012, 10.173], loss: 0.015873, mae: 0.138531, mean_q: 19.819740
 54300/100000: episode: 543, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 53.291, mean reward: 0.533 [0.319, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.464, 10.119], loss: 0.015220, mae: 0.137418, mean_q: 19.741186
 54400/100000: episode: 544, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 53.805, mean reward: 0.538 [0.228, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.427, 10.273], loss: 0.013727, mae: 0.129616, mean_q: 19.746269
 54500/100000: episode: 545, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 57.206, mean reward: 0.572 [0.408, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.685, 10.278], loss: 0.017441, mae: 0.147518, mean_q: 19.266516
 54600/100000: episode: 546, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 52.775, mean reward: 0.528 [0.334, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.797, 10.098], loss: 0.016616, mae: 0.141801, mean_q: 19.679588
 54700/100000: episode: 547, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 55.419, mean reward: 0.554 [0.364, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.612, 10.098], loss: 0.015868, mae: 0.137984, mean_q: 19.630980
 54800/100000: episode: 548, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 55.817, mean reward: 0.558 [0.262, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.587, 10.156], loss: 0.013532, mae: 0.128622, mean_q: 19.477692
 54900/100000: episode: 549, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 51.712, mean reward: 0.517 [0.286, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.550, 10.407], loss: 0.014256, mae: 0.130956, mean_q: 19.863184
 55000/100000: episode: 550, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 54.061, mean reward: 0.541 [0.302, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.288, 10.098], loss: 0.013858, mae: 0.130830, mean_q: 19.485157
 55100/100000: episode: 551, duration: 0.722s, episode steps: 100, steps per second: 139, episode reward: 51.444, mean reward: 0.514 [0.252, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.413, 10.133], loss: 0.018039, mae: 0.149900, mean_q: 19.598354
 55200/100000: episode: 552, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: 52.782, mean reward: 0.528 [0.258, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.726, 10.361], loss: 0.012810, mae: 0.125572, mean_q: 19.868700
 55300/100000: episode: 553, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 53.694, mean reward: 0.537 [0.246, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.701, 10.098], loss: 0.014570, mae: 0.134243, mean_q: 19.596386
 55400/100000: episode: 554, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 57.718, mean reward: 0.577 [0.377, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.011, 10.274], loss: 0.014645, mae: 0.133749, mean_q: 19.733711
 55500/100000: episode: 555, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 55.921, mean reward: 0.559 [0.310, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.620, 10.143], loss: 0.016835, mae: 0.142917, mean_q: 19.489212
 55600/100000: episode: 556, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 55.409, mean reward: 0.554 [0.375, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.582, 10.110], loss: 0.015050, mae: 0.135429, mean_q: 19.305719
 55700/100000: episode: 557, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 55.494, mean reward: 0.555 [0.267, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.806, 10.122], loss: 0.014753, mae: 0.132054, mean_q: 19.687857
 55800/100000: episode: 558, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 55.013, mean reward: 0.550 [0.297, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.278, 10.132], loss: 0.014780, mae: 0.134626, mean_q: 19.727072
 55900/100000: episode: 559, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 50.499, mean reward: 0.505 [0.257, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.845, 10.295], loss: 0.013039, mae: 0.125422, mean_q: 19.319405
 56000/100000: episode: 560, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 51.246, mean reward: 0.512 [0.358, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.409, 10.098], loss: 0.015662, mae: 0.138561, mean_q: 19.716394
 56100/100000: episode: 561, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 54.914, mean reward: 0.549 [0.242, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.261, 10.098], loss: 0.013662, mae: 0.129430, mean_q: 19.443762
 56200/100000: episode: 562, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 55.558, mean reward: 0.556 [0.296, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.835, 10.098], loss: 0.014393, mae: 0.134137, mean_q: 19.390629
 56300/100000: episode: 563, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 55.975, mean reward: 0.560 [0.316, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.492, 10.098], loss: 0.013385, mae: 0.128931, mean_q: 19.589867
 56400/100000: episode: 564, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 54.081, mean reward: 0.541 [0.331, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.718, 10.116], loss: 0.013641, mae: 0.129593, mean_q: 19.384243
 56500/100000: episode: 565, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 56.353, mean reward: 0.564 [0.274, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.375, 10.098], loss: 0.013826, mae: 0.129693, mean_q: 19.526670
 56600/100000: episode: 566, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 55.932, mean reward: 0.559 [0.368, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.208, 10.098], loss: 0.012798, mae: 0.124981, mean_q: 19.368793
 56700/100000: episode: 567, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 56.885, mean reward: 0.569 [0.219, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.006, 10.098], loss: 0.014717, mae: 0.134464, mean_q: 19.341053
 56800/100000: episode: 568, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 53.651, mean reward: 0.537 [0.256, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.539, 10.098], loss: 0.017717, mae: 0.146826, mean_q: 19.469049
 56900/100000: episode: 569, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 49.387, mean reward: 0.494 [0.330, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.451, 10.098], loss: 0.012202, mae: 0.122425, mean_q: 19.719673
 57000/100000: episode: 570, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 53.754, mean reward: 0.538 [0.298, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.930, 10.098], loss: 0.014035, mae: 0.130256, mean_q: 19.885773
 57100/100000: episode: 571, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 55.283, mean reward: 0.553 [0.356, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.264, 10.134], loss: 0.012584, mae: 0.124029, mean_q: 19.745222
 57200/100000: episode: 572, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 55.679, mean reward: 0.557 [0.283, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.182, 10.274], loss: 0.012533, mae: 0.123171, mean_q: 19.679220
 57300/100000: episode: 573, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 46.295, mean reward: 0.463 [0.244, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.244, 10.098], loss: 0.014351, mae: 0.132800, mean_q: 19.453724
 57400/100000: episode: 574, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 52.402, mean reward: 0.524 [0.283, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.513, 10.198], loss: 0.013483, mae: 0.127287, mean_q: 19.529537
 57500/100000: episode: 575, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 55.830, mean reward: 0.558 [0.351, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.248, 10.144], loss: 0.013844, mae: 0.129030, mean_q: 19.566084
 57600/100000: episode: 576, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 52.265, mean reward: 0.523 [0.356, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.260, 10.209], loss: 0.014007, mae: 0.131250, mean_q: 19.907593
 57700/100000: episode: 577, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 57.681, mean reward: 0.577 [0.329, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.590, 10.280], loss: 0.014036, mae: 0.131281, mean_q: 20.051546
 57800/100000: episode: 578, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 56.733, mean reward: 0.567 [0.254, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.432, 10.098], loss: 0.012967, mae: 0.126402, mean_q: 19.943178
 57900/100000: episode: 579, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 54.102, mean reward: 0.541 [0.313, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.761, 10.098], loss: 0.013774, mae: 0.128044, mean_q: 19.557467
 58000/100000: episode: 580, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 52.175, mean reward: 0.522 [0.114, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.381, 10.098], loss: 0.014998, mae: 0.134423, mean_q: 19.857355
 58100/100000: episode: 581, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 54.761, mean reward: 0.548 [0.303, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.369, 10.098], loss: 0.013375, mae: 0.127423, mean_q: 19.717033
 58200/100000: episode: 582, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 54.238, mean reward: 0.542 [0.146, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.305, 10.098], loss: 0.013425, mae: 0.128239, mean_q: 19.439678
 58300/100000: episode: 583, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 53.901, mean reward: 0.539 [0.283, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.872, 10.325], loss: 0.013618, mae: 0.129324, mean_q: 20.101416
 58400/100000: episode: 584, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 49.921, mean reward: 0.499 [0.297, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.053, 10.396], loss: 0.013802, mae: 0.130242, mean_q: 19.872631
 58500/100000: episode: 585, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 54.682, mean reward: 0.547 [0.287, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.525, 10.156], loss: 0.014145, mae: 0.131563, mean_q: 19.862331
 58600/100000: episode: 586, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.220, mean reward: 0.532 [0.231, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.466, 10.494], loss: 0.013934, mae: 0.131229, mean_q: 19.872913
 58700/100000: episode: 587, duration: 0.601s, episode steps: 100, steps per second: 167, episode reward: 56.556, mean reward: 0.566 [0.312, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.829, 10.269], loss: 0.014559, mae: 0.133915, mean_q: 19.543900
 58800/100000: episode: 588, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 49.721, mean reward: 0.497 [0.185, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.662, 10.334], loss: 0.013544, mae: 0.128480, mean_q: 19.824030
 58900/100000: episode: 589, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 53.962, mean reward: 0.540 [0.317, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.678, 10.219], loss: 0.013526, mae: 0.129929, mean_q: 19.608328
 59000/100000: episode: 590, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 51.517, mean reward: 0.515 [0.284, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.809, 10.098], loss: 0.014691, mae: 0.134774, mean_q: 19.663946
 59100/100000: episode: 591, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 50.757, mean reward: 0.508 [0.203, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.346, 10.098], loss: 0.014997, mae: 0.135761, mean_q: 19.858339
 59200/100000: episode: 592, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 52.551, mean reward: 0.526 [0.269, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.334, 10.098], loss: 0.014026, mae: 0.131968, mean_q: 19.472929
 59300/100000: episode: 593, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 51.492, mean reward: 0.515 [0.162, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.687, 10.098], loss: 0.014429, mae: 0.133723, mean_q: 19.836525
 59400/100000: episode: 594, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 55.628, mean reward: 0.556 [0.194, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.636, 10.133], loss: 0.014479, mae: 0.133338, mean_q: 19.709314
 59500/100000: episode: 595, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 46.769, mean reward: 0.468 [0.226, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.569, 10.098], loss: 0.018168, mae: 0.150201, mean_q: 19.804173
 59600/100000: episode: 596, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 50.870, mean reward: 0.509 [0.258, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.364, 10.274], loss: 0.014098, mae: 0.133210, mean_q: 19.632555
 59700/100000: episode: 597, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 53.926, mean reward: 0.539 [0.336, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.729, 10.250], loss: 0.014212, mae: 0.132106, mean_q: 19.752617
 59800/100000: episode: 598, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 52.110, mean reward: 0.521 [0.283, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.190, 10.098], loss: 0.014505, mae: 0.134596, mean_q: 19.684578
 59900/100000: episode: 599, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 52.932, mean reward: 0.529 [0.134, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.162, 10.240], loss: 0.016253, mae: 0.141944, mean_q: 19.934456
 60000/100000: episode: 600, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 56.335, mean reward: 0.563 [0.332, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.827, 10.098], loss: 0.014931, mae: 0.137461, mean_q: 19.970095
 60100/100000: episode: 601, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 52.671, mean reward: 0.527 [0.293, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.460, 10.244], loss: 0.014841, mae: 0.136340, mean_q: 19.633732
 60200/100000: episode: 602, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 52.319, mean reward: 0.523 [0.197, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.962, 10.098], loss: 0.014209, mae: 0.133430, mean_q: 19.896172
 60300/100000: episode: 603, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 54.511, mean reward: 0.545 [0.241, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.633, 10.098], loss: 0.015637, mae: 0.138742, mean_q: 19.939346
 60400/100000: episode: 604, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 53.837, mean reward: 0.538 [0.341, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.621, 10.098], loss: 0.013044, mae: 0.125913, mean_q: 20.027052
 60500/100000: episode: 605, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 54.377, mean reward: 0.544 [0.350, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.883, 10.098], loss: 0.013583, mae: 0.130263, mean_q: 19.827583
 60600/100000: episode: 606, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 55.332, mean reward: 0.553 [0.302, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.092, 10.098], loss: 0.014939, mae: 0.136175, mean_q: 19.860754
 60700/100000: episode: 607, duration: 0.660s, episode steps: 100, steps per second: 151, episode reward: 51.954, mean reward: 0.520 [0.304, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.485, 10.208], loss: 0.017255, mae: 0.144447, mean_q: 19.846766
 60800/100000: episode: 608, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 50.432, mean reward: 0.504 [0.155, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.951, 10.098], loss: 0.014698, mae: 0.133139, mean_q: 19.816399
 60900/100000: episode: 609, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 54.051, mean reward: 0.541 [0.214, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.096, 10.098], loss: 0.015740, mae: 0.137511, mean_q: 19.785662
 61000/100000: episode: 610, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 56.230, mean reward: 0.562 [0.326, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.922, 10.098], loss: 0.013314, mae: 0.126945, mean_q: 19.804945
 61100/100000: episode: 611, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 50.614, mean reward: 0.506 [0.229, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.327, 10.098], loss: 0.013458, mae: 0.128080, mean_q: 19.632395
 61200/100000: episode: 612, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 46.463, mean reward: 0.465 [0.193, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.997, 10.098], loss: 0.013231, mae: 0.126254, mean_q: 19.760767
 61300/100000: episode: 613, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 49.538, mean reward: 0.495 [0.202, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.470, 10.098], loss: 0.015643, mae: 0.139618, mean_q: 19.674952
 61400/100000: episode: 614, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 54.924, mean reward: 0.549 [0.328, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.241, 10.187], loss: 0.014606, mae: 0.132816, mean_q: 19.979921
 61500/100000: episode: 615, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: 53.619, mean reward: 0.536 [0.336, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.835, 10.098], loss: 0.013837, mae: 0.129821, mean_q: 19.861397
 61600/100000: episode: 616, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 53.221, mean reward: 0.532 [0.256, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.055, 10.098], loss: 0.013937, mae: 0.130724, mean_q: 19.894003
 61700/100000: episode: 617, duration: 0.601s, episode steps: 100, steps per second: 167, episode reward: 52.724, mean reward: 0.527 [0.337, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.008, 10.315], loss: 0.017217, mae: 0.143939, mean_q: 19.821909
 61800/100000: episode: 618, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 48.846, mean reward: 0.488 [0.186, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.513, 10.098], loss: 0.014998, mae: 0.134495, mean_q: 19.899548
 61900/100000: episode: 619, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 54.637, mean reward: 0.546 [0.325, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.483, 10.098], loss: 0.017645, mae: 0.148355, mean_q: 19.881111
 62000/100000: episode: 620, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 56.979, mean reward: 0.570 [0.353, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.202, 10.098], loss: 0.016120, mae: 0.140255, mean_q: 19.740498
 62100/100000: episode: 621, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 52.891, mean reward: 0.529 [0.192, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.675, 10.098], loss: 0.013894, mae: 0.129327, mean_q: 19.783354
 62200/100000: episode: 622, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 55.289, mean reward: 0.553 [0.334, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.179, 10.261], loss: 0.012682, mae: 0.123409, mean_q: 19.954805
 62300/100000: episode: 623, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 55.401, mean reward: 0.554 [0.318, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.838, 10.124], loss: 0.013699, mae: 0.128302, mean_q: 19.708195
 62400/100000: episode: 624, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 52.950, mean reward: 0.530 [0.329, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.458, 10.285], loss: 0.015565, mae: 0.137564, mean_q: 19.729210
[RESULT] FALSIFICATION!
 62459/100000: episode: 625, duration: 0.350s, episode steps: 59, steps per second: 168, episode reward: 22.697, mean reward: 0.385 [-10.000, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.053 [-0.656, 6.319], loss: 0.013628, mae: 0.131071, mean_q: 19.633062
 62559/100000: episode: 626, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 52.372, mean reward: 0.524 [0.341, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.883, 10.098], loss: 0.013092, mae: 0.125258, mean_q: 19.960911
 62659/100000: episode: 627, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 53.125, mean reward: 0.531 [0.242, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.735, 10.098], loss: 0.164763, mae: 0.204545, mean_q: 19.980036
 62759/100000: episode: 628, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 55.417, mean reward: 0.554 [0.368, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.495, 10.160], loss: 0.326321, mae: 0.271078, mean_q: 19.928753
 62859/100000: episode: 629, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 54.729, mean reward: 0.547 [0.297, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.725, 10.098], loss: 0.277966, mae: 0.186897, mean_q: 19.850840
 62959/100000: episode: 630, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 53.231, mean reward: 0.532 [0.273, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.842, 10.214], loss: 0.320406, mae: 0.304318, mean_q: 19.699949
 63059/100000: episode: 631, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 51.855, mean reward: 0.519 [0.336, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.509, 10.143], loss: 0.145152, mae: 0.166896, mean_q: 19.636608
 63159/100000: episode: 632, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 49.116, mean reward: 0.491 [0.263, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.569, 10.443], loss: 0.146809, mae: 0.168316, mean_q: 19.906919
 63259/100000: episode: 633, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 54.094, mean reward: 0.541 [0.298, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.315, 10.098], loss: 0.145045, mae: 0.164170, mean_q: 19.637854
 63359/100000: episode: 634, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 52.523, mean reward: 0.525 [0.282, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.702, 10.227], loss: 0.149062, mae: 0.181483, mean_q: 20.083994
 63459/100000: episode: 635, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 48.898, mean reward: 0.489 [0.245, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.587, 10.098], loss: 0.149963, mae: 0.176974, mean_q: 20.199093
 63559/100000: episode: 636, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 54.569, mean reward: 0.546 [0.290, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.098], loss: 0.016312, mae: 0.137758, mean_q: 19.782824
 63659/100000: episode: 637, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 56.216, mean reward: 0.562 [0.332, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.842, 10.149], loss: 0.279378, mae: 0.217566, mean_q: 19.504827
 63759/100000: episode: 638, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 54.416, mean reward: 0.544 [0.308, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.615, 10.377], loss: 0.281574, mae: 0.219804, mean_q: 19.925421
 63859/100000: episode: 639, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 53.430, mean reward: 0.534 [0.269, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.176, 10.118], loss: 0.016503, mae: 0.140271, mean_q: 19.785837
 63959/100000: episode: 640, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 52.826, mean reward: 0.528 [0.256, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.938, 10.098], loss: 0.140947, mae: 0.157337, mean_q: 19.744944
 64059/100000: episode: 641, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 53.511, mean reward: 0.535 [0.339, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.668, 10.212], loss: 0.015154, mae: 0.133710, mean_q: 19.850597
 64159/100000: episode: 642, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 53.039, mean reward: 0.530 [0.271, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.392, 10.369], loss: 0.014189, mae: 0.128842, mean_q: 19.692551
 64259/100000: episode: 643, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 54.780, mean reward: 0.548 [0.272, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.323, 10.247], loss: 0.143204, mae: 0.165671, mean_q: 19.781897
 64359/100000: episode: 644, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 48.773, mean reward: 0.488 [0.208, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.770, 10.098], loss: 0.143716, mae: 0.168703, mean_q: 19.829401
 64459/100000: episode: 645, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 56.902, mean reward: 0.569 [0.389, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.598, 10.152], loss: 0.144653, mae: 0.173643, mean_q: 19.635357
 64559/100000: episode: 646, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: 51.246, mean reward: 0.512 [0.234, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.016, 10.160], loss: 0.514877, mae: 0.232428, mean_q: 19.304527
 64659/100000: episode: 647, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 53.019, mean reward: 0.530 [0.264, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.480, 10.098], loss: 0.020634, mae: 0.154951, mean_q: 19.523920
 64759/100000: episode: 648, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 51.043, mean reward: 0.510 [0.089, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.808, 10.098], loss: 0.013331, mae: 0.129128, mean_q: 19.717649
 64859/100000: episode: 649, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 53.513, mean reward: 0.535 [0.299, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.307, 10.098], loss: 0.405873, mae: 0.260698, mean_q: 19.567911
 64959/100000: episode: 650, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 52.084, mean reward: 0.521 [0.318, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.313, 10.098], loss: 0.145353, mae: 0.181610, mean_q: 19.605301
 65059/100000: episode: 651, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 56.156, mean reward: 0.562 [0.224, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.448, 10.098], loss: 0.270334, mae: 0.204417, mean_q: 19.450516
 65159/100000: episode: 652, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 50.427, mean reward: 0.504 [0.219, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.682, 10.098], loss: 0.407031, mae: 0.275598, mean_q: 19.512182
 65259/100000: episode: 653, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 53.692, mean reward: 0.537 [0.298, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.733, 10.142], loss: 0.019551, mae: 0.154628, mean_q: 19.371880
 65359/100000: episode: 654, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 51.649, mean reward: 0.516 [0.302, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.991, 10.098], loss: 0.137759, mae: 0.155648, mean_q: 19.398355
 65459/100000: episode: 655, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 50.549, mean reward: 0.505 [0.308, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.337, 10.098], loss: 0.410092, mae: 0.288191, mean_q: 19.406744
 65559/100000: episode: 656, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 54.693, mean reward: 0.547 [0.257, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.690, 10.098], loss: 0.017351, mae: 0.143798, mean_q: 20.041309
 65659/100000: episode: 657, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 54.093, mean reward: 0.541 [0.257, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.928, 10.098], loss: 0.145014, mae: 0.183930, mean_q: 19.502523
 65759/100000: episode: 658, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 53.304, mean reward: 0.533 [0.199, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.816, 10.098], loss: 0.014649, mae: 0.133698, mean_q: 19.399786
 65859/100000: episode: 659, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 53.578, mean reward: 0.536 [0.278, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.247, 10.231], loss: 0.016557, mae: 0.139589, mean_q: 19.720634
 65959/100000: episode: 660, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 55.632, mean reward: 0.556 [0.369, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.253, 10.250], loss: 0.148192, mae: 0.187152, mean_q: 19.402067
 66059/100000: episode: 661, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 52.316, mean reward: 0.523 [0.332, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.869, 10.098], loss: 0.016933, mae: 0.141109, mean_q: 19.502853
 66159/100000: episode: 662, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 55.520, mean reward: 0.555 [0.305, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.445, 10.275], loss: 0.015951, mae: 0.137472, mean_q: 19.533920
 66259/100000: episode: 663, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: 53.669, mean reward: 0.537 [0.313, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.559, 10.098], loss: 0.015315, mae: 0.135050, mean_q: 19.242178
 66359/100000: episode: 664, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 53.671, mean reward: 0.537 [0.262, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.609, 10.140], loss: 0.149408, mae: 0.182508, mean_q: 19.332830
 66459/100000: episode: 665, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 52.916, mean reward: 0.529 [0.369, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.555, 10.098], loss: 0.527911, mae: 0.261104, mean_q: 19.624840
 66559/100000: episode: 666, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 51.988, mean reward: 0.520 [0.315, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.598, 10.098], loss: 0.024679, mae: 0.171092, mean_q: 19.381952
 66659/100000: episode: 667, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: 52.133, mean reward: 0.521 [0.282, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.139, 10.098], loss: 0.017998, mae: 0.145256, mean_q: 19.576748
 66759/100000: episode: 668, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 58.042, mean reward: 0.580 [0.297, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.655, 10.211], loss: 0.419626, mae: 0.276598, mean_q: 19.795700
 66859/100000: episode: 669, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 54.616, mean reward: 0.546 [0.349, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.221, 10.178], loss: 0.145358, mae: 0.172570, mean_q: 19.505096
 66959/100000: episode: 670, duration: 0.601s, episode steps: 100, steps per second: 167, episode reward: 56.878, mean reward: 0.569 [0.315, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.410, 10.127], loss: 0.016664, mae: 0.140052, mean_q: 19.677513
 67059/100000: episode: 671, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 55.141, mean reward: 0.551 [0.384, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.641, 10.166], loss: 0.148882, mae: 0.180890, mean_q: 19.239935
 67159/100000: episode: 672, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 50.599, mean reward: 0.506 [0.220, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.464, 10.098], loss: 0.148326, mae: 0.179876, mean_q: 19.506607
 67259/100000: episode: 673, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 55.819, mean reward: 0.558 [0.376, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.467, 10.248], loss: 0.017418, mae: 0.143932, mean_q: 19.551455
 67359/100000: episode: 674, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 52.863, mean reward: 0.529 [0.242, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.266, 10.098], loss: 0.145859, mae: 0.171951, mean_q: 19.183500
 67459/100000: episode: 675, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 52.347, mean reward: 0.523 [0.271, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.908, 10.098], loss: 0.156722, mae: 0.201897, mean_q: 19.243986
 67559/100000: episode: 676, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: 52.506, mean reward: 0.525 [0.250, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.579, 10.098], loss: 0.015628, mae: 0.135126, mean_q: 19.217989
 67659/100000: episode: 677, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 57.549, mean reward: 0.575 [0.368, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.073, 10.289], loss: 0.016299, mae: 0.135665, mean_q: 19.499611
 67759/100000: episode: 678, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 50.028, mean reward: 0.500 [0.192, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.509, 10.098], loss: 0.015285, mae: 0.133801, mean_q: 19.336344
 67859/100000: episode: 679, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 53.009, mean reward: 0.530 [0.237, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.342, 10.098], loss: 0.013874, mae: 0.127801, mean_q: 19.301601
 67959/100000: episode: 680, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 50.479, mean reward: 0.505 [0.194, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.678, 10.098], loss: 0.015432, mae: 0.134518, mean_q: 19.004715
 68059/100000: episode: 681, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 56.951, mean reward: 0.570 [0.251, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.899, 10.098], loss: 0.014441, mae: 0.130970, mean_q: 19.390902
 68159/100000: episode: 682, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 57.342, mean reward: 0.573 [0.319, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.713, 10.170], loss: 0.015217, mae: 0.132757, mean_q: 19.235323
 68259/100000: episode: 683, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 49.575, mean reward: 0.496 [0.206, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.728, 10.182], loss: 0.015461, mae: 0.134135, mean_q: 19.563879
 68359/100000: episode: 684, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 55.169, mean reward: 0.552 [0.363, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.538, 10.221], loss: 0.016038, mae: 0.136605, mean_q: 19.578608
 68459/100000: episode: 685, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 51.903, mean reward: 0.519 [0.357, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.892, 10.129], loss: 0.014437, mae: 0.130026, mean_q: 19.410023
 68559/100000: episode: 686, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 54.560, mean reward: 0.546 [0.314, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.612, 10.098], loss: 0.015399, mae: 0.134111, mean_q: 19.673790
 68659/100000: episode: 687, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 54.949, mean reward: 0.549 [0.294, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.139, 10.176], loss: 0.016124, mae: 0.135269, mean_q: 19.412928
 68759/100000: episode: 688, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 52.603, mean reward: 0.526 [0.311, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.747, 10.098], loss: 0.015761, mae: 0.138934, mean_q: 19.436539
 68859/100000: episode: 689, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 49.602, mean reward: 0.496 [0.272, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.621, 10.098], loss: 0.016692, mae: 0.142165, mean_q: 19.381432
 68959/100000: episode: 690, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 54.452, mean reward: 0.545 [0.346, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.976, 10.244], loss: 0.014801, mae: 0.133040, mean_q: 19.524570
 69059/100000: episode: 691, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 55.462, mean reward: 0.555 [0.299, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.460, 10.099], loss: 0.014563, mae: 0.131372, mean_q: 19.210716
 69159/100000: episode: 692, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 56.684, mean reward: 0.567 [0.318, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.313, 10.106], loss: 0.014581, mae: 0.132699, mean_q: 19.416149
 69259/100000: episode: 693, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 51.244, mean reward: 0.512 [0.314, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.378, 10.098], loss: 0.015366, mae: 0.134912, mean_q: 19.634514
 69359/100000: episode: 694, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 51.836, mean reward: 0.518 [0.241, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.486, 10.098], loss: 0.014948, mae: 0.133685, mean_q: 19.313982
 69459/100000: episode: 695, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 47.787, mean reward: 0.478 [0.291, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.749, 10.297], loss: 0.014253, mae: 0.129691, mean_q: 19.622353
 69559/100000: episode: 696, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 52.819, mean reward: 0.528 [0.340, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.384, 10.098], loss: 0.015897, mae: 0.137330, mean_q: 19.335478
 69659/100000: episode: 697, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 57.153, mean reward: 0.572 [0.359, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.037, 10.098], loss: 0.015818, mae: 0.137032, mean_q: 19.606421
 69759/100000: episode: 698, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 53.740, mean reward: 0.537 [0.347, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.309, 10.140], loss: 0.016673, mae: 0.140659, mean_q: 19.400135
 69859/100000: episode: 699, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 48.188, mean reward: 0.482 [0.195, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.167, 10.098], loss: 0.014190, mae: 0.131229, mean_q: 19.357544
 69959/100000: episode: 700, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 53.425, mean reward: 0.534 [0.241, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.762, 10.098], loss: 0.015352, mae: 0.137439, mean_q: 19.202990
 70059/100000: episode: 701, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 52.072, mean reward: 0.521 [0.271, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.291, 10.098], loss: 0.015549, mae: 0.136274, mean_q: 19.465025
 70159/100000: episode: 702, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 47.953, mean reward: 0.480 [0.244, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.740, 10.098], loss: 0.016100, mae: 0.135692, mean_q: 19.586908
 70259/100000: episode: 703, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 53.307, mean reward: 0.533 [0.318, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.162, 10.134], loss: 0.018313, mae: 0.146007, mean_q: 19.522015
 70359/100000: episode: 704, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 51.798, mean reward: 0.518 [0.294, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.938, 10.309], loss: 0.018255, mae: 0.147906, mean_q: 19.466524
 70459/100000: episode: 705, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 54.380, mean reward: 0.544 [0.386, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.415, 10.145], loss: 0.015679, mae: 0.135160, mean_q: 19.563128
 70559/100000: episode: 706, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 55.742, mean reward: 0.557 [0.324, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.824, 10.098], loss: 0.022126, mae: 0.161532, mean_q: 19.608585
 70659/100000: episode: 707, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.056, mean reward: 0.571 [0.375, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.460, 10.249], loss: 0.018966, mae: 0.146867, mean_q: 19.700794
 70759/100000: episode: 708, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 56.035, mean reward: 0.560 [0.378, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.875, 10.216], loss: 0.019418, mae: 0.144296, mean_q: 19.424978
 70859/100000: episode: 709, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 54.194, mean reward: 0.542 [0.264, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.179, 10.283], loss: 0.017761, mae: 0.137996, mean_q: 19.686846
 70959/100000: episode: 710, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 52.271, mean reward: 0.523 [0.291, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.779, 10.226], loss: 0.022402, mae: 0.149845, mean_q: 19.400558
 71059/100000: episode: 711, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 55.365, mean reward: 0.554 [0.328, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.997, 10.218], loss: 0.023989, mae: 0.160313, mean_q: 19.811260
 71159/100000: episode: 712, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 48.301, mean reward: 0.483 [0.257, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.330, 10.404], loss: 0.022040, mae: 0.150938, mean_q: 19.409760
 71259/100000: episode: 713, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 53.365, mean reward: 0.534 [0.225, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.847, 10.108], loss: 0.022568, mae: 0.150382, mean_q: 19.748964
 71359/100000: episode: 714, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 56.147, mean reward: 0.561 [0.336, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.182, 10.297], loss: 0.023388, mae: 0.153038, mean_q: 19.449726
 71459/100000: episode: 715, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 49.334, mean reward: 0.493 [0.141, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.036, 10.579], loss: 0.023690, mae: 0.154903, mean_q: 19.569777
 71559/100000: episode: 716, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 51.442, mean reward: 0.514 [0.266, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.834, 10.228], loss: 0.024314, mae: 0.152315, mean_q: 19.406269
 71659/100000: episode: 717, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: 56.956, mean reward: 0.570 [0.327, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.665, 10.176], loss: 0.021079, mae: 0.141529, mean_q: 19.370937
 71759/100000: episode: 718, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 55.374, mean reward: 0.554 [0.259, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.991, 10.098], loss: 0.023921, mae: 0.146322, mean_q: 19.509912
 71859/100000: episode: 719, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 54.180, mean reward: 0.542 [0.304, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.628, 10.232], loss: 0.023173, mae: 0.147357, mean_q: 19.430944
 71959/100000: episode: 720, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 53.473, mean reward: 0.535 [0.213, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.116, 10.100], loss: 0.031016, mae: 0.168230, mean_q: 19.612503
 72059/100000: episode: 721, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 52.337, mean reward: 0.523 [0.146, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.508, 10.358], loss: 0.029861, mae: 0.154716, mean_q: 19.502911
 72159/100000: episode: 722, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 57.301, mean reward: 0.573 [0.400, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.146, 10.098], loss: 0.034944, mae: 0.167857, mean_q: 19.524193
 72259/100000: episode: 723, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 48.560, mean reward: 0.486 [0.246, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.012, 10.156], loss: 0.036137, mae: 0.171655, mean_q: 19.999428
 72359/100000: episode: 724, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 50.277, mean reward: 0.503 [0.237, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.097, 10.245], loss: 0.027305, mae: 0.143141, mean_q: 19.277615
 72459/100000: episode: 725, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 51.771, mean reward: 0.518 [0.324, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.694, 10.098], loss: 0.035571, mae: 0.167465, mean_q: 19.391399
 72559/100000: episode: 726, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 53.282, mean reward: 0.533 [0.266, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.725, 10.359], loss: 0.031920, mae: 0.161131, mean_q: 19.541395
 72659/100000: episode: 727, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 54.249, mean reward: 0.542 [0.290, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.825, 10.137], loss: 0.030744, mae: 0.147887, mean_q: 19.518530
 72759/100000: episode: 728, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 55.340, mean reward: 0.553 [0.322, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.870, 10.178], loss: 0.041145, mae: 0.172043, mean_q: 19.493481
 72859/100000: episode: 729, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 54.772, mean reward: 0.548 [0.238, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.545, 10.119], loss: 0.030503, mae: 0.146385, mean_q: 19.789003
 72959/100000: episode: 730, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 55.861, mean reward: 0.559 [0.389, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.118, 10.255], loss: 0.035160, mae: 0.163525, mean_q: 19.499912
 73059/100000: episode: 731, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 54.055, mean reward: 0.541 [0.269, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.523, 10.098], loss: 0.034574, mae: 0.148260, mean_q: 19.771084
 73159/100000: episode: 732, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 53.072, mean reward: 0.531 [0.272, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.488, 10.348], loss: 0.043404, mae: 0.172342, mean_q: 19.480392
 73259/100000: episode: 733, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 56.038, mean reward: 0.560 [0.374, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.465, 10.206], loss: 0.032029, mae: 0.149997, mean_q: 19.898888
 73359/100000: episode: 734, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 55.469, mean reward: 0.555 [0.296, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.641, 10.160], loss: 0.039229, mae: 0.159929, mean_q: 19.737928
 73459/100000: episode: 735, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 54.035, mean reward: 0.540 [0.267, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.790, 10.098], loss: 0.049242, mae: 0.181415, mean_q: 19.699598
 73559/100000: episode: 736, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 53.105, mean reward: 0.531 [0.293, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.201, 10.303], loss: 0.051317, mae: 0.192004, mean_q: 19.846413
 73659/100000: episode: 737, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 50.483, mean reward: 0.505 [0.236, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.018, 10.098], loss: 0.038616, mae: 0.165884, mean_q: 19.758255
 73759/100000: episode: 738, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 52.718, mean reward: 0.527 [0.270, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.801, 10.098], loss: 0.039023, mae: 0.170703, mean_q: 19.931015
 73859/100000: episode: 739, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 52.038, mean reward: 0.520 [0.257, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.286, 10.180], loss: 0.042052, mae: 0.170680, mean_q: 19.593731
 73959/100000: episode: 740, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 54.444, mean reward: 0.544 [0.315, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.832, 10.098], loss: 0.044566, mae: 0.174263, mean_q: 19.703794
 74059/100000: episode: 741, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 54.764, mean reward: 0.548 [0.314, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.530, 10.098], loss: 0.037014, mae: 0.159057, mean_q: 19.451544
 74159/100000: episode: 742, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 54.967, mean reward: 0.550 [0.267, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.413, 10.098], loss: 0.041850, mae: 0.176853, mean_q: 19.634281
 74259/100000: episode: 743, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 53.233, mean reward: 0.532 [0.328, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.972, 10.231], loss: 0.031586, mae: 0.164951, mean_q: 19.575573
 74359/100000: episode: 744, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 52.416, mean reward: 0.524 [0.354, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.176, 10.098], loss: 0.034809, mae: 0.163434, mean_q: 19.715565
 74459/100000: episode: 745, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 49.404, mean reward: 0.494 [0.260, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.763, 10.274], loss: 0.029029, mae: 0.152277, mean_q: 19.511436
 74559/100000: episode: 746, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.860, mean reward: 0.569 [0.371, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.760, 10.098], loss: 0.026653, mae: 0.149558, mean_q: 19.825808
 74659/100000: episode: 747, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 55.707, mean reward: 0.557 [0.378, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.581, 10.098], loss: 0.026022, mae: 0.147370, mean_q: 19.512320
 74759/100000: episode: 748, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 55.919, mean reward: 0.559 [0.314, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.397, 10.164], loss: 0.027030, mae: 0.163288, mean_q: 19.634089
 74859/100000: episode: 749, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 47.891, mean reward: 0.479 [0.267, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.237, 10.098], loss: 0.023116, mae: 0.146777, mean_q: 19.576698
 74959/100000: episode: 750, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.230, mean reward: 0.532 [0.182, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.363, 10.098], loss: 0.018119, mae: 0.133645, mean_q: 19.795029
 75059/100000: episode: 751, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 49.238, mean reward: 0.492 [0.251, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.268, 10.098], loss: 0.019328, mae: 0.141190, mean_q: 19.685972
 75159/100000: episode: 752, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 52.155, mean reward: 0.522 [0.283, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.730, 10.098], loss: 0.020725, mae: 0.151036, mean_q: 19.913305
 75259/100000: episode: 753, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 51.373, mean reward: 0.514 [0.263, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.979, 10.098], loss: 0.015422, mae: 0.132227, mean_q: 19.282059
 75359/100000: episode: 754, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 51.283, mean reward: 0.513 [0.292, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.848, 10.162], loss: 0.015808, mae: 0.134075, mean_q: 19.646622
 75459/100000: episode: 755, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 55.234, mean reward: 0.552 [0.333, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.953, 10.098], loss: 0.015926, mae: 0.136474, mean_q: 19.825045
 75559/100000: episode: 756, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 52.368, mean reward: 0.524 [0.340, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.724, 10.136], loss: 0.014013, mae: 0.128221, mean_q: 19.903566
 75659/100000: episode: 757, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 54.377, mean reward: 0.544 [0.287, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.044, 10.332], loss: 0.013588, mae: 0.128788, mean_q: 19.647469
 75759/100000: episode: 758, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 55.076, mean reward: 0.551 [0.344, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.762, 10.156], loss: 0.013884, mae: 0.128421, mean_q: 19.808918
 75859/100000: episode: 759, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 56.357, mean reward: 0.564 [0.345, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.532, 10.098], loss: 0.013611, mae: 0.128546, mean_q: 19.632662
 75959/100000: episode: 760, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 50.400, mean reward: 0.504 [0.282, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.618, 10.098], loss: 0.014298, mae: 0.131245, mean_q: 19.266397
 76059/100000: episode: 761, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 56.515, mean reward: 0.565 [0.367, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.582, 10.125], loss: 0.013500, mae: 0.128663, mean_q: 19.801882
 76159/100000: episode: 762, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 52.827, mean reward: 0.528 [0.233, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.856, 10.446], loss: 0.013343, mae: 0.127233, mean_q: 20.076900
 76259/100000: episode: 763, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 55.495, mean reward: 0.555 [0.335, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.530, 10.233], loss: 0.012552, mae: 0.122222, mean_q: 19.826605
 76359/100000: episode: 764, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 54.543, mean reward: 0.545 [0.236, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.368, 10.098], loss: 0.011755, mae: 0.119383, mean_q: 19.675823
 76459/100000: episode: 765, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 55.280, mean reward: 0.553 [0.211, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.794, 10.219], loss: 0.013253, mae: 0.127384, mean_q: 19.870779
 76559/100000: episode: 766, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 54.690, mean reward: 0.547 [0.291, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.260], loss: 0.013325, mae: 0.127446, mean_q: 19.566355
 76659/100000: episode: 767, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 50.137, mean reward: 0.501 [0.301, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.716, 10.098], loss: 0.012457, mae: 0.123327, mean_q: 19.639711
 76759/100000: episode: 768, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 56.549, mean reward: 0.565 [0.395, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.743, 10.131], loss: 0.014553, mae: 0.132060, mean_q: 19.660885
 76859/100000: episode: 769, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 52.157, mean reward: 0.522 [0.336, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.399, 10.128], loss: 0.013778, mae: 0.128000, mean_q: 19.879663
 76959/100000: episode: 770, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 55.909, mean reward: 0.559 [0.297, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.839, 10.203], loss: 0.013322, mae: 0.123781, mean_q: 19.586292
 77059/100000: episode: 771, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 51.574, mean reward: 0.516 [0.194, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.309, 10.230], loss: 0.014902, mae: 0.131029, mean_q: 19.543655
 77159/100000: episode: 772, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 52.964, mean reward: 0.530 [0.322, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.082, 10.098], loss: 0.014455, mae: 0.128352, mean_q: 19.795921
 77259/100000: episode: 773, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 48.955, mean reward: 0.490 [0.226, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.840, 10.325], loss: 0.018132, mae: 0.143989, mean_q: 19.733156
 77359/100000: episode: 774, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 54.401, mean reward: 0.544 [0.359, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.685, 10.098], loss: 0.014742, mae: 0.126705, mean_q: 19.566662
 77459/100000: episode: 775, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 56.731, mean reward: 0.567 [0.244, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.955, 10.272], loss: 0.018771, mae: 0.142362, mean_q: 20.073959
 77559/100000: episode: 776, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 48.820, mean reward: 0.488 [0.320, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.003, 10.098], loss: 0.015905, mae: 0.127397, mean_q: 19.602459
 77659/100000: episode: 777, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 49.136, mean reward: 0.491 [0.286, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.967, 10.098], loss: 0.020492, mae: 0.150185, mean_q: 19.616226
 77759/100000: episode: 778, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 49.951, mean reward: 0.500 [0.282, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.696, 10.308], loss: 0.018391, mae: 0.137550, mean_q: 19.892454
 77859/100000: episode: 779, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 54.435, mean reward: 0.544 [0.259, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.856, 10.098], loss: 0.023635, mae: 0.156023, mean_q: 19.401295
 77959/100000: episode: 780, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 54.990, mean reward: 0.550 [0.389, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.744, 10.322], loss: 0.019832, mae: 0.140279, mean_q: 19.625212
 78059/100000: episode: 781, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 49.385, mean reward: 0.494 [0.247, 0.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.321, 10.337], loss: 0.020679, mae: 0.141650, mean_q: 19.697283
 78159/100000: episode: 782, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 52.111, mean reward: 0.521 [0.327, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.705, 10.098], loss: 0.021642, mae: 0.150013, mean_q: 19.461473
 78259/100000: episode: 783, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 54.490, mean reward: 0.545 [0.293, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.130, 10.098], loss: 0.021092, mae: 0.146869, mean_q: 19.611538
 78359/100000: episode: 784, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 56.482, mean reward: 0.565 [0.370, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.097, 10.108], loss: 0.020098, mae: 0.142749, mean_q: 19.673935
 78459/100000: episode: 785, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 54.807, mean reward: 0.548 [0.221, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.541, 10.135], loss: 0.021097, mae: 0.147325, mean_q: 19.578074
 78559/100000: episode: 786, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 50.214, mean reward: 0.502 [0.264, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.218, 10.225], loss: 0.020140, mae: 0.141337, mean_q: 19.582216
 78659/100000: episode: 787, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: 56.096, mean reward: 0.561 [0.205, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.921, 10.170], loss: 0.021052, mae: 0.150120, mean_q: 19.341362
 78759/100000: episode: 788, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 53.681, mean reward: 0.537 [0.197, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.488, 10.098], loss: 0.019509, mae: 0.145712, mean_q: 19.815636
 78859/100000: episode: 789, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 53.300, mean reward: 0.533 [0.188, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.860, 10.318], loss: 0.021003, mae: 0.148132, mean_q: 19.486502
 78959/100000: episode: 790, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 49.874, mean reward: 0.499 [0.164, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.982, 10.129], loss: 0.019364, mae: 0.142710, mean_q: 19.417923
 79059/100000: episode: 791, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 52.669, mean reward: 0.527 [0.183, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.077, 10.257], loss: 0.019587, mae: 0.143139, mean_q: 19.608761
 79159/100000: episode: 792, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 55.356, mean reward: 0.554 [0.362, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.958, 10.178], loss: 0.018623, mae: 0.144360, mean_q: 19.688087
 79259/100000: episode: 793, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 49.606, mean reward: 0.496 [0.296, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.331, 10.098], loss: 0.016695, mae: 0.136735, mean_q: 19.733562
 79359/100000: episode: 794, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 50.159, mean reward: 0.502 [0.195, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.492, 10.098], loss: 0.017527, mae: 0.140305, mean_q: 19.685312
 79459/100000: episode: 795, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 53.723, mean reward: 0.537 [0.302, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.713, 10.098], loss: 0.015367, mae: 0.130629, mean_q: 19.513325
 79559/100000: episode: 796, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 55.368, mean reward: 0.554 [0.250, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.870, 10.098], loss: 0.016525, mae: 0.134943, mean_q: 19.731804
 79659/100000: episode: 797, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 53.541, mean reward: 0.535 [0.303, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.046, 10.175], loss: 0.015673, mae: 0.132685, mean_q: 19.579725
 79759/100000: episode: 798, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 53.425, mean reward: 0.534 [0.322, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.943, 10.098], loss: 0.015016, mae: 0.130645, mean_q: 19.655325
 79859/100000: episode: 799, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 55.774, mean reward: 0.558 [0.313, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.552, 10.179], loss: 0.018145, mae: 0.145886, mean_q: 19.659971
 79959/100000: episode: 800, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 52.892, mean reward: 0.529 [0.280, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.549, 10.231], loss: 0.013409, mae: 0.121446, mean_q: 19.612522
 80059/100000: episode: 801, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 47.995, mean reward: 0.480 [0.141, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.179, 10.325], loss: 0.014664, mae: 0.129354, mean_q: 19.735588
 80159/100000: episode: 802, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 51.717, mean reward: 0.517 [0.306, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.776, 10.185], loss: 0.016932, mae: 0.141425, mean_q: 19.758505
 80259/100000: episode: 803, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 51.342, mean reward: 0.513 [0.341, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.635, 10.249], loss: 0.016464, mae: 0.136619, mean_q: 19.723688
 80359/100000: episode: 804, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 54.803, mean reward: 0.548 [0.264, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.896, 10.316], loss: 0.015577, mae: 0.135302, mean_q: 19.259937
 80459/100000: episode: 805, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 53.345, mean reward: 0.533 [0.390, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.822, 10.098], loss: 0.016107, mae: 0.136360, mean_q: 19.863815
 80559/100000: episode: 806, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 51.015, mean reward: 0.510 [0.155, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.882, 10.117], loss: 0.014825, mae: 0.129918, mean_q: 19.803553
 80659/100000: episode: 807, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 51.710, mean reward: 0.517 [0.301, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.019, 10.098], loss: 0.014033, mae: 0.128045, mean_q: 19.674862
 80759/100000: episode: 808, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 48.849, mean reward: 0.488 [0.283, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.591, 10.098], loss: 0.015095, mae: 0.131199, mean_q: 19.612652
 80859/100000: episode: 809, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 52.232, mean reward: 0.522 [0.329, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.235, 10.098], loss: 0.016749, mae: 0.139065, mean_q: 19.629017
 80959/100000: episode: 810, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 52.263, mean reward: 0.523 [0.313, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.370, 10.177], loss: 0.016261, mae: 0.134667, mean_q: 19.410101
 81059/100000: episode: 811, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 49.277, mean reward: 0.493 [0.295, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.591, 10.098], loss: 0.016294, mae: 0.137127, mean_q: 19.401714
 81159/100000: episode: 812, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 45.556, mean reward: 0.456 [0.193, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.603, 10.263], loss: 0.015286, mae: 0.132312, mean_q: 19.599619
 81259/100000: episode: 813, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 54.892, mean reward: 0.549 [0.285, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.160, 10.179], loss: 0.014283, mae: 0.129500, mean_q: 19.518002
 81359/100000: episode: 814, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 54.415, mean reward: 0.544 [0.339, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.929, 10.098], loss: 0.016083, mae: 0.136162, mean_q: 19.312862
 81459/100000: episode: 815, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 51.136, mean reward: 0.511 [0.098, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.836, 10.135], loss: 0.014488, mae: 0.127423, mean_q: 19.311808
 81559/100000: episode: 816, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 56.759, mean reward: 0.568 [0.263, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.688, 10.098], loss: 0.015657, mae: 0.136662, mean_q: 19.562929
 81659/100000: episode: 817, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 54.152, mean reward: 0.542 [0.358, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.649, 10.142], loss: 0.013462, mae: 0.125231, mean_q: 19.544289
 81759/100000: episode: 818, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 52.792, mean reward: 0.528 [0.325, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.148, 10.168], loss: 0.013449, mae: 0.124603, mean_q: 19.237705
 81859/100000: episode: 819, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 51.746, mean reward: 0.517 [0.269, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.505, 10.098], loss: 0.016521, mae: 0.137957, mean_q: 19.540163
 81959/100000: episode: 820, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 51.246, mean reward: 0.512 [0.262, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.645, 10.107], loss: 0.014466, mae: 0.126934, mean_q: 19.503403
 82059/100000: episode: 821, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 51.530, mean reward: 0.515 [0.211, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.110, 10.098], loss: 0.017234, mae: 0.141182, mean_q: 19.547104
 82159/100000: episode: 822, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 55.143, mean reward: 0.551 [0.386, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.137, 10.098], loss: 0.017505, mae: 0.139586, mean_q: 19.530739
 82259/100000: episode: 823, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 53.628, mean reward: 0.536 [0.329, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.666, 10.164], loss: 0.016482, mae: 0.139682, mean_q: 19.505327
 82359/100000: episode: 824, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 52.518, mean reward: 0.525 [0.253, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.574, 10.098], loss: 0.016406, mae: 0.137513, mean_q: 19.561571
 82459/100000: episode: 825, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 54.066, mean reward: 0.541 [0.250, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.465, 10.261], loss: 0.015419, mae: 0.134239, mean_q: 19.411247
 82559/100000: episode: 826, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 51.908, mean reward: 0.519 [0.238, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.575, 10.098], loss: 0.013630, mae: 0.127238, mean_q: 19.491020
 82659/100000: episode: 827, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 49.735, mean reward: 0.497 [0.246, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.704, 10.098], loss: 0.015072, mae: 0.131744, mean_q: 19.575062
 82759/100000: episode: 828, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 51.538, mean reward: 0.515 [0.273, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.591, 10.098], loss: 0.016390, mae: 0.139411, mean_q: 19.407661
 82859/100000: episode: 829, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 50.936, mean reward: 0.509 [0.281, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.381, 10.098], loss: 0.018940, mae: 0.147922, mean_q: 19.536253
 82959/100000: episode: 830, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.684, mean reward: 0.537 [0.219, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.628, 10.193], loss: 0.016116, mae: 0.139083, mean_q: 19.365120
 83059/100000: episode: 831, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 51.604, mean reward: 0.516 [0.175, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.212, 10.241], loss: 0.016412, mae: 0.135748, mean_q: 19.345625
 83159/100000: episode: 832, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 55.145, mean reward: 0.551 [0.227, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.555, 10.375], loss: 0.014728, mae: 0.132871, mean_q: 19.625483
 83259/100000: episode: 833, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 52.488, mean reward: 0.525 [0.311, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.745, 10.186], loss: 0.014214, mae: 0.126558, mean_q: 19.625818
 83359/100000: episode: 834, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 54.086, mean reward: 0.541 [0.294, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.808, 10.138], loss: 0.021230, mae: 0.157308, mean_q: 19.503351
 83459/100000: episode: 835, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 53.473, mean reward: 0.535 [0.248, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.367, 10.098], loss: 0.017041, mae: 0.140297, mean_q: 19.701675
 83559/100000: episode: 836, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 56.315, mean reward: 0.563 [0.321, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.914, 10.098], loss: 0.015267, mae: 0.131820, mean_q: 19.612421
 83659/100000: episode: 837, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 54.574, mean reward: 0.546 [0.355, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.338, 10.221], loss: 0.013798, mae: 0.124433, mean_q: 19.575150
 83759/100000: episode: 838, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 57.384, mean reward: 0.574 [0.354, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.798, 10.098], loss: 0.015956, mae: 0.134662, mean_q: 19.447063
 83859/100000: episode: 839, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 55.101, mean reward: 0.551 [0.379, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.386, 10.360], loss: 0.017156, mae: 0.142379, mean_q: 19.592619
 83959/100000: episode: 840, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 56.755, mean reward: 0.568 [0.292, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.084, 10.098], loss: 0.018299, mae: 0.148832, mean_q: 19.590918
 84059/100000: episode: 841, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 54.939, mean reward: 0.549 [0.361, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.837, 10.098], loss: 0.016841, mae: 0.138180, mean_q: 19.783499
 84159/100000: episode: 842, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 53.326, mean reward: 0.533 [0.195, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.339, 10.180], loss: 0.015386, mae: 0.133416, mean_q: 19.453718
 84259/100000: episode: 843, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 55.904, mean reward: 0.559 [0.379, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.296, 10.135], loss: 0.017013, mae: 0.139519, mean_q: 19.454617
 84359/100000: episode: 844, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 53.435, mean reward: 0.534 [0.245, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.416, 10.138], loss: 0.018790, mae: 0.148795, mean_q: 19.484655
 84459/100000: episode: 845, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 49.947, mean reward: 0.499 [0.175, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.461, 10.098], loss: 0.017991, mae: 0.145221, mean_q: 19.719578
 84559/100000: episode: 846, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 53.383, mean reward: 0.534 [0.321, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.708, 10.271], loss: 0.016995, mae: 0.140275, mean_q: 19.373375
 84659/100000: episode: 847, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 55.468, mean reward: 0.555 [0.347, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.695, 10.098], loss: 0.015642, mae: 0.133113, mean_q: 19.449924
 84759/100000: episode: 848, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 50.467, mean reward: 0.505 [0.269, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.813, 10.098], loss: 0.016256, mae: 0.136589, mean_q: 19.448294
 84859/100000: episode: 849, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 56.172, mean reward: 0.562 [0.258, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.969, 10.098], loss: 0.017711, mae: 0.143101, mean_q: 19.536308
 84959/100000: episode: 850, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 54.733, mean reward: 0.547 [0.327, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.091, 10.098], loss: 0.017264, mae: 0.140587, mean_q: 19.466253
 85059/100000: episode: 851, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 53.571, mean reward: 0.536 [0.272, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.694, 10.259], loss: 0.015878, mae: 0.136366, mean_q: 19.614965
 85159/100000: episode: 852, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 44.583, mean reward: 0.446 [0.179, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.905, 10.388], loss: 0.016520, mae: 0.137604, mean_q: 19.639545
 85259/100000: episode: 853, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 54.748, mean reward: 0.547 [0.369, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.350, 10.098], loss: 0.017217, mae: 0.142696, mean_q: 19.342451
 85359/100000: episode: 854, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 52.862, mean reward: 0.529 [0.286, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.380, 10.204], loss: 0.016028, mae: 0.135958, mean_q: 19.429150
 85459/100000: episode: 855, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 54.756, mean reward: 0.548 [0.251, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.423, 10.098], loss: 0.016674, mae: 0.138844, mean_q: 19.580896
 85559/100000: episode: 856, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 48.208, mean reward: 0.482 [0.143, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.268, 10.098], loss: 0.018713, mae: 0.148436, mean_q: 19.497602
 85659/100000: episode: 857, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 53.162, mean reward: 0.532 [0.333, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.611, 10.231], loss: 0.016683, mae: 0.136572, mean_q: 19.489779
 85759/100000: episode: 858, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 53.053, mean reward: 0.531 [0.291, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.087, 10.158], loss: 0.017678, mae: 0.142782, mean_q: 19.166489
 85859/100000: episode: 859, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 53.396, mean reward: 0.534 [0.224, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.393, 10.127], loss: 0.019102, mae: 0.150105, mean_q: 19.521952
 85959/100000: episode: 860, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 51.392, mean reward: 0.514 [0.202, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.319], loss: 0.016179, mae: 0.134235, mean_q: 19.588039
 86059/100000: episode: 861, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 51.712, mean reward: 0.517 [0.233, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.558, 10.386], loss: 0.021908, mae: 0.161477, mean_q: 19.683105
 86159/100000: episode: 862, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 53.174, mean reward: 0.532 [0.279, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.996, 10.098], loss: 0.015371, mae: 0.131569, mean_q: 19.346769
 86259/100000: episode: 863, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 56.411, mean reward: 0.564 [0.360, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.004, 10.131], loss: 0.017072, mae: 0.139885, mean_q: 19.538034
 86359/100000: episode: 864, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 50.788, mean reward: 0.508 [0.304, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.097, 10.270], loss: 0.017261, mae: 0.139661, mean_q: 19.582792
 86459/100000: episode: 865, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 51.803, mean reward: 0.518 [0.319, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.984, 10.129], loss: 0.016907, mae: 0.137241, mean_q: 19.622942
 86559/100000: episode: 866, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 53.525, mean reward: 0.535 [0.281, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.428, 10.098], loss: 0.015039, mae: 0.131046, mean_q: 19.507650
 86659/100000: episode: 867, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 54.487, mean reward: 0.545 [0.323, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.491, 10.188], loss: 0.017906, mae: 0.141688, mean_q: 19.552889
 86759/100000: episode: 868, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 53.886, mean reward: 0.539 [0.338, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.670, 10.098], loss: 0.017509, mae: 0.141938, mean_q: 19.497517
 86859/100000: episode: 869, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 51.285, mean reward: 0.513 [0.293, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.655, 10.257], loss: 0.016396, mae: 0.136215, mean_q: 19.199999
 86959/100000: episode: 870, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 55.834, mean reward: 0.558 [0.292, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.116, 10.098], loss: 0.016501, mae: 0.138726, mean_q: 19.672508
 87059/100000: episode: 871, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 46.534, mean reward: 0.465 [0.261, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.484, 10.098], loss: 0.019632, mae: 0.149910, mean_q: 19.506109
 87159/100000: episode: 872, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 50.792, mean reward: 0.508 [0.204, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.985, 10.557], loss: 0.017279, mae: 0.141527, mean_q: 19.652809
 87259/100000: episode: 873, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 56.343, mean reward: 0.563 [0.403, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.387, 10.098], loss: 0.014082, mae: 0.126804, mean_q: 19.335642
 87359/100000: episode: 874, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 51.144, mean reward: 0.511 [0.268, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.090, 10.385], loss: 0.017911, mae: 0.145128, mean_q: 19.408840
 87459/100000: episode: 875, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 55.092, mean reward: 0.551 [0.338, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.028, 10.098], loss: 0.019626, mae: 0.154022, mean_q: 19.566120
 87559/100000: episode: 876, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 50.688, mean reward: 0.507 [0.309, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.644, 10.098], loss: 0.015825, mae: 0.133411, mean_q: 19.326359
 87659/100000: episode: 877, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 52.942, mean reward: 0.529 [0.311, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.141, 10.181], loss: 0.016084, mae: 0.136500, mean_q: 19.528658
 87759/100000: episode: 878, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 54.884, mean reward: 0.549 [0.357, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.013, 10.098], loss: 0.015109, mae: 0.131511, mean_q: 19.720905
 87859/100000: episode: 879, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 53.427, mean reward: 0.534 [0.171, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.098], loss: 0.014500, mae: 0.129478, mean_q: 19.416582
 87959/100000: episode: 880, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 52.332, mean reward: 0.523 [0.243, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.135, 10.098], loss: 0.014950, mae: 0.131275, mean_q: 19.632793
 88059/100000: episode: 881, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 51.230, mean reward: 0.512 [0.271, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.063, 10.362], loss: 0.018690, mae: 0.147906, mean_q: 19.382088
 88159/100000: episode: 882, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 51.398, mean reward: 0.514 [0.297, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.362, 10.323], loss: 0.016538, mae: 0.136628, mean_q: 19.718626
 88259/100000: episode: 883, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 54.657, mean reward: 0.547 [0.329, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.684, 10.128], loss: 0.019719, mae: 0.154910, mean_q: 19.399620
 88359/100000: episode: 884, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 53.534, mean reward: 0.535 [0.220, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.185, 10.220], loss: 0.019006, mae: 0.150864, mean_q: 19.824911
 88459/100000: episode: 885, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 46.069, mean reward: 0.461 [0.256, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.340, 10.098], loss: 0.015705, mae: 0.136067, mean_q: 19.208565
 88559/100000: episode: 886, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 52.316, mean reward: 0.523 [0.284, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.755, 10.098], loss: 0.016779, mae: 0.139625, mean_q: 19.605202
 88659/100000: episode: 887, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 56.587, mean reward: 0.566 [0.302, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.333, 10.228], loss: 0.016469, mae: 0.139830, mean_q: 19.764288
 88759/100000: episode: 888, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 55.099, mean reward: 0.551 [0.346, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.291, 10.159], loss: 0.015947, mae: 0.134900, mean_q: 19.441267
 88859/100000: episode: 889, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 53.206, mean reward: 0.532 [0.239, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.672, 10.098], loss: 0.015957, mae: 0.136092, mean_q: 19.572588
 88959/100000: episode: 890, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 54.788, mean reward: 0.548 [0.329, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.374, 10.098], loss: 0.015281, mae: 0.131815, mean_q: 19.642359
 89059/100000: episode: 891, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 55.206, mean reward: 0.552 [0.286, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.892, 10.099], loss: 0.013689, mae: 0.126147, mean_q: 19.704361
 89159/100000: episode: 892, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 53.789, mean reward: 0.538 [0.285, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.984, 10.099], loss: 0.014688, mae: 0.130115, mean_q: 19.949783
 89259/100000: episode: 893, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 47.952, mean reward: 0.480 [0.158, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.116, 10.191], loss: 0.012930, mae: 0.122759, mean_q: 19.480919
 89359/100000: episode: 894, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 54.271, mean reward: 0.543 [0.358, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.895, 10.098], loss: 0.013999, mae: 0.128126, mean_q: 19.621563
 89459/100000: episode: 895, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 52.112, mean reward: 0.521 [0.310, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.832, 10.098], loss: 0.014129, mae: 0.126970, mean_q: 19.813984
 89559/100000: episode: 896, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 51.695, mean reward: 0.517 [0.272, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.084, 10.294], loss: 0.014880, mae: 0.130508, mean_q: 19.515526
 89659/100000: episode: 897, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 52.937, mean reward: 0.529 [0.308, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.252, 10.098], loss: 0.015067, mae: 0.132125, mean_q: 19.494221
 89759/100000: episode: 898, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 50.444, mean reward: 0.504 [0.217, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.592, 10.098], loss: 0.015681, mae: 0.132319, mean_q: 19.995123
 89859/100000: episode: 899, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 51.492, mean reward: 0.515 [0.317, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.352, 10.098], loss: 0.015268, mae: 0.133533, mean_q: 19.351500
 89959/100000: episode: 900, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 54.413, mean reward: 0.544 [0.322, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.565, 10.230], loss: 0.016287, mae: 0.137193, mean_q: 19.704390
 90059/100000: episode: 901, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 50.770, mean reward: 0.508 [0.252, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.230, 10.203], loss: 0.016117, mae: 0.136202, mean_q: 19.775650
 90159/100000: episode: 902, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 53.858, mean reward: 0.539 [0.310, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.754, 10.098], loss: 0.013650, mae: 0.124363, mean_q: 19.745399
 90259/100000: episode: 903, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 51.283, mean reward: 0.513 [0.307, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.418, 10.152], loss: 0.014088, mae: 0.127020, mean_q: 19.287849
 90359/100000: episode: 904, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 53.802, mean reward: 0.538 [0.335, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.247, 10.332], loss: 0.015852, mae: 0.135241, mean_q: 19.541563
 90459/100000: episode: 905, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 54.145, mean reward: 0.541 [0.369, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.171, 10.355], loss: 0.017333, mae: 0.142300, mean_q: 19.399084
 90559/100000: episode: 906, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 53.564, mean reward: 0.536 [0.277, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.201, 10.098], loss: 0.013857, mae: 0.123883, mean_q: 19.922613
 90659/100000: episode: 907, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 53.558, mean reward: 0.536 [0.300, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.613, 10.192], loss: 0.012621, mae: 0.119646, mean_q: 19.414326
 90759/100000: episode: 908, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 50.719, mean reward: 0.507 [0.263, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.706, 10.098], loss: 0.014186, mae: 0.128591, mean_q: 19.754671
 90859/100000: episode: 909, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 46.913, mean reward: 0.469 [0.321, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.660, 10.098], loss: 0.012347, mae: 0.115479, mean_q: 19.576649
 90959/100000: episode: 910, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 55.480, mean reward: 0.555 [0.301, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.885, 10.098], loss: 0.013417, mae: 0.122568, mean_q: 19.509348
 91059/100000: episode: 911, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 52.090, mean reward: 0.521 [0.291, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.852, 10.098], loss: 0.011556, mae: 0.114634, mean_q: 19.801785
 91159/100000: episode: 912, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 52.714, mean reward: 0.527 [0.286, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.281, 10.098], loss: 0.013004, mae: 0.121971, mean_q: 19.814684
 91259/100000: episode: 913, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 53.937, mean reward: 0.539 [0.241, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.547, 10.226], loss: 0.013236, mae: 0.122290, mean_q: 19.640541
 91359/100000: episode: 914, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 53.171, mean reward: 0.532 [0.237, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.933, 10.166], loss: 0.012751, mae: 0.120775, mean_q: 19.460354
 91459/100000: episode: 915, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 51.270, mean reward: 0.513 [0.152, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.161, 10.098], loss: 0.017021, mae: 0.140735, mean_q: 19.288153
 91559/100000: episode: 916, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 52.547, mean reward: 0.525 [0.113, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.752, 10.268], loss: 0.013491, mae: 0.124273, mean_q: 19.552982
 91659/100000: episode: 917, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 56.289, mean reward: 0.563 [0.313, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.240, 10.128], loss: 0.014654, mae: 0.131811, mean_q: 19.582314
 91759/100000: episode: 918, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 49.811, mean reward: 0.498 [0.278, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.688, 10.406], loss: 0.015920, mae: 0.139318, mean_q: 19.465479
 91859/100000: episode: 919, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 53.710, mean reward: 0.537 [0.328, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.645, 10.098], loss: 0.015639, mae: 0.134469, mean_q: 19.328737
 91959/100000: episode: 920, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 54.176, mean reward: 0.542 [0.329, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.702, 10.098], loss: 0.015711, mae: 0.133177, mean_q: 19.195639
 92059/100000: episode: 921, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 53.627, mean reward: 0.536 [0.335, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.705, 10.186], loss: 0.014277, mae: 0.129628, mean_q: 19.469372
 92159/100000: episode: 922, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 53.143, mean reward: 0.531 [0.312, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.809, 10.258], loss: 0.014723, mae: 0.132867, mean_q: 19.602468
 92259/100000: episode: 923, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 51.456, mean reward: 0.515 [0.216, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.802, 10.098], loss: 0.013904, mae: 0.129215, mean_q: 19.482214
 92359/100000: episode: 924, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 50.776, mean reward: 0.508 [0.232, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.607, 10.264], loss: 0.014709, mae: 0.128492, mean_q: 19.484392
 92459/100000: episode: 925, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 55.477, mean reward: 0.555 [0.360, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.905, 10.192], loss: 0.013113, mae: 0.124360, mean_q: 19.390091
 92559/100000: episode: 926, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 54.136, mean reward: 0.541 [0.356, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.494, 10.098], loss: 0.015369, mae: 0.134949, mean_q: 19.422125
 92659/100000: episode: 927, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 56.170, mean reward: 0.562 [0.238, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.795, 10.346], loss: 0.014015, mae: 0.128111, mean_q: 19.580870
 92759/100000: episode: 928, duration: 0.638s, episode steps: 100, steps per second: 157, episode reward: 53.602, mean reward: 0.536 [0.295, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.008, 10.165], loss: 0.016428, mae: 0.140628, mean_q: 19.395872
 92859/100000: episode: 929, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 53.740, mean reward: 0.537 [0.347, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.926, 10.311], loss: 0.013412, mae: 0.125667, mean_q: 19.430147
 92959/100000: episode: 930, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 49.704, mean reward: 0.497 [0.176, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.293, 10.375], loss: 0.012573, mae: 0.122191, mean_q: 19.336245
 93059/100000: episode: 931, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 55.537, mean reward: 0.555 [0.307, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.811, 10.156], loss: 0.014081, mae: 0.130812, mean_q: 19.558270
 93159/100000: episode: 932, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 56.060, mean reward: 0.561 [0.345, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.963, 10.098], loss: 0.015143, mae: 0.135525, mean_q: 19.586346
 93259/100000: episode: 933, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 54.134, mean reward: 0.541 [0.286, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.561, 10.098], loss: 0.016358, mae: 0.141817, mean_q: 19.568380
 93359/100000: episode: 934, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 53.404, mean reward: 0.534 [0.286, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.946, 10.098], loss: 0.018691, mae: 0.151035, mean_q: 19.200008
 93459/100000: episode: 935, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 49.973, mean reward: 0.500 [0.266, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.731, 10.232], loss: 0.014886, mae: 0.136392, mean_q: 19.510160
 93559/100000: episode: 936, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 53.430, mean reward: 0.534 [0.282, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.285, 10.158], loss: 0.015841, mae: 0.136709, mean_q: 19.654274
 93659/100000: episode: 937, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 57.257, mean reward: 0.573 [0.392, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.262, 10.131], loss: 0.015831, mae: 0.138460, mean_q: 19.390390
 93759/100000: episode: 938, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 52.566, mean reward: 0.526 [0.083, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.194, 10.283], loss: 0.013280, mae: 0.126992, mean_q: 19.798584
 93859/100000: episode: 939, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 50.461, mean reward: 0.505 [0.195, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.784, 10.253], loss: 0.012833, mae: 0.122701, mean_q: 19.648386
 93959/100000: episode: 940, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 51.860, mean reward: 0.519 [0.275, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.204, 10.098], loss: 0.016331, mae: 0.142035, mean_q: 19.588469
 94059/100000: episode: 941, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 52.355, mean reward: 0.524 [0.323, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.574, 10.429], loss: 0.014196, mae: 0.130258, mean_q: 19.540739
 94159/100000: episode: 942, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 50.058, mean reward: 0.501 [0.194, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.267, 10.098], loss: 0.015397, mae: 0.136969, mean_q: 19.479979
 94259/100000: episode: 943, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 52.261, mean reward: 0.523 [0.333, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.910, 10.098], loss: 0.017684, mae: 0.149826, mean_q: 19.676899
 94359/100000: episode: 944, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 51.607, mean reward: 0.516 [0.312, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.849, 10.098], loss: 0.014017, mae: 0.128335, mean_q: 19.608597
 94459/100000: episode: 945, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 55.154, mean reward: 0.552 [0.381, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.339, 10.098], loss: 0.013886, mae: 0.128235, mean_q: 19.310755
 94559/100000: episode: 946, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 56.571, mean reward: 0.566 [0.362, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.835, 10.166], loss: 0.014620, mae: 0.131692, mean_q: 19.332386
 94659/100000: episode: 947, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 52.325, mean reward: 0.523 [0.215, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.773, 10.098], loss: 0.014443, mae: 0.131444, mean_q: 19.370478
 94759/100000: episode: 948, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 52.768, mean reward: 0.528 [0.163, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.155, 10.098], loss: 0.017561, mae: 0.143444, mean_q: 19.733561
 94859/100000: episode: 949, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 57.014, mean reward: 0.570 [0.302, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.351, 10.134], loss: 0.016016, mae: 0.136929, mean_q: 19.551323
 94959/100000: episode: 950, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 50.968, mean reward: 0.510 [0.299, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.955, 10.098], loss: 0.015929, mae: 0.136896, mean_q: 19.527052
 95059/100000: episode: 951, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 54.943, mean reward: 0.549 [0.353, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.030, 10.146], loss: 0.016372, mae: 0.136672, mean_q: 19.506788
 95159/100000: episode: 952, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 53.863, mean reward: 0.539 [0.295, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.186, 10.237], loss: 0.016197, mae: 0.139157, mean_q: 19.604944
 95259/100000: episode: 953, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 55.766, mean reward: 0.558 [0.411, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.850, 10.125], loss: 0.015387, mae: 0.131949, mean_q: 19.432444
 95359/100000: episode: 954, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 51.832, mean reward: 0.518 [0.286, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.720, 10.098], loss: 0.015012, mae: 0.130482, mean_q: 19.363388
 95459/100000: episode: 955, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 53.208, mean reward: 0.532 [0.273, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.711, 10.245], loss: 0.019200, mae: 0.143580, mean_q: 19.551661
 95559/100000: episode: 956, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 54.516, mean reward: 0.545 [0.338, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.687, 10.179], loss: 0.016848, mae: 0.139455, mean_q: 19.867805
 95659/100000: episode: 957, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 51.953, mean reward: 0.520 [0.315, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.569, 10.221], loss: 0.015421, mae: 0.133036, mean_q: 19.573797
 95759/100000: episode: 958, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 55.911, mean reward: 0.559 [0.251, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.081, 10.171], loss: 0.018695, mae: 0.144919, mean_q: 19.433613
 95859/100000: episode: 959, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 50.206, mean reward: 0.502 [0.238, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.011, 10.233], loss: 0.016878, mae: 0.137359, mean_q: 19.103720
 95959/100000: episode: 960, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 54.364, mean reward: 0.544 [0.353, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.548, 10.271], loss: 0.015642, mae: 0.129761, mean_q: 19.693031
 96059/100000: episode: 961, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 53.698, mean reward: 0.537 [0.332, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.193, 10.099], loss: 0.019541, mae: 0.144474, mean_q: 19.378036
 96159/100000: episode: 962, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 53.782, mean reward: 0.538 [0.353, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.046, 10.098], loss: 0.018167, mae: 0.138924, mean_q: 19.566422
 96259/100000: episode: 963, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 52.871, mean reward: 0.529 [0.294, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.799, 10.464], loss: 0.015933, mae: 0.134205, mean_q: 19.710430
 96359/100000: episode: 964, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 51.151, mean reward: 0.512 [0.310, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.375, 10.098], loss: 0.020073, mae: 0.144738, mean_q: 19.476513
 96459/100000: episode: 965, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 54.642, mean reward: 0.546 [0.208, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.850, 10.098], loss: 0.019723, mae: 0.147537, mean_q: 19.304037
 96559/100000: episode: 966, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 51.622, mean reward: 0.516 [0.169, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.485, 10.291], loss: 0.018353, mae: 0.136690, mean_q: 19.453051
 96659/100000: episode: 967, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 53.467, mean reward: 0.535 [0.309, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.548, 10.099], loss: 0.020836, mae: 0.154245, mean_q: 19.600945
 96759/100000: episode: 968, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 50.661, mean reward: 0.507 [0.309, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.295, 10.248], loss: 0.019419, mae: 0.145449, mean_q: 19.589989
 96859/100000: episode: 969, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 44.542, mean reward: 0.445 [0.168, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.468, 10.391], loss: 0.017649, mae: 0.142263, mean_q: 19.813499
 96959/100000: episode: 970, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 53.071, mean reward: 0.531 [0.248, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.237, 10.342], loss: 0.016384, mae: 0.131642, mean_q: 19.296793
 97059/100000: episode: 971, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 54.005, mean reward: 0.540 [0.287, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.896, 10.098], loss: 0.021354, mae: 0.152393, mean_q: 19.493393
 97159/100000: episode: 972, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 49.464, mean reward: 0.495 [0.313, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.083, 10.276], loss: 0.020305, mae: 0.149173, mean_q: 19.555817
 97259/100000: episode: 973, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 54.987, mean reward: 0.550 [0.222, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.353, 10.325], loss: 0.017191, mae: 0.137672, mean_q: 19.587408
 97359/100000: episode: 974, duration: 0.830s, episode steps: 100, steps per second: 120, episode reward: 55.208, mean reward: 0.552 [0.297, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.780, 10.098], loss: 0.020697, mae: 0.150055, mean_q: 19.372858
 97459/100000: episode: 975, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 52.022, mean reward: 0.520 [0.271, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.297, 10.098], loss: 0.022713, mae: 0.155788, mean_q: 19.439171
 97559/100000: episode: 976, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 49.867, mean reward: 0.499 [0.220, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.107, 10.336], loss: 0.015807, mae: 0.130428, mean_q: 19.464699
 97659/100000: episode: 977, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 52.952, mean reward: 0.530 [0.263, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.155, 10.123], loss: 0.019348, mae: 0.142649, mean_q: 19.519495
 97759/100000: episode: 978, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 48.639, mean reward: 0.486 [0.216, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.177, 10.141], loss: 0.017978, mae: 0.136651, mean_q: 19.596262
 97859/100000: episode: 979, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 51.041, mean reward: 0.510 [0.321, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.857, 10.098], loss: 0.021515, mae: 0.152922, mean_q: 19.605234
 97959/100000: episode: 980, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 54.148, mean reward: 0.541 [0.257, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.606, 10.157], loss: 0.017627, mae: 0.139459, mean_q: 19.599480
 98059/100000: episode: 981, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 49.210, mean reward: 0.492 [0.300, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.538, 10.309], loss: 0.016434, mae: 0.135959, mean_q: 19.690556
 98159/100000: episode: 982, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 53.073, mean reward: 0.531 [0.195, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.686, 10.151], loss: 0.018483, mae: 0.143715, mean_q: 19.666864
 98259/100000: episode: 983, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 56.087, mean reward: 0.561 [0.334, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.000, 10.098], loss: 0.016660, mae: 0.137409, mean_q: 19.448483
 98359/100000: episode: 984, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: 50.953, mean reward: 0.510 [0.281, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.286, 10.098], loss: 0.017929, mae: 0.145155, mean_q: 19.352867
 98459/100000: episode: 985, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 54.871, mean reward: 0.549 [0.280, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.222, 10.155], loss: 0.019793, mae: 0.150191, mean_q: 19.643801
 98559/100000: episode: 986, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 53.439, mean reward: 0.534 [0.157, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.486, 10.098], loss: 0.022012, mae: 0.157829, mean_q: 19.307167
 98659/100000: episode: 987, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 49.692, mean reward: 0.497 [0.189, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.234, 10.229], loss: 0.019627, mae: 0.150545, mean_q: 19.264122
 98759/100000: episode: 988, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 52.114, mean reward: 0.521 [0.292, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.162, 10.346], loss: 0.019917, mae: 0.149406, mean_q: 19.603209
 98859/100000: episode: 989, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 51.261, mean reward: 0.513 [0.239, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.614, 10.312], loss: 0.020004, mae: 0.155098, mean_q: 19.482725
 98959/100000: episode: 990, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 54.750, mean reward: 0.547 [0.342, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.827, 10.324], loss: 0.021242, mae: 0.157302, mean_q: 19.306807
 99059/100000: episode: 991, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 53.276, mean reward: 0.533 [0.242, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.309, 10.401], loss: 0.018913, mae: 0.146883, mean_q: 19.461315
 99159/100000: episode: 992, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 52.120, mean reward: 0.521 [0.320, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.403, 10.352], loss: 0.018268, mae: 0.141171, mean_q: 19.571619
 99259/100000: episode: 993, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 57.101, mean reward: 0.571 [0.332, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.503, 10.244], loss: 0.017863, mae: 0.144016, mean_q: 19.643530
 99359/100000: episode: 994, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 51.978, mean reward: 0.520 [0.319, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.798, 10.098], loss: 0.017663, mae: 0.145349, mean_q: 19.427940
 99459/100000: episode: 995, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 50.397, mean reward: 0.504 [0.231, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.306, 10.098], loss: 0.019032, mae: 0.149102, mean_q: 19.542690
 99559/100000: episode: 996, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 48.163, mean reward: 0.482 [0.240, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.365, 10.098], loss: 0.019476, mae: 0.145138, mean_q: 19.288733
 99659/100000: episode: 997, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 51.456, mean reward: 0.515 [0.257, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.621, 10.500], loss: 0.020603, mae: 0.149431, mean_q: 19.206398
 99759/100000: episode: 998, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 49.497, mean reward: 0.495 [0.269, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.780, 10.343], loss: 0.021926, mae: 0.151690, mean_q: 19.526009
 99859/100000: episode: 999, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 55.891, mean reward: 0.559 [0.275, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.576, 10.098], loss: 0.025242, mae: 0.169004, mean_q: 19.537924
 99959/100000: episode: 1000, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 52.512, mean reward: 0.525 [0.220, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.199, 10.098], loss: 0.023048, mae: 0.155680, mean_q: 19.619287
done, took 620.247 seconds
[Info] End Uniform Random Simulation.
