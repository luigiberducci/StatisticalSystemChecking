Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.177s, episode steps: 100, steps per second: 566, episode reward: 52.761, mean reward: 0.528 [0.114, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.056, 10.101], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.062s, episode steps: 100, steps per second: 1617, episode reward: 53.457, mean reward: 0.535 [0.285, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.100, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.070s, episode steps: 100, steps per second: 1434, episode reward: 54.769, mean reward: 0.548 [0.222, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.480, 10.135], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.076s, episode steps: 100, steps per second: 1322, episode reward: 54.444, mean reward: 0.544 [0.273, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.421, 10.362], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.081s, episode steps: 100, steps per second: 1228, episode reward: 54.020, mean reward: 0.540 [0.324, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.712, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.232s, episode steps: 100, steps per second: 81, episode reward: 51.050, mean reward: 0.511 [0.117, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.925, 10.098], loss: 0.103086, mae: 0.206683, mean_q: 3.363978
   700/100000: episode: 7, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 53.847, mean reward: 0.538 [0.305, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.948, 10.099], loss: 0.095518, mae: 0.180077, mean_q: 3.820459
   800/100000: episode: 8, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 55.155, mean reward: 0.552 [0.322, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.670, 10.124], loss: 0.115195, mae: 0.192597, mean_q: 4.227684
   900/100000: episode: 9, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 53.340, mean reward: 0.533 [0.227, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.162, 10.154], loss: 0.078902, mae: 0.155376, mean_q: 4.698817
  1000/100000: episode: 10, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 54.181, mean reward: 0.542 [0.391, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.500, 10.098], loss: 0.110726, mae: 0.198588, mean_q: 5.094022
  1100/100000: episode: 11, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 52.690, mean reward: 0.527 [0.292, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.038, 10.272], loss: 0.076527, mae: 0.162743, mean_q: 5.519545
  1200/100000: episode: 12, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 52.671, mean reward: 0.527 [0.311, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.743, 10.098], loss: 0.074188, mae: 0.164239, mean_q: 5.959808
  1300/100000: episode: 13, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 55.647, mean reward: 0.556 [0.345, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.028, 10.098], loss: 0.063528, mae: 0.153277, mean_q: 6.362359
  1400/100000: episode: 14, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 50.932, mean reward: 0.509 [0.226, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.363, 10.236], loss: 0.058171, mae: 0.143182, mean_q: 6.777163
  1500/100000: episode: 15, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 52.351, mean reward: 0.524 [0.238, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.471, 10.552], loss: 0.043505, mae: 0.149062, mean_q: 7.165494
  1600/100000: episode: 16, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 51.358, mean reward: 0.514 [0.242, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.585, 10.098], loss: 0.041280, mae: 0.138484, mean_q: 7.594940
  1700/100000: episode: 17, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 52.034, mean reward: 0.520 [0.294, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.675, 10.098], loss: 0.056510, mae: 0.165830, mean_q: 7.928696
  1800/100000: episode: 18, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 49.689, mean reward: 0.497 [0.310, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.813, 10.151], loss: 0.038632, mae: 0.146355, mean_q: 8.284857
  1900/100000: episode: 19, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 53.163, mean reward: 0.532 [0.354, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.888, 10.293], loss: 0.030136, mae: 0.134079, mean_q: 8.606913
  2000/100000: episode: 20, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 45.454, mean reward: 0.455 [0.171, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.733, 10.458], loss: 0.035773, mae: 0.155364, mean_q: 8.983768
  2100/100000: episode: 21, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 54.370, mean reward: 0.544 [0.316, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.643, 10.182], loss: 0.034170, mae: 0.162216, mean_q: 9.370029
  2200/100000: episode: 22, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 56.759, mean reward: 0.568 [0.289, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.712, 10.112], loss: 0.024134, mae: 0.131326, mean_q: 9.724059
  2300/100000: episode: 23, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 52.111, mean reward: 0.521 [0.277, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.765, 10.300], loss: 0.015475, mae: 0.120047, mean_q: 10.054995
  2400/100000: episode: 24, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 47.177, mean reward: 0.472 [0.184, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.521, 10.552], loss: 0.016981, mae: 0.120751, mean_q: 10.420763
  2500/100000: episode: 25, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 56.135, mean reward: 0.561 [0.331, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.939, 10.408], loss: 0.013595, mae: 0.116374, mean_q: 10.670309
  2600/100000: episode: 26, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 55.312, mean reward: 0.553 [0.345, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.450, 10.189], loss: 0.018009, mae: 0.129442, mean_q: 10.919814
  2700/100000: episode: 27, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 55.739, mean reward: 0.557 [0.403, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.735, 10.170], loss: 0.015128, mae: 0.123636, mean_q: 11.390000
  2800/100000: episode: 28, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 54.912, mean reward: 0.549 [0.219, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.970, 10.098], loss: 0.014898, mae: 0.123787, mean_q: 11.671675
  2900/100000: episode: 29, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 53.145, mean reward: 0.531 [0.324, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.340], loss: 0.016151, mae: 0.127570, mean_q: 11.854971
  3000/100000: episode: 30, duration: 0.615s, episode steps: 100, steps per second: 162, episode reward: 52.362, mean reward: 0.524 [0.347, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.417, 10.361], loss: 0.014720, mae: 0.126865, mean_q: 12.209531
  3100/100000: episode: 31, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 53.250, mean reward: 0.532 [0.342, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.805, 10.357], loss: 0.016205, mae: 0.130530, mean_q: 12.494630
  3200/100000: episode: 32, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.746, mean reward: 0.567 [0.414, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.175, 10.319], loss: 0.015691, mae: 0.131134, mean_q: 12.677381
  3300/100000: episode: 33, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 54.423, mean reward: 0.544 [0.297, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.798, 10.147], loss: 0.015310, mae: 0.123754, mean_q: 13.034436
  3400/100000: episode: 34, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 56.175, mean reward: 0.562 [0.363, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.625, 10.161], loss: 0.016301, mae: 0.127047, mean_q: 13.260643
  3500/100000: episode: 35, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 54.227, mean reward: 0.542 [0.165, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.365, 10.098], loss: 0.014268, mae: 0.122346, mean_q: 13.348499
  3600/100000: episode: 36, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 49.902, mean reward: 0.499 [0.146, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.354, 10.190], loss: 0.013725, mae: 0.121462, mean_q: 13.770514
  3700/100000: episode: 37, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 55.495, mean reward: 0.555 [0.327, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.991, 10.111], loss: 0.016309, mae: 0.132031, mean_q: 13.975247
  3800/100000: episode: 38, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 55.306, mean reward: 0.553 [0.303, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.662, 10.098], loss: 0.017136, mae: 0.139094, mean_q: 14.083766
  3900/100000: episode: 39, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 54.954, mean reward: 0.550 [0.231, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.393, 10.293], loss: 0.017170, mae: 0.135075, mean_q: 14.325313
  4000/100000: episode: 40, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 50.488, mean reward: 0.505 [0.275, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.074, 10.493], loss: 0.016908, mae: 0.133125, mean_q: 14.688397
  4100/100000: episode: 41, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 54.851, mean reward: 0.549 [0.299, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.862, 10.098], loss: 0.016388, mae: 0.129723, mean_q: 14.975231
  4200/100000: episode: 42, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 55.326, mean reward: 0.553 [0.339, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.200, 10.209], loss: 0.017192, mae: 0.133197, mean_q: 15.092112
  4300/100000: episode: 43, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 53.713, mean reward: 0.537 [0.244, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.315, 10.123], loss: 0.017859, mae: 0.132622, mean_q: 15.087371
  4400/100000: episode: 44, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 52.614, mean reward: 0.526 [0.329, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.650, 10.098], loss: 0.019754, mae: 0.138221, mean_q: 15.278151
  4500/100000: episode: 45, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 51.573, mean reward: 0.516 [0.303, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.803, 10.325], loss: 0.016215, mae: 0.128506, mean_q: 15.655386
  4600/100000: episode: 46, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 49.717, mean reward: 0.497 [0.226, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.708, 10.196], loss: 0.018403, mae: 0.136008, mean_q: 15.866590
  4700/100000: episode: 47, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 53.257, mean reward: 0.533 [0.281, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.632, 10.286], loss: 0.017849, mae: 0.129540, mean_q: 15.964656
  4800/100000: episode: 48, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 54.764, mean reward: 0.548 [0.150, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.089, 10.098], loss: 0.020396, mae: 0.144406, mean_q: 16.213360
  4900/100000: episode: 49, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 54.352, mean reward: 0.544 [0.322, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.612, 10.197], loss: 0.013976, mae: 0.122346, mean_q: 16.397955
  5000/100000: episode: 50, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 50.496, mean reward: 0.505 [0.244, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.993, 10.098], loss: 0.016525, mae: 0.129062, mean_q: 16.626633
  5100/100000: episode: 51, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 52.778, mean reward: 0.528 [0.293, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.111, 10.098], loss: 0.017989, mae: 0.139715, mean_q: 16.821486
  5200/100000: episode: 52, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 56.707, mean reward: 0.567 [0.327, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.885, 10.194], loss: 0.020159, mae: 0.139141, mean_q: 16.798220
  5300/100000: episode: 53, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 51.218, mean reward: 0.512 [0.231, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.909, 10.312], loss: 0.016932, mae: 0.135727, mean_q: 17.050179
  5400/100000: episode: 54, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 51.627, mean reward: 0.516 [0.210, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.528, 10.098], loss: 0.016877, mae: 0.130509, mean_q: 17.030680
  5500/100000: episode: 55, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 53.596, mean reward: 0.536 [0.231, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.267, 10.126], loss: 0.018508, mae: 0.135334, mean_q: 17.284687
  5600/100000: episode: 56, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 54.632, mean reward: 0.546 [0.346, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.039, 10.271], loss: 0.017057, mae: 0.128753, mean_q: 17.326532
  5700/100000: episode: 57, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 54.976, mean reward: 0.550 [0.307, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.105, 10.333], loss: 0.016161, mae: 0.136297, mean_q: 17.577635
  5800/100000: episode: 58, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 52.861, mean reward: 0.529 [0.329, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.834, 10.098], loss: 0.018539, mae: 0.141020, mean_q: 17.530876
  5900/100000: episode: 59, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 56.001, mean reward: 0.560 [0.373, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.259, 10.175], loss: 0.013155, mae: 0.121897, mean_q: 17.977987
  6000/100000: episode: 60, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 54.014, mean reward: 0.540 [0.244, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.694, 10.242], loss: 0.014896, mae: 0.131327, mean_q: 17.953754
  6100/100000: episode: 61, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 53.943, mean reward: 0.539 [0.298, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.759, 10.358], loss: 0.015184, mae: 0.131441, mean_q: 18.084082
  6200/100000: episode: 62, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.004, mean reward: 0.570 [0.409, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.396, 10.098], loss: 0.016188, mae: 0.137783, mean_q: 18.079123
  6300/100000: episode: 63, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 47.683, mean reward: 0.477 [0.184, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.880, 10.281], loss: 0.015117, mae: 0.131364, mean_q: 18.093422
  6400/100000: episode: 64, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 52.338, mean reward: 0.523 [0.336, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.209, 10.241], loss: 0.013946, mae: 0.126794, mean_q: 18.237564
  6500/100000: episode: 65, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 51.832, mean reward: 0.518 [0.224, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.487, 10.098], loss: 0.014328, mae: 0.128491, mean_q: 18.302849
  6600/100000: episode: 66, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 54.880, mean reward: 0.549 [0.323, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.253, 10.098], loss: 0.015837, mae: 0.135608, mean_q: 18.512239
  6700/100000: episode: 67, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 53.330, mean reward: 0.533 [0.218, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.444, 10.179], loss: 0.015226, mae: 0.131346, mean_q: 18.496496
  6800/100000: episode: 68, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 53.613, mean reward: 0.536 [0.350, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.056, 10.217], loss: 0.015878, mae: 0.136696, mean_q: 18.557192
  6900/100000: episode: 69, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 56.083, mean reward: 0.561 [0.198, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.044, 10.098], loss: 0.016268, mae: 0.138168, mean_q: 18.523060
  7000/100000: episode: 70, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 50.466, mean reward: 0.505 [0.309, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.030, 10.098], loss: 0.016877, mae: 0.139042, mean_q: 18.854332
  7100/100000: episode: 71, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 50.772, mean reward: 0.508 [0.268, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.630, 10.276], loss: 0.017304, mae: 0.144641, mean_q: 18.931528
  7200/100000: episode: 72, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 51.506, mean reward: 0.515 [0.316, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.738, 10.244], loss: 0.019707, mae: 0.152490, mean_q: 18.962263
  7300/100000: episode: 73, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 54.203, mean reward: 0.542 [0.193, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.484, 10.098], loss: 0.016484, mae: 0.140270, mean_q: 19.105976
  7400/100000: episode: 74, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 54.473, mean reward: 0.545 [0.351, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.296, 10.098], loss: 0.017665, mae: 0.142379, mean_q: 19.187897
  7500/100000: episode: 75, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 52.972, mean reward: 0.530 [0.310, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.710, 10.098], loss: 0.014580, mae: 0.131553, mean_q: 19.187883
  7600/100000: episode: 76, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 52.567, mean reward: 0.526 [0.303, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.787, 10.257], loss: 0.015280, mae: 0.133922, mean_q: 19.467613
  7700/100000: episode: 77, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 55.393, mean reward: 0.554 [0.307, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.608, 10.195], loss: 0.015598, mae: 0.135848, mean_q: 18.977760
  7800/100000: episode: 78, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 55.320, mean reward: 0.553 [0.284, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.955, 10.098], loss: 0.015065, mae: 0.131828, mean_q: 19.424118
  7900/100000: episode: 79, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 53.145, mean reward: 0.531 [0.343, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.972, 10.312], loss: 0.016350, mae: 0.139783, mean_q: 19.430998
  8000/100000: episode: 80, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 51.746, mean reward: 0.517 [0.240, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.827, 10.303], loss: 0.013486, mae: 0.125773, mean_q: 19.139814
  8100/100000: episode: 81, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 44.886, mean reward: 0.449 [0.195, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.356, 10.098], loss: 0.013791, mae: 0.126774, mean_q: 19.237354
  8200/100000: episode: 82, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 54.515, mean reward: 0.545 [0.243, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.727, 10.098], loss: 0.016445, mae: 0.139567, mean_q: 19.333055
  8300/100000: episode: 83, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 54.516, mean reward: 0.545 [0.344, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.376, 10.098], loss: 0.015437, mae: 0.137326, mean_q: 19.227985
  8400/100000: episode: 84, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 54.785, mean reward: 0.548 [0.235, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.162, 10.223], loss: 0.014434, mae: 0.129084, mean_q: 19.521591
  8500/100000: episode: 85, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.123, mean reward: 0.561 [0.358, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.451, 10.232], loss: 0.015582, mae: 0.134630, mean_q: 19.344978
  8600/100000: episode: 86, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 52.264, mean reward: 0.523 [0.301, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.029, 10.441], loss: 0.015854, mae: 0.138266, mean_q: 19.474434
  8700/100000: episode: 87, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 56.562, mean reward: 0.566 [0.341, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.459, 10.098], loss: 0.015026, mae: 0.134829, mean_q: 19.673294
  8800/100000: episode: 88, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 56.331, mean reward: 0.563 [0.232, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.859, 10.098], loss: 0.018074, mae: 0.147322, mean_q: 19.564867
  8900/100000: episode: 89, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 52.296, mean reward: 0.523 [0.341, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.627, 10.178], loss: 0.016638, mae: 0.140636, mean_q: 19.694494
  9000/100000: episode: 90, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 54.361, mean reward: 0.544 [0.327, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.904, 10.227], loss: 0.015985, mae: 0.136318, mean_q: 19.432144
  9100/100000: episode: 91, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 56.203, mean reward: 0.562 [0.381, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.803, 10.098], loss: 0.017325, mae: 0.143202, mean_q: 19.229506
  9200/100000: episode: 92, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 51.420, mean reward: 0.514 [0.278, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.788, 10.295], loss: 0.014995, mae: 0.134166, mean_q: 19.749060
  9300/100000: episode: 93, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 50.229, mean reward: 0.502 [0.218, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.148, 10.340], loss: 0.015430, mae: 0.136509, mean_q: 19.940392
  9400/100000: episode: 94, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 50.895, mean reward: 0.509 [0.183, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.090, 10.098], loss: 0.015302, mae: 0.135001, mean_q: 19.644583
  9500/100000: episode: 95, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 56.140, mean reward: 0.561 [0.358, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.810, 10.098], loss: 0.015484, mae: 0.135323, mean_q: 19.778122
  9600/100000: episode: 96, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 57.381, mean reward: 0.574 [0.238, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.473, 10.098], loss: 0.013288, mae: 0.126331, mean_q: 19.408388
  9700/100000: episode: 97, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 54.555, mean reward: 0.546 [0.234, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.580, 10.098], loss: 0.015333, mae: 0.136110, mean_q: 19.632824
  9800/100000: episode: 98, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 55.547, mean reward: 0.555 [0.337, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.146, 10.098], loss: 0.014951, mae: 0.133289, mean_q: 19.826170
  9900/100000: episode: 99, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 54.731, mean reward: 0.547 [0.235, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.028, 10.099], loss: 0.014073, mae: 0.130423, mean_q: 19.963440
 10000/100000: episode: 100, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 48.897, mean reward: 0.489 [0.282, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.194, 10.468], loss: 0.015604, mae: 0.134667, mean_q: 19.311529
 10100/100000: episode: 101, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 53.144, mean reward: 0.531 [0.285, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.706, 10.480], loss: 0.014895, mae: 0.132479, mean_q: 19.797722
 10200/100000: episode: 102, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 56.337, mean reward: 0.563 [0.347, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.813, 10.275], loss: 0.014701, mae: 0.132425, mean_q: 19.812798
 10300/100000: episode: 103, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 55.165, mean reward: 0.552 [0.318, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.982, 10.098], loss: 0.013442, mae: 0.126623, mean_q: 19.914867
 10400/100000: episode: 104, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 52.212, mean reward: 0.522 [0.231, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.050, 10.098], loss: 0.012995, mae: 0.123727, mean_q: 19.768005
 10500/100000: episode: 105, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 48.081, mean reward: 0.481 [0.251, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.582, 10.098], loss: 0.014029, mae: 0.130285, mean_q: 19.481571
 10600/100000: episode: 106, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 56.687, mean reward: 0.567 [0.241, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.439, 10.098], loss: 0.013805, mae: 0.128564, mean_q: 19.702662
 10700/100000: episode: 107, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 50.775, mean reward: 0.508 [0.319, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.098], loss: 0.014698, mae: 0.131498, mean_q: 19.955269
 10800/100000: episode: 108, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 55.134, mean reward: 0.551 [0.343, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.258], loss: 0.014868, mae: 0.132499, mean_q: 19.665577
 10900/100000: episode: 109, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.523, mean reward: 0.575 [0.261, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.506, 10.098], loss: 0.012881, mae: 0.124386, mean_q: 20.341688
 11000/100000: episode: 110, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 51.687, mean reward: 0.517 [0.307, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.845, 10.125], loss: 0.014243, mae: 0.130723, mean_q: 19.995806
 11100/100000: episode: 111, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 57.103, mean reward: 0.571 [0.363, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.269, 10.098], loss: 0.013576, mae: 0.127286, mean_q: 19.337990
 11200/100000: episode: 112, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 51.066, mean reward: 0.511 [0.275, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.838, 10.098], loss: 0.013317, mae: 0.125843, mean_q: 19.675772
 11300/100000: episode: 113, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 52.195, mean reward: 0.522 [0.250, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.117, 10.098], loss: 0.013236, mae: 0.126891, mean_q: 19.703671
 11400/100000: episode: 114, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 55.202, mean reward: 0.552 [0.334, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.429, 10.098], loss: 0.013419, mae: 0.125651, mean_q: 19.847191
 11500/100000: episode: 115, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 54.985, mean reward: 0.550 [0.269, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.285, 10.314], loss: 0.013872, mae: 0.127998, mean_q: 19.231325
 11600/100000: episode: 116, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 55.402, mean reward: 0.554 [0.239, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.366, 10.098], loss: 0.014445, mae: 0.132147, mean_q: 19.631369
 11700/100000: episode: 117, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.316, mean reward: 0.583 [0.418, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.014, 10.107], loss: 0.013701, mae: 0.126854, mean_q: 19.626081
 11800/100000: episode: 118, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 55.524, mean reward: 0.555 [0.240, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.570, 10.103], loss: 0.012095, mae: 0.119121, mean_q: 19.612970
 11900/100000: episode: 119, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 49.661, mean reward: 0.497 [0.201, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.604, 10.098], loss: 0.012535, mae: 0.122858, mean_q: 19.493423
 12000/100000: episode: 120, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 56.224, mean reward: 0.562 [0.384, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.915, 10.098], loss: 0.011664, mae: 0.118628, mean_q: 19.641136
 12100/100000: episode: 121, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 49.253, mean reward: 0.493 [0.335, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.032, 10.098], loss: 0.012224, mae: 0.121994, mean_q: 19.944643
 12200/100000: episode: 122, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 53.939, mean reward: 0.539 [0.343, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.852, 10.098], loss: 0.011728, mae: 0.117444, mean_q: 19.673243
 12300/100000: episode: 123, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: 50.965, mean reward: 0.510 [0.166, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.749, 10.165], loss: 0.013036, mae: 0.123920, mean_q: 19.329182
 12400/100000: episode: 124, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: 52.954, mean reward: 0.530 [0.328, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.771, 10.098], loss: 0.011292, mae: 0.116775, mean_q: 19.552399
 12500/100000: episode: 125, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 55.772, mean reward: 0.558 [0.363, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.305, 10.098], loss: 0.013970, mae: 0.130493, mean_q: 19.662302
 12600/100000: episode: 126, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward: 52.997, mean reward: 0.530 [0.312, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.806, 10.176], loss: 0.012275, mae: 0.121351, mean_q: 19.922270
 12700/100000: episode: 127, duration: 0.840s, episode steps: 100, steps per second: 119, episode reward: 55.347, mean reward: 0.553 [0.361, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.852, 10.175], loss: 0.011742, mae: 0.119861, mean_q: 19.912363
 12800/100000: episode: 128, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 47.159, mean reward: 0.472 [0.285, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.400, 10.098], loss: 0.011881, mae: 0.118885, mean_q: 19.609194
 12900/100000: episode: 129, duration: 0.812s, episode steps: 100, steps per second: 123, episode reward: 54.581, mean reward: 0.546 [0.286, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.353, 10.200], loss: 0.017734, mae: 0.147933, mean_q: 19.704889
 13000/100000: episode: 130, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: 53.243, mean reward: 0.532 [0.333, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.344, 10.365], loss: 0.011710, mae: 0.119423, mean_q: 19.797125
 13100/100000: episode: 131, duration: 0.938s, episode steps: 100, steps per second: 107, episode reward: 51.035, mean reward: 0.510 [0.296, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.972, 10.233], loss: 0.012870, mae: 0.122846, mean_q: 19.430315
 13200/100000: episode: 132, duration: 1.095s, episode steps: 100, steps per second: 91, episode reward: 49.126, mean reward: 0.491 [0.171, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.943, 10.098], loss: 0.012530, mae: 0.122775, mean_q: 19.587330
 13300/100000: episode: 133, duration: 1.093s, episode steps: 100, steps per second: 92, episode reward: 54.990, mean reward: 0.550 [0.389, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.054, 10.307], loss: 0.013384, mae: 0.124896, mean_q: 19.805746
 13400/100000: episode: 134, duration: 0.946s, episode steps: 100, steps per second: 106, episode reward: 55.112, mean reward: 0.551 [0.330, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.352, 10.130], loss: 0.011849, mae: 0.119063, mean_q: 19.919127
 13500/100000: episode: 135, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 54.503, mean reward: 0.545 [0.329, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.827, 10.098], loss: 0.012768, mae: 0.123990, mean_q: 19.690262
 13600/100000: episode: 136, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 53.109, mean reward: 0.531 [0.350, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.886, 10.247], loss: 0.012287, mae: 0.121030, mean_q: 19.760340
 13700/100000: episode: 137, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: 52.399, mean reward: 0.524 [0.324, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.177, 10.279], loss: 0.012817, mae: 0.123565, mean_q: 19.679214
 13800/100000: episode: 138, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 51.215, mean reward: 0.512 [0.341, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.158, 10.098], loss: 0.014317, mae: 0.132783, mean_q: 20.025734
 13900/100000: episode: 139, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 52.265, mean reward: 0.523 [0.378, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.619, 10.277], loss: 0.013754, mae: 0.129874, mean_q: 20.031391
 14000/100000: episode: 140, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 52.133, mean reward: 0.521 [0.335, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.470, 10.098], loss: 0.013195, mae: 0.124636, mean_q: 19.636103
 14100/100000: episode: 141, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.181, mean reward: 0.582 [0.366, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.382, 10.098], loss: 0.012585, mae: 0.121067, mean_q: 19.780258
 14200/100000: episode: 142, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 54.885, mean reward: 0.549 [0.325, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.409, 10.098], loss: 0.012992, mae: 0.122182, mean_q: 20.092239
 14300/100000: episode: 143, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 50.079, mean reward: 0.501 [0.224, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.367, 10.307], loss: 0.014110, mae: 0.129361, mean_q: 19.768679
 14400/100000: episode: 144, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 54.499, mean reward: 0.545 [0.313, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.285, 10.175], loss: 0.012840, mae: 0.122716, mean_q: 19.834677
 14500/100000: episode: 145, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 53.811, mean reward: 0.538 [0.260, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.524, 10.357], loss: 0.011728, mae: 0.118162, mean_q: 19.670591
 14600/100000: episode: 146, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 53.214, mean reward: 0.532 [0.312, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.353, 10.098], loss: 0.014050, mae: 0.129863, mean_q: 19.666908
 14700/100000: episode: 147, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 55.183, mean reward: 0.552 [0.325, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.618, 10.130], loss: 0.015407, mae: 0.135124, mean_q: 19.944883
 14800/100000: episode: 148, duration: 1.031s, episode steps: 100, steps per second: 97, episode reward: 53.816, mean reward: 0.538 [0.333, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.490, 10.098], loss: 0.012670, mae: 0.121571, mean_q: 19.851442
 14900/100000: episode: 149, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 52.747, mean reward: 0.527 [0.341, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.382, 10.098], loss: 0.014876, mae: 0.133748, mean_q: 19.505690
[Info] New level: 0.48159265518188477 | Considering 10/90 traces
 15000/100000: episode: 150, duration: 6.056s, episode steps: 100, steps per second: 17, episode reward: 51.158, mean reward: 0.512 [0.234, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.082, 10.279], loss: 0.014584, mae: 0.131697, mean_q: 19.601452
 15002/100000: episode: 151, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.859, mean reward: 0.430 [0.414, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.193, 10.100], loss: 0.009809, mae: 0.110309, mean_q: 18.960167
 15004/100000: episode: 152, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.940, mean reward: 0.470 [0.437, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.238, 10.100], loss: 0.012904, mae: 0.123300, mean_q: 21.154406
 15007/100000: episode: 153, duration: 0.021s, episode steps: 3, steps per second: 143, episode reward: 0.954, mean reward: 0.318 [0.301, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.357, 10.100], loss: 0.009247, mae: 0.102835, mean_q: 19.746550
 15009/100000: episode: 154, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 1.190, mean reward: 0.595 [0.585, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.209, 10.100], loss: 0.009715, mae: 0.111640, mean_q: 19.596098
 15012/100000: episode: 155, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 0.920, mean reward: 0.307 [0.291, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.333, 10.100], loss: 0.007805, mae: 0.094888, mean_q: 19.827385
 15015/100000: episode: 156, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 1.123, mean reward: 0.374 [0.315, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.262, 10.100], loss: 0.008414, mae: 0.100877, mean_q: 20.156693
 15018/100000: episode: 157, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 0.666, mean reward: 0.222 [0.175, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.367, 10.100], loss: 0.011158, mae: 0.115510, mean_q: 21.873514
 15021/100000: episode: 158, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 0.883, mean reward: 0.294 [0.219, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.269, 10.100], loss: 0.011251, mae: 0.117242, mean_q: 19.073008
 15023/100000: episode: 159, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 1.047, mean reward: 0.523 [0.515, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.110, 10.100], loss: 0.014439, mae: 0.137620, mean_q: 21.373358
 15025/100000: episode: 160, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.899, mean reward: 0.450 [0.443, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.219, 10.100], loss: 0.012562, mae: 0.117646, mean_q: 19.986404
 15027/100000: episode: 161, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.095, mean reward: 0.547 [0.508, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.241, 10.100], loss: 0.009225, mae: 0.097796, mean_q: 17.908791
 15029/100000: episode: 162, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.876, mean reward: 0.438 [0.376, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.183, 10.100], loss: 0.010498, mae: 0.111070, mean_q: 18.775284
 15032/100000: episode: 163, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 0.847, mean reward: 0.282 [0.238, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.227, 10.100], loss: 0.013283, mae: 0.127501, mean_q: 21.341898
 15034/100000: episode: 164, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.093, mean reward: 0.546 [0.510, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.181, 10.100], loss: 0.016601, mae: 0.145578, mean_q: 18.772659
 15036/100000: episode: 165, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.923, mean reward: 0.461 [0.449, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.187, 10.100], loss: 0.011817, mae: 0.120198, mean_q: 18.117815
 15038/100000: episode: 166, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.998, mean reward: 0.499 [0.487, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.076, 10.100], loss: 0.019788, mae: 0.157450, mean_q: 19.288570
 15040/100000: episode: 167, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 1.104, mean reward: 0.552 [0.505, 0.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.176, 10.100], loss: 0.010737, mae: 0.122551, mean_q: 18.690998
 15042/100000: episode: 168, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.161, mean reward: 0.580 [0.550, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.536, 10.100], loss: 0.022943, mae: 0.186144, mean_q: 20.371836
 15044/100000: episode: 169, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 1.283, mean reward: 0.642 [0.638, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.189, 10.100], loss: 0.014736, mae: 0.140224, mean_q: 16.892570
 15046/100000: episode: 170, duration: 0.021s, episode steps: 2, steps per second: 93, episode reward: 1.166, mean reward: 0.583 [0.557, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.209, 10.100], loss: 0.010405, mae: 0.110776, mean_q: 21.840149
 15048/100000: episode: 171, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.917, mean reward: 0.459 [0.433, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.227, 10.100], loss: 0.015850, mae: 0.123911, mean_q: 19.184704
 15050/100000: episode: 172, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.906, mean reward: 0.453 [0.452, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.282, 10.100], loss: 0.013787, mae: 0.142326, mean_q: 21.468149
 15052/100000: episode: 173, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 1.021, mean reward: 0.510 [0.507, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.205, 10.100], loss: 0.013032, mae: 0.125813, mean_q: 21.296679
 15054/100000: episode: 174, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 1.084, mean reward: 0.542 [0.507, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.286, 10.100], loss: 0.009650, mae: 0.118147, mean_q: 19.687891
 15056/100000: episode: 175, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.709, mean reward: 0.354 [0.323, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.212, 10.100], loss: 0.013257, mae: 0.128902, mean_q: 21.986759
 15059/100000: episode: 176, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 0.827, mean reward: 0.276 [0.259, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.713, 10.100], loss: 0.014030, mae: 0.132642, mean_q: 19.118929
 15061/100000: episode: 177, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.956, mean reward: 0.478 [0.447, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.175, 10.100], loss: 0.014541, mae: 0.143908, mean_q: 17.986355
 15063/100000: episode: 178, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 0.928, mean reward: 0.464 [0.457, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.140, 10.100], loss: 0.008289, mae: 0.102302, mean_q: 17.738150
 15065/100000: episode: 179, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 1.176, mean reward: 0.588 [0.562, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.122, 10.100], loss: 0.014181, mae: 0.135234, mean_q: 18.146856
 15067/100000: episode: 180, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.861, mean reward: 0.430 [0.418, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.248, 10.100], loss: 0.014258, mae: 0.139047, mean_q: 19.846767
 15070/100000: episode: 181, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 0.983, mean reward: 0.328 [0.323, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.289, 10.100], loss: 0.012391, mae: 0.120268, mean_q: 19.141260
 15073/100000: episode: 182, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 1.040, mean reward: 0.347 [0.309, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.262, 10.100], loss: 0.013636, mae: 0.126887, mean_q: 20.117538
 15075/100000: episode: 183, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.068, mean reward: 0.534 [0.493, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.220, 10.100], loss: 0.010137, mae: 0.110464, mean_q: 21.841011
 15077/100000: episode: 184, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.954, mean reward: 0.477 [0.448, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.166, 10.100], loss: 0.013455, mae: 0.129336, mean_q: 19.799431
 15079/100000: episode: 185, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.060, mean reward: 0.530 [0.525, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.290, 10.100], loss: 0.013342, mae: 0.136106, mean_q: 20.835773
 15081/100000: episode: 186, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.999, mean reward: 0.499 [0.492, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.108, 10.100], loss: 0.014925, mae: 0.136930, mean_q: 21.253010
 15084/100000: episode: 187, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 1.113, mean reward: 0.371 [0.352, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.312, 10.100], loss: 0.009877, mae: 0.111442, mean_q: 20.655878
 15086/100000: episode: 188, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 1.159, mean reward: 0.580 [0.575, 0.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.202, 10.100], loss: 0.009409, mae: 0.103555, mean_q: 17.602173
 15088/100000: episode: 189, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 1.078, mean reward: 0.539 [0.530, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.269, 10.100], loss: 0.009835, mae: 0.105232, mean_q: 20.346558
 15090/100000: episode: 190, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.899, mean reward: 0.449 [0.372, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.300, 10.100], loss: 0.012721, mae: 0.133336, mean_q: 23.231583
 15093/100000: episode: 191, duration: 0.024s, episode steps: 3, steps per second: 126, episode reward: 1.123, mean reward: 0.374 [0.346, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.230, 10.100], loss: 0.012305, mae: 0.110812, mean_q: 20.624046
 15095/100000: episode: 192, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.971, mean reward: 0.486 [0.440, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.121, 10.100], loss: 0.011392, mae: 0.115604, mean_q: 20.435547
 15097/100000: episode: 193, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.990, mean reward: 0.495 [0.468, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.162, 10.100], loss: 0.015997, mae: 0.141163, mean_q: 20.071980
 15099/100000: episode: 194, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.003, mean reward: 0.502 [0.491, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.206, 10.100], loss: 0.014471, mae: 0.124413, mean_q: 16.146595
 15102/100000: episode: 195, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 0.949, mean reward: 0.316 [0.294, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.277, 10.100], loss: 0.012725, mae: 0.134478, mean_q: 20.170593
 15104/100000: episode: 196, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.164, mean reward: 0.582 [0.520, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.233, 10.100], loss: 0.013456, mae: 0.132473, mean_q: 21.099705
 15106/100000: episode: 197, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.649, mean reward: 0.324 [0.251, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.223, 10.100], loss: 0.015241, mae: 0.129966, mean_q: 19.035957
 15108/100000: episode: 198, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 1.056, mean reward: 0.528 [0.492, 0.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.225, 10.100], loss: 0.014521, mae: 0.122989, mean_q: 20.032070
 15110/100000: episode: 199, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.827, mean reward: 0.414 [0.389, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.201, 10.100], loss: 0.011070, mae: 0.121672, mean_q: 18.237400
 15113/100000: episode: 200, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 0.936, mean reward: 0.312 [0.264, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.391, 10.100], loss: 0.013576, mae: 0.121299, mean_q: 19.132837
 15115/100000: episode: 201, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 1.024, mean reward: 0.512 [0.495, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.225, 10.100], loss: 0.008841, mae: 0.107681, mean_q: 19.050547
 15117/100000: episode: 202, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 1.197, mean reward: 0.599 [0.550, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.200, 10.100], loss: 0.017609, mae: 0.139793, mean_q: 17.852398
 15119/100000: episode: 203, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.035, mean reward: 0.517 [0.487, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.197, 10.100], loss: 0.012811, mae: 0.120158, mean_q: 19.591518
 15121/100000: episode: 204, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.944, mean reward: 0.472 [0.415, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.237, 10.100], loss: 0.013268, mae: 0.123542, mean_q: 19.936192
 15123/100000: episode: 205, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.732, mean reward: 0.366 [0.294, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.234, 10.100], loss: 0.015690, mae: 0.143611, mean_q: 19.992083
 15125/100000: episode: 206, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 1.219, mean reward: 0.610 [0.555, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.245, 10.100], loss: 0.011045, mae: 0.112415, mean_q: 18.742119
 15127/100000: episode: 207, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.996, mean reward: 0.498 [0.498, 0.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.141, 10.100], loss: 0.015036, mae: 0.125123, mean_q: 19.824600
 15129/100000: episode: 208, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.202, mean reward: 0.601 [0.552, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.264, 10.100], loss: 0.014268, mae: 0.120711, mean_q: 20.066854
 15131/100000: episode: 209, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.919, mean reward: 0.460 [0.446, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.237, 10.100], loss: 0.007209, mae: 0.096227, mean_q: 19.648842
 15134/100000: episode: 210, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 0.723, mean reward: 0.241 [0.219, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.379, 10.100], loss: 0.013227, mae: 0.128178, mean_q: 19.216866
 15137/100000: episode: 211, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 0.669, mean reward: 0.223 [0.199, 0.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.367, 10.100], loss: 0.015068, mae: 0.136819, mean_q: 19.992064
 15139/100000: episode: 212, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.929, mean reward: 0.465 [0.388, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.337, 10.100], loss: 0.014462, mae: 0.138688, mean_q: 22.042238
 15142/100000: episode: 213, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 1.113, mean reward: 0.371 [0.368, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.275, 10.100], loss: 0.011785, mae: 0.111727, mean_q: 18.836525
 15144/100000: episode: 214, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 1.030, mean reward: 0.515 [0.504, 0.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.207, 10.100], loss: 0.010506, mae: 0.108625, mean_q: 19.283886
 15146/100000: episode: 215, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.973, mean reward: 0.486 [0.397, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.260, 10.100], loss: 0.012694, mae: 0.117461, mean_q: 17.476135
 15148/100000: episode: 216, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.153, mean reward: 0.577 [0.529, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.306, 10.100], loss: 0.007655, mae: 0.093601, mean_q: 18.682238
 15150/100000: episode: 217, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.869, mean reward: 0.435 [0.429, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.253, 10.100], loss: 0.008717, mae: 0.101123, mean_q: 19.864237
 15152/100000: episode: 218, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 1.007, mean reward: 0.503 [0.445, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.121, 10.100], loss: 0.009859, mae: 0.116601, mean_q: 17.613106
 15154/100000: episode: 219, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 1.129, mean reward: 0.565 [0.550, 0.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.123, 10.100], loss: 0.010352, mae: 0.112450, mean_q: 18.918816
 15156/100000: episode: 220, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 1.222, mean reward: 0.611 [0.608, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.110, 10.100], loss: 0.008446, mae: 0.097936, mean_q: 19.136646
 15158/100000: episode: 221, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.111, mean reward: 0.555 [0.554, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.148, 10.100], loss: 0.013512, mae: 0.132898, mean_q: 17.743084
 15160/100000: episode: 222, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.988, mean reward: 0.494 [0.478, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.231, 10.100], loss: 0.018817, mae: 0.146488, mean_q: 18.168631
 15162/100000: episode: 223, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.846, mean reward: 0.423 [0.416, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.203, 10.100], loss: 0.009610, mae: 0.106500, mean_q: 21.018188
 15164/100000: episode: 224, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 1.130, mean reward: 0.565 [0.535, 0.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.259, 10.100], loss: 0.011520, mae: 0.112777, mean_q: 21.451399
 15166/100000: episode: 225, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 1.090, mean reward: 0.545 [0.513, 0.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.237, 10.100], loss: 0.013682, mae: 0.114393, mean_q: 18.970623
 15168/100000: episode: 226, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.144, mean reward: 0.572 [0.521, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.228, 10.100], loss: 0.009014, mae: 0.110953, mean_q: 19.257675
 15170/100000: episode: 227, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.860, mean reward: 0.430 [0.399, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.233, 10.100], loss: 0.008969, mae: 0.105765, mean_q: 17.644890
 15172/100000: episode: 228, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.832, mean reward: 0.416 [0.400, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.155, 10.100], loss: 0.012900, mae: 0.129840, mean_q: 19.317961
 15174/100000: episode: 229, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.937, mean reward: 0.469 [0.467, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.176, 10.100], loss: 0.016723, mae: 0.147056, mean_q: 17.964722
 15176/100000: episode: 230, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.076, mean reward: 0.538 [0.528, 0.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.253, 10.100], loss: 0.012302, mae: 0.123904, mean_q: 18.255211
 15178/100000: episode: 231, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.903, mean reward: 0.452 [0.380, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.236, 10.105], loss: 0.009299, mae: 0.113769, mean_q: 20.182993
 15181/100000: episode: 232, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 1.141, mean reward: 0.380 [0.339, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.672, 10.100], loss: 0.010582, mae: 0.111568, mean_q: 18.819185
 15183/100000: episode: 233, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.859, mean reward: 0.430 [0.408, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.233, 10.100], loss: 0.011928, mae: 0.122954, mean_q: 17.905525
 15185/100000: episode: 234, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 1.109, mean reward: 0.555 [0.522, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.152, 10.100], loss: 0.014909, mae: 0.132998, mean_q: 19.406067
 15187/100000: episode: 235, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.123, mean reward: 0.562 [0.517, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.770, 10.100], loss: 0.015034, mae: 0.136341, mean_q: 18.779463
 15189/100000: episode: 236, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.913, mean reward: 0.457 [0.453, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.296, 10.100], loss: 0.012314, mae: 0.125734, mean_q: 19.153830
 15191/100000: episode: 237, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.644, mean reward: 0.322 [0.269, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.567, 10.100], loss: 0.016911, mae: 0.139979, mean_q: 18.843817
 15193/100000: episode: 238, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 1.037, mean reward: 0.518 [0.515, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.096, 10.100], loss: 0.017434, mae: 0.140133, mean_q: 16.784229
 15195/100000: episode: 239, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 1.157, mean reward: 0.578 [0.567, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.289, 10.100], loss: 0.020840, mae: 0.151882, mean_q: 20.481449
[Info] Not found new level, current best level reached = 0.48159265518188477
 15197/100000: episode: 240, duration: 4.276s, episode steps: 2, steps per second: 0, episode reward: 1.191, mean reward: 0.595 [0.595, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.128, 10.100], loss: 0.014330, mae: 0.138929, mean_q: 19.700272
 15297/100000: episode: 241, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 46.576, mean reward: 0.466 [0.259, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.803, 10.098], loss: 0.013727, mae: 0.127064, mean_q: 18.643299
 15397/100000: episode: 242, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 53.061, mean reward: 0.531 [0.329, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.477, 10.107], loss: 0.014704, mae: 0.131980, mean_q: 19.047052
 15497/100000: episode: 243, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 53.545, mean reward: 0.535 [0.346, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.152, 10.182], loss: 0.013573, mae: 0.125245, mean_q: 18.803267
 15597/100000: episode: 244, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 52.395, mean reward: 0.524 [0.224, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.442, 10.185], loss: 0.015449, mae: 0.134648, mean_q: 18.818810
 15697/100000: episode: 245, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.776, mean reward: 0.548 [0.311, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.038, 10.217], loss: 0.015155, mae: 0.132824, mean_q: 19.100348
 15797/100000: episode: 246, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 55.176, mean reward: 0.552 [0.241, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.128, 10.111], loss: 0.014318, mae: 0.128411, mean_q: 18.729473
 15897/100000: episode: 247, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 52.164, mean reward: 0.522 [0.281, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.812, 10.098], loss: 0.014038, mae: 0.126489, mean_q: 18.938189
 15997/100000: episode: 248, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 55.512, mean reward: 0.555 [0.248, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.805, 10.098], loss: 0.014883, mae: 0.131734, mean_q: 19.075438
 16097/100000: episode: 249, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 54.182, mean reward: 0.542 [0.225, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.602, 10.098], loss: 0.013963, mae: 0.126180, mean_q: 18.769554
 16197/100000: episode: 250, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 54.753, mean reward: 0.548 [0.302, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.682, 10.098], loss: 0.017300, mae: 0.139740, mean_q: 19.191917
 16297/100000: episode: 251, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 51.750, mean reward: 0.518 [0.276, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.361, 10.120], loss: 0.014523, mae: 0.129635, mean_q: 19.229507
 16397/100000: episode: 252, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 51.260, mean reward: 0.513 [0.350, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.098], loss: 0.015174, mae: 0.131850, mean_q: 18.787285
 16497/100000: episode: 253, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 54.950, mean reward: 0.549 [0.276, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.093, 10.098], loss: 0.013326, mae: 0.121258, mean_q: 19.128229
 16597/100000: episode: 254, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 56.122, mean reward: 0.561 [0.377, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.346, 10.322], loss: 0.013773, mae: 0.126127, mean_q: 18.893415
 16697/100000: episode: 255, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 53.091, mean reward: 0.531 [0.223, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.338, 10.186], loss: 0.015429, mae: 0.134271, mean_q: 19.273932
 16797/100000: episode: 256, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 53.018, mean reward: 0.530 [0.257, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.896, 10.281], loss: 0.015155, mae: 0.130221, mean_q: 18.741314
 16897/100000: episode: 257, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 52.641, mean reward: 0.526 [0.236, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.051, 10.249], loss: 0.014339, mae: 0.129588, mean_q: 18.993420
 16997/100000: episode: 258, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 56.935, mean reward: 0.569 [0.349, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.196, 10.111], loss: 0.014065, mae: 0.126905, mean_q: 18.802607
 17097/100000: episode: 259, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 52.893, mean reward: 0.529 [0.327, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.648, 10.216], loss: 0.018354, mae: 0.147577, mean_q: 18.811872
 17197/100000: episode: 260, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 55.335, mean reward: 0.553 [0.336, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.872, 10.098], loss: 0.013561, mae: 0.125226, mean_q: 19.078541
 17297/100000: episode: 261, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 55.179, mean reward: 0.552 [0.292, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.534, 10.098], loss: 0.013905, mae: 0.125486, mean_q: 18.781477
 17397/100000: episode: 262, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 55.680, mean reward: 0.557 [0.379, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.794, 10.177], loss: 0.012290, mae: 0.118143, mean_q: 19.302425
 17497/100000: episode: 263, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 51.179, mean reward: 0.512 [0.358, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.352, 10.286], loss: 0.014701, mae: 0.130367, mean_q: 18.671112
 17597/100000: episode: 264, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 52.698, mean reward: 0.527 [0.175, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.699, 10.197], loss: 0.013059, mae: 0.122901, mean_q: 19.270231
 17697/100000: episode: 265, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 51.940, mean reward: 0.519 [0.292, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.045, 10.098], loss: 0.012941, mae: 0.121425, mean_q: 18.973707
 17797/100000: episode: 266, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 49.557, mean reward: 0.496 [0.288, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.822, 10.242], loss: 0.012034, mae: 0.116963, mean_q: 18.793201
 17897/100000: episode: 267, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 56.186, mean reward: 0.562 [0.258, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.969, 10.098], loss: 0.017767, mae: 0.145708, mean_q: 18.978649
 17997/100000: episode: 268, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.689, mean reward: 0.587 [0.420, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.786, 10.098], loss: 0.011982, mae: 0.118181, mean_q: 19.168764
 18097/100000: episode: 269, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 53.102, mean reward: 0.531 [0.221, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.702, 10.180], loss: 0.012754, mae: 0.120440, mean_q: 19.021124
 18197/100000: episode: 270, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 54.065, mean reward: 0.541 [0.202, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.410, 10.223], loss: 0.012166, mae: 0.118167, mean_q: 19.179512
 18297/100000: episode: 271, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 55.225, mean reward: 0.552 [0.324, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.202, 10.342], loss: 0.012811, mae: 0.118424, mean_q: 19.110716
 18397/100000: episode: 272, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 53.328, mean reward: 0.533 [0.230, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.168, 10.163], loss: 0.011430, mae: 0.115893, mean_q: 19.097229
 18497/100000: episode: 273, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 52.473, mean reward: 0.525 [0.308, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.943, 10.114], loss: 0.011411, mae: 0.113995, mean_q: 18.863419
 18597/100000: episode: 274, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 56.351, mean reward: 0.564 [0.332, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.186, 10.165], loss: 0.013538, mae: 0.124267, mean_q: 19.113375
 18697/100000: episode: 275, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 51.607, mean reward: 0.516 [0.203, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.948, 10.427], loss: 0.013336, mae: 0.127596, mean_q: 18.948906
 18797/100000: episode: 276, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 54.348, mean reward: 0.543 [0.313, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.859, 10.098], loss: 0.012023, mae: 0.116732, mean_q: 18.736557
 18897/100000: episode: 277, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 54.642, mean reward: 0.546 [0.320, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.387, 10.098], loss: 0.012843, mae: 0.120982, mean_q: 18.837679
 18997/100000: episode: 278, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 49.217, mean reward: 0.492 [0.324, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.090, 10.098], loss: 0.012960, mae: 0.122900, mean_q: 19.196165
 19097/100000: episode: 279, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 55.478, mean reward: 0.555 [0.379, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.350, 10.253], loss: 0.012602, mae: 0.116851, mean_q: 18.779854
 19197/100000: episode: 280, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: 52.921, mean reward: 0.529 [0.288, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.066, 10.098], loss: 0.014807, mae: 0.131555, mean_q: 19.044575
 19297/100000: episode: 281, duration: 0.660s, episode steps: 100, steps per second: 151, episode reward: 52.042, mean reward: 0.520 [0.259, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.187, 10.242], loss: 0.012054, mae: 0.118021, mean_q: 19.152197
 19397/100000: episode: 282, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 53.883, mean reward: 0.539 [0.333, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.781, 10.127], loss: 0.011593, mae: 0.114645, mean_q: 19.105062
 19497/100000: episode: 283, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 54.497, mean reward: 0.545 [0.251, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.807, 10.098], loss: 0.009688, mae: 0.105994, mean_q: 18.907665
 19597/100000: episode: 284, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 52.062, mean reward: 0.521 [0.321, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.504, 10.169], loss: 0.011402, mae: 0.114762, mean_q: 18.686203
 19697/100000: episode: 285, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 55.401, mean reward: 0.554 [0.401, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.184, 10.162], loss: 0.013793, mae: 0.127070, mean_q: 18.718689
 19797/100000: episode: 286, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 54.844, mean reward: 0.548 [0.196, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.857, 10.098], loss: 0.013349, mae: 0.124162, mean_q: 18.844681
 19897/100000: episode: 287, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.563, mean reward: 0.566 [0.270, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.380, 10.098], loss: 0.011123, mae: 0.112005, mean_q: 19.036758
 19997/100000: episode: 288, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 54.844, mean reward: 0.548 [0.302, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.043, 10.115], loss: 0.011843, mae: 0.117106, mean_q: 19.285419
 20097/100000: episode: 289, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 52.458, mean reward: 0.525 [0.334, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.788, 10.098], loss: 0.013727, mae: 0.123119, mean_q: 19.734312
 20197/100000: episode: 290, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 55.327, mean reward: 0.553 [0.217, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.655, 10.271], loss: 0.012266, mae: 0.118679, mean_q: 19.674103
 20297/100000: episode: 291, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 53.584, mean reward: 0.536 [0.296, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.148, 10.098], loss: 0.010051, mae: 0.107240, mean_q: 19.755972
 20397/100000: episode: 292, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 48.946, mean reward: 0.489 [0.298, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.256, 10.376], loss: 0.015728, mae: 0.136748, mean_q: 19.778305
 20497/100000: episode: 293, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 53.876, mean reward: 0.539 [0.336, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.241, 10.098], loss: 0.012558, mae: 0.119691, mean_q: 19.615681
 20597/100000: episode: 294, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 55.810, mean reward: 0.558 [0.340, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.077, 10.293], loss: 0.012120, mae: 0.119176, mean_q: 20.030350
 20697/100000: episode: 295, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 51.229, mean reward: 0.512 [0.321, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.349, 10.098], loss: 0.009838, mae: 0.106722, mean_q: 19.857384
 20797/100000: episode: 296, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 54.999, mean reward: 0.550 [0.294, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.961, 10.238], loss: 0.010367, mae: 0.108868, mean_q: 19.620974
 20897/100000: episode: 297, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 53.666, mean reward: 0.537 [0.236, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.098], loss: 0.010025, mae: 0.109295, mean_q: 19.906353
 20997/100000: episode: 298, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 53.730, mean reward: 0.537 [0.270, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.331, 10.098], loss: 0.010750, mae: 0.109634, mean_q: 20.008493
 21097/100000: episode: 299, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 57.141, mean reward: 0.571 [0.272, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.947, 10.098], loss: 0.011495, mae: 0.114783, mean_q: 20.012524
 21197/100000: episode: 300, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 54.792, mean reward: 0.548 [0.296, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.604, 10.267], loss: 0.012097, mae: 0.117422, mean_q: 19.987419
 21297/100000: episode: 301, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 53.108, mean reward: 0.531 [0.336, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.507, 10.098], loss: 0.009454, mae: 0.105013, mean_q: 20.187916
 21397/100000: episode: 302, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 53.324, mean reward: 0.533 [0.229, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.545, 10.374], loss: 0.010499, mae: 0.108813, mean_q: 20.029669
 21497/100000: episode: 303, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 52.396, mean reward: 0.524 [0.193, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.451, 10.098], loss: 0.009065, mae: 0.103503, mean_q: 19.707823
 21597/100000: episode: 304, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 46.906, mean reward: 0.469 [0.252, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.724, 10.266], loss: 0.011461, mae: 0.115252, mean_q: 20.059959
 21697/100000: episode: 305, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: 55.047, mean reward: 0.550 [0.291, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.607, 10.317], loss: 0.010498, mae: 0.108469, mean_q: 20.019041
 21797/100000: episode: 306, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 53.528, mean reward: 0.535 [0.228, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.787, 10.098], loss: 0.011285, mae: 0.113845, mean_q: 19.920450
 21897/100000: episode: 307, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.041, mean reward: 0.550 [0.309, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.276, 10.098], loss: 0.011957, mae: 0.117077, mean_q: 19.777040
 21997/100000: episode: 308, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 53.929, mean reward: 0.539 [0.261, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.184, 10.098], loss: 0.009746, mae: 0.105925, mean_q: 20.114302
 22097/100000: episode: 309, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 49.578, mean reward: 0.496 [0.269, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.747, 10.128], loss: 0.011116, mae: 0.109727, mean_q: 19.839869
 22197/100000: episode: 310, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 55.652, mean reward: 0.557 [0.364, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.199, 10.098], loss: 0.008984, mae: 0.101377, mean_q: 20.001331
 22297/100000: episode: 311, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 54.208, mean reward: 0.542 [0.316, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.329, 10.141], loss: 0.011037, mae: 0.114803, mean_q: 20.164549
 22397/100000: episode: 312, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 56.279, mean reward: 0.563 [0.237, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.377, 10.098], loss: 0.011380, mae: 0.116155, mean_q: 20.053108
 22497/100000: episode: 313, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 50.790, mean reward: 0.508 [0.281, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.371, 10.098], loss: 0.008844, mae: 0.101566, mean_q: 19.999474
 22597/100000: episode: 314, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 53.997, mean reward: 0.540 [0.368, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.990, 10.137], loss: 0.008569, mae: 0.101329, mean_q: 19.839989
 22697/100000: episode: 315, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 55.921, mean reward: 0.559 [0.216, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.890, 10.198], loss: 0.008078, mae: 0.096039, mean_q: 19.830994
 22797/100000: episode: 316, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 55.443, mean reward: 0.554 [0.243, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.405, 10.098], loss: 0.010253, mae: 0.109834, mean_q: 19.895679
 22897/100000: episode: 317, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 55.134, mean reward: 0.551 [0.254, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.363, 10.133], loss: 0.010104, mae: 0.109128, mean_q: 19.913242
 22997/100000: episode: 318, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 55.058, mean reward: 0.551 [0.294, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.903, 10.098], loss: 0.009432, mae: 0.105425, mean_q: 19.878973
 23097/100000: episode: 319, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 55.010, mean reward: 0.550 [0.317, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.277, 10.107], loss: 0.010315, mae: 0.110288, mean_q: 19.845295
 23197/100000: episode: 320, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 55.603, mean reward: 0.556 [0.397, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.659, 10.244], loss: 0.009642, mae: 0.106300, mean_q: 19.699715
 23297/100000: episode: 321, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.628, mean reward: 0.576 [0.373, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.632, 10.196], loss: 0.010819, mae: 0.111803, mean_q: 19.886333
 23397/100000: episode: 322, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 55.104, mean reward: 0.551 [0.285, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.826, 10.323], loss: 0.008521, mae: 0.099517, mean_q: 20.070572
 23497/100000: episode: 323, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 54.144, mean reward: 0.541 [0.266, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.227, 10.098], loss: 0.010246, mae: 0.108594, mean_q: 19.994904
 23597/100000: episode: 324, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 54.401, mean reward: 0.544 [0.319, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.645, 10.336], loss: 0.009985, mae: 0.107941, mean_q: 20.075539
 23697/100000: episode: 325, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 50.494, mean reward: 0.505 [0.285, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.263, 10.187], loss: 0.008655, mae: 0.099945, mean_q: 20.030664
 23797/100000: episode: 326, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 52.213, mean reward: 0.522 [0.229, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.386, 10.098], loss: 0.011060, mae: 0.114119, mean_q: 19.994133
 23897/100000: episode: 327, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 54.352, mean reward: 0.544 [0.364, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.244, 10.098], loss: 0.010150, mae: 0.109335, mean_q: 20.168007
 23997/100000: episode: 328, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 54.074, mean reward: 0.541 [0.374, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.098], loss: 0.013807, mae: 0.127218, mean_q: 19.842346
 24097/100000: episode: 329, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 55.934, mean reward: 0.559 [0.325, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.832, 10.098], loss: 0.008783, mae: 0.098372, mean_q: 19.927349
 24197/100000: episode: 330, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 53.519, mean reward: 0.535 [0.347, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.204, 10.098], loss: 0.010538, mae: 0.110895, mean_q: 19.899443
 24297/100000: episode: 331, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 48.333, mean reward: 0.483 [0.120, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.517, 10.182], loss: 0.009195, mae: 0.104250, mean_q: 20.002842
 24397/100000: episode: 332, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 55.888, mean reward: 0.559 [0.327, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.871, 10.168], loss: 0.009368, mae: 0.103357, mean_q: 20.073957
 24497/100000: episode: 333, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 47.736, mean reward: 0.477 [0.186, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.286, 10.098], loss: 0.010339, mae: 0.110410, mean_q: 20.121553
 24597/100000: episode: 334, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 46.305, mean reward: 0.463 [0.150, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.541, 10.098], loss: 0.008195, mae: 0.097242, mean_q: 19.809057
 24697/100000: episode: 335, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 52.541, mean reward: 0.525 [0.217, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.870, 10.098], loss: 0.010666, mae: 0.114517, mean_q: 19.981918
 24797/100000: episode: 336, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 43.355, mean reward: 0.434 [0.151, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.902, 10.098], loss: 0.009290, mae: 0.105189, mean_q: 19.924099
 24897/100000: episode: 337, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 56.202, mean reward: 0.562 [0.361, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.181, 10.098], loss: 0.009878, mae: 0.106954, mean_q: 19.848648
 24997/100000: episode: 338, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 57.769, mean reward: 0.578 [0.376, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.938, 10.144], loss: 0.011834, mae: 0.119223, mean_q: 20.113888
 25097/100000: episode: 339, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 51.282, mean reward: 0.513 [0.282, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.548, 10.326], loss: 0.010524, mae: 0.112343, mean_q: 20.047234
[Info] New level: 0.416853666305542 | Considering 10/90 traces
 25197/100000: episode: 340, duration: 4.737s, episode steps: 100, steps per second: 21, episode reward: 49.989, mean reward: 0.500 [0.126, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.566, 10.282], loss: 0.010744, mae: 0.111603, mean_q: 19.959517
 25199/100000: episode: 341, duration: 0.021s, episode steps: 2, steps per second: 96, episode reward: 1.069, mean reward: 0.535 [0.506, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.176, 10.100], loss: 0.006840, mae: 0.090808, mean_q: 19.051664
 25201/100000: episode: 342, duration: 0.020s, episode steps: 2, steps per second: 98, episode reward: 0.843, mean reward: 0.422 [0.421, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.105, 10.100], loss: 0.006953, mae: 0.084588, mean_q: 20.858335
 25203/100000: episode: 343, duration: 0.015s, episode steps: 2, steps per second: 129, episode reward: 1.183, mean reward: 0.591 [0.575, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.090, 10.100], loss: 0.006219, mae: 0.083913, mean_q: 18.988449
 25205/100000: episode: 344, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.928, mean reward: 0.464 [0.419, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.083, 10.100], loss: 0.010169, mae: 0.103866, mean_q: 19.874403
 25207/100000: episode: 345, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 1.313, mean reward: 0.656 [0.646, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.165, 10.121], loss: 0.012889, mae: 0.118186, mean_q: 21.156881
 25209/100000: episode: 346, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 1.208, mean reward: 0.604 [0.573, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.094, 10.100], loss: 0.007669, mae: 0.102067, mean_q: 20.473766
 25211/100000: episode: 347, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 0.977, mean reward: 0.489 [0.428, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.047, 10.100], loss: 0.006743, mae: 0.089206, mean_q: 18.705515
 25213/100000: episode: 348, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 0.938, mean reward: 0.469 [0.377, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.478, 10.100], loss: 0.016168, mae: 0.129286, mean_q: 20.904037
 25215/100000: episode: 349, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 1.161, mean reward: 0.581 [0.505, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.073, 10.100], loss: 0.007115, mae: 0.092793, mean_q: 20.456343
 25217/100000: episode: 350, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 1.231, mean reward: 0.615 [0.567, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.104, 10.100], loss: 0.010326, mae: 0.114652, mean_q: 20.022823
 25219/100000: episode: 351, duration: 0.018s, episode steps: 2, steps per second: 108, episode reward: 1.200, mean reward: 0.600 [0.582, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.226, 10.100], loss: 0.011398, mae: 0.111898, mean_q: 18.600700
 25221/100000: episode: 352, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 1.179, mean reward: 0.589 [0.572, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.106, 10.100], loss: 0.012945, mae: 0.129362, mean_q: 19.119822
 25223/100000: episode: 353, duration: 0.023s, episode steps: 2, steps per second: 89, episode reward: 1.138, mean reward: 0.569 [0.562, 0.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.069, 10.181], loss: 0.009123, mae: 0.111184, mean_q: 16.478933
 25225/100000: episode: 354, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 1.062, mean reward: 0.531 [0.502, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.111, 10.100], loss: 0.008763, mae: 0.104717, mean_q: 18.672535
 25227/100000: episode: 355, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.760, mean reward: 0.380 [0.337, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.192, 10.100], loss: 0.011168, mae: 0.095655, mean_q: 17.914871
 25229/100000: episode: 356, duration: 0.021s, episode steps: 2, steps per second: 93, episode reward: 1.119, mean reward: 0.560 [0.551, 0.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.100], loss: 0.008561, mae: 0.101394, mean_q: 19.576647
 25231/100000: episode: 357, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 0.873, mean reward: 0.437 [0.425, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.315, 10.100], loss: 0.011153, mae: 0.108322, mean_q: 19.535797
 25233/100000: episode: 358, duration: 0.023s, episode steps: 2, steps per second: 86, episode reward: 1.081, mean reward: 0.541 [0.451, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.068, 10.100], loss: 0.010285, mae: 0.115945, mean_q: 19.742800
 25235/100000: episode: 359, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 1.282, mean reward: 0.641 [0.637, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.097, 10.146], loss: 0.010518, mae: 0.120900, mean_q: 20.238117
 25237/100000: episode: 360, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 1.153, mean reward: 0.576 [0.568, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.116, 10.100], loss: 0.013827, mae: 0.133500, mean_q: 19.318295
 25239/100000: episode: 361, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 1.096, mean reward: 0.548 [0.521, 0.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.049, 10.100], loss: 0.011491, mae: 0.132287, mean_q: 20.300728
 25241/100000: episode: 362, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.980, mean reward: 0.490 [0.458, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.161, 10.100], loss: 0.011403, mae: 0.104868, mean_q: 18.394245
 25243/100000: episode: 363, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.698, mean reward: 0.349 [0.318, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.211, 10.100], loss: 0.008844, mae: 0.102140, mean_q: 19.836491
 25245/100000: episode: 364, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.756, mean reward: 0.378 [0.315, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.958, 10.100], loss: 0.006866, mae: 0.086341, mean_q: 19.338833
 25247/100000: episode: 365, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.582, mean reward: 0.291 [0.281, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.226, 10.100], loss: 0.008766, mae: 0.102922, mean_q: 21.547867
 25249/100000: episode: 366, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.815, mean reward: 0.408 [0.358, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.254, 10.100], loss: 0.009171, mae: 0.105802, mean_q: 22.296638
 25251/100000: episode: 367, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 1.100, mean reward: 0.550 [0.513, 0.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.113, 10.100], loss: 0.006502, mae: 0.090130, mean_q: 18.993668
 25253/100000: episode: 368, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 1.217, mean reward: 0.609 [0.605, 0.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.137, 10.100], loss: 0.013709, mae: 0.132930, mean_q: 19.418987
 25255/100000: episode: 369, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 1.258, mean reward: 0.629 [0.625, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.094, 10.100], loss: 0.008534, mae: 0.093021, mean_q: 21.109055
 25257/100000: episode: 370, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 1.051, mean reward: 0.525 [0.510, 0.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.100], loss: 0.009788, mae: 0.100446, mean_q: 18.664211
 25259/100000: episode: 371, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.724, mean reward: 0.362 [0.330, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.284, 10.100], loss: 0.010262, mae: 0.104918, mean_q: 20.778967
 25261/100000: episode: 372, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 1.044, mean reward: 0.522 [0.521, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.158, 10.100], loss: 0.007314, mae: 0.094685, mean_q: 20.036339
 25263/100000: episode: 373, duration: 0.015s, episode steps: 2, steps per second: 133, episode reward: 1.074, mean reward: 0.537 [0.515, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.179, 10.100], loss: 0.006827, mae: 0.094585, mean_q: 19.804508
 25265/100000: episode: 374, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 1.199, mean reward: 0.600 [0.555, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.137, 10.100], loss: 0.007550, mae: 0.095101, mean_q: 20.271591
 25267/100000: episode: 375, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.957, mean reward: 0.479 [0.478, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.153, 10.100], loss: 0.008482, mae: 0.097664, mean_q: 20.358528
 25269/100000: episode: 376, duration: 0.023s, episode steps: 2, steps per second: 88, episode reward: 0.909, mean reward: 0.454 [0.454, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.124, 10.143], loss: 0.006077, mae: 0.089751, mean_q: 18.118620
 25271/100000: episode: 377, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 1.240, mean reward: 0.620 [0.550, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.097, 10.100], loss: 0.006634, mae: 0.091212, mean_q: 19.714405
 25273/100000: episode: 378, duration: 0.015s, episode steps: 2, steps per second: 129, episode reward: 1.133, mean reward: 0.567 [0.506, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.074, 10.100], loss: 0.010413, mae: 0.115658, mean_q: 18.223934
 25275/100000: episode: 379, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 1.101, mean reward: 0.551 [0.497, 0.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.096, 10.100], loss: 0.007306, mae: 0.097597, mean_q: 20.356369
 25277/100000: episode: 380, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.896, mean reward: 0.448 [0.422, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.244, 10.100], loss: 0.008977, mae: 0.106337, mean_q: 18.431316
 25279/100000: episode: 381, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 1.124, mean reward: 0.562 [0.561, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.088, 10.100], loss: 0.006043, mae: 0.083977, mean_q: 21.150229
 25281/100000: episode: 382, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 1.105, mean reward: 0.552 [0.520, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.168, 10.100], loss: 0.009546, mae: 0.109982, mean_q: 17.509975
 25283/100000: episode: 383, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 1.224, mean reward: 0.612 [0.567, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.050, 10.100], loss: 0.006942, mae: 0.089430, mean_q: 18.630810
 25285/100000: episode: 384, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 1.098, mean reward: 0.549 [0.533, 0.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.157, 10.100], loss: 0.007442, mae: 0.096046, mean_q: 19.650459
 25287/100000: episode: 385, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 1.021, mean reward: 0.510 [0.448, 0.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.196, 10.100], loss: 0.008926, mae: 0.103928, mean_q: 18.377420
 25289/100000: episode: 386, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 1.178, mean reward: 0.589 [0.523, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.174, 10.100], loss: 0.006174, mae: 0.086809, mean_q: 20.168224
 25291/100000: episode: 387, duration: 0.021s, episode steps: 2, steps per second: 95, episode reward: 1.028, mean reward: 0.514 [0.494, 0.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.079, 10.100], loss: 0.007995, mae: 0.099760, mean_q: 20.187603
 25293/100000: episode: 388, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 1.133, mean reward: 0.566 [0.531, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.063, 10.100], loss: 0.014102, mae: 0.117224, mean_q: 19.146542
 25295/100000: episode: 389, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.843, mean reward: 0.422 [0.420, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.275, 10.100], loss: 0.012558, mae: 0.121286, mean_q: 19.782951
 25297/100000: episode: 390, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.117, mean reward: 0.559 [0.548, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.174, 10.100], loss: 0.011406, mae: 0.116046, mean_q: 20.385864
 25299/100000: episode: 391, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.195, mean reward: 0.598 [0.566, 0.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.092, 10.100], loss: 0.009766, mae: 0.106455, mean_q: 19.473038
 25301/100000: episode: 392, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 1.196, mean reward: 0.598 [0.563, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.078, 10.163], loss: 0.020939, mae: 0.141957, mean_q: 20.552162
 25303/100000: episode: 393, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.848, mean reward: 0.424 [0.388, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-1.064, 10.100], loss: 0.008162, mae: 0.107740, mean_q: 21.030735
 25305/100000: episode: 394, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.869, mean reward: 0.435 [0.407, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.171, 10.100], loss: 0.015828, mae: 0.142467, mean_q: 18.342335
 25307/100000: episode: 395, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.922, mean reward: 0.461 [0.451, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.127, 10.100], loss: 0.022055, mae: 0.173080, mean_q: 19.714966
 25309/100000: episode: 396, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.879, mean reward: 0.440 [0.400, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.089, 10.100], loss: 0.022390, mae: 0.169158, mean_q: 18.838789
 25311/100000: episode: 397, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.249, mean reward: 0.625 [0.582, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.080, 10.100], loss: 0.012747, mae: 0.126903, mean_q: 17.766665
 25313/100000: episode: 398, duration: 0.015s, episode steps: 2, steps per second: 129, episode reward: 1.039, mean reward: 0.519 [0.487, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.125, 10.100], loss: 0.013873, mae: 0.136408, mean_q: 19.721428
 25315/100000: episode: 399, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.900, mean reward: 0.450 [0.448, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.300, 10.100], loss: 0.016708, mae: 0.142609, mean_q: 17.082453
 25317/100000: episode: 400, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.101, mean reward: 0.551 [0.548, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.261, 10.100], loss: 0.014308, mae: 0.129880, mean_q: 20.039087
 25319/100000: episode: 401, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 0.529, mean reward: 0.264 [0.235, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.309, 10.100], loss: 0.014842, mae: 0.122823, mean_q: 17.844406
 25321/100000: episode: 402, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.807, mean reward: 0.403 [0.385, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.272, 10.100], loss: 0.013827, mae: 0.123908, mean_q: 19.680012
 25323/100000: episode: 403, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 1.163, mean reward: 0.581 [0.580, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.141, 10.100], loss: 0.010241, mae: 0.122886, mean_q: 20.485445
 25325/100000: episode: 404, duration: 0.016s, episode steps: 2, steps per second: 121, episode reward: 1.073, mean reward: 0.536 [0.511, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.198, 10.100], loss: 0.014145, mae: 0.129287, mean_q: 20.070950
 25327/100000: episode: 405, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.973, mean reward: 0.487 [0.467, 0.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.110, 10.100], loss: 0.017548, mae: 0.160738, mean_q: 17.131655
 25329/100000: episode: 406, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 0.700, mean reward: 0.350 [0.346, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.185, 10.100], loss: 0.015859, mae: 0.139267, mean_q: 22.263355
 25331/100000: episode: 407, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 1.281, mean reward: 0.641 [0.627, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.143, 10.100], loss: 0.011419, mae: 0.116248, mean_q: 19.081730
 25333/100000: episode: 408, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.998, mean reward: 0.499 [0.487, 0.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.133, 10.100], loss: 0.012112, mae: 0.122521, mean_q: 19.882551
 25335/100000: episode: 409, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.690, mean reward: 0.345 [0.320, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.174, 10.100], loss: 0.006670, mae: 0.089815, mean_q: 18.050373
 25337/100000: episode: 410, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.727, mean reward: 0.364 [0.352, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.230, 10.100], loss: 0.007075, mae: 0.093252, mean_q: 20.200939
 25339/100000: episode: 411, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 1.029, mean reward: 0.514 [0.459, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.122, 10.100], loss: 0.008359, mae: 0.110860, mean_q: 18.702795
 25341/100000: episode: 412, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.792, mean reward: 0.396 [0.359, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.233, 10.100], loss: 0.010266, mae: 0.113414, mean_q: 19.284781
 25343/100000: episode: 413, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.667, mean reward: 0.334 [0.329, 0.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.204, 10.100], loss: 0.010007, mae: 0.110523, mean_q: 17.452700
 25345/100000: episode: 414, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.152, mean reward: 0.576 [0.543, 0.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.153, 10.100], loss: 0.011214, mae: 0.112317, mean_q: 19.928080
 25347/100000: episode: 415, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 1.054, mean reward: 0.527 [0.397, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.186, 10.100], loss: 0.006709, mae: 0.088764, mean_q: 17.553501
 25349/100000: episode: 416, duration: 0.018s, episode steps: 2, steps per second: 108, episode reward: 0.876, mean reward: 0.438 [0.418, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.130, 10.100], loss: 0.005636, mae: 0.078299, mean_q: 19.785717
 25351/100000: episode: 417, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.592, mean reward: 0.296 [0.224, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.284, 10.100], loss: 0.009762, mae: 0.106352, mean_q: 21.719938
 25353/100000: episode: 418, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 1.205, mean reward: 0.603 [0.597, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.042, 10.100], loss: 0.008690, mae: 0.096893, mean_q: 19.529018
 25355/100000: episode: 419, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 1.279, mean reward: 0.639 [0.615, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.112, 10.100], loss: 0.015759, mae: 0.121027, mean_q: 18.709492
 25357/100000: episode: 420, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.703, mean reward: 0.351 [0.337, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.281, 10.100], loss: 0.012241, mae: 0.119464, mean_q: 18.783245
 25359/100000: episode: 421, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.308, mean reward: 0.654 [0.635, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.125, 10.100], loss: 0.012052, mae: 0.126041, mean_q: 19.466312
 25361/100000: episode: 422, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 0.873, mean reward: 0.437 [0.372, 0.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.206, 10.100], loss: 0.018576, mae: 0.158591, mean_q: 19.591633
 25363/100000: episode: 423, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 1.191, mean reward: 0.595 [0.559, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.087, 10.100], loss: 0.011557, mae: 0.126146, mean_q: 19.246456
 25365/100000: episode: 424, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 1.057, mean reward: 0.528 [0.520, 0.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.136, 10.177], loss: 0.009918, mae: 0.108314, mean_q: 21.572083
 25367/100000: episode: 425, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 1.250, mean reward: 0.625 [0.612, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.063, 10.100], loss: 0.010054, mae: 0.120611, mean_q: 20.294273
 25369/100000: episode: 426, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.869, mean reward: 0.434 [0.327, 0.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.155, 10.100], loss: 0.011502, mae: 0.115361, mean_q: 18.668036
 25371/100000: episode: 427, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 1.184, mean reward: 0.592 [0.563, 0.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.077, 10.100], loss: 0.010911, mae: 0.115875, mean_q: 19.476904
 25373/100000: episode: 428, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 1.205, mean reward: 0.603 [0.564, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.097, 10.125], loss: 0.011953, mae: 0.109049, mean_q: 17.357950
 25375/100000: episode: 429, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.292, mean reward: 0.646 [0.627, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.170, 10.100], loss: 0.007674, mae: 0.097585, mean_q: 19.825726
[Info] Not found new level, current best level reached = 0.416853666305542
 25377/100000: episode: 430, duration: 4.141s, episode steps: 2, steps per second: 0, episode reward: 1.241, mean reward: 0.620 [0.563, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.133, 10.118], loss: 0.010650, mae: 0.113308, mean_q: 18.466345
 25477/100000: episode: 431, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 52.331, mean reward: 0.523 [0.309, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.125, 10.106], loss: 0.011042, mae: 0.114784, mean_q: 18.933910
 25577/100000: episode: 432, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 53.839, mean reward: 0.538 [0.335, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.138, 10.098], loss: 0.011264, mae: 0.112306, mean_q: 19.299208
 25677/100000: episode: 433, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 54.549, mean reward: 0.545 [0.389, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.812, 10.323], loss: 0.013536, mae: 0.124831, mean_q: 19.364862
 25777/100000: episode: 434, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 57.660, mean reward: 0.577 [0.363, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.854, 10.112], loss: 0.013747, mae: 0.127054, mean_q: 19.364929
 25877/100000: episode: 435, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 56.476, mean reward: 0.565 [0.378, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.325, 10.138], loss: 0.013392, mae: 0.125270, mean_q: 19.279408
 25977/100000: episode: 436, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 54.131, mean reward: 0.541 [0.322, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.067, 10.273], loss: 0.013135, mae: 0.123129, mean_q: 19.513731
 26077/100000: episode: 437, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 51.215, mean reward: 0.512 [0.303, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.893, 10.315], loss: 0.013846, mae: 0.126763, mean_q: 19.221638
 26177/100000: episode: 438, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.326, mean reward: 0.573 [0.378, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.046, 10.101], loss: 0.016167, mae: 0.139265, mean_q: 19.131977
 26277/100000: episode: 439, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 52.657, mean reward: 0.527 [0.325, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.625, 10.399], loss: 0.014856, mae: 0.129597, mean_q: 19.437698
 26377/100000: episode: 440, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: 51.388, mean reward: 0.514 [0.190, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.221, 10.336], loss: 0.011583, mae: 0.115661, mean_q: 19.097189
 26477/100000: episode: 441, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 55.440, mean reward: 0.554 [0.371, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.909, 10.098], loss: 0.011424, mae: 0.116012, mean_q: 19.321409
 26577/100000: episode: 442, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 55.514, mean reward: 0.555 [0.324, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.148, 10.233], loss: 0.013546, mae: 0.123913, mean_q: 19.303627
 26677/100000: episode: 443, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 53.355, mean reward: 0.534 [0.331, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.471, 10.098], loss: 0.014907, mae: 0.130830, mean_q: 19.239511
 26777/100000: episode: 444, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 55.923, mean reward: 0.559 [0.282, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.218, 10.119], loss: 0.012754, mae: 0.119233, mean_q: 19.180021
 26877/100000: episode: 445, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 53.363, mean reward: 0.534 [0.317, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.657, 10.424], loss: 0.014601, mae: 0.129980, mean_q: 18.927784
 26977/100000: episode: 446, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 46.959, mean reward: 0.470 [0.226, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.578, 10.098], loss: 0.015672, mae: 0.129221, mean_q: 19.138521
 27077/100000: episode: 447, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 53.870, mean reward: 0.539 [0.257, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.677, 10.100], loss: 0.013301, mae: 0.122659, mean_q: 19.342619
 27177/100000: episode: 448, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 55.196, mean reward: 0.552 [0.290, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.443, 10.098], loss: 0.014349, mae: 0.125250, mean_q: 19.381559
 27277/100000: episode: 449, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 53.379, mean reward: 0.534 [0.355, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.455, 10.098], loss: 0.012847, mae: 0.121816, mean_q: 19.363504
 27377/100000: episode: 450, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 51.369, mean reward: 0.514 [0.271, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.761, 10.098], loss: 0.014522, mae: 0.127304, mean_q: 19.338125
 27477/100000: episode: 451, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 51.179, mean reward: 0.512 [0.319, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.931, 10.338], loss: 0.014197, mae: 0.124483, mean_q: 18.942884
 27577/100000: episode: 452, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 53.741, mean reward: 0.537 [0.239, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.511, 10.111], loss: 0.013558, mae: 0.120996, mean_q: 19.387697
 27677/100000: episode: 453, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 41.377, mean reward: 0.414 [0.228, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.498, 10.098], loss: 0.017502, mae: 0.141048, mean_q: 19.167618
 27777/100000: episode: 454, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 58.784, mean reward: 0.588 [0.326, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.862, 10.148], loss: 0.018346, mae: 0.148417, mean_q: 18.828239
 27877/100000: episode: 455, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 53.455, mean reward: 0.535 [0.304, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.375, 10.108], loss: 0.013384, mae: 0.121397, mean_q: 19.101284
 27977/100000: episode: 456, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 54.524, mean reward: 0.545 [0.335, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.128, 10.179], loss: 0.011730, mae: 0.115802, mean_q: 19.074688
 28077/100000: episode: 457, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 54.714, mean reward: 0.547 [0.295, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.825, 10.166], loss: 0.013617, mae: 0.121346, mean_q: 19.183155
 28177/100000: episode: 458, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 56.623, mean reward: 0.566 [0.388, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.107, 10.098], loss: 0.013703, mae: 0.121348, mean_q: 19.161966
 28277/100000: episode: 459, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 50.903, mean reward: 0.509 [0.187, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.705, 10.261], loss: 0.014861, mae: 0.127617, mean_q: 19.238886
 28377/100000: episode: 460, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 54.143, mean reward: 0.541 [0.339, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.469, 10.165], loss: 0.017507, mae: 0.141873, mean_q: 19.069698
 28477/100000: episode: 461, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 54.172, mean reward: 0.542 [0.295, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.513, 10.215], loss: 0.013732, mae: 0.123922, mean_q: 19.015789
 28577/100000: episode: 462, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.536, mean reward: 0.535 [0.315, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.080, 10.098], loss: 0.014131, mae: 0.123849, mean_q: 19.148268
 28677/100000: episode: 463, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 53.709, mean reward: 0.537 [0.237, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.043, 10.147], loss: 0.015188, mae: 0.129061, mean_q: 18.741070
 28777/100000: episode: 464, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 47.094, mean reward: 0.471 [0.106, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.230, 10.445], loss: 0.013271, mae: 0.123387, mean_q: 19.272738
 28877/100000: episode: 465, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 56.849, mean reward: 0.568 [0.406, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.542, 10.232], loss: 0.015287, mae: 0.130205, mean_q: 18.989008
 28977/100000: episode: 466, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 53.031, mean reward: 0.530 [0.310, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.746, 10.098], loss: 0.014071, mae: 0.127674, mean_q: 19.145950
 29077/100000: episode: 467, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 56.356, mean reward: 0.564 [0.329, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.553, 10.098], loss: 0.013759, mae: 0.120064, mean_q: 18.646141
 29177/100000: episode: 468, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.255, mean reward: 0.573 [0.432, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.703, 10.262], loss: 0.015149, mae: 0.130870, mean_q: 19.230440
 29277/100000: episode: 469, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 48.071, mean reward: 0.481 [0.252, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.794, 10.098], loss: 0.014942, mae: 0.131393, mean_q: 19.111370
 29377/100000: episode: 470, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 53.203, mean reward: 0.532 [0.288, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.479, 10.098], loss: 0.015730, mae: 0.132832, mean_q: 19.025999
 29477/100000: episode: 471, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 43.558, mean reward: 0.436 [0.164, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.459, 10.257], loss: 0.013301, mae: 0.120891, mean_q: 18.924202
 29577/100000: episode: 472, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 55.755, mean reward: 0.558 [0.343, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.159, 10.177], loss: 0.012625, mae: 0.116948, mean_q: 19.359669
 29677/100000: episode: 473, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 54.799, mean reward: 0.548 [0.332, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.219, 10.098], loss: 0.013928, mae: 0.124867, mean_q: 19.009094
 29777/100000: episode: 474, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 51.338, mean reward: 0.513 [0.327, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.214, 10.098], loss: 0.014498, mae: 0.131561, mean_q: 18.853436
 29877/100000: episode: 475, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 51.552, mean reward: 0.516 [0.342, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.719, 10.098], loss: 0.012044, mae: 0.114382, mean_q: 18.911152
 29977/100000: episode: 476, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 50.720, mean reward: 0.507 [0.279, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.864, 10.240], loss: 0.011797, mae: 0.115133, mean_q: 19.001440
 30077/100000: episode: 477, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 46.845, mean reward: 0.468 [0.115, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.855, 10.098], loss: 0.014911, mae: 0.127289, mean_q: 18.789322
 30177/100000: episode: 478, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 54.732, mean reward: 0.547 [0.293, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.869, 10.156], loss: 0.014454, mae: 0.127739, mean_q: 19.443060
 30277/100000: episode: 479, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 47.796, mean reward: 0.478 [0.258, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.268, 10.098], loss: 0.013177, mae: 0.123683, mean_q: 19.664343
 30377/100000: episode: 480, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 52.111, mean reward: 0.521 [0.330, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.372, 10.314], loss: 0.013755, mae: 0.125621, mean_q: 19.726137
 30477/100000: episode: 481, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 55.052, mean reward: 0.551 [0.200, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.376, 10.098], loss: 0.012766, mae: 0.122535, mean_q: 19.717268
 30577/100000: episode: 482, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 42.185, mean reward: 0.422 [0.175, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.925, 10.432], loss: 0.012411, mae: 0.119810, mean_q: 19.839983
 30677/100000: episode: 483, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 53.395, mean reward: 0.534 [0.322, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.019, 10.098], loss: 0.010894, mae: 0.112440, mean_q: 19.629318
 30777/100000: episode: 484, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 49.099, mean reward: 0.491 [0.186, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.098], loss: 0.012765, mae: 0.120934, mean_q: 19.455360
 30877/100000: episode: 485, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 55.949, mean reward: 0.559 [0.294, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.301, 10.254], loss: 0.013954, mae: 0.129299, mean_q: 19.759958
 30977/100000: episode: 486, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 50.724, mean reward: 0.507 [0.201, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.485, 10.098], loss: 0.011121, mae: 0.115275, mean_q: 19.411982
 31077/100000: episode: 487, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 52.387, mean reward: 0.524 [0.334, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.398, 10.098], loss: 0.011447, mae: 0.117627, mean_q: 19.669886
 31177/100000: episode: 488, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 53.439, mean reward: 0.534 [0.308, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.494, 10.098], loss: 0.009982, mae: 0.109014, mean_q: 19.321564
 31277/100000: episode: 489, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.387, mean reward: 0.564 [0.336, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.929, 10.148], loss: 0.009569, mae: 0.108475, mean_q: 19.685320
 31377/100000: episode: 490, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 50.061, mean reward: 0.501 [0.190, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.559, 10.318], loss: 0.012657, mae: 0.122755, mean_q: 19.811487
 31477/100000: episode: 491, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 47.434, mean reward: 0.474 [0.237, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.785, 10.375], loss: 0.012600, mae: 0.123734, mean_q: 19.665394
 31577/100000: episode: 492, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 51.182, mean reward: 0.512 [0.322, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.573, 10.273], loss: 0.010088, mae: 0.109672, mean_q: 19.438770
 31677/100000: episode: 493, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 52.274, mean reward: 0.523 [0.231, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.790, 10.445], loss: 0.010600, mae: 0.114014, mean_q: 19.569359
 31777/100000: episode: 494, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 49.504, mean reward: 0.495 [0.234, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.184, 10.098], loss: 0.011165, mae: 0.115389, mean_q: 19.575909
 31877/100000: episode: 495, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 50.662, mean reward: 0.507 [0.238, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.393, 10.098], loss: 0.013482, mae: 0.125926, mean_q: 19.555639
 31977/100000: episode: 496, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 55.538, mean reward: 0.555 [0.217, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.923, 10.098], loss: 0.011189, mae: 0.115984, mean_q: 19.505871
 32077/100000: episode: 497, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 52.651, mean reward: 0.527 [0.274, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.421, 10.098], loss: 0.010924, mae: 0.114735, mean_q: 19.709183
 32177/100000: episode: 498, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 46.785, mean reward: 0.468 [0.240, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.877, 10.098], loss: 0.013436, mae: 0.126690, mean_q: 19.393150
 32277/100000: episode: 499, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 55.815, mean reward: 0.558 [0.363, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.498, 10.187], loss: 0.011318, mae: 0.115853, mean_q: 19.787243
 32377/100000: episode: 500, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 54.453, mean reward: 0.545 [0.109, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.020, 10.098], loss: 0.010805, mae: 0.111386, mean_q: 19.661289
 32477/100000: episode: 501, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 53.696, mean reward: 0.537 [0.231, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.181, 10.155], loss: 0.012945, mae: 0.121716, mean_q: 19.276190
 32577/100000: episode: 502, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 55.345, mean reward: 0.553 [0.325, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.065, 10.243], loss: 0.015769, mae: 0.137178, mean_q: 19.534019
 32677/100000: episode: 503, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 54.390, mean reward: 0.544 [0.286, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.834, 10.098], loss: 0.012281, mae: 0.121441, mean_q: 19.506445
 32777/100000: episode: 504, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 51.214, mean reward: 0.512 [0.305, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.765, 10.274], loss: 0.013211, mae: 0.123824, mean_q: 19.432163
 32877/100000: episode: 505, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 54.032, mean reward: 0.540 [0.187, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.184, 10.438], loss: 0.016100, mae: 0.136913, mean_q: 19.372169
 32977/100000: episode: 506, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 54.310, mean reward: 0.543 [0.323, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.973, 10.098], loss: 0.015189, mae: 0.129838, mean_q: 19.509254
 33077/100000: episode: 507, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: 53.102, mean reward: 0.531 [0.296, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.779, 10.283], loss: 0.013119, mae: 0.116661, mean_q: 19.465715
 33177/100000: episode: 508, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 55.951, mean reward: 0.560 [0.327, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.919, 10.098], loss: 0.015707, mae: 0.129829, mean_q: 19.561073
 33277/100000: episode: 509, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 50.617, mean reward: 0.506 [0.296, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.458, 10.202], loss: 0.016068, mae: 0.131819, mean_q: 19.489647
 33377/100000: episode: 510, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 51.875, mean reward: 0.519 [0.208, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.543, 10.198], loss: 0.014820, mae: 0.123912, mean_q: 19.514772
 33477/100000: episode: 511, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 51.756, mean reward: 0.518 [0.343, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.050, 10.098], loss: 0.018532, mae: 0.138756, mean_q: 19.792091
 33577/100000: episode: 512, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 51.874, mean reward: 0.519 [0.269, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.281, 10.104], loss: 0.015542, mae: 0.125573, mean_q: 19.547487
 33677/100000: episode: 513, duration: 0.803s, episode steps: 100, steps per second: 125, episode reward: 53.647, mean reward: 0.536 [0.398, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.705, 10.098], loss: 0.018248, mae: 0.137519, mean_q: 19.452431
 33777/100000: episode: 514, duration: 0.982s, episode steps: 100, steps per second: 102, episode reward: 52.443, mean reward: 0.524 [0.340, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.839, 10.098], loss: 0.020844, mae: 0.143389, mean_q: 19.713678
 33877/100000: episode: 515, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 46.237, mean reward: 0.462 [0.198, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.644, 10.323], loss: 0.016609, mae: 0.125126, mean_q: 19.423624
 33977/100000: episode: 516, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 55.665, mean reward: 0.557 [0.310, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.556, 10.151], loss: 0.023856, mae: 0.142932, mean_q: 19.443666
 34077/100000: episode: 517, duration: 0.940s, episode steps: 100, steps per second: 106, episode reward: 53.502, mean reward: 0.535 [0.221, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.680, 10.149], loss: 0.020676, mae: 0.140277, mean_q: 19.250713
 34177/100000: episode: 518, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 53.217, mean reward: 0.532 [0.275, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.946, 10.098], loss: 0.024151, mae: 0.151911, mean_q: 19.492445
 34277/100000: episode: 519, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 52.927, mean reward: 0.529 [0.255, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.613, 10.353], loss: 0.017764, mae: 0.125498, mean_q: 19.389866
 34377/100000: episode: 520, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: 56.253, mean reward: 0.563 [0.331, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.414, 10.098], loss: 0.018278, mae: 0.125974, mean_q: 19.328955
 34477/100000: episode: 521, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 51.105, mean reward: 0.511 [0.209, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.513, 10.285], loss: 0.024382, mae: 0.141700, mean_q: 19.716267
 34577/100000: episode: 522, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: 50.830, mean reward: 0.508 [0.305, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.097, 10.237], loss: 0.021410, mae: 0.128338, mean_q: 19.403034
 34677/100000: episode: 523, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 50.070, mean reward: 0.501 [0.183, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.822, 10.098], loss: 0.022300, mae: 0.134062, mean_q: 19.274324
 34777/100000: episode: 524, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 54.588, mean reward: 0.546 [0.353, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.598, 10.098], loss: 0.020286, mae: 0.134410, mean_q: 19.680525
 34877/100000: episode: 525, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 55.012, mean reward: 0.550 [0.308, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.032, 10.144], loss: 0.029295, mae: 0.156253, mean_q: 19.466656
 34977/100000: episode: 526, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 52.467, mean reward: 0.525 [0.231, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.568, 10.116], loss: 0.025103, mae: 0.137695, mean_q: 19.203997
 35077/100000: episode: 527, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 41.320, mean reward: 0.413 [0.249, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.393, 10.389], loss: 0.031993, mae: 0.157078, mean_q: 19.425535
 35177/100000: episode: 528, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 54.341, mean reward: 0.543 [0.335, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.163, 10.098], loss: 0.029986, mae: 0.144735, mean_q: 19.032454
 35277/100000: episode: 529, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 56.143, mean reward: 0.561 [0.324, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.924, 10.240], loss: 0.023880, mae: 0.140463, mean_q: 19.449293
[Info] New level: 1.2263826131820679 | Considering 10/90 traces
 35377/100000: episode: 530, duration: 4.845s, episode steps: 100, steps per second: 21, episode reward: 54.828, mean reward: 0.548 [0.382, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.652, 10.098], loss: 0.028515, mae: 0.153725, mean_q: 19.259409
 35379/100000: episode: 531, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.972, mean reward: 0.486 [0.468, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.292, 10.100], loss: 0.014151, mae: 0.143987, mean_q: 19.776012
 35381/100000: episode: 532, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.690, mean reward: 0.345 [0.316, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.246, 10.100], loss: 0.018164, mae: 0.158503, mean_q: 18.884651
 35384/100000: episode: 533, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 1.176, mean reward: 0.392 [0.365, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.330, 10.100], loss: 0.030377, mae: 0.168737, mean_q: 20.139555
 35386/100000: episode: 534, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.714, mean reward: 0.357 [0.354, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.406, 10.100], loss: 0.013341, mae: 0.128514, mean_q: 18.438517
 35388/100000: episode: 535, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.568, mean reward: 0.284 [0.281, 0.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.324, 10.100], loss: 0.029218, mae: 0.165713, mean_q: 18.547274
 35390/100000: episode: 536, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.741, mean reward: 0.371 [0.332, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.203, 10.100], loss: 0.007399, mae: 0.102627, mean_q: 19.790644
 35393/100000: episode: 537, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 1.028, mean reward: 0.343 [0.275, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.394, 10.100], loss: 0.015826, mae: 0.117864, mean_q: 19.512745
 35395/100000: episode: 538, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.754, mean reward: 0.377 [0.354, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.263, 10.100], loss: 0.029641, mae: 0.146861, mean_q: 18.597332
 35397/100000: episode: 539, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.522, mean reward: 0.261 [0.253, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.349, 10.100], loss: 0.009808, mae: 0.117180, mean_q: 20.069576
 35400/100000: episode: 540, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 0.999, mean reward: 0.333 [0.315, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.273, 10.100], loss: 0.042717, mae: 0.142860, mean_q: 19.357346
 35403/100000: episode: 541, duration: 0.018s, episode steps: 3, steps per second: 170, episode reward: 0.713, mean reward: 0.238 [0.222, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.410, 10.100], loss: 0.044905, mae: 0.159671, mean_q: 19.661350
 35406/100000: episode: 542, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 0.731, mean reward: 0.244 [0.225, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.325, 10.100], loss: 0.015028, mae: 0.133314, mean_q: 17.793444
 35408/100000: episode: 543, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.973, mean reward: 0.487 [0.481, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.404, 10.100], loss: 0.011842, mae: 0.121413, mean_q: 19.623932
 35410/100000: episode: 544, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.715, mean reward: 0.358 [0.354, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.313, 10.100], loss: 0.028267, mae: 0.159315, mean_q: 19.311634
 35412/100000: episode: 545, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.818, mean reward: 0.409 [0.398, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.320, 10.100], loss: 0.024509, mae: 0.144894, mean_q: 19.724619
 35414/100000: episode: 546, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.912, mean reward: 0.456 [0.434, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.331, 10.100], loss: 0.027851, mae: 0.150097, mean_q: 20.236324
 35417/100000: episode: 547, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 0.971, mean reward: 0.324 [0.272, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.462, 10.100], loss: 0.040541, mae: 0.171712, mean_q: 18.609865
 35419/100000: episode: 548, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.756, mean reward: 0.378 [0.339, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.337, 10.100], loss: 0.019758, mae: 0.133592, mean_q: 20.812395
 35421/100000: episode: 549, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.679, mean reward: 0.339 [0.315, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.291, 10.100], loss: 0.043082, mae: 0.175671, mean_q: 17.304455
 35424/100000: episode: 550, duration: 0.029s, episode steps: 3, steps per second: 105, episode reward: 1.015, mean reward: 0.338 [0.313, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.265, 10.100], loss: 0.018546, mae: 0.139235, mean_q: 16.994562
 35426/100000: episode: 551, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 0.562, mean reward: 0.281 [0.281, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.354, 10.100], loss: 0.063303, mae: 0.256567, mean_q: 19.407101
 35428/100000: episode: 552, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.777, mean reward: 0.389 [0.387, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.302, 10.100], loss: 0.036135, mae: 0.227154, mean_q: 19.012552
 35431/100000: episode: 553, duration: 0.024s, episode steps: 3, steps per second: 124, episode reward: 0.632, mean reward: 0.211 [0.190, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.519, 10.100], loss: 0.031956, mae: 0.208661, mean_q: 19.832438
 35434/100000: episode: 554, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: 1.135, mean reward: 0.378 [0.346, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.296, 10.100], loss: 0.070385, mae: 0.252669, mean_q: 18.099106
 35437/100000: episode: 555, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 0.654, mean reward: 0.218 [0.187, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.392, 10.100], loss: 0.026365, mae: 0.182501, mean_q: 18.356333
 35439/100000: episode: 556, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.839, mean reward: 0.420 [0.412, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.376, 10.100], loss: 0.033262, mae: 0.188344, mean_q: 21.022408
 35441/100000: episode: 557, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.684, mean reward: 0.342 [0.331, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.297, 10.100], loss: 0.029615, mae: 0.186961, mean_q: 20.099218
 35443/100000: episode: 558, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.918, mean reward: 0.459 [0.433, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.301, 10.100], loss: 0.046618, mae: 0.184023, mean_q: 18.866446
 35446/100000: episode: 559, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 1.220, mean reward: 0.407 [0.352, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.353, 10.100], loss: 0.044347, mae: 0.167771, mean_q: 18.769602
 35449/100000: episode: 560, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 0.867, mean reward: 0.289 [0.286, 0.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.421, 10.100], loss: 0.027560, mae: 0.160694, mean_q: 18.535162
 35451/100000: episode: 561, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.889, mean reward: 0.445 [0.442, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.315, 10.100], loss: 0.023704, mae: 0.163879, mean_q: 19.525948
 35454/100000: episode: 562, duration: 0.021s, episode steps: 3, steps per second: 143, episode reward: 0.970, mean reward: 0.323 [0.294, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.331, 10.100], loss: 0.015131, mae: 0.136613, mean_q: 19.752092
 35456/100000: episode: 563, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.787, mean reward: 0.394 [0.365, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.235, 10.100], loss: 0.024425, mae: 0.139515, mean_q: 19.956627
 35459/100000: episode: 564, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 1.235, mean reward: 0.412 [0.371, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.397, 10.100], loss: 0.023928, mae: 0.144579, mean_q: 20.322966
 35462/100000: episode: 565, duration: 0.030s, episode steps: 3, steps per second: 99, episode reward: 1.055, mean reward: 0.352 [0.348, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.310, 10.100], loss: 0.016225, mae: 0.142942, mean_q: 17.550041
 35464/100000: episode: 566, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 0.712, mean reward: 0.356 [0.343, 0.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.152, 10.100], loss: 0.049730, mae: 0.223277, mean_q: 16.952236
 35466/100000: episode: 567, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.786, mean reward: 0.393 [0.326, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.314, 10.100], loss: 0.057595, mae: 0.156442, mean_q: 16.940258
 35469/100000: episode: 568, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 0.968, mean reward: 0.323 [0.255, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.296, 10.100], loss: 0.012687, mae: 0.127609, mean_q: 19.051403
 35471/100000: episode: 569, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.650, mean reward: 0.325 [0.271, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.113, 10.100], loss: 0.029885, mae: 0.152901, mean_q: 19.561424
 35473/100000: episode: 570, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 0.860, mean reward: 0.430 [0.393, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.228, 10.100], loss: 0.018548, mae: 0.121037, mean_q: 18.709381
 35476/100000: episode: 571, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 0.876, mean reward: 0.292 [0.259, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.386, 10.100], loss: 0.037149, mae: 0.132747, mean_q: 19.376480
 35478/100000: episode: 572, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.972, mean reward: 0.486 [0.475, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.300, 10.100], loss: 0.031157, mae: 0.160331, mean_q: 16.740093
 35480/100000: episode: 573, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.804, mean reward: 0.402 [0.356, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.272, 10.100], loss: 0.014755, mae: 0.132205, mean_q: 19.904951
 35483/100000: episode: 574, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 0.888, mean reward: 0.296 [0.193, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.386, 10.100], loss: 0.044748, mae: 0.157730, mean_q: 16.292273
 35485/100000: episode: 575, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 0.920, mean reward: 0.460 [0.452, 0.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.252, 10.100], loss: 0.056683, mae: 0.144540, mean_q: 18.264185
 35487/100000: episode: 576, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.852, mean reward: 0.426 [0.415, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.277, 10.100], loss: 0.038321, mae: 0.155944, mean_q: 17.621504
 35489/100000: episode: 577, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.636, mean reward: 0.318 [0.315, 0.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.348, 10.100], loss: 0.021302, mae: 0.164936, mean_q: 18.296030
 35492/100000: episode: 578, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 0.852, mean reward: 0.284 [0.193, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.377, 10.100], loss: 0.023990, mae: 0.147562, mean_q: 17.842842
 35494/100000: episode: 579, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.707, mean reward: 0.353 [0.309, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.272, 10.100], loss: 0.030867, mae: 0.141449, mean_q: 19.081928
 35497/100000: episode: 580, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 1.143, mean reward: 0.381 [0.367, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.269, 10.100], loss: 0.011350, mae: 0.111444, mean_q: 19.870796
 35500/100000: episode: 581, duration: 0.023s, episode steps: 3, steps per second: 129, episode reward: 1.011, mean reward: 0.337 [0.309, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.403, 10.100], loss: 0.008429, mae: 0.103515, mean_q: 17.990419
 35503/100000: episode: 582, duration: 0.022s, episode steps: 3, steps per second: 135, episode reward: 1.238, mean reward: 0.413 [0.389, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.334, 10.100], loss: 0.029123, mae: 0.152893, mean_q: 19.404697
 35506/100000: episode: 583, duration: 0.023s, episode steps: 3, steps per second: 130, episode reward: 1.184, mean reward: 0.395 [0.349, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.271, 10.100], loss: 0.008673, mae: 0.106965, mean_q: 18.871035
 35509/100000: episode: 584, duration: 0.030s, episode steps: 3, steps per second: 101, episode reward: 0.866, mean reward: 0.289 [0.280, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.291, 10.100], loss: 0.012896, mae: 0.121510, mean_q: 17.586523
 35511/100000: episode: 585, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.754, mean reward: 0.377 [0.362, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.323, 10.100], loss: 0.040443, mae: 0.161903, mean_q: 16.311531
 35513/100000: episode: 586, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.805, mean reward: 0.402 [0.401, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.542, 10.100], loss: 0.095094, mae: 0.213571, mean_q: 19.673656
 35515/100000: episode: 587, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.726, mean reward: 0.363 [0.353, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.321, 10.100], loss: 0.017343, mae: 0.142612, mean_q: 19.810059
 35518/100000: episode: 588, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 1.331, mean reward: 0.444 [0.414, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.250, 10.100], loss: 0.013504, mae: 0.129978, mean_q: 18.862009
 35520/100000: episode: 589, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 0.961, mean reward: 0.480 [0.469, 0.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.247, 10.100], loss: 0.026015, mae: 0.132659, mean_q: 18.829884
 35522/100000: episode: 590, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 0.691, mean reward: 0.346 [0.337, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.339, 10.100], loss: 0.013572, mae: 0.130930, mean_q: 19.079004
 35524/100000: episode: 591, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.642, mean reward: 0.321 [0.297, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.316, 10.100], loss: 0.037142, mae: 0.156435, mean_q: 19.450443
 35527/100000: episode: 592, duration: 0.020s, episode steps: 3, steps per second: 153, episode reward: 0.968, mean reward: 0.323 [0.301, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.397, 10.100], loss: 0.016065, mae: 0.139066, mean_q: 19.814188
 35529/100000: episode: 593, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 0.744, mean reward: 0.372 [0.366, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.395, 10.100], loss: 0.034113, mae: 0.165324, mean_q: 18.056187
 35532/100000: episode: 594, duration: 0.025s, episode steps: 3, steps per second: 120, episode reward: 1.172, mean reward: 0.391 [0.367, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.294, 10.100], loss: 0.017038, mae: 0.148943, mean_q: 17.317022
 35534/100000: episode: 595, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.901, mean reward: 0.450 [0.448, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.258, 10.100], loss: 0.023032, mae: 0.160659, mean_q: 18.273602
 35537/100000: episode: 596, duration: 0.018s, episode steps: 3, steps per second: 169, episode reward: 0.744, mean reward: 0.248 [0.224, 0.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.386, 10.100], loss: 0.021153, mae: 0.155664, mean_q: 18.669561
 35540/100000: episode: 597, duration: 0.020s, episode steps: 3, steps per second: 148, episode reward: 1.049, mean reward: 0.350 [0.315, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.332, 10.100], loss: 0.017855, mae: 0.156846, mean_q: 17.738029
 35543/100000: episode: 598, duration: 0.030s, episode steps: 3, steps per second: 101, episode reward: 1.126, mean reward: 0.375 [0.361, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.346, 10.100], loss: 0.031251, mae: 0.172529, mean_q: 18.591684
 35546/100000: episode: 599, duration: 0.023s, episode steps: 3, steps per second: 132, episode reward: 1.181, mean reward: 0.394 [0.335, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.378, 10.100], loss: 0.014868, mae: 0.132276, mean_q: 17.689783
 35548/100000: episode: 600, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.967, mean reward: 0.483 [0.481, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.214, 10.100], loss: 0.014563, mae: 0.140761, mean_q: 17.340197
 35551/100000: episode: 601, duration: 0.020s, episode steps: 3, steps per second: 149, episode reward: 1.198, mean reward: 0.399 [0.388, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.309, 10.100], loss: 0.023071, mae: 0.118615, mean_q: 19.345024
 35553/100000: episode: 602, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.650, mean reward: 0.325 [0.304, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.337, 10.100], loss: 0.033929, mae: 0.206380, mean_q: 17.304594
 35555/100000: episode: 603, duration: 0.022s, episode steps: 2, steps per second: 89, episode reward: 0.915, mean reward: 0.457 [0.452, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.309, 10.100], loss: 0.026425, mae: 0.203617, mean_q: 20.859533
 35557/100000: episode: 604, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 1.127, mean reward: 0.564 [0.558, 0.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.172, 10.100], loss: 0.014311, mae: 0.145980, mean_q: 17.894991
 35559/100000: episode: 605, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 0.585, mean reward: 0.292 [0.195, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.240, 10.100], loss: 0.024629, mae: 0.133851, mean_q: 19.171963
 35562/100000: episode: 606, duration: 0.023s, episode steps: 3, steps per second: 133, episode reward: 1.055, mean reward: 0.352 [0.312, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.361, 10.100], loss: 0.038101, mae: 0.179867, mean_q: 17.459211
 35564/100000: episode: 607, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.829, mean reward: 0.414 [0.410, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.250, 10.100], loss: 0.015073, mae: 0.131703, mean_q: 19.564663
 35567/100000: episode: 608, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 0.667, mean reward: 0.222 [0.200, 0.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.440, 10.100], loss: 0.019833, mae: 0.137558, mean_q: 17.946951
 35570/100000: episode: 609, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: 1.160, mean reward: 0.387 [0.357, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.260, 10.100], loss: 0.039931, mae: 0.168322, mean_q: 19.263512
 35572/100000: episode: 610, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.830, mean reward: 0.415 [0.407, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.515, 10.100], loss: 0.015970, mae: 0.148211, mean_q: 16.928185
 35574/100000: episode: 611, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.902, mean reward: 0.451 [0.431, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.319, 10.100], loss: 0.022155, mae: 0.151662, mean_q: 16.271378
 35576/100000: episode: 612, duration: 0.021s, episode steps: 2, steps per second: 97, episode reward: 0.926, mean reward: 0.463 [0.462, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.301, 10.100], loss: 0.010593, mae: 0.119322, mean_q: 17.517006
 35578/100000: episode: 613, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.653, mean reward: 0.327 [0.317, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.474, 10.100], loss: 0.015260, mae: 0.128428, mean_q: 19.914671
 35581/100000: episode: 614, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 0.959, mean reward: 0.320 [0.288, 0.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.783, 10.100], loss: 0.018080, mae: 0.146838, mean_q: 16.151548
 35583/100000: episode: 615, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.976, mean reward: 0.488 [0.486, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.116, 10.100], loss: 0.021906, mae: 0.162796, mean_q: 19.444180
 35585/100000: episode: 616, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.868, mean reward: 0.434 [0.406, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.276, 10.100], loss: 0.019405, mae: 0.151885, mean_q: 18.179119
 35587/100000: episode: 617, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.790, mean reward: 0.395 [0.368, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.172, 10.100], loss: 0.007244, mae: 0.095008, mean_q: 19.278257
 35589/100000: episode: 618, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.981, mean reward: 0.490 [0.471, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.168, 10.100], loss: 0.025735, mae: 0.148388, mean_q: 17.527719
 35591/100000: episode: 619, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.790, mean reward: 0.395 [0.343, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.303, 10.100], loss: 0.009253, mae: 0.106732, mean_q: 20.430885
[Info] New level: 0.45294633507728577 | Considering 10/90 traces
 35593/100000: episode: 620, duration: 4.146s, episode steps: 2, steps per second: 0, episode reward: 0.787, mean reward: 0.393 [0.370, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.290, 10.100], loss: 0.035813, mae: 0.137153, mean_q: 19.144583
 35594/100000: episode: 621, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.278, mean reward: 0.278 [0.278, 0.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.410, 10.100], loss: 0.013697, mae: 0.138465, mean_q: 17.578840
 35595/100000: episode: 622, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.478, mean reward: 0.478 [0.478, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.336, 10.100], loss: 0.018464, mae: 0.157590, mean_q: 18.869604
 35597/100000: episode: 623, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.398, mean reward: 0.199 [0.192, 0.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.595, 10.100], loss: 0.015337, mae: 0.142535, mean_q: 19.380911
 35598/100000: episode: 624, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.291, mean reward: 0.291 [0.291, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.371, 10.100], loss: 0.007685, mae: 0.103476, mean_q: 20.933239
 35599/100000: episode: 625, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.345, mean reward: 0.345 [0.345, 0.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.343, 10.100], loss: 0.040867, mae: 0.167725, mean_q: 17.725368
 35600/100000: episode: 626, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.271, mean reward: 0.271 [0.271, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.370, 10.100], loss: 0.031719, mae: 0.220965, mean_q: 18.130596
 35601/100000: episode: 627, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.418, mean reward: 0.418 [0.418, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.319, 10.100], loss: 0.007237, mae: 0.097348, mean_q: 21.404682
 35602/100000: episode: 628, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.379, mean reward: 0.379 [0.379, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.287, 10.100], loss: 0.063719, mae: 0.195160, mean_q: 17.798943
 35603/100000: episode: 629, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.219, mean reward: 0.219 [0.219, 0.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.438, 10.100], loss: 0.062004, mae: 0.254247, mean_q: 16.141850
 35604/100000: episode: 630, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.206, mean reward: 0.206 [0.206, 0.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.420, 10.100], loss: 0.012499, mae: 0.127370, mean_q: 15.620658
 35605/100000: episode: 631, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.266, mean reward: 0.266 [0.266, 0.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.423, 10.100], loss: 0.012177, mae: 0.131855, mean_q: 19.362637
 35606/100000: episode: 632, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.251, mean reward: 0.251 [0.251, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.436, 10.100], loss: 0.024220, mae: 0.190855, mean_q: 21.165257
 35607/100000: episode: 633, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.355, mean reward: 0.355 [0.355, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.321, 10.100], loss: 0.015220, mae: 0.128886, mean_q: 18.502344
 35608/100000: episode: 634, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.288, 10.100], loss: 0.012891, mae: 0.124801, mean_q: 19.834980
 35609/100000: episode: 635, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.213, mean reward: 0.213 [0.213, 0.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.605, 10.100], loss: 0.011266, mae: 0.117175, mean_q: 15.020357
 35610/100000: episode: 636, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.237, mean reward: 0.237 [0.237, 0.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.393, 10.100], loss: 0.009204, mae: 0.110132, mean_q: 16.646133
 35611/100000: episode: 637, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.281, mean reward: 0.281 [0.281, 0.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.377, 10.100], loss: 0.050309, mae: 0.177198, mean_q: 15.618434
 35612/100000: episode: 638, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.314, mean reward: 0.314 [0.314, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.386, 10.100], loss: 0.030522, mae: 0.185818, mean_q: 17.443453
 35613/100000: episode: 639, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.228, mean reward: 0.228 [0.228, 0.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.427, 10.100], loss: 0.039156, mae: 0.174996, mean_q: 20.475954
 35614/100000: episode: 640, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.392, mean reward: 0.392 [0.392, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.302, 10.100], loss: 0.039410, mae: 0.172765, mean_q: 18.217648
 35615/100000: episode: 641, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.210, mean reward: 0.210 [0.210, 0.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.437, 10.100], loss: 0.017074, mae: 0.136586, mean_q: 19.357601
 35616/100000: episode: 642, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.229, mean reward: 0.229 [0.229, 0.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.420, 10.100], loss: 0.018273, mae: 0.140526, mean_q: 18.459074
 35617/100000: episode: 643, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.411, 10.100], loss: 0.025878, mae: 0.155867, mean_q: 17.629692
 35618/100000: episode: 644, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 0.274, mean reward: 0.274 [0.274, 0.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.408, 10.100], loss: 0.010386, mae: 0.112328, mean_q: 20.189411
 35620/100000: episode: 645, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.402, mean reward: 0.201 [0.176, 0.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.471, 10.100], loss: 0.013864, mae: 0.136593, mean_q: 18.191149
 35621/100000: episode: 646, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.251, mean reward: 0.251 [0.251, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.423, 10.100], loss: 0.042810, mae: 0.208375, mean_q: 18.583210
 35622/100000: episode: 647, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.375, mean reward: 0.375 [0.375, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.285, 10.100], loss: 0.015188, mae: 0.127605, mean_q: 17.395819
 35623/100000: episode: 648, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.246, mean reward: 0.246 [0.246, 0.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.371, 10.100], loss: 0.041280, mae: 0.166636, mean_q: 18.583143
 35624/100000: episode: 649, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.213, mean reward: 0.213 [0.213, 0.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.353 [-0.333, 10.100], loss: 0.013642, mae: 0.133692, mean_q: 18.938225
 35625/100000: episode: 650, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.271, mean reward: 0.271 [0.271, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.406, 10.100], loss: 0.006888, mae: 0.100498, mean_q: 15.877393
 35626/100000: episode: 651, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.405, mean reward: 0.405 [0.405, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.273, 10.100], loss: 0.013388, mae: 0.131303, mean_q: 20.121534
 35627/100000: episode: 652, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.443, mean reward: 0.443 [0.443, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.341, 10.100], loss: 0.046333, mae: 0.228261, mean_q: 19.180727
 35628/100000: episode: 653, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.334, mean reward: 0.334 [0.334, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.383, 10.100], loss: 0.012762, mae: 0.119438, mean_q: 17.676237
 35629/100000: episode: 654, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.431, mean reward: 0.431 [0.431, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.284, 10.100], loss: 0.039782, mae: 0.141472, mean_q: 18.196293
 35630/100000: episode: 655, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.301, mean reward: 0.301 [0.301, 0.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.431, 10.100], loss: 0.017237, mae: 0.150129, mean_q: 17.219461
 35631/100000: episode: 656, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.466, mean reward: 0.466 [0.466, 0.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.363, 10.100], loss: 0.012159, mae: 0.114904, mean_q: 17.984383
 35632/100000: episode: 657, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.255, mean reward: 0.255 [0.255, 0.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.378, 10.100], loss: 0.009060, mae: 0.099512, mean_q: 20.082907
 35633/100000: episode: 658, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.297, mean reward: 0.297 [0.297, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.358, 10.100], loss: 0.015682, mae: 0.144795, mean_q: 16.586288
 35635/100000: episode: 659, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.494, mean reward: 0.247 [0.224, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.478, 10.100], loss: 0.009998, mae: 0.106735, mean_q: 18.370911
 35636/100000: episode: 660, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.341, mean reward: 0.341 [0.341, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.363, 10.100], loss: 0.012943, mae: 0.115096, mean_q: 16.931690
 35637/100000: episode: 661, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.384, mean reward: 0.384 [0.384, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.371, 10.100], loss: 0.012081, mae: 0.123896, mean_q: 19.974438
 35638/100000: episode: 662, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.250, mean reward: 0.250 [0.250, 0.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.388, 10.100], loss: 0.007137, mae: 0.090499, mean_q: 18.359802
 35639/100000: episode: 663, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.240, mean reward: 0.240 [0.240, 0.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.432, 10.100], loss: 0.020030, mae: 0.163012, mean_q: 17.019127
 35640/100000: episode: 664, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.331, mean reward: 0.331 [0.331, 0.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.302, 10.100], loss: 0.009891, mae: 0.098890, mean_q: 16.485350
 35641/100000: episode: 665, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.268, mean reward: 0.268 [0.268, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.405, 10.100], loss: 0.012567, mae: 0.115886, mean_q: 21.155502
 35642/100000: episode: 666, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.427, mean reward: 0.427 [0.427, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.401, 10.100], loss: 0.008555, mae: 0.099768, mean_q: 17.589609
 35643/100000: episode: 667, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.415, mean reward: 0.415 [0.415, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.348, 10.100], loss: 0.008716, mae: 0.102002, mean_q: 20.060169
 35644/100000: episode: 668, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.382, mean reward: 0.382 [0.382, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.434, 10.100], loss: 0.011186, mae: 0.118395, mean_q: 19.591671
 35645/100000: episode: 669, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.291, mean reward: 0.291 [0.291, 0.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.418, 10.100], loss: 0.017308, mae: 0.144080, mean_q: 17.240192
 35646/100000: episode: 670, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.300, mean reward: 0.300 [0.300, 0.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.315, 10.100], loss: 0.008313, mae: 0.102939, mean_q: 19.058224
 35647/100000: episode: 671, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.292, mean reward: 0.292 [0.292, 0.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.412, 10.100], loss: 0.008911, mae: 0.099857, mean_q: 15.233477
 35648/100000: episode: 672, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.258, mean reward: 0.258 [0.258, 0.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.407, 10.100], loss: 0.006492, mae: 0.092456, mean_q: 15.537739
 35649/100000: episode: 673, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.223, mean reward: 0.223 [0.223, 0.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.370, 10.100], loss: 0.016124, mae: 0.125732, mean_q: 20.257229
 35650/100000: episode: 674, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.270, mean reward: 0.270 [0.270, 0.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.418, 10.100], loss: 0.016352, mae: 0.134831, mean_q: 18.400181
 35651/100000: episode: 675, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.242, mean reward: 0.242 [0.242, 0.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.354, 10.100], loss: 0.006782, mae: 0.092812, mean_q: 19.946966
 35652/100000: episode: 676, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.390, mean reward: 0.390 [0.390, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.348, 10.100], loss: 0.064884, mae: 0.153297, mean_q: 18.498669
 35653/100000: episode: 677, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.198, mean reward: 0.198 [0.198, 0.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.469, 10.100], loss: 0.005223, mae: 0.082024, mean_q: 17.360153
 35654/100000: episode: 678, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.272, mean reward: 0.272 [0.272, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.318, 10.100], loss: 0.023542, mae: 0.129135, mean_q: 19.940357
 35655/100000: episode: 679, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.236, mean reward: 0.236 [0.236, 0.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.445, 10.100], loss: 0.008261, mae: 0.110328, mean_q: 18.139513
 35656/100000: episode: 680, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.347, mean reward: 0.347 [0.347, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.329, 10.100], loss: 0.040974, mae: 0.169752, mean_q: 18.688629
 35657/100000: episode: 681, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.417, mean reward: 0.417 [0.417, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.320, 10.100], loss: 0.014633, mae: 0.127808, mean_q: 19.951523
 35659/100000: episode: 682, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 0.515, mean reward: 0.258 [0.243, 0.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.515, 10.100], loss: 0.012203, mae: 0.122593, mean_q: 18.901829
 35661/100000: episode: 683, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.344, mean reward: 0.172 [0.167, 0.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.478, 10.100], loss: 0.008059, mae: 0.101238, mean_q: 16.910969
 35663/100000: episode: 684, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.508, mean reward: 0.254 [0.236, 0.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.414, 10.100], loss: 0.024540, mae: 0.127771, mean_q: 18.393738
 35664/100000: episode: 685, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.449, mean reward: 0.449 [0.449, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.348, 10.100], loss: 0.026456, mae: 0.137986, mean_q: 17.690113
 35665/100000: episode: 686, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.410, mean reward: 0.410 [0.410, 0.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.351, 10.100], loss: 0.009549, mae: 0.100645, mean_q: 16.046410
 35666/100000: episode: 687, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.183, mean reward: 0.183 [0.183, 0.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.438, 10.100], loss: 0.010210, mae: 0.123493, mean_q: 16.926888
 35667/100000: episode: 688, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.279, mean reward: 0.279 [0.279, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.468, 10.100], loss: 0.009719, mae: 0.114828, mean_q: 21.419201
 35668/100000: episode: 689, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.296, mean reward: 0.296 [0.296, 0.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.311, 10.100], loss: 0.010709, mae: 0.121459, mean_q: 17.224745
 35669/100000: episode: 690, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.267, mean reward: 0.267 [0.267, 0.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.439, 10.100], loss: 0.007986, mae: 0.098493, mean_q: 17.348907
 35670/100000: episode: 691, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.367, 10.100], loss: 0.007203, mae: 0.088203, mean_q: 19.223984
 35671/100000: episode: 692, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.312, mean reward: 0.312 [0.312, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.327, 10.100], loss: 0.007389, mae: 0.098311, mean_q: 14.371764
 35672/100000: episode: 693, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.314, mean reward: 0.314 [0.314, 0.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.329, 10.100], loss: 0.008281, mae: 0.096975, mean_q: 20.222689
 35673/100000: episode: 694, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.360, 10.100], loss: 0.009976, mae: 0.105968, mean_q: 20.788261
 35675/100000: episode: 695, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.422, mean reward: 0.211 [0.209, 0.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.500, 10.100], loss: 0.015358, mae: 0.129110, mean_q: 21.292921
 35676/100000: episode: 696, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.139, mean reward: 0.139 [0.139, 0.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.452, 10.100], loss: 0.009773, mae: 0.120519, mean_q: 18.301113
 35678/100000: episode: 697, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.581, mean reward: 0.291 [0.237, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.453, 10.100], loss: 0.034846, mae: 0.137851, mean_q: 17.738457
 35679/100000: episode: 698, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.176, mean reward: 0.176 [0.176, 0.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.390, 10.100], loss: 0.009522, mae: 0.109830, mean_q: 14.986530
 35681/100000: episode: 699, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.451, mean reward: 0.225 [0.219, 0.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.475, 10.100], loss: 0.060337, mae: 0.187607, mean_q: 15.190340
 35682/100000: episode: 700, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.249, mean reward: 0.249 [0.249, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.415, 10.100], loss: 0.011383, mae: 0.122394, mean_q: 19.067825
 35683/100000: episode: 701, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.360, mean reward: 0.360 [0.360, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.339, 10.100], loss: 0.023544, mae: 0.163243, mean_q: 20.150221
 35684/100000: episode: 702, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.315, mean reward: 0.315 [0.315, 0.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.376, 10.100], loss: 0.014042, mae: 0.125478, mean_q: 15.053476
 35685/100000: episode: 703, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.414, mean reward: 0.414 [0.414, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.402, 10.100], loss: 0.016931, mae: 0.135794, mean_q: 14.822290
 35686/100000: episode: 704, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.384, mean reward: 0.384 [0.384, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.293, 10.100], loss: 0.014373, mae: 0.119989, mean_q: 17.670324
 35687/100000: episode: 705, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.487, mean reward: 0.487 [0.487, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.333, 10.100], loss: 0.008350, mae: 0.105397, mean_q: 14.606194
 35688/100000: episode: 706, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.375, mean reward: 0.375 [0.375, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.321, 10.100], loss: 0.009489, mae: 0.109183, mean_q: 16.895199
 35689/100000: episode: 707, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.268, mean reward: 0.268 [0.268, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.421, 10.100], loss: 0.021812, mae: 0.162573, mean_q: 20.775295
 35690/100000: episode: 708, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.280, mean reward: 0.280 [0.280, 0.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.373, 10.100], loss: 0.067025, mae: 0.158651, mean_q: 17.725197
 35691/100000: episode: 709, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.249, mean reward: 0.249 [0.249, 0.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.400, 10.100], loss: 0.019565, mae: 0.158677, mean_q: 19.247782
[Info] New level: 0.28386199474334717 | Considering 54/46 traces
 35692/100000: episode: 710, duration: 4.247s, episode steps: 1, steps per second: 0, episode reward: 0.360, mean reward: 0.360 [0.360, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.381, 10.100], loss: 0.037240, mae: 0.215543, mean_q: 18.542812
 35694/100000: episode: 711, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.473, mean reward: 0.237 [0.216, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.375, 10.100], loss: 0.035628, mae: 0.185864, mean_q: 18.193146
 35695/100000: episode: 712, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.213, mean reward: 0.213 [0.213, 0.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.409, 10.100], loss: 0.057491, mae: 0.151972, mean_q: 21.477009
 35697/100000: episode: 713, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.515, mean reward: 0.257 [0.251, 0.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.430, 10.100], loss: 0.010396, mae: 0.107474, mean_q: 17.560295
 35698/100000: episode: 714, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.209, mean reward: 0.209 [0.209, 0.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.418, 10.100], loss: 0.014400, mae: 0.122941, mean_q: 18.229969
 35699/100000: episode: 715, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.390, mean reward: 0.390 [0.390, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.326, 10.100], loss: 0.008760, mae: 0.103267, mean_q: 19.066650
 35700/100000: episode: 716, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.438, mean reward: 0.438 [0.438, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.397, 10.100], loss: 0.007777, mae: 0.099424, mean_q: 16.069845
 35701/100000: episode: 717, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.217, mean reward: 0.217 [0.217, 0.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.498, 10.100], loss: 0.008791, mae: 0.094993, mean_q: 18.785427
 35702/100000: episode: 718, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.331, 10.100], loss: 0.050285, mae: 0.168354, mean_q: 16.935766
 35703/100000: episode: 719, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.448, mean reward: 0.448 [0.448, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.301, 10.100], loss: 0.027618, mae: 0.137291, mean_q: 17.673916
 35705/100000: episode: 720, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.514, mean reward: 0.257 [0.256, 0.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.375, 10.100], loss: 0.008578, mae: 0.098677, mean_q: 18.150894
 35706/100000: episode: 721, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.219, mean reward: 0.219 [0.219, 0.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.381, 10.100], loss: 0.009651, mae: 0.118192, mean_q: 19.703524
 35707/100000: episode: 722, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.113, mean reward: 0.113 [0.113, 0.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.474, 10.100], loss: 0.029448, mae: 0.132529, mean_q: 14.267328
 35708/100000: episode: 723, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.275, mean reward: 0.275 [0.275, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.420, 10.100], loss: 0.011665, mae: 0.105051, mean_q: 16.915834
 35709/100000: episode: 724, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.279, mean reward: 0.279 [0.279, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.396, 10.100], loss: 0.012045, mae: 0.106820, mean_q: 14.725493
 35710/100000: episode: 725, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.221, mean reward: 0.221 [0.221, 0.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.432, 10.100], loss: 0.016315, mae: 0.137789, mean_q: 18.108360
 35711/100000: episode: 726, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.222, mean reward: 0.222 [0.222, 0.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.505, 10.100], loss: 0.008488, mae: 0.109854, mean_q: 20.235474
 35712/100000: episode: 727, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.335, mean reward: 0.335 [0.335, 0.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.399, 10.100], loss: 0.011187, mae: 0.116352, mean_q: 14.055635
 35713/100000: episode: 728, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.283, mean reward: 0.283 [0.283, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.432, 10.100], loss: 0.005403, mae: 0.082208, mean_q: 16.629271
 35714/100000: episode: 729, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.355, mean reward: 0.355 [0.355, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.331, 10.100], loss: 0.048320, mae: 0.177559, mean_q: 16.793856
 35715/100000: episode: 730, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: 0.275, mean reward: 0.275 [0.275, 0.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.336, 10.100], loss: 0.021856, mae: 0.111168, mean_q: 16.586823
 35716/100000: episode: 731, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.346, mean reward: 0.346 [0.346, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.378, 10.100], loss: 0.011277, mae: 0.117965, mean_q: 19.869825
 35718/100000: episode: 732, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.423, mean reward: 0.211 [0.209, 0.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.533, 10.100], loss: 0.007120, mae: 0.099858, mean_q: 16.102158
 35719/100000: episode: 733, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.207, mean reward: 0.207 [0.207, 0.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.474, 10.100], loss: 0.004916, mae: 0.076015, mean_q: 16.435175
 35720/100000: episode: 734, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.253, mean reward: 0.253 [0.253, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.472, 10.100], loss: 0.006352, mae: 0.094788, mean_q: 16.394609
 35721/100000: episode: 735, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.185, mean reward: 0.185 [0.185, 0.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.393, 10.100], loss: 0.009984, mae: 0.110434, mean_q: 18.377037
 35722/100000: episode: 736, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.427, mean reward: 0.427 [0.427, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.385, 10.100], loss: 0.008351, mae: 0.099541, mean_q: 20.321177
 35723/100000: episode: 737, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.204, mean reward: 0.204 [0.204, 0.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.408, 10.100], loss: 0.025809, mae: 0.141097, mean_q: 22.141975
 35724/100000: episode: 738, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.286, mean reward: 0.286 [0.286, 0.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.402, 10.100], loss: 0.010518, mae: 0.122921, mean_q: 18.749577
 35725/100000: episode: 739, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.235, mean reward: 0.235 [0.235, 0.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.378, 10.100], loss: 0.039306, mae: 0.117974, mean_q: 19.321466
 35726/100000: episode: 740, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 0.191, mean reward: 0.191 [0.191, 0.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.430, 10.100], loss: 0.007274, mae: 0.092132, mean_q: 16.727966
 35727/100000: episode: 741, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.202, mean reward: 0.202 [0.202, 0.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.392, 10.100], loss: 0.007741, mae: 0.096532, mean_q: 20.427006
 35728/100000: episode: 742, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.409, mean reward: 0.409 [0.409, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.379, 10.100], loss: 0.009399, mae: 0.095120, mean_q: 14.448193
 35729/100000: episode: 743, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.212, mean reward: 0.212 [0.212, 0.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.443, 10.100], loss: 0.024184, mae: 0.120142, mean_q: 17.468563
 35731/100000: episode: 744, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 0.571, mean reward: 0.286 [0.282, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.371, 10.100], loss: 0.009740, mae: 0.107216, mean_q: 16.839371
 35732/100000: episode: 745, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.229, mean reward: 0.229 [0.229, 0.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.518, 10.100], loss: 0.025795, mae: 0.165863, mean_q: 15.943768
 35733/100000: episode: 746, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.216, mean reward: 0.216 [0.216, 0.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.554, 10.100], loss: 0.009383, mae: 0.116253, mean_q: 16.863045
 35734/100000: episode: 747, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.285, mean reward: 0.285 [0.285, 0.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.360, 10.100], loss: 0.055116, mae: 0.158054, mean_q: 17.337320
 35735/100000: episode: 748, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.254, mean reward: 0.254 [0.254, 0.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.345, 10.100], loss: 0.044179, mae: 0.143586, mean_q: 18.152180
 35737/100000: episode: 749, duration: 0.027s, episode steps: 2, steps per second: 73, episode reward: 0.265, mean reward: 0.133 [0.080, 0.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.545, 10.100], loss: 0.016525, mae: 0.138283, mean_q: 17.868435
 35739/100000: episode: 750, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.663, mean reward: 0.331 [0.326, 0.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.359, 10.100], loss: 0.021524, mae: 0.152239, mean_q: 18.135662
 35740/100000: episode: 751, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.419, mean reward: 0.419 [0.419, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.411, 10.100], loss: 0.075058, mae: 0.204540, mean_q: 13.858402
 35741/100000: episode: 752, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 0.170, mean reward: 0.170 [0.170, 0.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.432, 10.100], loss: 0.016023, mae: 0.129041, mean_q: 17.108315
 35742/100000: episode: 753, duration: 0.023s, episode steps: 1, steps per second: 44, episode reward: 0.176, mean reward: 0.176 [0.176, 0.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.408, 10.100], loss: 0.015639, mae: 0.157916, mean_q: 18.882666
 35743/100000: episode: 754, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.402, 10.100], loss: 0.013527, mae: 0.131165, mean_q: 15.514729
 35745/100000: episode: 755, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 0.432, mean reward: 0.216 [0.185, 0.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.410, 10.100], loss: 0.008689, mae: 0.110325, mean_q: 18.988579
[Info] Not found new level, current best level reached = 0.28386199474334717
 35747/100000: episode: 756, duration: 4.392s, episode steps: 2, steps per second: 0, episode reward: 0.423, mean reward: 0.211 [0.189, 0.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.392, 10.100], loss: 0.015580, mae: 0.118971, mean_q: 17.761894
 35847/100000: episode: 757, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 51.375, mean reward: 0.514 [0.296, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.389, 10.365], loss: 0.025457, mae: 0.150197, mean_q: 17.633745
 35947/100000: episode: 758, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.456, mean reward: 0.535 [0.261, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.980, 10.098], loss: 0.020363, mae: 0.131772, mean_q: 17.829725
 36047/100000: episode: 759, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 54.831, mean reward: 0.548 [0.277, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.714, 10.098], loss: 0.019380, mae: 0.134364, mean_q: 18.040455
 36147/100000: episode: 760, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 50.238, mean reward: 0.502 [0.316, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.488, 10.098], loss: 0.019186, mae: 0.131799, mean_q: 17.948158
 36247/100000: episode: 761, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 53.932, mean reward: 0.539 [0.371, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.097, 10.098], loss: 0.017551, mae: 0.132849, mean_q: 17.780960
 36347/100000: episode: 762, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 53.218, mean reward: 0.532 [0.329, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.199, 10.098], loss: 0.016641, mae: 0.130125, mean_q: 17.694990
 36447/100000: episode: 763, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 50.219, mean reward: 0.502 [0.286, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.580, 10.357], loss: 0.014293, mae: 0.122609, mean_q: 17.797270
 36547/100000: episode: 764, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 55.216, mean reward: 0.552 [0.280, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.668, 10.186], loss: 0.013007, mae: 0.118646, mean_q: 17.949556
 36647/100000: episode: 765, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 53.805, mean reward: 0.538 [0.206, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.232, 10.098], loss: 0.015096, mae: 0.124527, mean_q: 17.502676
 36747/100000: episode: 766, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 51.927, mean reward: 0.519 [0.322, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.343, 10.288], loss: 0.015446, mae: 0.127120, mean_q: 17.893930
 36847/100000: episode: 767, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 53.346, mean reward: 0.533 [0.296, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.873, 10.399], loss: 0.013367, mae: 0.118007, mean_q: 17.792246
 36947/100000: episode: 768, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 54.481, mean reward: 0.545 [0.247, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.040, 10.207], loss: 0.014630, mae: 0.123707, mean_q: 18.080198
 37047/100000: episode: 769, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 54.129, mean reward: 0.541 [0.271, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.873, 10.293], loss: 0.017085, mae: 0.131909, mean_q: 17.728876
 37147/100000: episode: 770, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 55.479, mean reward: 0.555 [0.316, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.474, 10.098], loss: 0.017011, mae: 0.137491, mean_q: 17.874126
 37247/100000: episode: 771, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 53.583, mean reward: 0.536 [0.246, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.899, 10.147], loss: 0.015477, mae: 0.134306, mean_q: 17.942869
 37347/100000: episode: 772, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 51.143, mean reward: 0.511 [0.034, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.471, 10.323], loss: 0.016830, mae: 0.134606, mean_q: 17.673792
 37447/100000: episode: 773, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 50.706, mean reward: 0.507 [0.270, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.749, 10.098], loss: 0.016834, mae: 0.138657, mean_q: 17.846094
 37547/100000: episode: 774, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 52.468, mean reward: 0.525 [0.300, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.868, 10.227], loss: 0.013808, mae: 0.125610, mean_q: 18.020449
 37647/100000: episode: 775, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 54.690, mean reward: 0.547 [0.295, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.513, 10.281], loss: 0.014336, mae: 0.130497, mean_q: 17.862463
 37747/100000: episode: 776, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 52.711, mean reward: 0.527 [0.310, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.584, 10.313], loss: 0.013709, mae: 0.127627, mean_q: 17.997963
 37847/100000: episode: 777, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 49.453, mean reward: 0.495 [0.250, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.619, 10.098], loss: 0.016072, mae: 0.137477, mean_q: 17.970980
 37947/100000: episode: 778, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 51.657, mean reward: 0.517 [0.181, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.359, 10.098], loss: 0.013475, mae: 0.124016, mean_q: 18.143780
 38047/100000: episode: 779, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 54.272, mean reward: 0.543 [0.314, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.228, 10.098], loss: 0.015031, mae: 0.131234, mean_q: 17.992640
 38147/100000: episode: 780, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 53.477, mean reward: 0.535 [0.334, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.956, 10.173], loss: 0.013004, mae: 0.123437, mean_q: 18.019062
 38247/100000: episode: 781, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 52.884, mean reward: 0.529 [0.298, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.697, 10.210], loss: 0.016768, mae: 0.140346, mean_q: 17.667686
 38347/100000: episode: 782, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 54.423, mean reward: 0.544 [0.287, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.416, 10.098], loss: 0.012985, mae: 0.121827, mean_q: 17.578463
 38447/100000: episode: 783, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 56.585, mean reward: 0.566 [0.382, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.334, 10.104], loss: 0.017990, mae: 0.147638, mean_q: 17.903677
 38547/100000: episode: 784, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 54.609, mean reward: 0.546 [0.283, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.414, 10.204], loss: 0.014772, mae: 0.130707, mean_q: 17.745993
 38647/100000: episode: 785, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 56.935, mean reward: 0.569 [0.300, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.480, 10.098], loss: 0.013906, mae: 0.124184, mean_q: 17.801651
 38747/100000: episode: 786, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 49.808, mean reward: 0.498 [0.281, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.616, 10.098], loss: 0.014338, mae: 0.125499, mean_q: 17.658035
 38847/100000: episode: 787, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.191, mean reward: 0.572 [0.328, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.955, 10.284], loss: 0.012827, mae: 0.122696, mean_q: 17.644062
 38947/100000: episode: 788, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 46.934, mean reward: 0.469 [0.239, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.534, 10.122], loss: 0.015024, mae: 0.132592, mean_q: 18.143209
 39047/100000: episode: 789, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 52.342, mean reward: 0.523 [0.277, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.371, 10.098], loss: 0.014243, mae: 0.128866, mean_q: 17.718502
 39147/100000: episode: 790, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 49.091, mean reward: 0.491 [0.264, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.920, 10.243], loss: 0.015698, mae: 0.138260, mean_q: 17.990509
 39247/100000: episode: 791, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.241, mean reward: 0.562 [0.341, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.374, 10.204], loss: 0.013378, mae: 0.123996, mean_q: 17.780972
 39347/100000: episode: 792, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 51.862, mean reward: 0.519 [0.303, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.438, 10.098], loss: 0.013285, mae: 0.123303, mean_q: 17.515087
 39447/100000: episode: 793, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 52.705, mean reward: 0.527 [0.275, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.843, 10.098], loss: 0.013327, mae: 0.125115, mean_q: 17.358393
 39547/100000: episode: 794, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 53.214, mean reward: 0.532 [0.338, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.836, 10.319], loss: 0.013644, mae: 0.126375, mean_q: 17.967314
 39647/100000: episode: 795, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 53.331, mean reward: 0.533 [0.251, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.056, 10.246], loss: 0.013693, mae: 0.126450, mean_q: 18.149220
 39747/100000: episode: 796, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 54.438, mean reward: 0.544 [0.194, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.998, 10.239], loss: 0.012320, mae: 0.117527, mean_q: 18.104359
 39847/100000: episode: 797, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 51.388, mean reward: 0.514 [0.258, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.038, 10.145], loss: 0.011844, mae: 0.117307, mean_q: 17.625257
 39947/100000: episode: 798, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 53.934, mean reward: 0.539 [0.177, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.332, 10.288], loss: 0.014074, mae: 0.129263, mean_q: 17.568331
 40047/100000: episode: 799, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 45.885, mean reward: 0.459 [0.153, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.732, 10.098], loss: 0.014930, mae: 0.128898, mean_q: 18.152792
 40147/100000: episode: 800, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 54.506, mean reward: 0.545 [0.357, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.231, 10.098], loss: 0.016450, mae: 0.137229, mean_q: 17.706102
 40247/100000: episode: 801, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 50.397, mean reward: 0.504 [0.247, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.763, 10.098], loss: 0.013591, mae: 0.123053, mean_q: 17.680994
 40347/100000: episode: 802, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 55.287, mean reward: 0.553 [0.254, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.588, 10.098], loss: 0.014833, mae: 0.128430, mean_q: 18.437107
 40447/100000: episode: 803, duration: 0.587s, episode steps: 100, steps per second: 171, episode reward: 52.312, mean reward: 0.523 [0.260, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.393, 10.146], loss: 0.014088, mae: 0.124668, mean_q: 18.695349
 40547/100000: episode: 804, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 53.316, mean reward: 0.533 [0.366, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.309, 10.098], loss: 0.015678, mae: 0.132671, mean_q: 19.284103
 40647/100000: episode: 805, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 51.583, mean reward: 0.516 [0.255, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.594, 10.122], loss: 0.012137, mae: 0.117204, mean_q: 19.149118
 40747/100000: episode: 806, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 53.259, mean reward: 0.533 [0.348, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.995, 10.098], loss: 0.013905, mae: 0.123446, mean_q: 19.552380
 40847/100000: episode: 807, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 48.867, mean reward: 0.489 [0.136, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.813, 10.298], loss: 0.014403, mae: 0.121232, mean_q: 19.625507
 40947/100000: episode: 808, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 51.220, mean reward: 0.512 [0.308, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.812, 10.196], loss: 0.015514, mae: 0.128255, mean_q: 19.510551
 41047/100000: episode: 809, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 48.133, mean reward: 0.481 [0.073, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.719, 10.148], loss: 0.016105, mae: 0.133466, mean_q: 19.413618
 41147/100000: episode: 810, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 53.825, mean reward: 0.538 [0.226, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.500, 10.194], loss: 0.019670, mae: 0.147151, mean_q: 19.547850
 41247/100000: episode: 811, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 55.174, mean reward: 0.552 [0.313, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.008, 10.301], loss: 0.016951, mae: 0.134165, mean_q: 19.718603
 41347/100000: episode: 812, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 55.795, mean reward: 0.558 [0.288, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.440, 10.170], loss: 0.015891, mae: 0.127760, mean_q: 19.530249
 41447/100000: episode: 813, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 56.896, mean reward: 0.569 [0.391, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.238, 10.098], loss: 0.017074, mae: 0.132487, mean_q: 19.619724
 41547/100000: episode: 814, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 50.191, mean reward: 0.502 [0.317, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.486, 10.098], loss: 0.014192, mae: 0.122777, mean_q: 19.459522
 41647/100000: episode: 815, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 54.446, mean reward: 0.544 [0.341, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.767, 10.139], loss: 0.017405, mae: 0.135640, mean_q: 19.443003
 41747/100000: episode: 816, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 50.943, mean reward: 0.509 [0.182, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.056, 10.098], loss: 0.014916, mae: 0.127617, mean_q: 19.858248
 41847/100000: episode: 817, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: 57.399, mean reward: 0.574 [0.323, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.250, 10.187], loss: 0.016784, mae: 0.132506, mean_q: 19.257460
 41947/100000: episode: 818, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 55.143, mean reward: 0.551 [0.320, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.719, 10.098], loss: 0.017173, mae: 0.137223, mean_q: 19.446217
 42047/100000: episode: 819, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 53.559, mean reward: 0.536 [0.270, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.700, 10.148], loss: 0.014003, mae: 0.120491, mean_q: 19.908537
 42147/100000: episode: 820, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 51.805, mean reward: 0.518 [0.364, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.522, 10.098], loss: 0.017680, mae: 0.131985, mean_q: 19.169891
 42247/100000: episode: 821, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 53.501, mean reward: 0.535 [0.254, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.389, 10.414], loss: 0.014231, mae: 0.118932, mean_q: 19.678539
 42347/100000: episode: 822, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 49.082, mean reward: 0.491 [0.235, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.053, 10.098], loss: 0.018989, mae: 0.138106, mean_q: 19.395466
 42447/100000: episode: 823, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 49.553, mean reward: 0.496 [0.066, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.924, 10.098], loss: 0.018545, mae: 0.134517, mean_q: 19.688101
 42547/100000: episode: 824, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 53.873, mean reward: 0.539 [0.278, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.624, 10.240], loss: 0.017841, mae: 0.133499, mean_q: 19.513771
 42647/100000: episode: 825, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 47.752, mean reward: 0.478 [0.214, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.818, 10.098], loss: 0.018869, mae: 0.130524, mean_q: 19.373995
 42747/100000: episode: 826, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 49.506, mean reward: 0.495 [0.209, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.685, 10.122], loss: 0.015026, mae: 0.120191, mean_q: 19.561260
 42847/100000: episode: 827, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 51.753, mean reward: 0.518 [0.252, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.355, 10.248], loss: 0.017954, mae: 0.131123, mean_q: 19.412966
 42947/100000: episode: 828, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 43.854, mean reward: 0.439 [0.034, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.714, 10.341], loss: 0.020153, mae: 0.137994, mean_q: 19.401731
 43047/100000: episode: 829, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.429, mean reward: 0.574 [0.327, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.540, 10.255], loss: 0.017178, mae: 0.131056, mean_q: 19.424635
 43147/100000: episode: 830, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 54.148, mean reward: 0.541 [0.320, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.336, 10.114], loss: 0.017309, mae: 0.131343, mean_q: 19.526962
 43247/100000: episode: 831, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 52.996, mean reward: 0.530 [0.334, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.513, 10.252], loss: 0.019708, mae: 0.136436, mean_q: 19.414082
 43347/100000: episode: 832, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 56.572, mean reward: 0.566 [0.359, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.978, 10.098], loss: 0.013573, mae: 0.120296, mean_q: 19.553982
 43447/100000: episode: 833, duration: 1.003s, episode steps: 100, steps per second: 100, episode reward: 50.082, mean reward: 0.501 [0.185, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.836, 10.098], loss: 0.017408, mae: 0.132356, mean_q: 19.848631
 43547/100000: episode: 834, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 56.869, mean reward: 0.569 [0.337, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.285], loss: 0.017383, mae: 0.131742, mean_q: 19.427515
 43647/100000: episode: 835, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 52.365, mean reward: 0.524 [0.161, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.611, 10.105], loss: 0.020191, mae: 0.141643, mean_q: 19.538891
 43747/100000: episode: 836, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 52.881, mean reward: 0.529 [0.275, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.787, 10.248], loss: 0.018189, mae: 0.139922, mean_q: 19.431316
 43847/100000: episode: 837, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 52.604, mean reward: 0.526 [0.266, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.315, 10.098], loss: 0.017112, mae: 0.132360, mean_q: 19.279085
 43947/100000: episode: 838, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 51.251, mean reward: 0.513 [0.283, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.583, 10.098], loss: 0.014634, mae: 0.120702, mean_q: 19.593466
 44047/100000: episode: 839, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 54.933, mean reward: 0.549 [0.258, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.097, 10.188], loss: 0.013423, mae: 0.119291, mean_q: 19.436668
 44147/100000: episode: 840, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 49.275, mean reward: 0.493 [0.213, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.350, 10.155], loss: 0.017411, mae: 0.132524, mean_q: 19.708149
 44247/100000: episode: 841, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 57.120, mean reward: 0.571 [0.410, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.192, 10.226], loss: 0.013978, mae: 0.123193, mean_q: 19.260460
 44347/100000: episode: 842, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 54.957, mean reward: 0.550 [0.341, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.673, 10.164], loss: 0.015394, mae: 0.123361, mean_q: 19.500534
 44447/100000: episode: 843, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 54.557, mean reward: 0.546 [0.340, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.618, 10.163], loss: 0.015584, mae: 0.131415, mean_q: 19.715321
 44547/100000: episode: 844, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 54.615, mean reward: 0.546 [0.392, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.608, 10.116], loss: 0.015001, mae: 0.126437, mean_q: 19.512823
 44647/100000: episode: 845, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 52.711, mean reward: 0.527 [0.297, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.794, 10.098], loss: 0.013311, mae: 0.120036, mean_q: 19.109865
 44747/100000: episode: 846, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.020, mean reward: 0.570 [0.373, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.202, 10.167], loss: 0.014106, mae: 0.122996, mean_q: 19.540699
 44847/100000: episode: 847, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 54.022, mean reward: 0.540 [0.291, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.211, 10.226], loss: 0.013632, mae: 0.125940, mean_q: 19.587603
 44947/100000: episode: 848, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 55.516, mean reward: 0.555 [0.310, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.251, 10.135], loss: 0.012296, mae: 0.116908, mean_q: 19.443558
 45047/100000: episode: 849, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 55.897, mean reward: 0.559 [0.183, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.138, 10.098], loss: 0.013573, mae: 0.125556, mean_q: 19.461916
 45147/100000: episode: 850, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 52.880, mean reward: 0.529 [0.330, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.385, 10.282], loss: 0.011325, mae: 0.117216, mean_q: 19.323212
 45247/100000: episode: 851, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 51.161, mean reward: 0.512 [0.317, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.993, 10.171], loss: 0.011141, mae: 0.112734, mean_q: 19.438854
 45347/100000: episode: 852, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 52.259, mean reward: 0.523 [0.301, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.968, 10.098], loss: 0.013674, mae: 0.127526, mean_q: 19.708759
 45447/100000: episode: 853, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 57.585, mean reward: 0.576 [0.295, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.548, 10.098], loss: 0.013477, mae: 0.126970, mean_q: 19.348711
 45547/100000: episode: 854, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 56.497, mean reward: 0.565 [0.367, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.043, 10.318], loss: 0.011903, mae: 0.114943, mean_q: 19.354631
 45647/100000: episode: 855, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 54.341, mean reward: 0.543 [0.242, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.770, 10.098], loss: 0.012923, mae: 0.119634, mean_q: 19.314285
[Info] New level: 0.32915055751800537 | Considering 12/88 traces
 45747/100000: episode: 856, duration: 4.669s, episode steps: 100, steps per second: 21, episode reward: 55.033, mean reward: 0.550 [0.297, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.959, 10.098], loss: 0.010862, mae: 0.109060, mean_q: 19.338625
 45749/100000: episode: 857, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.984, mean reward: 0.492 [0.467, 0.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.101, 10.100], loss: 0.010364, mae: 0.121823, mean_q: 21.368395
 45752/100000: episode: 858, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 0.855, mean reward: 0.285 [0.206, 0.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.208, 10.100], loss: 0.010055, mae: 0.103947, mean_q: 19.495489
 45754/100000: episode: 859, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.760, mean reward: 0.380 [0.370, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.201, 10.100], loss: 0.008701, mae: 0.102661, mean_q: 18.731911
 45757/100000: episode: 860, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 0.932, mean reward: 0.311 [0.258, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.305, 10.100], loss: 0.012084, mae: 0.126651, mean_q: 16.594818
 45760/100000: episode: 861, duration: 0.019s, episode steps: 3, steps per second: 161, episode reward: 1.175, mean reward: 0.392 [0.349, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.252, 10.100], loss: 0.010535, mae: 0.111981, mean_q: 20.438967
 45762/100000: episode: 862, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.949, mean reward: 0.475 [0.465, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.237, 10.100], loss: 0.012400, mae: 0.119391, mean_q: 20.304972
 45765/100000: episode: 863, duration: 0.021s, episode steps: 3, steps per second: 142, episode reward: 1.000, mean reward: 0.333 [0.248, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.327, 10.100], loss: 0.011369, mae: 0.115510, mean_q: 19.592300
 45767/100000: episode: 864, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.831, mean reward: 0.415 [0.394, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.141, 10.100], loss: 0.009686, mae: 0.105060, mean_q: 19.783209
 45769/100000: episode: 865, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.821, mean reward: 0.410 [0.406, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.214, 10.100], loss: 0.007338, mae: 0.097977, mean_q: 20.106184
 45773/100000: episode: 866, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 1.279, mean reward: 0.320 [0.285, 0.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.466, 10.100], loss: 0.015160, mae: 0.132201, mean_q: 18.494011
 45775/100000: episode: 867, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 1.188, mean reward: 0.594 [0.571, 0.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.230, 10.100], loss: 0.012230, mae: 0.124191, mean_q: 18.533436
 45779/100000: episode: 868, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 1.005, mean reward: 0.251 [0.174, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.322, 10.100], loss: 0.010785, mae: 0.112237, mean_q: 19.190868
 45782/100000: episode: 869, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.053, mean reward: 0.351 [0.323, 0.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.181, 10.100], loss: 0.024473, mae: 0.143979, mean_q: 18.988550
 45784/100000: episode: 870, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.905, mean reward: 0.452 [0.441, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.170, 10.100], loss: 0.013318, mae: 0.134314, mean_q: 20.038292
 45786/100000: episode: 871, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 1.087, mean reward: 0.544 [0.494, 0.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.142, 10.100], loss: 0.015524, mae: 0.139803, mean_q: 20.266670
 45788/100000: episode: 872, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 0.834, mean reward: 0.417 [0.357, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.233, 10.100], loss: 0.017099, mae: 0.154007, mean_q: 17.634186
 45790/100000: episode: 873, duration: 0.018s, episode steps: 2, steps per second: 114, episode reward: 0.795, mean reward: 0.398 [0.388, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.165, 10.100], loss: 0.014968, mae: 0.136752, mean_q: 19.173222
 45792/100000: episode: 874, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.969, mean reward: 0.484 [0.474, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.122, 10.100], loss: 0.019749, mae: 0.167983, mean_q: 19.780449
 45794/100000: episode: 875, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.749, mean reward: 0.374 [0.358, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.204, 10.100], loss: 0.009989, mae: 0.113549, mean_q: 19.422222
 45796/100000: episode: 876, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.782, mean reward: 0.391 [0.384, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.884, 10.100], loss: 0.011499, mae: 0.120862, mean_q: 18.351997
 45798/100000: episode: 877, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.077, mean reward: 0.538 [0.536, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.472, 10.100], loss: 0.006533, mae: 0.088031, mean_q: 17.360203
 45801/100000: episode: 878, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.297, mean reward: 0.432 [0.372, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.389, 10.100], loss: 0.010463, mae: 0.115419, mean_q: 19.198980
 45803/100000: episode: 879, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.942, mean reward: 0.471 [0.388, 0.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.223, 10.100], loss: 0.012613, mae: 0.120933, mean_q: 19.068829
 45805/100000: episode: 880, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.900, mean reward: 0.450 [0.398, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.088, 10.100], loss: 0.007596, mae: 0.095523, mean_q: 17.646818
 45807/100000: episode: 881, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.921, mean reward: 0.461 [0.436, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.222, 10.100], loss: 0.010469, mae: 0.107903, mean_q: 19.231142
 45809/100000: episode: 882, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 1.097, mean reward: 0.548 [0.538, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.230, 10.100], loss: 0.013217, mae: 0.115651, mean_q: 21.241753
 45811/100000: episode: 883, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 1.094, mean reward: 0.547 [0.500, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.203, 10.100], loss: 0.008143, mae: 0.099728, mean_q: 20.446762
 45813/100000: episode: 884, duration: 0.023s, episode steps: 2, steps per second: 89, episode reward: 1.032, mean reward: 0.516 [0.496, 0.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.212, 10.100], loss: 0.015572, mae: 0.140174, mean_q: 18.265415
 45815/100000: episode: 885, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 1.174, mean reward: 0.587 [0.577, 0.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.179, 10.100], loss: 0.009576, mae: 0.112344, mean_q: 18.083395
 45818/100000: episode: 886, duration: 0.029s, episode steps: 3, steps per second: 102, episode reward: 1.247, mean reward: 0.416 [0.407, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.386, 10.100], loss: 0.012672, mae: 0.121913, mean_q: 19.722544
 45820/100000: episode: 887, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 0.881, mean reward: 0.441 [0.430, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.188, 10.100], loss: 0.016581, mae: 0.138735, mean_q: 19.832447
 45824/100000: episode: 888, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 1.051, mean reward: 0.263 [0.229, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.481, 10.100], loss: 0.010195, mae: 0.112071, mean_q: 19.995876
 45827/100000: episode: 889, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 1.139, mean reward: 0.380 [0.373, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.217, 10.100], loss: 0.011733, mae: 0.119038, mean_q: 18.567204
 45829/100000: episode: 890, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.864, mean reward: 0.432 [0.408, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.268, 10.100], loss: 0.011524, mae: 0.116362, mean_q: 20.329865
 45833/100000: episode: 891, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 1.145, mean reward: 0.286 [0.246, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.414, 10.100], loss: 0.014564, mae: 0.117737, mean_q: 20.015522
 45837/100000: episode: 892, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 0.973, mean reward: 0.243 [0.216, 0.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.329, 10.100], loss: 0.012431, mae: 0.121208, mean_q: 20.372700
 45840/100000: episode: 893, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 1.330, mean reward: 0.443 [0.349, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.355, 10.100], loss: 0.007960, mae: 0.098314, mean_q: 19.146042
 45842/100000: episode: 894, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.887, mean reward: 0.444 [0.408, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.125, 10.100], loss: 0.010304, mae: 0.117414, mean_q: 18.766945
 45844/100000: episode: 895, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.835, mean reward: 0.417 [0.399, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.246, 10.100], loss: 0.008415, mae: 0.103562, mean_q: 20.614561
 45846/100000: episode: 896, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.939, mean reward: 0.470 [0.457, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.195, 10.100], loss: 0.009467, mae: 0.097823, mean_q: 19.980721
 45848/100000: episode: 897, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.861, mean reward: 0.431 [0.352, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.216, 10.100], loss: 0.010040, mae: 0.103548, mean_q: 19.107765
 45850/100000: episode: 898, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 1.290, mean reward: 0.645 [0.640, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.117, 10.100], loss: 0.008846, mae: 0.096048, mean_q: 19.649950
 45852/100000: episode: 899, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 1.092, mean reward: 0.546 [0.509, 0.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.178, 10.100], loss: 0.008518, mae: 0.105945, mean_q: 18.686661
 45855/100000: episode: 900, duration: 0.020s, episode steps: 3, steps per second: 150, episode reward: 1.409, mean reward: 0.470 [0.447, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.384, 10.100], loss: 0.009979, mae: 0.111918, mean_q: 19.549583
 45859/100000: episode: 901, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 1.073, mean reward: 0.268 [0.217, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.386, 10.100], loss: 0.017424, mae: 0.121304, mean_q: 20.697067
 45861/100000: episode: 902, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.152, mean reward: 0.576 [0.520, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.146, 10.100], loss: 0.009552, mae: 0.107946, mean_q: 20.469311
 45863/100000: episode: 903, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.810, mean reward: 0.405 [0.384, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.250, 10.100], loss: 0.008144, mae: 0.102759, mean_q: 18.837717
 45865/100000: episode: 904, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.748, mean reward: 0.374 [0.303, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.227, 10.100], loss: 0.006650, mae: 0.093659, mean_q: 15.718853
 45869/100000: episode: 905, duration: 0.036s, episode steps: 4, steps per second: 112, episode reward: 1.139, mean reward: 0.285 [0.252, 0.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.368, 10.100], loss: 0.010985, mae: 0.113942, mean_q: 19.333485
 45871/100000: episode: 906, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.913, mean reward: 0.456 [0.444, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.255, 10.100], loss: 0.010483, mae: 0.115520, mean_q: 18.007694
 45873/100000: episode: 907, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.993, mean reward: 0.496 [0.483, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.130, 10.100], loss: 0.012937, mae: 0.123409, mean_q: 18.350563
 45875/100000: episode: 908, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 1.087, mean reward: 0.544 [0.528, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.224, 10.100], loss: 0.005831, mae: 0.082494, mean_q: 18.252777
 45878/100000: episode: 909, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 1.041, mean reward: 0.347 [0.320, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.237, 10.100], loss: 0.020449, mae: 0.143476, mean_q: 18.579237
 45880/100000: episode: 910, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.033, mean reward: 0.516 [0.500, 0.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.154, 10.100], loss: 0.020107, mae: 0.142545, mean_q: 20.852745
 45882/100000: episode: 911, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.548, mean reward: 0.274 [0.225, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.270, 10.100], loss: 0.007854, mae: 0.097727, mean_q: 18.442215
 45884/100000: episode: 912, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.886, mean reward: 0.443 [0.443, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.113, 10.100], loss: 0.011407, mae: 0.121696, mean_q: 18.389616
 45886/100000: episode: 913, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 1.061, mean reward: 0.531 [0.500, 0.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.206, 10.100], loss: 0.008750, mae: 0.106318, mean_q: 20.790892
 45889/100000: episode: 914, duration: 0.027s, episode steps: 3, steps per second: 112, episode reward: 1.402, mean reward: 0.467 [0.374, 0.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.328, 10.100], loss: 0.012815, mae: 0.123697, mean_q: 18.050604
 45891/100000: episode: 915, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.807, mean reward: 0.403 [0.392, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.154, 10.100], loss: 0.012119, mae: 0.125948, mean_q: 18.429256
 45893/100000: episode: 916, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.779, mean reward: 0.389 [0.341, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.168, 10.100], loss: 0.009094, mae: 0.105002, mean_q: 19.712601
 45895/100000: episode: 917, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 0.510, mean reward: 0.255 [0.250, 0.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.225, 10.100], loss: 0.006730, mae: 0.092564, mean_q: 18.018307
 45897/100000: episode: 918, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.863, mean reward: 0.431 [0.378, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.837, 10.100], loss: 0.010181, mae: 0.111929, mean_q: 19.944641
 45899/100000: episode: 919, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.929, mean reward: 0.464 [0.452, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.239, 10.100], loss: 0.011352, mae: 0.114954, mean_q: 18.557709
 45901/100000: episode: 920, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.853, mean reward: 0.427 [0.426, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.158, 10.100], loss: 0.008140, mae: 0.102719, mean_q: 21.274178
 45903/100000: episode: 921, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.971, mean reward: 0.486 [0.468, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.156, 10.100], loss: 0.016846, mae: 0.141516, mean_q: 17.954216
 45905/100000: episode: 922, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 1.028, mean reward: 0.514 [0.503, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.125, 10.100], loss: 0.015679, mae: 0.128423, mean_q: 17.982285
 45908/100000: episode: 923, duration: 0.025s, episode steps: 3, steps per second: 119, episode reward: 0.978, mean reward: 0.326 [0.297, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.115, 10.100], loss: 0.012725, mae: 0.116961, mean_q: 19.512276
 45910/100000: episode: 924, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.904, mean reward: 0.452 [0.394, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.175, 10.100], loss: 0.010699, mae: 0.107186, mean_q: 17.860603
 45912/100000: episode: 925, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 1.252, mean reward: 0.626 [0.619, 0.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.167, 10.100], loss: 0.010024, mae: 0.107773, mean_q: 17.744789
 45914/100000: episode: 926, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.954, mean reward: 0.477 [0.461, 0.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.272, 10.100], loss: 0.007778, mae: 0.096801, mean_q: 18.701366
 45916/100000: episode: 927, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 0.987, mean reward: 0.493 [0.463, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.214, 10.100], loss: 0.014487, mae: 0.107169, mean_q: 16.757351
 45918/100000: episode: 928, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.887, mean reward: 0.443 [0.360, 0.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.224, 10.100], loss: 0.009349, mae: 0.109040, mean_q: 19.816021
 45920/100000: episode: 929, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 0.886, mean reward: 0.443 [0.436, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.230, 10.100], loss: 0.010799, mae: 0.113059, mean_q: 18.150015
 45922/100000: episode: 930, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.886, mean reward: 0.443 [0.402, 0.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.290, 10.100], loss: 0.009105, mae: 0.093004, mean_q: 18.908211
 45924/100000: episode: 931, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.813, mean reward: 0.407 [0.377, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.226, 10.100], loss: 0.011594, mae: 0.116798, mean_q: 18.183603
 45926/100000: episode: 932, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.839, mean reward: 0.420 [0.407, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.225, 10.100], loss: 0.008317, mae: 0.103307, mean_q: 19.873552
 45928/100000: episode: 933, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 0.803, mean reward: 0.402 [0.400, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.209, 10.100], loss: 0.011257, mae: 0.126110, mean_q: 19.365942
 45932/100000: episode: 934, duration: 0.024s, episode steps: 4, steps per second: 166, episode reward: 1.069, mean reward: 0.267 [0.226, 0.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.361, 10.100], loss: 0.010368, mae: 0.112857, mean_q: 18.209526
 45934/100000: episode: 935, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.688, mean reward: 0.344 [0.311, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.195, 10.100], loss: 0.012253, mae: 0.118809, mean_q: 20.089188
 45937/100000: episode: 936, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 1.205, mean reward: 0.402 [0.387, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.182, 10.100], loss: 0.011792, mae: 0.102455, mean_q: 17.817396
 45939/100000: episode: 937, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.969, mean reward: 0.485 [0.472, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.120, 10.100], loss: 0.020137, mae: 0.141078, mean_q: 17.165873
 45942/100000: episode: 938, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 0.934, mean reward: 0.311 [0.251, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.242, 10.100], loss: 0.014218, mae: 0.135128, mean_q: 18.468908
 45944/100000: episode: 939, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 0.938, mean reward: 0.469 [0.459, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.082, 10.100], loss: 0.019137, mae: 0.167872, mean_q: 17.337574
 45947/100000: episode: 940, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 1.598, mean reward: 0.533 [0.484, 0.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.344, 10.100], loss: 0.015058, mae: 0.135841, mean_q: 19.250830
 45949/100000: episode: 941, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.840, mean reward: 0.420 [0.415, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.282, 10.100], loss: 0.018280, mae: 0.136670, mean_q: 16.769718
 45953/100000: episode: 942, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 1.111, mean reward: 0.278 [0.220, 0.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.363, 10.100], loss: 0.013578, mae: 0.132293, mean_q: 17.754330
 45955/100000: episode: 943, duration: 0.018s, episode steps: 2, steps per second: 110, episode reward: 0.822, mean reward: 0.411 [0.375, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.262, 10.100], loss: 0.020887, mae: 0.161958, mean_q: 16.711910
[Info] Not found new level, current best level reached = 0.32915055751800537
 45957/100000: episode: 944, duration: 4.173s, episode steps: 2, steps per second: 0, episode reward: 0.983, mean reward: 0.491 [0.437, 0.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.250, 10.100], loss: 0.019192, mae: 0.155587, mean_q: 17.922737
 46057/100000: episode: 945, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 53.342, mean reward: 0.533 [0.347, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.637, 10.134], loss: 0.012656, mae: 0.119501, mean_q: 18.657061
 46157/100000: episode: 946, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 53.092, mean reward: 0.531 [0.292, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.084, 10.282], loss: 0.015102, mae: 0.131032, mean_q: 18.823298
 46257/100000: episode: 947, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 52.892, mean reward: 0.529 [0.301, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.278, 10.202], loss: 0.014345, mae: 0.127216, mean_q: 18.762693
 46357/100000: episode: 948, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 53.380, mean reward: 0.534 [0.273, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.572, 10.098], loss: 0.013289, mae: 0.122312, mean_q: 18.683680
 46457/100000: episode: 949, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 55.598, mean reward: 0.556 [0.286, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.557, 10.231], loss: 0.015355, mae: 0.134588, mean_q: 19.020428
 46557/100000: episode: 950, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 51.961, mean reward: 0.520 [0.274, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.850, 10.098], loss: 0.015375, mae: 0.130301, mean_q: 18.630255
 46657/100000: episode: 951, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 55.633, mean reward: 0.556 [0.323, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.438, 10.202], loss: 0.017302, mae: 0.137572, mean_q: 18.716291
 46757/100000: episode: 952, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 52.100, mean reward: 0.521 [0.299, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.343, 10.098], loss: 0.014803, mae: 0.129056, mean_q: 18.900097
 46857/100000: episode: 953, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 54.332, mean reward: 0.543 [0.308, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.730, 10.163], loss: 0.014517, mae: 0.126683, mean_q: 18.552029
 46957/100000: episode: 954, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 52.225, mean reward: 0.522 [0.290, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.395, 10.098], loss: 0.015783, mae: 0.132150, mean_q: 18.745842
 47057/100000: episode: 955, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 55.432, mean reward: 0.554 [0.323, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.992, 10.245], loss: 0.013924, mae: 0.122513, mean_q: 18.625343
 47157/100000: episode: 956, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 53.060, mean reward: 0.531 [0.294, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.962, 10.276], loss: 0.015195, mae: 0.125708, mean_q: 18.870157
 47257/100000: episode: 957, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 51.603, mean reward: 0.516 [0.295, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.839, 10.417], loss: 0.016887, mae: 0.137049, mean_q: 19.089043
 47357/100000: episode: 958, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 55.731, mean reward: 0.557 [0.328, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.213, 10.098], loss: 0.016785, mae: 0.136542, mean_q: 18.696829
 47457/100000: episode: 959, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 50.682, mean reward: 0.507 [0.284, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.506, 10.098], loss: 0.014021, mae: 0.123339, mean_q: 19.093382
 47557/100000: episode: 960, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 56.436, mean reward: 0.564 [0.390, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.777, 10.380], loss: 0.013209, mae: 0.118327, mean_q: 19.012384
 47657/100000: episode: 961, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 56.251, mean reward: 0.563 [0.352, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.836, 10.098], loss: 0.015673, mae: 0.129666, mean_q: 18.987417
 47757/100000: episode: 962, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.230, mean reward: 0.572 [0.377, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.689, 10.207], loss: 0.016424, mae: 0.131703, mean_q: 18.649725
 47857/100000: episode: 963, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 54.423, mean reward: 0.544 [0.332, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.914, 10.100], loss: 0.015227, mae: 0.130701, mean_q: 18.901932
 47957/100000: episode: 964, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 54.426, mean reward: 0.544 [0.321, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.349, 10.098], loss: 0.015470, mae: 0.128386, mean_q: 18.896742
 48057/100000: episode: 965, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 54.279, mean reward: 0.543 [0.337, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.535, 10.098], loss: 0.014958, mae: 0.125689, mean_q: 18.821873
 48157/100000: episode: 966, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 45.842, mean reward: 0.458 [0.237, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.642, 10.219], loss: 0.015663, mae: 0.130078, mean_q: 18.709862
 48257/100000: episode: 967, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 49.513, mean reward: 0.495 [0.183, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.883, 10.098], loss: 0.018774, mae: 0.145509, mean_q: 18.902988
 48357/100000: episode: 968, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 54.583, mean reward: 0.546 [0.319, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.408, 10.171], loss: 0.013756, mae: 0.124079, mean_q: 18.728149
 48457/100000: episode: 969, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 49.915, mean reward: 0.499 [0.305, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.454, 10.098], loss: 0.014040, mae: 0.124396, mean_q: 19.086750
 48557/100000: episode: 970, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 47.207, mean reward: 0.472 [0.241, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.802, 10.098], loss: 0.015129, mae: 0.128731, mean_q: 18.745356
 48657/100000: episode: 971, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 53.623, mean reward: 0.536 [0.267, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.822, 10.098], loss: 0.014841, mae: 0.124180, mean_q: 18.788128
 48757/100000: episode: 972, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 54.528, mean reward: 0.545 [0.274, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.932, 10.214], loss: 0.015522, mae: 0.126676, mean_q: 18.626421
 48857/100000: episode: 973, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 53.684, mean reward: 0.537 [0.272, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.863, 10.098], loss: 0.016208, mae: 0.134701, mean_q: 18.803673
 48957/100000: episode: 974, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 53.791, mean reward: 0.538 [0.285, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.697, 10.159], loss: 0.014218, mae: 0.120840, mean_q: 18.977842
 49057/100000: episode: 975, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 56.619, mean reward: 0.566 [0.246, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.054, 10.098], loss: 0.015708, mae: 0.128029, mean_q: 19.031622
 49157/100000: episode: 976, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 50.752, mean reward: 0.508 [0.332, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.017, 10.223], loss: 0.012172, mae: 0.114419, mean_q: 18.874878
 49257/100000: episode: 977, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 53.963, mean reward: 0.540 [0.273, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.766, 10.098], loss: 0.014648, mae: 0.122120, mean_q: 19.219978
 49357/100000: episode: 978, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 55.589, mean reward: 0.556 [0.368, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.081, 10.098], loss: 0.012015, mae: 0.114142, mean_q: 18.751450
 49457/100000: episode: 979, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 55.164, mean reward: 0.552 [0.317, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.761, 10.318], loss: 0.013663, mae: 0.116729, mean_q: 18.954844
 49557/100000: episode: 980, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 50.506, mean reward: 0.505 [0.194, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.049, 10.098], loss: 0.013847, mae: 0.116992, mean_q: 19.297237
 49657/100000: episode: 981, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 52.401, mean reward: 0.524 [0.324, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.805, 10.098], loss: 0.015390, mae: 0.129616, mean_q: 18.951336
 49757/100000: episode: 982, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 52.942, mean reward: 0.529 [0.316, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.516, 10.098], loss: 0.013648, mae: 0.122135, mean_q: 18.736811
 49857/100000: episode: 983, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 51.623, mean reward: 0.516 [0.320, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.190, 10.098], loss: 0.012957, mae: 0.118873, mean_q: 19.060301
 49957/100000: episode: 984, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 52.514, mean reward: 0.525 [0.297, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.750, 10.098], loss: 0.013906, mae: 0.119677, mean_q: 19.060757
 50057/100000: episode: 985, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 54.664, mean reward: 0.547 [0.334, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.397, 10.098], loss: 0.014862, mae: 0.127976, mean_q: 18.872381
 50157/100000: episode: 986, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 55.201, mean reward: 0.552 [0.307, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.423, 10.098], loss: 0.014596, mae: 0.123934, mean_q: 19.176933
 50257/100000: episode: 987, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 54.712, mean reward: 0.547 [0.306, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.142, 10.125], loss: 0.013887, mae: 0.119715, mean_q: 19.047697
 50357/100000: episode: 988, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 54.763, mean reward: 0.548 [0.345, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.881, 10.098], loss: 0.013486, mae: 0.118817, mean_q: 19.005112
 50457/100000: episode: 989, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 51.941, mean reward: 0.519 [0.313, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.151, 10.272], loss: 0.014674, mae: 0.127664, mean_q: 18.731262
 50557/100000: episode: 990, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 49.840, mean reward: 0.498 [0.251, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.564, 10.310], loss: 0.014149, mae: 0.126680, mean_q: 18.641150
 50657/100000: episode: 991, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 53.279, mean reward: 0.533 [0.322, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.711, 10.098], loss: 0.011337, mae: 0.111573, mean_q: 19.215313
 50757/100000: episode: 992, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 46.392, mean reward: 0.464 [0.236, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.601, 10.098], loss: 0.012000, mae: 0.115100, mean_q: 19.033913
 50857/100000: episode: 993, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 53.748, mean reward: 0.537 [0.286, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.859, 10.224], loss: 0.013431, mae: 0.124115, mean_q: 19.507318
 50957/100000: episode: 994, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 51.981, mean reward: 0.520 [0.276, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.857, 10.098], loss: 0.011925, mae: 0.115789, mean_q: 19.881765
 51057/100000: episode: 995, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 53.520, mean reward: 0.535 [0.291, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.280, 10.316], loss: 0.015698, mae: 0.131538, mean_q: 19.707930
 51157/100000: episode: 996, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 54.780, mean reward: 0.548 [0.305, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.184, 10.098], loss: 0.014931, mae: 0.125985, mean_q: 19.954273
 51257/100000: episode: 997, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 53.139, mean reward: 0.531 [0.240, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.655, 10.098], loss: 0.014524, mae: 0.125715, mean_q: 19.644924
 51357/100000: episode: 998, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 55.210, mean reward: 0.552 [0.279, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.280, 10.098], loss: 0.011932, mae: 0.114716, mean_q: 19.847235
 51457/100000: episode: 999, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 54.349, mean reward: 0.543 [0.282, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.299, 10.098], loss: 0.012128, mae: 0.114003, mean_q: 19.755152
 51557/100000: episode: 1000, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 54.704, mean reward: 0.547 [0.338, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.601, 10.098], loss: 0.014685, mae: 0.126700, mean_q: 19.693590
 51657/100000: episode: 1001, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 54.151, mean reward: 0.542 [0.280, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.213, 10.098], loss: 0.011935, mae: 0.115094, mean_q: 19.874304
 51757/100000: episode: 1002, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 55.274, mean reward: 0.553 [0.355, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.774, 10.110], loss: 0.013394, mae: 0.120214, mean_q: 19.781584
 51857/100000: episode: 1003, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 54.184, mean reward: 0.542 [0.302, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.206, 10.098], loss: 0.013494, mae: 0.117849, mean_q: 19.834389
 51957/100000: episode: 1004, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 55.970, mean reward: 0.560 [0.299, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.560, 10.136], loss: 0.011275, mae: 0.110657, mean_q: 19.669405
 52057/100000: episode: 1005, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 54.530, mean reward: 0.545 [0.278, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.981, 10.098], loss: 0.012537, mae: 0.113518, mean_q: 19.510914
 52157/100000: episode: 1006, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 50.459, mean reward: 0.505 [0.255, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-2.998, 10.193], loss: 0.013039, mae: 0.116581, mean_q: 19.670612
 52257/100000: episode: 1007, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 55.881, mean reward: 0.559 [0.330, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.213, 10.098], loss: 0.013427, mae: 0.120005, mean_q: 19.894150
 52357/100000: episode: 1008, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 51.666, mean reward: 0.517 [0.333, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.815, 10.410], loss: 0.011852, mae: 0.114775, mean_q: 19.681595
 52457/100000: episode: 1009, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 53.433, mean reward: 0.534 [0.282, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.482, 10.220], loss: 0.012320, mae: 0.118091, mean_q: 19.653254
 52557/100000: episode: 1010, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 55.453, mean reward: 0.555 [0.362, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.863, 10.141], loss: 0.013110, mae: 0.118802, mean_q: 19.653954
 52657/100000: episode: 1011, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 49.318, mean reward: 0.493 [0.196, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.974, 10.272], loss: 0.013477, mae: 0.123211, mean_q: 19.878809
 52757/100000: episode: 1012, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 52.322, mean reward: 0.523 [0.266, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.905, 10.180], loss: 0.013505, mae: 0.119606, mean_q: 19.503435
 52857/100000: episode: 1013, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 54.723, mean reward: 0.547 [0.323, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.028, 10.098], loss: 0.015809, mae: 0.125983, mean_q: 19.827042
 52957/100000: episode: 1014, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 52.754, mean reward: 0.528 [0.329, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.036, 10.371], loss: 0.013880, mae: 0.123543, mean_q: 19.626860
 53057/100000: episode: 1015, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 52.410, mean reward: 0.524 [0.321, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.485, 10.136], loss: 0.016523, mae: 0.132601, mean_q: 19.505983
 53157/100000: episode: 1016, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 51.893, mean reward: 0.519 [0.350, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.780, 10.399], loss: 0.013421, mae: 0.122455, mean_q: 19.585079
 53257/100000: episode: 1017, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 55.117, mean reward: 0.551 [0.324, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.503, 10.178], loss: 0.011262, mae: 0.110866, mean_q: 19.830572
 53357/100000: episode: 1018, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 54.360, mean reward: 0.544 [0.314, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.774, 10.098], loss: 0.012856, mae: 0.115223, mean_q: 19.687654
 53457/100000: episode: 1019, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 50.746, mean reward: 0.507 [0.233, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.126, 10.343], loss: 0.015561, mae: 0.129196, mean_q: 19.720543
 53557/100000: episode: 1020, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 56.632, mean reward: 0.566 [0.333, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.687, 10.273], loss: 0.012647, mae: 0.116805, mean_q: 19.809141
 53657/100000: episode: 1021, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 52.549, mean reward: 0.525 [0.310, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.703, 10.221], loss: 0.014390, mae: 0.122560, mean_q: 19.870819
 53757/100000: episode: 1022, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 53.478, mean reward: 0.535 [0.337, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.640, 10.098], loss: 0.010861, mae: 0.106281, mean_q: 19.693611
 53857/100000: episode: 1023, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.829, mean reward: 0.538 [0.247, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.537, 10.415], loss: 0.013575, mae: 0.118487, mean_q: 19.549406
 53957/100000: episode: 1024, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 51.167, mean reward: 0.512 [0.257, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.681, 10.098], loss: 0.015356, mae: 0.122283, mean_q: 19.702179
 54057/100000: episode: 1025, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 49.638, mean reward: 0.496 [0.266, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.210, 10.098], loss: 0.012366, mae: 0.114106, mean_q: 19.898521
 54157/100000: episode: 1026, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 56.905, mean reward: 0.569 [0.324, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.301, 10.098], loss: 0.013844, mae: 0.120124, mean_q: 19.686340
 54257/100000: episode: 1027, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 48.818, mean reward: 0.488 [0.308, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.989, 10.098], loss: 0.011872, mae: 0.113396, mean_q: 19.866034
 54357/100000: episode: 1028, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 55.280, mean reward: 0.553 [0.241, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.691, 10.243], loss: 0.012241, mae: 0.115738, mean_q: 19.736221
 54457/100000: episode: 1029, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 51.237, mean reward: 0.512 [0.304, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.385, 10.098], loss: 0.013093, mae: 0.123523, mean_q: 19.765284
 54557/100000: episode: 1030, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 49.757, mean reward: 0.498 [0.245, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.611, 10.349], loss: 0.013122, mae: 0.117845, mean_q: 19.704409
 54657/100000: episode: 1031, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 56.093, mean reward: 0.561 [0.195, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.928, 10.098], loss: 0.011734, mae: 0.110523, mean_q: 19.721233
 54757/100000: episode: 1032, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 53.725, mean reward: 0.537 [0.399, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.909, 10.155], loss: 0.012982, mae: 0.120855, mean_q: 19.642340
 54857/100000: episode: 1033, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 54.993, mean reward: 0.550 [0.238, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.091, 10.098], loss: 0.012270, mae: 0.115183, mean_q: 19.753819
 54957/100000: episode: 1034, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 56.524, mean reward: 0.565 [0.365, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.863, 10.098], loss: 0.013591, mae: 0.126650, mean_q: 19.711311
 55057/100000: episode: 1035, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 48.544, mean reward: 0.485 [0.253, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.028, 10.098], loss: 0.013740, mae: 0.124595, mean_q: 19.752924
 55157/100000: episode: 1036, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.693, mean reward: 0.587 [0.247, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.728, 10.098], loss: 0.013561, mae: 0.123305, mean_q: 19.818504
 55257/100000: episode: 1037, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 54.139, mean reward: 0.541 [0.302, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.262, 10.239], loss: 0.014952, mae: 0.130430, mean_q: 20.068314
 55357/100000: episode: 1038, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 51.725, mean reward: 0.517 [0.331, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.376, 10.098], loss: 0.014394, mae: 0.123750, mean_q: 19.191536
 55457/100000: episode: 1039, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 54.019, mean reward: 0.540 [0.311, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.203, 10.159], loss: 0.013201, mae: 0.121313, mean_q: 19.833555
 55557/100000: episode: 1040, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 49.335, mean reward: 0.493 [0.280, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.647, 10.098], loss: 0.013918, mae: 0.124657, mean_q: 19.889935
 55657/100000: episode: 1041, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 48.248, mean reward: 0.482 [0.295, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.099, 10.374], loss: 0.016141, mae: 0.138870, mean_q: 19.717676
 55757/100000: episode: 1042, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 53.195, mean reward: 0.532 [0.130, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.995, 10.199], loss: 0.013705, mae: 0.126614, mean_q: 19.479647
 55857/100000: episode: 1043, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 52.161, mean reward: 0.522 [0.225, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.018, 10.098], loss: 0.014072, mae: 0.126800, mean_q: 19.561718
[Info] New level: 0.735599160194397 | Considering 10/90 traces
 55957/100000: episode: 1044, duration: 4.439s, episode steps: 100, steps per second: 23, episode reward: 51.900, mean reward: 0.519 [0.322, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.741, 10.135], loss: 0.014947, mae: 0.132356, mean_q: 19.731956
 55959/100000: episode: 1045, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.871, mean reward: 0.435 [0.432, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.267, 10.100], loss: 0.016992, mae: 0.126496, mean_q: 17.663036
 55961/100000: episode: 1046, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.630, mean reward: 0.315 [0.243, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.315, 10.100], loss: 0.011930, mae: 0.110962, mean_q: 19.459036
 55963/100000: episode: 1047, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.872, mean reward: 0.436 [0.432, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.387, 10.100], loss: 0.011483, mae: 0.120508, mean_q: 20.796215
 55965/100000: episode: 1048, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.817, mean reward: 0.409 [0.404, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.235, 10.100], loss: 0.010110, mae: 0.108644, mean_q: 18.008301
 55967/100000: episode: 1049, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.976, mean reward: 0.488 [0.484, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.162, 10.100], loss: 0.017647, mae: 0.134058, mean_q: 19.510691
 55969/100000: episode: 1050, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.753, mean reward: 0.376 [0.366, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.328, 10.100], loss: 0.008969, mae: 0.096871, mean_q: 22.572313
 55971/100000: episode: 1051, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 0.906, mean reward: 0.453 [0.449, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.312, 10.100], loss: 0.014474, mae: 0.123995, mean_q: 18.446142
 55973/100000: episode: 1052, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.806, mean reward: 0.403 [0.374, 0.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.356, 10.100], loss: 0.023631, mae: 0.120082, mean_q: 18.406023
 55975/100000: episode: 1053, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.887, mean reward: 0.443 [0.400, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.308, 10.100], loss: 0.009216, mae: 0.116046, mean_q: 20.603085
 55977/100000: episode: 1054, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.673, mean reward: 0.336 [0.326, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.339, 10.100], loss: 0.012384, mae: 0.116541, mean_q: 20.821033
 55979/100000: episode: 1055, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.966, mean reward: 0.483 [0.466, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.259, 10.100], loss: 0.010846, mae: 0.117231, mean_q: 20.537868
 55981/100000: episode: 1056, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.884, mean reward: 0.442 [0.437, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.320, 10.100], loss: 0.016057, mae: 0.135698, mean_q: 20.367266
 55983/100000: episode: 1057, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.823, mean reward: 0.412 [0.402, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.881, 10.100], loss: 0.018676, mae: 0.167080, mean_q: 19.194952
 55985/100000: episode: 1058, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.924, mean reward: 0.462 [0.440, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.333, 10.100], loss: 0.018983, mae: 0.151293, mean_q: 19.648521
 55987/100000: episode: 1059, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 0.925, mean reward: 0.462 [0.462, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.302, 10.100], loss: 0.014125, mae: 0.135173, mean_q: 20.110491
 55989/100000: episode: 1060, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.747, mean reward: 0.373 [0.362, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.295, 10.100], loss: 0.014091, mae: 0.126399, mean_q: 20.478321
 55991/100000: episode: 1061, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.910, mean reward: 0.455 [0.438, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.255, 10.100], loss: 0.011655, mae: 0.124424, mean_q: 21.676443
 55993/100000: episode: 1062, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.828, mean reward: 0.414 [0.402, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.345, 10.100], loss: 0.008759, mae: 0.105295, mean_q: 19.326599
 55995/100000: episode: 1063, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.832, mean reward: 0.416 [0.416, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.327, 10.100], loss: 0.029594, mae: 0.137065, mean_q: 20.882771
 55997/100000: episode: 1064, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.968, mean reward: 0.484 [0.482, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.306, 10.100], loss: 0.013433, mae: 0.130879, mean_q: 19.963718
 55999/100000: episode: 1065, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.952, mean reward: 0.476 [0.475, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.277, 10.100], loss: 0.022012, mae: 0.165159, mean_q: 19.329889
 56001/100000: episode: 1066, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.931, mean reward: 0.465 [0.441, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.360, 10.100], loss: 0.015580, mae: 0.144419, mean_q: 18.435774
 56003/100000: episode: 1067, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.940, mean reward: 0.470 [0.446, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.370, 10.100], loss: 0.020118, mae: 0.153078, mean_q: 18.793976
 56005/100000: episode: 1068, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.910, mean reward: 0.455 [0.439, 0.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.328, 10.100], loss: 0.012503, mae: 0.118197, mean_q: 18.724556
 56007/100000: episode: 1069, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.863, mean reward: 0.432 [0.364, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.241, 10.100], loss: 0.019821, mae: 0.153998, mean_q: 20.923016
 56009/100000: episode: 1070, duration: 0.016s, episode steps: 2, steps per second: 121, episode reward: 0.982, mean reward: 0.491 [0.480, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.256, 10.100], loss: 0.009736, mae: 0.102586, mean_q: 18.940382
 56011/100000: episode: 1071, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.949, mean reward: 0.474 [0.435, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.228, 10.100], loss: 0.016004, mae: 0.138869, mean_q: 20.949162
 56013/100000: episode: 1072, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 0.600, mean reward: 0.300 [0.283, 0.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.191, 10.100], loss: 0.018674, mae: 0.152976, mean_q: 19.272192
 56015/100000: episode: 1073, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.828, mean reward: 0.414 [0.392, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.326, 10.100], loss: 0.027842, mae: 0.168212, mean_q: 18.435507
 56017/100000: episode: 1074, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.936, mean reward: 0.468 [0.462, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.279, 10.100], loss: 0.014418, mae: 0.131853, mean_q: 19.897604
 56019/100000: episode: 1075, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.000, mean reward: 0.500 [0.497, 0.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.287, 10.100], loss: 0.011402, mae: 0.115681, mean_q: 17.955595
 56021/100000: episode: 1076, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.622, mean reward: 0.311 [0.245, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.466, 10.100], loss: 0.029353, mae: 0.155420, mean_q: 17.883347
 56023/100000: episode: 1077, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 0.880, mean reward: 0.440 [0.440, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.256, 10.100], loss: 0.018103, mae: 0.121262, mean_q: 19.487101
 56025/100000: episode: 1078, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.820, mean reward: 0.410 [0.381, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.204, 10.100], loss: 0.020274, mae: 0.155371, mean_q: 18.971085
 56027/100000: episode: 1079, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 0.916, mean reward: 0.458 [0.432, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.365, 10.100], loss: 0.008858, mae: 0.097313, mean_q: 18.022045
 56029/100000: episode: 1080, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.853, mean reward: 0.426 [0.363, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.197, 10.100], loss: 0.017565, mae: 0.124341, mean_q: 17.492552
 56031/100000: episode: 1081, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.566, mean reward: 0.283 [0.269, 0.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.174, 10.100], loss: 0.010617, mae: 0.120317, mean_q: 19.905081
 56033/100000: episode: 1082, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 0.876, mean reward: 0.438 [0.421, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.226, 10.100], loss: 0.010802, mae: 0.115944, mean_q: 17.594460
 56035/100000: episode: 1083, duration: 0.016s, episode steps: 2, steps per second: 128, episode reward: 0.888, mean reward: 0.444 [0.379, 0.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.306, 10.100], loss: 0.021199, mae: 0.162899, mean_q: 18.986317
 56037/100000: episode: 1084, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.922, mean reward: 0.461 [0.459, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.375, 10.100], loss: 0.021240, mae: 0.155230, mean_q: 17.311298
 56039/100000: episode: 1085, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.930, mean reward: 0.465 [0.440, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.374, 10.100], loss: 0.010467, mae: 0.122425, mean_q: 19.593113
 56041/100000: episode: 1086, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.667, mean reward: 0.334 [0.282, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.369, 10.100], loss: 0.015439, mae: 0.129234, mean_q: 18.577827
 56043/100000: episode: 1087, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 1.018, mean reward: 0.509 [0.493, 0.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.307, 10.100], loss: 0.017685, mae: 0.156579, mean_q: 17.636017
 56045/100000: episode: 1088, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 0.892, mean reward: 0.446 [0.436, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.372, 10.100], loss: 0.031638, mae: 0.154292, mean_q: 15.425142
 56047/100000: episode: 1089, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.845, mean reward: 0.422 [0.416, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.282, 10.100], loss: 0.015704, mae: 0.135425, mean_q: 22.157026
 56049/100000: episode: 1090, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 1.010, mean reward: 0.505 [0.489, 0.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.212, 10.100], loss: 0.008685, mae: 0.098108, mean_q: 20.975920
 56051/100000: episode: 1091, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.753, mean reward: 0.376 [0.352, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.217, 10.100], loss: 0.012684, mae: 0.125796, mean_q: 19.281878
 56053/100000: episode: 1092, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.733, mean reward: 0.367 [0.303, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.189, 10.100], loss: 0.007755, mae: 0.097094, mean_q: 18.301508
 56055/100000: episode: 1093, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.889, mean reward: 0.445 [0.420, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.203, 10.100], loss: 0.008910, mae: 0.103189, mean_q: 17.899719
 56057/100000: episode: 1094, duration: 0.020s, episode steps: 2, steps per second: 100, episode reward: 0.803, mean reward: 0.401 [0.380, 0.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.262, 10.100], loss: 0.009246, mae: 0.109165, mean_q: 20.337666
 56059/100000: episode: 1095, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 1.058, mean reward: 0.529 [0.526, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.204, 10.100], loss: 0.010098, mae: 0.108823, mean_q: 18.227417
 56061/100000: episode: 1096, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.408, mean reward: 0.204 [0.137, 0.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.368, 10.100], loss: 0.017102, mae: 0.139213, mean_q: 17.507607
 56063/100000: episode: 1097, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 1.054, mean reward: 0.527 [0.473, 0.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.205, 10.100], loss: 0.019143, mae: 0.141253, mean_q: 21.554516
 56065/100000: episode: 1098, duration: 0.021s, episode steps: 2, steps per second: 93, episode reward: 0.796, mean reward: 0.398 [0.398, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.269, 10.100], loss: 0.012286, mae: 0.117056, mean_q: 18.530512
 56067/100000: episode: 1099, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.720, mean reward: 0.360 [0.359, 0.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.269, 10.100], loss: 0.015014, mae: 0.132153, mean_q: 16.781361
 56069/100000: episode: 1100, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.610, mean reward: 0.305 [0.303, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.585, 10.100], loss: 0.012460, mae: 0.114428, mean_q: 18.270901
 56071/100000: episode: 1101, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 1.049, mean reward: 0.524 [0.517, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.311, 10.100], loss: 0.010320, mae: 0.109223, mean_q: 19.485210
 56073/100000: episode: 1102, duration: 0.021s, episode steps: 2, steps per second: 96, episode reward: 0.772, mean reward: 0.386 [0.385, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.277, 10.100], loss: 0.029490, mae: 0.150021, mean_q: 18.373417
 56075/100000: episode: 1103, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.527, mean reward: 0.263 [0.198, 0.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.332, 10.100], loss: 0.011850, mae: 0.123743, mean_q: 18.019505
 56077/100000: episode: 1104, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.843, mean reward: 0.422 [0.389, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.222, 10.100], loss: 0.009557, mae: 0.106042, mean_q: 19.722469
 56079/100000: episode: 1105, duration: 0.014s, episode steps: 2, steps per second: 141, episode reward: 1.010, mean reward: 0.505 [0.493, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.174, 10.100], loss: 0.017449, mae: 0.141292, mean_q: 20.320358
 56081/100000: episode: 1106, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.944, mean reward: 0.472 [0.444, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.364, 10.100], loss: 0.012826, mae: 0.115543, mean_q: 18.000473
 56083/100000: episode: 1107, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.911, mean reward: 0.455 [0.416, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.217, 10.100], loss: 0.014546, mae: 0.126220, mean_q: 18.200163
 56085/100000: episode: 1108, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: 0.634, mean reward: 0.317 [0.300, 0.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.291, 10.100], loss: 0.021715, mae: 0.139895, mean_q: 18.284386
 56087/100000: episode: 1109, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.939, mean reward: 0.470 [0.455, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.305, 10.100], loss: 0.015931, mae: 0.137304, mean_q: 17.747185
 56089/100000: episode: 1110, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 0.932, mean reward: 0.466 [0.465, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.369, 10.100], loss: 0.009582, mae: 0.113198, mean_q: 20.149326
 56091/100000: episode: 1111, duration: 0.018s, episode steps: 2, steps per second: 109, episode reward: 0.649, mean reward: 0.325 [0.237, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.254, 10.100], loss: 0.011856, mae: 0.120172, mean_q: 19.902201
 56093/100000: episode: 1112, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 0.752, mean reward: 0.376 [0.302, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.392, 10.100], loss: 0.010702, mae: 0.105999, mean_q: 19.612913
 56095/100000: episode: 1113, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 0.615, mean reward: 0.307 [0.257, 0.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.228, 10.100], loss: 0.012731, mae: 0.126763, mean_q: 19.521803
 56097/100000: episode: 1114, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.832, mean reward: 0.416 [0.385, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.307, 10.100], loss: 0.015728, mae: 0.138239, mean_q: 17.011953
 56099/100000: episode: 1115, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.890, mean reward: 0.445 [0.394, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.331, 10.100], loss: 0.016238, mae: 0.152054, mean_q: 18.306219
 56101/100000: episode: 1116, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 0.972, mean reward: 0.486 [0.475, 0.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.180, 10.100], loss: 0.010348, mae: 0.101872, mean_q: 18.959417
 56103/100000: episode: 1117, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 0.752, mean reward: 0.376 [0.373, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.314, 10.100], loss: 0.012760, mae: 0.117879, mean_q: 15.176049
 56105/100000: episode: 1118, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.815, mean reward: 0.407 [0.371, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.319, 10.100], loss: 0.010262, mae: 0.107855, mean_q: 18.550201
 56107/100000: episode: 1119, duration: 0.013s, episode steps: 2, steps per second: 153, episode reward: 0.472, mean reward: 0.236 [0.223, 0.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.334, 10.100], loss: 0.013962, mae: 0.129987, mean_q: 19.073263
 56109/100000: episode: 1120, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.836, mean reward: 0.418 [0.407, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.204, 10.100], loss: 0.013524, mae: 0.113057, mean_q: 18.607124
 56111/100000: episode: 1121, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.918, mean reward: 0.459 [0.447, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.345, 10.100], loss: 0.017520, mae: 0.157606, mean_q: 17.020927
 56113/100000: episode: 1122, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 1.025, mean reward: 0.513 [0.504, 0.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.151, 10.100], loss: 0.016399, mae: 0.125911, mean_q: 19.155590
 56115/100000: episode: 1123, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.821, mean reward: 0.411 [0.384, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.188, 10.100], loss: 0.015103, mae: 0.130504, mean_q: 17.375702
 56117/100000: episode: 1124, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 0.905, mean reward: 0.453 [0.450, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.302, 10.100], loss: 0.013179, mae: 0.124900, mean_q: 19.310951
 56119/100000: episode: 1125, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 0.725, mean reward: 0.363 [0.352, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.267, 10.100], loss: 0.014357, mae: 0.118577, mean_q: 19.722885
 56121/100000: episode: 1126, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 0.866, mean reward: 0.433 [0.423, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.212, 10.100], loss: 0.015646, mae: 0.132020, mean_q: 18.770054
 56123/100000: episode: 1127, duration: 0.020s, episode steps: 2, steps per second: 99, episode reward: 1.041, mean reward: 0.521 [0.510, 0.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.337, 10.100], loss: 0.008127, mae: 0.092602, mean_q: 19.323643
 56125/100000: episode: 1128, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.887, mean reward: 0.443 [0.424, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.427, 10.100], loss: 0.021824, mae: 0.140855, mean_q: 19.278046
 56127/100000: episode: 1129, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 0.940, mean reward: 0.470 [0.458, 0.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.366, 10.100], loss: 0.012247, mae: 0.119511, mean_q: 20.420918
 56129/100000: episode: 1130, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 0.814, mean reward: 0.407 [0.380, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.286, 10.100], loss: 0.009275, mae: 0.107824, mean_q: 18.539879
 56131/100000: episode: 1131, duration: 0.022s, episode steps: 2, steps per second: 92, episode reward: 0.718, mean reward: 0.359 [0.351, 0.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.198, 10.100], loss: 0.013230, mae: 0.112923, mean_q: 19.186768
 56133/100000: episode: 1132, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.785, mean reward: 0.393 [0.380, 0.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.396, 10.100], loss: 0.011941, mae: 0.126240, mean_q: 19.876463
 56135/100000: episode: 1133, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.912, mean reward: 0.456 [0.400, 0.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.299, 10.100], loss: 0.015217, mae: 0.131422, mean_q: 17.740459
[Info] New level: 0.6007648706436157 | Considering 10/90 traces
 56137/100000: episode: 1134, duration: 3.967s, episode steps: 2, steps per second: 1, episode reward: 0.846, mean reward: 0.423 [0.403, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.279, 10.100], loss: 0.011612, mae: 0.121135, mean_q: 21.204990
 56138/100000: episode: 1135, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.417, mean reward: 0.417 [0.417, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.245, 10.100], loss: 0.016303, mae: 0.130521, mean_q: 20.954052
 56139/100000: episode: 1136, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.312, mean reward: 0.312 [0.312, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.189, 10.100], loss: 0.012012, mae: 0.118611, mean_q: 16.610575
 56140/100000: episode: 1137, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.374, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.258, 10.100], loss: 0.020185, mae: 0.129983, mean_q: 16.318026
 56141/100000: episode: 1138, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.222, 10.100], loss: 0.011276, mae: 0.123891, mean_q: 14.555688
 56142/100000: episode: 1139, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.450, mean reward: 0.450 [0.450, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.177, 10.100], loss: 0.012881, mae: 0.124382, mean_q: 20.339760
 56143/100000: episode: 1140, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.375, mean reward: 0.375 [0.375, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.265, 10.100], loss: 0.013910, mae: 0.132688, mean_q: 20.790600
 56144/100000: episode: 1141, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.225, 10.100], loss: 0.010766, mae: 0.113398, mean_q: 18.674232
 56145/100000: episode: 1142, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.323, mean reward: 0.323 [0.323, 0.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.306, 10.100], loss: 0.041012, mae: 0.159486, mean_q: 18.637760
 56146/100000: episode: 1143, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.510, mean reward: 0.510 [0.510, 0.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.248, 10.100], loss: 0.025762, mae: 0.174096, mean_q: 16.324642
 56147/100000: episode: 1144, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.236, 10.100], loss: 0.010304, mae: 0.115926, mean_q: 16.949314
 56148/100000: episode: 1145, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.341, mean reward: 0.341 [0.341, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.167, 10.100], loss: 0.017345, mae: 0.147153, mean_q: 19.135838
 56149/100000: episode: 1146, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.340, mean reward: 0.340 [0.340, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.189, 10.100], loss: 0.018431, mae: 0.136588, mean_q: 19.786453
 56150/100000: episode: 1147, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.230, mean reward: 0.230 [0.230, 0.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.366 [-0.227, 10.100], loss: 0.008230, mae: 0.108419, mean_q: 19.304245
 56151/100000: episode: 1148, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.446, mean reward: 0.446 [0.446, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.276, 10.100], loss: 0.012367, mae: 0.119263, mean_q: 20.480450
 56152/100000: episode: 1149, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.392, mean reward: 0.392 [0.392, 0.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.213, 10.100], loss: 0.011273, mae: 0.116089, mean_q: 16.336388
 56153/100000: episode: 1150, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.384, mean reward: 0.384 [0.384, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.249, 10.100], loss: 0.014405, mae: 0.129135, mean_q: 18.234282
 56154/100000: episode: 1151, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.199, 10.100], loss: 0.045003, mae: 0.169184, mean_q: 15.623215
 56155/100000: episode: 1152, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.359, mean reward: 0.359 [0.359, 0.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.279, 10.100], loss: 0.006511, mae: 0.096917, mean_q: 18.845108
 56156/100000: episode: 1153, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.412, mean reward: 0.412 [0.412, 0.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.263, 10.100], loss: 0.010388, mae: 0.106138, mean_q: 19.196150
 56157/100000: episode: 1154, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.194, 10.100], loss: 0.014057, mae: 0.132805, mean_q: 17.117294
 56158/100000: episode: 1155, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.419, mean reward: 0.419 [0.419, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.216, 10.100], loss: 0.010467, mae: 0.118053, mean_q: 17.629032
 56159/100000: episode: 1156, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.419, mean reward: 0.419 [0.419, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.225, 10.100], loss: 0.008264, mae: 0.097430, mean_q: 20.031185
 56160/100000: episode: 1157, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.307, mean reward: 0.307 [0.307, 0.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.172, 10.100], loss: 0.012538, mae: 0.122022, mean_q: 17.334225
 56161/100000: episode: 1158, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.242, 10.100], loss: 0.016129, mae: 0.138460, mean_q: 18.033775
 56162/100000: episode: 1159, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.448, mean reward: 0.448 [0.448, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.257, 10.100], loss: 0.008532, mae: 0.103563, mean_q: 22.532700
 56163/100000: episode: 1160, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.310, mean reward: 0.310 [0.310, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.260, 10.100], loss: 0.011980, mae: 0.122287, mean_q: 18.638708
 56164/100000: episode: 1161, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.389, mean reward: 0.389 [0.389, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.147, 10.100], loss: 0.010112, mae: 0.122817, mean_q: 20.085831
 56165/100000: episode: 1162, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.324, mean reward: 0.324 [0.324, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.225, 10.100], loss: 0.013969, mae: 0.133629, mean_q: 20.895969
 56166/100000: episode: 1163, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.409, mean reward: 0.409 [0.409, 0.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.235, 10.100], loss: 0.007364, mae: 0.092751, mean_q: 21.045591
 56167/100000: episode: 1164, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.208, 10.100], loss: 0.012806, mae: 0.121101, mean_q: 20.204422
 56168/100000: episode: 1165, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.237, 10.100], loss: 0.012059, mae: 0.125552, mean_q: 21.428528
 56169/100000: episode: 1166, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.246, 10.100], loss: 0.018857, mae: 0.136133, mean_q: 19.983364
 56170/100000: episode: 1167, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.394, mean reward: 0.394 [0.394, 0.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.254, 10.100], loss: 0.016843, mae: 0.144284, mean_q: 22.449417
 56171/100000: episode: 1168, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.437, mean reward: 0.437 [0.437, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.279, 10.100], loss: 0.017578, mae: 0.142762, mean_q: 19.326262
 56172/100000: episode: 1169, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.289, mean reward: 0.289 [0.289, 0.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.206, 10.100], loss: 0.016912, mae: 0.132007, mean_q: 16.844467
 56173/100000: episode: 1170, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.389, mean reward: 0.389 [0.389, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.196, 10.100], loss: 0.010856, mae: 0.110679, mean_q: 19.703716
 56174/100000: episode: 1171, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.418, mean reward: 0.418 [0.418, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.133, 10.100], loss: 0.013963, mae: 0.127686, mean_q: 16.631289
 56175/100000: episode: 1172, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.425, mean reward: 0.425 [0.425, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.212, 10.100], loss: 0.021103, mae: 0.147083, mean_q: 17.794096
 56176/100000: episode: 1173, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.349, mean reward: 0.349 [0.349, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.255, 10.100], loss: 0.013334, mae: 0.117589, mean_q: 15.797212
 56177/100000: episode: 1174, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.354, mean reward: 0.354 [0.354, 0.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.350 [-0.175, 10.100], loss: 0.012136, mae: 0.117944, mean_q: 18.320992
 56178/100000: episode: 1175, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.189, 10.100], loss: 0.010087, mae: 0.114717, mean_q: 18.609934
 56179/100000: episode: 1176, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.471, mean reward: 0.471 [0.471, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.230, 10.100], loss: 0.043520, mae: 0.163825, mean_q: 15.706247
 56180/100000: episode: 1177, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.298, mean reward: 0.298 [0.298, 0.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.156, 10.100], loss: 0.017233, mae: 0.131037, mean_q: 20.275768
 56181/100000: episode: 1178, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.259, 10.100], loss: 0.014913, mae: 0.138185, mean_q: 18.123709
 56182/100000: episode: 1179, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.489, mean reward: 0.489 [0.489, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.231, 10.100], loss: 0.015083, mae: 0.132601, mean_q: 16.570965
 56183/100000: episode: 1180, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.442, mean reward: 0.442 [0.442, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.262, 10.100], loss: 0.018805, mae: 0.133249, mean_q: 17.226122
 56184/100000: episode: 1181, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.407, mean reward: 0.407 [0.407, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.235, 10.100], loss: 0.014086, mae: 0.145059, mean_q: 15.660517
 56185/100000: episode: 1182, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.349, mean reward: 0.349 [0.349, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.146, 10.100], loss: 0.008428, mae: 0.099914, mean_q: 21.614700
 56186/100000: episode: 1183, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.347, mean reward: 0.347 [0.347, 0.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.240, 10.100], loss: 0.011756, mae: 0.111714, mean_q: 19.001482
 56187/100000: episode: 1184, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.366, mean reward: 0.366 [0.366, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.231, 10.100], loss: 0.011579, mae: 0.114252, mean_q: 17.898270
 56188/100000: episode: 1185, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.416, mean reward: 0.416 [0.416, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.236, 10.100], loss: 0.015993, mae: 0.135622, mean_q: 17.934082
 56189/100000: episode: 1186, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.426, mean reward: 0.426 [0.426, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.231, 10.100], loss: 0.018484, mae: 0.143645, mean_q: 19.502565
 56190/100000: episode: 1187, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.498, mean reward: 0.498 [0.498, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.245, 10.100], loss: 0.008797, mae: 0.100279, mean_q: 24.105076
 56191/100000: episode: 1188, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.373, mean reward: 0.373 [0.373, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.183, 10.100], loss: 0.022645, mae: 0.139306, mean_q: 19.054722
 56192/100000: episode: 1189, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.508, mean reward: 0.508 [0.508, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.205, 10.100], loss: 0.009559, mae: 0.104743, mean_q: 20.162619
 56193/100000: episode: 1190, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.351, mean reward: 0.351 [0.351, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.227, 10.100], loss: 0.014065, mae: 0.136849, mean_q: 18.731245
 56194/100000: episode: 1191, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.349, mean reward: 0.349 [0.349, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.426, 10.100], loss: 0.016703, mae: 0.135039, mean_q: 13.686546
 56195/100000: episode: 1192, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.407, mean reward: 0.407 [0.407, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.246, 10.100], loss: 0.015361, mae: 0.148411, mean_q: 16.969322
 56196/100000: episode: 1193, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.452, mean reward: 0.452 [0.452, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.222, 10.100], loss: 0.016249, mae: 0.131945, mean_q: 15.121439
 56197/100000: episode: 1194, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 0.469, mean reward: 0.469 [0.469, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.245, 10.100], loss: 0.012523, mae: 0.134770, mean_q: 18.399094
 56198/100000: episode: 1195, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.351, mean reward: 0.351 [0.351, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.243, 10.100], loss: 0.012454, mae: 0.118208, mean_q: 14.670663
 56199/100000: episode: 1196, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.498, mean reward: 0.498 [0.498, 0.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.231, 10.100], loss: 0.013668, mae: 0.134208, mean_q: 16.662903
 56200/100000: episode: 1197, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.427, mean reward: 0.427 [0.427, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.157, 10.100], loss: 0.015725, mae: 0.144175, mean_q: 18.418518
 56201/100000: episode: 1198, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.407, mean reward: 0.407 [0.407, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.245, 10.100], loss: 0.006723, mae: 0.088971, mean_q: 17.641821
 56202/100000: episode: 1199, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.449, mean reward: 0.449 [0.449, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.220, 10.100], loss: 0.008691, mae: 0.098877, mean_q: 18.199669
 56203/100000: episode: 1200, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.169, mean reward: 0.169 [0.169, 0.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.764, 10.100], loss: 0.011334, mae: 0.120308, mean_q: 17.613485
 56204/100000: episode: 1201, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.440, mean reward: 0.440 [0.440, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.207, 10.100], loss: 0.009914, mae: 0.110328, mean_q: 18.326544
 56205/100000: episode: 1202, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.443, mean reward: 0.443 [0.443, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.187, 10.100], loss: 0.014555, mae: 0.143601, mean_q: 21.218685
 56206/100000: episode: 1203, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.203, 10.100], loss: 0.015338, mae: 0.125992, mean_q: 17.032232
 56207/100000: episode: 1204, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.249, 10.100], loss: 0.012870, mae: 0.130907, mean_q: 16.585886
 56208/100000: episode: 1205, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.445, mean reward: 0.445 [0.445, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.220, 10.100], loss: 0.015115, mae: 0.127066, mean_q: 18.512306
 56209/100000: episode: 1206, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.155, 10.100], loss: 0.011167, mae: 0.110735, mean_q: 19.194553
 56210/100000: episode: 1207, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.216, 10.100], loss: 0.023072, mae: 0.174331, mean_q: 22.033094
 56211/100000: episode: 1208, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 0.476, mean reward: 0.476 [0.476, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.202, 10.100], loss: 0.015491, mae: 0.118145, mean_q: 17.859612
 56212/100000: episode: 1209, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.243, 10.100], loss: 0.009762, mae: 0.112492, mean_q: 21.161186
 56213/100000: episode: 1210, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.380, mean reward: 0.380 [0.380, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.351 [-0.181, 10.100], loss: 0.019650, mae: 0.141776, mean_q: 17.501879
 56214/100000: episode: 1211, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.247, 10.100], loss: 0.008307, mae: 0.091529, mean_q: 18.379570
 56215/100000: episode: 1212, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.463, mean reward: 0.463 [0.463, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.278, 10.100], loss: 0.008543, mae: 0.110544, mean_q: 21.049980
 56216/100000: episode: 1213, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.380, mean reward: 0.380 [0.380, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.230, 10.100], loss: 0.009885, mae: 0.105352, mean_q: 20.110006
 56217/100000: episode: 1214, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.436, mean reward: 0.436 [0.436, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.244, 10.100], loss: 0.007584, mae: 0.091114, mean_q: 16.973232
 56218/100000: episode: 1215, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.417, mean reward: 0.417 [0.417, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.250, 10.100], loss: 0.011188, mae: 0.109920, mean_q: 19.156418
 56219/100000: episode: 1216, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.375, mean reward: 0.375 [0.375, 0.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.246, 10.100], loss: 0.013729, mae: 0.132519, mean_q: 18.265839
 56220/100000: episode: 1217, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.315, 10.100], loss: 0.009317, mae: 0.086565, mean_q: 18.838284
 56221/100000: episode: 1218, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.458, mean reward: 0.458 [0.458, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.201, 10.100], loss: 0.016878, mae: 0.142246, mean_q: 21.034983
 56222/100000: episode: 1219, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.342, mean reward: 0.342 [0.342, 0.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.162, 10.100], loss: 0.017323, mae: 0.140801, mean_q: 17.288231
 56223/100000: episode: 1220, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.421, mean reward: 0.421 [0.421, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.223, 10.100], loss: 0.013569, mae: 0.136518, mean_q: 18.289461
 56224/100000: episode: 1221, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.349, mean reward: 0.349 [0.349, 0.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.313, 10.100], loss: 0.015333, mae: 0.129171, mean_q: 15.979759
 56225/100000: episode: 1222, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.492, mean reward: 0.492 [0.492, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.227, 10.100], loss: 0.010521, mae: 0.111513, mean_q: 16.034149
 56226/100000: episode: 1223, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.430, mean reward: 0.430 [0.430, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.259, 10.100], loss: 0.010752, mae: 0.121788, mean_q: 17.660204
[Info] New level: 0.552155613899231 | Considering 10/90 traces
 56227/100000: episode: 1224, duration: 4.008s, episode steps: 1, steps per second: 0, episode reward: 0.487, mean reward: 0.487 [0.487, 0.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.199, 10.100], loss: 0.015350, mae: 0.124088, mean_q: 20.375935
 56228/100000: episode: 1225, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.228, 10.100], loss: 0.010790, mae: 0.110407, mean_q: 20.198666
 56229/100000: episode: 1226, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.237, 10.100], loss: 0.010478, mae: 0.112282, mean_q: 16.245790
 56230/100000: episode: 1227, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.286, 10.100], loss: 0.010425, mae: 0.117374, mean_q: 17.643642
 56231/100000: episode: 1228, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.231, 10.100], loss: 0.012454, mae: 0.121335, mean_q: 20.726486
 56232/100000: episode: 1229, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 0.343, mean reward: 0.343 [0.343, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.252, 10.100], loss: 0.012380, mae: 0.118065, mean_q: 19.572720
 56233/100000: episode: 1230, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.324, mean reward: 0.324 [0.324, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.285, 10.100], loss: 0.006361, mae: 0.086572, mean_q: 15.841916
 56234/100000: episode: 1231, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.471, mean reward: 0.471 [0.471, 0.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.221, 10.100], loss: 0.015116, mae: 0.133686, mean_q: 16.531242
 56235/100000: episode: 1232, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.313, mean reward: 0.313 [0.313, 0.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.276, 10.100], loss: 0.011456, mae: 0.111192, mean_q: 19.225210
 56236/100000: episode: 1233, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.402, mean reward: 0.402 [0.402, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.301, 10.100], loss: 0.013369, mae: 0.129633, mean_q: 18.785259
 56237/100000: episode: 1234, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.218, 10.100], loss: 0.013897, mae: 0.140663, mean_q: 18.951925
 56238/100000: episode: 1235, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.378, mean reward: 0.378 [0.378, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.259, 10.100], loss: 0.013190, mae: 0.115198, mean_q: 20.155094
 56239/100000: episode: 1236, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.444, mean reward: 0.444 [0.444, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.260, 10.100], loss: 0.012230, mae: 0.134885, mean_q: 21.107853
 56240/100000: episode: 1237, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.261, 10.100], loss: 0.011185, mae: 0.118597, mean_q: 17.624748
 56241/100000: episode: 1238, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.435, mean reward: 0.435 [0.435, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.281, 10.100], loss: 0.008352, mae: 0.101599, mean_q: 17.933708
 56242/100000: episode: 1239, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.340, mean reward: 0.340 [0.340, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.224, 10.100], loss: 0.016447, mae: 0.127636, mean_q: 17.104614
 56243/100000: episode: 1240, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.320, mean reward: 0.320 [0.320, 0.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.236, 10.100], loss: 0.021971, mae: 0.163136, mean_q: 16.422256
 56244/100000: episode: 1241, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.407, mean reward: 0.407 [0.407, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.274, 10.100], loss: 0.014874, mae: 0.141643, mean_q: 18.165176
 56245/100000: episode: 1242, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.441, mean reward: 0.441 [0.441, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.218, 10.100], loss: 0.018472, mae: 0.152838, mean_q: 16.327135
 56246/100000: episode: 1243, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.227, 10.100], loss: 0.004811, mae: 0.074157, mean_q: 21.962513
 56247/100000: episode: 1244, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.444, mean reward: 0.444 [0.444, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.270, 10.100], loss: 0.014870, mae: 0.125692, mean_q: 18.269117
 56248/100000: episode: 1245, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.259, 10.100], loss: 0.017573, mae: 0.158211, mean_q: 18.833731
 56249/100000: episode: 1246, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.453, mean reward: 0.453 [0.453, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.205, 10.100], loss: 0.017447, mae: 0.130639, mean_q: 13.745911
 56250/100000: episode: 1247, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.222, 10.100], loss: 0.012490, mae: 0.129992, mean_q: 18.732498
 56251/100000: episode: 1248, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.242, 10.100], loss: 0.016714, mae: 0.139999, mean_q: 19.713299
 56252/100000: episode: 1249, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.399, mean reward: 0.399 [0.399, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.282, 10.100], loss: 0.011118, mae: 0.123853, mean_q: 18.221619
 56253/100000: episode: 1250, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.456, mean reward: 0.456 [0.456, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.202, 10.100], loss: 0.019662, mae: 0.148651, mean_q: 19.741539
 56254/100000: episode: 1251, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.340, mean reward: 0.340 [0.340, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.285, 10.100], loss: 0.013208, mae: 0.134658, mean_q: 20.010937
 56255/100000: episode: 1252, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.251, 10.100], loss: 0.009763, mae: 0.099920, mean_q: 17.336620
 56256/100000: episode: 1253, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.430, mean reward: 0.430 [0.430, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.788, 10.100], loss: 0.026167, mae: 0.141214, mean_q: 16.921743
 56257/100000: episode: 1254, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.226, 10.100], loss: 0.007800, mae: 0.091425, mean_q: 21.922092
 56258/100000: episode: 1255, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.467, mean reward: 0.467 [0.467, 0.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.291, 10.100], loss: 0.013678, mae: 0.132383, mean_q: 19.645420
 56259/100000: episode: 1256, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.324, mean reward: 0.324 [0.324, 0.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.253, 10.100], loss: 0.022541, mae: 0.145504, mean_q: 19.588009
 56260/100000: episode: 1257, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.249, 10.100], loss: 0.013050, mae: 0.112179, mean_q: 19.740524
 56261/100000: episode: 1258, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.421, mean reward: 0.421 [0.421, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.251, 10.100], loss: 0.014813, mae: 0.128967, mean_q: 19.966085
 56262/100000: episode: 1259, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.274, 10.100], loss: 0.014432, mae: 0.125346, mean_q: 14.260844
 56263/100000: episode: 1260, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.387, mean reward: 0.387 [0.387, 0.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.201, 10.100], loss: 0.046916, mae: 0.177568, mean_q: 15.664862
 56264/100000: episode: 1261, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.380, mean reward: 0.380 [0.380, 0.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.272, 10.100], loss: 0.007932, mae: 0.102069, mean_q: 18.338810
 56265/100000: episode: 1262, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.289, 10.100], loss: 0.013166, mae: 0.122155, mean_q: 17.399483
 56266/100000: episode: 1263, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.427, mean reward: 0.427 [0.427, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.246, 10.100], loss: 0.009052, mae: 0.109170, mean_q: 15.502448
 56267/100000: episode: 1264, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.322, mean reward: 0.322 [0.322, 0.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.246, 10.100], loss: 0.009285, mae: 0.110026, mean_q: 16.689993
 56268/100000: episode: 1265, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.450, mean reward: 0.450 [0.450, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.280, 10.100], loss: 0.008921, mae: 0.089729, mean_q: 20.073566
 56269/100000: episode: 1266, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.473, mean reward: 0.473 [0.473, 0.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.265, 10.100], loss: 0.015450, mae: 0.146448, mean_q: 19.725565
 56270/100000: episode: 1267, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.398, mean reward: 0.398 [0.398, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.012, 10.100], loss: 0.014255, mae: 0.132116, mean_q: 23.304901
 56271/100000: episode: 1268, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.265, mean reward: 0.265 [0.265, 0.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.373, 10.100], loss: 0.007767, mae: 0.105867, mean_q: 20.854166
 56272/100000: episode: 1269, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.343, mean reward: 0.343 [0.343, 0.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.253, 10.100], loss: 0.020842, mae: 0.175862, mean_q: 19.198067
 56273/100000: episode: 1270, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.265, 10.100], loss: 0.014051, mae: 0.134700, mean_q: 19.830629
 56274/100000: episode: 1271, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.476, mean reward: 0.476 [0.476, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.209, 10.100], loss: 0.009048, mae: 0.100634, mean_q: 14.119560
 56275/100000: episode: 1272, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.312, mean reward: 0.312 [0.312, 0.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.248, 10.100], loss: 0.006004, mae: 0.087992, mean_q: 19.066925
 56276/100000: episode: 1273, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.341, mean reward: 0.341 [0.341, 0.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.287, 10.100], loss: 0.010549, mae: 0.109859, mean_q: 18.058529
 56277/100000: episode: 1274, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.383, mean reward: 0.383 [0.383, 0.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.209, 10.100], loss: 0.012720, mae: 0.128332, mean_q: 21.272469
 56278/100000: episode: 1275, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.443, mean reward: 0.443 [0.443, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.680, 10.100], loss: 0.020316, mae: 0.159937, mean_q: 20.738869
 56279/100000: episode: 1276, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.268, mean reward: 0.268 [0.268, 0.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.299, 10.100], loss: 0.012482, mae: 0.124044, mean_q: 19.651852
 56280/100000: episode: 1277, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 0.432, mean reward: 0.432 [0.432, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.227, 10.100], loss: 0.012172, mae: 0.113680, mean_q: 18.892071
 56281/100000: episode: 1278, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.474, mean reward: 0.474 [0.474, 0.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.232, 10.100], loss: 0.009851, mae: 0.102775, mean_q: 20.045410
 56282/100000: episode: 1279, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.253, mean reward: 0.253 [0.253, 0.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.265, 10.100], loss: 0.005843, mae: 0.086950, mean_q: 19.285736
 56283/100000: episode: 1280, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.507, mean reward: 0.507 [0.507, 0.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.246, 10.100], loss: 0.007782, mae: 0.100451, mean_q: 19.054077
 56284/100000: episode: 1281, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.426, mean reward: 0.426 [0.426, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.255, 10.100], loss: 0.004486, mae: 0.079674, mean_q: 20.680172
 56285/100000: episode: 1282, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.371, mean reward: 0.371 [0.371, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.237, 10.100], loss: 0.012642, mae: 0.128761, mean_q: 17.588533
 56286/100000: episode: 1283, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.262, mean reward: 0.262 [0.262, 0.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.360, 10.100], loss: 0.011449, mae: 0.122636, mean_q: 16.805664
 56287/100000: episode: 1284, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.279, mean reward: 0.279 [0.279, 0.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.275, 10.100], loss: 0.011261, mae: 0.121763, mean_q: 16.038960
 56288/100000: episode: 1285, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.480, mean reward: 0.480 [0.480, 0.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.238, 10.100], loss: 0.011914, mae: 0.118701, mean_q: 19.982985
 56289/100000: episode: 1286, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.462, mean reward: 0.462 [0.462, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.193, 10.100], loss: 0.012064, mae: 0.122616, mean_q: 18.952185
 56290/100000: episode: 1287, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.378, mean reward: 0.378 [0.378, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.266, 10.100], loss: 0.015225, mae: 0.125663, mean_q: 22.353920
 56291/100000: episode: 1288, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.398, mean reward: 0.398 [0.398, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.266, 10.100], loss: 0.010176, mae: 0.108747, mean_q: 18.728313
 56292/100000: episode: 1289, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.416, mean reward: 0.416 [0.416, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.260, 10.100], loss: 0.006564, mae: 0.096761, mean_q: 17.997135
 56293/100000: episode: 1290, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.362, mean reward: 0.362 [0.362, 0.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.298, 10.100], loss: 0.010947, mae: 0.124499, mean_q: 17.977352
 56294/100000: episode: 1291, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.244, mean reward: 0.244 [0.244, 0.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.415 [-0.249, 10.100], loss: 0.008505, mae: 0.107698, mean_q: 17.505285
 56295/100000: episode: 1292, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.440, mean reward: 0.440 [0.440, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.271, 10.100], loss: 0.006170, mae: 0.081706, mean_q: 18.402601
 56296/100000: episode: 1293, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.259, 10.100], loss: 0.015931, mae: 0.143277, mean_q: 17.801895
 56297/100000: episode: 1294, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.355, mean reward: 0.355 [0.355, 0.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.266, 10.100], loss: 0.026348, mae: 0.174808, mean_q: 15.130375
 56298/100000: episode: 1295, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.377, mean reward: 0.377 [0.377, 0.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.261, 10.100], loss: 0.005657, mae: 0.083263, mean_q: 18.017281
 56299/100000: episode: 1296, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.425, mean reward: 0.425 [0.425, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.382, 10.100], loss: 0.009974, mae: 0.106899, mean_q: 20.466885
 56300/100000: episode: 1297, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.444, mean reward: 0.444 [0.444, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.265, 10.100], loss: 0.006337, mae: 0.089068, mean_q: 18.893978
 56301/100000: episode: 1298, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.263, 10.100], loss: 0.013018, mae: 0.126446, mean_q: 18.552643
 56302/100000: episode: 1299, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.462, mean reward: 0.462 [0.462, 0.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.282, 10.100], loss: 0.006906, mae: 0.094226, mean_q: 20.743483
 56303/100000: episode: 1300, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 0.448, mean reward: 0.448 [0.448, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.251, 10.100], loss: 0.007051, mae: 0.090563, mean_q: 15.799175
 56304/100000: episode: 1301, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.438, mean reward: 0.438 [0.438, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.241, 10.100], loss: 0.011062, mae: 0.117319, mean_q: 19.357880
 56305/100000: episode: 1302, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.223, 10.100], loss: 0.013927, mae: 0.134147, mean_q: 17.612823
 56306/100000: episode: 1303, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.275, 10.100], loss: 0.006758, mae: 0.093435, mean_q: 16.231888
 56307/100000: episode: 1304, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.461, mean reward: 0.461 [0.461, 0.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.276, 10.100], loss: 0.012023, mae: 0.114567, mean_q: 17.181622
 56308/100000: episode: 1305, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.475, mean reward: 0.475 [0.475, 0.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.682, 10.100], loss: 0.008821, mae: 0.110633, mean_q: 19.787409
 56309/100000: episode: 1306, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.417, mean reward: 0.417 [0.417, 0.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.205, 10.100], loss: 0.008832, mae: 0.107094, mean_q: 18.363708
 56310/100000: episode: 1307, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.446, mean reward: 0.446 [0.446, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.219, 10.100], loss: 0.010743, mae: 0.116110, mean_q: 19.422340
 56311/100000: episode: 1308, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.403, mean reward: 0.403 [0.403, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.251, 10.100], loss: 0.006512, mae: 0.096379, mean_q: 18.557562
 56312/100000: episode: 1309, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.283, mean reward: 0.283 [0.283, 0.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.249, 10.100], loss: 0.009973, mae: 0.111978, mean_q: 21.130852
 56313/100000: episode: 1310, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.460, mean reward: 0.460 [0.460, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.201, 10.100], loss: 0.015956, mae: 0.137576, mean_q: 21.068081
 56314/100000: episode: 1311, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.353, mean reward: 0.353 [0.353, 0.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.275, 10.100], loss: 0.013438, mae: 0.129148, mean_q: 15.767845
 56315/100000: episode: 1312, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.430, mean reward: 0.430 [0.430, 0.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.259, 10.100], loss: 0.012860, mae: 0.113699, mean_q: 20.751228
 56316/100000: episode: 1313, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.366, mean reward: 0.366 [0.366, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.234, 10.100], loss: 0.013563, mae: 0.127538, mean_q: 16.414143
[Info] New level: 0.4309503734111786 | Considering 10/90 traces
 56317/100000: episode: 1314, duration: 4.015s, episode steps: 1, steps per second: 0, episode reward: 0.516, mean reward: 0.516 [0.516, 0.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.242, 10.100], loss: 0.020347, mae: 0.149737, mean_q: 18.716089
 56318/100000: episode: 1315, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.219, 10.100], loss: 0.007797, mae: 0.104643, mean_q: 17.257032
 56319/100000: episode: 1316, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.459, mean reward: 0.459 [0.459, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.228, 10.100], loss: 0.010505, mae: 0.101971, mean_q: 17.561806
 56320/100000: episode: 1317, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.332, mean reward: 0.332 [0.332, 0.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.208, 10.100], loss: 0.007742, mae: 0.105579, mean_q: 20.430998
 56321/100000: episode: 1318, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.397, mean reward: 0.397 [0.397, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.240, 10.100], loss: 0.013130, mae: 0.137270, mean_q: 18.565485
 56322/100000: episode: 1319, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.346, mean reward: 0.346 [0.346, 0.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.223, 10.100], loss: 0.010172, mae: 0.104199, mean_q: 17.510365
 56323/100000: episode: 1320, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.376, mean reward: 0.376 [0.376, 0.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.801, 10.100], loss: 0.010777, mae: 0.115778, mean_q: 14.771049
 56324/100000: episode: 1321, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.248, 10.100], loss: 0.011979, mae: 0.135681, mean_q: 18.902519
 56325/100000: episode: 1322, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.496, mean reward: 0.496 [0.496, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.221, 10.100], loss: 0.013142, mae: 0.128039, mean_q: 17.415752
 56326/100000: episode: 1323, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.243, 10.100], loss: 0.007809, mae: 0.105297, mean_q: 17.942070
 56327/100000: episode: 1324, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.445, mean reward: 0.445 [0.445, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.270, 10.100], loss: 0.015845, mae: 0.146995, mean_q: 16.278826
 56328/100000: episode: 1325, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.205, 10.100], loss: 0.010355, mae: 0.101338, mean_q: 17.608742
 56329/100000: episode: 1326, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.209, 10.100], loss: 0.017455, mae: 0.136419, mean_q: 19.712566
 56330/100000: episode: 1327, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 0.440, mean reward: 0.440 [0.440, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.241, 10.100], loss: 0.014081, mae: 0.142612, mean_q: 17.778927
 56331/100000: episode: 1328, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.488, mean reward: 0.488 [0.488, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.241, 10.100], loss: 0.009860, mae: 0.109015, mean_q: 18.264256
 56332/100000: episode: 1329, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.465, mean reward: 0.465 [0.465, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.200, 10.100], loss: 0.007087, mae: 0.098484, mean_q: 19.146679
 56333/100000: episode: 1330, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.200, 10.100], loss: 0.007327, mae: 0.100967, mean_q: 20.133713
 56334/100000: episode: 1331, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.452, mean reward: 0.452 [0.452, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.201, 10.100], loss: 0.008610, mae: 0.107292, mean_q: 17.217068
 56335/100000: episode: 1332, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.459, mean reward: 0.459 [0.459, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.193, 10.100], loss: 0.006786, mae: 0.097116, mean_q: 21.913183
 56336/100000: episode: 1333, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.457, mean reward: 0.457 [0.457, 0.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.273, 10.100], loss: 0.014268, mae: 0.134958, mean_q: 21.501617
 56337/100000: episode: 1334, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.193, 10.100], loss: 0.009091, mae: 0.098514, mean_q: 18.757637
 56338/100000: episode: 1335, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.438, mean reward: 0.438 [0.438, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.296, 10.100], loss: 0.017802, mae: 0.127341, mean_q: 13.527390
 56339/100000: episode: 1336, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.432, mean reward: 0.432 [0.432, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.248, 10.100], loss: 0.047994, mae: 0.200806, mean_q: 17.087547
 56340/100000: episode: 1337, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.309, mean reward: 0.309 [0.309, 0.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.232, 10.100], loss: 0.017413, mae: 0.147200, mean_q: 17.897606
 56341/100000: episode: 1338, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.263, 10.100], loss: 0.008194, mae: 0.105392, mean_q: 18.079510
 56342/100000: episode: 1339, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.413, mean reward: 0.413 [0.413, 0.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.277, 10.100], loss: 0.015521, mae: 0.136274, mean_q: 15.537268
 56343/100000: episode: 1340, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.408, mean reward: 0.408 [0.408, 0.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.267, 10.100], loss: 0.003780, mae: 0.073342, mean_q: 20.508167
 56344/100000: episode: 1341, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.366, mean reward: 0.366 [0.366, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.219, 10.100], loss: 0.010435, mae: 0.112783, mean_q: 18.379284
 56345/100000: episode: 1342, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.398, mean reward: 0.398 [0.398, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.207, 10.100], loss: 0.012828, mae: 0.123019, mean_q: 16.362045
 56346/100000: episode: 1343, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.402, mean reward: 0.402 [0.402, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.292, 10.100], loss: 0.016275, mae: 0.124468, mean_q: 18.913464
 56347/100000: episode: 1344, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.459, mean reward: 0.459 [0.459, 0.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.242, 10.100], loss: 0.016796, mae: 0.147067, mean_q: 15.083070
 56348/100000: episode: 1345, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.318, mean reward: 0.318 [0.318, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.285, 10.100], loss: 0.006850, mae: 0.098423, mean_q: 17.905447
 56349/100000: episode: 1346, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.438, mean reward: 0.438 [0.438, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.252, 10.100], loss: 0.008566, mae: 0.095713, mean_q: 14.233662
 56350/100000: episode: 1347, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 0.282, mean reward: 0.282 [0.282, 0.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.231, 10.100], loss: 0.014778, mae: 0.128028, mean_q: 21.128448
 56351/100000: episode: 1348, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 0.492, mean reward: 0.492 [0.492, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.253, 10.100], loss: 0.010343, mae: 0.109190, mean_q: 19.073662
 56352/100000: episode: 1349, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.426, mean reward: 0.426 [0.426, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.238, 10.100], loss: 0.013821, mae: 0.134729, mean_q: 14.558639
 56353/100000: episode: 1350, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.445, mean reward: 0.445 [0.445, 0.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.205, 10.100], loss: 0.012455, mae: 0.123145, mean_q: 17.799564
 56354/100000: episode: 1351, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.465, mean reward: 0.465 [0.465, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.217, 10.100], loss: 0.007603, mae: 0.094982, mean_q: 21.163263
 56355/100000: episode: 1352, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.443, mean reward: 0.443 [0.443, 0.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.230, 10.100], loss: 0.017595, mae: 0.145526, mean_q: 19.220112
 56356/100000: episode: 1353, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.454, mean reward: 0.454 [0.454, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.238, 10.100], loss: 0.006662, mae: 0.090644, mean_q: 17.440903
 56357/100000: episode: 1354, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.442, mean reward: 0.442 [0.442, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.219, 10.100], loss: 0.006333, mae: 0.090758, mean_q: 15.609461
 56358/100000: episode: 1355, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.454, mean reward: 0.454 [0.454, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.234, 10.100], loss: 0.011235, mae: 0.120949, mean_q: 17.463465
 56359/100000: episode: 1356, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.456, mean reward: 0.456 [0.456, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.261, 10.100], loss: 0.008100, mae: 0.102444, mean_q: 14.705404
 56360/100000: episode: 1357, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.163, 10.100], loss: 0.012499, mae: 0.122314, mean_q: 20.818937
 56361/100000: episode: 1358, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.386, mean reward: 0.386 [0.386, 0.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.228, 10.100], loss: 0.008208, mae: 0.091268, mean_q: 17.876644
 56362/100000: episode: 1359, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.262, 10.100], loss: 0.013356, mae: 0.121694, mean_q: 16.540970
 56363/100000: episode: 1360, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.451, mean reward: 0.451 [0.451, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.254, 10.100], loss: 0.008742, mae: 0.096583, mean_q: 18.140820
 56364/100000: episode: 1361, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.351, mean reward: 0.351 [0.351, 0.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.221, 10.100], loss: 0.010110, mae: 0.100499, mean_q: 17.396273
 56365/100000: episode: 1362, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.373, mean reward: 0.373 [0.373, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.244, 10.100], loss: 0.015730, mae: 0.138934, mean_q: 16.625114
 56366/100000: episode: 1363, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.407, mean reward: 0.407 [0.407, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.285, 10.100], loss: 0.024450, mae: 0.141731, mean_q: 17.028915
 56367/100000: episode: 1364, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 0.340, mean reward: 0.340 [0.340, 0.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.239, 10.100], loss: 0.012563, mae: 0.122928, mean_q: 19.557777
 56368/100000: episode: 1365, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.481, mean reward: 0.481 [0.481, 0.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.796, 10.100], loss: 0.009081, mae: 0.103239, mean_q: 17.067509
 56369/100000: episode: 1366, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.425, mean reward: 0.425 [0.425, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.227, 10.100], loss: 0.011560, mae: 0.128335, mean_q: 15.971949
 56370/100000: episode: 1367, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.228, 10.100], loss: 0.018946, mae: 0.155246, mean_q: 20.908802
 56371/100000: episode: 1368, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.216, 10.100], loss: 0.009268, mae: 0.113486, mean_q: 15.537426
 56372/100000: episode: 1369, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.460, mean reward: 0.460 [0.460, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.344, 10.100], loss: 0.008901, mae: 0.104316, mean_q: 15.413206
 56373/100000: episode: 1370, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.454, mean reward: 0.454 [0.454, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.250, 10.100], loss: 0.011020, mae: 0.100815, mean_q: 20.638592
 56374/100000: episode: 1371, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.407, mean reward: 0.407 [0.407, 0.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.205, 10.100], loss: 0.006743, mae: 0.084496, mean_q: 18.444382
 56375/100000: episode: 1372, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.476, mean reward: 0.476 [0.476, 0.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.231, 10.100], loss: 0.008354, mae: 0.104188, mean_q: 19.402714
 56376/100000: episode: 1373, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.310, mean reward: 0.310 [0.310, 0.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.735, 10.100], loss: 0.018367, mae: 0.124741, mean_q: 14.630334
 56377/100000: episode: 1374, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.495, mean reward: 0.495 [0.495, 0.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.194, 10.100], loss: 0.008062, mae: 0.096430, mean_q: 18.379124
 56378/100000: episode: 1375, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.449, mean reward: 0.449 [0.449, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.175, 10.100], loss: 0.016867, mae: 0.135997, mean_q: 19.098469
 56379/100000: episode: 1376, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.415, mean reward: 0.415 [0.415, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.222, 10.100], loss: 0.013484, mae: 0.131230, mean_q: 18.679377
 56380/100000: episode: 1377, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.444, mean reward: 0.444 [0.444, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.259, 10.100], loss: 0.006062, mae: 0.092389, mean_q: 20.834209
 56381/100000: episode: 1378, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.328, mean reward: 0.328 [0.328, 0.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.217, 10.100], loss: 0.012182, mae: 0.122020, mean_q: 15.063908
 56382/100000: episode: 1379, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.460, mean reward: 0.460 [0.460, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.235, 10.100], loss: 0.013710, mae: 0.136623, mean_q: 19.357052
 56383/100000: episode: 1380, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.450, mean reward: 0.450 [0.450, 0.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.256, 10.100], loss: 0.009950, mae: 0.112578, mean_q: 18.930199
 56384/100000: episode: 1381, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.415, mean reward: 0.415 [0.415, 0.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.616, 10.100], loss: 0.010232, mae: 0.122570, mean_q: 18.732412
 56385/100000: episode: 1382, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.448, mean reward: 0.448 [0.448, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.246, 10.100], loss: 0.022563, mae: 0.170337, mean_q: 16.357014
 56386/100000: episode: 1383, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.398, mean reward: 0.398 [0.398, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.195, 10.100], loss: 0.010615, mae: 0.111418, mean_q: 18.132378
 56387/100000: episode: 1384, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.187, 10.100], loss: 0.012747, mae: 0.121782, mean_q: 18.534485
 56388/100000: episode: 1385, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.382, mean reward: 0.382 [0.382, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.234, 10.100], loss: 0.014983, mae: 0.134621, mean_q: 18.676491
 56389/100000: episode: 1386, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.437, mean reward: 0.437 [0.437, 0.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.236, 10.100], loss: 0.018658, mae: 0.154858, mean_q: 18.150770
 56390/100000: episode: 1387, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.429, mean reward: 0.429 [0.429, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.227, 10.100], loss: 0.008002, mae: 0.099523, mean_q: 19.193117
 56391/100000: episode: 1388, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.204, 10.100], loss: 0.007851, mae: 0.099628, mean_q: 18.992563
 56392/100000: episode: 1389, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.423, mean reward: 0.423 [0.423, 0.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.245, 10.100], loss: 0.026341, mae: 0.156194, mean_q: 18.298441
 56393/100000: episode: 1390, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.242, 10.100], loss: 0.007727, mae: 0.105586, mean_q: 12.450794
 56394/100000: episode: 1391, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.458, mean reward: 0.458 [0.458, 0.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.290, 10.100], loss: 0.009013, mae: 0.099901, mean_q: 15.740356
 56395/100000: episode: 1392, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.330, mean reward: 0.330 [0.330, 0.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.255, 10.100], loss: 0.009851, mae: 0.105755, mean_q: 19.145866
 56396/100000: episode: 1393, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.451, mean reward: 0.451 [0.451, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.232, 10.100], loss: 0.024199, mae: 0.160793, mean_q: 16.178469
 56397/100000: episode: 1394, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.325, mean reward: 0.325 [0.325, 0.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.288, 10.100], loss: 0.012619, mae: 0.121591, mean_q: 14.136139
 56398/100000: episode: 1395, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 0.484, mean reward: 0.484 [0.484, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.212, 10.100], loss: 0.011429, mae: 0.123940, mean_q: 18.809742
 56399/100000: episode: 1396, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.447, mean reward: 0.447 [0.447, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.241, 10.100], loss: 0.017831, mae: 0.147949, mean_q: 15.895004
 56400/100000: episode: 1397, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.456, mean reward: 0.456 [0.456, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.204, 10.100], loss: 0.015427, mae: 0.132883, mean_q: 21.273296
 56401/100000: episode: 1398, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 0.398, mean reward: 0.398 [0.398, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.191, 10.100], loss: 0.021459, mae: 0.153913, mean_q: 19.470974
 56402/100000: episode: 1399, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.490, mean reward: 0.490 [0.490, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.243, 10.100], loss: 0.011806, mae: 0.113997, mean_q: 17.295212
 56403/100000: episode: 1400, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.452, mean reward: 0.452 [0.452, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.236, 10.100], loss: 0.010008, mae: 0.120275, mean_q: 17.950171
 56404/100000: episode: 1401, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.402, mean reward: 0.402 [0.402, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.253, 10.100], loss: 0.010830, mae: 0.126133, mean_q: 20.138966
 56405/100000: episode: 1402, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.396, mean reward: 0.396 [0.396, 0.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.249, 10.100], loss: 0.012953, mae: 0.140687, mean_q: 18.165649
 56406/100000: episode: 1403, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.416, mean reward: 0.416 [0.416, 0.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.202, 10.100], loss: 0.009122, mae: 0.112621, mean_q: 20.987488
[Info] New level: 0.41404181718826294 | Considering 10/90 traces
 56407/100000: episode: 1404, duration: 3.962s, episode steps: 1, steps per second: 0, episode reward: 0.486, mean reward: 0.486 [0.486, 0.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.233, 10.100], loss: 0.010136, mae: 0.108348, mean_q: 16.948231
 56408/100000: episode: 1405, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 0.390, mean reward: 0.390 [0.390, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.219, 10.100], loss: 0.016028, mae: 0.143608, mean_q: 16.728207
 56409/100000: episode: 1406, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.428, mean reward: 0.428 [0.428, 0.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.277, 10.100], loss: 0.015891, mae: 0.132557, mean_q: 16.198406
 56410/100000: episode: 1407, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.304, 10.100], loss: 0.008969, mae: 0.104177, mean_q: 15.735312
 56411/100000: episode: 1408, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.374, mean reward: 0.374 [0.374, 0.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.232, 10.100], loss: 0.008319, mae: 0.099280, mean_q: 17.945908
 56412/100000: episode: 1409, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.242, 10.100], loss: 0.008305, mae: 0.088712, mean_q: 16.774017
 56413/100000: episode: 1410, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.451, mean reward: 0.451 [0.451, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.285, 10.100], loss: 0.016850, mae: 0.146206, mean_q: 18.884270
 56414/100000: episode: 1411, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.447, mean reward: 0.447 [0.447, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.268, 10.100], loss: 0.006450, mae: 0.085849, mean_q: 17.589407
 56415/100000: episode: 1412, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.288, 10.100], loss: 0.013019, mae: 0.115022, mean_q: 16.711905
 56416/100000: episode: 1413, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.494, mean reward: 0.494 [0.494, 0.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.208, 10.100], loss: 0.009846, mae: 0.101884, mean_q: 15.599570
 56417/100000: episode: 1414, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.438, mean reward: 0.438 [0.438, 0.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.266, 10.100], loss: 0.004986, mae: 0.080660, mean_q: 15.110995
 56418/100000: episode: 1415, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.446, mean reward: 0.446 [0.446, 0.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.236, 10.100], loss: 0.015771, mae: 0.122533, mean_q: 15.165588
 56419/100000: episode: 1416, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.404, mean reward: 0.404 [0.404, 0.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.261, 10.100], loss: 0.009297, mae: 0.112300, mean_q: 18.280590
 56420/100000: episode: 1417, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.449, mean reward: 0.449 [0.449, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.237, 10.100], loss: 0.004957, mae: 0.079288, mean_q: 17.950251
 56421/100000: episode: 1418, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 0.393, mean reward: 0.393 [0.393, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.276, 10.100], loss: 0.006896, mae: 0.088420, mean_q: 15.410105
 56422/100000: episode: 1419, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.248, 10.100], loss: 0.009150, mae: 0.101681, mean_q: 14.296478
 56423/100000: episode: 1420, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.389, mean reward: 0.389 [0.389, 0.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.225, 10.100], loss: 0.006180, mae: 0.082850, mean_q: 17.686058
 56424/100000: episode: 1421, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 0.440, mean reward: 0.440 [0.440, 0.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.222, 10.100], loss: 0.012381, mae: 0.123203, mean_q: 18.259016
 56425/100000: episode: 1422, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.260, 10.100], loss: 0.007474, mae: 0.097591, mean_q: 16.630764
 56426/100000: episode: 1423, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.402, mean reward: 0.402 [0.402, 0.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.257, 10.100], loss: 0.005295, mae: 0.077746, mean_q: 17.580875
 56427/100000: episode: 1424, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.193, 10.100], loss: 0.012913, mae: 0.118679, mean_q: 17.003214
 56428/100000: episode: 1425, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 0.357, mean reward: 0.357 [0.357, 0.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.269, 10.100], loss: 0.008536, mae: 0.109600, mean_q: 16.375174
 56429/100000: episode: 1426, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 0.397, mean reward: 0.397 [0.397, 0.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.235, 10.100], loss: 0.007687, mae: 0.102466, mean_q: 17.890152
 56430/100000: episode: 1427, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 0.456, mean reward: 0.456 [0.456, 0.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.246, 10.100], loss: 0.011342, mae: 0.108228, mean_q: 19.103218
 56431/100000: episode: 1428, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.356, mean reward: 0.356 [0.356, 0.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.292, 10.100], loss: 0.010363, mae: 0.111515, mean_q: 20.438164
 56432/100000: episode: 1429, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.454, mean reward: 0.454 [0.454, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.208, 10.100], loss: 0.009159, mae: 0.107927, mean_q: 12.855371
 56433/100000: episode: 1430, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.384, mean reward: 0.384 [0.384, 0.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.221, 10.100], loss: 0.014642, mae: 0.141204, mean_q: 19.065769
 56434/100000: episode: 1431, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.399, mean reward: 0.399 [0.399, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.217, 10.100], loss: 0.010892, mae: 0.111827, mean_q: 18.169867
 56435/100000: episode: 1432, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.449, mean reward: 0.449 [0.449, 0.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.276, 10.100], loss: 0.025101, mae: 0.181759, mean_q: 23.944324
 56436/100000: episode: 1433, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.251, mean reward: 0.251 [0.251, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.221, 10.100], loss: 0.013141, mae: 0.123198, mean_q: 22.695011
 56437/100000: episode: 1434, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.379, mean reward: 0.379 [0.379, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.262, 10.100], loss: 0.010812, mae: 0.106898, mean_q: 19.315090
 56438/100000: episode: 1435, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 0.372, mean reward: 0.372 [0.372, 0.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.255, 10.100], loss: 0.007800, mae: 0.087863, mean_q: 19.636868
 56439/100000: episode: 1436, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.218, 10.100], loss: 0.009733, mae: 0.104249, mean_q: 19.056955
 56440/100000: episode: 1437, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.212, 10.100], loss: 0.006282, mae: 0.090815, mean_q: 21.860550
 56441/100000: episode: 1438, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.339, mean reward: 0.339 [0.339, 0.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.213, 10.100], loss: 0.005220, mae: 0.084673, mean_q: 15.783671
 56442/100000: episode: 1439, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.419, mean reward: 0.419 [0.419, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.264, 10.100], loss: 0.009784, mae: 0.109895, mean_q: 19.449373
 56443/100000: episode: 1440, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 0.318, mean reward: 0.318 [0.318, 0.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.233, 10.100], loss: 0.010786, mae: 0.103480, mean_q: 20.140621
 56444/100000: episode: 1441, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 0.398, mean reward: 0.398 [0.398, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.274, 10.100], loss: 0.016052, mae: 0.134449, mean_q: 21.068548
 56445/100000: episode: 1442, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 0.478, mean reward: 0.478 [0.478, 0.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.274, 10.100], loss: 0.012855, mae: 0.121445, mean_q: 18.748163
 56446/100000: episode: 1443, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 0.431, mean reward: 0.431 [0.431, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.298, 10.100], loss: 0.015350, mae: 0.141513, mean_q: 16.907879
 56447/100000: episode: 1444, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.365, mean reward: 0.365 [0.365, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.234, 10.100], loss: 0.010447, mae: 0.111751, mean_q: 15.063262
 56448/100000: episode: 1445, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 0.435, mean reward: 0.435 [0.435, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.246, 10.100], loss: 0.005986, mae: 0.081959, mean_q: 15.721369
 56449/100000: episode: 1446, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 0.451, mean reward: 0.451 [0.451, 0.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.232, 10.100], loss: 0.007716, mae: 0.094396, mean_q: 16.493118
 56450/100000: episode: 1447, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.489, mean reward: 0.489 [0.489, 0.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.255, 10.100], loss: 0.009777, mae: 0.097122, mean_q: 19.197645
 56451/100000: episode: 1448, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.419, mean reward: 0.419 [0.419, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.184, 10.100], loss: 0.013541, mae: 0.139327, mean_q: 17.468929
 56452/100000: episode: 1449, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.395, mean reward: 0.395 [0.395, 0.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.273, 10.100], loss: 0.011387, mae: 0.115252, mean_q: 18.332550
 56453/100000: episode: 1450, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.460, mean reward: 0.460 [0.460, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.198, 10.100], loss: 0.012334, mae: 0.119189, mean_q: 15.314856
 56454/100000: episode: 1451, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 0.469, mean reward: 0.469 [0.469, 0.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.191, 10.100], loss: 0.014533, mae: 0.137486, mean_q: 13.208974
 56455/100000: episode: 1452, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.419, mean reward: 0.419 [0.419, 0.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.230, 10.100], loss: 0.010725, mae: 0.117714, mean_q: 18.369398
 56456/100000: episode: 1453, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 0.400, mean reward: 0.400 [0.400, 0.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.248, 10.100], loss: 0.018816, mae: 0.131182, mean_q: 17.351490
 56457/100000: episode: 1454, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.442, mean reward: 0.442 [0.442, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.219, 10.100], loss: 0.015918, mae: 0.133469, mean_q: 18.074177
 56458/100000: episode: 1455, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.421, mean reward: 0.421 [0.421, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.217, 10.100], loss: 0.012034, mae: 0.130960, mean_q: 17.523300
 56459/100000: episode: 1456, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.444, mean reward: 0.444 [0.444, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.212, 10.100], loss: 0.011389, mae: 0.100594, mean_q: 17.579521
 56460/100000: episode: 1457, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.431, mean reward: 0.431 [0.431, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.238, 10.100], loss: 0.017499, mae: 0.146305, mean_q: 18.655567
 56461/100000: episode: 1458, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.426, mean reward: 0.426 [0.426, 0.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.209, 10.100], loss: 0.018564, mae: 0.146511, mean_q: 16.677645
 56462/100000: episode: 1459, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.381, mean reward: 0.381 [0.381, 0.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.258, 10.100], loss: 0.010247, mae: 0.111740, mean_q: 18.075211
 56463/100000: episode: 1460, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 0.366, mean reward: 0.366 [0.366, 0.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.219, 10.100], loss: 0.009384, mae: 0.109911, mean_q: 19.072376
 56464/100000: episode: 1461, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.453, mean reward: 0.453 [0.453, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.231, 10.100], loss: 0.007843, mae: 0.093593, mean_q: 17.292416
 56465/100000: episode: 1462, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.378, mean reward: 0.378 [0.378, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.233, 10.100], loss: 0.006182, mae: 0.087425, mean_q: 16.167749
 56466/100000: episode: 1463, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.479, mean reward: 0.479 [0.479, 0.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.220, 10.100], loss: 0.011148, mae: 0.111362, mean_q: 17.293766
 56467/100000: episode: 1464, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 0.382, mean reward: 0.382 [0.382, 0.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.202, 10.100], loss: 0.012023, mae: 0.122836, mean_q: 16.818390
 56468/100000: episode: 1465, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.477, mean reward: 0.477 [0.477, 0.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.263, 10.100], loss: 0.013669, mae: 0.138574, mean_q: 14.762667
 56469/100000: episode: 1466, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.352, mean reward: 0.352 [0.352, 0.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.223, 10.100], loss: 0.007216, mae: 0.091666, mean_q: 14.193880
 56470/100000: episode: 1467, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.269, 10.100], loss: 0.007676, mae: 0.113154, mean_q: 17.297646
 56471/100000: episode: 1468, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 0.336, mean reward: 0.336 [0.336, 0.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.333, 10.100], loss: 0.008688, mae: 0.099576, mean_q: 20.010918
 56472/100000: episode: 1469, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.411, mean reward: 0.411 [0.411, 0.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.248, 10.100], loss: 0.015325, mae: 0.139222, mean_q: 13.611904
 56473/100000: episode: 1470, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 0.465, mean reward: 0.465 [0.465, 0.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.252, 10.100], loss: 0.010905, mae: 0.114786, mean_q: 19.072184
 56474/100000: episode: 1471, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 0.484, mean reward: 0.484 [0.484, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.206, 10.100], loss: 0.012299, mae: 0.113166, mean_q: 15.793261
 56475/100000: episode: 1472, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.364, mean reward: 0.364 [0.364, 0.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.241, 10.100], loss: 0.013629, mae: 0.127580, mean_q: 19.735992
 56476/100000: episode: 1473, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 0.464, mean reward: 0.464 [0.464, 0.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.187, 10.100], loss: 0.013715, mae: 0.134260, mean_q: 17.405754
 56477/100000: episode: 1474, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 0.420, mean reward: 0.420 [0.420, 0.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.238, 10.100], loss: 0.013083, mae: 0.135009, mean_q: 16.653667
 56478/100000: episode: 1475, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.488, mean reward: 0.488 [0.488, 0.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.216, 10.100], loss: 0.021683, mae: 0.167348, mean_q: 17.683304
 56479/100000: episode: 1476, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 0.453, mean reward: 0.453 [0.453, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.211, 10.100], loss: 0.014173, mae: 0.119392, mean_q: 13.764437
 56480/100000: episode: 1477, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 0.390, mean reward: 0.390 [0.390, 0.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.195, 10.100], loss: 0.023780, mae: 0.123853, mean_q: 21.610611
 56481/100000: episode: 1478, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.228, 10.100], loss: 0.009826, mae: 0.107600, mean_q: 17.779690
 56482/100000: episode: 1479, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 0.425, mean reward: 0.425 [0.425, 0.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.261, 10.100], loss: 0.007903, mae: 0.099668, mean_q: 18.411507
 56483/100000: episode: 1480, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 0.405, mean reward: 0.405 [0.405, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.269, 10.100], loss: 0.011176, mae: 0.097526, mean_q: 17.349581
 56484/100000: episode: 1481, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 0.405, mean reward: 0.405 [0.405, 0.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.260, 10.100], loss: 0.008165, mae: 0.101628, mean_q: 14.562073
 56485/100000: episode: 1482, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.365, mean reward: 0.365 [0.365, 0.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.282, 10.100], loss: 0.014188, mae: 0.136391, mean_q: 15.419902
 56486/100000: episode: 1483, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.452, mean reward: 0.452 [0.452, 0.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.223, 10.100], loss: 0.012059, mae: 0.111934, mean_q: 16.089201
 56487/100000: episode: 1484, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 0.391, mean reward: 0.391 [0.391, 0.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.263, 10.100], loss: 0.008949, mae: 0.108266, mean_q: 16.892273
 56488/100000: episode: 1485, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 0.418, mean reward: 0.418 [0.418, 0.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.315, 10.100], loss: 0.013088, mae: 0.126394, mean_q: 17.187771
 56489/100000: episode: 1486, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.504, mean reward: 0.504 [0.504, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.219, 10.100], loss: 0.013787, mae: 0.134510, mean_q: 15.647575
 56490/100000: episode: 1487, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 0.398, mean reward: 0.398 [0.398, 0.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.250, 10.100], loss: 0.005541, mae: 0.083431, mean_q: 17.722202
 56491/100000: episode: 1488, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 0.444, mean reward: 0.444 [0.444, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.237, 10.100], loss: 0.010453, mae: 0.126125, mean_q: 21.874691
 56492/100000: episode: 1489, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 0.453, mean reward: 0.453 [0.453, 0.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.234, 10.100], loss: 0.010366, mae: 0.105445, mean_q: 15.881235
 56493/100000: episode: 1490, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 0.403, mean reward: 0.403 [0.403, 0.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.268, 10.100], loss: 0.010165, mae: 0.115292, mean_q: 20.615871
 56494/100000: episode: 1491, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 0.385, mean reward: 0.385 [0.385, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.238, 10.100], loss: 0.017197, mae: 0.160111, mean_q: 18.515141
 56495/100000: episode: 1492, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 0.439, mean reward: 0.439 [0.439, 0.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.233, 10.100], loss: 0.010186, mae: 0.109121, mean_q: 14.532749
 56496/100000: episode: 1493, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 0.434, mean reward: 0.434 [0.434, 0.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.267, 10.100], loss: 0.018506, mae: 0.160337, mean_q: 17.531918
[Info] Not found new level, current best level reached = 0.41404181718826294
 56497/100000: episode: 1494, duration: 4.038s, episode steps: 1, steps per second: 0, episode reward: 0.436, mean reward: 0.436 [0.436, 0.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.267, 10.100], loss: 0.010739, mae: 0.112202, mean_q: 17.041248
 56597/100000: episode: 1495, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 54.150, mean reward: 0.541 [0.378, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.742, 10.226], loss: 0.013160, mae: 0.122623, mean_q: 17.446087
 56697/100000: episode: 1496, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 53.209, mean reward: 0.532 [0.254, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.469, 10.256], loss: 0.010844, mae: 0.111011, mean_q: 17.761711
 56797/100000: episode: 1497, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 50.832, mean reward: 0.508 [0.230, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.232, 10.098], loss: 0.011454, mae: 0.111345, mean_q: 17.467232
 56897/100000: episode: 1498, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 54.079, mean reward: 0.541 [0.289, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.855, 10.170], loss: 0.011813, mae: 0.112620, mean_q: 17.440323
 56997/100000: episode: 1499, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 54.666, mean reward: 0.547 [0.277, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.605, 10.098], loss: 0.010373, mae: 0.109481, mean_q: 17.372919
 57097/100000: episode: 1500, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 53.972, mean reward: 0.540 [0.241, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.151, 10.098], loss: 0.011115, mae: 0.112193, mean_q: 17.509506
 57197/100000: episode: 1501, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 52.896, mean reward: 0.529 [0.322, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.243, 10.112], loss: 0.011935, mae: 0.115573, mean_q: 17.854311
 57297/100000: episode: 1502, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 49.603, mean reward: 0.496 [0.241, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.270, 10.389], loss: 0.012496, mae: 0.119102, mean_q: 17.515657
 57397/100000: episode: 1503, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 54.518, mean reward: 0.545 [0.236, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.971, 10.098], loss: 0.011081, mae: 0.115241, mean_q: 17.228540
 57497/100000: episode: 1504, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 55.005, mean reward: 0.550 [0.341, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.251, 10.116], loss: 0.010918, mae: 0.112802, mean_q: 17.697048
 57597/100000: episode: 1505, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 50.637, mean reward: 0.506 [0.225, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.107, 10.098], loss: 0.011489, mae: 0.113072, mean_q: 17.642155
 57697/100000: episode: 1506, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 49.998, mean reward: 0.500 [0.227, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.812, 10.128], loss: 0.010555, mae: 0.110857, mean_q: 17.014057
 57797/100000: episode: 1507, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 50.792, mean reward: 0.508 [0.250, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.362, 10.265], loss: 0.011001, mae: 0.114602, mean_q: 17.687140
 57897/100000: episode: 1508, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 53.518, mean reward: 0.535 [0.278, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.202, 10.098], loss: 0.010432, mae: 0.111258, mean_q: 17.467278
 57997/100000: episode: 1509, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.224, mean reward: 0.582 [0.411, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.627, 10.233], loss: 0.011438, mae: 0.115645, mean_q: 17.379290
 58097/100000: episode: 1510, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 51.488, mean reward: 0.515 [0.339, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.586, 10.098], loss: 0.011303, mae: 0.116482, mean_q: 17.525192
 58197/100000: episode: 1511, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 56.584, mean reward: 0.566 [0.389, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.465, 10.098], loss: 0.013137, mae: 0.125166, mean_q: 17.478182
 58297/100000: episode: 1512, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 55.024, mean reward: 0.550 [0.241, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.549, 10.098], loss: 0.010291, mae: 0.108969, mean_q: 17.570053
 58397/100000: episode: 1513, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 56.103, mean reward: 0.561 [0.358, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.245, 10.098], loss: 0.010161, mae: 0.107518, mean_q: 17.461327
 58497/100000: episode: 1514, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 53.029, mean reward: 0.530 [0.334, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.492, 10.128], loss: 0.012334, mae: 0.121275, mean_q: 17.287512
 58597/100000: episode: 1515, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 48.567, mean reward: 0.486 [0.251, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.866, 10.098], loss: 0.009596, mae: 0.106446, mean_q: 17.425152
 58697/100000: episode: 1516, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 56.204, mean reward: 0.562 [0.365, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.455, 10.098], loss: 0.012003, mae: 0.116364, mean_q: 17.379822
 58797/100000: episode: 1517, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 49.749, mean reward: 0.497 [0.229, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.588, 10.098], loss: 0.009703, mae: 0.107492, mean_q: 17.483870
 58897/100000: episode: 1518, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 46.132, mean reward: 0.461 [0.161, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.853, 10.133], loss: 0.009566, mae: 0.104366, mean_q: 17.314852
 58997/100000: episode: 1519, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 53.437, mean reward: 0.534 [0.310, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.043, 10.247], loss: 0.011835, mae: 0.118289, mean_q: 17.345541
 59097/100000: episode: 1520, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 50.276, mean reward: 0.503 [0.283, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.559, 10.098], loss: 0.010133, mae: 0.107978, mean_q: 17.191282
 59197/100000: episode: 1521, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 54.596, mean reward: 0.546 [0.266, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.482, 10.098], loss: 0.010573, mae: 0.109036, mean_q: 17.445063
 59297/100000: episode: 1522, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 53.557, mean reward: 0.536 [0.268, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.150], loss: 0.011144, mae: 0.110194, mean_q: 17.408592
 59397/100000: episode: 1523, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 56.143, mean reward: 0.561 [0.326, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.880, 10.098], loss: 0.011748, mae: 0.116090, mean_q: 17.904722
 59497/100000: episode: 1524, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 51.963, mean reward: 0.520 [0.289, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.915, 10.281], loss: 0.010096, mae: 0.106898, mean_q: 17.233843
 59597/100000: episode: 1525, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 53.241, mean reward: 0.532 [0.314, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.395, 10.098], loss: 0.010666, mae: 0.111681, mean_q: 17.632093
 59697/100000: episode: 1526, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 52.765, mean reward: 0.528 [0.328, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.620, 10.207], loss: 0.009790, mae: 0.106725, mean_q: 17.233404
 59797/100000: episode: 1527, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 56.195, mean reward: 0.562 [0.281, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.722, 10.098], loss: 0.010798, mae: 0.111660, mean_q: 17.530523
 59897/100000: episode: 1528, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 51.827, mean reward: 0.518 [0.351, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.439, 10.344], loss: 0.010478, mae: 0.109611, mean_q: 17.243303
 59997/100000: episode: 1529, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.163, mean reward: 0.552 [0.255, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.913, 10.098], loss: 0.010927, mae: 0.112194, mean_q: 17.607836
 60097/100000: episode: 1530, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 54.555, mean reward: 0.546 [0.357, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.460, 10.108], loss: 0.012081, mae: 0.116006, mean_q: 17.047951
 60197/100000: episode: 1531, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.551, mean reward: 0.566 [0.333, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.722, 10.098], loss: 0.010959, mae: 0.113090, mean_q: 17.170797
 60297/100000: episode: 1532, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 48.576, mean reward: 0.486 [0.196, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.956, 10.298], loss: 0.011205, mae: 0.112863, mean_q: 17.277403
 60397/100000: episode: 1533, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 49.278, mean reward: 0.493 [0.114, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.200, 10.098], loss: 0.009958, mae: 0.105899, mean_q: 17.115894
 60497/100000: episode: 1534, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 55.640, mean reward: 0.556 [0.356, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.453, 10.098], loss: 0.011782, mae: 0.116551, mean_q: 17.229134
 60597/100000: episode: 1535, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 55.478, mean reward: 0.555 [0.343, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.742, 10.149], loss: 0.014063, mae: 0.128256, mean_q: 17.689411
 60697/100000: episode: 1536, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 52.079, mean reward: 0.521 [0.218, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.100, 10.224], loss: 0.010709, mae: 0.109117, mean_q: 17.985785
 60797/100000: episode: 1537, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 53.424, mean reward: 0.534 [0.260, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.926, 10.098], loss: 0.011006, mae: 0.108748, mean_q: 18.217190
 60897/100000: episode: 1538, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 56.556, mean reward: 0.566 [0.329, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.502, 10.098], loss: 0.010108, mae: 0.106968, mean_q: 18.428633
 60997/100000: episode: 1539, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 55.758, mean reward: 0.558 [0.336, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.725, 10.098], loss: 0.007551, mae: 0.092766, mean_q: 18.659967
 61097/100000: episode: 1540, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 56.560, mean reward: 0.566 [0.296, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.547, 10.180], loss: 0.009122, mae: 0.101626, mean_q: 18.997370
 61197/100000: episode: 1541, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 53.401, mean reward: 0.534 [0.219, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.744, 10.170], loss: 0.009273, mae: 0.102481, mean_q: 18.946602
 61297/100000: episode: 1542, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.038, mean reward: 0.540 [0.235, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.211, 10.103], loss: 0.008503, mae: 0.098455, mean_q: 19.156675
 61397/100000: episode: 1543, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 54.013, mean reward: 0.540 [0.264, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.822, 10.332], loss: 0.009628, mae: 0.105591, mean_q: 19.338264
 61497/100000: episode: 1544, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 51.159, mean reward: 0.512 [0.311, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.041, 10.367], loss: 0.008990, mae: 0.101242, mean_q: 19.296158
 61597/100000: episode: 1545, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 54.713, mean reward: 0.547 [0.239, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.708, 10.122], loss: 0.009699, mae: 0.106179, mean_q: 19.574362
 61697/100000: episode: 1546, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 52.196, mean reward: 0.522 [0.273, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.455, 10.165], loss: 0.008721, mae: 0.098664, mean_q: 19.662521
 61797/100000: episode: 1547, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 54.176, mean reward: 0.542 [0.358, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.414, 10.128], loss: 0.010731, mae: 0.111746, mean_q: 19.723745
 61897/100000: episode: 1548, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 51.827, mean reward: 0.518 [0.298, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.554, 10.160], loss: 0.009649, mae: 0.104647, mean_q: 19.525589
 61997/100000: episode: 1549, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 49.930, mean reward: 0.499 [0.208, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.934, 10.098], loss: 0.009745, mae: 0.108699, mean_q: 19.449484
 62097/100000: episode: 1550, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 51.527, mean reward: 0.515 [0.234, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.801, 10.098], loss: 0.008536, mae: 0.100672, mean_q: 19.605047
 62197/100000: episode: 1551, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 49.062, mean reward: 0.491 [0.125, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.210, 10.300], loss: 0.010735, mae: 0.110785, mean_q: 19.551237
 62297/100000: episode: 1552, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 56.697, mean reward: 0.567 [0.295, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.265, 10.139], loss: 0.009720, mae: 0.105664, mean_q: 19.466944
 62397/100000: episode: 1553, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 52.602, mean reward: 0.526 [0.265, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.432, 10.098], loss: 0.009207, mae: 0.102883, mean_q: 19.281801
 62497/100000: episode: 1554, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 51.147, mean reward: 0.511 [0.250, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.920, 10.197], loss: 0.010025, mae: 0.107459, mean_q: 19.758310
 62597/100000: episode: 1555, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 54.646, mean reward: 0.546 [0.152, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.092, 10.098], loss: 0.010359, mae: 0.112288, mean_q: 19.592897
 62697/100000: episode: 1556, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 53.385, mean reward: 0.534 [0.233, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.063, 10.187], loss: 0.010555, mae: 0.110276, mean_q: 19.655361
 62797/100000: episode: 1557, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.050, mean reward: 0.541 [0.360, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.957, 10.098], loss: 0.009761, mae: 0.107917, mean_q: 19.457500
 62897/100000: episode: 1558, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 54.373, mean reward: 0.544 [0.287, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.803, 10.098], loss: 0.009896, mae: 0.105425, mean_q: 19.435291
 62997/100000: episode: 1559, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 43.677, mean reward: 0.437 [0.205, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.286, 10.098], loss: 0.012165, mae: 0.119708, mean_q: 19.716578
 63097/100000: episode: 1560, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 55.001, mean reward: 0.550 [0.388, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.602, 10.276], loss: 0.010934, mae: 0.111267, mean_q: 19.559971
 63197/100000: episode: 1561, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 51.033, mean reward: 0.510 [0.282, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.497, 10.098], loss: 0.011216, mae: 0.113658, mean_q: 19.370129
 63297/100000: episode: 1562, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 55.744, mean reward: 0.557 [0.305, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.228, 10.173], loss: 0.010399, mae: 0.110001, mean_q: 19.784935
 63397/100000: episode: 1563, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 55.132, mean reward: 0.551 [0.137, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.122, 10.150], loss: 0.010259, mae: 0.107713, mean_q: 19.637386
 63497/100000: episode: 1564, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 53.667, mean reward: 0.537 [0.241, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.725, 10.400], loss: 0.010075, mae: 0.110292, mean_q: 19.722841
 63597/100000: episode: 1565, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 56.029, mean reward: 0.560 [0.217, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.481, 10.098], loss: 0.012298, mae: 0.120083, mean_q: 19.622707
 63697/100000: episode: 1566, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 55.833, mean reward: 0.558 [0.412, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.362, 10.178], loss: 0.011986, mae: 0.119018, mean_q: 19.648932
 63797/100000: episode: 1567, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 55.188, mean reward: 0.552 [0.273, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.487, 10.098], loss: 0.012152, mae: 0.117516, mean_q: 19.953146
 63897/100000: episode: 1568, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 50.582, mean reward: 0.506 [0.274, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.685, 10.219], loss: 0.010732, mae: 0.112168, mean_q: 19.562475
 63997/100000: episode: 1569, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 45.433, mean reward: 0.454 [0.191, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.641, 10.435], loss: 0.013166, mae: 0.125445, mean_q: 19.827654
 64097/100000: episode: 1570, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 53.534, mean reward: 0.535 [0.277, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.204, 10.308], loss: 0.012189, mae: 0.117503, mean_q: 19.489779
 64197/100000: episode: 1571, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 53.741, mean reward: 0.537 [0.285, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.372, 10.098], loss: 0.014364, mae: 0.126207, mean_q: 19.453257
 64297/100000: episode: 1572, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 56.135, mean reward: 0.561 [0.383, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.733, 10.098], loss: 0.009725, mae: 0.104712, mean_q: 19.591547
 64397/100000: episode: 1573, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 49.831, mean reward: 0.498 [0.135, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-2.176, 10.098], loss: 0.010370, mae: 0.109003, mean_q: 19.482943
 64497/100000: episode: 1574, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 47.862, mean reward: 0.479 [0.299, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.312, 10.255], loss: 0.010759, mae: 0.110454, mean_q: 19.728388
 64597/100000: episode: 1575, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 47.857, mean reward: 0.479 [0.156, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.163, 10.485], loss: 0.010717, mae: 0.112226, mean_q: 19.816315
 64697/100000: episode: 1576, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 53.460, mean reward: 0.535 [0.291, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.813, 10.098], loss: 0.010666, mae: 0.109541, mean_q: 19.600506
 64797/100000: episode: 1577, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 53.508, mean reward: 0.535 [0.308, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.162, 10.098], loss: 0.011682, mae: 0.115316, mean_q: 19.670586
 64897/100000: episode: 1578, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 51.219, mean reward: 0.512 [0.254, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.119, 10.295], loss: 0.010580, mae: 0.112223, mean_q: 19.948219
 64997/100000: episode: 1579, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 55.120, mean reward: 0.551 [0.206, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.261, 10.202], loss: 0.009734, mae: 0.106686, mean_q: 19.918245
 65097/100000: episode: 1580, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 55.279, mean reward: 0.553 [0.334, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.565, 10.098], loss: 0.010600, mae: 0.111668, mean_q: 19.674446
 65197/100000: episode: 1581, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.909, mean reward: 0.579 [0.369, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.915, 10.180], loss: 0.012110, mae: 0.117440, mean_q: 19.924004
 65297/100000: episode: 1582, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 51.523, mean reward: 0.515 [0.282, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.687, 10.409], loss: 0.011157, mae: 0.113832, mean_q: 19.875313
 65397/100000: episode: 1583, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 51.853, mean reward: 0.519 [0.293, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.485, 10.098], loss: 0.010244, mae: 0.108017, mean_q: 19.660446
 65497/100000: episode: 1584, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 57.096, mean reward: 0.571 [0.225, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.493, 10.116], loss: 0.009191, mae: 0.103156, mean_q: 19.368084
 65597/100000: episode: 1585, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 49.035, mean reward: 0.490 [0.229, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.344, 10.220], loss: 0.009837, mae: 0.107682, mean_q: 19.767038
 65697/100000: episode: 1586, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 55.978, mean reward: 0.560 [0.336, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.276, 10.098], loss: 0.011152, mae: 0.113564, mean_q: 19.733398
 65797/100000: episode: 1587, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 53.905, mean reward: 0.539 [0.335, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.711, 10.100], loss: 0.011380, mae: 0.114082, mean_q: 19.573954
 65897/100000: episode: 1588, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 53.890, mean reward: 0.539 [0.310, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.564, 10.275], loss: 0.012222, mae: 0.114221, mean_q: 19.576658
 65997/100000: episode: 1589, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 49.270, mean reward: 0.493 [0.315, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.496, 10.243], loss: 0.010920, mae: 0.112093, mean_q: 19.658937
 66097/100000: episode: 1590, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.294, mean reward: 0.573 [0.290, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.185, 10.106], loss: 0.009213, mae: 0.102182, mean_q: 19.813955
 66197/100000: episode: 1591, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 56.291, mean reward: 0.563 [0.310, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.308, 10.181], loss: 0.009844, mae: 0.106783, mean_q: 19.861635
 66297/100000: episode: 1592, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 52.882, mean reward: 0.529 [0.305, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.342, 10.221], loss: 0.009454, mae: 0.102892, mean_q: 19.764780
 66397/100000: episode: 1593, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 53.957, mean reward: 0.540 [0.240, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.381, 10.112], loss: 0.009790, mae: 0.107676, mean_q: 19.544977
[Info] New level: 0.44133424758911133 | Considering 42/58 traces
 66497/100000: episode: 1594, duration: 4.446s, episode steps: 100, steps per second: 22, episode reward: 52.725, mean reward: 0.527 [0.376, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.406, 10.179], loss: 0.009162, mae: 0.102420, mean_q: 19.971180
 66500/100000: episode: 1595, duration: 0.024s, episode steps: 3, steps per second: 127, episode reward: 1.183, mean reward: 0.394 [0.374, 0.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.275, 10.100], loss: 0.009491, mae: 0.106803, mean_q: 18.877092
 66502/100000: episode: 1596, duration: 0.019s, episode steps: 2, steps per second: 103, episode reward: 1.029, mean reward: 0.514 [0.470, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.102, 10.100], loss: 0.008317, mae: 0.095976, mean_q: 20.584450
 66504/100000: episode: 1597, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 0.942, mean reward: 0.471 [0.434, 0.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.147, 10.100], loss: 0.008760, mae: 0.106246, mean_q: 18.544308
 66506/100000: episode: 1598, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.092, mean reward: 0.546 [0.543, 0.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.096, 10.100], loss: 0.008294, mae: 0.101083, mean_q: 22.839165
 66508/100000: episode: 1599, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.907, mean reward: 0.454 [0.422, 0.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.112, 10.100], loss: 0.008450, mae: 0.101734, mean_q: 18.809990
 66510/100000: episode: 1600, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.246, mean reward: 0.623 [0.595, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.245, 10.128], loss: 0.009293, mae: 0.093720, mean_q: 18.517317
 66512/100000: episode: 1601, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 1.083, mean reward: 0.541 [0.505, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.164, 10.100], loss: 0.016870, mae: 0.128777, mean_q: 19.582552
 66514/100000: episode: 1602, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 1.149, mean reward: 0.575 [0.514, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.159, 10.100], loss: 0.007599, mae: 0.097651, mean_q: 21.074726
 66516/100000: episode: 1603, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 1.227, mean reward: 0.614 [0.613, 0.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.165, 10.100], loss: 0.006578, mae: 0.096292, mean_q: 19.599749
 66518/100000: episode: 1604, duration: 0.017s, episode steps: 2, steps per second: 115, episode reward: 1.126, mean reward: 0.563 [0.530, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.082, 10.100], loss: 0.010058, mae: 0.110662, mean_q: 19.341812
 66520/100000: episode: 1605, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 0.813, mean reward: 0.407 [0.365, 0.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.247, 10.100], loss: 0.009363, mae: 0.116382, mean_q: 20.369310
 66522/100000: episode: 1606, duration: 0.019s, episode steps: 2, steps per second: 107, episode reward: 0.921, mean reward: 0.460 [0.457, 0.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.211, 10.100], loss: 0.010162, mae: 0.099085, mean_q: 20.544823
 66524/100000: episode: 1607, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.743, mean reward: 0.372 [0.322, 0.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.079, 10.100], loss: 0.007369, mae: 0.098085, mean_q: 20.325186
 66526/100000: episode: 1608, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 1.012, mean reward: 0.506 [0.483, 0.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.096, 10.100], loss: 0.007284, mae: 0.091844, mean_q: 20.873045
 66528/100000: episode: 1609, duration: 0.015s, episode steps: 2, steps per second: 130, episode reward: 0.954, mean reward: 0.477 [0.411, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.074, 10.100], loss: 0.008978, mae: 0.103353, mean_q: 20.555298
 66530/100000: episode: 1610, duration: 0.014s, episode steps: 2, steps per second: 140, episode reward: 1.001, mean reward: 0.500 [0.496, 0.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.121, 10.122], loss: 0.008001, mae: 0.101673, mean_q: 20.819221
 66532/100000: episode: 1611, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 0.898, mean reward: 0.449 [0.438, 0.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.082, 10.100], loss: 0.006487, mae: 0.090503, mean_q: 17.841019
 66534/100000: episode: 1612, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 1.323, mean reward: 0.661 [0.659, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.121, 10.139], loss: 0.010688, mae: 0.105951, mean_q: 20.165848
 66536/100000: episode: 1613, duration: 0.017s, episode steps: 2, steps per second: 117, episode reward: 0.694, mean reward: 0.347 [0.347, 0.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.118, 10.100], loss: 0.009074, mae: 0.109666, mean_q: 18.175243
 66538/100000: episode: 1614, duration: 0.021s, episode steps: 2, steps per second: 93, episode reward: 1.085, mean reward: 0.543 [0.523, 0.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.124, 10.100], loss: 0.010754, mae: 0.118403, mean_q: 21.447336
 66540/100000: episode: 1615, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 1.158, mean reward: 0.579 [0.533, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.179, 10.100], loss: 0.012308, mae: 0.127002, mean_q: 18.844736
 66542/100000: episode: 1616, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.980, mean reward: 0.490 [0.484, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.103, 10.100], loss: 0.012046, mae: 0.113709, mean_q: 16.650064
 66544/100000: episode: 1617, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.152, mean reward: 0.576 [0.561, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.166, 10.100], loss: 0.017775, mae: 0.159606, mean_q: 19.227354
 66546/100000: episode: 1618, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.860, mean reward: 0.430 [0.390, 0.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.252, 10.100], loss: 0.011101, mae: 0.126016, mean_q: 17.741489
 66548/100000: episode: 1619, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.196, mean reward: 0.598 [0.545, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.093, 10.165], loss: 0.010900, mae: 0.116154, mean_q: 18.178646
 66550/100000: episode: 1620, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 0.956, mean reward: 0.478 [0.455, 0.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.190, 10.100], loss: 0.014426, mae: 0.133105, mean_q: 17.352678
 66553/100000: episode: 1621, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 1.195, mean reward: 0.398 [0.348, 0.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.446, 10.100], loss: 0.009565, mae: 0.101073, mean_q: 19.227613
 66555/100000: episode: 1622, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 1.084, mean reward: 0.542 [0.495, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.270, 10.100], loss: 0.011527, mae: 0.120539, mean_q: 19.745358
 66557/100000: episode: 1623, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.887, mean reward: 0.443 [0.432, 0.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.045, 10.100], loss: 0.009776, mae: 0.104277, mean_q: 17.803249
 66559/100000: episode: 1624, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.071, mean reward: 0.536 [0.528, 0.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.090, 10.100], loss: 0.010804, mae: 0.118704, mean_q: 20.922335
 66561/100000: episode: 1625, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 1.058, mean reward: 0.529 [0.501, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.206, 10.100], loss: 0.007340, mae: 0.096069, mean_q: 18.461550
 66563/100000: episode: 1626, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 0.994, mean reward: 0.497 [0.494, 0.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.120, 10.100], loss: 0.011252, mae: 0.124320, mean_q: 18.624123
 66565/100000: episode: 1627, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 0.799, mean reward: 0.400 [0.345, 0.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.231, 10.100], loss: 0.007617, mae: 0.095354, mean_q: 17.574432
 66567/100000: episode: 1628, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: 0.788, mean reward: 0.394 [0.360, 0.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.114, 10.100], loss: 0.007246, mae: 0.099696, mean_q: 20.260832
 66569/100000: episode: 1629, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 0.857, mean reward: 0.429 [0.343, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.103, 10.163], loss: 0.011747, mae: 0.115487, mean_q: 20.021282
 66571/100000: episode: 1630, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 1.214, mean reward: 0.607 [0.606, 0.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.079, 10.100], loss: 0.011681, mae: 0.111755, mean_q: 20.817619
 66573/100000: episode: 1631, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.165, mean reward: 0.582 [0.539, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.617, 10.100], loss: 0.010728, mae: 0.113270, mean_q: 21.241295
 66575/100000: episode: 1632, duration: 0.016s, episode steps: 2, steps per second: 127, episode reward: 1.137, mean reward: 0.568 [0.562, 0.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.075, 10.101], loss: 0.008823, mae: 0.109601, mean_q: 20.907715
 66578/100000: episode: 1633, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 0.994, mean reward: 0.331 [0.255, 0.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.354, 10.100], loss: 0.020071, mae: 0.170949, mean_q: 19.273157
 66580/100000: episode: 1634, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.162, mean reward: 0.581 [0.530, 0.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.110, 10.100], loss: 0.014483, mae: 0.132896, mean_q: 19.003355
 66582/100000: episode: 1635, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.127, mean reward: 0.564 [0.549, 0.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.142, 10.100], loss: 0.014125, mae: 0.141893, mean_q: 18.564072
 66584/100000: episode: 1636, duration: 0.015s, episode steps: 2, steps per second: 135, episode reward: 0.922, mean reward: 0.461 [0.399, 0.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.056, 10.100], loss: 0.010805, mae: 0.107484, mean_q: 18.620523
 66586/100000: episode: 1637, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 0.889, mean reward: 0.445 [0.374, 0.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.203, 10.100], loss: 0.010695, mae: 0.125510, mean_q: 20.805832
 66588/100000: episode: 1638, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.173, mean reward: 0.586 [0.584, 0.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.169, 10.109], loss: 0.009756, mae: 0.104093, mean_q: 19.010767
 66590/100000: episode: 1639, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 1.227, mean reward: 0.613 [0.612, 0.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.095, 10.100], loss: 0.008464, mae: 0.103422, mean_q: 18.688366
 66592/100000: episode: 1640, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 0.984, mean reward: 0.492 [0.488, 0.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.074, 10.100], loss: 0.012200, mae: 0.114823, mean_q: 20.236622
 66594/100000: episode: 1641, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 0.920, mean reward: 0.460 [0.278, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.060, 10.100], loss: 0.008604, mae: 0.095799, mean_q: 21.235546
 66596/100000: episode: 1642, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.115, mean reward: 0.557 [0.555, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.157, 10.100], loss: 0.007384, mae: 0.090389, mean_q: 17.887562
 66599/100000: episode: 1643, duration: 0.018s, episode steps: 3, steps per second: 164, episode reward: 1.080, mean reward: 0.360 [0.332, 0.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.363, 10.100], loss: 0.009127, mae: 0.103539, mean_q: 17.050598
 66601/100000: episode: 1644, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.120, mean reward: 0.560 [0.540, 0.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.084, 10.111], loss: 0.012226, mae: 0.134756, mean_q: 17.878414
 66603/100000: episode: 1645, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 0.954, mean reward: 0.477 [0.426, 0.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.128, 10.100], loss: 0.008818, mae: 0.105827, mean_q: 19.044258
 66605/100000: episode: 1646, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.227, mean reward: 0.613 [0.592, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.120, 10.100], loss: 0.007864, mae: 0.103867, mean_q: 18.593559
 66607/100000: episode: 1647, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.757, mean reward: 0.378 [0.372, 0.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.100], loss: 0.014313, mae: 0.124028, mean_q: 19.737434
 66610/100000: episode: 1648, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 1.115, mean reward: 0.372 [0.299, 0.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.292, 10.100], loss: 0.011626, mae: 0.119228, mean_q: 17.790621
 66612/100000: episode: 1649, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 0.957, mean reward: 0.478 [0.467, 0.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.311, 10.100], loss: 0.008611, mae: 0.101535, mean_q: 21.709679
 66614/100000: episode: 1650, duration: 0.016s, episode steps: 2, steps per second: 125, episode reward: 1.126, mean reward: 0.563 [0.481, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.154, 10.125], loss: 0.008849, mae: 0.108179, mean_q: 19.632608
 66616/100000: episode: 1651, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 1.047, mean reward: 0.524 [0.507, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.188, 10.100], loss: 0.007535, mae: 0.089616, mean_q: 19.126778
[Info] Not found new level, current best level reached = 0.44133424758911133
 66618/100000: episode: 1652, duration: 3.982s, episode steps: 2, steps per second: 1, episode reward: 0.701, mean reward: 0.351 [0.330, 0.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.264, 10.100], loss: 0.011143, mae: 0.116420, mean_q: 19.126509
 66718/100000: episode: 1653, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 50.004, mean reward: 0.500 [0.266, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.803, 10.305], loss: 0.008856, mae: 0.101726, mean_q: 19.383154
 66818/100000: episode: 1654, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 51.501, mean reward: 0.515 [0.306, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.290, 10.100], loss: 0.010514, mae: 0.110081, mean_q: 18.864578
 66918/100000: episode: 1655, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 54.670, mean reward: 0.547 [0.241, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.720, 10.334], loss: 0.011036, mae: 0.111853, mean_q: 19.057793
 67018/100000: episode: 1656, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 53.275, mean reward: 0.533 [0.354, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.205, 10.098], loss: 0.011163, mae: 0.112083, mean_q: 19.286869
 67118/100000: episode: 1657, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 53.480, mean reward: 0.535 [0.218, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.356, 10.098], loss: 0.011308, mae: 0.111709, mean_q: 19.342703
 67218/100000: episode: 1658, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 52.036, mean reward: 0.520 [0.255, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.885, 10.117], loss: 0.011219, mae: 0.112723, mean_q: 19.073765
 67318/100000: episode: 1659, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 56.174, mean reward: 0.562 [0.338, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.851, 10.098], loss: 0.011907, mae: 0.116971, mean_q: 19.471687
 67418/100000: episode: 1660, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 54.854, mean reward: 0.549 [0.303, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.727, 10.098], loss: 0.011259, mae: 0.112639, mean_q: 19.184744
 67518/100000: episode: 1661, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 54.953, mean reward: 0.550 [0.334, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.856, 10.098], loss: 0.011502, mae: 0.113921, mean_q: 19.171593
 67618/100000: episode: 1662, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 54.165, mean reward: 0.542 [0.298, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.707, 10.098], loss: 0.010644, mae: 0.108807, mean_q: 19.255049
 67718/100000: episode: 1663, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 49.231, mean reward: 0.492 [0.241, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.318, 10.217], loss: 0.011266, mae: 0.113648, mean_q: 19.346983
 67818/100000: episode: 1664, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 55.854, mean reward: 0.559 [0.377, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.115, 10.098], loss: 0.009566, mae: 0.103425, mean_q: 19.057051
 67918/100000: episode: 1665, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 52.820, mean reward: 0.528 [0.305, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.272, 10.208], loss: 0.010720, mae: 0.110126, mean_q: 19.057123
 68018/100000: episode: 1666, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 54.967, mean reward: 0.550 [0.334, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.600, 10.323], loss: 0.010059, mae: 0.105288, mean_q: 19.138226
 68118/100000: episode: 1667, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 54.140, mean reward: 0.541 [0.361, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.052, 10.340], loss: 0.011388, mae: 0.113707, mean_q: 19.383530
 68218/100000: episode: 1668, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 56.851, mean reward: 0.569 [0.370, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.466, 10.235], loss: 0.011816, mae: 0.115677, mean_q: 19.352627
 68318/100000: episode: 1669, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.077, mean reward: 0.571 [0.329, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.122], loss: 0.008447, mae: 0.098522, mean_q: 19.232580
 68418/100000: episode: 1670, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 47.094, mean reward: 0.471 [0.228, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.551, 10.098], loss: 0.010270, mae: 0.107988, mean_q: 19.555929
 68518/100000: episode: 1671, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 51.879, mean reward: 0.519 [0.347, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.365, 10.122], loss: 0.010402, mae: 0.110029, mean_q: 19.123232
 68618/100000: episode: 1672, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 54.203, mean reward: 0.542 [0.325, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.402, 10.098], loss: 0.011165, mae: 0.111206, mean_q: 19.296162
 68718/100000: episode: 1673, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 53.068, mean reward: 0.531 [0.161, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.213, 10.098], loss: 0.010580, mae: 0.111198, mean_q: 19.413342
 68818/100000: episode: 1674, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 55.408, mean reward: 0.554 [0.289, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.612, 10.098], loss: 0.012108, mae: 0.117905, mean_q: 19.322969
 68918/100000: episode: 1675, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 52.883, mean reward: 0.529 [0.290, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.597, 10.098], loss: 0.009716, mae: 0.105252, mean_q: 18.803846
 69018/100000: episode: 1676, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 52.288, mean reward: 0.523 [0.289, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.547, 10.098], loss: 0.011615, mae: 0.116001, mean_q: 19.331121
 69118/100000: episode: 1677, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 54.034, mean reward: 0.540 [0.277, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.595, 10.098], loss: 0.008688, mae: 0.100586, mean_q: 19.191717
 69218/100000: episode: 1678, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 53.687, mean reward: 0.537 [0.284, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.091, 10.138], loss: 0.010452, mae: 0.110777, mean_q: 19.094454
 69318/100000: episode: 1679, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 54.802, mean reward: 0.548 [0.351, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.089, 10.147], loss: 0.010329, mae: 0.108668, mean_q: 19.185602
 69418/100000: episode: 1680, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 53.463, mean reward: 0.535 [0.269, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.002, 10.098], loss: 0.008378, mae: 0.098311, mean_q: 19.158092
 69518/100000: episode: 1681, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 48.618, mean reward: 0.486 [0.276, 0.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.791, 10.243], loss: 0.010223, mae: 0.108170, mean_q: 19.216850
 69618/100000: episode: 1682, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.520, mean reward: 0.535 [0.343, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.617, 10.098], loss: 0.011353, mae: 0.114083, mean_q: 18.995216
 69718/100000: episode: 1683, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 56.194, mean reward: 0.562 [0.336, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.702, 10.250], loss: 0.009300, mae: 0.102825, mean_q: 19.166895
 69818/100000: episode: 1684, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 54.937, mean reward: 0.549 [0.310, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.592, 10.129], loss: 0.009994, mae: 0.105542, mean_q: 19.020325
 69918/100000: episode: 1685, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 52.185, mean reward: 0.522 [0.211, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.237, 10.285], loss: 0.009868, mae: 0.108309, mean_q: 19.382753
 70018/100000: episode: 1686, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 51.007, mean reward: 0.510 [0.286, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.805, 10.300], loss: 0.011936, mae: 0.116756, mean_q: 19.223377
 70118/100000: episode: 1687, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: 53.140, mean reward: 0.531 [0.286, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.528, 10.356], loss: 0.009337, mae: 0.102128, mean_q: 19.308466
 70218/100000: episode: 1688, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 55.657, mean reward: 0.557 [0.283, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.648, 10.166], loss: 0.009045, mae: 0.099936, mean_q: 19.048332
 70318/100000: episode: 1689, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 54.469, mean reward: 0.545 [0.244, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.317, 10.195], loss: 0.008761, mae: 0.098910, mean_q: 19.272551
 70418/100000: episode: 1690, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 50.425, mean reward: 0.504 [0.174, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.098, 10.098], loss: 0.009209, mae: 0.103018, mean_q: 19.392248
 70518/100000: episode: 1691, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 45.577, mean reward: 0.456 [0.153, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.258, 10.544], loss: 0.009259, mae: 0.103385, mean_q: 19.273436
 70618/100000: episode: 1692, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 52.116, mean reward: 0.521 [0.314, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.165, 10.098], loss: 0.008445, mae: 0.098196, mean_q: 19.144119
 70718/100000: episode: 1693, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 55.623, mean reward: 0.556 [0.365, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.357, 10.098], loss: 0.009419, mae: 0.104602, mean_q: 19.221439
 70818/100000: episode: 1694, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 53.930, mean reward: 0.539 [0.331, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.279, 10.098], loss: 0.009556, mae: 0.104807, mean_q: 19.401541
 70918/100000: episode: 1695, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 52.333, mean reward: 0.523 [0.325, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.898, 10.098], loss: 0.009790, mae: 0.106710, mean_q: 19.267603
 71018/100000: episode: 1696, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 45.233, mean reward: 0.452 [0.159, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.073, 10.098], loss: 0.009624, mae: 0.105286, mean_q: 18.948868
 71118/100000: episode: 1697, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 56.327, mean reward: 0.563 [0.372, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.412, 10.098], loss: 0.009632, mae: 0.106779, mean_q: 19.251852
 71218/100000: episode: 1698, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 56.439, mean reward: 0.564 [0.352, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.618, 10.341], loss: 0.009496, mae: 0.105615, mean_q: 19.164572
 71318/100000: episode: 1699, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 52.662, mean reward: 0.527 [0.304, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.702, 10.127], loss: 0.009869, mae: 0.109354, mean_q: 19.352211
 71418/100000: episode: 1700, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 53.926, mean reward: 0.539 [0.307, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.238, 10.098], loss: 0.008898, mae: 0.101733, mean_q: 19.252392
 71518/100000: episode: 1701, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 52.390, mean reward: 0.524 [0.246, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.865, 10.186], loss: 0.009358, mae: 0.107077, mean_q: 19.241352
 71618/100000: episode: 1702, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 55.513, mean reward: 0.555 [0.385, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.743, 10.288], loss: 0.008374, mae: 0.101086, mean_q: 19.661522
 71718/100000: episode: 1703, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 50.171, mean reward: 0.502 [0.107, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.503, 10.098], loss: 0.007408, mae: 0.093920, mean_q: 19.495989
 71818/100000: episode: 1704, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 53.215, mean reward: 0.532 [0.261, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.472, 10.535], loss: 0.008802, mae: 0.101580, mean_q: 19.772741
 71918/100000: episode: 1705, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 51.595, mean reward: 0.516 [0.316, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.519, 10.202], loss: 0.007655, mae: 0.096675, mean_q: 19.737690
 72018/100000: episode: 1706, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.712, mean reward: 0.547 [0.393, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.645, 10.174], loss: 0.009102, mae: 0.103814, mean_q: 19.558470
 72118/100000: episode: 1707, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 55.310, mean reward: 0.553 [0.334, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.692, 10.193], loss: 0.008149, mae: 0.100271, mean_q: 19.821131
 72218/100000: episode: 1708, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 54.158, mean reward: 0.542 [0.286, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.632, 10.114], loss: 0.008149, mae: 0.099903, mean_q: 20.098448
 72318/100000: episode: 1709, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 46.017, mean reward: 0.460 [0.083, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.233, 10.098], loss: 0.007491, mae: 0.095121, mean_q: 19.342684
 72418/100000: episode: 1710, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 52.679, mean reward: 0.527 [0.357, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.662, 10.313], loss: 0.008376, mae: 0.099478, mean_q: 19.560663
 72518/100000: episode: 1711, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 52.265, mean reward: 0.523 [0.218, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.989, 10.344], loss: 0.007654, mae: 0.095655, mean_q: 19.830322
 72618/100000: episode: 1712, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 51.987, mean reward: 0.520 [0.293, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.892, 10.331], loss: 0.008280, mae: 0.100105, mean_q: 19.753328
 72718/100000: episode: 1713, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 51.305, mean reward: 0.513 [0.303, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.926, 10.168], loss: 0.008350, mae: 0.101040, mean_q: 19.499348
 72818/100000: episode: 1714, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 47.957, mean reward: 0.480 [0.268, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.678, 10.098], loss: 0.008870, mae: 0.104420, mean_q: 19.742104
 72918/100000: episode: 1715, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 50.254, mean reward: 0.503 [0.359, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.188, 10.098], loss: 0.009108, mae: 0.104172, mean_q: 19.834215
 73018/100000: episode: 1716, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 53.054, mean reward: 0.531 [0.354, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.639, 10.107], loss: 0.008851, mae: 0.103885, mean_q: 19.862791
 73118/100000: episode: 1717, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 51.043, mean reward: 0.510 [0.213, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.829, 10.098], loss: 0.009069, mae: 0.105824, mean_q: 19.659376
 73218/100000: episode: 1718, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 51.809, mean reward: 0.518 [0.164, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.421, 10.446], loss: 0.007600, mae: 0.096404, mean_q: 19.438637
 73318/100000: episode: 1719, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 57.100, mean reward: 0.571 [0.332, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.393, 10.160], loss: 0.007490, mae: 0.096461, mean_q: 19.732012
 73418/100000: episode: 1720, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.171, mean reward: 0.572 [0.358, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.522, 10.098], loss: 0.008983, mae: 0.104437, mean_q: 19.734032
 73518/100000: episode: 1721, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 54.679, mean reward: 0.547 [0.259, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.433, 10.204], loss: 0.009908, mae: 0.108656, mean_q: 19.816256
 73618/100000: episode: 1722, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 54.943, mean reward: 0.549 [0.287, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.180, 10.098], loss: 0.007494, mae: 0.095859, mean_q: 19.590271
 73718/100000: episode: 1723, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 50.378, mean reward: 0.504 [0.249, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.746, 10.201], loss: 0.007770, mae: 0.097478, mean_q: 19.734818
 73818/100000: episode: 1724, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 47.851, mean reward: 0.479 [0.236, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.764, 10.098], loss: 0.008180, mae: 0.100952, mean_q: 19.865889
 73918/100000: episode: 1725, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 55.160, mean reward: 0.552 [0.303, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.769, 10.098], loss: 0.007685, mae: 0.096166, mean_q: 19.400385
 74018/100000: episode: 1726, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 52.220, mean reward: 0.522 [0.202, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.133, 10.098], loss: 0.006701, mae: 0.090417, mean_q: 19.465719
 74118/100000: episode: 1727, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 54.415, mean reward: 0.544 [0.296, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.749, 10.098], loss: 0.007229, mae: 0.093779, mean_q: 20.065956
 74218/100000: episode: 1728, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 54.121, mean reward: 0.541 [0.351, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.295], loss: 0.007384, mae: 0.095295, mean_q: 19.773632
 74318/100000: episode: 1729, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 51.303, mean reward: 0.513 [0.139, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.246, 10.219], loss: 0.009718, mae: 0.106510, mean_q: 19.514048
 74418/100000: episode: 1730, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 46.931, mean reward: 0.469 [0.227, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.806, 10.163], loss: 0.007398, mae: 0.094970, mean_q: 19.766764
 74518/100000: episode: 1731, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 54.890, mean reward: 0.549 [0.345, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.598, 10.098], loss: 0.007714, mae: 0.096545, mean_q: 19.435574
 74618/100000: episode: 1732, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 50.983, mean reward: 0.510 [0.161, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.314, 10.145], loss: 0.007641, mae: 0.097059, mean_q: 19.560932
 74718/100000: episode: 1733, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 53.520, mean reward: 0.535 [0.257, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.133, 10.153], loss: 0.008324, mae: 0.101171, mean_q: 19.304943
 74818/100000: episode: 1734, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 50.489, mean reward: 0.505 [0.281, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.775, 10.305], loss: 0.008435, mae: 0.103192, mean_q: 19.757889
 74918/100000: episode: 1735, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 50.542, mean reward: 0.505 [0.317, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.485, 10.115], loss: 0.010490, mae: 0.111990, mean_q: 19.175703
 75018/100000: episode: 1736, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 55.693, mean reward: 0.557 [0.276, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.005, 10.216], loss: 0.007281, mae: 0.093776, mean_q: 19.594521
 75118/100000: episode: 1737, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 56.432, mean reward: 0.564 [0.348, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.596, 10.098], loss: 0.007543, mae: 0.096101, mean_q: 19.550074
 75218/100000: episode: 1738, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 54.509, mean reward: 0.545 [0.319, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.088, 10.210], loss: 0.008792, mae: 0.103874, mean_q: 19.275881
 75318/100000: episode: 1739, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 55.260, mean reward: 0.553 [0.220, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.412, 10.098], loss: 0.007324, mae: 0.094544, mean_q: 19.421124
 75418/100000: episode: 1740, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 54.372, mean reward: 0.544 [0.312, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.109, 10.098], loss: 0.008804, mae: 0.105257, mean_q: 19.543549
 75518/100000: episode: 1741, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 48.125, mean reward: 0.481 [0.227, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.559, 10.098], loss: 0.008585, mae: 0.103107, mean_q: 19.710793
 75618/100000: episode: 1742, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 42.720, mean reward: 0.427 [0.211, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.650, 10.513], loss: 0.009820, mae: 0.110479, mean_q: 19.531933
 75718/100000: episode: 1743, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 49.562, mean reward: 0.496 [0.287, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.286, 10.155], loss: 0.007952, mae: 0.098516, mean_q: 19.065981
 75818/100000: episode: 1744, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 53.600, mean reward: 0.536 [0.277, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.172, 10.098], loss: 0.007348, mae: 0.094445, mean_q: 19.519367
 75918/100000: episode: 1745, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 52.105, mean reward: 0.521 [0.318, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.371, 10.098], loss: 0.007719, mae: 0.097229, mean_q: 19.667944
 76018/100000: episode: 1746, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 50.384, mean reward: 0.504 [0.264, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.737, 10.098], loss: 0.008043, mae: 0.100249, mean_q: 19.345512
 76118/100000: episode: 1747, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 49.613, mean reward: 0.496 [0.225, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.864, 10.098], loss: 0.007712, mae: 0.096729, mean_q: 19.258736
 76218/100000: episode: 1748, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 51.998, mean reward: 0.520 [0.338, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.764, 10.098], loss: 0.008342, mae: 0.100347, mean_q: 19.726540
 76318/100000: episode: 1749, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 42.481, mean reward: 0.425 [0.199, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.627, 10.098], loss: 0.008890, mae: 0.103179, mean_q: 19.309994
 76418/100000: episode: 1750, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 49.767, mean reward: 0.498 [0.153, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.597, 10.370], loss: 0.008627, mae: 0.103367, mean_q: 19.323570
 76518/100000: episode: 1751, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 50.082, mean reward: 0.501 [0.270, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.902, 10.098], loss: 0.009114, mae: 0.105504, mean_q: 19.797256
[Info] New level: 0.4585416913032532 | Considering 87/13 traces
 76618/100000: episode: 1752, duration: 4.465s, episode steps: 100, steps per second: 22, episode reward: 54.527, mean reward: 0.545 [0.202, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.376, 10.098], loss: 0.008428, mae: 0.101417, mean_q: 19.407404
 76620/100000: episode: 1753, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 1.142, mean reward: 0.571 [0.548, 0.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.098, 10.100], loss: 0.008649, mae: 0.097846, mean_q: 18.507660
 76623/100000: episode: 1754, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 1.574, mean reward: 0.525 [0.517, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.131, 10.100], loss: 0.009132, mae: 0.104465, mean_q: 19.463934
 76625/100000: episode: 1755, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 1.031, mean reward: 0.516 [0.512, 0.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.565, 10.286], loss: 0.008660, mae: 0.099687, mean_q: 18.933151
 76627/100000: episode: 1756, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.805, mean reward: 0.402 [0.363, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.775, 10.351], loss: 0.006352, mae: 0.084475, mean_q: 19.213152
 76629/100000: episode: 1757, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 1.060, mean reward: 0.530 [0.503, 0.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.208, 10.100], loss: 0.008593, mae: 0.107665, mean_q: 17.679157
 76631/100000: episode: 1758, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 0.859, mean reward: 0.430 [0.417, 0.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.119, 10.100], loss: 0.008231, mae: 0.102638, mean_q: 17.928280
 76633/100000: episode: 1759, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 1.199, mean reward: 0.599 [0.574, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.100], loss: 0.012667, mae: 0.125474, mean_q: 18.090019
 76636/100000: episode: 1760, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 1.046, mean reward: 0.349 [0.318, 0.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.308, 10.100], loss: 0.007253, mae: 0.095848, mean_q: 18.665152
 76638/100000: episode: 1761, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.158, mean reward: 0.579 [0.537, 0.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.181], loss: 0.006115, mae: 0.086664, mean_q: 21.761660
 76641/100000: episode: 1762, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 1.219, mean reward: 0.406 [0.375, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.119, 10.100], loss: 0.008243, mae: 0.098389, mean_q: 20.128603
 76643/100000: episode: 1763, duration: 0.023s, episode steps: 2, steps per second: 85, episode reward: 1.053, mean reward: 0.527 [0.502, 0.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.292], loss: 0.011501, mae: 0.110089, mean_q: 18.041859
 76645/100000: episode: 1764, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 0.932, mean reward: 0.466 [0.440, 0.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.042, 10.100], loss: 0.008039, mae: 0.098157, mean_q: 18.457794
[Info] New level: 0.45827487111091614 | Considering 100/0 traces
 76647/100000: episode: 1765, duration: 3.965s, episode steps: 2, steps per second: 1, episode reward: 0.413, mean reward: 0.207 [0.162, 0.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.455, 10.100], loss: 0.008283, mae: 0.100465, mean_q: 20.517899
[Info] Not found new level, current best level reached = 0.45827487111091614
 76651/100000: episode: 1766, duration: 3.953s, episode steps: 4, steps per second: 1, episode reward: 0.872, mean reward: 0.218 [0.091, 0.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.942, 10.100], loss: 0.009310, mae: 0.104124, mean_q: 20.080568
 76751/100000: episode: 1767, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 56.178, mean reward: 0.562 [0.386, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.985, 10.098], loss: 0.009033, mae: 0.104326, mean_q: 19.402189
 76851/100000: episode: 1768, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.556, mean reward: 0.566 [0.386, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.476, 10.192], loss: 0.008549, mae: 0.101121, mean_q: 19.362329
 76951/100000: episode: 1769, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.583, mean reward: 0.576 [0.369, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.677, 10.259], loss: 0.007692, mae: 0.097447, mean_q: 19.343031
 77051/100000: episode: 1770, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 51.844, mean reward: 0.518 [0.267, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.436, 10.493], loss: 0.008387, mae: 0.102527, mean_q: 19.062101
 77151/100000: episode: 1771, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 49.912, mean reward: 0.499 [0.295, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.360, 10.098], loss: 0.009960, mae: 0.110382, mean_q: 19.333092
 77251/100000: episode: 1772, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 54.850, mean reward: 0.549 [0.302, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.370, 10.325], loss: 0.007933, mae: 0.097705, mean_q: 19.578089
 77351/100000: episode: 1773, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 54.275, mean reward: 0.543 [0.265, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.901, 10.215], loss: 0.007939, mae: 0.098198, mean_q: 19.250486
 77451/100000: episode: 1774, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 56.041, mean reward: 0.560 [0.332, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.338, 10.131], loss: 0.008939, mae: 0.103323, mean_q: 19.439859
 77551/100000: episode: 1775, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 55.411, mean reward: 0.554 [0.176, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.892, 10.175], loss: 0.008937, mae: 0.106613, mean_q: 19.277864
 77651/100000: episode: 1776, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 54.842, mean reward: 0.548 [0.350, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.973, 10.098], loss: 0.009117, mae: 0.105774, mean_q: 19.257507
 77751/100000: episode: 1777, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 52.147, mean reward: 0.521 [0.327, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.188, 10.098], loss: 0.008690, mae: 0.102735, mean_q: 19.286798
 77851/100000: episode: 1778, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 55.313, mean reward: 0.553 [0.348, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.657, 10.179], loss: 0.009506, mae: 0.108365, mean_q: 19.493525
 77951/100000: episode: 1779, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 45.309, mean reward: 0.453 [0.209, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.202, 10.109], loss: 0.009521, mae: 0.108400, mean_q: 19.271837
 78051/100000: episode: 1780, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 56.549, mean reward: 0.565 [0.316, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.667, 10.165], loss: 0.009730, mae: 0.109110, mean_q: 19.376322
 78151/100000: episode: 1781, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 50.536, mean reward: 0.505 [0.236, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.110, 10.098], loss: 0.008199, mae: 0.099575, mean_q: 19.071571
 78251/100000: episode: 1782, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 48.395, mean reward: 0.484 [0.194, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.722, 10.376], loss: 0.008089, mae: 0.099776, mean_q: 19.355621
 78351/100000: episode: 1783, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 56.976, mean reward: 0.570 [0.290, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.417, 10.152], loss: 0.008221, mae: 0.099459, mean_q: 19.612545
 78451/100000: episode: 1784, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 51.200, mean reward: 0.512 [0.285, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.729, 10.287], loss: 0.011317, mae: 0.115617, mean_q: 19.174194
 78551/100000: episode: 1785, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 51.133, mean reward: 0.511 [0.306, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.750, 10.345], loss: 0.008539, mae: 0.101236, mean_q: 19.349497
 78651/100000: episode: 1786, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 53.937, mean reward: 0.539 [0.307, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.401, 10.098], loss: 0.008729, mae: 0.101854, mean_q: 19.417843
 78751/100000: episode: 1787, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 53.965, mean reward: 0.540 [0.333, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.481, 10.175], loss: 0.008121, mae: 0.099291, mean_q: 19.436583
 78851/100000: episode: 1788, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.722, mean reward: 0.537 [0.335, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.318, 10.098], loss: 0.010534, mae: 0.113519, mean_q: 19.189415
 78951/100000: episode: 1789, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 53.297, mean reward: 0.533 [0.239, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.978, 10.098], loss: 0.008125, mae: 0.100078, mean_q: 19.469429
 79051/100000: episode: 1790, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 46.423, mean reward: 0.464 [0.120, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.843, 10.098], loss: 0.008785, mae: 0.102658, mean_q: 19.324709
 79151/100000: episode: 1791, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 55.694, mean reward: 0.557 [0.331, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.769, 10.098], loss: 0.008226, mae: 0.098798, mean_q: 19.189665
 79251/100000: episode: 1792, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 53.920, mean reward: 0.539 [0.253, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.518, 10.098], loss: 0.009339, mae: 0.106058, mean_q: 19.521664
 79351/100000: episode: 1793, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 50.375, mean reward: 0.504 [0.262, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.241, 10.098], loss: 0.008208, mae: 0.100821, mean_q: 19.049566
 79451/100000: episode: 1794, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 54.988, mean reward: 0.550 [0.306, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.183, 10.098], loss: 0.008607, mae: 0.102451, mean_q: 19.340954
 79551/100000: episode: 1795, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 54.974, mean reward: 0.550 [0.280, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.560, 10.173], loss: 0.009113, mae: 0.105151, mean_q: 19.352043
 79651/100000: episode: 1796, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 53.981, mean reward: 0.540 [0.285, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.705, 10.098], loss: 0.009705, mae: 0.109409, mean_q: 19.363358
 79751/100000: episode: 1797, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 54.064, mean reward: 0.541 [0.363, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.229, 10.098], loss: 0.008156, mae: 0.100265, mean_q: 19.024145
 79851/100000: episode: 1798, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 56.244, mean reward: 0.562 [0.312, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.542, 10.098], loss: 0.008674, mae: 0.103080, mean_q: 19.249708
 79951/100000: episode: 1799, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 51.650, mean reward: 0.516 [0.314, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.237, 10.178], loss: 0.009002, mae: 0.104288, mean_q: 19.378298
 80051/100000: episode: 1800, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 54.552, mean reward: 0.546 [0.389, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.612, 10.123], loss: 0.009466, mae: 0.107796, mean_q: 19.170588
 80151/100000: episode: 1801, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 46.260, mean reward: 0.463 [0.232, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.621, 10.098], loss: 0.009733, mae: 0.108302, mean_q: 19.161751
 80251/100000: episode: 1802, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 49.798, mean reward: 0.498 [0.247, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.817, 10.157], loss: 0.009863, mae: 0.109849, mean_q: 19.267830
 80351/100000: episode: 1803, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 54.793, mean reward: 0.548 [0.219, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.792, 10.098], loss: 0.009205, mae: 0.106301, mean_q: 19.248224
 80451/100000: episode: 1804, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 53.613, mean reward: 0.536 [0.333, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.800, 10.248], loss: 0.008227, mae: 0.100185, mean_q: 19.077900
 80551/100000: episode: 1805, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 55.992, mean reward: 0.560 [0.361, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.691, 10.098], loss: 0.009320, mae: 0.107494, mean_q: 19.120522
 80651/100000: episode: 1806, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 53.520, mean reward: 0.535 [0.259, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.467, 10.466], loss: 0.009020, mae: 0.104834, mean_q: 19.363964
 80751/100000: episode: 1807, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 47.796, mean reward: 0.478 [0.287, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.366, 10.257], loss: 0.008615, mae: 0.101692, mean_q: 19.176277
 80851/100000: episode: 1808, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 56.199, mean reward: 0.562 [0.338, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.175, 10.098], loss: 0.007834, mae: 0.096407, mean_q: 19.238014
 80951/100000: episode: 1809, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 54.049, mean reward: 0.540 [0.315, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.900, 10.148], loss: 0.009716, mae: 0.106867, mean_q: 19.256323
 81051/100000: episode: 1810, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 54.617, mean reward: 0.546 [0.323, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.350, 10.184], loss: 0.008002, mae: 0.098416, mean_q: 18.992609
 81151/100000: episode: 1811, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 55.410, mean reward: 0.554 [0.374, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.545, 10.284], loss: 0.009065, mae: 0.103514, mean_q: 19.239159
 81251/100000: episode: 1812, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 54.248, mean reward: 0.542 [0.323, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.331, 10.098], loss: 0.007209, mae: 0.093476, mean_q: 19.391312
 81351/100000: episode: 1813, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 54.152, mean reward: 0.542 [0.246, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.080, 10.098], loss: 0.007713, mae: 0.095199, mean_q: 19.540504
 81451/100000: episode: 1814, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 55.852, mean reward: 0.559 [0.271, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.285, 10.202], loss: 0.008885, mae: 0.104752, mean_q: 19.129301
 81551/100000: episode: 1815, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 49.602, mean reward: 0.496 [0.223, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.249, 10.098], loss: 0.009316, mae: 0.105937, mean_q: 19.503899
 81651/100000: episode: 1816, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 45.623, mean reward: 0.456 [0.246, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.729, 10.155], loss: 0.010296, mae: 0.109964, mean_q: 19.032104
 81751/100000: episode: 1817, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 51.859, mean reward: 0.519 [0.127, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.030, 10.310], loss: 0.008937, mae: 0.103095, mean_q: 19.421240
 81851/100000: episode: 1818, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 51.007, mean reward: 0.510 [0.232, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.566, 10.098], loss: 0.008790, mae: 0.102183, mean_q: 19.490931
 81951/100000: episode: 1819, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 52.540, mean reward: 0.525 [0.278, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.313, 10.098], loss: 0.008899, mae: 0.102010, mean_q: 19.383268
 82051/100000: episode: 1820, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 55.346, mean reward: 0.553 [0.369, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.201, 10.098], loss: 0.009694, mae: 0.107657, mean_q: 19.376898
 82151/100000: episode: 1821, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 52.902, mean reward: 0.529 [0.346, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.388, 10.098], loss: 0.008399, mae: 0.100064, mean_q: 19.202555
 82251/100000: episode: 1822, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.214, mean reward: 0.542 [0.271, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.241, 10.411], loss: 0.008302, mae: 0.099974, mean_q: 19.297173
 82351/100000: episode: 1823, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 53.107, mean reward: 0.531 [0.252, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.211, 10.314], loss: 0.008321, mae: 0.099957, mean_q: 19.636639
 82451/100000: episode: 1824, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 49.729, mean reward: 0.497 [0.268, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.378, 10.098], loss: 0.008418, mae: 0.102248, mean_q: 19.662605
 82551/100000: episode: 1825, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 51.024, mean reward: 0.510 [0.232, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.317, 10.215], loss: 0.008297, mae: 0.099936, mean_q: 19.506277
 82651/100000: episode: 1826, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 55.160, mean reward: 0.552 [0.321, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.649, 10.223], loss: 0.008942, mae: 0.103173, mean_q: 19.187492
 82751/100000: episode: 1827, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 54.801, mean reward: 0.548 [0.233, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.521, 10.195], loss: 0.009280, mae: 0.107136, mean_q: 19.389881
 82851/100000: episode: 1828, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 53.789, mean reward: 0.538 [0.340, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.021, 10.204], loss: 0.008063, mae: 0.097918, mean_q: 19.350975
 82951/100000: episode: 1829, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 49.385, mean reward: 0.494 [0.260, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.755, 10.150], loss: 0.008420, mae: 0.101286, mean_q: 19.525999
 83051/100000: episode: 1830, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 54.155, mean reward: 0.542 [0.314, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.873, 10.098], loss: 0.007960, mae: 0.096918, mean_q: 19.288675
 83151/100000: episode: 1831, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 52.736, mean reward: 0.527 [0.315, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.186, 10.217], loss: 0.008669, mae: 0.102492, mean_q: 19.237215
 83251/100000: episode: 1832, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 50.629, mean reward: 0.506 [0.195, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.979, 10.366], loss: 0.007879, mae: 0.097483, mean_q: 19.391954
 83351/100000: episode: 1833, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 51.675, mean reward: 0.517 [0.261, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.275, 10.339], loss: 0.007322, mae: 0.095346, mean_q: 19.576708
 83451/100000: episode: 1834, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 50.493, mean reward: 0.505 [0.044, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.954, 10.359], loss: 0.008972, mae: 0.104229, mean_q: 19.172565
 83551/100000: episode: 1835, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 51.032, mean reward: 0.510 [0.374, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.172, 10.136], loss: 0.009164, mae: 0.105780, mean_q: 19.235615
 83651/100000: episode: 1836, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 50.294, mean reward: 0.503 [0.279, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.589, 10.438], loss: 0.007792, mae: 0.098079, mean_q: 19.316120
 83751/100000: episode: 1837, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 53.074, mean reward: 0.531 [0.311, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.486, 10.098], loss: 0.008572, mae: 0.102126, mean_q: 19.443459
 83851/100000: episode: 1838, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 46.706, mean reward: 0.467 [0.217, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.149, 10.098], loss: 0.009539, mae: 0.107101, mean_q: 18.759361
 83951/100000: episode: 1839, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 53.286, mean reward: 0.533 [0.261, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.340, 10.098], loss: 0.010308, mae: 0.111725, mean_q: 19.334648
 84051/100000: episode: 1840, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 54.916, mean reward: 0.549 [0.312, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.002, 10.098], loss: 0.008031, mae: 0.098265, mean_q: 19.583906
 84151/100000: episode: 1841, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 44.881, mean reward: 0.449 [0.164, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.291, 10.098], loss: 0.009942, mae: 0.110273, mean_q: 19.372726
 84251/100000: episode: 1842, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 51.648, mean reward: 0.516 [0.003, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.449, 10.098], loss: 0.008895, mae: 0.102852, mean_q: 19.154446
 84351/100000: episode: 1843, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 48.237, mean reward: 0.482 [0.262, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.865, 10.098], loss: 0.008483, mae: 0.102128, mean_q: 19.312464
 84451/100000: episode: 1844, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 49.894, mean reward: 0.499 [0.271, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.589, 10.098], loss: 0.008034, mae: 0.099455, mean_q: 19.346085
 84551/100000: episode: 1845, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 55.843, mean reward: 0.558 [0.155, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.421, 10.100], loss: 0.009317, mae: 0.105969, mean_q: 19.229511
 84651/100000: episode: 1846, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 55.526, mean reward: 0.555 [0.266, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.334, 10.098], loss: 0.007771, mae: 0.098110, mean_q: 19.490379
 84751/100000: episode: 1847, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 52.103, mean reward: 0.521 [0.350, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.283, 10.148], loss: 0.008678, mae: 0.102815, mean_q: 19.366192
 84851/100000: episode: 1848, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 51.902, mean reward: 0.519 [0.226, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.339, 10.098], loss: 0.008432, mae: 0.101226, mean_q: 19.273479
 84951/100000: episode: 1849, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 39.402, mean reward: 0.394 [0.171, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.186, 10.470], loss: 0.008712, mae: 0.102618, mean_q: 19.587387
 85051/100000: episode: 1850, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 56.625, mean reward: 0.566 [0.386, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.194, 10.098], loss: 0.008693, mae: 0.104043, mean_q: 19.691103
 85151/100000: episode: 1851, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 52.103, mean reward: 0.521 [0.222, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.662, 10.315], loss: 0.009088, mae: 0.106117, mean_q: 18.870066
 85251/100000: episode: 1852, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 55.578, mean reward: 0.556 [0.224, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.689, 10.148], loss: 0.009136, mae: 0.105951, mean_q: 19.431902
 85351/100000: episode: 1853, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 53.311, mean reward: 0.533 [0.133, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.920, 10.098], loss: 0.008970, mae: 0.103685, mean_q: 19.348701
 85451/100000: episode: 1854, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 48.930, mean reward: 0.489 [0.221, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.344, 10.098], loss: 0.007847, mae: 0.098126, mean_q: 19.367090
 85551/100000: episode: 1855, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 55.541, mean reward: 0.555 [0.365, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.486, 10.098], loss: 0.008181, mae: 0.100085, mean_q: 19.158058
 85651/100000: episode: 1856, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 56.162, mean reward: 0.562 [0.347, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.664, 10.171], loss: 0.008694, mae: 0.102760, mean_q: 19.217136
 85751/100000: episode: 1857, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 49.346, mean reward: 0.493 [0.274, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.538, 10.301], loss: 0.008292, mae: 0.101104, mean_q: 19.313330
 85851/100000: episode: 1858, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 53.746, mean reward: 0.537 [0.369, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.825, 10.098], loss: 0.008002, mae: 0.099554, mean_q: 19.215906
 85951/100000: episode: 1859, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 52.146, mean reward: 0.521 [0.095, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.142, 10.431], loss: 0.007608, mae: 0.095908, mean_q: 19.504566
 86051/100000: episode: 1860, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 54.879, mean reward: 0.549 [0.237, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.516, 10.098], loss: 0.007557, mae: 0.095596, mean_q: 19.492640
 86151/100000: episode: 1861, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 55.370, mean reward: 0.554 [0.339, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.982, 10.098], loss: 0.008243, mae: 0.101363, mean_q: 19.304869
 86251/100000: episode: 1862, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.517, mean reward: 0.575 [0.344, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.775, 10.114], loss: 0.009386, mae: 0.107051, mean_q: 19.415915
 86351/100000: episode: 1863, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 48.263, mean reward: 0.483 [0.174, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.061, 10.098], loss: 0.009271, mae: 0.106863, mean_q: 19.475866
 86451/100000: episode: 1864, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 51.294, mean reward: 0.513 [0.271, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.280, 10.114], loss: 0.008992, mae: 0.104903, mean_q: 18.878166
 86551/100000: episode: 1865, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 56.233, mean reward: 0.562 [0.403, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.024, 10.098], loss: 0.008786, mae: 0.102852, mean_q: 19.400906
[Info] New level: 0.47069287300109863 | Considering 53/47 traces
 86651/100000: episode: 1866, duration: 4.443s, episode steps: 100, steps per second: 23, episode reward: 53.897, mean reward: 0.539 [0.248, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.080, 10.098], loss: 0.008807, mae: 0.103292, mean_q: 19.391491
 86653/100000: episode: 1867, duration: 0.015s, episode steps: 2, steps per second: 134, episode reward: 1.355, mean reward: 0.677 [0.665, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.048, 10.117], loss: 0.008198, mae: 0.098077, mean_q: 21.252499
 86655/100000: episode: 1868, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 1.019, mean reward: 0.509 [0.504, 0.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.081, 10.100], loss: 0.009936, mae: 0.115310, mean_q: 17.961868
 86657/100000: episode: 1869, duration: 0.016s, episode steps: 2, steps per second: 129, episode reward: 1.231, mean reward: 0.616 [0.593, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.100], loss: 0.008951, mae: 0.101134, mean_q: 19.046745
 86659/100000: episode: 1870, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.740, mean reward: 0.370 [0.367, 0.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.844, 10.100], loss: 0.007202, mae: 0.094367, mean_q: 20.150646
 86661/100000: episode: 1871, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.139, mean reward: 0.570 [0.521, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.058, 10.101], loss: 0.010931, mae: 0.114745, mean_q: 19.231993
 86663/100000: episode: 1872, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.192, mean reward: 0.596 [0.574, 0.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.041, 10.100], loss: 0.010825, mae: 0.112687, mean_q: 17.451935
 86665/100000: episode: 1873, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 1.164, mean reward: 0.582 [0.563, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.245, 10.100], loss: 0.010746, mae: 0.112532, mean_q: 19.758274
 86667/100000: episode: 1874, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.210, mean reward: 0.605 [0.584, 0.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.046, 10.100], loss: 0.009798, mae: 0.110646, mean_q: 18.704227
 86669/100000: episode: 1875, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 1.192, mean reward: 0.596 [0.587, 0.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.093, 10.100], loss: 0.018282, mae: 0.147820, mean_q: 17.895908
 86671/100000: episode: 1876, duration: 0.018s, episode steps: 2, steps per second: 113, episode reward: 1.062, mean reward: 0.531 [0.523, 0.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.137, 10.100], loss: 0.010803, mae: 0.109711, mean_q: 20.566792
 86673/100000: episode: 1877, duration: 0.016s, episode steps: 2, steps per second: 126, episode reward: 1.208, mean reward: 0.604 [0.598, 0.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.102], loss: 0.011387, mae: 0.121262, mean_q: 20.552004
 86675/100000: episode: 1878, duration: 0.015s, episode steps: 2, steps per second: 138, episode reward: 1.108, mean reward: 0.554 [0.545, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.068, 10.100], loss: 0.009410, mae: 0.099399, mean_q: 19.693203
 86677/100000: episode: 1879, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 1.174, mean reward: 0.587 [0.549, 0.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.124, 10.100], loss: 0.013175, mae: 0.132990, mean_q: 18.376837
 86679/100000: episode: 1880, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 1.203, mean reward: 0.602 [0.568, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.100], loss: 0.014351, mae: 0.131197, mean_q: 20.130190
 86681/100000: episode: 1881, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 0.838, mean reward: 0.419 [0.354, 0.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.065, 10.100], loss: 0.008664, mae: 0.108973, mean_q: 21.133184
 86683/100000: episode: 1882, duration: 0.015s, episode steps: 2, steps per second: 137, episode reward: 1.121, mean reward: 0.560 [0.558, 0.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.051, 10.100], loss: 0.008511, mae: 0.100789, mean_q: 18.820679
 86685/100000: episode: 1883, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 1.088, mean reward: 0.544 [0.517, 0.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.223], loss: 0.010986, mae: 0.118398, mean_q: 19.465580
 86687/100000: episode: 1884, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 0.962, mean reward: 0.481 [0.449, 0.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.234], loss: 0.008098, mae: 0.098152, mean_q: 20.773182
 86689/100000: episode: 1885, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 1.091, mean reward: 0.545 [0.490, 0.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.471, 10.100], loss: 0.008761, mae: 0.096866, mean_q: 17.377542
 86691/100000: episode: 1886, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 0.775, mean reward: 0.387 [0.343, 0.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.100], loss: 0.011174, mae: 0.118354, mean_q: 20.163534
 86693/100000: episode: 1887, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.074, mean reward: 0.537 [0.527, 0.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.277, 10.100], loss: 0.011937, mae: 0.127730, mean_q: 21.048298
 86695/100000: episode: 1888, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 1.034, mean reward: 0.517 [0.510, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.100], loss: 0.008772, mae: 0.103104, mean_q: 19.027607
 86698/100000: episode: 1889, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 1.554, mean reward: 0.518 [0.504, 0.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.241, 10.100], loss: 0.015629, mae: 0.139986, mean_q: 19.618549
 86700/100000: episode: 1890, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.245, mean reward: 0.622 [0.596, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.055, 10.100], loss: 0.007949, mae: 0.097177, mean_q: 18.994514
 86702/100000: episode: 1891, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 1.219, mean reward: 0.609 [0.582, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.355, 10.100], loss: 0.021791, mae: 0.160496, mean_q: 18.043671
 86704/100000: episode: 1892, duration: 0.018s, episode steps: 2, steps per second: 111, episode reward: 1.147, mean reward: 0.573 [0.479, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.140], loss: 0.013392, mae: 0.117377, mean_q: 18.408924
 86706/100000: episode: 1893, duration: 0.014s, episode steps: 2, steps per second: 139, episode reward: 1.042, mean reward: 0.521 [0.483, 0.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.271, 10.133], loss: 0.012243, mae: 0.121350, mean_q: 18.198774
 86709/100000: episode: 1894, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: 1.185, mean reward: 0.395 [0.353, 0.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.244, 10.100], loss: 0.011694, mae: 0.105316, mean_q: 18.704748
 86711/100000: episode: 1895, duration: 0.020s, episode steps: 2, steps per second: 102, episode reward: 1.068, mean reward: 0.534 [0.527, 0.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.266], loss: 0.013146, mae: 0.117122, mean_q: 18.747530
 86713/100000: episode: 1896, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.024, mean reward: 0.512 [0.506, 0.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.342 [-0.035, 10.292], loss: 0.006434, mae: 0.085888, mean_q: 21.728521
 86715/100000: episode: 1897, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.214, mean reward: 0.607 [0.590, 0.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.100], loss: 0.009126, mae: 0.103728, mean_q: 19.575239
 86718/100000: episode: 1898, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 1.282, mean reward: 0.427 [0.398, 0.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.319, 10.100], loss: 0.009890, mae: 0.113112, mean_q: 18.888391
 86720/100000: episode: 1899, duration: 0.017s, episode steps: 2, steps per second: 119, episode reward: 1.294, mean reward: 0.647 [0.597, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.122], loss: 0.012514, mae: 0.126823, mean_q: 17.657839
 86722/100000: episode: 1900, duration: 0.013s, episode steps: 2, steps per second: 152, episode reward: 1.160, mean reward: 0.580 [0.576, 0.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.171], loss: 0.017077, mae: 0.152397, mean_q: 18.082067
 86724/100000: episode: 1901, duration: 0.015s, episode steps: 2, steps per second: 131, episode reward: 0.764, mean reward: 0.382 [0.365, 0.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.085, 10.100], loss: 0.016342, mae: 0.154541, mean_q: 17.903484
 86726/100000: episode: 1902, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 1.267, mean reward: 0.634 [0.626, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.206, 10.136], loss: 0.009405, mae: 0.104865, mean_q: 19.030951
 86728/100000: episode: 1903, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 1.235, mean reward: 0.617 [0.580, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.887, 10.100], loss: 0.010318, mae: 0.120949, mean_q: 17.201454
 86730/100000: episode: 1904, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 0.988, mean reward: 0.494 [0.464, 0.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.130, 10.100], loss: 0.013256, mae: 0.133938, mean_q: 19.388023
 86732/100000: episode: 1905, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: 1.082, mean reward: 0.541 [0.492, 0.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.220], loss: 0.013765, mae: 0.130042, mean_q: 15.955112
 86734/100000: episode: 1906, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 1.143, mean reward: 0.571 [0.551, 0.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.183], loss: 0.007609, mae: 0.097891, mean_q: 17.122837
 86736/100000: episode: 1907, duration: 0.021s, episode steps: 2, steps per second: 97, episode reward: 1.205, mean reward: 0.602 [0.578, 0.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.094, 10.145], loss: 0.009451, mae: 0.107553, mean_q: 18.226637
 86738/100000: episode: 1908, duration: 0.021s, episode steps: 2, steps per second: 94, episode reward: 1.144, mean reward: 0.572 [0.500, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.147], loss: 0.011528, mae: 0.120338, mean_q: 19.318949
 86740/100000: episode: 1909, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 0.996, mean reward: 0.498 [0.385, 0.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.100], loss: 0.007881, mae: 0.101450, mean_q: 16.749887
 86743/100000: episode: 1910, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 1.620, mean reward: 0.540 [0.529, 0.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.235, 10.100], loss: 0.010262, mae: 0.115475, mean_q: 19.130756
 86745/100000: episode: 1911, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 1.230, mean reward: 0.615 [0.600, 0.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.050, 10.115], loss: 0.007659, mae: 0.093803, mean_q: 16.540321
 86748/100000: episode: 1912, duration: 0.024s, episode steps: 3, steps per second: 122, episode reward: 1.207, mean reward: 0.402 [0.370, 0.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.244, 10.100], loss: 0.010086, mae: 0.115305, mean_q: 17.558043
[Info] Not found new level, current best level reached = 0.47069287300109863
 86750/100000: episode: 1913, duration: 3.957s, episode steps: 2, steps per second: 1, episode reward: 1.154, mean reward: 0.577 [0.558, 0.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.202, 10.100], loss: 0.010050, mae: 0.102630, mean_q: 18.971115
 86850/100000: episode: 1914, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 54.623, mean reward: 0.546 [0.321, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.212], loss: 0.009929, mae: 0.109409, mean_q: 18.545036
 86950/100000: episode: 1915, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.235, mean reward: 0.572 [0.335, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.694, 10.139], loss: 0.009307, mae: 0.105937, mean_q: 18.764807
 87050/100000: episode: 1916, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 51.475, mean reward: 0.515 [0.091, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.796, 10.212], loss: 0.009967, mae: 0.110767, mean_q: 18.815037
 87150/100000: episode: 1917, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 50.598, mean reward: 0.506 [0.256, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.751, 10.098], loss: 0.011009, mae: 0.114868, mean_q: 18.779774
 87250/100000: episode: 1918, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 53.918, mean reward: 0.539 [0.225, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.204, 10.240], loss: 0.011232, mae: 0.117028, mean_q: 19.037424
 87350/100000: episode: 1919, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 50.256, mean reward: 0.503 [0.262, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.784, 10.098], loss: 0.010777, mae: 0.113952, mean_q: 18.890537
 87450/100000: episode: 1920, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 56.977, mean reward: 0.570 [0.271, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.078, 10.189], loss: 0.010182, mae: 0.108591, mean_q: 18.951599
 87550/100000: episode: 1921, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 54.182, mean reward: 0.542 [0.302, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.370, 10.098], loss: 0.011014, mae: 0.113419, mean_q: 19.113035
 87650/100000: episode: 1922, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 49.284, mean reward: 0.493 [0.204, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.867, 10.301], loss: 0.010047, mae: 0.109964, mean_q: 18.855225
 87750/100000: episode: 1923, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 53.965, mean reward: 0.540 [0.333, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.253, 10.098], loss: 0.009139, mae: 0.101776, mean_q: 18.551947
 87850/100000: episode: 1924, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 55.498, mean reward: 0.555 [0.341, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.928, 10.098], loss: 0.010010, mae: 0.108135, mean_q: 18.777504
 87950/100000: episode: 1925, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 51.595, mean reward: 0.516 [0.296, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.993, 10.098], loss: 0.009377, mae: 0.104965, mean_q: 18.807133
 88050/100000: episode: 1926, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 56.645, mean reward: 0.566 [0.348, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.217, 10.252], loss: 0.010402, mae: 0.110267, mean_q: 18.957653
 88150/100000: episode: 1927, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 52.757, mean reward: 0.528 [0.310, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.860, 10.098], loss: 0.010485, mae: 0.110751, mean_q: 18.811272
 88250/100000: episode: 1928, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 53.370, mean reward: 0.534 [0.327, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.542, 10.351], loss: 0.011831, mae: 0.118512, mean_q: 18.950762
 88350/100000: episode: 1929, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 54.154, mean reward: 0.542 [0.245, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.076, 10.098], loss: 0.013150, mae: 0.127511, mean_q: 18.888721
 88450/100000: episode: 1930, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 50.792, mean reward: 0.508 [0.226, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.478, 10.275], loss: 0.010127, mae: 0.107550, mean_q: 19.290716
 88550/100000: episode: 1931, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 54.059, mean reward: 0.541 [0.252, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.138, 10.220], loss: 0.011386, mae: 0.115285, mean_q: 18.610529
 88650/100000: episode: 1932, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 54.275, mean reward: 0.543 [0.285, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.053, 10.325], loss: 0.009905, mae: 0.107312, mean_q: 19.023855
 88750/100000: episode: 1933, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 55.048, mean reward: 0.550 [0.275, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.088, 10.187], loss: 0.010447, mae: 0.110632, mean_q: 18.945292
 88850/100000: episode: 1934, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 52.221, mean reward: 0.522 [0.267, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.466, 10.388], loss: 0.009467, mae: 0.104175, mean_q: 19.341181
 88950/100000: episode: 1935, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 54.633, mean reward: 0.546 [0.238, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.961, 10.174], loss: 0.011578, mae: 0.114773, mean_q: 18.947847
 89050/100000: episode: 1936, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 57.224, mean reward: 0.572 [0.330, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.255, 10.098], loss: 0.009927, mae: 0.106125, mean_q: 18.735584
 89150/100000: episode: 1937, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 48.182, mean reward: 0.482 [0.230, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.212, 10.499], loss: 0.009913, mae: 0.106612, mean_q: 19.101490
 89250/100000: episode: 1938, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 55.615, mean reward: 0.556 [0.380, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.107, 10.098], loss: 0.010422, mae: 0.108348, mean_q: 19.002636
 89350/100000: episode: 1939, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 50.995, mean reward: 0.510 [0.308, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.107, 10.347], loss: 0.009343, mae: 0.102494, mean_q: 18.635918
 89450/100000: episode: 1940, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 51.069, mean reward: 0.511 [0.223, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.032, 10.206], loss: 0.010749, mae: 0.112649, mean_q: 19.064564
 89550/100000: episode: 1941, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 52.051, mean reward: 0.521 [0.317, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.649, 10.424], loss: 0.009443, mae: 0.105112, mean_q: 18.896324
 89650/100000: episode: 1942, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 54.614, mean reward: 0.546 [0.336, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.037, 10.162], loss: 0.010324, mae: 0.107073, mean_q: 18.652727
 89750/100000: episode: 1943, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 52.316, mean reward: 0.523 [0.353, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.729, 10.098], loss: 0.009646, mae: 0.107456, mean_q: 19.098618
 89850/100000: episode: 1944, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 55.469, mean reward: 0.555 [0.172, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.681, 10.124], loss: 0.011123, mae: 0.112863, mean_q: 19.104887
 89950/100000: episode: 1945, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 54.316, mean reward: 0.543 [0.346, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.851, 10.326], loss: 0.009432, mae: 0.103590, mean_q: 19.142332
 90050/100000: episode: 1946, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 55.901, mean reward: 0.559 [0.369, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.863, 10.218], loss: 0.009663, mae: 0.103947, mean_q: 19.218044
 90150/100000: episode: 1947, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 54.427, mean reward: 0.544 [0.320, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.256, 10.336], loss: 0.010335, mae: 0.105544, mean_q: 18.902176
 90250/100000: episode: 1948, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 54.422, mean reward: 0.544 [0.241, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.389, 10.098], loss: 0.009769, mae: 0.104307, mean_q: 19.141949
 90350/100000: episode: 1949, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 55.298, mean reward: 0.553 [0.273, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.248, 10.173], loss: 0.010643, mae: 0.110865, mean_q: 19.265839
 90450/100000: episode: 1950, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 50.163, mean reward: 0.502 [0.195, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.936, 10.098], loss: 0.011257, mae: 0.114061, mean_q: 19.065086
 90550/100000: episode: 1951, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 53.617, mean reward: 0.536 [0.239, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.420, 10.098], loss: 0.010229, mae: 0.109544, mean_q: 19.166010
 90650/100000: episode: 1952, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 51.001, mean reward: 0.510 [0.309, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.638, 10.098], loss: 0.010482, mae: 0.107485, mean_q: 19.162298
 90750/100000: episode: 1953, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 55.762, mean reward: 0.558 [0.264, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.099, 10.107], loss: 0.010638, mae: 0.110028, mean_q: 18.994162
 90850/100000: episode: 1954, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 53.550, mean reward: 0.536 [0.314, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.654, 10.098], loss: 0.008256, mae: 0.097446, mean_q: 18.912218
 90950/100000: episode: 1955, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 53.713, mean reward: 0.537 [0.303, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.421, 10.140], loss: 0.009288, mae: 0.103095, mean_q: 19.200642
 91050/100000: episode: 1956, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.548, mean reward: 0.575 [0.372, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.647, 10.293], loss: 0.010181, mae: 0.107689, mean_q: 19.231140
 91150/100000: episode: 1957, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 46.668, mean reward: 0.467 [0.219, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.005, 10.401], loss: 0.009380, mae: 0.104011, mean_q: 19.222532
 91250/100000: episode: 1958, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 55.242, mean reward: 0.552 [0.287, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.705, 10.197], loss: 0.010436, mae: 0.110008, mean_q: 19.156126
 91350/100000: episode: 1959, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 54.620, mean reward: 0.546 [0.341, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.798, 10.304], loss: 0.008876, mae: 0.101832, mean_q: 19.064888
 91450/100000: episode: 1960, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 52.636, mean reward: 0.526 [0.368, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.070, 10.230], loss: 0.009330, mae: 0.104025, mean_q: 19.372330
 91550/100000: episode: 1961, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 48.403, mean reward: 0.484 [0.294, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.240, 10.098], loss: 0.009544, mae: 0.105714, mean_q: 19.053522
 91650/100000: episode: 1962, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 52.002, mean reward: 0.520 [0.293, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.399, 10.098], loss: 0.009825, mae: 0.107522, mean_q: 19.400286
 91750/100000: episode: 1963, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 51.738, mean reward: 0.517 [0.342, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.846, 10.277], loss: 0.009180, mae: 0.103401, mean_q: 19.473797
 91850/100000: episode: 1964, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 54.626, mean reward: 0.546 [0.294, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.667, 10.153], loss: 0.008765, mae: 0.102146, mean_q: 19.530298
 91950/100000: episode: 1965, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 51.659, mean reward: 0.517 [0.312, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.802, 10.098], loss: 0.008649, mae: 0.101617, mean_q: 19.501350
 92050/100000: episode: 1966, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 50.692, mean reward: 0.507 [0.255, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.929, 10.162], loss: 0.009033, mae: 0.105431, mean_q: 19.928476
 92150/100000: episode: 1967, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 52.951, mean reward: 0.530 [0.286, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.556, 10.098], loss: 0.008099, mae: 0.100060, mean_q: 19.441547
 92250/100000: episode: 1968, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 55.638, mean reward: 0.556 [0.274, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.218, 10.122], loss: 0.010856, mae: 0.113869, mean_q: 19.226509
 92350/100000: episode: 1969, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 50.451, mean reward: 0.505 [0.249, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.584, 10.098], loss: 0.010694, mae: 0.115184, mean_q: 19.318495
 92450/100000: episode: 1970, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 51.509, mean reward: 0.515 [0.162, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.172, 10.404], loss: 0.008248, mae: 0.099114, mean_q: 19.472170
 92550/100000: episode: 1971, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 52.345, mean reward: 0.523 [0.275, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.181, 10.098], loss: 0.009047, mae: 0.104930, mean_q: 19.655085
 92650/100000: episode: 1972, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 48.237, mean reward: 0.482 [0.267, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.189, 10.403], loss: 0.010213, mae: 0.111322, mean_q: 19.509026
 92750/100000: episode: 1973, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 56.800, mean reward: 0.568 [0.336, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.447, 10.098], loss: 0.008809, mae: 0.104245, mean_q: 19.669544
 92850/100000: episode: 1974, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 55.894, mean reward: 0.559 [0.297, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.464, 10.202], loss: 0.009315, mae: 0.105829, mean_q: 19.647354
 92950/100000: episode: 1975, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 55.169, mean reward: 0.552 [0.354, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.526, 10.199], loss: 0.007760, mae: 0.097837, mean_q: 19.619133
 93050/100000: episode: 1976, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.148, mean reward: 0.561 [0.355, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.276, 10.098], loss: 0.008252, mae: 0.101040, mean_q: 19.774633
 93150/100000: episode: 1977, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 56.654, mean reward: 0.567 [0.298, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.848, 10.188], loss: 0.007774, mae: 0.095599, mean_q: 19.679001
 93250/100000: episode: 1978, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 54.050, mean reward: 0.540 [0.322, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.863, 10.098], loss: 0.009203, mae: 0.106059, mean_q: 19.635180
 93350/100000: episode: 1979, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 52.703, mean reward: 0.527 [0.307, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.911, 10.098], loss: 0.008487, mae: 0.103544, mean_q: 19.350349
 93450/100000: episode: 1980, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 55.733, mean reward: 0.557 [0.249, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.606, 10.108], loss: 0.008195, mae: 0.100738, mean_q: 19.329134
 93550/100000: episode: 1981, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 55.167, mean reward: 0.552 [0.259, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.727, 10.212], loss: 0.007198, mae: 0.094176, mean_q: 19.565853
 93650/100000: episode: 1982, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 51.638, mean reward: 0.516 [0.019, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.612, 10.098], loss: 0.008297, mae: 0.101471, mean_q: 19.571716
 93750/100000: episode: 1983, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 54.368, mean reward: 0.544 [0.251, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.188, 10.168], loss: 0.007104, mae: 0.094362, mean_q: 19.852568
 93850/100000: episode: 1984, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 54.766, mean reward: 0.548 [0.337, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.500, 10.098], loss: 0.008107, mae: 0.100107, mean_q: 19.596899
 93950/100000: episode: 1985, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 53.724, mean reward: 0.537 [0.340, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.153, 10.116], loss: 0.008277, mae: 0.101848, mean_q: 19.769524
 94050/100000: episode: 1986, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 47.657, mean reward: 0.477 [0.171, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.434, 10.498], loss: 0.008941, mae: 0.104789, mean_q: 19.840391
 94150/100000: episode: 1987, duration: 0.474s, episode steps: 100, steps per second: 211, episode reward: 56.871, mean reward: 0.569 [0.365, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.227, 10.112], loss: 0.007348, mae: 0.094402, mean_q: 19.531603
 94250/100000: episode: 1988, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 54.519, mean reward: 0.545 [0.315, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.677, 10.098], loss: 0.008784, mae: 0.104525, mean_q: 19.744434
 94350/100000: episode: 1989, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 56.666, mean reward: 0.567 [0.319, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.277, 10.285], loss: 0.008833, mae: 0.104660, mean_q: 19.561396
 94450/100000: episode: 1990, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 57.264, mean reward: 0.573 [0.359, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.996, 10.188], loss: 0.007066, mae: 0.093918, mean_q: 19.777559
 94550/100000: episode: 1991, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 52.049, mean reward: 0.520 [0.303, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.206, 10.336], loss: 0.008231, mae: 0.101484, mean_q: 19.754210
 94650/100000: episode: 1992, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 55.925, mean reward: 0.559 [0.332, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.012, 10.098], loss: 0.010365, mae: 0.112129, mean_q: 19.493681
 94750/100000: episode: 1993, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 55.328, mean reward: 0.553 [0.320, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.655, 10.204], loss: 0.008566, mae: 0.100775, mean_q: 19.697380
 94850/100000: episode: 1994, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 52.972, mean reward: 0.530 [0.268, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.419, 10.158], loss: 0.008145, mae: 0.098916, mean_q: 19.925180
 94950/100000: episode: 1995, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 54.741, mean reward: 0.547 [0.363, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.381, 10.140], loss: 0.009203, mae: 0.106287, mean_q: 19.670753
 95050/100000: episode: 1996, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 52.933, mean reward: 0.529 [0.316, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.687, 10.098], loss: 0.007941, mae: 0.098333, mean_q: 19.875420
 95150/100000: episode: 1997, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 54.756, mean reward: 0.548 [0.343, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.961, 10.372], loss: 0.007955, mae: 0.099374, mean_q: 19.636690
 95250/100000: episode: 1998, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 55.054, mean reward: 0.551 [0.311, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.670, 10.102], loss: 0.009513, mae: 0.108649, mean_q: 19.665577
 95350/100000: episode: 1999, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 53.012, mean reward: 0.530 [0.249, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.426, 10.120], loss: 0.010714, mae: 0.115770, mean_q: 19.652847
 95450/100000: episode: 2000, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 55.150, mean reward: 0.552 [0.311, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.860, 10.098], loss: 0.007848, mae: 0.099390, mean_q: 19.830151
 95550/100000: episode: 2001, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 55.148, mean reward: 0.551 [0.347, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.710, 10.207], loss: 0.007536, mae: 0.095563, mean_q: 19.872875
 95650/100000: episode: 2002, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 51.970, mean reward: 0.520 [0.306, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.763, 10.310], loss: 0.007209, mae: 0.093063, mean_q: 19.919237
 95750/100000: episode: 2003, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 47.713, mean reward: 0.477 [0.204, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.518, 10.373], loss: 0.007554, mae: 0.096892, mean_q: 19.749632
 95850/100000: episode: 2004, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 51.067, mean reward: 0.511 [0.248, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.542, 10.098], loss: 0.007592, mae: 0.097267, mean_q: 19.914238
 95950/100000: episode: 2005, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 56.985, mean reward: 0.570 [0.168, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.408, 10.218], loss: 0.008196, mae: 0.099193, mean_q: 19.449238
 96050/100000: episode: 2006, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 52.706, mean reward: 0.527 [0.333, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.608, 10.340], loss: 0.008626, mae: 0.102714, mean_q: 19.781195
 96150/100000: episode: 2007, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 52.595, mean reward: 0.526 [0.321, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.315, 10.193], loss: 0.007938, mae: 0.098864, mean_q: 19.236252
 96250/100000: episode: 2008, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.808, mean reward: 0.568 [0.251, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.339, 10.119], loss: 0.007897, mae: 0.097635, mean_q: 19.700260
 96350/100000: episode: 2009, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 53.233, mean reward: 0.532 [0.267, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.864, 10.098], loss: 0.009219, mae: 0.105940, mean_q: 19.606792
 96450/100000: episode: 2010, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 55.976, mean reward: 0.560 [0.299, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.038, 10.098], loss: 0.008488, mae: 0.100443, mean_q: 19.861355
 96550/100000: episode: 2011, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 50.952, mean reward: 0.510 [0.360, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.574, 10.271], loss: 0.008687, mae: 0.103056, mean_q: 19.668879
 96650/100000: episode: 2012, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 54.458, mean reward: 0.545 [0.274, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.760, 10.098], loss: 0.008689, mae: 0.103152, mean_q: 19.806885
[Info] New level: 0.4870838224887848 | Considering 99/1 traces
 96750/100000: episode: 2013, duration: 4.424s, episode steps: 100, steps per second: 23, episode reward: 56.326, mean reward: 0.563 [0.342, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.105], loss: 0.009341, mae: 0.106234, mean_q: 19.702837
[Info] Not found new level, current best level reached = 0.4870838224887848
 96752/100000: episode: 2014, duration: 3.947s, episode steps: 2, steps per second: 1, episode reward: 1.324, mean reward: 0.662 [0.660, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.077, 10.100], loss: 0.007488, mae: 0.103068, mean_q: 20.218634
 96852/100000: episode: 2015, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 52.792, mean reward: 0.528 [0.146, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.696, 10.098], loss: 0.010028, mae: 0.110799, mean_q: 19.661089
 96952/100000: episode: 2016, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 53.742, mean reward: 0.537 [0.290, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.865, 10.098], loss: 0.009556, mae: 0.107915, mean_q: 19.683205
 97052/100000: episode: 2017, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 54.482, mean reward: 0.545 [0.281, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.113, 10.098], loss: 0.008888, mae: 0.103555, mean_q: 19.637812
 97152/100000: episode: 2018, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 54.738, mean reward: 0.547 [0.301, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.335, 10.221], loss: 0.009440, mae: 0.107960, mean_q: 19.454817
 97252/100000: episode: 2019, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 52.617, mean reward: 0.526 [0.181, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.625, 10.158], loss: 0.008071, mae: 0.099550, mean_q: 19.884518
 97352/100000: episode: 2020, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 49.226, mean reward: 0.492 [0.271, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.180, 10.202], loss: 0.007962, mae: 0.098745, mean_q: 20.076574
 97452/100000: episode: 2021, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 48.343, mean reward: 0.483 [0.057, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.660, 10.368], loss: 0.007927, mae: 0.099798, mean_q: 19.704315
 97552/100000: episode: 2022, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 50.023, mean reward: 0.500 [0.163, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.840, 10.479], loss: 0.007966, mae: 0.099302, mean_q: 19.791719
 97652/100000: episode: 2023, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 55.466, mean reward: 0.555 [0.217, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.931, 10.103], loss: 0.008222, mae: 0.099828, mean_q: 19.861900
 97752/100000: episode: 2024, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 55.151, mean reward: 0.552 [0.333, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.953, 10.098], loss: 0.008925, mae: 0.104070, mean_q: 19.608421
 97852/100000: episode: 2025, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 52.625, mean reward: 0.526 [0.314, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.747, 10.098], loss: 0.010412, mae: 0.114156, mean_q: 19.700005
 97952/100000: episode: 2026, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 53.712, mean reward: 0.537 [0.288, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.151, 10.274], loss: 0.008020, mae: 0.099156, mean_q: 19.680979
 98052/100000: episode: 2027, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 49.757, mean reward: 0.498 [0.238, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.641, 10.444], loss: 0.008946, mae: 0.105357, mean_q: 19.581354
 98152/100000: episode: 2028, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 51.667, mean reward: 0.517 [0.289, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.984, 10.098], loss: 0.008114, mae: 0.101053, mean_q: 19.894505
 98252/100000: episode: 2029, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 48.637, mean reward: 0.486 [0.187, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.017, 10.192], loss: 0.008912, mae: 0.102061, mean_q: 19.677069
 98352/100000: episode: 2030, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 54.887, mean reward: 0.549 [0.394, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.766, 10.098], loss: 0.009766, mae: 0.109751, mean_q: 19.522223
 98452/100000: episode: 2031, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 54.110, mean reward: 0.541 [0.296, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.488, 10.107], loss: 0.008289, mae: 0.101221, mean_q: 19.958824
 98552/100000: episode: 2032, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 52.442, mean reward: 0.524 [0.346, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.513, 10.098], loss: 0.009451, mae: 0.106866, mean_q: 20.067526
 98652/100000: episode: 2033, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 52.029, mean reward: 0.520 [0.346, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.844, 10.098], loss: 0.009568, mae: 0.108042, mean_q: 19.789221
 98752/100000: episode: 2034, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 50.057, mean reward: 0.501 [0.255, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.693, 10.182], loss: 0.009329, mae: 0.106921, mean_q: 19.636547
 98852/100000: episode: 2035, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 53.428, mean reward: 0.534 [0.278, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.669, 10.098], loss: 0.009479, mae: 0.106810, mean_q: 19.840597
 98952/100000: episode: 2036, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 54.457, mean reward: 0.545 [0.268, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.967, 10.162], loss: 0.009379, mae: 0.107055, mean_q: 19.498503
 99052/100000: episode: 2037, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 51.364, mean reward: 0.514 [0.231, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.434, 10.098], loss: 0.007903, mae: 0.098796, mean_q: 19.567871
 99152/100000: episode: 2038, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 54.127, mean reward: 0.541 [0.351, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.631, 10.177], loss: 0.007863, mae: 0.097644, mean_q: 19.843063
 99252/100000: episode: 2039, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 53.596, mean reward: 0.536 [0.255, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.405, 10.098], loss: 0.008018, mae: 0.099796, mean_q: 19.372786
 99352/100000: episode: 2040, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 54.859, mean reward: 0.549 [0.292, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.824, 10.283], loss: 0.009841, mae: 0.109874, mean_q: 20.050896
 99452/100000: episode: 2041, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 52.670, mean reward: 0.527 [0.359, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.145, 10.098], loss: 0.007796, mae: 0.097961, mean_q: 20.120178
 99552/100000: episode: 2042, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 50.119, mean reward: 0.501 [0.109, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.837, 10.098], loss: 0.008621, mae: 0.103287, mean_q: 19.717762
 99652/100000: episode: 2043, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 55.563, mean reward: 0.556 [0.342, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.278, 10.098], loss: 0.009274, mae: 0.107212, mean_q: 19.796776
 99752/100000: episode: 2044, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 55.140, mean reward: 0.551 [0.320, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.589, 10.098], loss: 0.011326, mae: 0.118641, mean_q: 19.749880
 99852/100000: episode: 2045, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 50.956, mean reward: 0.510 [0.269, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.539, 10.098], loss: 0.008031, mae: 0.099100, mean_q: 19.803442
 99952/100000: episode: 2046, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 48.918, mean reward: 0.489 [0.227, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.946, 10.247], loss: 0.007347, mae: 0.095510, mean_q: 19.988955
done, took 649.708 seconds
[Info] End Importance Splitting.
