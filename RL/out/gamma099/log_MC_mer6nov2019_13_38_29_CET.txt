Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Uniform Random Simulation.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.158s, episode steps: 100, steps per second: 633, episode reward: 53.717, mean reward: 0.537 [0.286, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.325, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 53.862, mean reward: 0.539 [0.377, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.888, 10.177], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.063s, episode steps: 100, steps per second: 1580, episode reward: 54.191, mean reward: 0.542 [0.173, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.720, 10.306], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 44.067, mean reward: 0.441 [0.181, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.602, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.068s, episode steps: 100, steps per second: 1480, episode reward: 51.228, mean reward: 0.512 [0.326, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.024, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 1.265s, episode steps: 100, steps per second: 79, episode reward: 50.002, mean reward: 0.500 [0.289, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.419, 10.362], loss: 0.042757, mae: 0.145667, mean_q: 2.245287
   700/100000: episode: 7, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 51.744, mean reward: 0.517 [0.267, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.719, 10.264], loss: 0.054591, mae: 0.156616, mean_q: 2.709001
   800/100000: episode: 8, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 50.871, mean reward: 0.509 [0.290, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.896, 10.346], loss: 0.053014, mae: 0.136000, mean_q: 3.141244
   900/100000: episode: 9, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 54.730, mean reward: 0.547 [0.302, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.966, 10.140], loss: 0.048772, mae: 0.131062, mean_q: 3.589233
  1000/100000: episode: 10, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 50.971, mean reward: 0.510 [0.325, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.853, 10.098], loss: 0.063618, mae: 0.154032, mean_q: 4.007799
  1100/100000: episode: 11, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 54.358, mean reward: 0.544 [0.273, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.616, 10.098], loss: 0.054504, mae: 0.141727, mean_q: 4.445674
  1200/100000: episode: 12, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 49.220, mean reward: 0.492 [0.264, 0.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.492, 10.098], loss: 0.079439, mae: 0.185670, mean_q: 4.852842
  1300/100000: episode: 13, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 55.654, mean reward: 0.557 [0.331, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.834, 10.108], loss: 0.076557, mae: 0.165781, mean_q: 5.259027
  1400/100000: episode: 14, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 51.368, mean reward: 0.514 [0.288, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.405, 10.098], loss: 0.073070, mae: 0.174242, mean_q: 5.666781
  1500/100000: episode: 15, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 55.287, mean reward: 0.553 [0.306, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.060, 10.307], loss: 0.047189, mae: 0.135483, mean_q: 6.114861
  1600/100000: episode: 16, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 48.911, mean reward: 0.489 [0.238, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.314, 10.135], loss: 0.059537, mae: 0.168063, mean_q: 6.493247
  1700/100000: episode: 17, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 46.446, mean reward: 0.464 [0.260, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.992, 10.098], loss: 0.057419, mae: 0.147893, mean_q: 6.860729
  1800/100000: episode: 18, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 57.976, mean reward: 0.580 [0.384, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.797, 10.177], loss: 0.054992, mae: 0.163933, mean_q: 7.225398
  1900/100000: episode: 19, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 52.930, mean reward: 0.529 [0.211, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.819, 10.306], loss: 0.049056, mae: 0.170578, mean_q: 7.611862
  2000/100000: episode: 20, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 53.569, mean reward: 0.536 [0.296, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.283, 10.098], loss: 0.051057, mae: 0.172900, mean_q: 7.973904
  2100/100000: episode: 21, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 46.260, mean reward: 0.463 [0.243, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.345, 10.496], loss: 0.049831, mae: 0.165437, mean_q: 8.324486
  2200/100000: episode: 22, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 54.223, mean reward: 0.542 [0.276, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.934, 10.098], loss: 0.038747, mae: 0.156625, mean_q: 8.685331
  2300/100000: episode: 23, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 54.056, mean reward: 0.541 [0.303, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.838, 10.125], loss: 0.029678, mae: 0.139610, mean_q: 8.963997
  2400/100000: episode: 24, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 51.400, mean reward: 0.514 [0.319, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.838, 10.098], loss: 0.035564, mae: 0.147346, mean_q: 9.276484
  2500/100000: episode: 25, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 46.613, mean reward: 0.466 [0.177, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.824, 10.098], loss: 0.021840, mae: 0.129360, mean_q: 9.693457
  2600/100000: episode: 26, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 44.480, mean reward: 0.445 [0.131, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.860, 10.436], loss: 0.023884, mae: 0.137761, mean_q: 9.863793
  2700/100000: episode: 27, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 52.493, mean reward: 0.525 [0.266, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.378, 10.098], loss: 0.027688, mae: 0.152304, mean_q: 10.185066
  2800/100000: episode: 28, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 47.503, mean reward: 0.475 [0.234, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.452, 10.307], loss: 0.027535, mae: 0.164802, mean_q: 10.444561
  2900/100000: episode: 29, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 52.598, mean reward: 0.526 [0.327, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.609, 10.292], loss: 0.021212, mae: 0.139164, mean_q: 10.788392
  3000/100000: episode: 30, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 52.994, mean reward: 0.530 [0.227, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.797, 10.230], loss: 0.018458, mae: 0.136884, mean_q: 11.163758
  3100/100000: episode: 31, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 54.956, mean reward: 0.550 [0.350, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.452, 10.290], loss: 0.020780, mae: 0.152617, mean_q: 11.417118
  3200/100000: episode: 32, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 54.349, mean reward: 0.543 [0.223, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.320, 10.138], loss: 0.018751, mae: 0.141218, mean_q: 11.720272
  3300/100000: episode: 33, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 53.805, mean reward: 0.538 [0.358, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.516, 10.325], loss: 0.017400, mae: 0.141093, mean_q: 11.927546
  3400/100000: episode: 34, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 47.820, mean reward: 0.478 [0.176, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.946, 10.248], loss: 0.020413, mae: 0.149204, mean_q: 12.307370
  3500/100000: episode: 35, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 52.061, mean reward: 0.521 [0.242, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.877, 10.098], loss: 0.016549, mae: 0.135638, mean_q: 12.479095
  3600/100000: episode: 36, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 53.073, mean reward: 0.531 [0.255, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.649, 10.264], loss: 0.019833, mae: 0.147185, mean_q: 12.788177
  3700/100000: episode: 37, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 54.859, mean reward: 0.549 [0.346, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.692, 10.153], loss: 0.019726, mae: 0.151421, mean_q: 12.917825
  3800/100000: episode: 38, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 50.468, mean reward: 0.505 [0.258, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.738, 10.098], loss: 0.017341, mae: 0.141666, mean_q: 13.039607
  3900/100000: episode: 39, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 56.699, mean reward: 0.567 [0.403, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.286, 10.100], loss: 0.018693, mae: 0.146195, mean_q: 13.428549
  4000/100000: episode: 40, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 54.465, mean reward: 0.545 [0.277, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.037, 10.235], loss: 0.018459, mae: 0.142497, mean_q: 13.552140
  4100/100000: episode: 41, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 56.484, mean reward: 0.565 [0.347, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.982, 10.169], loss: 0.019601, mae: 0.148929, mean_q: 13.846166
  4200/100000: episode: 42, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 55.167, mean reward: 0.552 [0.306, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.095, 10.244], loss: 0.018981, mae: 0.147450, mean_q: 14.171518
  4300/100000: episode: 43, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 52.659, mean reward: 0.527 [0.376, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.853, 10.098], loss: 0.019111, mae: 0.146596, mean_q: 14.194244
  4400/100000: episode: 44, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 55.283, mean reward: 0.553 [0.274, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.872, 10.468], loss: 0.020867, mae: 0.154850, mean_q: 14.546892
  4500/100000: episode: 45, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 54.576, mean reward: 0.546 [0.313, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.312, 10.128], loss: 0.022457, mae: 0.160356, mean_q: 14.606459
  4600/100000: episode: 46, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 50.131, mean reward: 0.501 [0.288, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.594, 10.098], loss: 0.018411, mae: 0.147394, mean_q: 14.962953
  4700/100000: episode: 47, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 56.788, mean reward: 0.568 [0.257, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.192, 10.098], loss: 0.020700, mae: 0.152504, mean_q: 15.190958
  4800/100000: episode: 48, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 55.031, mean reward: 0.550 [0.271, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.999, 10.098], loss: 0.021556, mae: 0.158068, mean_q: 15.094765
  4900/100000: episode: 49, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 52.589, mean reward: 0.526 [0.291, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.777, 10.114], loss: 0.022884, mae: 0.163496, mean_q: 15.383422
  5000/100000: episode: 50, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 54.018, mean reward: 0.540 [0.263, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.355, 10.124], loss: 0.019975, mae: 0.150765, mean_q: 15.502286
  5100/100000: episode: 51, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 54.655, mean reward: 0.547 [0.390, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.747, 10.152], loss: 0.020905, mae: 0.156168, mean_q: 15.887459
  5200/100000: episode: 52, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 55.693, mean reward: 0.557 [0.369, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.475, 10.105], loss: 0.023652, mae: 0.167436, mean_q: 15.711596
  5300/100000: episode: 53, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 55.113, mean reward: 0.551 [0.276, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.337, 10.241], loss: 0.021625, mae: 0.161735, mean_q: 16.044750
  5400/100000: episode: 54, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 54.756, mean reward: 0.548 [0.198, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.864, 10.183], loss: 0.019381, mae: 0.150109, mean_q: 16.365503
  5500/100000: episode: 55, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 54.765, mean reward: 0.548 [0.336, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.137, 10.183], loss: 0.019941, mae: 0.153998, mean_q: 16.364513
  5600/100000: episode: 56, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 49.638, mean reward: 0.496 [0.164, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.518, 10.098], loss: 0.022146, mae: 0.162052, mean_q: 16.546906
  5700/100000: episode: 57, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 51.980, mean reward: 0.520 [0.241, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.913, 10.098], loss: 0.021456, mae: 0.158647, mean_q: 16.600536
  5800/100000: episode: 58, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 51.755, mean reward: 0.518 [0.322, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.796, 10.400], loss: 0.022763, mae: 0.162870, mean_q: 16.763607
  5900/100000: episode: 59, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 54.258, mean reward: 0.543 [0.335, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.799, 10.343], loss: 0.019491, mae: 0.148829, mean_q: 16.908968
  6000/100000: episode: 60, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 52.806, mean reward: 0.528 [0.292, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.869, 10.098], loss: 0.023966, mae: 0.165413, mean_q: 16.772951
  6100/100000: episode: 61, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 55.775, mean reward: 0.558 [0.346, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.614, 10.321], loss: 0.019296, mae: 0.149593, mean_q: 17.119736
  6200/100000: episode: 62, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 56.888, mean reward: 0.569 [0.342, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.429, 10.263], loss: 0.022078, mae: 0.156450, mean_q: 17.386511
  6300/100000: episode: 63, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 54.923, mean reward: 0.549 [0.299, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.963, 10.098], loss: 0.019778, mae: 0.150712, mean_q: 17.233976
  6400/100000: episode: 64, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 54.148, mean reward: 0.541 [0.388, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.351, 10.319], loss: 0.021984, mae: 0.161028, mean_q: 17.496841
  6500/100000: episode: 65, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 53.546, mean reward: 0.535 [0.339, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.058, 10.224], loss: 0.020525, mae: 0.149815, mean_q: 17.495565
  6600/100000: episode: 66, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 53.964, mean reward: 0.540 [0.185, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.549, 10.124], loss: 0.021801, mae: 0.156705, mean_q: 17.674179
  6700/100000: episode: 67, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 51.964, mean reward: 0.520 [0.323, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.552, 10.276], loss: 0.017466, mae: 0.143323, mean_q: 18.053282
  6800/100000: episode: 68, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 41.734, mean reward: 0.417 [0.119, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.989, 10.098], loss: 0.021723, mae: 0.158293, mean_q: 17.908613
  6900/100000: episode: 69, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 55.046, mean reward: 0.550 [0.372, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.165, 10.146], loss: 0.018006, mae: 0.147048, mean_q: 17.804316
  7000/100000: episode: 70, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.069, mean reward: 0.551 [0.392, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.399, 10.234], loss: 0.019489, mae: 0.151751, mean_q: 18.112686
  7100/100000: episode: 71, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 52.021, mean reward: 0.520 [0.250, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.907, 10.211], loss: 0.025613, mae: 0.178094, mean_q: 18.089899
  7200/100000: episode: 72, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 53.056, mean reward: 0.531 [0.308, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.240, 10.154], loss: 0.022210, mae: 0.160608, mean_q: 18.321913
  7300/100000: episode: 73, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 53.441, mean reward: 0.534 [0.119, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.732, 10.098], loss: 0.018508, mae: 0.145968, mean_q: 18.325109
  7400/100000: episode: 74, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 55.285, mean reward: 0.553 [0.272, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.020, 10.105], loss: 0.023699, mae: 0.162555, mean_q: 18.239822
  7500/100000: episode: 75, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 50.337, mean reward: 0.503 [0.328, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.349, 10.098], loss: 0.019668, mae: 0.149285, mean_q: 18.592911
  7600/100000: episode: 76, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 55.557, mean reward: 0.556 [0.252, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.876, 10.232], loss: 0.017311, mae: 0.144346, mean_q: 18.519054
  7700/100000: episode: 77, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 54.418, mean reward: 0.544 [0.281, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.743, 10.098], loss: 0.017051, mae: 0.138858, mean_q: 18.541847
  7800/100000: episode: 78, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 53.418, mean reward: 0.534 [0.225, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.503, 10.264], loss: 0.019174, mae: 0.149462, mean_q: 18.645779
  7900/100000: episode: 79, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 54.542, mean reward: 0.545 [0.291, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.399, 10.149], loss: 0.020378, mae: 0.151131, mean_q: 18.791388
  8000/100000: episode: 80, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 54.046, mean reward: 0.540 [0.278, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.010, 10.098], loss: 0.022299, mae: 0.158742, mean_q: 18.892170
  8100/100000: episode: 81, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 52.915, mean reward: 0.529 [0.245, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.607, 10.431], loss: 0.020093, mae: 0.157200, mean_q: 18.953394
  8200/100000: episode: 82, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 53.970, mean reward: 0.540 [0.288, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.099, 10.237], loss: 0.019660, mae: 0.145807, mean_q: 18.802057
  8300/100000: episode: 83, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 52.009, mean reward: 0.520 [0.329, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.859, 10.098], loss: 0.018675, mae: 0.148693, mean_q: 18.716988
  8400/100000: episode: 84, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 54.679, mean reward: 0.547 [0.328, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.084, 10.251], loss: 0.019421, mae: 0.150063, mean_q: 18.881262
  8500/100000: episode: 85, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 50.667, mean reward: 0.507 [0.262, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.553, 10.388], loss: 0.017705, mae: 0.145759, mean_q: 19.014372
  8600/100000: episode: 86, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 50.394, mean reward: 0.504 [0.332, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.659, 10.328], loss: 0.019819, mae: 0.154496, mean_q: 19.094967
  8700/100000: episode: 87, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 53.227, mean reward: 0.532 [0.281, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.643, 10.098], loss: 0.019638, mae: 0.150555, mean_q: 19.219250
  8800/100000: episode: 88, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 52.505, mean reward: 0.525 [0.240, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.634, 10.249], loss: 0.017794, mae: 0.143154, mean_q: 19.027718
  8900/100000: episode: 89, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 57.446, mean reward: 0.574 [0.330, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.104, 10.176], loss: 0.018412, mae: 0.147025, mean_q: 19.410412
  9000/100000: episode: 90, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 47.695, mean reward: 0.477 [0.230, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.664, 10.098], loss: 0.020139, mae: 0.151280, mean_q: 19.434700
  9100/100000: episode: 91, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 51.718, mean reward: 0.517 [0.271, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.112, 10.098], loss: 0.016955, mae: 0.141901, mean_q: 19.439398
  9200/100000: episode: 92, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 50.755, mean reward: 0.508 [0.169, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.384, 10.495], loss: 0.016472, mae: 0.141791, mean_q: 19.495001
  9300/100000: episode: 93, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 53.446, mean reward: 0.534 [0.266, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.099, 10.098], loss: 0.020811, mae: 0.153960, mean_q: 19.325005
  9400/100000: episode: 94, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 53.599, mean reward: 0.536 [0.284, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.328, 10.235], loss: 0.019777, mae: 0.148253, mean_q: 19.132641
  9500/100000: episode: 95, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 52.327, mean reward: 0.523 [0.262, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.363, 10.098], loss: 0.018200, mae: 0.142578, mean_q: 19.544407
  9600/100000: episode: 96, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 54.955, mean reward: 0.550 [0.349, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.286, 10.312], loss: 0.021100, mae: 0.154682, mean_q: 19.271080
  9700/100000: episode: 97, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 52.960, mean reward: 0.530 [0.272, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.821, 10.156], loss: 0.021512, mae: 0.159257, mean_q: 19.247984
  9800/100000: episode: 98, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 54.732, mean reward: 0.547 [0.270, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.796, 10.192], loss: 0.016090, mae: 0.138079, mean_q: 19.437168
  9900/100000: episode: 99, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 55.284, mean reward: 0.553 [0.349, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.308, 10.330], loss: 0.021817, mae: 0.161136, mean_q: 19.351997
 10000/100000: episode: 100, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 52.302, mean reward: 0.523 [0.291, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.720, 10.098], loss: 0.019602, mae: 0.149809, mean_q: 19.466772
 10100/100000: episode: 101, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 55.018, mean reward: 0.550 [0.321, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.840, 10.161], loss: 0.019865, mae: 0.149611, mean_q: 19.383455
 10200/100000: episode: 102, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 56.956, mean reward: 0.570 [0.357, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.031, 10.118], loss: 0.019730, mae: 0.150544, mean_q: 19.440933
 10300/100000: episode: 103, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 53.706, mean reward: 0.537 [0.324, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.523, 10.211], loss: 0.021999, mae: 0.156566, mean_q: 19.252491
 10400/100000: episode: 104, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 52.724, mean reward: 0.527 [0.313, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.037, 10.098], loss: 0.021349, mae: 0.153823, mean_q: 19.353415
 10500/100000: episode: 105, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 53.342, mean reward: 0.533 [0.327, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.052, 10.167], loss: 0.023100, mae: 0.157017, mean_q: 19.834688
 10600/100000: episode: 106, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 54.155, mean reward: 0.542 [0.304, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.942, 10.098], loss: 0.021735, mae: 0.153626, mean_q: 19.582401
 10700/100000: episode: 107, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 56.121, mean reward: 0.561 [0.319, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.690, 10.098], loss: 0.023075, mae: 0.159394, mean_q: 19.513680
 10800/100000: episode: 108, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 55.647, mean reward: 0.556 [0.304, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.281, 10.098], loss: 0.024483, mae: 0.166231, mean_q: 19.384783
 10900/100000: episode: 109, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 55.790, mean reward: 0.558 [0.384, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.570, 10.324], loss: 0.019675, mae: 0.148974, mean_q: 19.307226
 11000/100000: episode: 110, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 51.566, mean reward: 0.516 [0.270, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.112, 10.106], loss: 0.022452, mae: 0.154347, mean_q: 19.460623
 11100/100000: episode: 111, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 48.016, mean reward: 0.480 [0.309, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.255, 10.376], loss: 0.022508, mae: 0.154180, mean_q: 19.358997
 11200/100000: episode: 112, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 51.775, mean reward: 0.518 [0.308, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.489, 10.098], loss: 0.020889, mae: 0.151542, mean_q: 19.343359
 11300/100000: episode: 113, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 55.163, mean reward: 0.552 [0.395, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.798, 10.343], loss: 0.021053, mae: 0.147358, mean_q: 19.819820
 11400/100000: episode: 114, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 50.287, mean reward: 0.503 [0.189, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.616, 10.277], loss: 0.022003, mae: 0.148029, mean_q: 19.712904
 11500/100000: episode: 115, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 51.769, mean reward: 0.518 [0.314, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.451, 10.098], loss: 0.023980, mae: 0.158343, mean_q: 19.544985
 11600/100000: episode: 116, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 51.893, mean reward: 0.519 [0.279, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.565, 10.098], loss: 0.020371, mae: 0.145769, mean_q: 19.795599
 11700/100000: episode: 117, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 55.042, mean reward: 0.550 [0.321, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.536, 10.209], loss: 0.026171, mae: 0.157928, mean_q: 19.674583
 11800/100000: episode: 118, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 54.776, mean reward: 0.548 [0.239, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.486, 10.143], loss: 0.025229, mae: 0.153247, mean_q: 19.509968
 11900/100000: episode: 119, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 51.502, mean reward: 0.515 [0.193, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.173], loss: 0.027015, mae: 0.155990, mean_q: 19.596891
 12000/100000: episode: 120, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 54.883, mean reward: 0.549 [0.333, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.966, 10.098], loss: 0.026987, mae: 0.162073, mean_q: 19.962652
 12100/100000: episode: 121, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 53.450, mean reward: 0.535 [0.270, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.525, 10.098], loss: 0.028022, mae: 0.159289, mean_q: 19.883547
 12200/100000: episode: 122, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 51.813, mean reward: 0.518 [0.256, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.653, 10.113], loss: 0.029592, mae: 0.162273, mean_q: 19.627977
 12300/100000: episode: 123, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 53.903, mean reward: 0.539 [0.315, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.592, 10.098], loss: 0.032017, mae: 0.165352, mean_q: 19.756037
 12400/100000: episode: 124, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 56.857, mean reward: 0.569 [0.340, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.504, 10.177], loss: 0.030555, mae: 0.167270, mean_q: 19.702490
 12500/100000: episode: 125, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 51.440, mean reward: 0.514 [0.254, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.232, 10.098], loss: 0.027495, mae: 0.152278, mean_q: 19.376923
 12600/100000: episode: 126, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 51.097, mean reward: 0.511 [0.125, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.549, 10.098], loss: 0.033585, mae: 0.169388, mean_q: 19.852449
 12700/100000: episode: 127, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 54.837, mean reward: 0.548 [0.335, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.736, 10.098], loss: 0.028881, mae: 0.152909, mean_q: 19.586523
 12800/100000: episode: 128, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 55.327, mean reward: 0.553 [0.289, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.987, 10.339], loss: 0.036565, mae: 0.172784, mean_q: 19.810963
 12900/100000: episode: 129, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 45.255, mean reward: 0.453 [0.192, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.357, 10.098], loss: 0.030151, mae: 0.162703, mean_q: 19.523149
 13000/100000: episode: 130, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 52.835, mean reward: 0.528 [0.301, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.352, 10.098], loss: 0.028236, mae: 0.154406, mean_q: 19.591127
 13100/100000: episode: 131, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 52.745, mean reward: 0.527 [0.252, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.725, 10.246], loss: 0.034499, mae: 0.171859, mean_q: 19.922127
 13200/100000: episode: 132, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 53.338, mean reward: 0.533 [0.347, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.021, 10.290], loss: 0.037410, mae: 0.169151, mean_q: 19.511370
 13300/100000: episode: 133, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 54.930, mean reward: 0.549 [0.360, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.539, 10.150], loss: 0.036315, mae: 0.179681, mean_q: 19.356644
 13400/100000: episode: 134, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 55.307, mean reward: 0.553 [0.334, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.519, 10.125], loss: 0.026073, mae: 0.154293, mean_q: 19.631874
 13500/100000: episode: 135, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 51.508, mean reward: 0.515 [0.087, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.927, 10.164], loss: 0.042046, mae: 0.202082, mean_q: 19.414665
 13600/100000: episode: 136, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 53.230, mean reward: 0.532 [0.282, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.149, 10.170], loss: 0.031148, mae: 0.162692, mean_q: 19.369709
 13700/100000: episode: 137, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 54.064, mean reward: 0.541 [0.350, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.615, 10.098], loss: 0.028685, mae: 0.164594, mean_q: 19.525158
 13800/100000: episode: 138, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 48.290, mean reward: 0.483 [0.227, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.300, 10.466], loss: 0.031953, mae: 0.173747, mean_q: 19.737152
 13900/100000: episode: 139, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 52.278, mean reward: 0.523 [0.283, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.748, 10.098], loss: 0.027074, mae: 0.158574, mean_q: 19.544001
 14000/100000: episode: 140, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 53.966, mean reward: 0.540 [0.333, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.717, 10.124], loss: 0.029848, mae: 0.172185, mean_q: 19.560558
 14100/100000: episode: 141, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 46.513, mean reward: 0.465 [0.241, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.347, 10.098], loss: 0.026088, mae: 0.163010, mean_q: 19.849510
 14200/100000: episode: 142, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 55.759, mean reward: 0.558 [0.339, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.788, 10.326], loss: 0.025340, mae: 0.158306, mean_q: 19.451523
 14300/100000: episode: 143, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 53.535, mean reward: 0.535 [0.343, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.997, 10.208], loss: 0.029609, mae: 0.168627, mean_q: 19.563206
 14400/100000: episode: 144, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 54.305, mean reward: 0.543 [0.363, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.644, 10.098], loss: 0.027860, mae: 0.166866, mean_q: 19.673120
 14500/100000: episode: 145, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 51.592, mean reward: 0.516 [0.268, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.413, 10.098], loss: 0.024831, mae: 0.160074, mean_q: 19.550051
 14600/100000: episode: 146, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 52.123, mean reward: 0.521 [0.257, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.428, 10.098], loss: 0.023244, mae: 0.152802, mean_q: 19.671288
 14700/100000: episode: 147, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 55.554, mean reward: 0.556 [0.368, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.017, 10.168], loss: 0.018904, mae: 0.143690, mean_q: 19.180029
 14800/100000: episode: 148, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 54.241, mean reward: 0.542 [0.331, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.714, 10.098], loss: 0.021942, mae: 0.151761, mean_q: 19.226889
 14900/100000: episode: 149, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 53.177, mean reward: 0.532 [0.257, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.478, 10.098], loss: 0.018707, mae: 0.148183, mean_q: 19.275667
 15000/100000: episode: 150, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.118, mean reward: 0.551 [0.312, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.450, 10.098], loss: 0.023145, mae: 0.157862, mean_q: 19.520638
 15100/100000: episode: 151, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 53.795, mean reward: 0.538 [0.260, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.420, 10.098], loss: 0.022223, mae: 0.150996, mean_q: 19.303768
 15200/100000: episode: 152, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 55.054, mean reward: 0.551 [0.272, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.941, 10.098], loss: 0.022551, mae: 0.158333, mean_q: 19.485924
 15300/100000: episode: 153, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 48.084, mean reward: 0.481 [0.109, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.114, 10.402], loss: 0.023870, mae: 0.159323, mean_q: 19.505808
 15400/100000: episode: 154, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 53.555, mean reward: 0.536 [0.185, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.866, 10.326], loss: 0.022693, mae: 0.158714, mean_q: 19.539339
 15500/100000: episode: 155, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 54.431, mean reward: 0.544 [0.150, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.951, 10.231], loss: 0.018598, mae: 0.143891, mean_q: 19.729477
 15600/100000: episode: 156, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 53.480, mean reward: 0.535 [0.213, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.206, 10.098], loss: 0.017493, mae: 0.138460, mean_q: 19.443830
 15700/100000: episode: 157, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 52.495, mean reward: 0.525 [0.180, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.678, 10.335], loss: 0.020105, mae: 0.152052, mean_q: 19.164518
 15800/100000: episode: 158, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 51.153, mean reward: 0.512 [0.178, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.283, 10.098], loss: 0.018144, mae: 0.142258, mean_q: 19.474085
 15900/100000: episode: 159, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 48.836, mean reward: 0.488 [0.220, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.895, 10.439], loss: 0.016006, mae: 0.137177, mean_q: 19.899307
 16000/100000: episode: 160, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 56.122, mean reward: 0.561 [0.201, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.701, 10.098], loss: 0.019117, mae: 0.145023, mean_q: 19.361980
 16100/100000: episode: 161, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 53.115, mean reward: 0.531 [0.305, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.456, 10.392], loss: 0.022880, mae: 0.155886, mean_q: 19.539749
 16200/100000: episode: 162, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 53.992, mean reward: 0.540 [0.265, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.261, 10.098], loss: 0.019314, mae: 0.145227, mean_q: 19.657743
 16300/100000: episode: 163, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 53.594, mean reward: 0.536 [0.305, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.597, 10.110], loss: 0.016513, mae: 0.136233, mean_q: 19.516762
 16400/100000: episode: 164, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 54.869, mean reward: 0.549 [0.276, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.484, 10.098], loss: 0.016109, mae: 0.131368, mean_q: 19.444756
 16500/100000: episode: 165, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 54.552, mean reward: 0.546 [0.251, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.672, 10.285], loss: 0.021390, mae: 0.158657, mean_q: 19.678749
 16600/100000: episode: 166, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 53.416, mean reward: 0.534 [0.251, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.902, 10.176], loss: 0.021026, mae: 0.155062, mean_q: 19.457886
 16700/100000: episode: 167, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 54.582, mean reward: 0.546 [0.360, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.282, 10.167], loss: 0.021094, mae: 0.150941, mean_q: 19.098976
 16800/100000: episode: 168, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 51.716, mean reward: 0.517 [0.330, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.407, 10.152], loss: 0.018281, mae: 0.145615, mean_q: 19.426367
 16900/100000: episode: 169, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 55.311, mean reward: 0.553 [0.222, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.310, 10.098], loss: 0.020249, mae: 0.151712, mean_q: 19.609564
 17000/100000: episode: 170, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 51.337, mean reward: 0.513 [0.220, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.124, 10.100], loss: 0.020228, mae: 0.150512, mean_q: 19.771288
 17100/100000: episode: 171, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 54.330, mean reward: 0.543 [0.312, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.860, 10.171], loss: 0.019601, mae: 0.142306, mean_q: 19.545837
 17200/100000: episode: 172, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 54.890, mean reward: 0.549 [0.315, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.595, 10.107], loss: 0.021052, mae: 0.147506, mean_q: 19.454166
 17300/100000: episode: 173, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 56.105, mean reward: 0.561 [0.363, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.242, 10.098], loss: 0.024065, mae: 0.157122, mean_q: 19.729504
 17400/100000: episode: 174, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 52.214, mean reward: 0.522 [0.183, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.711, 10.098], loss: 0.020015, mae: 0.143862, mean_q: 19.615456
 17500/100000: episode: 175, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 52.948, mean reward: 0.529 [0.234, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.537, 10.098], loss: 0.023209, mae: 0.152681, mean_q: 19.434689
 17600/100000: episode: 176, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 51.511, mean reward: 0.515 [0.173, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.800, 10.098], loss: 0.022892, mae: 0.154100, mean_q: 19.378731
 17700/100000: episode: 177, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 49.439, mean reward: 0.494 [0.228, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.245, 10.445], loss: 0.024110, mae: 0.156159, mean_q: 19.317745
 17800/100000: episode: 178, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 54.330, mean reward: 0.543 [0.324, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.210, 10.098], loss: 0.023268, mae: 0.153418, mean_q: 19.595545
 17900/100000: episode: 179, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 48.341, mean reward: 0.483 [0.272, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.788, 10.098], loss: 0.024628, mae: 0.153629, mean_q: 19.503769
 18000/100000: episode: 180, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 53.197, mean reward: 0.532 [0.367, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.432, 10.098], loss: 0.020358, mae: 0.144976, mean_q: 19.589491
 18100/100000: episode: 181, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 52.320, mean reward: 0.523 [0.315, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.845, 10.126], loss: 0.024749, mae: 0.153396, mean_q: 19.474966
 18200/100000: episode: 182, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 55.329, mean reward: 0.553 [0.320, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.885, 10.098], loss: 0.026294, mae: 0.158357, mean_q: 19.446217
 18300/100000: episode: 183, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.810, mean reward: 0.578 [0.400, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.787, 10.135], loss: 0.028571, mae: 0.165311, mean_q: 19.457947
 18400/100000: episode: 184, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 55.333, mean reward: 0.553 [0.332, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.207, 10.230], loss: 0.020535, mae: 0.141410, mean_q: 19.440536
 18500/100000: episode: 185, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 56.319, mean reward: 0.563 [0.330, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.393, 10.209], loss: 0.023088, mae: 0.147816, mean_q: 19.461952
 18600/100000: episode: 186, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 52.967, mean reward: 0.530 [0.286, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.428, 10.127], loss: 0.020531, mae: 0.140224, mean_q: 19.712784
 18700/100000: episode: 187, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.196, mean reward: 0.582 [0.354, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.403, 10.189], loss: 0.025499, mae: 0.154186, mean_q: 19.473156
 18800/100000: episode: 188, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 53.518, mean reward: 0.535 [0.341, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.981, 10.223], loss: 0.026217, mae: 0.156615, mean_q: 19.673037
 18900/100000: episode: 189, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 52.118, mean reward: 0.521 [0.274, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.523, 10.259], loss: 0.021480, mae: 0.144262, mean_q: 19.459692
 19000/100000: episode: 190, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 51.627, mean reward: 0.516 [0.316, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.198, 10.106], loss: 0.024157, mae: 0.152592, mean_q: 19.559155
 19100/100000: episode: 191, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 55.671, mean reward: 0.557 [0.332, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.632, 10.098], loss: 0.025830, mae: 0.164218, mean_q: 19.548811
 19200/100000: episode: 192, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 53.774, mean reward: 0.538 [0.339, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.798, 10.132], loss: 0.020621, mae: 0.145124, mean_q: 19.492126
 19300/100000: episode: 193, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 54.941, mean reward: 0.549 [0.209, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.462, 10.259], loss: 0.018151, mae: 0.134611, mean_q: 19.592976
 19400/100000: episode: 194, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 48.063, mean reward: 0.481 [0.259, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.163, 10.098], loss: 0.019761, mae: 0.140733, mean_q: 19.671335
 19500/100000: episode: 195, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 53.349, mean reward: 0.533 [0.273, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.886, 10.098], loss: 0.019623, mae: 0.140931, mean_q: 19.266640
 19600/100000: episode: 196, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 55.417, mean reward: 0.554 [0.317, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.783, 10.187], loss: 0.020826, mae: 0.141480, mean_q: 19.597252
 19700/100000: episode: 197, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 55.537, mean reward: 0.555 [0.358, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.264, 10.098], loss: 0.016280, mae: 0.129824, mean_q: 19.334885
 19800/100000: episode: 198, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 49.709, mean reward: 0.497 [0.237, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.378, 10.282], loss: 0.019164, mae: 0.135999, mean_q: 19.558447
 19900/100000: episode: 199, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 54.507, mean reward: 0.545 [0.287, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.614, 10.098], loss: 0.017611, mae: 0.134169, mean_q: 19.401482
 20000/100000: episode: 200, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 52.751, mean reward: 0.528 [0.328, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.684, 10.267], loss: 0.016727, mae: 0.132876, mean_q: 19.643255
 20100/100000: episode: 201, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 52.750, mean reward: 0.528 [0.300, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.884, 10.197], loss: 0.015860, mae: 0.133644, mean_q: 19.466412
 20200/100000: episode: 202, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 54.328, mean reward: 0.543 [0.348, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.380, 10.111], loss: 0.022343, mae: 0.155146, mean_q: 19.667088
 20300/100000: episode: 203, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 49.879, mean reward: 0.499 [0.272, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.930, 10.164], loss: 0.015636, mae: 0.132947, mean_q: 19.541704
 20400/100000: episode: 204, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.916, mean reward: 0.539 [0.231, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.755, 10.098], loss: 0.015563, mae: 0.135289, mean_q: 19.516926
 20500/100000: episode: 205, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 54.093, mean reward: 0.541 [0.244, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.694, 10.098], loss: 0.014201, mae: 0.125835, mean_q: 19.578531
 20600/100000: episode: 206, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 53.725, mean reward: 0.537 [0.265, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.211, 10.111], loss: 0.016173, mae: 0.135503, mean_q: 19.560858
 20700/100000: episode: 207, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 55.234, mean reward: 0.552 [0.371, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.483, 10.243], loss: 0.013848, mae: 0.124851, mean_q: 19.442490
 20800/100000: episode: 208, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 56.427, mean reward: 0.564 [0.326, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.233, 10.098], loss: 0.015026, mae: 0.133059, mean_q: 19.408533
 20900/100000: episode: 209, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 53.051, mean reward: 0.531 [0.285, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.119, 10.142], loss: 0.013457, mae: 0.126332, mean_q: 19.593367
 21000/100000: episode: 210, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 58.396, mean reward: 0.584 [0.259, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.583, 10.199], loss: 0.016165, mae: 0.137835, mean_q: 19.563675
 21100/100000: episode: 211, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 50.381, mean reward: 0.504 [0.226, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.180, 10.332], loss: 0.015679, mae: 0.135496, mean_q: 19.430632
 21200/100000: episode: 212, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 52.562, mean reward: 0.526 [0.345, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.696, 10.098], loss: 0.015493, mae: 0.134990, mean_q: 19.916109
 21300/100000: episode: 213, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 46.070, mean reward: 0.461 [0.116, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.238, 10.280], loss: 0.012298, mae: 0.119880, mean_q: 19.472490
 21400/100000: episode: 214, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 54.844, mean reward: 0.548 [0.246, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.686, 10.098], loss: 0.014532, mae: 0.131399, mean_q: 19.528532
 21500/100000: episode: 215, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 54.727, mean reward: 0.547 [0.346, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.920, 10.358], loss: 0.016371, mae: 0.136271, mean_q: 19.703241
 21600/100000: episode: 216, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 52.696, mean reward: 0.527 [0.250, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.180, 10.325], loss: 0.014240, mae: 0.126592, mean_q: 19.811922
 21700/100000: episode: 217, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 53.613, mean reward: 0.536 [0.353, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.569, 10.098], loss: 0.015289, mae: 0.131428, mean_q: 19.396904
 21800/100000: episode: 218, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 51.009, mean reward: 0.510 [0.188, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.502, 10.189], loss: 0.015982, mae: 0.135110, mean_q: 19.712879
 21900/100000: episode: 219, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 55.406, mean reward: 0.554 [0.230, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.669, 10.142], loss: 0.016129, mae: 0.134411, mean_q: 19.575321
 22000/100000: episode: 220, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 54.514, mean reward: 0.545 [0.375, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.390, 10.098], loss: 0.014282, mae: 0.128558, mean_q: 19.592955
 22100/100000: episode: 221, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 56.223, mean reward: 0.562 [0.321, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.525, 10.098], loss: 0.013117, mae: 0.122503, mean_q: 19.251026
 22200/100000: episode: 222, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 50.541, mean reward: 0.505 [0.217, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.780, 10.098], loss: 0.014476, mae: 0.125842, mean_q: 19.742271
 22300/100000: episode: 223, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 51.873, mean reward: 0.519 [0.325, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.462, 10.098], loss: 0.015847, mae: 0.129776, mean_q: 19.366549
 22400/100000: episode: 224, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 55.828, mean reward: 0.558 [0.387, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.680, 10.098], loss: 0.014245, mae: 0.128563, mean_q: 19.918219
 22500/100000: episode: 225, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 49.933, mean reward: 0.499 [0.236, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.377, 10.098], loss: 0.014247, mae: 0.129715, mean_q: 19.575911
 22600/100000: episode: 226, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 50.300, mean reward: 0.503 [0.259, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.537, 10.098], loss: 0.013036, mae: 0.122767, mean_q: 19.482733
 22700/100000: episode: 227, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 52.967, mean reward: 0.530 [0.332, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.326, 10.098], loss: 0.013412, mae: 0.125387, mean_q: 19.625446
 22800/100000: episode: 228, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 52.263, mean reward: 0.523 [0.309, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.499, 10.205], loss: 0.014490, mae: 0.130334, mean_q: 19.757067
 22900/100000: episode: 229, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 52.670, mean reward: 0.527 [0.365, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.116, 10.357], loss: 0.014716, mae: 0.128418, mean_q: 19.737434
 23000/100000: episode: 230, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 53.193, mean reward: 0.532 [0.267, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.800, 10.164], loss: 0.013772, mae: 0.126202, mean_q: 19.867323
 23100/100000: episode: 231, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 52.319, mean reward: 0.523 [0.307, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.511, 10.098], loss: 0.013955, mae: 0.129676, mean_q: 19.400482
 23200/100000: episode: 232, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 57.029, mean reward: 0.570 [0.273, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.509, 10.304], loss: 0.013927, mae: 0.127811, mean_q: 19.911873
 23300/100000: episode: 233, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 46.426, mean reward: 0.464 [0.225, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.745, 10.098], loss: 0.015339, mae: 0.136078, mean_q: 19.728134
 23400/100000: episode: 234, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 55.471, mean reward: 0.555 [0.336, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.288, 10.098], loss: 0.018181, mae: 0.144500, mean_q: 19.723940
 23500/100000: episode: 235, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 45.227, mean reward: 0.452 [0.213, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.862, 10.098], loss: 0.014160, mae: 0.128519, mean_q: 19.598402
 23600/100000: episode: 236, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 47.485, mean reward: 0.475 [0.181, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.945, 10.098], loss: 0.015581, mae: 0.138182, mean_q: 19.771132
 23700/100000: episode: 237, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 56.128, mean reward: 0.561 [0.377, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.937, 10.098], loss: 0.018260, mae: 0.146107, mean_q: 19.605192
 23800/100000: episode: 238, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 56.317, mean reward: 0.563 [0.268, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.579, 10.126], loss: 0.016408, mae: 0.139942, mean_q: 19.746742
 23900/100000: episode: 239, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 52.202, mean reward: 0.522 [0.323, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.323, 10.098], loss: 0.016791, mae: 0.143009, mean_q: 19.696548
 24000/100000: episode: 240, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 54.786, mean reward: 0.548 [0.309, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.363, 10.098], loss: 0.015857, mae: 0.136487, mean_q: 19.615517
 24100/100000: episode: 241, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 54.267, mean reward: 0.543 [0.315, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.285, 10.098], loss: 0.016408, mae: 0.137966, mean_q: 19.858271
 24200/100000: episode: 242, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 54.458, mean reward: 0.545 [0.299, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.042, 10.098], loss: 0.015400, mae: 0.135029, mean_q: 19.759563
 24300/100000: episode: 243, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 50.151, mean reward: 0.502 [0.098, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.706, 10.123], loss: 0.015775, mae: 0.135308, mean_q: 19.695307
 24400/100000: episode: 244, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 50.673, mean reward: 0.507 [0.180, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.643, 10.184], loss: 0.019991, mae: 0.152637, mean_q: 19.571449
 24500/100000: episode: 245, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 55.705, mean reward: 0.557 [0.333, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.319, 10.158], loss: 0.016945, mae: 0.139978, mean_q: 19.636009
 24600/100000: episode: 246, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 45.191, mean reward: 0.452 [0.171, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.440, 10.471], loss: 0.016117, mae: 0.136779, mean_q: 19.883003
 24700/100000: episode: 247, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 53.136, mean reward: 0.531 [0.347, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.518, 10.098], loss: 0.019286, mae: 0.149581, mean_q: 19.430767
 24800/100000: episode: 248, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.524, mean reward: 0.575 [0.307, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.978, 10.272], loss: 0.018829, mae: 0.146901, mean_q: 19.707520
 24900/100000: episode: 249, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 54.621, mean reward: 0.546 [0.398, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.453, 10.098], loss: 0.017388, mae: 0.143321, mean_q: 19.659912
 25000/100000: episode: 250, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.690, mean reward: 0.587 [0.366, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.711, 10.189], loss: 0.015661, mae: 0.135571, mean_q: 19.745333
 25100/100000: episode: 251, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 54.372, mean reward: 0.544 [0.272, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.700, 10.098], loss: 0.019150, mae: 0.145569, mean_q: 19.665136
 25200/100000: episode: 252, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 50.028, mean reward: 0.500 [0.248, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.913, 10.098], loss: 0.019581, mae: 0.147908, mean_q: 19.944365
 25300/100000: episode: 253, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 53.107, mean reward: 0.531 [0.215, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.918, 10.287], loss: 0.017981, mae: 0.148305, mean_q: 19.501379
 25400/100000: episode: 254, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 55.242, mean reward: 0.552 [0.245, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.786, 10.098], loss: 0.017186, mae: 0.139131, mean_q: 19.734982
 25500/100000: episode: 255, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 52.241, mean reward: 0.522 [0.285, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.959, 10.380], loss: 0.016639, mae: 0.137830, mean_q: 19.923876
 25600/100000: episode: 256, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 49.316, mean reward: 0.493 [0.241, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.887, 10.098], loss: 0.017808, mae: 0.140806, mean_q: 19.741781
 25700/100000: episode: 257, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 46.361, mean reward: 0.464 [0.174, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.355, 10.098], loss: 0.017525, mae: 0.141723, mean_q: 19.494165
 25800/100000: episode: 258, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 54.754, mean reward: 0.548 [0.321, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.251, 10.190], loss: 0.018858, mae: 0.146817, mean_q: 19.973499
 25900/100000: episode: 259, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 54.282, mean reward: 0.543 [0.329, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.535, 10.098], loss: 0.018582, mae: 0.146020, mean_q: 19.773138
 26000/100000: episode: 260, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 50.173, mean reward: 0.502 [0.310, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.096, 10.244], loss: 0.021480, mae: 0.158286, mean_q: 19.282242
 26100/100000: episode: 261, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 53.570, mean reward: 0.536 [0.353, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.141, 10.098], loss: 0.017042, mae: 0.140923, mean_q: 19.566774
 26200/100000: episode: 262, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 55.007, mean reward: 0.550 [0.375, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.702, 10.160], loss: 0.016012, mae: 0.135930, mean_q: 19.917580
 26300/100000: episode: 263, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 47.340, mean reward: 0.473 [0.219, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.551, 10.443], loss: 0.019456, mae: 0.149203, mean_q: 19.558552
 26400/100000: episode: 264, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 52.678, mean reward: 0.527 [0.260, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.985, 10.204], loss: 0.022124, mae: 0.152017, mean_q: 19.858559
 26500/100000: episode: 265, duration: 0.662s, episode steps: 100, steps per second: 151, episode reward: 53.154, mean reward: 0.532 [0.326, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.638, 10.108], loss: 0.017554, mae: 0.144941, mean_q: 19.872988
 26600/100000: episode: 266, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 50.950, mean reward: 0.510 [0.304, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.950, 10.098], loss: 0.020247, mae: 0.151988, mean_q: 19.513937
 26700/100000: episode: 267, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 51.539, mean reward: 0.515 [0.273, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.296, 10.098], loss: 0.021693, mae: 0.150047, mean_q: 19.756256
 26800/100000: episode: 268, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 50.062, mean reward: 0.501 [0.239, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.617, 10.098], loss: 0.021581, mae: 0.156542, mean_q: 19.702000
 26900/100000: episode: 269, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 51.314, mean reward: 0.513 [0.247, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.904, 10.098], loss: 0.015613, mae: 0.132115, mean_q: 19.582829
 27000/100000: episode: 270, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 54.165, mean reward: 0.542 [0.349, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.010, 10.098], loss: 0.017336, mae: 0.138742, mean_q: 19.825893
 27100/100000: episode: 271, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 54.506, mean reward: 0.545 [0.330, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.221, 10.116], loss: 0.017494, mae: 0.134821, mean_q: 19.667038
 27200/100000: episode: 272, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 54.800, mean reward: 0.548 [0.343, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.060, 10.236], loss: 0.017528, mae: 0.138504, mean_q: 19.672802
 27300/100000: episode: 273, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 52.737, mean reward: 0.527 [0.265, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.521, 10.098], loss: 0.017583, mae: 0.140791, mean_q: 19.690985
 27400/100000: episode: 274, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 47.673, mean reward: 0.477 [0.199, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.460, 10.305], loss: 0.020418, mae: 0.151391, mean_q: 19.692680
 27500/100000: episode: 275, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 54.353, mean reward: 0.544 [0.262, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.268, 10.098], loss: 0.017285, mae: 0.140901, mean_q: 19.835052
 27600/100000: episode: 276, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 53.230, mean reward: 0.532 [0.248, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.802, 10.216], loss: 0.015159, mae: 0.131011, mean_q: 19.530684
 27700/100000: episode: 277, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 52.713, mean reward: 0.527 [0.351, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.022, 10.191], loss: 0.018310, mae: 0.148536, mean_q: 19.702713
 27800/100000: episode: 278, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 52.643, mean reward: 0.526 [0.268, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.334, 10.260], loss: 0.014735, mae: 0.133874, mean_q: 19.520258
 27900/100000: episode: 279, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 52.765, mean reward: 0.528 [0.307, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.807, 10.098], loss: 0.016831, mae: 0.136731, mean_q: 19.227381
 28000/100000: episode: 280, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 49.120, mean reward: 0.491 [0.124, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.338, 10.255], loss: 0.018305, mae: 0.145449, mean_q: 19.410397
 28100/100000: episode: 281, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 51.983, mean reward: 0.520 [0.289, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.296, 10.098], loss: 0.017935, mae: 0.143583, mean_q: 19.547358
 28200/100000: episode: 282, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 52.468, mean reward: 0.525 [0.318, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.287, 10.131], loss: 0.017471, mae: 0.140921, mean_q: 19.338959
 28300/100000: episode: 283, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 51.341, mean reward: 0.513 [0.322, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.040, 10.448], loss: 0.019374, mae: 0.147793, mean_q: 19.456682
 28400/100000: episode: 284, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 50.422, mean reward: 0.504 [0.273, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.492, 10.098], loss: 0.021006, mae: 0.151185, mean_q: 19.360456
 28500/100000: episode: 285, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 55.543, mean reward: 0.555 [0.333, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.999, 10.098], loss: 0.018386, mae: 0.145583, mean_q: 19.600040
 28600/100000: episode: 286, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 49.492, mean reward: 0.495 [0.287, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.159, 10.098], loss: 0.017235, mae: 0.139325, mean_q: 19.581610
 28700/100000: episode: 287, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 50.206, mean reward: 0.502 [0.203, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.099, 10.320], loss: 0.014574, mae: 0.130028, mean_q: 19.659365
 28800/100000: episode: 288, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 52.960, mean reward: 0.530 [0.309, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.736, 10.245], loss: 0.017384, mae: 0.136234, mean_q: 19.506441
 28900/100000: episode: 289, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 46.809, mean reward: 0.468 [0.159, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.055, 10.098], loss: 0.018659, mae: 0.143111, mean_q: 19.353434
 29000/100000: episode: 290, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 53.506, mean reward: 0.535 [0.182, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.405, 10.098], loss: 0.021593, mae: 0.157395, mean_q: 19.398529
 29100/100000: episode: 291, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 54.489, mean reward: 0.545 [0.187, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.748, 10.224], loss: 0.021764, mae: 0.158824, mean_q: 19.422873
 29200/100000: episode: 292, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 57.059, mean reward: 0.571 [0.316, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.863, 10.098], loss: 0.015856, mae: 0.135075, mean_q: 19.710234
 29300/100000: episode: 293, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 47.392, mean reward: 0.474 [0.183, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.589, 10.098], loss: 0.017980, mae: 0.141789, mean_q: 19.273441
 29400/100000: episode: 294, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 53.355, mean reward: 0.534 [0.340, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.866, 10.098], loss: 0.020117, mae: 0.150272, mean_q: 19.514223
 29500/100000: episode: 295, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 53.126, mean reward: 0.531 [0.331, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.788, 10.098], loss: 0.016412, mae: 0.135909, mean_q: 19.494738
 29600/100000: episode: 296, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 56.951, mean reward: 0.570 [0.416, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.814, 10.098], loss: 0.014348, mae: 0.128315, mean_q: 19.772568
 29700/100000: episode: 297, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 52.381, mean reward: 0.524 [0.299, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.949, 10.098], loss: 0.015836, mae: 0.137169, mean_q: 19.001356
 29800/100000: episode: 298, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 56.745, mean reward: 0.567 [0.357, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.765, 10.125], loss: 0.013874, mae: 0.128171, mean_q: 19.613472
 29900/100000: episode: 299, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 52.856, mean reward: 0.529 [0.306, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.429, 10.098], loss: 0.015560, mae: 0.134754, mean_q: 19.237066
 30000/100000: episode: 300, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 54.248, mean reward: 0.542 [0.339, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.863, 10.098], loss: 0.016887, mae: 0.138221, mean_q: 19.497322
 30100/100000: episode: 301, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 51.038, mean reward: 0.510 [0.291, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.435, 10.098], loss: 0.014694, mae: 0.128459, mean_q: 19.335035
 30200/100000: episode: 302, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 52.979, mean reward: 0.530 [0.145, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.044, 10.370], loss: 0.017432, mae: 0.141502, mean_q: 19.308504
 30300/100000: episode: 303, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 55.366, mean reward: 0.554 [0.260, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.872, 10.223], loss: 0.015656, mae: 0.136441, mean_q: 19.462976
 30400/100000: episode: 304, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 51.016, mean reward: 0.510 [0.294, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.181, 10.177], loss: 0.017774, mae: 0.141664, mean_q: 19.744511
 30500/100000: episode: 305, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 53.064, mean reward: 0.531 [0.191, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.271, 10.098], loss: 0.017030, mae: 0.141098, mean_q: 19.614254
 30600/100000: episode: 306, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 55.237, mean reward: 0.552 [0.262, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.268, 10.098], loss: 0.019411, mae: 0.151341, mean_q: 19.361185
 30700/100000: episode: 307, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 50.930, mean reward: 0.509 [0.275, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.584, 10.098], loss: 0.015066, mae: 0.132424, mean_q: 19.120098
 30800/100000: episode: 308, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 53.600, mean reward: 0.536 [0.326, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.858, 10.098], loss: 0.018617, mae: 0.148228, mean_q: 18.922674
 30900/100000: episode: 309, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 55.887, mean reward: 0.559 [0.349, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.893, 10.177], loss: 0.019253, mae: 0.151700, mean_q: 19.501928
 31000/100000: episode: 310, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.092, mean reward: 0.571 [0.369, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.095, 10.180], loss: 0.016217, mae: 0.137792, mean_q: 19.590454
 31100/100000: episode: 311, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 53.050, mean reward: 0.531 [0.308, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.714, 10.098], loss: 0.016208, mae: 0.138843, mean_q: 19.317432
 31200/100000: episode: 312, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 55.787, mean reward: 0.558 [0.341, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.615, 10.098], loss: 0.016973, mae: 0.142744, mean_q: 19.445166
 31300/100000: episode: 313, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 52.551, mean reward: 0.526 [0.320, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.030, 10.098], loss: 0.015178, mae: 0.133655, mean_q: 19.365694
 31400/100000: episode: 314, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 51.584, mean reward: 0.516 [0.191, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.156, 10.423], loss: 0.018514, mae: 0.148391, mean_q: 19.279190
 31500/100000: episode: 315, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 53.140, mean reward: 0.531 [0.166, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.585, 10.106], loss: 0.014314, mae: 0.131739, mean_q: 19.346718
 31600/100000: episode: 316, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 48.647, mean reward: 0.486 [0.170, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.087, 10.149], loss: 0.015459, mae: 0.135711, mean_q: 19.192745
 31700/100000: episode: 317, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 55.506, mean reward: 0.555 [0.294, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.197, 10.098], loss: 0.016558, mae: 0.139755, mean_q: 19.500395
 31800/100000: episode: 318, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 47.434, mean reward: 0.474 [0.190, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.399, 10.098], loss: 0.015111, mae: 0.135521, mean_q: 19.219328
 31900/100000: episode: 319, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 50.883, mean reward: 0.509 [0.231, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.562, 10.108], loss: 0.014978, mae: 0.134346, mean_q: 19.392136
 32000/100000: episode: 320, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 53.717, mean reward: 0.537 [0.267, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.533, 10.306], loss: 0.014915, mae: 0.134608, mean_q: 19.290977
 32100/100000: episode: 321, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 55.967, mean reward: 0.560 [0.275, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.415, 10.335], loss: 0.015655, mae: 0.136784, mean_q: 19.291798
 32200/100000: episode: 322, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 55.176, mean reward: 0.552 [0.316, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.667, 10.098], loss: 0.012762, mae: 0.123971, mean_q: 19.358431
 32300/100000: episode: 323, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 54.319, mean reward: 0.543 [0.264, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.850, 10.098], loss: 0.014308, mae: 0.132080, mean_q: 19.141888
 32400/100000: episode: 324, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 52.058, mean reward: 0.521 [0.134, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.905, 10.098], loss: 0.014262, mae: 0.129698, mean_q: 19.076176
 32500/100000: episode: 325, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 54.194, mean reward: 0.542 [0.343, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.357, 10.098], loss: 0.016931, mae: 0.141195, mean_q: 19.439272
 32600/100000: episode: 326, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 56.217, mean reward: 0.562 [0.278, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.891, 10.167], loss: 0.015398, mae: 0.137074, mean_q: 19.377001
 32700/100000: episode: 327, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 52.355, mean reward: 0.524 [0.305, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.227, 10.098], loss: 0.015193, mae: 0.135797, mean_q: 19.339111
 32800/100000: episode: 328, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 52.247, mean reward: 0.522 [0.238, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.742, 10.158], loss: 0.018244, mae: 0.148414, mean_q: 19.245522
 32900/100000: episode: 329, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 50.643, mean reward: 0.506 [0.233, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.020, 10.157], loss: 0.014808, mae: 0.134217, mean_q: 19.651300
 33000/100000: episode: 330, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 53.906, mean reward: 0.539 [0.225, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.874, 10.251], loss: 0.013054, mae: 0.126225, mean_q: 19.685873
 33100/100000: episode: 331, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 56.302, mean reward: 0.563 [0.390, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.060, 10.163], loss: 0.014167, mae: 0.130163, mean_q: 19.369967
 33200/100000: episode: 332, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 55.561, mean reward: 0.556 [0.202, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.032, 10.098], loss: 0.016515, mae: 0.140960, mean_q: 19.434927
 33300/100000: episode: 333, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 52.793, mean reward: 0.528 [0.286, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.365, 10.144], loss: 0.017278, mae: 0.141887, mean_q: 19.025152
 33400/100000: episode: 334, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 56.231, mean reward: 0.562 [0.227, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.499, 10.373], loss: 0.016356, mae: 0.141862, mean_q: 19.626842
 33500/100000: episode: 335, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 49.023, mean reward: 0.490 [0.282, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.086, 10.342], loss: 0.014845, mae: 0.134131, mean_q: 19.113092
 33600/100000: episode: 336, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 54.466, mean reward: 0.545 [0.317, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.310, 10.125], loss: 0.014638, mae: 0.132754, mean_q: 19.445271
 33700/100000: episode: 337, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 56.177, mean reward: 0.562 [0.305, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.356, 10.223], loss: 0.013019, mae: 0.125145, mean_q: 19.195738
 33800/100000: episode: 338, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 53.091, mean reward: 0.531 [0.256, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.354, 10.098], loss: 0.014600, mae: 0.133337, mean_q: 19.486851
 33900/100000: episode: 339, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 50.789, mean reward: 0.508 [0.288, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.266, 10.428], loss: 0.014900, mae: 0.133701, mean_q: 19.653299
 34000/100000: episode: 340, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 51.972, mean reward: 0.520 [0.273, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.922, 10.337], loss: 0.014776, mae: 0.133549, mean_q: 19.505333
 34100/100000: episode: 341, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 56.193, mean reward: 0.562 [0.362, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.280, 10.098], loss: 0.014542, mae: 0.132622, mean_q: 19.373898
 34200/100000: episode: 342, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 48.777, mean reward: 0.488 [0.134, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.100, 10.176], loss: 0.013909, mae: 0.128999, mean_q: 19.387547
 34300/100000: episode: 343, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 52.159, mean reward: 0.522 [0.268, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.915, 10.348], loss: 0.015573, mae: 0.137396, mean_q: 19.432032
 34400/100000: episode: 344, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 53.518, mean reward: 0.535 [0.374, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.390, 10.256], loss: 0.015832, mae: 0.138819, mean_q: 18.940033
 34500/100000: episode: 345, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 55.210, mean reward: 0.552 [0.308, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.567, 10.098], loss: 0.014824, mae: 0.133944, mean_q: 19.735014
 34600/100000: episode: 346, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 44.123, mean reward: 0.441 [0.180, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.301, 10.098], loss: 0.014946, mae: 0.135634, mean_q: 19.569469
 34700/100000: episode: 347, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.990, mean reward: 0.560 [0.388, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.142, 10.098], loss: 0.018744, mae: 0.152865, mean_q: 19.665257
 34800/100000: episode: 348, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 52.661, mean reward: 0.527 [0.205, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.286, 10.124], loss: 0.015720, mae: 0.137433, mean_q: 19.556435
 34900/100000: episode: 349, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 53.527, mean reward: 0.535 [0.366, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.584, 10.098], loss: 0.014025, mae: 0.132339, mean_q: 19.238052
 35000/100000: episode: 350, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 52.695, mean reward: 0.527 [0.340, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.740, 10.098], loss: 0.015352, mae: 0.136841, mean_q: 19.584436
 35100/100000: episode: 351, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 54.884, mean reward: 0.549 [0.347, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.007, 10.154], loss: 0.013779, mae: 0.128116, mean_q: 19.495974
 35200/100000: episode: 352, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 47.285, mean reward: 0.473 [0.132, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.624, 10.184], loss: 0.014847, mae: 0.134964, mean_q: 19.634333
 35300/100000: episode: 353, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.396, mean reward: 0.574 [0.387, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.739, 10.314], loss: 0.017023, mae: 0.145898, mean_q: 19.571062
 35400/100000: episode: 354, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 54.111, mean reward: 0.541 [0.323, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.920, 10.098], loss: 0.013167, mae: 0.126096, mean_q: 19.433252
 35500/100000: episode: 355, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 48.265, mean reward: 0.483 [0.194, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.614, 10.374], loss: 0.013467, mae: 0.128899, mean_q: 19.404509
 35600/100000: episode: 356, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 52.158, mean reward: 0.522 [0.203, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.452, 10.098], loss: 0.016516, mae: 0.143452, mean_q: 19.515593
 35700/100000: episode: 357, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 54.036, mean reward: 0.540 [0.104, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.042, 10.110], loss: 0.016030, mae: 0.140298, mean_q: 19.121349
 35800/100000: episode: 358, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 46.352, mean reward: 0.464 [0.156, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.097, 10.621], loss: 0.015365, mae: 0.136325, mean_q: 19.447121
 35900/100000: episode: 359, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 52.094, mean reward: 0.521 [0.314, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.419, 10.205], loss: 0.015475, mae: 0.137565, mean_q: 19.438093
 36000/100000: episode: 360, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 44.272, mean reward: 0.443 [0.166, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.012, 10.203], loss: 0.014446, mae: 0.133223, mean_q: 19.449772
 36100/100000: episode: 361, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 50.811, mean reward: 0.508 [0.307, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.869, 10.098], loss: 0.016740, mae: 0.144024, mean_q: 19.583612
 36200/100000: episode: 362, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 53.508, mean reward: 0.535 [0.374, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.929, 10.188], loss: 0.016379, mae: 0.143465, mean_q: 19.303774
 36300/100000: episode: 363, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 56.743, mean reward: 0.567 [0.386, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.396, 10.098], loss: 0.015611, mae: 0.138337, mean_q: 19.490297
 36400/100000: episode: 364, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 56.925, mean reward: 0.569 [0.323, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.905, 10.108], loss: 0.014724, mae: 0.133656, mean_q: 19.419855
 36500/100000: episode: 365, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 52.713, mean reward: 0.527 [0.292, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.678, 10.147], loss: 0.015661, mae: 0.138550, mean_q: 19.650602
 36600/100000: episode: 366, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 55.013, mean reward: 0.550 [0.335, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.542, 10.247], loss: 0.013173, mae: 0.126520, mean_q: 19.626226
 36700/100000: episode: 367, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 54.107, mean reward: 0.541 [0.299, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.858, 10.098], loss: 0.014743, mae: 0.135522, mean_q: 19.195566
 36800/100000: episode: 368, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 56.719, mean reward: 0.567 [0.379, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.222, 10.098], loss: 0.015078, mae: 0.137356, mean_q: 19.672155
 36900/100000: episode: 369, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 55.951, mean reward: 0.560 [0.296, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.301, 10.313], loss: 0.015663, mae: 0.139351, mean_q: 19.346895
 37000/100000: episode: 370, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 53.702, mean reward: 0.537 [0.372, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.990, 10.098], loss: 0.012621, mae: 0.124541, mean_q: 19.483274
 37100/100000: episode: 371, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 53.419, mean reward: 0.534 [0.264, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.822, 10.098], loss: 0.014753, mae: 0.133299, mean_q: 19.359102
 37200/100000: episode: 372, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 52.405, mean reward: 0.524 [0.307, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.816, 10.236], loss: 0.016252, mae: 0.137237, mean_q: 19.357216
 37300/100000: episode: 373, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 54.577, mean reward: 0.546 [0.302, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.834, 10.105], loss: 0.014113, mae: 0.129771, mean_q: 19.429546
 37400/100000: episode: 374, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 53.638, mean reward: 0.536 [0.240, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.004, 10.098], loss: 0.012896, mae: 0.126995, mean_q: 19.470823
 37500/100000: episode: 375, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 53.291, mean reward: 0.533 [0.338, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.723, 10.098], loss: 0.016880, mae: 0.144138, mean_q: 19.774719
 37600/100000: episode: 376, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 47.303, mean reward: 0.473 [0.097, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.721, 10.098], loss: 0.014505, mae: 0.133059, mean_q: 19.927155
 37700/100000: episode: 377, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 56.347, mean reward: 0.563 [0.384, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.736, 10.149], loss: 0.016077, mae: 0.140730, mean_q: 19.711462
 37800/100000: episode: 378, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 54.577, mean reward: 0.546 [0.357, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.508, 10.098], loss: 0.016165, mae: 0.140061, mean_q: 19.688683
 37900/100000: episode: 379, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 52.817, mean reward: 0.528 [0.340, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.641, 10.354], loss: 0.012828, mae: 0.124289, mean_q: 19.440542
 38000/100000: episode: 380, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 53.534, mean reward: 0.535 [0.145, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.925, 10.197], loss: 0.015439, mae: 0.137422, mean_q: 19.488174
 38100/100000: episode: 381, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 49.339, mean reward: 0.493 [0.274, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.925, 10.352], loss: 0.016297, mae: 0.141317, mean_q: 19.529209
 38200/100000: episode: 382, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 55.728, mean reward: 0.557 [0.348, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.205, 10.098], loss: 0.014927, mae: 0.132937, mean_q: 19.473120
 38300/100000: episode: 383, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 55.430, mean reward: 0.554 [0.268, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.920, 10.217], loss: 0.015521, mae: 0.137807, mean_q: 19.749676
 38400/100000: episode: 384, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 55.390, mean reward: 0.554 [0.378, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.115], loss: 0.016082, mae: 0.137765, mean_q: 19.420837
 38500/100000: episode: 385, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 46.006, mean reward: 0.460 [0.228, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.847, 10.098], loss: 0.015382, mae: 0.135726, mean_q: 19.540279
 38600/100000: episode: 386, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.661, mean reward: 0.567 [0.341, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.427, 10.098], loss: 0.015844, mae: 0.136822, mean_q: 19.764046
 38700/100000: episode: 387, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.095, mean reward: 0.571 [0.292, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.056, 10.126], loss: 0.015587, mae: 0.136253, mean_q: 19.588213
 38800/100000: episode: 388, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 51.073, mean reward: 0.511 [0.177, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.861, 10.177], loss: 0.017932, mae: 0.144056, mean_q: 19.317242
 38900/100000: episode: 389, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 54.582, mean reward: 0.546 [0.274, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.519, 10.156], loss: 0.016755, mae: 0.138677, mean_q: 19.599155
 39000/100000: episode: 390, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 53.186, mean reward: 0.532 [0.287, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.989, 10.098], loss: 0.017344, mae: 0.141644, mean_q: 19.641006
 39100/100000: episode: 391, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 53.083, mean reward: 0.531 [0.367, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.704, 10.334], loss: 0.017434, mae: 0.141677, mean_q: 19.530245
 39200/100000: episode: 392, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 54.978, mean reward: 0.550 [0.291, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.675, 10.098], loss: 0.016679, mae: 0.135400, mean_q: 19.512032
 39300/100000: episode: 393, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 47.320, mean reward: 0.473 [0.301, 0.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.756, 10.324], loss: 0.018793, mae: 0.148770, mean_q: 19.596481
 39400/100000: episode: 394, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 52.700, mean reward: 0.527 [0.249, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.386, 10.331], loss: 0.017417, mae: 0.138026, mean_q: 19.663639
 39500/100000: episode: 395, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 54.768, mean reward: 0.548 [0.399, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.556, 10.170], loss: 0.019610, mae: 0.146276, mean_q: 19.532305
 39600/100000: episode: 396, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 47.310, mean reward: 0.473 [0.273, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.881, 10.342], loss: 0.018764, mae: 0.146543, mean_q: 19.572353
 39700/100000: episode: 397, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 51.962, mean reward: 0.520 [0.303, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.681, 10.098], loss: 0.018488, mae: 0.144621, mean_q: 19.637367
 39800/100000: episode: 398, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 50.216, mean reward: 0.502 [0.289, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.552, 10.401], loss: 0.020102, mae: 0.148143, mean_q: 19.218630
 39900/100000: episode: 399, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 54.241, mean reward: 0.542 [0.298, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.693, 10.098], loss: 0.019998, mae: 0.146998, mean_q: 19.655088
 40000/100000: episode: 400, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 54.277, mean reward: 0.543 [0.324, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.597, 10.405], loss: 0.023545, mae: 0.160723, mean_q: 19.666708
 40100/100000: episode: 401, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 54.085, mean reward: 0.541 [0.269, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.579, 10.098], loss: 0.022088, mae: 0.150226, mean_q: 19.722176
 40200/100000: episode: 402, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 41.357, mean reward: 0.414 [0.239, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.828, 10.314], loss: 0.022280, mae: 0.150825, mean_q: 19.584934
 40300/100000: episode: 403, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 55.783, mean reward: 0.558 [0.348, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.863, 10.203], loss: 0.019168, mae: 0.139483, mean_q: 19.582705
 40400/100000: episode: 404, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.032, mean reward: 0.530 [0.246, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.565, 10.226], loss: 0.022035, mae: 0.151983, mean_q: 19.601700
 40500/100000: episode: 405, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 51.824, mean reward: 0.518 [0.160, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.615, 10.311], loss: 0.025477, mae: 0.154713, mean_q: 19.839703
 40600/100000: episode: 406, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 48.474, mean reward: 0.485 [0.246, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.393, 10.098], loss: 0.020816, mae: 0.141053, mean_q: 19.689268
 40700/100000: episode: 407, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 55.780, mean reward: 0.558 [0.340, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.483, 10.199], loss: 0.029601, mae: 0.172754, mean_q: 19.403809
 40800/100000: episode: 408, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 50.381, mean reward: 0.504 [0.279, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.204, 10.412], loss: 0.025165, mae: 0.154014, mean_q: 19.494528
 40900/100000: episode: 409, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 55.067, mean reward: 0.551 [0.358, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.010, 10.102], loss: 0.026224, mae: 0.160324, mean_q: 19.566328
 41000/100000: episode: 410, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 56.690, mean reward: 0.567 [0.331, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.081, 10.187], loss: 0.027419, mae: 0.165016, mean_q: 19.742630
 41100/100000: episode: 411, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 53.328, mean reward: 0.533 [0.285, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.705, 10.098], loss: 0.023585, mae: 0.153096, mean_q: 19.524061
 41200/100000: episode: 412, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 51.699, mean reward: 0.517 [0.155, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.976, 10.187], loss: 0.025634, mae: 0.161314, mean_q: 19.397316
 41300/100000: episode: 413, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 53.489, mean reward: 0.535 [0.300, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.209, 10.098], loss: 0.026036, mae: 0.161973, mean_q: 19.693386
 41400/100000: episode: 414, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 54.740, mean reward: 0.547 [0.272, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.783, 10.175], loss: 0.024905, mae: 0.153133, mean_q: 19.582773
 41500/100000: episode: 415, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.505, mean reward: 0.535 [0.314, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.406, 10.118], loss: 0.022793, mae: 0.144959, mean_q: 19.393745
 41600/100000: episode: 416, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 55.544, mean reward: 0.555 [0.290, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.411, 10.098], loss: 0.023728, mae: 0.151430, mean_q: 19.320387
 41700/100000: episode: 417, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 51.077, mean reward: 0.511 [0.267, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.508, 10.098], loss: 0.022842, mae: 0.151525, mean_q: 19.528263
 41800/100000: episode: 418, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.783, mean reward: 0.578 [0.246, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.776, 10.098], loss: 0.021869, mae: 0.148357, mean_q: 19.257463
 41900/100000: episode: 419, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 53.543, mean reward: 0.535 [0.304, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.897, 10.098], loss: 0.018465, mae: 0.138318, mean_q: 19.810692
 42000/100000: episode: 420, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 52.731, mean reward: 0.527 [0.306, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.488, 10.184], loss: 0.021218, mae: 0.150562, mean_q: 19.314112
 42100/100000: episode: 421, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 49.096, mean reward: 0.491 [0.333, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.217, 10.098], loss: 0.019502, mae: 0.141637, mean_q: 19.418707
 42200/100000: episode: 422, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 55.514, mean reward: 0.555 [0.349, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.878, 10.285], loss: 0.019099, mae: 0.143661, mean_q: 19.426472
 42300/100000: episode: 423, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 54.055, mean reward: 0.541 [0.340, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.204, 10.339], loss: 0.019423, mae: 0.146609, mean_q: 19.645039
 42400/100000: episode: 424, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 55.156, mean reward: 0.552 [0.317, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.813, 10.114], loss: 0.020107, mae: 0.149575, mean_q: 19.242657
 42500/100000: episode: 425, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 53.325, mean reward: 0.533 [0.252, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.485, 10.098], loss: 0.018061, mae: 0.140683, mean_q: 19.629610
 42600/100000: episode: 426, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 55.675, mean reward: 0.557 [0.382, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.880, 10.259], loss: 0.017249, mae: 0.133244, mean_q: 19.243265
 42700/100000: episode: 427, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 54.000, mean reward: 0.540 [0.312, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.270, 10.244], loss: 0.017558, mae: 0.138603, mean_q: 19.307255
 42800/100000: episode: 428, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 52.231, mean reward: 0.522 [0.256, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.526, 10.240], loss: 0.017501, mae: 0.139673, mean_q: 19.537134
 42900/100000: episode: 429, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 52.113, mean reward: 0.521 [0.271, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.695, 10.268], loss: 0.016103, mae: 0.131387, mean_q: 19.592699
 43000/100000: episode: 430, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 54.318, mean reward: 0.543 [0.253, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.665, 10.098], loss: 0.019182, mae: 0.145933, mean_q: 19.666224
 43100/100000: episode: 431, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 53.156, mean reward: 0.532 [0.271, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.345, 10.148], loss: 0.016752, mae: 0.137422, mean_q: 19.349430
 43200/100000: episode: 432, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 52.958, mean reward: 0.530 [0.341, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.945, 10.098], loss: 0.014692, mae: 0.128425, mean_q: 19.455664
 43300/100000: episode: 433, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 50.529, mean reward: 0.505 [0.309, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.795, 10.098], loss: 0.016664, mae: 0.140075, mean_q: 19.365049
 43400/100000: episode: 434, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 49.634, mean reward: 0.496 [0.254, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.667, 10.362], loss: 0.017791, mae: 0.144706, mean_q: 19.588909
 43500/100000: episode: 435, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 55.689, mean reward: 0.557 [0.310, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.635, 10.136], loss: 0.014969, mae: 0.132012, mean_q: 19.195971
 43600/100000: episode: 436, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 55.376, mean reward: 0.554 [0.284, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.950, 10.185], loss: 0.016202, mae: 0.137519, mean_q: 19.535120
 43700/100000: episode: 437, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 55.264, mean reward: 0.553 [0.344, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.798, 10.116], loss: 0.019290, mae: 0.150125, mean_q: 19.584532
 43800/100000: episode: 438, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 52.056, mean reward: 0.521 [0.240, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.566, 10.175], loss: 0.016948, mae: 0.143077, mean_q: 19.652868
 43900/100000: episode: 439, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 50.852, mean reward: 0.509 [0.319, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.743, 10.269], loss: 0.014446, mae: 0.134146, mean_q: 19.693731
 44000/100000: episode: 440, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 52.866, mean reward: 0.529 [0.291, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.599, 10.303], loss: 0.016487, mae: 0.139524, mean_q: 19.530106
 44100/100000: episode: 441, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 55.039, mean reward: 0.550 [0.386, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.732, 10.381], loss: 0.014550, mae: 0.132457, mean_q: 19.513809
 44200/100000: episode: 442, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 53.612, mean reward: 0.536 [0.290, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.369, 10.098], loss: 0.015106, mae: 0.134919, mean_q: 19.654247
 44300/100000: episode: 443, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 53.239, mean reward: 0.532 [0.302, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.879, 10.098], loss: 0.014121, mae: 0.131915, mean_q: 19.729679
 44400/100000: episode: 444, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 45.600, mean reward: 0.456 [0.133, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.509, 10.308], loss: 0.013011, mae: 0.126284, mean_q: 19.312057
 44500/100000: episode: 445, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 53.511, mean reward: 0.535 [0.258, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.540, 10.107], loss: 0.014363, mae: 0.131971, mean_q: 19.366314
 44600/100000: episode: 446, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 47.944, mean reward: 0.479 [0.276, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.708, 10.395], loss: 0.016692, mae: 0.140847, mean_q: 19.528473
 44700/100000: episode: 447, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 55.427, mean reward: 0.554 [0.246, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.244], loss: 0.016533, mae: 0.139536, mean_q: 19.372744
 44800/100000: episode: 448, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 55.792, mean reward: 0.558 [0.338, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.020, 10.098], loss: 0.015950, mae: 0.138302, mean_q: 19.492954
 44900/100000: episode: 449, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.484, mean reward: 0.575 [0.376, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.312, 10.098], loss: 0.015603, mae: 0.133591, mean_q: 19.378914
 45000/100000: episode: 450, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 56.911, mean reward: 0.569 [0.299, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.709, 10.106], loss: 0.014791, mae: 0.133879, mean_q: 19.418047
 45100/100000: episode: 451, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 53.755, mean reward: 0.538 [0.337, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.436, 10.294], loss: 0.013311, mae: 0.125437, mean_q: 19.623978
 45200/100000: episode: 452, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 51.107, mean reward: 0.511 [0.242, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.700, 10.306], loss: 0.019762, mae: 0.149229, mean_q: 19.425917
 45300/100000: episode: 453, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 53.453, mean reward: 0.535 [0.256, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.813, 10.169], loss: 0.014686, mae: 0.129404, mean_q: 19.704466
 45400/100000: episode: 454, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 53.425, mean reward: 0.534 [0.287, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.896, 10.098], loss: 0.013134, mae: 0.124603, mean_q: 19.462442
 45500/100000: episode: 455, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 55.434, mean reward: 0.554 [0.330, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.031, 10.098], loss: 0.013615, mae: 0.126057, mean_q: 19.629219
 45600/100000: episode: 456, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.034, mean reward: 0.560 [0.345, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.677, 10.098], loss: 0.014634, mae: 0.130055, mean_q: 19.407240
 45700/100000: episode: 457, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 53.468, mean reward: 0.535 [0.343, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.002, 10.098], loss: 0.014769, mae: 0.132426, mean_q: 19.852734
 45800/100000: episode: 458, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 53.543, mean reward: 0.535 [0.326, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.560, 10.200], loss: 0.014638, mae: 0.130654, mean_q: 19.162758
 45900/100000: episode: 459, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 56.001, mean reward: 0.560 [0.344, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.587, 10.191], loss: 0.012200, mae: 0.121939, mean_q: 19.618574
 46000/100000: episode: 460, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 52.923, mean reward: 0.529 [0.323, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.543, 10.152], loss: 0.013448, mae: 0.123117, mean_q: 19.672121
 46100/100000: episode: 461, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 51.545, mean reward: 0.515 [0.243, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.869, 10.232], loss: 0.014653, mae: 0.131346, mean_q: 19.706404
 46200/100000: episode: 462, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 48.494, mean reward: 0.485 [0.200, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.666, 10.098], loss: 0.014090, mae: 0.127203, mean_q: 19.470831
 46300/100000: episode: 463, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 54.450, mean reward: 0.545 [0.270, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.266, 10.295], loss: 0.012916, mae: 0.120270, mean_q: 19.089184
 46400/100000: episode: 464, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 51.216, mean reward: 0.512 [0.178, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.017, 10.098], loss: 0.013097, mae: 0.123207, mean_q: 19.169491
 46500/100000: episode: 465, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 54.139, mean reward: 0.541 [0.130, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.481, 10.186], loss: 0.014535, mae: 0.128159, mean_q: 19.643131
 46600/100000: episode: 466, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 46.132, mean reward: 0.461 [0.266, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.415, 10.098], loss: 0.015514, mae: 0.135140, mean_q: 19.560827
 46700/100000: episode: 467, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 53.045, mean reward: 0.530 [0.354, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.631, 10.143], loss: 0.014980, mae: 0.133062, mean_q: 19.698301
 46800/100000: episode: 468, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 56.005, mean reward: 0.560 [0.356, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.832, 10.098], loss: 0.013746, mae: 0.126443, mean_q: 19.819201
 46900/100000: episode: 469, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 52.212, mean reward: 0.522 [0.205, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.385, 10.342], loss: 0.014848, mae: 0.130104, mean_q: 19.497347
 47000/100000: episode: 470, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.273, mean reward: 0.573 [0.336, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.204, 10.105], loss: 0.014711, mae: 0.130625, mean_q: 19.591143
 47100/100000: episode: 471, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 51.929, mean reward: 0.519 [0.152, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.939, 10.282], loss: 0.015995, mae: 0.137564, mean_q: 19.320259
 47200/100000: episode: 472, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 50.030, mean reward: 0.500 [0.348, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.292, 10.293], loss: 0.014032, mae: 0.128703, mean_q: 19.406113
 47300/100000: episode: 473, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 53.793, mean reward: 0.538 [0.291, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.268, 10.098], loss: 0.013630, mae: 0.126990, mean_q: 19.585289
 47400/100000: episode: 474, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.878, mean reward: 0.539 [0.395, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.189, 10.098], loss: 0.015805, mae: 0.134294, mean_q: 19.499138
 47500/100000: episode: 475, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 49.979, mean reward: 0.500 [0.204, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.149, 10.250], loss: 0.013831, mae: 0.123877, mean_q: 19.536295
 47600/100000: episode: 476, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 51.090, mean reward: 0.511 [0.274, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.354, 10.118], loss: 0.012635, mae: 0.122836, mean_q: 19.899681
 47700/100000: episode: 477, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 53.544, mean reward: 0.535 [0.352, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.851, 10.098], loss: 0.013162, mae: 0.124489, mean_q: 19.464197
 47800/100000: episode: 478, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 54.417, mean reward: 0.544 [0.367, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.450, 10.308], loss: 0.017044, mae: 0.141263, mean_q: 19.429518
 47900/100000: episode: 479, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 53.102, mean reward: 0.531 [0.218, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.747, 10.098], loss: 0.017951, mae: 0.141400, mean_q: 19.446753
 48000/100000: episode: 480, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 48.227, mean reward: 0.482 [0.260, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.758, 10.159], loss: 0.014955, mae: 0.129952, mean_q: 19.466892
 48100/100000: episode: 481, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.063, mean reward: 0.571 [0.379, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.507, 10.120], loss: 0.013106, mae: 0.120535, mean_q: 19.854435
 48200/100000: episode: 482, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 52.958, mean reward: 0.530 [0.328, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.747, 10.123], loss: 0.014438, mae: 0.127798, mean_q: 19.087210
 48300/100000: episode: 483, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 51.624, mean reward: 0.516 [0.320, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.714, 10.178], loss: 0.016940, mae: 0.141496, mean_q: 19.619041
 48400/100000: episode: 484, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 56.102, mean reward: 0.561 [0.301, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.603, 10.098], loss: 0.013904, mae: 0.127492, mean_q: 19.436342
 48500/100000: episode: 485, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 54.130, mean reward: 0.541 [0.233, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.594, 10.098], loss: 0.013199, mae: 0.125108, mean_q: 19.368633
 48600/100000: episode: 486, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 54.184, mean reward: 0.542 [0.318, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.007, 10.154], loss: 0.014860, mae: 0.128929, mean_q: 19.473862
 48700/100000: episode: 487, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 45.735, mean reward: 0.457 [0.155, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.968, 10.098], loss: 0.013901, mae: 0.127652, mean_q: 19.389938
 48800/100000: episode: 488, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 53.583, mean reward: 0.536 [0.286, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.470, 10.098], loss: 0.015001, mae: 0.133456, mean_q: 19.641991
 48900/100000: episode: 489, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.027, mean reward: 0.560 [0.325, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.753, 10.098], loss: 0.013942, mae: 0.128688, mean_q: 19.374809
 49000/100000: episode: 490, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 50.625, mean reward: 0.506 [0.259, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.777, 10.269], loss: 0.013898, mae: 0.124441, mean_q: 19.442226
 49100/100000: episode: 491, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.654, mean reward: 0.557 [0.387, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.858, 10.182], loss: 0.014667, mae: 0.129942, mean_q: 19.911963
 49200/100000: episode: 492, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 56.660, mean reward: 0.567 [0.291, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.258, 10.189], loss: 0.013504, mae: 0.125307, mean_q: 19.668098
 49300/100000: episode: 493, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 53.851, mean reward: 0.539 [0.317, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.801, 10.296], loss: 0.013530, mae: 0.123467, mean_q: 19.455717
 49400/100000: episode: 494, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 52.461, mean reward: 0.525 [0.343, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.759, 10.335], loss: 0.014735, mae: 0.130299, mean_q: 19.737808
 49500/100000: episode: 495, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 52.284, mean reward: 0.523 [0.273, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.702, 10.098], loss: 0.013850, mae: 0.124384, mean_q: 19.697498
 49600/100000: episode: 496, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 51.933, mean reward: 0.519 [0.289, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.214, 10.152], loss: 0.013205, mae: 0.122673, mean_q: 19.392599
 49700/100000: episode: 497, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 52.575, mean reward: 0.526 [0.268, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.443, 10.130], loss: 0.012891, mae: 0.121255, mean_q: 19.514463
 49800/100000: episode: 498, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 48.943, mean reward: 0.489 [0.163, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.697, 10.098], loss: 0.013017, mae: 0.122284, mean_q: 19.604132
 49900/100000: episode: 499, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 55.138, mean reward: 0.551 [0.321, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.511, 10.239], loss: 0.012873, mae: 0.121645, mean_q: 19.628115
 50000/100000: episode: 500, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 54.720, mean reward: 0.547 [0.346, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.210, 10.098], loss: 0.016810, mae: 0.140498, mean_q: 19.450232
 50100/100000: episode: 501, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 55.254, mean reward: 0.553 [0.279, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.520, 10.143], loss: 0.013099, mae: 0.121876, mean_q: 19.591442
 50200/100000: episode: 502, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 54.731, mean reward: 0.547 [0.305, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.196, 10.123], loss: 0.013306, mae: 0.126934, mean_q: 19.446871
 50300/100000: episode: 503, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 54.324, mean reward: 0.543 [0.335, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.918, 10.098], loss: 0.013028, mae: 0.122807, mean_q: 19.535828
 50400/100000: episode: 504, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 52.791, mean reward: 0.528 [0.304, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.183, 10.098], loss: 0.012367, mae: 0.119915, mean_q: 19.507565
 50500/100000: episode: 505, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 54.824, mean reward: 0.548 [0.283, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.937, 10.146], loss: 0.012834, mae: 0.121956, mean_q: 19.312811
 50600/100000: episode: 506, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 52.247, mean reward: 0.522 [0.307, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.841, 10.098], loss: 0.012661, mae: 0.121893, mean_q: 19.585281
 50700/100000: episode: 507, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 47.493, mean reward: 0.475 [0.193, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.506, 10.098], loss: 0.012823, mae: 0.120247, mean_q: 19.746552
 50800/100000: episode: 508, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 53.462, mean reward: 0.535 [0.320, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.212, 10.252], loss: 0.013831, mae: 0.127113, mean_q: 19.815487
 50900/100000: episode: 509, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 54.507, mean reward: 0.545 [0.384, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.850, 10.098], loss: 0.011807, mae: 0.117971, mean_q: 19.618097
 51000/100000: episode: 510, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 56.318, mean reward: 0.563 [0.320, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.619, 10.136], loss: 0.011883, mae: 0.119169, mean_q: 19.426933
 51100/100000: episode: 511, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 54.644, mean reward: 0.546 [0.307, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.162, 10.098], loss: 0.012617, mae: 0.123611, mean_q: 19.440338
 51200/100000: episode: 512, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 50.888, mean reward: 0.509 [0.238, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.597, 10.277], loss: 0.013195, mae: 0.124121, mean_q: 19.404644
 51300/100000: episode: 513, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 48.853, mean reward: 0.489 [0.226, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.389, 10.317], loss: 0.013660, mae: 0.126781, mean_q: 19.395475
 51400/100000: episode: 514, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 53.687, mean reward: 0.537 [0.219, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.249, 10.197], loss: 0.012090, mae: 0.119331, mean_q: 19.676462
 51500/100000: episode: 515, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 54.376, mean reward: 0.544 [0.357, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.358, 10.226], loss: 0.015484, mae: 0.133650, mean_q: 19.268381
 51600/100000: episode: 516, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 53.526, mean reward: 0.535 [0.241, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.833, 10.098], loss: 0.012722, mae: 0.122291, mean_q: 19.441517
 51700/100000: episode: 517, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 53.223, mean reward: 0.532 [0.250, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.751, 10.206], loss: 0.014737, mae: 0.132823, mean_q: 19.649284
 51800/100000: episode: 518, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 52.259, mean reward: 0.523 [0.245, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.544, 10.175], loss: 0.013719, mae: 0.125754, mean_q: 19.652058
 51900/100000: episode: 519, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 55.203, mean reward: 0.552 [0.246, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.295, 10.098], loss: 0.014073, mae: 0.126245, mean_q: 19.135134
 52000/100000: episode: 520, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 55.235, mean reward: 0.552 [0.284, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.365, 10.098], loss: 0.013869, mae: 0.125357, mean_q: 19.325630
 52100/100000: episode: 521, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 55.656, mean reward: 0.557 [0.196, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.947, 10.098], loss: 0.014247, mae: 0.125737, mean_q: 19.511629
 52200/100000: episode: 522, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 56.284, mean reward: 0.563 [0.286, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.001, 10.098], loss: 0.015882, mae: 0.133454, mean_q: 19.710817
 52300/100000: episode: 523, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 53.962, mean reward: 0.540 [0.299, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.419, 10.317], loss: 0.011984, mae: 0.117158, mean_q: 19.563288
 52400/100000: episode: 524, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 52.525, mean reward: 0.525 [0.200, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.358, 10.098], loss: 0.011684, mae: 0.117421, mean_q: 19.718792
 52500/100000: episode: 525, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 55.310, mean reward: 0.553 [0.273, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.094, 10.098], loss: 0.014255, mae: 0.128296, mean_q: 19.765354
 52600/100000: episode: 526, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 55.969, mean reward: 0.560 [0.331, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.728, 10.152], loss: 0.016098, mae: 0.134483, mean_q: 19.774439
 52700/100000: episode: 527, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 50.785, mean reward: 0.508 [0.263, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.887, 10.098], loss: 0.014244, mae: 0.126990, mean_q: 19.463531
 52800/100000: episode: 528, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 56.197, mean reward: 0.562 [0.334, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.744, 10.168], loss: 0.012793, mae: 0.120484, mean_q: 19.563080
 52900/100000: episode: 529, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 55.213, mean reward: 0.552 [0.354, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.549, 10.200], loss: 0.013372, mae: 0.120311, mean_q: 19.899555
 53000/100000: episode: 530, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 51.212, mean reward: 0.512 [0.317, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.875, 10.098], loss: 0.013235, mae: 0.119082, mean_q: 19.493256
 53100/100000: episode: 531, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 53.622, mean reward: 0.536 [0.378, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.887, 10.112], loss: 0.012033, mae: 0.116679, mean_q: 19.672750
 53200/100000: episode: 532, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 51.040, mean reward: 0.510 [0.338, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.966, 10.098], loss: 0.012363, mae: 0.115117, mean_q: 19.617573
 53300/100000: episode: 533, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 54.663, mean reward: 0.547 [0.238, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.532, 10.191], loss: 0.015027, mae: 0.125526, mean_q: 19.864840
 53400/100000: episode: 534, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 54.287, mean reward: 0.543 [0.274, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.849, 10.234], loss: 0.014465, mae: 0.123486, mean_q: 19.504725
 53500/100000: episode: 535, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 53.029, mean reward: 0.530 [0.276, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.879, 10.214], loss: 0.015612, mae: 0.131554, mean_q: 19.374939
 53600/100000: episode: 536, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 53.022, mean reward: 0.530 [0.272, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.955, 10.098], loss: 0.013081, mae: 0.122189, mean_q: 19.554169
 53700/100000: episode: 537, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 48.104, mean reward: 0.481 [0.183, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.261, 10.458], loss: 0.014686, mae: 0.123498, mean_q: 19.905415
 53800/100000: episode: 538, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 51.295, mean reward: 0.513 [0.272, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.998, 10.481], loss: 0.015476, mae: 0.131790, mean_q: 19.526169
 53900/100000: episode: 539, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 51.454, mean reward: 0.515 [0.233, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.150, 10.334], loss: 0.015062, mae: 0.126417, mean_q: 19.613300
 54000/100000: episode: 540, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 53.187, mean reward: 0.532 [0.280, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.759, 10.274], loss: 0.016965, mae: 0.136107, mean_q: 19.978619
 54100/100000: episode: 541, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 56.527, mean reward: 0.565 [0.359, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.494, 10.145], loss: 0.014475, mae: 0.120793, mean_q: 19.902647
 54200/100000: episode: 542, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.106, mean reward: 0.571 [0.388, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.816, 10.098], loss: 0.015426, mae: 0.125910, mean_q: 19.686735
 54300/100000: episode: 543, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 54.454, mean reward: 0.545 [0.170, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.626, 10.098], loss: 0.011520, mae: 0.114232, mean_q: 19.900476
 54400/100000: episode: 544, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 49.228, mean reward: 0.492 [0.240, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.628, 10.523], loss: 0.012768, mae: 0.115801, mean_q: 19.366060
 54500/100000: episode: 545, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 53.973, mean reward: 0.540 [0.309, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.640, 10.098], loss: 0.014712, mae: 0.128688, mean_q: 19.658215
 54600/100000: episode: 546, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 53.950, mean reward: 0.539 [0.237, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.642, 10.129], loss: 0.019564, mae: 0.144096, mean_q: 19.543673
 54700/100000: episode: 547, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 45.247, mean reward: 0.452 [0.149, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.620, 10.098], loss: 0.017511, mae: 0.132550, mean_q: 19.635628
 54800/100000: episode: 548, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 50.484, mean reward: 0.505 [0.196, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-2.729, 10.308], loss: 0.015301, mae: 0.128338, mean_q: 19.570976
 54900/100000: episode: 549, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 54.625, mean reward: 0.546 [0.363, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.127, 10.228], loss: 0.012188, mae: 0.116801, mean_q: 19.712706
 55000/100000: episode: 550, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 55.750, mean reward: 0.557 [0.344, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.776, 10.105], loss: 0.015614, mae: 0.125719, mean_q: 19.994177
 55100/100000: episode: 551, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 53.121, mean reward: 0.531 [0.268, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.779, 10.098], loss: 0.012328, mae: 0.116545, mean_q: 19.532829
 55200/100000: episode: 552, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 54.291, mean reward: 0.543 [0.335, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.182, 10.098], loss: 0.014544, mae: 0.124587, mean_q: 19.686922
 55300/100000: episode: 553, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 52.438, mean reward: 0.524 [0.298, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.076, 10.234], loss: 0.012964, mae: 0.120927, mean_q: 19.567469
 55400/100000: episode: 554, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 50.982, mean reward: 0.510 [0.189, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.285, 10.184], loss: 0.014445, mae: 0.122075, mean_q: 19.635193
 55500/100000: episode: 555, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 55.956, mean reward: 0.560 [0.359, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.155, 10.135], loss: 0.014455, mae: 0.119777, mean_q: 19.442060
 55600/100000: episode: 556, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 53.833, mean reward: 0.538 [0.358, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.952, 10.098], loss: 0.014881, mae: 0.123732, mean_q: 19.231762
 55700/100000: episode: 557, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 52.404, mean reward: 0.524 [0.249, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.792, 10.122], loss: 0.013493, mae: 0.119895, mean_q: 19.788277
 55800/100000: episode: 558, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 45.070, mean reward: 0.451 [0.103, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.684, 10.116], loss: 0.014094, mae: 0.125296, mean_q: 19.731844
 55900/100000: episode: 559, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 56.124, mean reward: 0.561 [0.292, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.538, 10.098], loss: 0.011841, mae: 0.116644, mean_q: 20.008615
 56000/100000: episode: 560, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 48.443, mean reward: 0.484 [0.287, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.281, 10.379], loss: 0.011106, mae: 0.112808, mean_q: 19.647581
 56100/100000: episode: 561, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 54.690, mean reward: 0.547 [0.306, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.470, 10.273], loss: 0.014991, mae: 0.122228, mean_q: 19.720417
 56200/100000: episode: 562, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 49.920, mean reward: 0.499 [0.260, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.351, 10.419], loss: 0.013539, mae: 0.116205, mean_q: 19.462585
 56300/100000: episode: 563, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 49.931, mean reward: 0.499 [0.341, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.052, 10.098], loss: 0.013321, mae: 0.121340, mean_q: 19.732065
 56400/100000: episode: 564, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 52.407, mean reward: 0.524 [0.304, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.655, 10.211], loss: 0.013909, mae: 0.123180, mean_q: 19.819275
 56500/100000: episode: 565, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 49.802, mean reward: 0.498 [0.238, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.812, 10.098], loss: 0.014177, mae: 0.124146, mean_q: 19.624971
 56600/100000: episode: 566, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 58.607, mean reward: 0.586 [0.405, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.427, 10.102], loss: 0.011919, mae: 0.116452, mean_q: 19.857725
 56700/100000: episode: 567, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 52.272, mean reward: 0.523 [0.234, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.953, 10.098], loss: 0.012861, mae: 0.116257, mean_q: 19.683800
 56800/100000: episode: 568, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 56.422, mean reward: 0.564 [0.427, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.185, 10.098], loss: 0.011957, mae: 0.115141, mean_q: 19.869484
 56900/100000: episode: 569, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 55.604, mean reward: 0.556 [0.314, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.433, 10.141], loss: 0.013051, mae: 0.120245, mean_q: 19.539137
 57000/100000: episode: 570, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 55.175, mean reward: 0.552 [0.275, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.692, 10.098], loss: 0.013939, mae: 0.120315, mean_q: 19.608980
 57100/100000: episode: 571, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 55.125, mean reward: 0.551 [0.351, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.566, 10.265], loss: 0.012100, mae: 0.115559, mean_q: 19.832888
 57200/100000: episode: 572, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.630, mean reward: 0.586 [0.377, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.147, 10.268], loss: 0.011788, mae: 0.113569, mean_q: 19.902315
 57300/100000: episode: 573, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 50.356, mean reward: 0.504 [0.329, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.936, 10.098], loss: 0.013776, mae: 0.122767, mean_q: 19.846382
 57400/100000: episode: 574, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 54.680, mean reward: 0.547 [0.242, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.239, 10.218], loss: 0.014567, mae: 0.127464, mean_q: 19.754053
 57500/100000: episode: 575, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 55.057, mean reward: 0.551 [0.207, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.817, 10.250], loss: 0.012147, mae: 0.114961, mean_q: 19.498110
 57600/100000: episode: 576, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 52.484, mean reward: 0.525 [0.230, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.563, 10.293], loss: 0.014040, mae: 0.125268, mean_q: 19.622097
 57700/100000: episode: 577, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 54.881, mean reward: 0.549 [0.290, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.865, 10.098], loss: 0.015111, mae: 0.129312, mean_q: 19.607992
 57800/100000: episode: 578, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.169, mean reward: 0.552 [0.367, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.518, 10.101], loss: 0.011974, mae: 0.113288, mean_q: 19.755468
 57900/100000: episode: 579, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 53.682, mean reward: 0.537 [0.361, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.907, 10.293], loss: 0.014574, mae: 0.127158, mean_q: 19.869955
 58000/100000: episode: 580, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 50.162, mean reward: 0.502 [0.329, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.344, 10.109], loss: 0.012621, mae: 0.116868, mean_q: 19.633114
 58100/100000: episode: 581, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 50.637, mean reward: 0.506 [0.323, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.996, 10.360], loss: 0.011937, mae: 0.117120, mean_q: 19.251791
 58200/100000: episode: 582, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: 50.007, mean reward: 0.500 [0.297, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.722, 10.324], loss: 0.015477, mae: 0.129252, mean_q: 19.541693
 58300/100000: episode: 583, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 52.430, mean reward: 0.524 [0.283, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.624, 10.098], loss: 0.013687, mae: 0.121587, mean_q: 19.589790
 58400/100000: episode: 584, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 46.028, mean reward: 0.460 [0.215, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.541, 10.098], loss: 0.012896, mae: 0.118572, mean_q: 19.490381
 58500/100000: episode: 585, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 51.900, mean reward: 0.519 [0.321, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.387, 10.098], loss: 0.013943, mae: 0.119892, mean_q: 19.638149
 58600/100000: episode: 586, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 53.924, mean reward: 0.539 [0.229, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.600, 10.098], loss: 0.011071, mae: 0.113019, mean_q: 19.796402
 58700/100000: episode: 587, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 49.717, mean reward: 0.497 [0.263, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.274, 10.098], loss: 0.010785, mae: 0.110974, mean_q: 19.558357
 58800/100000: episode: 588, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 55.992, mean reward: 0.560 [0.313, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.381, 10.098], loss: 0.011962, mae: 0.118460, mean_q: 19.428755
 58900/100000: episode: 589, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 50.426, mean reward: 0.504 [0.289, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.860, 10.287], loss: 0.011111, mae: 0.114097, mean_q: 19.696730
 59000/100000: episode: 590, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 53.323, mean reward: 0.533 [0.299, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.370, 10.104], loss: 0.013538, mae: 0.121610, mean_q: 19.583122
 59100/100000: episode: 591, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 55.315, mean reward: 0.553 [0.262, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.056, 10.098], loss: 0.011344, mae: 0.115868, mean_q: 19.365498
 59200/100000: episode: 592, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 53.663, mean reward: 0.537 [0.330, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.347, 10.098], loss: 0.012083, mae: 0.115887, mean_q: 19.661932
 59300/100000: episode: 593, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 52.244, mean reward: 0.522 [0.332, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.574, 10.098], loss: 0.013359, mae: 0.124640, mean_q: 19.521183
 59400/100000: episode: 594, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 54.911, mean reward: 0.549 [0.350, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.029, 10.327], loss: 0.016903, mae: 0.138048, mean_q: 19.276388
 59500/100000: episode: 595, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 53.224, mean reward: 0.532 [0.261, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.078, 10.139], loss: 0.012701, mae: 0.120416, mean_q: 19.337152
 59600/100000: episode: 596, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 52.030, mean reward: 0.520 [0.206, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.374, 10.159], loss: 0.011615, mae: 0.115027, mean_q: 19.396589
 59700/100000: episode: 597, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 52.690, mean reward: 0.527 [0.306, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.625, 10.098], loss: 0.011334, mae: 0.114754, mean_q: 19.748760
 59800/100000: episode: 598, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 55.384, mean reward: 0.554 [0.275, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.854, 10.159], loss: 0.011906, mae: 0.117081, mean_q: 19.240715
 59900/100000: episode: 599, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 53.257, mean reward: 0.533 [0.336, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.125, 10.200], loss: 0.011933, mae: 0.115742, mean_q: 19.727421
 60000/100000: episode: 600, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 50.981, mean reward: 0.510 [0.105, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.135, 10.098], loss: 0.011789, mae: 0.117968, mean_q: 19.629934
 60100/100000: episode: 601, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 54.033, mean reward: 0.540 [0.273, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.228, 10.098], loss: 0.014555, mae: 0.128968, mean_q: 19.626787
 60200/100000: episode: 602, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 55.788, mean reward: 0.558 [0.324, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.727, 10.184], loss: 0.012586, mae: 0.120998, mean_q: 19.768532
 60300/100000: episode: 603, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 50.603, mean reward: 0.506 [0.342, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.708, 10.098], loss: 0.012079, mae: 0.115412, mean_q: 19.623974
 60400/100000: episode: 604, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 53.291, mean reward: 0.533 [0.197, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.497, 10.098], loss: 0.013732, mae: 0.125228, mean_q: 19.722576
 60500/100000: episode: 605, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 53.247, mean reward: 0.532 [0.268, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.321, 10.098], loss: 0.012298, mae: 0.116971, mean_q: 19.491594
 60600/100000: episode: 606, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 48.174, mean reward: 0.482 [0.246, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.613, 10.446], loss: 0.012090, mae: 0.117169, mean_q: 19.740124
 60700/100000: episode: 607, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 54.823, mean reward: 0.548 [0.333, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.135, 10.098], loss: 0.012718, mae: 0.119772, mean_q: 19.612015
 60800/100000: episode: 608, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 48.796, mean reward: 0.488 [0.267, 0.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.581, 10.098], loss: 0.012295, mae: 0.114897, mean_q: 19.580137
 60900/100000: episode: 609, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 54.227, mean reward: 0.542 [0.376, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.394, 10.193], loss: 0.012470, mae: 0.120219, mean_q: 19.503122
 61000/100000: episode: 610, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 45.292, mean reward: 0.453 [0.200, 0.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.170, 10.098], loss: 0.010158, mae: 0.106517, mean_q: 19.288113
 61100/100000: episode: 611, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 47.015, mean reward: 0.470 [0.146, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.615, 10.289], loss: 0.013672, mae: 0.124794, mean_q: 19.421476
 61200/100000: episode: 612, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 52.852, mean reward: 0.529 [0.268, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.902, 10.098], loss: 0.012140, mae: 0.117938, mean_q: 19.646656
 61300/100000: episode: 613, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 48.142, mean reward: 0.481 [0.199, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.260, 10.356], loss: 0.011483, mae: 0.115663, mean_q: 19.553064
 61400/100000: episode: 614, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 45.322, mean reward: 0.453 [0.221, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.736, 10.098], loss: 0.011437, mae: 0.113821, mean_q: 19.679224
 61500/100000: episode: 615, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 56.831, mean reward: 0.568 [0.309, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.687, 10.126], loss: 0.014381, mae: 0.129948, mean_q: 19.538389
 61600/100000: episode: 616, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 53.138, mean reward: 0.531 [0.338, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.793, 10.209], loss: 0.012069, mae: 0.117840, mean_q: 19.584387
 61700/100000: episode: 617, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 55.400, mean reward: 0.554 [0.385, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.767, 10.365], loss: 0.013273, mae: 0.120367, mean_q: 19.230009
 61800/100000: episode: 618, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 54.416, mean reward: 0.544 [0.292, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.769, 10.267], loss: 0.011843, mae: 0.114131, mean_q: 19.501253
 61900/100000: episode: 619, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 53.831, mean reward: 0.538 [0.246, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.128], loss: 0.012730, mae: 0.118571, mean_q: 19.543470
 62000/100000: episode: 620, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 47.093, mean reward: 0.471 [0.204, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.824, 10.098], loss: 0.013599, mae: 0.122195, mean_q: 19.835480
 62100/100000: episode: 621, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 53.988, mean reward: 0.540 [0.228, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.726, 10.098], loss: 0.013050, mae: 0.119312, mean_q: 19.209816
 62200/100000: episode: 622, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 54.324, mean reward: 0.543 [0.301, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.929, 10.098], loss: 0.011521, mae: 0.114571, mean_q: 19.457275
 62300/100000: episode: 623, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 56.010, mean reward: 0.560 [0.179, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.334, 10.245], loss: 0.013137, mae: 0.123406, mean_q: 19.589691
 62400/100000: episode: 624, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 55.457, mean reward: 0.555 [0.286, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.389, 10.098], loss: 0.012924, mae: 0.122479, mean_q: 19.496248
 62500/100000: episode: 625, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 55.340, mean reward: 0.553 [0.311, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.803, 10.174], loss: 0.013221, mae: 0.119100, mean_q: 19.397701
 62600/100000: episode: 626, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 49.766, mean reward: 0.498 [0.275, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.575, 10.287], loss: 0.014203, mae: 0.122639, mean_q: 19.409063
 62700/100000: episode: 627, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 54.962, mean reward: 0.550 [0.357, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.496, 10.175], loss: 0.011840, mae: 0.116078, mean_q: 19.268345
 62800/100000: episode: 628, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 53.125, mean reward: 0.531 [0.231, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.791, 10.098], loss: 0.013123, mae: 0.116830, mean_q: 19.807142
 62900/100000: episode: 629, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 50.263, mean reward: 0.503 [0.056, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.277, 10.534], loss: 0.013530, mae: 0.119510, mean_q: 19.535275
 63000/100000: episode: 630, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 53.387, mean reward: 0.534 [0.344, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.113, 10.278], loss: 0.012376, mae: 0.122176, mean_q: 19.502316
 63100/100000: episode: 631, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 51.701, mean reward: 0.517 [0.307, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.318, 10.251], loss: 0.013213, mae: 0.124589, mean_q: 19.514381
 63200/100000: episode: 632, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 55.477, mean reward: 0.555 [0.329, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.398, 10.098], loss: 0.013986, mae: 0.126034, mean_q: 19.054756
 63300/100000: episode: 633, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 48.396, mean reward: 0.484 [0.186, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.621, 10.098], loss: 0.013719, mae: 0.122081, mean_q: 19.211582
 63400/100000: episode: 634, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 56.158, mean reward: 0.562 [0.366, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.367, 10.314], loss: 0.014505, mae: 0.129382, mean_q: 19.725609
 63500/100000: episode: 635, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 54.116, mean reward: 0.541 [0.350, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.155, 10.257], loss: 0.011383, mae: 0.116217, mean_q: 19.594118
 63600/100000: episode: 636, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 52.345, mean reward: 0.523 [0.269, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.152, 10.255], loss: 0.012418, mae: 0.119644, mean_q: 19.417150
 63700/100000: episode: 637, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 54.358, mean reward: 0.544 [0.351, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.637, 10.143], loss: 0.012723, mae: 0.119424, mean_q: 19.551311
 63800/100000: episode: 638, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 56.076, mean reward: 0.561 [0.204, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.522, 10.178], loss: 0.011853, mae: 0.113831, mean_q: 19.167934
 63900/100000: episode: 639, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 49.576, mean reward: 0.496 [0.231, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.136, 10.258], loss: 0.016919, mae: 0.137605, mean_q: 19.507500
 64000/100000: episode: 640, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 55.234, mean reward: 0.552 [0.297, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.652, 10.098], loss: 0.013012, mae: 0.119912, mean_q: 19.678095
 64100/100000: episode: 641, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 52.195, mean reward: 0.522 [0.287, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.845, 10.112], loss: 0.013481, mae: 0.121637, mean_q: 19.614182
 64200/100000: episode: 642, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 48.093, mean reward: 0.481 [0.242, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.521, 10.323], loss: 0.012714, mae: 0.119479, mean_q: 19.478205
 64300/100000: episode: 643, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 54.876, mean reward: 0.549 [0.335, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.190, 10.140], loss: 0.012903, mae: 0.117547, mean_q: 19.583900
 64400/100000: episode: 644, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 54.621, mean reward: 0.546 [0.310, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.619, 10.098], loss: 0.012043, mae: 0.113860, mean_q: 19.403820
 64500/100000: episode: 645, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 54.575, mean reward: 0.546 [0.253, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.845, 10.098], loss: 0.012647, mae: 0.118401, mean_q: 19.606819
 64600/100000: episode: 646, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 53.528, mean reward: 0.535 [0.275, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.896, 10.239], loss: 0.012942, mae: 0.120342, mean_q: 19.274391
 64700/100000: episode: 647, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 52.546, mean reward: 0.525 [0.321, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.760, 10.168], loss: 0.012772, mae: 0.119270, mean_q: 19.438034
 64800/100000: episode: 648, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 54.156, mean reward: 0.542 [0.376, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.623, 10.098], loss: 0.011506, mae: 0.112953, mean_q: 19.584579
 64900/100000: episode: 649, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 51.152, mean reward: 0.512 [0.288, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.792, 10.195], loss: 0.015296, mae: 0.127171, mean_q: 19.395370
 65000/100000: episode: 650, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 53.182, mean reward: 0.532 [0.202, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.601, 10.207], loss: 0.013998, mae: 0.122247, mean_q: 19.586678
 65100/100000: episode: 651, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 52.132, mean reward: 0.521 [0.272, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.787, 10.248], loss: 0.014139, mae: 0.119279, mean_q: 19.253984
 65200/100000: episode: 652, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 52.400, mean reward: 0.524 [0.169, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.245, 10.098], loss: 0.013078, mae: 0.118515, mean_q: 19.364498
 65300/100000: episode: 653, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 43.878, mean reward: 0.439 [0.106, 0.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.325, 10.098], loss: 0.012567, mae: 0.117248, mean_q: 19.467306
 65400/100000: episode: 654, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 50.908, mean reward: 0.509 [0.294, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.301, 10.098], loss: 0.010314, mae: 0.109100, mean_q: 19.786119
 65500/100000: episode: 655, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 54.269, mean reward: 0.543 [0.165, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.813, 10.285], loss: 0.014162, mae: 0.125001, mean_q: 19.585037
 65600/100000: episode: 656, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 55.139, mean reward: 0.551 [0.305, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.553, 10.154], loss: 0.011607, mae: 0.117324, mean_q: 19.340416
 65700/100000: episode: 657, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 56.542, mean reward: 0.565 [0.376, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.130, 10.098], loss: 0.014257, mae: 0.126353, mean_q: 19.340567
 65800/100000: episode: 658, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 53.143, mean reward: 0.531 [0.161, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.605, 10.221], loss: 0.012205, mae: 0.116111, mean_q: 19.306665
 65900/100000: episode: 659, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 49.322, mean reward: 0.493 [0.126, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.375, 10.347], loss: 0.012393, mae: 0.119390, mean_q: 19.659100
 66000/100000: episode: 660, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 53.376, mean reward: 0.534 [0.249, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.849, 10.156], loss: 0.011514, mae: 0.112985, mean_q: 19.483885
 66100/100000: episode: 661, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 54.131, mean reward: 0.541 [0.216, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.784, 10.374], loss: 0.012041, mae: 0.114066, mean_q: 19.256102
 66200/100000: episode: 662, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 50.616, mean reward: 0.506 [0.184, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.686, 10.110], loss: 0.014303, mae: 0.129110, mean_q: 19.546251
 66300/100000: episode: 663, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 54.268, mean reward: 0.543 [0.334, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.976, 10.122], loss: 0.011750, mae: 0.116722, mean_q: 19.432947
 66400/100000: episode: 664, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 51.151, mean reward: 0.512 [0.284, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.790, 10.129], loss: 0.012270, mae: 0.116768, mean_q: 19.333040
 66500/100000: episode: 665, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 54.634, mean reward: 0.546 [0.341, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.765, 10.158], loss: 0.013175, mae: 0.118278, mean_q: 19.214882
 66600/100000: episode: 666, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 55.346, mean reward: 0.553 [0.360, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.413, 10.098], loss: 0.010863, mae: 0.110544, mean_q: 19.340082
 66700/100000: episode: 667, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 52.309, mean reward: 0.523 [0.038, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.081, 10.209], loss: 0.013702, mae: 0.120408, mean_q: 19.479206
 66800/100000: episode: 668, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 52.867, mean reward: 0.529 [0.281, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.831, 10.103], loss: 0.011931, mae: 0.116258, mean_q: 19.615625
 66900/100000: episode: 669, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 55.103, mean reward: 0.551 [0.265, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.863, 10.098], loss: 0.011112, mae: 0.111416, mean_q: 19.666998
 67000/100000: episode: 670, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 55.871, mean reward: 0.559 [0.352, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.318, 10.132], loss: 0.012013, mae: 0.115465, mean_q: 19.174574
 67100/100000: episode: 671, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 55.856, mean reward: 0.559 [0.352, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.648, 10.146], loss: 0.012726, mae: 0.121140, mean_q: 19.475039
 67200/100000: episode: 672, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.137, mean reward: 0.571 [0.355, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.431, 10.180], loss: 0.012826, mae: 0.117606, mean_q: 19.464067
 67300/100000: episode: 673, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 50.116, mean reward: 0.501 [0.229, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.422, 10.409], loss: 0.011719, mae: 0.113394, mean_q: 19.550098
 67400/100000: episode: 674, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 51.052, mean reward: 0.511 [0.256, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.260, 10.153], loss: 0.012817, mae: 0.119054, mean_q: 19.702679
 67500/100000: episode: 675, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 47.087, mean reward: 0.471 [0.275, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.253, 10.098], loss: 0.014505, mae: 0.121711, mean_q: 19.402010
 67600/100000: episode: 676, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.702, mean reward: 0.587 [0.315, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.381, 10.122], loss: 0.013347, mae: 0.115167, mean_q: 19.417700
 67700/100000: episode: 677, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 48.456, mean reward: 0.485 [0.281, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.818, 10.098], loss: 0.013655, mae: 0.120274, mean_q: 19.412451
 67800/100000: episode: 678, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 51.651, mean reward: 0.517 [0.345, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.921, 10.321], loss: 0.014288, mae: 0.124330, mean_q: 19.185385
 67900/100000: episode: 679, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 55.392, mean reward: 0.554 [0.407, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.008, 10.162], loss: 0.013608, mae: 0.119721, mean_q: 19.234959
 68000/100000: episode: 680, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 49.654, mean reward: 0.497 [0.301, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.223, 10.098], loss: 0.014391, mae: 0.124599, mean_q: 19.670315
 68100/100000: episode: 681, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 52.909, mean reward: 0.529 [0.317, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.572, 10.098], loss: 0.012263, mae: 0.113259, mean_q: 19.459833
 68200/100000: episode: 682, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 56.867, mean reward: 0.569 [0.325, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.994, 10.098], loss: 0.010898, mae: 0.111738, mean_q: 19.510038
 68300/100000: episode: 683, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 53.119, mean reward: 0.531 [0.300, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.125, 10.339], loss: 0.011424, mae: 0.110486, mean_q: 19.230972
 68400/100000: episode: 684, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 54.674, mean reward: 0.547 [0.233, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.480, 10.098], loss: 0.013017, mae: 0.116640, mean_q: 19.364067
 68500/100000: episode: 685, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 53.776, mean reward: 0.538 [0.299, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.025, 10.166], loss: 0.011549, mae: 0.111423, mean_q: 19.680195
 68600/100000: episode: 686, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 50.199, mean reward: 0.502 [0.195, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.967, 10.207], loss: 0.012479, mae: 0.115911, mean_q: 19.324024
 68700/100000: episode: 687, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 50.513, mean reward: 0.505 [0.298, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.185, 10.241], loss: 0.013482, mae: 0.118280, mean_q: 19.542618
 68800/100000: episode: 688, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 52.425, mean reward: 0.524 [0.329, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.922, 10.098], loss: 0.015282, mae: 0.128848, mean_q: 19.554176
 68900/100000: episode: 689, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 48.380, mean reward: 0.484 [0.196, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.700, 10.483], loss: 0.011420, mae: 0.109941, mean_q: 19.323742
 69000/100000: episode: 690, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.314, mean reward: 0.533 [0.360, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.197, 10.112], loss: 0.012840, mae: 0.115453, mean_q: 19.466148
 69100/100000: episode: 691, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 46.217, mean reward: 0.462 [0.198, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.234, 10.336], loss: 0.013759, mae: 0.117940, mean_q: 19.686926
 69200/100000: episode: 692, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 55.683, mean reward: 0.557 [0.397, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.674, 10.218], loss: 0.011583, mae: 0.115207, mean_q: 19.737776
 69300/100000: episode: 693, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 55.308, mean reward: 0.553 [0.308, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.360, 10.098], loss: 0.012908, mae: 0.116379, mean_q: 19.511721
 69400/100000: episode: 694, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 50.990, mean reward: 0.510 [0.252, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.104, 10.497], loss: 0.013241, mae: 0.122575, mean_q: 19.223551
 69500/100000: episode: 695, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 53.730, mean reward: 0.537 [0.270, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.444, 10.312], loss: 0.011917, mae: 0.115021, mean_q: 19.739857
 69600/100000: episode: 696, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 48.903, mean reward: 0.489 [0.053, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.911, 10.098], loss: 0.011556, mae: 0.113471, mean_q: 19.661652
 69700/100000: episode: 697, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 47.909, mean reward: 0.479 [0.241, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.753, 10.294], loss: 0.013046, mae: 0.114981, mean_q: 19.238991
 69800/100000: episode: 698, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 49.666, mean reward: 0.497 [0.291, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.636, 10.098], loss: 0.013870, mae: 0.120532, mean_q: 19.491459
 69900/100000: episode: 699, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 55.435, mean reward: 0.554 [0.391, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.657, 10.222], loss: 0.012498, mae: 0.116185, mean_q: 19.452974
 70000/100000: episode: 700, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 54.950, mean reward: 0.549 [0.329, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.523, 10.098], loss: 0.013351, mae: 0.122078, mean_q: 19.357382
 70100/100000: episode: 701, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 52.231, mean reward: 0.522 [0.327, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.816, 10.297], loss: 0.013649, mae: 0.121661, mean_q: 19.859129
 70200/100000: episode: 702, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 55.338, mean reward: 0.553 [0.252, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.075, 10.306], loss: 0.012973, mae: 0.115673, mean_q: 19.708530
 70300/100000: episode: 703, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 53.046, mean reward: 0.530 [0.325, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.103, 10.238], loss: 0.013492, mae: 0.122771, mean_q: 19.706978
 70400/100000: episode: 704, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 54.113, mean reward: 0.541 [0.281, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.930, 10.098], loss: 0.012199, mae: 0.115895, mean_q: 19.535070
 70500/100000: episode: 705, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 50.917, mean reward: 0.509 [0.292, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.135, 10.098], loss: 0.010427, mae: 0.104524, mean_q: 19.760273
 70600/100000: episode: 706, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 51.536, mean reward: 0.515 [0.329, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.506, 10.178], loss: 0.013719, mae: 0.121352, mean_q: 19.014799
 70700/100000: episode: 707, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 56.559, mean reward: 0.566 [0.335, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.542, 10.130], loss: 0.013052, mae: 0.114403, mean_q: 19.426353
 70800/100000: episode: 708, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 41.985, mean reward: 0.420 [0.225, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.628, 10.098], loss: 0.011845, mae: 0.114376, mean_q: 19.470165
 70900/100000: episode: 709, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 45.591, mean reward: 0.456 [0.221, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.333, 10.376], loss: 0.010469, mae: 0.108981, mean_q: 19.919662
 71000/100000: episode: 710, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 50.598, mean reward: 0.506 [0.306, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.294, 10.138], loss: 0.012341, mae: 0.118001, mean_q: 19.426878
 71100/100000: episode: 711, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 55.035, mean reward: 0.550 [0.278, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.955, 10.165], loss: 0.012706, mae: 0.117585, mean_q: 19.613567
 71200/100000: episode: 712, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 51.628, mean reward: 0.516 [0.314, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.464, 10.098], loss: 0.014864, mae: 0.126115, mean_q: 19.559158
 71300/100000: episode: 713, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 56.871, mean reward: 0.569 [0.353, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.395, 10.098], loss: 0.012918, mae: 0.117964, mean_q: 19.192595
 71400/100000: episode: 714, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 53.996, mean reward: 0.540 [0.314, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.184, 10.272], loss: 0.008822, mae: 0.101743, mean_q: 19.406237
 71500/100000: episode: 715, duration: 0.660s, episode steps: 100, steps per second: 151, episode reward: 55.135, mean reward: 0.551 [0.114, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.984, 10.135], loss: 0.012163, mae: 0.116877, mean_q: 19.433100
 71600/100000: episode: 716, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 54.777, mean reward: 0.548 [0.353, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.873, 10.098], loss: 0.013263, mae: 0.119522, mean_q: 19.560308
 71700/100000: episode: 717, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 52.063, mean reward: 0.521 [0.293, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.546, 10.098], loss: 0.015920, mae: 0.128331, mean_q: 19.365341
 71800/100000: episode: 718, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 51.024, mean reward: 0.510 [0.247, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.576, 10.352], loss: 0.011556, mae: 0.112115, mean_q: 19.802492
 71900/100000: episode: 719, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 55.882, mean reward: 0.559 [0.375, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.900, 10.205], loss: 0.012664, mae: 0.119711, mean_q: 19.621487
 72000/100000: episode: 720, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 54.335, mean reward: 0.543 [0.170, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.833, 10.098], loss: 0.011303, mae: 0.108878, mean_q: 19.238087
 72100/100000: episode: 721, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 54.803, mean reward: 0.548 [0.305, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.103, 10.213], loss: 0.012251, mae: 0.112624, mean_q: 19.380974
 72200/100000: episode: 722, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 54.310, mean reward: 0.543 [0.343, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.331, 10.138], loss: 0.014113, mae: 0.121889, mean_q: 19.262110
 72300/100000: episode: 723, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 42.809, mean reward: 0.428 [0.138, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.203, 10.213], loss: 0.011540, mae: 0.112818, mean_q: 19.618959
 72400/100000: episode: 724, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 52.262, mean reward: 0.523 [0.377, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.565, 10.157], loss: 0.013119, mae: 0.118367, mean_q: 19.411406
 72500/100000: episode: 725, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 57.381, mean reward: 0.574 [0.367, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.443, 10.144], loss: 0.013509, mae: 0.119853, mean_q: 19.330938
 72600/100000: episode: 726, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 55.242, mean reward: 0.552 [0.303, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.236, 10.223], loss: 0.013221, mae: 0.120439, mean_q: 19.560823
 72700/100000: episode: 727, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 48.455, mean reward: 0.485 [0.258, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.324], loss: 0.012695, mae: 0.113952, mean_q: 19.106745
 72800/100000: episode: 728, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 51.076, mean reward: 0.511 [0.322, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.409, 10.098], loss: 0.012429, mae: 0.115340, mean_q: 19.269411
 72900/100000: episode: 729, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 56.190, mean reward: 0.562 [0.387, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.767, 10.098], loss: 0.015280, mae: 0.133008, mean_q: 19.305866
 73000/100000: episode: 730, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 51.763, mean reward: 0.518 [0.296, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.508, 10.458], loss: 0.010190, mae: 0.107011, mean_q: 19.637066
 73100/100000: episode: 731, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 56.625, mean reward: 0.566 [0.317, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.665, 10.098], loss: 0.012757, mae: 0.120109, mean_q: 19.602272
 73200/100000: episode: 732, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 48.008, mean reward: 0.480 [0.248, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.277, 10.239], loss: 0.011028, mae: 0.110969, mean_q: 19.196178
 73300/100000: episode: 733, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 51.125, mean reward: 0.511 [0.319, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.660, 10.098], loss: 0.014172, mae: 0.122765, mean_q: 19.670294
 73400/100000: episode: 734, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 55.010, mean reward: 0.550 [0.238, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.513, 10.098], loss: 0.011242, mae: 0.110126, mean_q: 19.154364
 73500/100000: episode: 735, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 54.752, mean reward: 0.548 [0.393, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.368, 10.098], loss: 0.010986, mae: 0.112194, mean_q: 19.444208
 73600/100000: episode: 736, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 50.666, mean reward: 0.507 [0.298, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.912, 10.146], loss: 0.014160, mae: 0.124658, mean_q: 19.446688
 73700/100000: episode: 737, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 45.927, mean reward: 0.459 [0.178, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.310, 10.098], loss: 0.012226, mae: 0.117830, mean_q: 19.523260
 73800/100000: episode: 738, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 54.427, mean reward: 0.544 [0.244, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.868, 10.264], loss: 0.011921, mae: 0.114583, mean_q: 19.666433
 73900/100000: episode: 739, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 54.153, mean reward: 0.542 [0.209, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.873, 10.108], loss: 0.011542, mae: 0.113695, mean_q: 19.365250
 74000/100000: episode: 740, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 56.073, mean reward: 0.561 [0.348, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.194, 10.201], loss: 0.010596, mae: 0.109340, mean_q: 19.064051
 74100/100000: episode: 741, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 52.578, mean reward: 0.526 [0.321, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.333, 10.373], loss: 0.012171, mae: 0.115647, mean_q: 19.017553
 74200/100000: episode: 742, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 47.015, mean reward: 0.470 [0.170, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.969, 10.098], loss: 0.011010, mae: 0.111312, mean_q: 19.552635
 74300/100000: episode: 743, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 49.057, mean reward: 0.491 [0.148, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.953, 10.098], loss: 0.011294, mae: 0.113066, mean_q: 19.334242
 74400/100000: episode: 744, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 52.571, mean reward: 0.526 [0.261, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.978, 10.180], loss: 0.012280, mae: 0.118167, mean_q: 19.311190
 74500/100000: episode: 745, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 54.939, mean reward: 0.549 [0.290, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.946, 10.176], loss: 0.011168, mae: 0.110368, mean_q: 19.498051
 74600/100000: episode: 746, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 55.765, mean reward: 0.558 [0.249, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.598, 10.132], loss: 0.012208, mae: 0.118466, mean_q: 19.446688
 74700/100000: episode: 747, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 50.833, mean reward: 0.508 [0.313, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.707, 10.098], loss: 0.010823, mae: 0.110253, mean_q: 19.509802
 74800/100000: episode: 748, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 53.221, mean reward: 0.532 [0.270, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.455, 10.102], loss: 0.012812, mae: 0.118492, mean_q: 19.123369
 74900/100000: episode: 749, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 49.744, mean reward: 0.497 [0.201, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.603, 10.275], loss: 0.012287, mae: 0.119366, mean_q: 19.194971
 75000/100000: episode: 750, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 52.926, mean reward: 0.529 [0.307, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.666, 10.098], loss: 0.013851, mae: 0.129615, mean_q: 19.280676
 75100/100000: episode: 751, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 53.986, mean reward: 0.540 [0.254, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.222, 10.255], loss: 0.011690, mae: 0.113479, mean_q: 19.533360
 75200/100000: episode: 752, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 54.454, mean reward: 0.545 [0.323, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.117, 10.162], loss: 0.013476, mae: 0.122174, mean_q: 19.878889
 75300/100000: episode: 753, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 53.422, mean reward: 0.534 [0.312, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.127, 10.098], loss: 0.013137, mae: 0.122014, mean_q: 19.238659
 75400/100000: episode: 754, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 56.620, mean reward: 0.566 [0.352, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.322, 10.273], loss: 0.012592, mae: 0.116941, mean_q: 19.156490
 75500/100000: episode: 755, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 43.513, mean reward: 0.435 [0.173, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.332, 10.465], loss: 0.011607, mae: 0.114085, mean_q: 19.363350
 75600/100000: episode: 756, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 53.737, mean reward: 0.537 [0.134, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.035, 10.138], loss: 0.013817, mae: 0.122372, mean_q: 19.393757
 75700/100000: episode: 757, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 52.635, mean reward: 0.526 [0.298, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.712, 10.339], loss: 0.012469, mae: 0.120777, mean_q: 19.420012
[RESULT] FALSIFICATION!
 75790/100000: episode: 758, duration: 0.545s, episode steps: 90, steps per second: 165, episode reward: 37.505, mean reward: 0.417 [-10.000, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.336 [-0.986, 9.836], loss: 0.011446, mae: 0.113173, mean_q: 19.410229
 75890/100000: episode: 759, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 55.270, mean reward: 0.553 [0.260, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.889, 10.098], loss: 0.015511, mae: 0.126794, mean_q: 19.218300
 75990/100000: episode: 760, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 50.657, mean reward: 0.507 [0.297, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.832, 10.098], loss: 0.013731, mae: 0.119882, mean_q: 19.112539
 76090/100000: episode: 761, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 52.668, mean reward: 0.527 [0.291, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.343, 10.235], loss: 0.060143, mae: 0.149688, mean_q: 18.929335
 76190/100000: episode: 762, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 51.618, mean reward: 0.516 [0.284, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.263, 10.098], loss: 0.011593, mae: 0.112990, mean_q: 19.219610
 76290/100000: episode: 763, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 57.329, mean reward: 0.573 [0.390, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.412, 10.149], loss: 0.059833, mae: 0.149253, mean_q: 19.075186
 76390/100000: episode: 764, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 53.693, mean reward: 0.537 [0.237, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.098], loss: 0.012135, mae: 0.115278, mean_q: 19.332367
 76490/100000: episode: 765, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 53.747, mean reward: 0.537 [0.270, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.066, 10.098], loss: 0.011244, mae: 0.112598, mean_q: 19.129589
 76590/100000: episode: 766, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 54.941, mean reward: 0.549 [0.242, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.241, 10.154], loss: 0.011855, mae: 0.112782, mean_q: 19.341652
 76690/100000: episode: 767, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: 51.232, mean reward: 0.512 [0.278, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.704, 10.205], loss: 0.014149, mae: 0.121704, mean_q: 19.191633
 76790/100000: episode: 768, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 53.364, mean reward: 0.534 [0.279, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.620, 10.109], loss: 0.009929, mae: 0.108271, mean_q: 19.203043
 76890/100000: episode: 769, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 56.127, mean reward: 0.561 [0.384, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.308, 10.099], loss: 0.012222, mae: 0.113508, mean_q: 19.601845
 76990/100000: episode: 770, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 52.560, mean reward: 0.526 [0.306, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.660, 10.098], loss: 0.012609, mae: 0.117962, mean_q: 19.445015
 77090/100000: episode: 771, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 54.780, mean reward: 0.548 [0.371, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.625, 10.098], loss: 0.062254, mae: 0.151895, mean_q: 19.230999
 77190/100000: episode: 772, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 56.786, mean reward: 0.568 [0.381, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.233, 10.138], loss: 0.010807, mae: 0.110439, mean_q: 19.311491
 77290/100000: episode: 773, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 53.356, mean reward: 0.534 [0.297, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.399, 10.197], loss: 0.010131, mae: 0.106742, mean_q: 19.518589
 77390/100000: episode: 774, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 56.920, mean reward: 0.569 [0.197, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.905, 10.098], loss: 0.012648, mae: 0.119974, mean_q: 19.581717
 77490/100000: episode: 775, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 55.692, mean reward: 0.557 [0.365, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.911, 10.232], loss: 0.013445, mae: 0.122224, mean_q: 19.677301
 77590/100000: episode: 776, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 55.831, mean reward: 0.558 [0.327, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.959, 10.190], loss: 0.062079, mae: 0.155032, mean_q: 19.496510
 77690/100000: episode: 777, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 51.433, mean reward: 0.514 [0.334, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.830, 10.098], loss: 0.142554, mae: 0.178012, mean_q: 19.193548
 77790/100000: episode: 778, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 53.716, mean reward: 0.537 [0.220, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.812, 10.098], loss: 0.096797, mae: 0.145864, mean_q: 19.479149
 77890/100000: episode: 779, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 54.483, mean reward: 0.545 [0.307, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.243, 10.129], loss: 0.015469, mae: 0.133938, mean_q: 19.470501
 77990/100000: episode: 780, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 50.372, mean reward: 0.504 [0.338, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.355, 10.098], loss: 0.011489, mae: 0.112834, mean_q: 19.308409
 78090/100000: episode: 781, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 54.611, mean reward: 0.546 [0.323, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.705, 10.098], loss: 0.011508, mae: 0.115195, mean_q: 19.465017
 78190/100000: episode: 782, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 52.857, mean reward: 0.529 [0.179, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.438, 10.357], loss: 0.100578, mae: 0.162817, mean_q: 19.428242
 78290/100000: episode: 783, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 55.048, mean reward: 0.550 [0.296, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.127, 10.151], loss: 0.055792, mae: 0.138477, mean_q: 19.220070
 78390/100000: episode: 784, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 48.472, mean reward: 0.485 [0.297, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.407, 10.412], loss: 0.010481, mae: 0.107042, mean_q: 19.454191
 78490/100000: episode: 785, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 52.061, mean reward: 0.521 [0.324, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.232, 10.098], loss: 0.055415, mae: 0.136931, mean_q: 19.717793
 78590/100000: episode: 786, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 52.958, mean reward: 0.530 [0.333, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.323, 10.098], loss: 0.010664, mae: 0.109629, mean_q: 19.075804
 78690/100000: episode: 787, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 54.894, mean reward: 0.549 [0.330, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.814, 10.098], loss: 0.011290, mae: 0.114125, mean_q: 19.609673
 78790/100000: episode: 788, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 50.575, mean reward: 0.506 [0.276, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.843, 10.150], loss: 0.052625, mae: 0.125202, mean_q: 19.352310
 78890/100000: episode: 789, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 52.847, mean reward: 0.528 [0.294, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.136, 10.475], loss: 0.013839, mae: 0.125559, mean_q: 19.479391
 78990/100000: episode: 790, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 54.963, mean reward: 0.550 [0.322, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.182, 10.098], loss: 0.056385, mae: 0.137814, mean_q: 19.105553
 79090/100000: episode: 791, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 55.388, mean reward: 0.554 [0.342, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.556, 10.139], loss: 0.010171, mae: 0.106719, mean_q: 19.630955
 79190/100000: episode: 792, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 54.734, mean reward: 0.547 [0.300, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.256, 10.098], loss: 0.010458, mae: 0.110120, mean_q: 19.312668
 79290/100000: episode: 793, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 52.779, mean reward: 0.528 [0.311, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.261, 10.098], loss: 0.099435, mae: 0.153753, mean_q: 19.161057
 79390/100000: episode: 794, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 54.341, mean reward: 0.543 [0.272, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.439, 10.172], loss: 0.051792, mae: 0.125351, mean_q: 19.243010
 79490/100000: episode: 795, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 53.554, mean reward: 0.536 [0.257, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.313, 10.385], loss: 0.054218, mae: 0.134976, mean_q: 19.050627
 79590/100000: episode: 796, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 54.021, mean reward: 0.540 [0.244, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.314, 10.307], loss: 0.009198, mae: 0.101628, mean_q: 19.453495
 79690/100000: episode: 797, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 52.579, mean reward: 0.526 [0.288, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.627, 10.224], loss: 0.009097, mae: 0.104087, mean_q: 19.514687
 79790/100000: episode: 798, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 56.413, mean reward: 0.564 [0.373, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.645, 10.098], loss: 0.010995, mae: 0.112371, mean_q: 19.346046
 79890/100000: episode: 799, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 55.943, mean reward: 0.559 [0.319, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.237], loss: 0.098338, mae: 0.158623, mean_q: 19.395004
 79990/100000: episode: 800, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 56.805, mean reward: 0.568 [0.306, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.894, 10.098], loss: 0.012921, mae: 0.120126, mean_q: 19.324684
 80090/100000: episode: 801, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 54.932, mean reward: 0.549 [0.377, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.911, 10.236], loss: 0.050979, mae: 0.118231, mean_q: 19.558237
 80190/100000: episode: 802, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 54.294, mean reward: 0.543 [0.307, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.124, 10.098], loss: 0.011011, mae: 0.111992, mean_q: 19.354645
 80290/100000: episode: 803, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 54.062, mean reward: 0.541 [0.175, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.342, 10.167], loss: 0.056319, mae: 0.140293, mean_q: 18.977903
 80390/100000: episode: 804, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 56.206, mean reward: 0.562 [0.319, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.066, 10.098], loss: 0.008994, mae: 0.100875, mean_q: 19.931784
 80490/100000: episode: 805, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 54.274, mean reward: 0.543 [0.332, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.908, 10.102], loss: 0.055769, mae: 0.139612, mean_q: 19.499424
 80590/100000: episode: 806, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 50.414, mean reward: 0.504 [0.285, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.192, 10.482], loss: 0.008758, mae: 0.099448, mean_q: 19.663052
 80690/100000: episode: 807, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 52.623, mean reward: 0.526 [0.264, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.194, 10.220], loss: 0.008877, mae: 0.101641, mean_q: 19.614151
 80790/100000: episode: 808, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 52.862, mean reward: 0.529 [0.271, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.324, 10.098], loss: 0.008832, mae: 0.100531, mean_q: 19.579617
 80890/100000: episode: 809, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 50.937, mean reward: 0.509 [0.257, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.290, 10.098], loss: 0.010874, mae: 0.112231, mean_q: 19.363741
 80990/100000: episode: 810, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 50.968, mean reward: 0.510 [0.267, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.386, 10.098], loss: 0.008219, mae: 0.097486, mean_q: 19.399620
 81090/100000: episode: 811, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 51.019, mean reward: 0.510 [0.353, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.202, 10.098], loss: 0.008956, mae: 0.099044, mean_q: 19.453695
 81190/100000: episode: 812, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 55.289, mean reward: 0.553 [0.342, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.895, 10.214], loss: 0.009436, mae: 0.103201, mean_q: 19.741236
 81290/100000: episode: 813, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 49.406, mean reward: 0.494 [0.291, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.537, 10.315], loss: 0.009991, mae: 0.106087, mean_q: 19.654797
 81390/100000: episode: 814, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 55.837, mean reward: 0.558 [0.315, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.216, 10.098], loss: 0.009414, mae: 0.102969, mean_q: 19.333000
 81490/100000: episode: 815, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 55.707, mean reward: 0.557 [0.266, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.358, 10.102], loss: 0.009837, mae: 0.104158, mean_q: 19.567661
 81590/100000: episode: 816, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 55.261, mean reward: 0.553 [0.323, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.702, 10.098], loss: 0.010580, mae: 0.110859, mean_q: 19.708061
 81690/100000: episode: 817, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 53.594, mean reward: 0.536 [0.277, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.804, 10.210], loss: 0.008170, mae: 0.098342, mean_q: 19.482170
 81790/100000: episode: 818, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 53.592, mean reward: 0.536 [0.305, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.395, 10.172], loss: 0.009538, mae: 0.104250, mean_q: 19.712879
 81890/100000: episode: 819, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 55.681, mean reward: 0.557 [0.301, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.538, 10.098], loss: 0.011245, mae: 0.113060, mean_q: 19.857124
 81990/100000: episode: 820, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 54.708, mean reward: 0.547 [0.309, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.643, 10.103], loss: 0.010030, mae: 0.104643, mean_q: 19.159260
 82090/100000: episode: 821, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 54.084, mean reward: 0.541 [0.296, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.915, 10.098], loss: 0.008927, mae: 0.101412, mean_q: 19.821726
 82190/100000: episode: 822, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 56.116, mean reward: 0.561 [0.308, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.271, 10.098], loss: 0.011926, mae: 0.112972, mean_q: 19.514605
 82290/100000: episode: 823, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 53.157, mean reward: 0.532 [0.371, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.433, 10.201], loss: 0.009377, mae: 0.103231, mean_q: 20.065550
 82390/100000: episode: 824, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 53.719, mean reward: 0.537 [0.328, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.906, 10.098], loss: 0.011032, mae: 0.113370, mean_q: 19.582203
 82490/100000: episode: 825, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 49.083, mean reward: 0.491 [0.245, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.579, 10.300], loss: 0.011026, mae: 0.108117, mean_q: 19.787905
 82590/100000: episode: 826, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 49.644, mean reward: 0.496 [0.161, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.135, 10.297], loss: 0.010145, mae: 0.103699, mean_q: 19.637314
 82690/100000: episode: 827, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 52.851, mean reward: 0.529 [0.232, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.243, 10.125], loss: 0.010113, mae: 0.103979, mean_q: 19.827536
 82790/100000: episode: 828, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 56.118, mean reward: 0.561 [0.332, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.201, 10.098], loss: 0.010815, mae: 0.109341, mean_q: 19.985283
 82890/100000: episode: 829, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 53.821, mean reward: 0.538 [0.283, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.046, 10.098], loss: 0.011733, mae: 0.111818, mean_q: 19.977825
 82990/100000: episode: 830, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 52.679, mean reward: 0.527 [0.261, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.327, 10.098], loss: 0.011426, mae: 0.113638, mean_q: 19.649628
 83090/100000: episode: 831, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 53.015, mean reward: 0.530 [0.151, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.264, 10.275], loss: 0.011099, mae: 0.112684, mean_q: 19.784813
 83190/100000: episode: 832, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 52.274, mean reward: 0.523 [0.298, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.575, 10.394], loss: 0.010286, mae: 0.105618, mean_q: 19.932163
 83290/100000: episode: 833, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 52.272, mean reward: 0.523 [0.230, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.510, 10.164], loss: 0.012560, mae: 0.115078, mean_q: 19.513449
 83390/100000: episode: 834, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 56.178, mean reward: 0.562 [0.294, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.478, 10.128], loss: 0.010518, mae: 0.107510, mean_q: 19.500643
 83490/100000: episode: 835, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 54.301, mean reward: 0.543 [0.250, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.134], loss: 0.009961, mae: 0.107311, mean_q: 19.412077
 83590/100000: episode: 836, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 46.643, mean reward: 0.466 [0.216, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.634, 10.143], loss: 0.010542, mae: 0.108076, mean_q: 19.915081
 83690/100000: episode: 837, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 49.738, mean reward: 0.497 [0.313, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.961, 10.223], loss: 0.012045, mae: 0.111969, mean_q: 19.483635
 83790/100000: episode: 838, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 52.159, mean reward: 0.522 [0.334, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.692, 10.098], loss: 0.011104, mae: 0.111876, mean_q: 19.453381
 83890/100000: episode: 839, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.417, mean reward: 0.564 [0.359, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.774, 10.149], loss: 0.009501, mae: 0.103651, mean_q: 19.703302
 83990/100000: episode: 840, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 52.354, mean reward: 0.524 [0.114, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.914, 10.098], loss: 0.010032, mae: 0.107377, mean_q: 19.728050
 84090/100000: episode: 841, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 54.293, mean reward: 0.543 [0.324, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.972, 10.098], loss: 0.009542, mae: 0.105004, mean_q: 19.811556
 84190/100000: episode: 842, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 54.621, mean reward: 0.546 [0.361, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.608, 10.343], loss: 0.011040, mae: 0.111825, mean_q: 19.978876
 84290/100000: episode: 843, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 54.249, mean reward: 0.542 [0.340, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.563, 10.098], loss: 0.009077, mae: 0.102279, mean_q: 19.678373
 84390/100000: episode: 844, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 55.381, mean reward: 0.554 [0.275, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.705, 10.328], loss: 0.011451, mae: 0.114066, mean_q: 19.559402
 84490/100000: episode: 845, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 52.138, mean reward: 0.521 [0.322, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.152, 10.217], loss: 0.011282, mae: 0.111758, mean_q: 19.771639
 84590/100000: episode: 846, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 52.291, mean reward: 0.523 [0.255, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.831, 10.098], loss: 0.010441, mae: 0.109580, mean_q: 19.728186
 84690/100000: episode: 847, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 52.842, mean reward: 0.528 [0.269, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.806, 10.098], loss: 0.009842, mae: 0.103483, mean_q: 19.859451
 84790/100000: episode: 848, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 52.679, mean reward: 0.527 [0.208, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.509, 10.098], loss: 0.009732, mae: 0.105306, mean_q: 19.784126
 84890/100000: episode: 849, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 52.438, mean reward: 0.524 [0.293, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.835, 10.098], loss: 0.008730, mae: 0.101724, mean_q: 19.801102
 84990/100000: episode: 850, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 51.979, mean reward: 0.520 [0.275, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.193, 10.189], loss: 0.009443, mae: 0.103500, mean_q: 19.712204
 85090/100000: episode: 851, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 54.888, mean reward: 0.549 [0.384, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.775, 10.098], loss: 0.008763, mae: 0.099556, mean_q: 19.674957
 85190/100000: episode: 852, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 53.813, mean reward: 0.538 [0.323, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.055, 10.171], loss: 0.010357, mae: 0.109016, mean_q: 19.897554
 85290/100000: episode: 853, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 55.089, mean reward: 0.551 [0.296, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.781, 10.098], loss: 0.011218, mae: 0.114321, mean_q: 19.703682
 85390/100000: episode: 854, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 52.013, mean reward: 0.520 [0.263, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.724, 10.175], loss: 0.011325, mae: 0.114220, mean_q: 19.700926
 85490/100000: episode: 855, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 55.132, mean reward: 0.551 [0.333, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.567, 10.098], loss: 0.009545, mae: 0.104953, mean_q: 19.571169
 85590/100000: episode: 856, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 54.605, mean reward: 0.546 [0.281, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.784, 10.158], loss: 0.009368, mae: 0.102537, mean_q: 19.544718
 85690/100000: episode: 857, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 55.149, mean reward: 0.551 [0.186, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.812, 10.098], loss: 0.008122, mae: 0.097406, mean_q: 19.743614
 85790/100000: episode: 858, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 52.830, mean reward: 0.528 [0.291, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.030, 10.225], loss: 0.010698, mae: 0.110531, mean_q: 19.853765
 85890/100000: episode: 859, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 53.726, mean reward: 0.537 [0.190, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.445, 10.107], loss: 0.009543, mae: 0.103776, mean_q: 19.833988
 85990/100000: episode: 860, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 54.549, mean reward: 0.545 [0.224, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.729, 10.112], loss: 0.010666, mae: 0.108985, mean_q: 19.600487
 86090/100000: episode: 861, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 54.166, mean reward: 0.542 [0.313, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.435, 10.237], loss: 0.010039, mae: 0.107432, mean_q: 19.894157
 86190/100000: episode: 862, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 55.190, mean reward: 0.552 [0.365, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.659, 10.098], loss: 0.009265, mae: 0.102545, mean_q: 19.926155
 86290/100000: episode: 863, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 56.078, mean reward: 0.561 [0.335, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.452, 10.334], loss: 0.010703, mae: 0.110158, mean_q: 19.445246
 86390/100000: episode: 864, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 49.733, mean reward: 0.497 [0.241, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.305, 10.233], loss: 0.013040, mae: 0.120173, mean_q: 19.869148
 86490/100000: episode: 865, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.339, mean reward: 0.563 [0.390, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.627, 10.190], loss: 0.011046, mae: 0.111112, mean_q: 19.821024
 86590/100000: episode: 866, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 52.357, mean reward: 0.524 [0.308, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.912, 10.402], loss: 0.010307, mae: 0.110529, mean_q: 19.901386
 86690/100000: episode: 867, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 50.897, mean reward: 0.509 [0.265, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.928, 10.291], loss: 0.011374, mae: 0.111879, mean_q: 19.525862
 86790/100000: episode: 868, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 52.463, mean reward: 0.525 [0.297, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.470, 10.098], loss: 0.010626, mae: 0.108862, mean_q: 20.046785
 86890/100000: episode: 869, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 53.928, mean reward: 0.539 [0.290, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.955, 10.143], loss: 0.010553, mae: 0.109553, mean_q: 19.673294
 86990/100000: episode: 870, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 55.044, mean reward: 0.550 [0.388, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.704, 10.098], loss: 0.009314, mae: 0.101315, mean_q: 19.977530
 87090/100000: episode: 871, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 52.387, mean reward: 0.524 [0.294, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.193, 10.291], loss: 0.009679, mae: 0.103146, mean_q: 19.783159
 87190/100000: episode: 872, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 56.506, mean reward: 0.565 [0.327, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.618, 10.098], loss: 0.009193, mae: 0.101118, mean_q: 19.744116
 87290/100000: episode: 873, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 50.003, mean reward: 0.500 [0.267, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.617, 10.098], loss: 0.009114, mae: 0.101940, mean_q: 19.987610
 87390/100000: episode: 874, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 54.732, mean reward: 0.547 [0.229, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.855, 10.193], loss: 0.009392, mae: 0.105148, mean_q: 19.680626
 87490/100000: episode: 875, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 46.992, mean reward: 0.470 [0.222, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.621, 10.394], loss: 0.009004, mae: 0.102526, mean_q: 19.874468
 87590/100000: episode: 876, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 57.709, mean reward: 0.577 [0.305, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.673, 10.281], loss: 0.011054, mae: 0.110147, mean_q: 19.900713
 87690/100000: episode: 877, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 53.384, mean reward: 0.534 [0.274, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.024, 10.144], loss: 0.011782, mae: 0.117787, mean_q: 19.878242
 87790/100000: episode: 878, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 52.521, mean reward: 0.525 [0.250, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.935, 10.098], loss: 0.008965, mae: 0.100173, mean_q: 19.842934
 87890/100000: episode: 879, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 56.089, mean reward: 0.561 [0.237, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.879, 10.098], loss: 0.009845, mae: 0.106704, mean_q: 19.862673
 87990/100000: episode: 880, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 50.721, mean reward: 0.507 [0.297, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.409 [-1.354, 10.098], loss: 0.010313, mae: 0.107164, mean_q: 19.996117
 88090/100000: episode: 881, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 52.584, mean reward: 0.526 [0.370, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.032, 10.280], loss: 0.009319, mae: 0.102813, mean_q: 19.856123
 88190/100000: episode: 882, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 54.455, mean reward: 0.545 [0.254, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.090, 10.098], loss: 0.009262, mae: 0.101716, mean_q: 19.728661
 88290/100000: episode: 883, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 47.396, mean reward: 0.474 [0.251, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.566, 10.098], loss: 0.010027, mae: 0.104898, mean_q: 19.704855
 88390/100000: episode: 884, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 54.555, mean reward: 0.546 [0.300, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.767, 10.098], loss: 0.010313, mae: 0.108639, mean_q: 19.640675
 88490/100000: episode: 885, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 52.208, mean reward: 0.522 [0.273, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.813, 10.328], loss: 0.009570, mae: 0.104454, mean_q: 19.797264
 88590/100000: episode: 886, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 53.675, mean reward: 0.537 [0.303, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.732, 10.098], loss: 0.009775, mae: 0.105315, mean_q: 19.934069
 88690/100000: episode: 887, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 55.211, mean reward: 0.552 [0.326, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.284, 10.098], loss: 0.008949, mae: 0.102457, mean_q: 19.951733
 88790/100000: episode: 888, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 51.741, mean reward: 0.517 [0.284, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.444, 10.305], loss: 0.011682, mae: 0.116405, mean_q: 19.580252
 88890/100000: episode: 889, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 57.693, mean reward: 0.577 [0.401, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.764, 10.098], loss: 0.010432, mae: 0.110340, mean_q: 19.783546
 88990/100000: episode: 890, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 49.993, mean reward: 0.500 [0.150, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.371, 10.098], loss: 0.009947, mae: 0.105676, mean_q: 19.750891
 89090/100000: episode: 891, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 52.296, mean reward: 0.523 [0.287, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.514, 10.287], loss: 0.010636, mae: 0.110600, mean_q: 19.965332
 89190/100000: episode: 892, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 51.626, mean reward: 0.516 [0.309, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.401, 10.441], loss: 0.010342, mae: 0.108330, mean_q: 19.725151
 89290/100000: episode: 893, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 53.487, mean reward: 0.535 [0.327, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.993, 10.186], loss: 0.011304, mae: 0.113422, mean_q: 19.654146
 89390/100000: episode: 894, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 52.116, mean reward: 0.521 [0.298, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.573, 10.235], loss: 0.008926, mae: 0.101688, mean_q: 19.751026
 89490/100000: episode: 895, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 56.284, mean reward: 0.563 [0.347, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.906, 10.098], loss: 0.011237, mae: 0.115778, mean_q: 19.827225
 89590/100000: episode: 896, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 53.257, mean reward: 0.533 [0.288, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.239, 10.098], loss: 0.011198, mae: 0.114342, mean_q: 20.094843
 89690/100000: episode: 897, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 47.130, mean reward: 0.471 [0.173, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.876, 10.446], loss: 0.010071, mae: 0.108720, mean_q: 19.606123
 89790/100000: episode: 898, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 51.050, mean reward: 0.511 [0.349, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.084, 10.356], loss: 0.009171, mae: 0.103930, mean_q: 19.851831
 89890/100000: episode: 899, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 51.749, mean reward: 0.517 [0.269, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.209, 10.272], loss: 0.008982, mae: 0.102065, mean_q: 19.931713
 89990/100000: episode: 900, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 48.767, mean reward: 0.488 [0.274, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.328, 10.286], loss: 0.010872, mae: 0.111502, mean_q: 20.021755
 90090/100000: episode: 901, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 55.568, mean reward: 0.556 [0.391, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.429, 10.303], loss: 0.009266, mae: 0.103337, mean_q: 19.775354
 90190/100000: episode: 902, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 53.267, mean reward: 0.533 [0.233, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.689, 10.101], loss: 0.011670, mae: 0.116120, mean_q: 19.771500
 90290/100000: episode: 903, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 52.428, mean reward: 0.524 [0.245, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.833, 10.147], loss: 0.010640, mae: 0.113350, mean_q: 20.107668
 90390/100000: episode: 904, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 53.977, mean reward: 0.540 [0.375, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.202, 10.098], loss: 0.009907, mae: 0.106734, mean_q: 20.005884
 90490/100000: episode: 905, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 53.535, mean reward: 0.535 [0.286, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.404, 10.098], loss: 0.011272, mae: 0.114766, mean_q: 19.741308
 90590/100000: episode: 906, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 47.248, mean reward: 0.472 [0.230, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.915, 10.524], loss: 0.009169, mae: 0.105129, mean_q: 19.842226
 90690/100000: episode: 907, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 53.675, mean reward: 0.537 [0.171, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.567, 10.317], loss: 0.008690, mae: 0.102014, mean_q: 19.821476
 90790/100000: episode: 908, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 56.892, mean reward: 0.569 [0.233, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.438, 10.126], loss: 0.008311, mae: 0.099032, mean_q: 19.878294
 90890/100000: episode: 909, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 57.828, mean reward: 0.578 [0.268, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.715, 10.205], loss: 0.009317, mae: 0.105328, mean_q: 19.766729
 90990/100000: episode: 910, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 54.643, mean reward: 0.546 [0.220, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.539, 10.225], loss: 0.010512, mae: 0.112265, mean_q: 19.776592
 91090/100000: episode: 911, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.155, mean reward: 0.572 [0.337, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.423, 10.098], loss: 0.011216, mae: 0.114742, mean_q: 19.667635
 91190/100000: episode: 912, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 53.529, mean reward: 0.535 [0.267, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.070, 10.118], loss: 0.008269, mae: 0.100713, mean_q: 19.647455
 91290/100000: episode: 913, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 56.171, mean reward: 0.562 [0.225, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.525, 10.142], loss: 0.007946, mae: 0.097155, mean_q: 19.762276
 91390/100000: episode: 914, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 52.254, mean reward: 0.523 [0.241, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.798, 10.098], loss: 0.010113, mae: 0.110938, mean_q: 20.122393
 91490/100000: episode: 915, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 54.856, mean reward: 0.549 [0.347, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.185, 10.158], loss: 0.008890, mae: 0.103704, mean_q: 19.851219
 91590/100000: episode: 916, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 52.686, mean reward: 0.527 [0.264, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.590, 10.098], loss: 0.008972, mae: 0.105044, mean_q: 19.803217
 91690/100000: episode: 917, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 50.745, mean reward: 0.507 [0.290, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.950, 10.224], loss: 0.009553, mae: 0.106646, mean_q: 19.973986
 91790/100000: episode: 918, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 53.577, mean reward: 0.536 [0.268, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.539, 10.167], loss: 0.010389, mae: 0.113035, mean_q: 19.303881
 91890/100000: episode: 919, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 56.129, mean reward: 0.561 [0.415, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.119, 10.098], loss: 0.009767, mae: 0.107553, mean_q: 19.494074
 91990/100000: episode: 920, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 55.660, mean reward: 0.557 [0.315, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.101, 10.135], loss: 0.010616, mae: 0.112294, mean_q: 19.983658
 92090/100000: episode: 921, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 53.910, mean reward: 0.539 [0.325, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.635, 10.254], loss: 0.007774, mae: 0.097090, mean_q: 19.664038
 92190/100000: episode: 922, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 54.553, mean reward: 0.546 [0.293, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.003, 10.287], loss: 0.009208, mae: 0.104439, mean_q: 19.646887
 92290/100000: episode: 923, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 53.230, mean reward: 0.532 [0.315, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.305, 10.098], loss: 0.010356, mae: 0.112074, mean_q: 19.884584
 92390/100000: episode: 924, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 54.033, mean reward: 0.540 [0.288, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.449, 10.297], loss: 0.009239, mae: 0.104383, mean_q: 19.933962
 92490/100000: episode: 925, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.187, mean reward: 0.572 [0.330, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.715, 10.098], loss: 0.011101, mae: 0.117906, mean_q: 19.526518
 92590/100000: episode: 926, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 53.344, mean reward: 0.533 [0.308, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.509, 10.246], loss: 0.010966, mae: 0.115366, mean_q: 19.959944
 92690/100000: episode: 927, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 52.958, mean reward: 0.530 [0.339, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.501, 10.235], loss: 0.009109, mae: 0.105231, mean_q: 19.475630
 92790/100000: episode: 928, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 49.191, mean reward: 0.492 [0.236, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.638, 10.098], loss: 0.008169, mae: 0.098847, mean_q: 19.723858
 92890/100000: episode: 929, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 48.826, mean reward: 0.488 [0.250, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.895, 10.279], loss: 0.008869, mae: 0.103799, mean_q: 20.037901
 92990/100000: episode: 930, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 40.892, mean reward: 0.409 [0.204, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.556, 10.422], loss: 0.009991, mae: 0.110738, mean_q: 19.737164
 93090/100000: episode: 931, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 51.623, mean reward: 0.516 [0.306, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.599, 10.211], loss: 0.007798, mae: 0.097114, mean_q: 19.717768
 93190/100000: episode: 932, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 48.561, mean reward: 0.486 [0.278, 0.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.182, 10.098], loss: 0.009528, mae: 0.105649, mean_q: 19.823109
 93290/100000: episode: 933, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 51.331, mean reward: 0.513 [0.307, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.810, 10.125], loss: 0.009258, mae: 0.104521, mean_q: 19.616587
 93390/100000: episode: 934, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 54.378, mean reward: 0.544 [0.358, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.423, 10.354], loss: 0.007818, mae: 0.097416, mean_q: 19.651546
 93490/100000: episode: 935, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 53.504, mean reward: 0.535 [0.313, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.102, 10.098], loss: 0.008655, mae: 0.101838, mean_q: 19.642630
 93590/100000: episode: 936, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 55.948, mean reward: 0.559 [0.278, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.114, 10.176], loss: 0.007825, mae: 0.097949, mean_q: 19.795649
 93690/100000: episode: 937, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 54.137, mean reward: 0.541 [0.261, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.356, 10.258], loss: 0.008933, mae: 0.103720, mean_q: 19.744936
 93790/100000: episode: 938, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 54.721, mean reward: 0.547 [0.277, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.615, 10.098], loss: 0.008291, mae: 0.099175, mean_q: 19.866657
 93890/100000: episode: 939, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 56.393, mean reward: 0.564 [0.324, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.100, 10.193], loss: 0.008553, mae: 0.101197, mean_q: 19.470230
 93990/100000: episode: 940, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 53.558, mean reward: 0.536 [0.225, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.316, 10.376], loss: 0.008840, mae: 0.103594, mean_q: 19.972839
 94090/100000: episode: 941, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 50.544, mean reward: 0.505 [0.274, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.363, 10.514], loss: 0.008298, mae: 0.099164, mean_q: 19.425957
 94190/100000: episode: 942, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 54.953, mean reward: 0.550 [0.390, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.932, 10.098], loss: 0.008954, mae: 0.102945, mean_q: 19.414417
 94290/100000: episode: 943, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 53.404, mean reward: 0.534 [0.348, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.345, 10.138], loss: 0.008354, mae: 0.100998, mean_q: 19.795191
 94390/100000: episode: 944, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 53.472, mean reward: 0.535 [0.334, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.861, 10.124], loss: 0.009884, mae: 0.107960, mean_q: 19.478447
 94490/100000: episode: 945, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 55.320, mean reward: 0.553 [0.211, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.464, 10.173], loss: 0.008083, mae: 0.098862, mean_q: 19.478909
 94590/100000: episode: 946, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 51.634, mean reward: 0.516 [0.263, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.461, 10.098], loss: 0.008155, mae: 0.097452, mean_q: 19.750616
 94690/100000: episode: 947, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 53.239, mean reward: 0.532 [0.288, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.961, 10.307], loss: 0.009204, mae: 0.105176, mean_q: 19.732119
 94790/100000: episode: 948, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 54.161, mean reward: 0.542 [0.275, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.931, 10.098], loss: 0.008931, mae: 0.103516, mean_q: 19.764492
 94890/100000: episode: 949, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 54.712, mean reward: 0.547 [0.244, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.713, 10.221], loss: 0.009092, mae: 0.103770, mean_q: 19.453665
 94990/100000: episode: 950, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 52.359, mean reward: 0.524 [0.273, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.346, 10.098], loss: 0.010440, mae: 0.110116, mean_q: 19.418612
 95090/100000: episode: 951, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 53.662, mean reward: 0.537 [0.358, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.841, 10.098], loss: 0.008330, mae: 0.099735, mean_q: 19.699373
 95190/100000: episode: 952, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 52.867, mean reward: 0.529 [0.301, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.764, 10.098], loss: 0.008443, mae: 0.100306, mean_q: 19.440540
 95290/100000: episode: 953, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 50.478, mean reward: 0.505 [0.230, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.739, 10.470], loss: 0.007599, mae: 0.096503, mean_q: 19.635111
 95390/100000: episode: 954, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 49.189, mean reward: 0.492 [0.127, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.115, 10.098], loss: 0.008703, mae: 0.102764, mean_q: 19.687288
 95490/100000: episode: 955, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 55.607, mean reward: 0.556 [0.216, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.345, 10.098], loss: 0.009125, mae: 0.105036, mean_q: 19.533928
 95590/100000: episode: 956, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 52.400, mean reward: 0.524 [0.331, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.292, 10.386], loss: 0.009895, mae: 0.108328, mean_q: 19.291595
 95690/100000: episode: 957, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 47.923, mean reward: 0.479 [0.280, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.564, 10.316], loss: 0.008815, mae: 0.102513, mean_q: 19.663616
 95790/100000: episode: 958, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 51.596, mean reward: 0.516 [0.294, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.792, 10.098], loss: 0.008516, mae: 0.099562, mean_q: 19.644905
 95890/100000: episode: 959, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 56.474, mean reward: 0.565 [0.287, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.935, 10.098], loss: 0.009865, mae: 0.109296, mean_q: 19.398611
 95990/100000: episode: 960, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 51.393, mean reward: 0.514 [0.267, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.454, 10.098], loss: 0.008093, mae: 0.098368, mean_q: 19.742912
 96090/100000: episode: 961, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 52.625, mean reward: 0.526 [0.306, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.064, 10.280], loss: 0.008093, mae: 0.098745, mean_q: 19.850903
 96190/100000: episode: 962, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 51.682, mean reward: 0.517 [0.229, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.331, 10.098], loss: 0.009926, mae: 0.110409, mean_q: 19.595152
 96290/100000: episode: 963, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 53.227, mean reward: 0.532 [0.276, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.395, 10.098], loss: 0.008417, mae: 0.101157, mean_q: 19.430716
 96390/100000: episode: 964, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 53.805, mean reward: 0.538 [0.087, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.773, 10.106], loss: 0.009066, mae: 0.105479, mean_q: 19.750776
 96490/100000: episode: 965, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 53.833, mean reward: 0.538 [0.185, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.686, 10.124], loss: 0.008877, mae: 0.103311, mean_q: 19.725929
 96590/100000: episode: 966, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 50.184, mean reward: 0.502 [0.218, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.098], loss: 0.008916, mae: 0.103500, mean_q: 19.469656
 96690/100000: episode: 967, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 52.940, mean reward: 0.529 [0.289, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.251, 10.098], loss: 0.009845, mae: 0.109136, mean_q: 19.568220
 96790/100000: episode: 968, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 50.353, mean reward: 0.504 [0.294, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.038, 10.098], loss: 0.009207, mae: 0.106414, mean_q: 19.481535
 96890/100000: episode: 969, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 54.318, mean reward: 0.543 [0.234, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.372, 10.098], loss: 0.008126, mae: 0.098812, mean_q: 19.469692
 96990/100000: episode: 970, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 47.105, mean reward: 0.471 [0.286, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.560, 10.098], loss: 0.008449, mae: 0.099895, mean_q: 19.675558
 97090/100000: episode: 971, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 56.352, mean reward: 0.564 [0.295, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.196, 10.122], loss: 0.008395, mae: 0.100269, mean_q: 19.473112
 97190/100000: episode: 972, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 53.183, mean reward: 0.532 [0.288, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.248, 10.439], loss: 0.009362, mae: 0.106173, mean_q: 19.564377
 97290/100000: episode: 973, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 56.160, mean reward: 0.562 [0.280, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.825, 10.240], loss: 0.009340, mae: 0.105553, mean_q: 19.479534
 97390/100000: episode: 974, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 49.983, mean reward: 0.500 [0.202, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.880, 10.278], loss: 0.010330, mae: 0.112888, mean_q: 19.445284
 97490/100000: episode: 975, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 50.992, mean reward: 0.510 [0.256, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.912, 10.098], loss: 0.009224, mae: 0.107289, mean_q: 19.515150
 97590/100000: episode: 976, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 51.603, mean reward: 0.516 [0.288, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.132, 10.098], loss: 0.010343, mae: 0.112804, mean_q: 19.625118
 97690/100000: episode: 977, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 53.769, mean reward: 0.538 [0.299, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.426, 10.211], loss: 0.009620, mae: 0.108807, mean_q: 19.556959
 97790/100000: episode: 978, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 54.472, mean reward: 0.545 [0.360, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.098], loss: 0.009174, mae: 0.105781, mean_q: 19.448347
 97890/100000: episode: 979, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 52.114, mean reward: 0.521 [0.250, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.091, 10.266], loss: 0.010305, mae: 0.110970, mean_q: 19.616831
 97990/100000: episode: 980, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 55.140, mean reward: 0.551 [0.331, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.660, 10.289], loss: 0.008852, mae: 0.102991, mean_q: 19.690260
 98090/100000: episode: 981, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 51.268, mean reward: 0.513 [0.200, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.258, 10.167], loss: 0.010022, mae: 0.110024, mean_q: 19.772970
 98190/100000: episode: 982, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 55.592, mean reward: 0.556 [0.289, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.398, 10.098], loss: 0.008274, mae: 0.099826, mean_q: 19.574387
 98290/100000: episode: 983, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 53.862, mean reward: 0.539 [0.348, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.690, 10.098], loss: 0.010457, mae: 0.112056, mean_q: 19.353289
 98390/100000: episode: 984, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 56.291, mean reward: 0.563 [0.366, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.395, 10.216], loss: 0.010144, mae: 0.109932, mean_q: 19.320892
 98490/100000: episode: 985, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 51.407, mean reward: 0.514 [0.272, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.685, 10.098], loss: 0.008030, mae: 0.098263, mean_q: 19.464819
 98590/100000: episode: 986, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 52.464, mean reward: 0.525 [0.308, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.758, 10.108], loss: 0.009530, mae: 0.108210, mean_q: 19.905624
 98690/100000: episode: 987, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 55.755, mean reward: 0.558 [0.345, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.499, 10.214], loss: 0.009347, mae: 0.106352, mean_q: 19.391169
 98790/100000: episode: 988, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 52.504, mean reward: 0.525 [0.324, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.557, 10.098], loss: 0.010154, mae: 0.109369, mean_q: 19.146978
 98890/100000: episode: 989, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 53.140, mean reward: 0.531 [0.178, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.859, 10.292], loss: 0.011702, mae: 0.119077, mean_q: 19.569372
 98990/100000: episode: 990, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 54.592, mean reward: 0.546 [0.245, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.835, 10.213], loss: 0.009448, mae: 0.107023, mean_q: 19.558279
 99090/100000: episode: 991, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 54.262, mean reward: 0.543 [0.265, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.678, 10.098], loss: 0.009303, mae: 0.105298, mean_q: 19.825994
 99190/100000: episode: 992, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 48.364, mean reward: 0.484 [0.216, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.084, 10.098], loss: 0.009385, mae: 0.106829, mean_q: 19.776266
 99290/100000: episode: 993, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 50.707, mean reward: 0.507 [0.262, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.950, 10.098], loss: 0.008910, mae: 0.104134, mean_q: 19.443171
 99390/100000: episode: 994, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 52.454, mean reward: 0.525 [0.391, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.676, 10.098], loss: 0.008592, mae: 0.101606, mean_q: 19.514687
 99490/100000: episode: 995, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 50.351, mean reward: 0.504 [0.325, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.735, 10.098], loss: 0.010100, mae: 0.111312, mean_q: 19.678900
 99590/100000: episode: 996, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 56.987, mean reward: 0.570 [0.292, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.614, 10.219], loss: 0.008865, mae: 0.104385, mean_q: 19.636377
 99690/100000: episode: 997, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 49.493, mean reward: 0.495 [0.177, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.601, 10.098], loss: 0.008993, mae: 0.104537, mean_q: 19.158569
 99790/100000: episode: 998, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 50.722, mean reward: 0.507 [0.263, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.653, 10.367], loss: 0.007893, mae: 0.097132, mean_q: 19.488073
 99890/100000: episode: 999, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 51.533, mean reward: 0.515 [0.261, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.708, 10.098], loss: 0.008392, mae: 0.099352, mean_q: 19.338394
 99990/100000: episode: 1000, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 45.828, mean reward: 0.458 [0.246, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.130, 10.282], loss: 0.008740, mae: 0.103336, mean_q: 19.102903
done, took 562.089 seconds
[Info] End Uniform Random Simulation.
