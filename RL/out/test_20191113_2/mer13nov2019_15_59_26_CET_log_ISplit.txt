Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.164s, episode steps: 100, steps per second: 611, episode reward: 58.328, mean reward: 0.583 [0.507, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.494, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.062s, episode steps: 100, steps per second: 1600, episode reward: 59.813, mean reward: 0.598 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.008, 10.377], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.068s, episode steps: 100, steps per second: 1469, episode reward: 58.724, mean reward: 0.587 [0.503, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.292, 10.369], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.071s, episode steps: 100, steps per second: 1401, episode reward: 59.042, mean reward: 0.590 [0.506, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.690, 10.371], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.073s, episode steps: 100, steps per second: 1366, episode reward: 57.588, mean reward: 0.576 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.015, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.069s, episode steps: 100, steps per second: 1446, episode reward: 56.536, mean reward: 0.565 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.866, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 61.219, mean reward: 0.612 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.524, 10.491], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.062s, episode steps: 100, steps per second: 1610, episode reward: 60.469, mean reward: 0.605 [0.507, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.411, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.061s, episode steps: 100, steps per second: 1630, episode reward: 59.097, mean reward: 0.591 [0.507, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.880, 10.269], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.062s, episode steps: 100, steps per second: 1622, episode reward: 59.491, mean reward: 0.595 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.336, 10.127], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 57.693, mean reward: 0.577 [0.501, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.443, 10.250], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.062s, episode steps: 100, steps per second: 1607, episode reward: 57.968, mean reward: 0.580 [0.513, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.817, 10.340], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.062s, episode steps: 100, steps per second: 1610, episode reward: 57.331, mean reward: 0.573 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.891, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.078s, episode steps: 100, steps per second: 1284, episode reward: 61.797, mean reward: 0.618 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.883, 10.184], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.063s, episode steps: 100, steps per second: 1594, episode reward: 59.138, mean reward: 0.591 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.470, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.075s, episode steps: 100, steps per second: 1329, episode reward: 58.573, mean reward: 0.586 [0.504, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.112, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.074s, episode steps: 100, steps per second: 1344, episode reward: 61.285, mean reward: 0.613 [0.509, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.464, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.093s, episode steps: 100, steps per second: 1075, episode reward: 60.137, mean reward: 0.601 [0.501, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.049, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.063s, episode steps: 100, steps per second: 1599, episode reward: 58.768, mean reward: 0.588 [0.499, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.601, 10.182], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.062s, episode steps: 100, steps per second: 1600, episode reward: 59.161, mean reward: 0.592 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.472, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.080s, episode steps: 100, steps per second: 1249, episode reward: 58.691, mean reward: 0.587 [0.511, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.053, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.093s, episode steps: 100, steps per second: 1070, episode reward: 56.889, mean reward: 0.569 [0.505, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.910, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.062s, episode steps: 100, steps per second: 1603, episode reward: 59.926, mean reward: 0.599 [0.501, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.068, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.063s, episode steps: 100, steps per second: 1595, episode reward: 59.068, mean reward: 0.591 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.653, 10.177], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.062s, episode steps: 100, steps per second: 1620, episode reward: 57.743, mean reward: 0.577 [0.503, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.223, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.072s, episode steps: 100, steps per second: 1382, episode reward: 57.453, mean reward: 0.575 [0.498, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.216, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.069s, episode steps: 100, steps per second: 1447, episode reward: 60.264, mean reward: 0.603 [0.517, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.526, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.062s, episode steps: 100, steps per second: 1613, episode reward: 58.620, mean reward: 0.586 [0.512, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.622, 10.316], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.063s, episode steps: 100, steps per second: 1595, episode reward: 59.325, mean reward: 0.593 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.136, 10.226], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.075s, episode steps: 100, steps per second: 1331, episode reward: 64.314, mean reward: 0.643 [0.501, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.808, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.064s, episode steps: 100, steps per second: 1557, episode reward: 56.798, mean reward: 0.568 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.361, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.062s, episode steps: 100, steps per second: 1604, episode reward: 57.872, mean reward: 0.579 [0.509, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.419, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.062s, episode steps: 100, steps per second: 1603, episode reward: 58.653, mean reward: 0.587 [0.506, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.177, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.070s, episode steps: 100, steps per second: 1428, episode reward: 58.058, mean reward: 0.581 [0.499, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.095, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.071s, episode steps: 100, steps per second: 1409, episode reward: 57.702, mean reward: 0.577 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.272, 10.260], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.072s, episode steps: 100, steps per second: 1385, episode reward: 58.837, mean reward: 0.588 [0.508, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.206, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.063s, episode steps: 100, steps per second: 1588, episode reward: 60.176, mean reward: 0.602 [0.507, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.161, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.068s, episode steps: 100, steps per second: 1464, episode reward: 58.659, mean reward: 0.587 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.522, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.075s, episode steps: 100, steps per second: 1338, episode reward: 59.920, mean reward: 0.599 [0.510, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.735, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 57.125, mean reward: 0.571 [0.505, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.798, 10.213], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.078s, episode steps: 100, steps per second: 1282, episode reward: 55.624, mean reward: 0.556 [0.499, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.261, 10.099], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.070s, episode steps: 100, steps per second: 1419, episode reward: 57.657, mean reward: 0.577 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.676, 10.380], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.068s, episode steps: 100, steps per second: 1477, episode reward: 58.215, mean reward: 0.582 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.917, 10.194], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.073s, episode steps: 100, steps per second: 1375, episode reward: 64.160, mean reward: 0.642 [0.506, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.001, 10.465], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.063s, episode steps: 100, steps per second: 1599, episode reward: 56.465, mean reward: 0.565 [0.498, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.811, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.076s, episode steps: 100, steps per second: 1316, episode reward: 58.589, mean reward: 0.586 [0.507, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.160, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.064s, episode steps: 100, steps per second: 1555, episode reward: 57.758, mean reward: 0.578 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.982, 10.277], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.070s, episode steps: 100, steps per second: 1420, episode reward: 58.705, mean reward: 0.587 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.704, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 61.245, mean reward: 0.612 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.336, 10.364], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.063s, episode steps: 100, steps per second: 1585, episode reward: 58.732, mean reward: 0.587 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.209, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.254s, episode steps: 100, steps per second: 80, episode reward: 57.804, mean reward: 0.578 [0.498, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.848, 10.190], loss: 0.007655, mae: 0.086170, mean_q: 0.794171
  5200/100000: episode: 52, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 60.541, mean reward: 0.605 [0.508, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.463, 10.098], loss: 0.002758, mae: 0.054695, mean_q: 0.940034
  5300/100000: episode: 53, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.846, mean reward: 0.588 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.978, 10.130], loss: 0.002756, mae: 0.052538, mean_q: 1.027819
  5400/100000: episode: 54, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 56.774, mean reward: 0.568 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.163, 10.136], loss: 0.002843, mae: 0.054781, mean_q: 1.082082
  5500/100000: episode: 55, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.962, mean reward: 0.590 [0.511, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.338, 10.144], loss: 0.003084, mae: 0.055142, mean_q: 1.113977
  5600/100000: episode: 56, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.929, mean reward: 0.589 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.164], loss: 0.003138, mae: 0.054287, mean_q: 1.135286
  5700/100000: episode: 57, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.515, mean reward: 0.595 [0.498, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.161, 10.098], loss: 0.002525, mae: 0.050609, mean_q: 1.147319
  5800/100000: episode: 58, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.671, mean reward: 0.587 [0.503, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.828, 10.098], loss: 0.002909, mae: 0.054497, mean_q: 1.156426
  5900/100000: episode: 59, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.748, mean reward: 0.587 [0.513, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.608, 10.170], loss: 0.003097, mae: 0.055858, mean_q: 1.157782
  6000/100000: episode: 60, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.847, mean reward: 0.598 [0.498, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.555, 10.098], loss: 0.002839, mae: 0.054167, mean_q: 1.162134
  6100/100000: episode: 61, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.412, mean reward: 0.604 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.066, 10.098], loss: 0.002826, mae: 0.053409, mean_q: 1.164699
  6200/100000: episode: 62, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.082, mean reward: 0.591 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.638, 10.098], loss: 0.002887, mae: 0.052255, mean_q: 1.163281
  6300/100000: episode: 63, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.405, mean reward: 0.594 [0.515, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.858, 10.279], loss: 0.002792, mae: 0.052349, mean_q: 1.166581
  6400/100000: episode: 64, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 59.427, mean reward: 0.594 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.247, 10.296], loss: 0.002734, mae: 0.052031, mean_q: 1.167340
  6500/100000: episode: 65, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 61.040, mean reward: 0.610 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.663, 10.539], loss: 0.002951, mae: 0.053576, mean_q: 1.164506
  6600/100000: episode: 66, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 60.309, mean reward: 0.603 [0.507, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.817, 10.098], loss: 0.002513, mae: 0.050538, mean_q: 1.168283
  6700/100000: episode: 67, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.884, mean reward: 0.579 [0.502, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.226, 10.180], loss: 0.002768, mae: 0.052163, mean_q: 1.166934
  6800/100000: episode: 68, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 60.554, mean reward: 0.606 [0.508, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.352, 10.143], loss: 0.003262, mae: 0.054933, mean_q: 1.164946
  6900/100000: episode: 69, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.643, mean reward: 0.586 [0.510, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.209, 10.172], loss: 0.002904, mae: 0.053109, mean_q: 1.166416
  7000/100000: episode: 70, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 61.114, mean reward: 0.611 [0.516, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.374, 10.294], loss: 0.002871, mae: 0.053623, mean_q: 1.168597
  7100/100000: episode: 71, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.384, mean reward: 0.584 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.427, 10.150], loss: 0.002695, mae: 0.052726, mean_q: 1.165488
  7200/100000: episode: 72, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.545, mean reward: 0.575 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.740, 10.098], loss: 0.002627, mae: 0.051616, mean_q: 1.169192
  7300/100000: episode: 73, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.646, mean reward: 0.606 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.983, 10.102], loss: 0.002638, mae: 0.052080, mean_q: 1.168404
  7400/100000: episode: 74, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.489, mean reward: 0.605 [0.515, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.086, 10.098], loss: 0.002730, mae: 0.051980, mean_q: 1.167892
  7500/100000: episode: 75, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.426, mean reward: 0.584 [0.505, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.754, 10.199], loss: 0.002752, mae: 0.053104, mean_q: 1.168239
  7600/100000: episode: 76, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.940, mean reward: 0.589 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.786, 10.098], loss: 0.002963, mae: 0.052936, mean_q: 1.170774
  7700/100000: episode: 77, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.533, mean reward: 0.585 [0.525, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.637, 10.229], loss: 0.002794, mae: 0.051273, mean_q: 1.166049
  7800/100000: episode: 78, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.437, mean reward: 0.574 [0.509, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.688, 10.098], loss: 0.002884, mae: 0.052914, mean_q: 1.169111
  7900/100000: episode: 79, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 59.445, mean reward: 0.594 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.023, 10.098], loss: 0.002453, mae: 0.050718, mean_q: 1.172725
  8000/100000: episode: 80, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.832, mean reward: 0.588 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.815, 10.098], loss: 0.002446, mae: 0.050573, mean_q: 1.169255
  8100/100000: episode: 81, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.076, mean reward: 0.581 [0.508, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.754, 10.111], loss: 0.002420, mae: 0.049455, mean_q: 1.166366
  8200/100000: episode: 82, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.009, mean reward: 0.580 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.167, 10.128], loss: 0.002402, mae: 0.049783, mean_q: 1.168777
  8300/100000: episode: 83, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 61.589, mean reward: 0.616 [0.502, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.857, 10.354], loss: 0.002440, mae: 0.050241, mean_q: 1.168927
  8400/100000: episode: 84, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 58.497, mean reward: 0.585 [0.501, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.134, 10.151], loss: 0.002450, mae: 0.049826, mean_q: 1.168397
  8500/100000: episode: 85, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.295, mean reward: 0.593 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.668, 10.098], loss: 0.002293, mae: 0.049042, mean_q: 1.169192
  8600/100000: episode: 86, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.740, mean reward: 0.577 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.531, 10.098], loss: 0.002570, mae: 0.051163, mean_q: 1.172930
  8700/100000: episode: 87, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.471, mean reward: 0.605 [0.511, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.884, 10.311], loss: 0.002454, mae: 0.051104, mean_q: 1.173285
  8800/100000: episode: 88, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.193, mean reward: 0.592 [0.506, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.077, 10.098], loss: 0.002988, mae: 0.053610, mean_q: 1.169351
  8900/100000: episode: 89, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.186, mean reward: 0.592 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.223, 10.148], loss: 0.002195, mae: 0.048870, mean_q: 1.172126
  9000/100000: episode: 90, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 60.378, mean reward: 0.604 [0.510, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.025, 10.098], loss: 0.002495, mae: 0.050320, mean_q: 1.172080
  9100/100000: episode: 91, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 57.521, mean reward: 0.575 [0.499, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.851, 10.285], loss: 0.002589, mae: 0.052431, mean_q: 1.172305
  9200/100000: episode: 92, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.515, mean reward: 0.575 [0.501, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.639, 10.098], loss: 0.002454, mae: 0.050543, mean_q: 1.171820
  9300/100000: episode: 93, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.660, mean reward: 0.587 [0.508, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.597, 10.098], loss: 0.002268, mae: 0.049628, mean_q: 1.172931
  9400/100000: episode: 94, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 58.265, mean reward: 0.583 [0.498, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.657, 10.244], loss: 0.002568, mae: 0.051355, mean_q: 1.168210
  9500/100000: episode: 95, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 61.893, mean reward: 0.619 [0.510, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.381, 10.401], loss: 0.002257, mae: 0.049413, mean_q: 1.171485
  9600/100000: episode: 96, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.454, mean reward: 0.585 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.877, 10.116], loss: 0.002294, mae: 0.048830, mean_q: 1.169716
  9700/100000: episode: 97, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.169, mean reward: 0.582 [0.502, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.200, 10.143], loss: 0.001919, mae: 0.046206, mean_q: 1.170640
  9800/100000: episode: 98, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.049, mean reward: 0.580 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.373, 10.133], loss: 0.002214, mae: 0.049377, mean_q: 1.169581
  9900/100000: episode: 99, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.500, mean reward: 0.575 [0.500, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.299, 10.148], loss: 0.002548, mae: 0.051478, mean_q: 1.168519
 10000/100000: episode: 100, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.746, mean reward: 0.577 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.427, 10.206], loss: 0.002529, mae: 0.051652, mean_q: 1.167699
 10100/100000: episode: 101, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.525, mean reward: 0.575 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.784, 10.220], loss: 0.002247, mae: 0.049387, mean_q: 1.169148
 10200/100000: episode: 102, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.230, mean reward: 0.592 [0.512, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.721, 10.098], loss: 0.002086, mae: 0.047733, mean_q: 1.166557
 10300/100000: episode: 103, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.682, mean reward: 0.587 [0.511, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.908, 10.141], loss: 0.002150, mae: 0.048224, mean_q: 1.169995
 10400/100000: episode: 104, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 63.542, mean reward: 0.635 [0.502, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.758, 10.215], loss: 0.002131, mae: 0.048172, mean_q: 1.167069
 10500/100000: episode: 105, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 57.905, mean reward: 0.579 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.174, 10.098], loss: 0.002035, mae: 0.047016, mean_q: 1.169306
 10600/100000: episode: 106, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.381, mean reward: 0.584 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.668, 10.144], loss: 0.002153, mae: 0.049062, mean_q: 1.170107
 10700/100000: episode: 107, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.356, mean reward: 0.584 [0.509, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.797, 10.098], loss: 0.001988, mae: 0.047243, mean_q: 1.168520
 10800/100000: episode: 108, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.156, mean reward: 0.582 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.821, 10.154], loss: 0.002100, mae: 0.048855, mean_q: 1.173297
 10900/100000: episode: 109, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.297, mean reward: 0.583 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.326, 10.098], loss: 0.002126, mae: 0.049038, mean_q: 1.171885
 11000/100000: episode: 110, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 61.049, mean reward: 0.610 [0.501, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.376, 10.098], loss: 0.002008, mae: 0.048016, mean_q: 1.170537
 11100/100000: episode: 111, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.308, mean reward: 0.583 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.254, 10.326], loss: 0.002038, mae: 0.047261, mean_q: 1.168411
 11200/100000: episode: 112, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.513, mean reward: 0.585 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.925, 10.163], loss: 0.002154, mae: 0.047698, mean_q: 1.164354
 11300/100000: episode: 113, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 60.652, mean reward: 0.607 [0.510, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.968, 10.307], loss: 0.001852, mae: 0.046137, mean_q: 1.171082
 11400/100000: episode: 114, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.772, mean reward: 0.588 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.539, 10.098], loss: 0.002125, mae: 0.048908, mean_q: 1.168639
 11500/100000: episode: 115, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.294, mean reward: 0.593 [0.510, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.311, 10.380], loss: 0.002023, mae: 0.047440, mean_q: 1.169979
 11600/100000: episode: 116, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.831, mean reward: 0.598 [0.507, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.827, 10.098], loss: 0.001999, mae: 0.047808, mean_q: 1.171315
 11700/100000: episode: 117, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 56.538, mean reward: 0.565 [0.500, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.284, 10.164], loss: 0.002003, mae: 0.047323, mean_q: 1.164617
 11800/100000: episode: 118, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 66.246, mean reward: 0.662 [0.544, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.943, 10.244], loss: 0.002036, mae: 0.048120, mean_q: 1.166005
 11900/100000: episode: 119, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.990, mean reward: 0.580 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.871, 10.144], loss: 0.002284, mae: 0.049936, mean_q: 1.168546
 12000/100000: episode: 120, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.188, mean reward: 0.582 [0.509, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.940, 10.282], loss: 0.002013, mae: 0.047770, mean_q: 1.170550
 12100/100000: episode: 121, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 63.244, mean reward: 0.632 [0.519, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.956, 10.098], loss: 0.002220, mae: 0.049611, mean_q: 1.168989
 12200/100000: episode: 122, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.644, mean reward: 0.586 [0.507, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.592, 10.098], loss: 0.002689, mae: 0.050358, mean_q: 1.168521
 12300/100000: episode: 123, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 60.641, mean reward: 0.606 [0.508, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.893, 10.294], loss: 0.002636, mae: 0.051520, mean_q: 1.169265
 12400/100000: episode: 124, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.425, mean reward: 0.584 [0.513, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.359, 10.098], loss: 0.002109, mae: 0.048944, mean_q: 1.170089
 12500/100000: episode: 125, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.011, mean reward: 0.570 [0.503, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.185, 10.098], loss: 0.002110, mae: 0.048567, mean_q: 1.169345
 12600/100000: episode: 126, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.967, mean reward: 0.590 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.336, 10.098], loss: 0.001921, mae: 0.046768, mean_q: 1.168472
 12700/100000: episode: 127, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.834, mean reward: 0.588 [0.499, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.884, 10.341], loss: 0.002161, mae: 0.049268, mean_q: 1.166710
 12800/100000: episode: 128, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.307, mean reward: 0.583 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.785, 10.232], loss: 0.002331, mae: 0.050676, mean_q: 1.171463
 12900/100000: episode: 129, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.201, mean reward: 0.572 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.820, 10.098], loss: 0.002177, mae: 0.049520, mean_q: 1.168415
 13000/100000: episode: 130, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.013, mean reward: 0.590 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.969, 10.098], loss: 0.002665, mae: 0.052633, mean_q: 1.165608
 13100/100000: episode: 131, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.607, mean reward: 0.586 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.263, 10.098], loss: 0.002260, mae: 0.049646, mean_q: 1.165505
 13200/100000: episode: 132, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.969, mean reward: 0.590 [0.508, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.601, 10.206], loss: 0.002062, mae: 0.048290, mean_q: 1.168771
 13300/100000: episode: 133, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.873, mean reward: 0.589 [0.500, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.359, 10.149], loss: 0.001743, mae: 0.044509, mean_q: 1.166761
 13400/100000: episode: 134, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 59.319, mean reward: 0.593 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.160, 10.314], loss: 0.001993, mae: 0.047121, mean_q: 1.167837
 13500/100000: episode: 135, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.748, mean reward: 0.577 [0.507, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.442, 10.167], loss: 0.001884, mae: 0.046323, mean_q: 1.166674
 13600/100000: episode: 136, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.085, mean reward: 0.591 [0.509, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.889, 10.098], loss: 0.001801, mae: 0.045719, mean_q: 1.170074
 13700/100000: episode: 137, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.835, mean reward: 0.608 [0.517, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.184, 10.098], loss: 0.001963, mae: 0.046973, mean_q: 1.168079
 13800/100000: episode: 138, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 59.339, mean reward: 0.593 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.624, 10.327], loss: 0.001972, mae: 0.047611, mean_q: 1.166685
 13900/100000: episode: 139, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.181, mean reward: 0.582 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.193, 10.098], loss: 0.001950, mae: 0.047123, mean_q: 1.168078
 14000/100000: episode: 140, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.604, mean reward: 0.596 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.710, 10.264], loss: 0.001753, mae: 0.044690, mean_q: 1.168993
 14100/100000: episode: 141, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 56.073, mean reward: 0.561 [0.508, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.255, 10.098], loss: 0.001750, mae: 0.045130, mean_q: 1.166366
 14200/100000: episode: 142, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.210, mean reward: 0.572 [0.499, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.171, 10.098], loss: 0.001686, mae: 0.044369, mean_q: 1.166594
 14300/100000: episode: 143, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.905, mean reward: 0.579 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.329, 10.111], loss: 0.002424, mae: 0.049751, mean_q: 1.168228
 14400/100000: episode: 144, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.613, mean reward: 0.576 [0.498, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.526, 10.106], loss: 0.002677, mae: 0.050876, mean_q: 1.167062
 14500/100000: episode: 145, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 56.618, mean reward: 0.566 [0.506, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.370, 10.130], loss: 0.002586, mae: 0.051198, mean_q: 1.161970
 14600/100000: episode: 146, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 60.444, mean reward: 0.604 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.891, 10.098], loss: 0.001754, mae: 0.045222, mean_q: 1.165921
 14700/100000: episode: 147, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 58.839, mean reward: 0.588 [0.505, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.491, 10.098], loss: 0.002070, mae: 0.047663, mean_q: 1.168821
 14800/100000: episode: 148, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 60.084, mean reward: 0.601 [0.504, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.579, 10.364], loss: 0.002135, mae: 0.047343, mean_q: 1.166804
 14900/100000: episode: 149, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.781, mean reward: 0.588 [0.500, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.524, 10.103], loss: 0.002632, mae: 0.049413, mean_q: 1.167621
[Info] 1-TH LEVEL FOUND: 1.2316869497299194, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.164s, episode steps: 100, steps per second: 19, episode reward: 57.323, mean reward: 0.573 [0.510, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.725, 10.098], loss: 0.002650, mae: 0.049359, mean_q: 1.169400
 15040/100000: episode: 151, duration: 0.228s, episode steps: 40, steps per second: 175, episode reward: 25.266, mean reward: 0.632 [0.563, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.401, 10.240], loss: 0.002736, mae: 0.050397, mean_q: 1.164891
 15073/100000: episode: 152, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 22.042, mean reward: 0.668 [0.609, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.143, 10.388], loss: 0.002528, mae: 0.049933, mean_q: 1.168302
 15102/100000: episode: 153, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 17.137, mean reward: 0.591 [0.530, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.359, 10.286], loss: 0.002794, mae: 0.051886, mean_q: 1.164085
 15131/100000: episode: 154, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 18.175, mean reward: 0.627 [0.536, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.409, 10.188], loss: 0.002565, mae: 0.051618, mean_q: 1.165502
 15166/100000: episode: 155, duration: 0.186s, episode steps: 35, steps per second: 189, episode reward: 25.424, mean reward: 0.726 [0.598, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.197, 10.589], loss: 0.002312, mae: 0.049154, mean_q: 1.173365
 15201/100000: episode: 156, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 20.094, mean reward: 0.574 [0.502, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.606, 10.201], loss: 0.002675, mae: 0.051597, mean_q: 1.170544
 15234/100000: episode: 157, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 19.018, mean reward: 0.576 [0.509, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.229, 10.132], loss: 0.002370, mae: 0.049902, mean_q: 1.171314
 15269/100000: episode: 158, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 19.950, mean reward: 0.570 [0.499, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.763, 10.215], loss: 0.002421, mae: 0.050602, mean_q: 1.171732
 15302/100000: episode: 159, duration: 0.168s, episode steps: 33, steps per second: 196, episode reward: 20.688, mean reward: 0.627 [0.571, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.178, 10.263], loss: 0.002596, mae: 0.051337, mean_q: 1.167328
 15337/100000: episode: 160, duration: 0.184s, episode steps: 35, steps per second: 191, episode reward: 21.466, mean reward: 0.613 [0.510, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.328, 10.100], loss: 0.002526, mae: 0.051989, mean_q: 1.170463
 15375/100000: episode: 161, duration: 0.218s, episode steps: 38, steps per second: 174, episode reward: 27.270, mean reward: 0.718 [0.532, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.226, 10.155], loss: 0.002375, mae: 0.049425, mean_q: 1.165148
 15409/100000: episode: 162, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 23.554, mean reward: 0.693 [0.624, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.035, 10.402], loss: 0.002408, mae: 0.051647, mean_q: 1.171232
 15442/100000: episode: 163, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 20.278, mean reward: 0.614 [0.551, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.456, 10.360], loss: 0.002155, mae: 0.049147, mean_q: 1.177777
 15477/100000: episode: 164, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 20.455, mean reward: 0.584 [0.516, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.035, 10.154], loss: 0.002199, mae: 0.048835, mean_q: 1.174432
 15510/100000: episode: 165, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 21.858, mean reward: 0.662 [0.531, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.259, 10.247], loss: 0.002375, mae: 0.050323, mean_q: 1.176628
 15544/100000: episode: 166, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 22.906, mean reward: 0.674 [0.552, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.839, 10.203], loss: 0.002848, mae: 0.053626, mean_q: 1.176763
 15578/100000: episode: 167, duration: 0.171s, episode steps: 34, steps per second: 198, episode reward: 21.852, mean reward: 0.643 [0.544, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.119, 10.196], loss: 0.002396, mae: 0.051028, mean_q: 1.183421
 15611/100000: episode: 168, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 21.292, mean reward: 0.645 [0.533, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.160, 10.216], loss: 0.002584, mae: 0.052895, mean_q: 1.173523
 15646/100000: episode: 169, duration: 0.185s, episode steps: 35, steps per second: 190, episode reward: 23.889, mean reward: 0.683 [0.582, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.485, 10.269], loss: 0.003006, mae: 0.054064, mean_q: 1.171062
 15674/100000: episode: 170, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 17.540, mean reward: 0.626 [0.513, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.722, 10.200], loss: 0.002644, mae: 0.050564, mean_q: 1.173156
 15708/100000: episode: 171, duration: 0.186s, episode steps: 34, steps per second: 182, episode reward: 21.309, mean reward: 0.627 [0.536, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.719, 10.200], loss: 0.002535, mae: 0.050373, mean_q: 1.170388
 15748/100000: episode: 172, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 25.884, mean reward: 0.647 [0.584, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.201, 10.386], loss: 0.002150, mae: 0.049237, mean_q: 1.178195
 15776/100000: episode: 173, duration: 0.136s, episode steps: 28, steps per second: 206, episode reward: 16.647, mean reward: 0.595 [0.514, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.152], loss: 0.002246, mae: 0.050476, mean_q: 1.182510
 15804/100000: episode: 174, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 17.693, mean reward: 0.632 [0.537, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.093, 10.195], loss: 0.001808, mae: 0.045414, mean_q: 1.176209
 15833/100000: episode: 175, duration: 0.169s, episode steps: 29, steps per second: 172, episode reward: 18.014, mean reward: 0.621 [0.543, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.263], loss: 0.001781, mae: 0.045778, mean_q: 1.179516
 15866/100000: episode: 176, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 19.774, mean reward: 0.599 [0.501, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.134, 10.318], loss: 0.002022, mae: 0.048520, mean_q: 1.175287
 15901/100000: episode: 177, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 22.355, mean reward: 0.639 [0.555, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.120, 10.407], loss: 0.002474, mae: 0.051179, mean_q: 1.177027
 15930/100000: episode: 178, duration: 0.169s, episode steps: 29, steps per second: 172, episode reward: 16.463, mean reward: 0.568 [0.519, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.205, 10.181], loss: 0.002422, mae: 0.049270, mean_q: 1.171092
 15965/100000: episode: 179, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 21.835, mean reward: 0.624 [0.548, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.455, 10.382], loss: 0.002088, mae: 0.049363, mean_q: 1.182317
 16000/100000: episode: 180, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 22.051, mean reward: 0.630 [0.555, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.724, 10.368], loss: 0.001993, mae: 0.048079, mean_q: 1.178318
 16033/100000: episode: 181, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 21.337, mean reward: 0.647 [0.510, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.195, 10.121], loss: 0.002335, mae: 0.049220, mean_q: 1.179510
 16066/100000: episode: 182, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 19.990, mean reward: 0.606 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.133, 10.123], loss: 0.002213, mae: 0.051194, mean_q: 1.174632
 16094/100000: episode: 183, duration: 0.139s, episode steps: 28, steps per second: 202, episode reward: 17.608, mean reward: 0.629 [0.532, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.398], loss: 0.002009, mae: 0.048183, mean_q: 1.182614
 16129/100000: episode: 184, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 23.418, mean reward: 0.669 [0.600, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.433, 10.361], loss: 0.002494, mae: 0.052377, mean_q: 1.180718
 16163/100000: episode: 185, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 20.773, mean reward: 0.611 [0.509, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.164], loss: 0.001860, mae: 0.047632, mean_q: 1.188161
 16198/100000: episode: 186, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 20.854, mean reward: 0.596 [0.539, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.609, 10.270], loss: 0.001851, mae: 0.046815, mean_q: 1.183538
 16232/100000: episode: 187, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 21.491, mean reward: 0.632 [0.558, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.339, 10.259], loss: 0.002083, mae: 0.048078, mean_q: 1.180236
 16260/100000: episode: 188, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 17.546, mean reward: 0.627 [0.555, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.692, 10.383], loss: 0.002002, mae: 0.046970, mean_q: 1.181421
 16294/100000: episode: 189, duration: 0.184s, episode steps: 34, steps per second: 184, episode reward: 24.457, mean reward: 0.719 [0.618, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.416, 10.517], loss: 0.001911, mae: 0.046464, mean_q: 1.178271
 16332/100000: episode: 190, duration: 0.207s, episode steps: 38, steps per second: 183, episode reward: 26.768, mean reward: 0.704 [0.635, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.564, 10.436], loss: 0.001763, mae: 0.044550, mean_q: 1.179311
 16365/100000: episode: 191, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 21.302, mean reward: 0.646 [0.579, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.196, 10.380], loss: 0.002433, mae: 0.050708, mean_q: 1.182116
 16405/100000: episode: 192, duration: 0.211s, episode steps: 40, steps per second: 190, episode reward: 25.502, mean reward: 0.638 [0.543, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.920, 10.204], loss: 0.002340, mae: 0.051810, mean_q: 1.181235
 16439/100000: episode: 193, duration: 0.176s, episode steps: 34, steps per second: 194, episode reward: 21.865, mean reward: 0.643 [0.527, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.181], loss: 0.002066, mae: 0.047586, mean_q: 1.185808
 16467/100000: episode: 194, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 16.462, mean reward: 0.588 [0.534, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.075, 10.100], loss: 0.002215, mae: 0.050361, mean_q: 1.179897
 16501/100000: episode: 195, duration: 0.188s, episode steps: 34, steps per second: 180, episode reward: 23.042, mean reward: 0.678 [0.612, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.035, 10.543], loss: 0.001614, mae: 0.043873, mean_q: 1.184607
 16529/100000: episode: 196, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 16.853, mean reward: 0.602 [0.514, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.280, 10.419], loss: 0.002110, mae: 0.047630, mean_q: 1.184427
 16563/100000: episode: 197, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 21.384, mean reward: 0.629 [0.516, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.035, 10.212], loss: 0.002804, mae: 0.053984, mean_q: 1.187542
 16596/100000: episode: 198, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 22.290, mean reward: 0.675 [0.614, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.644, 10.415], loss: 0.002544, mae: 0.049950, mean_q: 1.191017
 16634/100000: episode: 199, duration: 0.212s, episode steps: 38, steps per second: 179, episode reward: 25.603, mean reward: 0.674 [0.568, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.730, 10.277], loss: 0.002412, mae: 0.048860, mean_q: 1.192384
 16663/100000: episode: 200, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 17.329, mean reward: 0.598 [0.514, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.101], loss: 0.002659, mae: 0.052418, mean_q: 1.196706
 16703/100000: episode: 201, duration: 0.219s, episode steps: 40, steps per second: 182, episode reward: 25.220, mean reward: 0.631 [0.563, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.635, 10.371], loss: 0.002511, mae: 0.051534, mean_q: 1.186634
 16743/100000: episode: 202, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 25.151, mean reward: 0.629 [0.545, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.449, 10.340], loss: 0.002471, mae: 0.051871, mean_q: 1.185837
 16778/100000: episode: 203, duration: 0.188s, episode steps: 35, steps per second: 187, episode reward: 21.657, mean reward: 0.619 [0.554, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.458, 10.205], loss: 0.002084, mae: 0.047721, mean_q: 1.187720
 16816/100000: episode: 204, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 23.252, mean reward: 0.612 [0.519, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.321, 10.187], loss: 0.002240, mae: 0.049912, mean_q: 1.189689
 16849/100000: episode: 205, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 19.535, mean reward: 0.592 [0.505, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.235, 10.100], loss: 0.001945, mae: 0.046887, mean_q: 1.182674
 16882/100000: episode: 206, duration: 0.184s, episode steps: 33, steps per second: 180, episode reward: 19.929, mean reward: 0.604 [0.543, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.279, 10.264], loss: 0.002284, mae: 0.049758, mean_q: 1.189298
 16916/100000: episode: 207, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 22.272, mean reward: 0.655 [0.605, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.159, 10.320], loss: 0.001888, mae: 0.045588, mean_q: 1.188139
 16950/100000: episode: 208, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 21.201, mean reward: 0.624 [0.566, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.245, 10.322], loss: 0.002537, mae: 0.052634, mean_q: 1.185886
 16990/100000: episode: 209, duration: 0.218s, episode steps: 40, steps per second: 184, episode reward: 26.022, mean reward: 0.651 [0.559, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.857, 10.438], loss: 0.001929, mae: 0.047162, mean_q: 1.191106
 17025/100000: episode: 210, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 22.241, mean reward: 0.635 [0.553, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.015, 10.347], loss: 0.001907, mae: 0.046276, mean_q: 1.190503
 17063/100000: episode: 211, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 27.557, mean reward: 0.725 [0.640, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.229, 10.627], loss: 0.001922, mae: 0.047926, mean_q: 1.192617
 17101/100000: episode: 212, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 25.546, mean reward: 0.672 [0.586, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-1.323, 10.363], loss: 0.002396, mae: 0.051659, mean_q: 1.192272
 17139/100000: episode: 213, duration: 0.198s, episode steps: 38, steps per second: 191, episode reward: 27.142, mean reward: 0.714 [0.660, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.509, 10.419], loss: 0.001680, mae: 0.044937, mean_q: 1.188985
 17167/100000: episode: 214, duration: 0.143s, episode steps: 28, steps per second: 195, episode reward: 16.669, mean reward: 0.595 [0.536, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.903, 10.175], loss: 0.001634, mae: 0.045199, mean_q: 1.195246
 17202/100000: episode: 215, duration: 0.208s, episode steps: 35, steps per second: 169, episode reward: 21.157, mean reward: 0.604 [0.545, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.035, 10.202], loss: 0.001852, mae: 0.047354, mean_q: 1.198114
 17242/100000: episode: 216, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 26.155, mean reward: 0.654 [0.588, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.199, 10.277], loss: 0.001899, mae: 0.046732, mean_q: 1.188879
 17275/100000: episode: 217, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 22.674, mean reward: 0.687 [0.604, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.524, 10.397], loss: 0.001705, mae: 0.045277, mean_q: 1.200880
 17309/100000: episode: 218, duration: 0.179s, episode steps: 34, steps per second: 189, episode reward: 22.217, mean reward: 0.653 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.050, 10.247], loss: 0.001805, mae: 0.046199, mean_q: 1.193289
 17344/100000: episode: 219, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 20.245, mean reward: 0.578 [0.500, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.213, 10.100], loss: 0.001569, mae: 0.043487, mean_q: 1.198099
 17377/100000: episode: 220, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 22.753, mean reward: 0.689 [0.552, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.104, 10.264], loss: 0.002262, mae: 0.049168, mean_q: 1.188484
 17417/100000: episode: 221, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 25.590, mean reward: 0.640 [0.576, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-1.198, 10.461], loss: 0.002030, mae: 0.048408, mean_q: 1.195710
 17452/100000: episode: 222, duration: 0.201s, episode steps: 35, steps per second: 175, episode reward: 22.884, mean reward: 0.654 [0.551, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.035, 10.282], loss: 0.001724, mae: 0.045077, mean_q: 1.194882
 17490/100000: episode: 223, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 27.534, mean reward: 0.725 [0.670, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.017, 10.446], loss: 0.001717, mae: 0.044722, mean_q: 1.197116
 17530/100000: episode: 224, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 25.384, mean reward: 0.635 [0.557, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.247, 10.254], loss: 0.001675, mae: 0.044922, mean_q: 1.203106
 17568/100000: episode: 225, duration: 0.192s, episode steps: 38, steps per second: 198, episode reward: 28.102, mean reward: 0.740 [0.625, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.494, 10.403], loss: 0.001907, mae: 0.047270, mean_q: 1.201715
 17601/100000: episode: 226, duration: 0.197s, episode steps: 33, steps per second: 168, episode reward: 20.919, mean reward: 0.634 [0.553, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.035, 10.229], loss: 0.002269, mae: 0.051477, mean_q: 1.208008
 17634/100000: episode: 227, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 20.698, mean reward: 0.627 [0.548, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.241], loss: 0.001511, mae: 0.042863, mean_q: 1.204531
 17667/100000: episode: 228, duration: 0.189s, episode steps: 33, steps per second: 174, episode reward: 21.255, mean reward: 0.644 [0.564, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.335], loss: 0.002720, mae: 0.053502, mean_q: 1.191759
 17696/100000: episode: 229, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 18.710, mean reward: 0.645 [0.590, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.041, 10.361], loss: 0.002342, mae: 0.052324, mean_q: 1.204345
 17724/100000: episode: 230, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 17.538, mean reward: 0.626 [0.533, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.915, 10.277], loss: 0.001979, mae: 0.047407, mean_q: 1.202544
 17757/100000: episode: 231, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 19.269, mean reward: 0.584 [0.512, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.551, 10.100], loss: 0.001675, mae: 0.045408, mean_q: 1.198018
 17792/100000: episode: 232, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 21.701, mean reward: 0.620 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.303, 10.137], loss: 0.001677, mae: 0.044364, mean_q: 1.206739
 17827/100000: episode: 233, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 22.648, mean reward: 0.647 [0.571, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.300, 10.331], loss: 0.001993, mae: 0.046921, mean_q: 1.202925
 17855/100000: episode: 234, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 18.273, mean reward: 0.653 [0.612, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.035, 10.387], loss: 0.002018, mae: 0.046702, mean_q: 1.200927
 17888/100000: episode: 235, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 20.312, mean reward: 0.616 [0.526, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.159, 10.100], loss: 0.001947, mae: 0.047062, mean_q: 1.200667
 17926/100000: episode: 236, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 29.302, mean reward: 0.771 [0.671, 0.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.482, 10.451], loss: 0.001633, mae: 0.043556, mean_q: 1.214756
 17955/100000: episode: 237, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 18.885, mean reward: 0.651 [0.582, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.802, 10.336], loss: 0.001724, mae: 0.044844, mean_q: 1.205454
 17990/100000: episode: 238, duration: 0.178s, episode steps: 35, steps per second: 197, episode reward: 22.638, mean reward: 0.647 [0.554, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.695, 10.391], loss: 0.001928, mae: 0.047788, mean_q: 1.212341
 18023/100000: episode: 239, duration: 0.177s, episode steps: 33, steps per second: 187, episode reward: 20.549, mean reward: 0.623 [0.505, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.448, 10.150], loss: 0.001776, mae: 0.046162, mean_q: 1.219524
[Info] 2-TH LEVEL FOUND: 1.3931466341018677, Considering 10/90 traces
 18056/100000: episode: 240, duration: 4.432s, episode steps: 33, steps per second: 7, episode reward: 21.652, mean reward: 0.656 [0.580, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.842, 10.353], loss: 0.002090, mae: 0.050836, mean_q: 1.212816
 18076/100000: episode: 241, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 15.045, mean reward: 0.752 [0.704, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.609, 10.519], loss: 0.001744, mae: 0.044508, mean_q: 1.215021
 18113/100000: episode: 242, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 24.631, mean reward: 0.666 [0.572, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.466, 10.270], loss: 0.001723, mae: 0.044284, mean_q: 1.227010
 18148/100000: episode: 243, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 24.836, mean reward: 0.710 [0.539, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.613, 10.276], loss: 0.001617, mae: 0.043169, mean_q: 1.216869
 18180/100000: episode: 244, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 22.758, mean reward: 0.711 [0.620, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.035, 10.321], loss: 0.001920, mae: 0.048223, mean_q: 1.225333
 18210/100000: episode: 245, duration: 0.194s, episode steps: 30, steps per second: 154, episode reward: 21.188, mean reward: 0.706 [0.561, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.303], loss: 0.001634, mae: 0.043244, mean_q: 1.200945
 18240/100000: episode: 246, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 20.622, mean reward: 0.687 [0.588, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.814, 10.291], loss: 0.001817, mae: 0.045313, mean_q: 1.222087
 18275/100000: episode: 247, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 25.929, mean reward: 0.741 [0.639, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.384, 10.399], loss: 0.002666, mae: 0.051149, mean_q: 1.222390
 18295/100000: episode: 248, duration: 0.127s, episode steps: 20, steps per second: 157, episode reward: 14.766, mean reward: 0.738 [0.704, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.404], loss: 0.002464, mae: 0.052297, mean_q: 1.216011
 18330/100000: episode: 249, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 26.051, mean reward: 0.744 [0.661, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.140, 10.626], loss: 0.002110, mae: 0.048110, mean_q: 1.215628
 18360/100000: episode: 250, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 21.221, mean reward: 0.707 [0.548, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.277, 10.375], loss: 0.002175, mae: 0.049525, mean_q: 1.223651
 18397/100000: episode: 251, duration: 0.207s, episode steps: 37, steps per second: 179, episode reward: 25.742, mean reward: 0.696 [0.602, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.499, 10.349], loss: 0.001874, mae: 0.045317, mean_q: 1.219439
 18421/100000: episode: 252, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 16.428, mean reward: 0.685 [0.578, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.431, 10.362], loss: 0.001853, mae: 0.045009, mean_q: 1.226332
 18440/100000: episode: 253, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 13.810, mean reward: 0.727 [0.660, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.587, 10.465], loss: 0.001676, mae: 0.043163, mean_q: 1.228707
 18460/100000: episode: 254, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 14.467, mean reward: 0.723 [0.573, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.146, 10.380], loss: 0.001835, mae: 0.044123, mean_q: 1.234591
 18492/100000: episode: 255, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 23.088, mean reward: 0.721 [0.567, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.560, 10.292], loss: 0.002151, mae: 0.048655, mean_q: 1.225440
 18524/100000: episode: 256, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 23.617, mean reward: 0.738 [0.645, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.440], loss: 0.001875, mae: 0.045901, mean_q: 1.239372
 18559/100000: episode: 257, duration: 0.214s, episode steps: 35, steps per second: 164, episode reward: 23.421, mean reward: 0.669 [0.619, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.700, 10.354], loss: 0.001951, mae: 0.046494, mean_q: 1.229549
 18595/100000: episode: 258, duration: 0.198s, episode steps: 36, steps per second: 181, episode reward: 22.338, mean reward: 0.620 [0.525, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.350, 10.176], loss: 0.001700, mae: 0.044512, mean_q: 1.234722
 18614/100000: episode: 259, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 13.157, mean reward: 0.692 [0.613, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.332], loss: 0.002128, mae: 0.050012, mean_q: 1.228958
 18646/100000: episode: 260, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 21.274, mean reward: 0.665 [0.564, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.237, 10.279], loss: 0.001716, mae: 0.044264, mean_q: 1.227848
 18666/100000: episode: 261, duration: 0.115s, episode steps: 20, steps per second: 175, episode reward: 14.946, mean reward: 0.747 [0.653, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.233, 10.513], loss: 0.002302, mae: 0.050515, mean_q: 1.222642
 18701/100000: episode: 262, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 23.830, mean reward: 0.681 [0.573, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.401, 10.396], loss: 0.002154, mae: 0.050226, mean_q: 1.222976
 18736/100000: episode: 263, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 27.966, mean reward: 0.799 [0.704, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.349, 10.538], loss: 0.001928, mae: 0.046641, mean_q: 1.240557
 18771/100000: episode: 264, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 22.325, mean reward: 0.638 [0.554, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.554, 10.374], loss: 0.002124, mae: 0.049620, mean_q: 1.235107
 18801/100000: episode: 265, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 19.542, mean reward: 0.651 [0.521, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.677, 10.164], loss: 0.002131, mae: 0.050121, mean_q: 1.234519
 18836/100000: episode: 266, duration: 0.198s, episode steps: 35, steps per second: 176, episode reward: 27.692, mean reward: 0.791 [0.732, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.360, 10.555], loss: 0.001933, mae: 0.046900, mean_q: 1.232912
 18855/100000: episode: 267, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 14.074, mean reward: 0.741 [0.648, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.277, 10.487], loss: 0.002113, mae: 0.047939, mean_q: 1.227417
 18874/100000: episode: 268, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 13.762, mean reward: 0.724 [0.660, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.407, 10.583], loss: 0.001387, mae: 0.040381, mean_q: 1.240561
 18911/100000: episode: 269, duration: 0.190s, episode steps: 37, steps per second: 194, episode reward: 26.419, mean reward: 0.714 [0.627, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.844, 10.350], loss: 0.001752, mae: 0.045572, mean_q: 1.231920
 18946/100000: episode: 270, duration: 0.176s, episode steps: 35, steps per second: 198, episode reward: 25.715, mean reward: 0.735 [0.686, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.135, 10.491], loss: 0.001796, mae: 0.045642, mean_q: 1.243531
 18983/100000: episode: 271, duration: 0.196s, episode steps: 37, steps per second: 188, episode reward: 25.727, mean reward: 0.695 [0.619, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.285, 10.326], loss: 0.001833, mae: 0.047353, mean_q: 1.246915
 19018/100000: episode: 272, duration: 0.191s, episode steps: 35, steps per second: 184, episode reward: 23.294, mean reward: 0.666 [0.568, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.141, 10.115], loss: 0.002350, mae: 0.050130, mean_q: 1.245599
 19055/100000: episode: 273, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 25.606, mean reward: 0.692 [0.629, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.162, 10.494], loss: 0.002951, mae: 0.055321, mean_q: 1.244155
 19088/100000: episode: 274, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 23.195, mean reward: 0.703 [0.586, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.433, 10.375], loss: 0.001818, mae: 0.045834, mean_q: 1.248115
 19125/100000: episode: 275, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 26.674, mean reward: 0.721 [0.624, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.349, 10.393], loss: 0.002114, mae: 0.049308, mean_q: 1.255210
 19144/100000: episode: 276, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 13.602, mean reward: 0.716 [0.640, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.556], loss: 0.002649, mae: 0.055346, mean_q: 1.253386
 19179/100000: episode: 277, duration: 0.202s, episode steps: 35, steps per second: 173, episode reward: 23.336, mean reward: 0.667 [0.606, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.603, 10.306], loss: 0.002356, mae: 0.051760, mean_q: 1.251759
 19203/100000: episode: 278, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 15.058, mean reward: 0.627 [0.537, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.199], loss: 0.002042, mae: 0.050604, mean_q: 1.253131
 19235/100000: episode: 279, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 23.137, mean reward: 0.723 [0.654, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.636, 10.481], loss: 0.001990, mae: 0.046667, mean_q: 1.257069
 19268/100000: episode: 280, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 24.500, mean reward: 0.742 [0.656, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.035, 10.464], loss: 0.001855, mae: 0.045404, mean_q: 1.253172
 19301/100000: episode: 281, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 22.540, mean reward: 0.683 [0.588, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.035, 10.325], loss: 0.001772, mae: 0.045198, mean_q: 1.263426
 19320/100000: episode: 282, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 14.125, mean reward: 0.743 [0.611, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.526, 10.431], loss: 0.001840, mae: 0.047148, mean_q: 1.253065
 19353/100000: episode: 283, duration: 0.186s, episode steps: 33, steps per second: 178, episode reward: 21.840, mean reward: 0.662 [0.573, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.473, 10.203], loss: 0.002187, mae: 0.049412, mean_q: 1.268793
 19373/100000: episode: 284, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 15.939, mean reward: 0.797 [0.756, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.455], loss: 0.001832, mae: 0.046120, mean_q: 1.266121
 19393/100000: episode: 285, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 15.310, mean reward: 0.766 [0.701, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.490], loss: 0.001984, mae: 0.045963, mean_q: 1.258939
 19428/100000: episode: 286, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 27.066, mean reward: 0.773 [0.705, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.058, 10.477], loss: 0.001995, mae: 0.047370, mean_q: 1.272101
 19465/100000: episode: 287, duration: 0.201s, episode steps: 37, steps per second: 185, episode reward: 25.190, mean reward: 0.681 [0.546, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.976, 10.103], loss: 0.001927, mae: 0.046216, mean_q: 1.272563
 19502/100000: episode: 288, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 28.300, mean reward: 0.765 [0.685, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.120, 10.581], loss: 0.001671, mae: 0.044530, mean_q: 1.269503
 19522/100000: episode: 289, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 13.263, mean reward: 0.663 [0.550, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.866, 10.251], loss: 0.001552, mae: 0.043630, mean_q: 1.278439
 19554/100000: episode: 290, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 23.156, mean reward: 0.724 [0.660, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.134, 10.502], loss: 0.001636, mae: 0.044306, mean_q: 1.283575
 19589/100000: episode: 291, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 24.401, mean reward: 0.697 [0.597, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.892, 10.347], loss: 0.001632, mae: 0.042816, mean_q: 1.281423
 19613/100000: episode: 292, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 17.480, mean reward: 0.728 [0.644, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.703, 10.458], loss: 0.001680, mae: 0.042405, mean_q: 1.265798
 19637/100000: episode: 293, duration: 0.127s, episode steps: 24, steps per second: 188, episode reward: 16.113, mean reward: 0.671 [0.558, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.302], loss: 0.001486, mae: 0.040387, mean_q: 1.272770
 19656/100000: episode: 294, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 14.336, mean reward: 0.755 [0.684, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.578, 10.589], loss: 0.001414, mae: 0.040292, mean_q: 1.288620
 19692/100000: episode: 295, duration: 0.218s, episode steps: 36, steps per second: 165, episode reward: 22.818, mean reward: 0.634 [0.513, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.186, 10.264], loss: 0.001573, mae: 0.043111, mean_q: 1.278415
 19727/100000: episode: 296, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 27.697, mean reward: 0.791 [0.714, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.254, 10.679], loss: 0.001552, mae: 0.042105, mean_q: 1.281188
 19764/100000: episode: 297, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 27.176, mean reward: 0.734 [0.594, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.147, 10.370], loss: 0.001680, mae: 0.044136, mean_q: 1.281994
 19799/100000: episode: 298, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 27.455, mean reward: 0.784 [0.689, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.786, 10.486], loss: 0.002019, mae: 0.048164, mean_q: 1.292513
 19829/100000: episode: 299, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 20.398, mean reward: 0.680 [0.613, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.763, 10.331], loss: 0.001992, mae: 0.048086, mean_q: 1.287026
 19864/100000: episode: 300, duration: 0.187s, episode steps: 35, steps per second: 188, episode reward: 22.041, mean reward: 0.630 [0.520, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.045, 10.191], loss: 0.001785, mae: 0.045780, mean_q: 1.289649
 19883/100000: episode: 301, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 14.350, mean reward: 0.755 [0.690, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.490], loss: 0.001599, mae: 0.042643, mean_q: 1.273579
 19915/100000: episode: 302, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 22.149, mean reward: 0.692 [0.590, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.609, 10.310], loss: 0.003035, mae: 0.052253, mean_q: 1.289611
 19951/100000: episode: 303, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 27.633, mean reward: 0.768 [0.632, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.035, 10.470], loss: 0.002411, mae: 0.052366, mean_q: 1.288321
 19975/100000: episode: 304, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 16.861, mean reward: 0.703 [0.614, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.810, 10.464], loss: 0.002158, mae: 0.047171, mean_q: 1.287087
 19999/100000: episode: 305, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 15.595, mean reward: 0.650 [0.536, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.201], loss: 0.001492, mae: 0.042685, mean_q: 1.305823
 20031/100000: episode: 306, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 22.769, mean reward: 0.712 [0.607, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.861, 10.602], loss: 0.001583, mae: 0.042690, mean_q: 1.295298
 20068/100000: episode: 307, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 26.053, mean reward: 0.704 [0.655, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-2.215, 10.340], loss: 0.001851, mae: 0.046468, mean_q: 1.292205
 20098/100000: episode: 308, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 20.515, mean reward: 0.684 [0.615, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.560, 10.384], loss: 0.001477, mae: 0.041405, mean_q: 1.292585
 20122/100000: episode: 309, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 18.943, mean reward: 0.789 [0.705, 0.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.062, 10.715], loss: 0.001607, mae: 0.043444, mean_q: 1.295492
 20146/100000: episode: 310, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 16.880, mean reward: 0.703 [0.645, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.383], loss: 0.001586, mae: 0.042717, mean_q: 1.304456
 20165/100000: episode: 311, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 15.242, mean reward: 0.802 [0.691, 0.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.586, 10.608], loss: 0.001822, mae: 0.044776, mean_q: 1.301116
 20198/100000: episode: 312, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 23.190, mean reward: 0.703 [0.523, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.257, 10.145], loss: 0.001651, mae: 0.045072, mean_q: 1.312660
 20230/100000: episode: 313, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 23.587, mean reward: 0.737 [0.668, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.041, 10.465], loss: 0.001693, mae: 0.044161, mean_q: 1.296087
 20265/100000: episode: 314, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 25.482, mean reward: 0.728 [0.623, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.035, 10.313], loss: 0.001582, mae: 0.043432, mean_q: 1.299471
 20301/100000: episode: 315, duration: 0.204s, episode steps: 36, steps per second: 176, episode reward: 22.318, mean reward: 0.620 [0.509, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.643, 10.168], loss: 0.001572, mae: 0.043519, mean_q: 1.307034
 20334/100000: episode: 316, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 25.092, mean reward: 0.760 [0.661, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.590, 10.358], loss: 0.001685, mae: 0.043917, mean_q: 1.298653
 20354/100000: episode: 317, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 13.549, mean reward: 0.677 [0.620, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.509, 10.377], loss: 0.001498, mae: 0.041714, mean_q: 1.303586
 20389/100000: episode: 318, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 23.297, mean reward: 0.666 [0.530, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.094, 10.185], loss: 0.002094, mae: 0.047769, mean_q: 1.307243
 20424/100000: episode: 319, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 25.425, mean reward: 0.726 [0.647, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.739, 10.381], loss: 0.001645, mae: 0.044491, mean_q: 1.312641
 20443/100000: episode: 320, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 12.903, mean reward: 0.679 [0.607, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.312, 10.352], loss: 0.001682, mae: 0.043903, mean_q: 1.311909
 20480/100000: episode: 321, duration: 0.213s, episode steps: 37, steps per second: 173, episode reward: 24.433, mean reward: 0.660 [0.553, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.616, 10.213], loss: 0.002013, mae: 0.046259, mean_q: 1.303347
 20515/100000: episode: 322, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 23.564, mean reward: 0.673 [0.541, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.451, 10.176], loss: 0.001812, mae: 0.045401, mean_q: 1.301540
 20535/100000: episode: 323, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 14.279, mean reward: 0.714 [0.630, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.937, 10.361], loss: 0.001715, mae: 0.044594, mean_q: 1.296216
 20555/100000: episode: 324, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 13.812, mean reward: 0.691 [0.633, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.378], loss: 0.001770, mae: 0.045055, mean_q: 1.302431
 20591/100000: episode: 325, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 23.828, mean reward: 0.662 [0.576, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.138, 10.322], loss: 0.001991, mae: 0.047457, mean_q: 1.309330
 20626/100000: episode: 326, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 24.754, mean reward: 0.707 [0.613, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.289, 10.538], loss: 0.001814, mae: 0.045974, mean_q: 1.302339
 20645/100000: episode: 327, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 12.803, mean reward: 0.674 [0.630, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.351], loss: 0.001637, mae: 0.043887, mean_q: 1.305918
 20681/100000: episode: 328, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 24.390, mean reward: 0.677 [0.583, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.313, 10.288], loss: 0.001507, mae: 0.042194, mean_q: 1.317338
 20716/100000: episode: 329, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 25.723, mean reward: 0.735 [0.644, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.406, 10.420], loss: 0.002474, mae: 0.050601, mean_q: 1.316752
[Info] 3-TH LEVEL FOUND: 1.585777759552002, Considering 10/90 traces
 20735/100000: episode: 330, duration: 4.204s, episode steps: 19, steps per second: 5, episode reward: 13.635, mean reward: 0.718 [0.615, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.354], loss: 0.002053, mae: 0.046609, mean_q: 1.316685
 20762/100000: episode: 331, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 22.676, mean reward: 0.840 [0.748, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.018, 10.651], loss: 0.001768, mae: 0.046990, mean_q: 1.317954
 20784/100000: episode: 332, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 17.801, mean reward: 0.809 [0.763, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.078, 10.542], loss: 0.001656, mae: 0.043951, mean_q: 1.309099
 20801/100000: episode: 333, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 13.554, mean reward: 0.797 [0.730, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.725, 10.505], loss: 0.001637, mae: 0.044204, mean_q: 1.319451
 20827/100000: episode: 334, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 19.836, mean reward: 0.763 [0.656, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.469, 10.439], loss: 0.001582, mae: 0.043491, mean_q: 1.320583
 20855/100000: episode: 335, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 21.039, mean reward: 0.751 [0.673, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.273, 10.506], loss: 0.001543, mae: 0.043284, mean_q: 1.328394
 20882/100000: episode: 336, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 20.145, mean reward: 0.746 [0.628, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.313, 10.334], loss: 0.001822, mae: 0.044302, mean_q: 1.321169
 20901/100000: episode: 337, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 13.757, mean reward: 0.724 [0.616, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.420], loss: 0.001432, mae: 0.043129, mean_q: 1.323814
 20927/100000: episode: 338, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 19.500, mean reward: 0.750 [0.578, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.308, 10.376], loss: 0.001519, mae: 0.043085, mean_q: 1.328116
 20949/100000: episode: 339, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 16.771, mean reward: 0.762 [0.675, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.492], loss: 0.001375, mae: 0.041262, mean_q: 1.316755
 20968/100000: episode: 340, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 13.516, mean reward: 0.711 [0.635, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.332, 10.462], loss: 0.001880, mae: 0.046610, mean_q: 1.311544
 20985/100000: episode: 341, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 14.813, mean reward: 0.871 [0.795, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.089, 10.532], loss: 0.001739, mae: 0.045751, mean_q: 1.319054
 21007/100000: episode: 342, duration: 0.109s, episode steps: 22, steps per second: 203, episode reward: 18.783, mean reward: 0.854 [0.783, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.665], loss: 0.001583, mae: 0.044191, mean_q: 1.335217
 21029/100000: episode: 343, duration: 0.110s, episode steps: 22, steps per second: 201, episode reward: 18.575, mean reward: 0.844 [0.752, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.339, 10.500], loss: 0.001724, mae: 0.046289, mean_q: 1.318958
 21056/100000: episode: 344, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 20.032, mean reward: 0.742 [0.671, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.604, 10.559], loss: 0.001542, mae: 0.041609, mean_q: 1.337210
 21078/100000: episode: 345, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 19.238, mean reward: 0.874 [0.785, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.164, 10.509], loss: 0.002423, mae: 0.049181, mean_q: 1.337699
 21097/100000: episode: 346, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 13.949, mean reward: 0.734 [0.664, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.565, 10.434], loss: 0.002452, mae: 0.051621, mean_q: 1.339499
 21124/100000: episode: 347, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 20.310, mean reward: 0.752 [0.695, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.471], loss: 0.001853, mae: 0.046692, mean_q: 1.322522
 21146/100000: episode: 348, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 16.786, mean reward: 0.763 [0.608, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.302], loss: 0.001534, mae: 0.042245, mean_q: 1.326557
 21173/100000: episode: 349, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 20.767, mean reward: 0.769 [0.634, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.035, 10.409], loss: 0.001488, mae: 0.042261, mean_q: 1.329903
 21200/100000: episode: 350, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 20.856, mean reward: 0.772 [0.687, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.641, 10.503], loss: 0.002203, mae: 0.048376, mean_q: 1.332708
 21227/100000: episode: 351, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 21.386, mean reward: 0.792 [0.650, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.620, 10.415], loss: 0.001951, mae: 0.047817, mean_q: 1.345907
 21253/100000: episode: 352, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 19.411, mean reward: 0.747 [0.653, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.228, 10.337], loss: 0.001713, mae: 0.045013, mean_q: 1.345219
 21280/100000: episode: 353, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 22.946, mean reward: 0.850 [0.753, 0.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.451, 10.660], loss: 0.001869, mae: 0.047238, mean_q: 1.341057
 21297/100000: episode: 354, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 13.331, mean reward: 0.784 [0.736, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.404], loss: 0.001643, mae: 0.044353, mean_q: 1.330451
 21303/100000: episode: 355, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 4.966, mean reward: 0.828 [0.782, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.521], loss: 0.001279, mae: 0.038651, mean_q: 1.343514
 21330/100000: episode: 356, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 23.314, mean reward: 0.863 [0.795, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.485, 10.532], loss: 0.001767, mae: 0.045420, mean_q: 1.340897
 21347/100000: episode: 357, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 14.094, mean reward: 0.829 [0.745, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.492], loss: 0.001853, mae: 0.046875, mean_q: 1.335310
 21366/100000: episode: 358, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 14.125, mean reward: 0.743 [0.658, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.666, 10.407], loss: 0.002204, mae: 0.051508, mean_q: 1.337244
 21388/100000: episode: 359, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 16.102, mean reward: 0.732 [0.627, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.263, 10.378], loss: 0.002496, mae: 0.053950, mean_q: 1.342772
 21410/100000: episode: 360, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 16.648, mean reward: 0.757 [0.679, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.531], loss: 0.001838, mae: 0.047367, mean_q: 1.339983
 21432/100000: episode: 361, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 18.156, mean reward: 0.825 [0.720, 0.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.638, 10.444], loss: 0.001542, mae: 0.042334, mean_q: 1.353790
 21438/100000: episode: 362, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 4.951, mean reward: 0.825 [0.806, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.552], loss: 0.001826, mae: 0.046198, mean_q: 1.353456
 21464/100000: episode: 363, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 18.921, mean reward: 0.728 [0.658, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.713, 10.438], loss: 0.001716, mae: 0.046021, mean_q: 1.358493
 21470/100000: episode: 364, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 5.390, mean reward: 0.898 [0.883, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.612], loss: 0.001581, mae: 0.041818, mean_q: 1.344125
 21497/100000: episode: 365, duration: 0.133s, episode steps: 27, steps per second: 202, episode reward: 19.538, mean reward: 0.724 [0.617, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.908, 10.377], loss: 0.001634, mae: 0.043427, mean_q: 1.343703
 21526/100000: episode: 366, duration: 0.141s, episode steps: 29, steps per second: 206, episode reward: 22.301, mean reward: 0.769 [0.659, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.035, 10.380], loss: 0.001819, mae: 0.045763, mean_q: 1.349519
 21543/100000: episode: 367, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 13.074, mean reward: 0.769 [0.715, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.499], loss: 0.001544, mae: 0.042546, mean_q: 1.355271
 21569/100000: episode: 368, duration: 0.139s, episode steps: 26, steps per second: 186, episode reward: 19.019, mean reward: 0.732 [0.552, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.084, 10.353], loss: 0.001699, mae: 0.045195, mean_q: 1.358000
 21597/100000: episode: 369, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 21.154, mean reward: 0.756 [0.674, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.644, 10.482], loss: 0.001614, mae: 0.043824, mean_q: 1.346840
 21625/100000: episode: 370, duration: 0.135s, episode steps: 28, steps per second: 208, episode reward: 19.818, mean reward: 0.708 [0.574, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.301], loss: 0.001370, mae: 0.040550, mean_q: 1.374124
 21652/100000: episode: 371, duration: 0.163s, episode steps: 27, steps per second: 165, episode reward: 22.561, mean reward: 0.836 [0.676, 0.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.412], loss: 0.001538, mae: 0.043618, mean_q: 1.356722
 21658/100000: episode: 372, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 5.582, mean reward: 0.930 [0.892, 0.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.761], loss: 0.001394, mae: 0.039814, mean_q: 1.363714
[Info] FALSIFICATION!
 21669/100000: episode: 373, duration: 0.441s, episode steps: 11, steps per second: 25, episode reward: 9.551, mean reward: 0.868 [0.825, 1.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.479, 9.451], loss: 0.001477, mae: 0.042438, mean_q: 1.359371
 21688/100000: episode: 374, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 15.019, mean reward: 0.790 [0.733, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.524], loss: 0.001440, mae: 0.041734, mean_q: 1.376237
 21716/100000: episode: 375, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 19.072, mean reward: 0.681 [0.543, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.226], loss: 0.001886, mae: 0.047562, mean_q: 1.364668
 21733/100000: episode: 376, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 13.867, mean reward: 0.816 [0.761, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.531], loss: 0.002405, mae: 0.048619, mean_q: 1.356540
 21750/100000: episode: 377, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 13.251, mean reward: 0.779 [0.751, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.292, 10.467], loss: 0.001948, mae: 0.046595, mean_q: 1.344648
 21778/100000: episode: 378, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 22.957, mean reward: 0.820 [0.682, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.153, 10.489], loss: 0.001667, mae: 0.042436, mean_q: 1.373429
 21807/100000: episode: 379, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 22.078, mean reward: 0.761 [0.674, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.490, 10.362], loss: 0.002041, mae: 0.049465, mean_q: 1.380761
 21813/100000: episode: 380, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 5.384, mean reward: 0.897 [0.885, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.318, 10.690], loss: 0.002364, mae: 0.054115, mean_q: 1.370032
 21842/100000: episode: 381, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 23.248, mean reward: 0.802 [0.700, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.472, 10.364], loss: 0.001785, mae: 0.045680, mean_q: 1.378113
 21869/100000: episode: 382, duration: 0.133s, episode steps: 27, steps per second: 204, episode reward: 22.197, mean reward: 0.822 [0.703, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.058, 10.517], loss: 0.001828, mae: 0.047452, mean_q: 1.349329
 21891/100000: episode: 383, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 16.866, mean reward: 0.767 [0.687, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.742, 10.471], loss: 0.001490, mae: 0.043368, mean_q: 1.378886
 21908/100000: episode: 384, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 13.936, mean reward: 0.820 [0.776, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.591], loss: 0.001885, mae: 0.046580, mean_q: 1.371457
 21935/100000: episode: 385, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 23.611, mean reward: 0.874 [0.735, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.313, 10.581], loss: 0.001641, mae: 0.043687, mean_q: 1.374765
 21961/100000: episode: 386, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 17.843, mean reward: 0.686 [0.593, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.179, 10.441], loss: 0.002193, mae: 0.050442, mean_q: 1.381232
 21990/100000: episode: 387, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 19.925, mean reward: 0.687 [0.587, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.049, 10.287], loss: 0.002449, mae: 0.050915, mean_q: 1.381604
[Info] FALSIFICATION!
 21998/100000: episode: 388, duration: 0.214s, episode steps: 8, steps per second: 37, episode reward: 7.318, mean reward: 0.915 [0.800, 1.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.022, 9.928], loss: 0.002271, mae: 0.050776, mean_q: 1.369018
 22020/100000: episode: 389, duration: 0.114s, episode steps: 22, steps per second: 194, episode reward: 17.089, mean reward: 0.777 [0.716, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.503], loss: 0.001893, mae: 0.046343, mean_q: 1.380561
 22049/100000: episode: 390, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 25.245, mean reward: 0.871 [0.797, 0.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.082, 10.617], loss: 0.001762, mae: 0.046560, mean_q: 1.387707
 22071/100000: episode: 391, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 17.499, mean reward: 0.795 [0.721, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.180, 10.473], loss: 0.001384, mae: 0.041282, mean_q: 1.380918
 22097/100000: episode: 392, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 19.221, mean reward: 0.739 [0.623, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.394, 10.349], loss: 0.001797, mae: 0.043840, mean_q: 1.379484
 22124/100000: episode: 393, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 19.806, mean reward: 0.734 [0.639, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.518, 10.375], loss: 0.001868, mae: 0.047401, mean_q: 1.376366
 22151/100000: episode: 394, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 19.990, mean reward: 0.740 [0.588, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.239, 10.272], loss: 0.002055, mae: 0.047747, mean_q: 1.389297
 22178/100000: episode: 395, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 19.853, mean reward: 0.735 [0.650, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.202, 10.454], loss: 0.002469, mae: 0.050078, mean_q: 1.387446
 22206/100000: episode: 396, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 20.816, mean reward: 0.743 [0.663, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.645, 10.439], loss: 0.002419, mae: 0.052170, mean_q: 1.395342
 22212/100000: episode: 397, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 5.159, mean reward: 0.860 [0.844, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.042, 10.651], loss: 0.001845, mae: 0.045439, mean_q: 1.368570
 22218/100000: episode: 398, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 5.067, mean reward: 0.844 [0.817, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.638], loss: 0.001493, mae: 0.043301, mean_q: 1.407977
 22224/100000: episode: 399, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 5.163, mean reward: 0.860 [0.831, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.664], loss: 0.001538, mae: 0.043369, mean_q: 1.410958
 22246/100000: episode: 400, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 18.392, mean reward: 0.836 [0.767, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.618, 10.534], loss: 0.001325, mae: 0.040910, mean_q: 1.385662
 22273/100000: episode: 401, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 21.177, mean reward: 0.784 [0.677, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.721, 10.463], loss: 0.001558, mae: 0.044044, mean_q: 1.387168
 22301/100000: episode: 402, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 19.748, mean reward: 0.705 [0.574, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.138, 10.269], loss: 0.001712, mae: 0.045526, mean_q: 1.391685
[Info] FALSIFICATION!
 22307/100000: episode: 403, duration: 0.202s, episode steps: 6, steps per second: 30, episode reward: 5.357, mean reward: 0.893 [0.794, 1.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.021, 9.712], loss: 0.001356, mae: 0.040368, mean_q: 1.370751
 22334/100000: episode: 404, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 20.899, mean reward: 0.774 [0.667, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.017, 10.415], loss: 0.001715, mae: 0.043526, mean_q: 1.391019
 22356/100000: episode: 405, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 17.268, mean reward: 0.785 [0.614, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.312, 10.507], loss: 0.001766, mae: 0.042579, mean_q: 1.388443
 22378/100000: episode: 406, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 17.620, mean reward: 0.801 [0.757, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.244, 10.601], loss: 0.001760, mae: 0.041938, mean_q: 1.392998
 22404/100000: episode: 407, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 22.054, mean reward: 0.848 [0.793, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.238, 10.607], loss: 0.001546, mae: 0.043616, mean_q: 1.394487
 22431/100000: episode: 408, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 20.788, mean reward: 0.770 [0.649, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.711, 10.487], loss: 0.001882, mae: 0.045377, mean_q: 1.394444
 22448/100000: episode: 409, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 14.174, mean reward: 0.834 [0.731, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-1.105, 10.486], loss: 0.001363, mae: 0.041198, mean_q: 1.390627
 22477/100000: episode: 410, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 20.574, mean reward: 0.709 [0.551, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.061, 10.135], loss: 0.001829, mae: 0.047767, mean_q: 1.395626
 22499/100000: episode: 411, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 15.826, mean reward: 0.719 [0.642, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.378, 10.363], loss: 0.002221, mae: 0.049956, mean_q: 1.403536
 22527/100000: episode: 412, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 22.627, mean reward: 0.808 [0.758, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.220, 10.567], loss: 0.001721, mae: 0.045581, mean_q: 1.410138
 22553/100000: episode: 413, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 19.184, mean reward: 0.738 [0.625, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.276, 10.359], loss: 0.002122, mae: 0.047369, mean_q: 1.398293
 22559/100000: episode: 414, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 5.058, mean reward: 0.843 [0.800, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.114, 10.556], loss: 0.001605, mae: 0.044494, mean_q: 1.394111
[Info] FALSIFICATION!
 22573/100000: episode: 415, duration: 0.260s, episode steps: 14, steps per second: 54, episode reward: 12.059, mean reward: 0.861 [0.802, 1.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.806, 9.695], loss: 0.001776, mae: 0.046768, mean_q: 1.404304
[Info] FALSIFICATION!
 22585/100000: episode: 416, duration: 0.233s, episode steps: 12, steps per second: 51, episode reward: 10.628, mean reward: 0.886 [0.808, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.022, 10.048], loss: 0.001611, mae: 0.044643, mean_q: 1.386899
 22604/100000: episode: 417, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 14.159, mean reward: 0.745 [0.666, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.354, 10.427], loss: 0.001658, mae: 0.045608, mean_q: 1.399096
 22632/100000: episode: 418, duration: 0.159s, episode steps: 28, steps per second: 177, episode reward: 21.708, mean reward: 0.775 [0.699, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.615, 10.596], loss: 0.002204, mae: 0.046592, mean_q: 1.418715
 22651/100000: episode: 419, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 14.656, mean reward: 0.771 [0.702, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.527, 10.485], loss: 0.002989, mae: 0.052834, mean_q: 1.385224
[Info] Complete ISplit Iteration
[Info] Levels: [1.231687, 1.3931466, 1.5857778, 1.7105445]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.22]
[Info] Error Prob: 0.00022000000000000006

 22678/100000: episode: 420, duration: 4.412s, episode steps: 27, steps per second: 6, episode reward: 21.927, mean reward: 0.812 [0.683, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-1.089, 10.514], loss: 0.002068, mae: 0.049961, mean_q: 1.408744
 22778/100000: episode: 421, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 56.625, mean reward: 0.566 [0.498, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.567, 10.206], loss: 0.002059, mae: 0.048941, mean_q: 1.399426
 22878/100000: episode: 422, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.077, mean reward: 0.581 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.884, 10.098], loss: 0.002204, mae: 0.046225, mean_q: 1.406075
 22978/100000: episode: 423, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 56.788, mean reward: 0.568 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.130, 10.098], loss: 0.001526, mae: 0.042761, mean_q: 1.405441
 23078/100000: episode: 424, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.672, mean reward: 0.577 [0.500, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.959, 10.294], loss: 0.001930, mae: 0.046320, mean_q: 1.404105
 23178/100000: episode: 425, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 60.150, mean reward: 0.601 [0.515, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.841, 10.171], loss: 0.001835, mae: 0.044816, mean_q: 1.394972
 23278/100000: episode: 426, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.531, mean reward: 0.595 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.655, 10.182], loss: 0.002070, mae: 0.048462, mean_q: 1.383887
 23378/100000: episode: 427, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.422, mean reward: 0.584 [0.506, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.455, 10.098], loss: 0.002146, mae: 0.047332, mean_q: 1.380758
 23478/100000: episode: 428, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.186, mean reward: 0.592 [0.506, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.845, 10.098], loss: 0.001773, mae: 0.045522, mean_q: 1.376333
 23578/100000: episode: 429, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 56.688, mean reward: 0.567 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.158, 10.316], loss: 0.002021, mae: 0.048301, mean_q: 1.363654
 23678/100000: episode: 430, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.471, mean reward: 0.585 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.618, 10.376], loss: 0.002354, mae: 0.049420, mean_q: 1.369180
 23778/100000: episode: 431, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.746, mean reward: 0.587 [0.506, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.896, 10.119], loss: 0.002029, mae: 0.046364, mean_q: 1.368341
 23878/100000: episode: 432, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 62.423, mean reward: 0.624 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.311, 10.098], loss: 0.002201, mae: 0.047745, mean_q: 1.364609
 23978/100000: episode: 433, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.990, mean reward: 0.590 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.254, 10.222], loss: 0.001984, mae: 0.046149, mean_q: 1.359884
 24078/100000: episode: 434, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.746, mean reward: 0.597 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.504, 10.109], loss: 0.001738, mae: 0.044151, mean_q: 1.353829
 24178/100000: episode: 435, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.452, mean reward: 0.585 [0.514, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.120, 10.218], loss: 0.001709, mae: 0.044470, mean_q: 1.354404
 24278/100000: episode: 436, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 56.082, mean reward: 0.561 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.970, 10.210], loss: 0.001851, mae: 0.044679, mean_q: 1.346386
 24378/100000: episode: 437, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 58.314, mean reward: 0.583 [0.507, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.872, 10.110], loss: 0.002234, mae: 0.048316, mean_q: 1.334839
 24478/100000: episode: 438, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.079, mean reward: 0.571 [0.510, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.756, 10.165], loss: 0.002195, mae: 0.047805, mean_q: 1.337851
 24578/100000: episode: 439, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.379, mean reward: 0.584 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.185, 10.224], loss: 0.002039, mae: 0.047207, mean_q: 1.329485
 24678/100000: episode: 440, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.164, mean reward: 0.592 [0.510, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.430, 10.255], loss: 0.001841, mae: 0.044372, mean_q: 1.327082
 24778/100000: episode: 441, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.823, mean reward: 0.568 [0.500, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.318, 10.098], loss: 0.002753, mae: 0.052771, mean_q: 1.321788
 24878/100000: episode: 442, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 61.916, mean reward: 0.619 [0.506, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.998, 10.098], loss: 0.001877, mae: 0.045358, mean_q: 1.320150
 24978/100000: episode: 443, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.227, mean reward: 0.592 [0.510, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.371, 10.307], loss: 0.001884, mae: 0.046664, mean_q: 1.311573
 25078/100000: episode: 444, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.488, mean reward: 0.575 [0.497, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.130, 10.112], loss: 0.001875, mae: 0.046391, mean_q: 1.310317
 25178/100000: episode: 445, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.775, mean reward: 0.588 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.760, 10.189], loss: 0.001918, mae: 0.045753, mean_q: 1.303349
 25278/100000: episode: 446, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 57.406, mean reward: 0.574 [0.505, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.798, 10.098], loss: 0.001656, mae: 0.044042, mean_q: 1.305218
 25378/100000: episode: 447, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 56.669, mean reward: 0.567 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.534, 10.098], loss: 0.002086, mae: 0.046570, mean_q: 1.300790
 25478/100000: episode: 448, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.359, mean reward: 0.594 [0.498, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.324, 10.337], loss: 0.001902, mae: 0.046179, mean_q: 1.298537
 25578/100000: episode: 449, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.289, mean reward: 0.573 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.303, 10.199], loss: 0.002152, mae: 0.045379, mean_q: 1.288110
 25678/100000: episode: 450, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.067, mean reward: 0.581 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.267, 10.098], loss: 0.002225, mae: 0.047119, mean_q: 1.289489
 25778/100000: episode: 451, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.246, mean reward: 0.572 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.510, 10.098], loss: 0.002233, mae: 0.047878, mean_q: 1.273874
 25878/100000: episode: 452, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.822, mean reward: 0.588 [0.505, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.221, 10.098], loss: 0.001975, mae: 0.045414, mean_q: 1.272946
 25978/100000: episode: 453, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.174, mean reward: 0.572 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.361, 10.248], loss: 0.001978, mae: 0.046149, mean_q: 1.266672
 26078/100000: episode: 454, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 60.143, mean reward: 0.601 [0.512, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.814, 10.098], loss: 0.002242, mae: 0.045939, mean_q: 1.254111
 26178/100000: episode: 455, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.031, mean reward: 0.580 [0.509, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.475, 10.098], loss: 0.001876, mae: 0.044934, mean_q: 1.251058
 26278/100000: episode: 456, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.830, mean reward: 0.598 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.596, 10.098], loss: 0.001883, mae: 0.044520, mean_q: 1.244073
 26378/100000: episode: 457, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.521, mean reward: 0.595 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.940, 10.098], loss: 0.001743, mae: 0.044069, mean_q: 1.237881
 26478/100000: episode: 458, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.113, mean reward: 0.601 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.935, 10.098], loss: 0.001706, mae: 0.044176, mean_q: 1.239339
 26578/100000: episode: 459, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.055, mean reward: 0.591 [0.506, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.135, 10.188], loss: 0.001833, mae: 0.043446, mean_q: 1.225637
 26678/100000: episode: 460, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.242, mean reward: 0.572 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.725, 10.173], loss: 0.001905, mae: 0.045513, mean_q: 1.217935
 26778/100000: episode: 461, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.899, mean reward: 0.579 [0.502, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.205, 10.130], loss: 0.002020, mae: 0.044762, mean_q: 1.209557
 26878/100000: episode: 462, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.351, mean reward: 0.584 [0.501, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.704, 10.098], loss: 0.001886, mae: 0.044193, mean_q: 1.208858
 26978/100000: episode: 463, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 60.294, mean reward: 0.603 [0.513, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.185, 10.104], loss: 0.001681, mae: 0.044103, mean_q: 1.201699
 27078/100000: episode: 464, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.609, mean reward: 0.596 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.906, 10.239], loss: 0.001672, mae: 0.043025, mean_q: 1.198353
 27178/100000: episode: 465, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.813, mean reward: 0.588 [0.518, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.778, 10.206], loss: 0.001931, mae: 0.045120, mean_q: 1.190319
 27278/100000: episode: 466, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.041, mean reward: 0.580 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.940, 10.248], loss: 0.001647, mae: 0.043376, mean_q: 1.182104
 27378/100000: episode: 467, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.236, mean reward: 0.582 [0.509, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.576, 10.103], loss: 0.001571, mae: 0.042764, mean_q: 1.177389
 27478/100000: episode: 468, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.577, mean reward: 0.586 [0.506, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.304, 10.238], loss: 0.001675, mae: 0.042181, mean_q: 1.171159
 27578/100000: episode: 469, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.568, mean reward: 0.576 [0.499, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.041, 10.103], loss: 0.001863, mae: 0.046078, mean_q: 1.165667
 27678/100000: episode: 470, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 60.199, mean reward: 0.602 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.892, 10.098], loss: 0.001441, mae: 0.042001, mean_q: 1.158980
 27778/100000: episode: 471, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.703, mean reward: 0.587 [0.505, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.632, 10.098], loss: 0.001490, mae: 0.042018, mean_q: 1.158518
 27878/100000: episode: 472, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.981, mean reward: 0.580 [0.509, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.923, 10.253], loss: 0.001317, mae: 0.040296, mean_q: 1.157683
 27978/100000: episode: 473, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.930, mean reward: 0.579 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.282, 10.154], loss: 0.001415, mae: 0.041582, mean_q: 1.161194
 28078/100000: episode: 474, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 56.174, mean reward: 0.562 [0.506, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.593, 10.098], loss: 0.001373, mae: 0.040810, mean_q: 1.159628
 28178/100000: episode: 475, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.160, mean reward: 0.592 [0.510, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.560, 10.207], loss: 0.001356, mae: 0.040872, mean_q: 1.159404
 28278/100000: episode: 476, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 57.252, mean reward: 0.573 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.301, 10.150], loss: 0.001386, mae: 0.040725, mean_q: 1.156976
 28378/100000: episode: 477, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.356, mean reward: 0.594 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.282, 10.194], loss: 0.001441, mae: 0.042371, mean_q: 1.157437
 28478/100000: episode: 478, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.855, mean reward: 0.589 [0.498, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.714, 10.244], loss: 0.001443, mae: 0.042275, mean_q: 1.160817
 28578/100000: episode: 479, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 61.904, mean reward: 0.619 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.936, 10.160], loss: 0.001417, mae: 0.041276, mean_q: 1.158727
 28678/100000: episode: 480, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.213, mean reward: 0.582 [0.498, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.055, 10.133], loss: 0.001455, mae: 0.041536, mean_q: 1.156781
 28778/100000: episode: 481, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.779, mean reward: 0.588 [0.507, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.596, 10.098], loss: 0.001491, mae: 0.042565, mean_q: 1.161372
 28878/100000: episode: 482, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 61.606, mean reward: 0.616 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.556, 10.098], loss: 0.001551, mae: 0.042074, mean_q: 1.158566
 28978/100000: episode: 483, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.860, mean reward: 0.599 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.782, 10.098], loss: 0.001447, mae: 0.042083, mean_q: 1.160559
 29078/100000: episode: 484, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 60.209, mean reward: 0.602 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.130, 10.181], loss: 0.001390, mae: 0.041684, mean_q: 1.159755
 29178/100000: episode: 485, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.828, mean reward: 0.608 [0.530, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.474, 10.182], loss: 0.001492, mae: 0.042605, mean_q: 1.159238
 29278/100000: episode: 486, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 59.329, mean reward: 0.593 [0.509, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.867, 10.131], loss: 0.001713, mae: 0.044752, mean_q: 1.163549
 29378/100000: episode: 487, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.375, mean reward: 0.574 [0.504, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.899, 10.098], loss: 0.001785, mae: 0.044700, mean_q: 1.162050
 29478/100000: episode: 488, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 59.239, mean reward: 0.592 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.850, 10.371], loss: 0.001760, mae: 0.045259, mean_q: 1.161425
 29578/100000: episode: 489, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 59.656, mean reward: 0.597 [0.504, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.535, 10.175], loss: 0.001564, mae: 0.042621, mean_q: 1.162558
 29678/100000: episode: 490, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.346, mean reward: 0.583 [0.502, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.439, 10.165], loss: 0.001509, mae: 0.042332, mean_q: 1.158445
 29778/100000: episode: 491, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.851, mean reward: 0.589 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.147, 10.380], loss: 0.001342, mae: 0.040800, mean_q: 1.161705
 29878/100000: episode: 492, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.352, mean reward: 0.574 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.376, 10.211], loss: 0.001593, mae: 0.043207, mean_q: 1.161885
 29978/100000: episode: 493, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.404, mean reward: 0.574 [0.506, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.735, 10.098], loss: 0.001694, mae: 0.044581, mean_q: 1.160041
 30078/100000: episode: 494, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.924, mean reward: 0.599 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.184, 10.098], loss: 0.001395, mae: 0.041476, mean_q: 1.165340
 30178/100000: episode: 495, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 62.136, mean reward: 0.621 [0.510, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.291, 10.400], loss: 0.001339, mae: 0.040661, mean_q: 1.166290
 30278/100000: episode: 496, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.277, mean reward: 0.583 [0.513, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.831, 10.098], loss: 0.001545, mae: 0.043174, mean_q: 1.164524
 30378/100000: episode: 497, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.409, mean reward: 0.584 [0.510, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.614, 10.098], loss: 0.001716, mae: 0.042906, mean_q: 1.164635
 30478/100000: episode: 498, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.023, mean reward: 0.590 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.609, 10.250], loss: 0.001527, mae: 0.042696, mean_q: 1.166938
 30578/100000: episode: 499, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 62.280, mean reward: 0.623 [0.520, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.231, 10.098], loss: 0.001485, mae: 0.042485, mean_q: 1.162253
 30678/100000: episode: 500, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 61.447, mean reward: 0.614 [0.520, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.991, 10.098], loss: 0.001736, mae: 0.044733, mean_q: 1.165295
 30778/100000: episode: 501, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.384, mean reward: 0.584 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.926, 10.098], loss: 0.001404, mae: 0.041001, mean_q: 1.167155
 30878/100000: episode: 502, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 61.117, mean reward: 0.611 [0.508, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.782, 10.153], loss: 0.001572, mae: 0.042621, mean_q: 1.163737
 30978/100000: episode: 503, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.783, mean reward: 0.588 [0.512, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.082, 10.279], loss: 0.001335, mae: 0.039936, mean_q: 1.164178
 31078/100000: episode: 504, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.351, mean reward: 0.574 [0.506, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.782, 10.098], loss: 0.001489, mae: 0.041970, mean_q: 1.168191
 31178/100000: episode: 505, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 61.030, mean reward: 0.610 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.398, 10.188], loss: 0.001390, mae: 0.041583, mean_q: 1.169116
 31278/100000: episode: 506, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.662, mean reward: 0.587 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.209, 10.242], loss: 0.001365, mae: 0.040597, mean_q: 1.168294
 31378/100000: episode: 507, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.633, mean reward: 0.586 [0.501, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.910, 10.209], loss: 0.001345, mae: 0.040479, mean_q: 1.172819
 31478/100000: episode: 508, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.417, mean reward: 0.604 [0.505, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.105, 10.098], loss: 0.001491, mae: 0.042303, mean_q: 1.168073
 31578/100000: episode: 509, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.883, mean reward: 0.579 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.711, 10.168], loss: 0.001326, mae: 0.040161, mean_q: 1.170441
 31678/100000: episode: 510, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.377, mean reward: 0.584 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.064, 10.098], loss: 0.001625, mae: 0.043842, mean_q: 1.169834
 31778/100000: episode: 511, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 64.007, mean reward: 0.640 [0.511, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.644, 10.098], loss: 0.001422, mae: 0.041447, mean_q: 1.171154
 31878/100000: episode: 512, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.892, mean reward: 0.579 [0.501, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.482, 10.106], loss: 0.001674, mae: 0.044157, mean_q: 1.173268
 31978/100000: episode: 513, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 57.107, mean reward: 0.571 [0.497, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.941, 10.098], loss: 0.001716, mae: 0.044787, mean_q: 1.169394
 32078/100000: episode: 514, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.309, mean reward: 0.583 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.924, 10.098], loss: 0.001796, mae: 0.044303, mean_q: 1.170104
 32178/100000: episode: 515, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.473, mean reward: 0.575 [0.506, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.726, 10.098], loss: 0.001429, mae: 0.040867, mean_q: 1.169061
 32278/100000: episode: 516, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 60.172, mean reward: 0.602 [0.520, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.855, 10.352], loss: 0.001615, mae: 0.043044, mean_q: 1.170975
 32378/100000: episode: 517, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.343, mean reward: 0.583 [0.508, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.648, 10.098], loss: 0.001524, mae: 0.042188, mean_q: 1.170682
 32478/100000: episode: 518, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 63.077, mean reward: 0.631 [0.509, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.642, 10.098], loss: 0.001607, mae: 0.042980, mean_q: 1.168824
 32578/100000: episode: 519, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.705, mean reward: 0.587 [0.502, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.719, 10.142], loss: 0.001585, mae: 0.043006, mean_q: 1.167976
[Info] 1-TH LEVEL FOUND: 1.3803904056549072, Considering 10/90 traces
 32678/100000: episode: 520, duration: 4.687s, episode steps: 100, steps per second: 21, episode reward: 61.318, mean reward: 0.613 [0.516, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.140, 10.098], loss: 0.001383, mae: 0.041035, mean_q: 1.170573
 32694/100000: episode: 521, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 10.907, mean reward: 0.682 [0.613, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.254, 10.100], loss: 0.001502, mae: 0.042536, mean_q: 1.173069
 32726/100000: episode: 522, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 20.095, mean reward: 0.628 [0.530, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.449, 10.369], loss: 0.001579, mae: 0.043238, mean_q: 1.176033
 32758/100000: episode: 523, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 20.000, mean reward: 0.625 [0.524, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.326, 10.100], loss: 0.001338, mae: 0.039402, mean_q: 1.170065
 32774/100000: episode: 524, duration: 0.087s, episode steps: 16, steps per second: 185, episode reward: 10.960, mean reward: 0.685 [0.600, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.329, 10.100], loss: 0.001268, mae: 0.039461, mean_q: 1.168969
 32806/100000: episode: 525, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 18.818, mean reward: 0.588 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.866, 10.145], loss: 0.001452, mae: 0.040704, mean_q: 1.173840
 32813/100000: episode: 526, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 4.470, mean reward: 0.639 [0.602, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.419], loss: 0.001625, mae: 0.044921, mean_q: 1.175399
 32841/100000: episode: 527, duration: 0.142s, episode steps: 28, steps per second: 196, episode reward: 17.242, mean reward: 0.616 [0.549, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.282], loss: 0.001899, mae: 0.045768, mean_q: 1.171243
 32863/100000: episode: 528, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 13.380, mean reward: 0.608 [0.537, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.575, 10.153], loss: 0.001410, mae: 0.040569, mean_q: 1.174875
 32885/100000: episode: 529, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 13.923, mean reward: 0.633 [0.574, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.460, 10.234], loss: 0.001756, mae: 0.044716, mean_q: 1.171862
 32917/100000: episode: 530, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 20.651, mean reward: 0.645 [0.581, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.084, 10.412], loss: 0.001427, mae: 0.041642, mean_q: 1.174122
 32923/100000: episode: 531, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 4.087, mean reward: 0.681 [0.630, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.977, 10.257], loss: 0.001092, mae: 0.033313, mean_q: 1.165176
 32945/100000: episode: 532, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 13.623, mean reward: 0.619 [0.572, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.899, 10.240], loss: 0.001272, mae: 0.038890, mean_q: 1.173377
 32951/100000: episode: 533, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 3.613, mean reward: 0.602 [0.561, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.281], loss: 0.001708, mae: 0.043471, mean_q: 1.181493
 32983/100000: episode: 534, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 20.550, mean reward: 0.642 [0.578, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.485, 10.299], loss: 0.001421, mae: 0.040880, mean_q: 1.176149
 33006/100000: episode: 535, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 13.695, mean reward: 0.595 [0.520, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.814, 10.255], loss: 0.001604, mae: 0.043087, mean_q: 1.174660
 33038/100000: episode: 536, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 22.607, mean reward: 0.706 [0.619, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.331, 10.441], loss: 0.001716, mae: 0.044807, mean_q: 1.168639
 33077/100000: episode: 537, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 24.619, mean reward: 0.631 [0.522, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.224, 10.270], loss: 0.001614, mae: 0.042892, mean_q: 1.174603
 33100/100000: episode: 538, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 13.745, mean reward: 0.598 [0.504, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.246, 10.130], loss: 0.001717, mae: 0.043909, mean_q: 1.169092
 33123/100000: episode: 539, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 16.171, mean reward: 0.703 [0.581, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.242, 10.453], loss: 0.001580, mae: 0.043260, mean_q: 1.177685
 33162/100000: episode: 540, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 23.441, mean reward: 0.601 [0.512, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.994, 10.126], loss: 0.001474, mae: 0.040824, mean_q: 1.179876
 33185/100000: episode: 541, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 14.002, mean reward: 0.609 [0.570, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.288], loss: 0.001409, mae: 0.039889, mean_q: 1.179122
 33221/100000: episode: 542, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 22.212, mean reward: 0.617 [0.508, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.840, 10.176], loss: 0.001669, mae: 0.042520, mean_q: 1.174510
 33244/100000: episode: 543, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 15.837, mean reward: 0.689 [0.628, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.801, 10.456], loss: 0.001516, mae: 0.041947, mean_q: 1.178242
 33272/100000: episode: 544, duration: 0.153s, episode steps: 28, steps per second: 184, episode reward: 18.005, mean reward: 0.643 [0.579, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.035, 10.267], loss: 0.001594, mae: 0.041590, mean_q: 1.177872
 33308/100000: episode: 545, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 22.398, mean reward: 0.622 [0.501, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.136, 10.100], loss: 0.001797, mae: 0.045417, mean_q: 1.179062
 33339/100000: episode: 546, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 19.544, mean reward: 0.630 [0.564, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.035, 10.344], loss: 0.002045, mae: 0.047040, mean_q: 1.175525
 33371/100000: episode: 547, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 20.140, mean reward: 0.629 [0.572, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.832, 10.308], loss: 0.001576, mae: 0.042801, mean_q: 1.177245
 33387/100000: episode: 548, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 11.453, mean reward: 0.716 [0.661, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.207, 10.100], loss: 0.001784, mae: 0.046307, mean_q: 1.178291
 33423/100000: episode: 549, duration: 0.211s, episode steps: 36, steps per second: 170, episode reward: 24.650, mean reward: 0.685 [0.627, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.417, 10.392], loss: 0.001733, mae: 0.044423, mean_q: 1.180184
 33462/100000: episode: 550, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 22.840, mean reward: 0.586 [0.504, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.352, 10.115], loss: 0.001539, mae: 0.042027, mean_q: 1.179682
 33498/100000: episode: 551, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 21.409, mean reward: 0.595 [0.515, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.603, 10.100], loss: 0.001617, mae: 0.042712, mean_q: 1.184545
 33534/100000: episode: 552, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 21.717, mean reward: 0.603 [0.503, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.636, 10.152], loss: 0.001415, mae: 0.040365, mean_q: 1.179844
 33541/100000: episode: 553, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 4.613, mean reward: 0.659 [0.610, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.866, 10.411], loss: 0.001436, mae: 0.042039, mean_q: 1.187866
 33564/100000: episode: 554, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 14.648, mean reward: 0.637 [0.577, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.215, 10.339], loss: 0.001969, mae: 0.045147, mean_q: 1.185225
 33580/100000: episode: 555, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 12.278, mean reward: 0.767 [0.675, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.673, 10.100], loss: 0.001727, mae: 0.044530, mean_q: 1.175574
 33586/100000: episode: 556, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 3.753, mean reward: 0.626 [0.605, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.339], loss: 0.001788, mae: 0.043581, mean_q: 1.174198
 33617/100000: episode: 557, duration: 0.182s, episode steps: 31, steps per second: 171, episode reward: 19.779, mean reward: 0.638 [0.570, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.940, 10.409], loss: 0.001851, mae: 0.044259, mean_q: 1.175374
 33656/100000: episode: 558, duration: 0.209s, episode steps: 39, steps per second: 186, episode reward: 26.270, mean reward: 0.674 [0.602, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.625, 10.280], loss: 0.002039, mae: 0.047004, mean_q: 1.181974
 33662/100000: episode: 559, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 3.888, mean reward: 0.648 [0.596, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.940, 10.333], loss: 0.001365, mae: 0.040508, mean_q: 1.199848
 33684/100000: episode: 560, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 14.379, mean reward: 0.654 [0.603, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.145, 10.445], loss: 0.001781, mae: 0.044095, mean_q: 1.186146
 33691/100000: episode: 561, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 4.291, mean reward: 0.613 [0.591, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.285, 10.367], loss: 0.001572, mae: 0.040222, mean_q: 1.181482
 33713/100000: episode: 562, duration: 0.110s, episode steps: 22, steps per second: 201, episode reward: 14.380, mean reward: 0.654 [0.603, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.680, 10.422], loss: 0.001860, mae: 0.043610, mean_q: 1.183422
 33752/100000: episode: 563, duration: 0.201s, episode steps: 39, steps per second: 194, episode reward: 25.959, mean reward: 0.666 [0.594, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.863, 10.299], loss: 0.001756, mae: 0.045590, mean_q: 1.181234
 33791/100000: episode: 564, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 28.383, mean reward: 0.728 [0.632, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.530, 10.450], loss: 0.001652, mae: 0.042673, mean_q: 1.181133
 33822/100000: episode: 565, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 19.234, mean reward: 0.620 [0.537, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.721, 10.114], loss: 0.001510, mae: 0.041440, mean_q: 1.181968
 33844/100000: episode: 566, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 13.337, mean reward: 0.606 [0.533, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.066, 10.318], loss: 0.001555, mae: 0.042741, mean_q: 1.186345
 33851/100000: episode: 567, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 4.856, mean reward: 0.694 [0.646, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.463], loss: 0.001665, mae: 0.042608, mean_q: 1.195800
 33883/100000: episode: 568, duration: 0.155s, episode steps: 32, steps per second: 206, episode reward: 19.541, mean reward: 0.611 [0.537, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.035, 10.185], loss: 0.001439, mae: 0.040943, mean_q: 1.180756
 33889/100000: episode: 569, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 4.376, mean reward: 0.729 [0.679, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.255], loss: 0.002042, mae: 0.047496, mean_q: 1.177653
 33895/100000: episode: 570, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 3.588, mean reward: 0.598 [0.569, 0.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.329], loss: 0.001811, mae: 0.045449, mean_q: 1.200052
 33918/100000: episode: 571, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 14.696, mean reward: 0.639 [0.587, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.499, 10.452], loss: 0.002660, mae: 0.051783, mean_q: 1.181363
 33957/100000: episode: 572, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 26.499, mean reward: 0.679 [0.561, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.210, 10.305], loss: 0.001917, mae: 0.046227, mean_q: 1.186655
 33993/100000: episode: 573, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 22.113, mean reward: 0.614 [0.510, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.035, 10.121], loss: 0.001731, mae: 0.044708, mean_q: 1.185003
 34016/100000: episode: 574, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 16.439, mean reward: 0.715 [0.657, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.682, 10.378], loss: 0.001714, mae: 0.043169, mean_q: 1.186397
 34022/100000: episode: 575, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 3.782, mean reward: 0.630 [0.583, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.112, 10.261], loss: 0.002068, mae: 0.049800, mean_q: 1.203655
 34045/100000: episode: 576, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 14.990, mean reward: 0.652 [0.580, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.438, 10.374], loss: 0.001546, mae: 0.040295, mean_q: 1.180663
 34068/100000: episode: 577, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 15.607, mean reward: 0.679 [0.611, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.602, 10.333], loss: 0.001558, mae: 0.041070, mean_q: 1.184216
 34074/100000: episode: 578, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 3.748, mean reward: 0.625 [0.574, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.249], loss: 0.001938, mae: 0.045465, mean_q: 1.205541
 34105/100000: episode: 579, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 21.319, mean reward: 0.688 [0.612, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.620, 10.427], loss: 0.001660, mae: 0.043534, mean_q: 1.183334
 34133/100000: episode: 580, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 17.466, mean reward: 0.624 [0.566, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.225, 10.282], loss: 0.001782, mae: 0.045156, mean_q: 1.190112
 34140/100000: episode: 581, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 4.279, mean reward: 0.611 [0.581, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.329], loss: 0.002033, mae: 0.050583, mean_q: 1.202231
 34179/100000: episode: 582, duration: 0.207s, episode steps: 39, steps per second: 189, episode reward: 26.378, mean reward: 0.676 [0.575, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.307, 10.243], loss: 0.001765, mae: 0.044789, mean_q: 1.180571
 34215/100000: episode: 583, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 22.339, mean reward: 0.621 [0.540, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.812, 10.206], loss: 0.001604, mae: 0.042176, mean_q: 1.184448
 34243/100000: episode: 584, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 17.401, mean reward: 0.621 [0.542, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.304], loss: 0.001883, mae: 0.046165, mean_q: 1.189192
 34282/100000: episode: 585, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 23.591, mean reward: 0.605 [0.518, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.774, 10.258], loss: 0.001642, mae: 0.042936, mean_q: 1.185155
 34313/100000: episode: 586, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 21.402, mean reward: 0.690 [0.595, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.802, 10.573], loss: 0.001550, mae: 0.042072, mean_q: 1.192586
 34336/100000: episode: 587, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 13.962, mean reward: 0.607 [0.529, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.899, 10.142], loss: 0.001791, mae: 0.044826, mean_q: 1.189500
 34358/100000: episode: 588, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 14.165, mean reward: 0.644 [0.609, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.391, 10.358], loss: 0.001696, mae: 0.042774, mean_q: 1.199284
 34389/100000: episode: 589, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 19.877, mean reward: 0.641 [0.564, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.336], loss: 0.001845, mae: 0.046479, mean_q: 1.197369
 34396/100000: episode: 590, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 4.613, mean reward: 0.659 [0.614, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.323], loss: 0.001258, mae: 0.040125, mean_q: 1.190122
 34427/100000: episode: 591, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 21.855, mean reward: 0.705 [0.626, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.589, 10.467], loss: 0.001509, mae: 0.041829, mean_q: 1.197355
 34433/100000: episode: 592, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 3.968, mean reward: 0.661 [0.611, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.882, 10.366], loss: 0.001659, mae: 0.047066, mean_q: 1.207472
 34439/100000: episode: 593, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 4.000, mean reward: 0.667 [0.612, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.392], loss: 0.001670, mae: 0.043121, mean_q: 1.188667
 34446/100000: episode: 594, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 4.771, mean reward: 0.682 [0.652, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.056, 10.295], loss: 0.001916, mae: 0.044771, mean_q: 1.194728
 34462/100000: episode: 595, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 12.160, mean reward: 0.760 [0.721, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.930, 10.100], loss: 0.002243, mae: 0.049840, mean_q: 1.205315
 34484/100000: episode: 596, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 13.032, mean reward: 0.592 [0.515, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.100], loss: 0.002008, mae: 0.045478, mean_q: 1.188672
 34520/100000: episode: 597, duration: 0.190s, episode steps: 36, steps per second: 190, episode reward: 22.592, mean reward: 0.628 [0.546, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.047, 10.212], loss: 0.001657, mae: 0.043682, mean_q: 1.191453
 34536/100000: episode: 598, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.891, mean reward: 0.743 [0.685, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.251, 10.100], loss: 0.001569, mae: 0.041625, mean_q: 1.199444
 34572/100000: episode: 599, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 23.153, mean reward: 0.643 [0.506, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-1.768, 10.190], loss: 0.001540, mae: 0.040733, mean_q: 1.191992
 34578/100000: episode: 600, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 4.027, mean reward: 0.671 [0.638, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.420], loss: 0.001510, mae: 0.042728, mean_q: 1.184946
 34584/100000: episode: 601, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 3.999, mean reward: 0.666 [0.598, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.425], loss: 0.001076, mae: 0.037608, mean_q: 1.210173
 34612/100000: episode: 602, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 17.777, mean reward: 0.635 [0.533, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.358, 10.325], loss: 0.001585, mae: 0.042754, mean_q: 1.196740
 34619/100000: episode: 603, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 4.572, mean reward: 0.653 [0.630, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.407], loss: 0.001856, mae: 0.046177, mean_q: 1.187936
 34641/100000: episode: 604, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 14.469, mean reward: 0.658 [0.592, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.240, 10.433], loss: 0.001959, mae: 0.046267, mean_q: 1.196485
 34663/100000: episode: 605, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 13.768, mean reward: 0.626 [0.569, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.330, 10.295], loss: 0.001674, mae: 0.042712, mean_q: 1.191496
 34679/100000: episode: 606, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.968, mean reward: 0.748 [0.682, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.213, 10.100], loss: 0.001635, mae: 0.044095, mean_q: 1.201610
 34701/100000: episode: 607, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 13.335, mean reward: 0.606 [0.545, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.355, 10.228], loss: 0.001463, mae: 0.041408, mean_q: 1.191478
 34707/100000: episode: 608, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 3.749, mean reward: 0.625 [0.595, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.393], loss: 0.001434, mae: 0.040271, mean_q: 1.199418
 34730/100000: episode: 609, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 16.311, mean reward: 0.709 [0.631, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.162, 10.346], loss: 0.001861, mae: 0.044544, mean_q: 1.194518
[Info] 2-TH LEVEL FOUND: 1.437329649925232, Considering 10/90 traces
 34737/100000: episode: 610, duration: 4.139s, episode steps: 7, steps per second: 2, episode reward: 4.630, mean reward: 0.661 [0.616, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.483], loss: 0.001498, mae: 0.042142, mean_q: 1.218887
 34753/100000: episode: 611, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 11.837, mean reward: 0.740 [0.678, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.271, 10.100], loss: 0.002162, mae: 0.046500, mean_q: 1.186668
 34785/100000: episode: 612, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 22.470, mean reward: 0.702 [0.551, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.946, 10.208], loss: 0.001986, mae: 0.046316, mean_q: 1.193969
 34801/100000: episode: 613, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 10.838, mean reward: 0.677 [0.634, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.519, 10.100], loss: 0.001999, mae: 0.046227, mean_q: 1.190592
 34835/100000: episode: 614, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 24.151, mean reward: 0.710 [0.656, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.404, 10.394], loss: 0.001901, mae: 0.046190, mean_q: 1.203243
 34851/100000: episode: 615, duration: 0.103s, episode steps: 16, steps per second: 155, episode reward: 10.129, mean reward: 0.633 [0.554, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-1.034, 10.100], loss: 0.001394, mae: 0.039898, mean_q: 1.201853
 34867/100000: episode: 616, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 11.524, mean reward: 0.720 [0.670, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.748, 10.100], loss: 0.001850, mae: 0.044549, mean_q: 1.185690
 34883/100000: episode: 617, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 12.664, mean reward: 0.791 [0.696, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.450, 10.100], loss: 0.001561, mae: 0.041962, mean_q: 1.198800
 34899/100000: episode: 618, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 11.226, mean reward: 0.702 [0.629, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.788, 10.100], loss: 0.002029, mae: 0.047381, mean_q: 1.205164
 34915/100000: episode: 619, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 11.614, mean reward: 0.726 [0.644, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.376, 10.100], loss: 0.001545, mae: 0.043149, mean_q: 1.211167
 34931/100000: episode: 620, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 11.763, mean reward: 0.735 [0.678, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.037, 10.100], loss: 0.001625, mae: 0.041646, mean_q: 1.201885
 34947/100000: episode: 621, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 10.948, mean reward: 0.684 [0.639, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.242, 10.100], loss: 0.002195, mae: 0.048011, mean_q: 1.201021
 34963/100000: episode: 622, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 11.079, mean reward: 0.692 [0.633, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.384, 10.100], loss: 0.001976, mae: 0.047357, mean_q: 1.212853
 34997/100000: episode: 623, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 22.652, mean reward: 0.666 [0.565, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.713, 10.401], loss: 0.001687, mae: 0.043741, mean_q: 1.204972
 35013/100000: episode: 624, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 11.939, mean reward: 0.746 [0.683, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.287, 10.100], loss: 0.001881, mae: 0.043586, mean_q: 1.214777
 35029/100000: episode: 625, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 11.771, mean reward: 0.736 [0.685, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.106, 10.100], loss: 0.002196, mae: 0.049305, mean_q: 1.212840
 35045/100000: episode: 626, duration: 0.100s, episode steps: 16, steps per second: 159, episode reward: 11.297, mean reward: 0.706 [0.655, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.435, 10.100], loss: 0.002008, mae: 0.047326, mean_q: 1.216498
 35061/100000: episode: 627, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 12.409, mean reward: 0.776 [0.721, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.224, 10.100], loss: 0.001974, mae: 0.045587, mean_q: 1.215178
 35093/100000: episode: 628, duration: 0.195s, episode steps: 32, steps per second: 164, episode reward: 22.710, mean reward: 0.710 [0.560, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.267, 10.272], loss: 0.001689, mae: 0.043152, mean_q: 1.217668
 35109/100000: episode: 629, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 10.235, mean reward: 0.640 [0.541, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.130, 10.100], loss: 0.002364, mae: 0.049322, mean_q: 1.208373
 35125/100000: episode: 630, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 10.982, mean reward: 0.686 [0.633, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.500, 10.100], loss: 0.001842, mae: 0.042685, mean_q: 1.210428
 35141/100000: episode: 631, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 10.728, mean reward: 0.671 [0.617, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.336, 10.100], loss: 0.001697, mae: 0.044077, mean_q: 1.225396
 35173/100000: episode: 632, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 20.720, mean reward: 0.648 [0.554, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.451, 10.215], loss: 0.001653, mae: 0.042990, mean_q: 1.215222
 35189/100000: episode: 633, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.344, mean reward: 0.709 [0.652, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.560, 10.100], loss: 0.001481, mae: 0.039918, mean_q: 1.223753
 35205/100000: episode: 634, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 12.024, mean reward: 0.752 [0.680, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.248, 10.100], loss: 0.001630, mae: 0.042635, mean_q: 1.225663
 35221/100000: episode: 635, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 11.413, mean reward: 0.713 [0.652, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.437, 10.100], loss: 0.001473, mae: 0.040087, mean_q: 1.221673
 35237/100000: episode: 636, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 11.505, mean reward: 0.719 [0.685, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.232, 10.100], loss: 0.001792, mae: 0.043592, mean_q: 1.213567
 35253/100000: episode: 637, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 11.964, mean reward: 0.748 [0.672, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.300, 10.100], loss: 0.001221, mae: 0.038632, mean_q: 1.215888
 35269/100000: episode: 638, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 11.809, mean reward: 0.738 [0.697, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.391, 10.100], loss: 0.001485, mae: 0.040944, mean_q: 1.229710
 35285/100000: episode: 639, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 12.127, mean reward: 0.758 [0.657, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.396, 10.100], loss: 0.002161, mae: 0.048662, mean_q: 1.220265
 35301/100000: episode: 640, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 11.239, mean reward: 0.702 [0.667, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.266, 10.100], loss: 0.002062, mae: 0.046790, mean_q: 1.216021
 35317/100000: episode: 641, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 11.122, mean reward: 0.695 [0.638, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.179, 10.100], loss: 0.002255, mae: 0.047908, mean_q: 1.222389
 35333/100000: episode: 642, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.562, mean reward: 0.723 [0.671, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.230, 10.100], loss: 0.001942, mae: 0.046297, mean_q: 1.220008
 35349/100000: episode: 643, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 10.837, mean reward: 0.677 [0.612, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.258, 10.100], loss: 0.002389, mae: 0.051002, mean_q: 1.232943
 35365/100000: episode: 644, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 10.856, mean reward: 0.678 [0.587, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.282, 10.100], loss: 0.001877, mae: 0.046296, mean_q: 1.216412
 35381/100000: episode: 645, duration: 0.081s, episode steps: 16, steps per second: 196, episode reward: 11.813, mean reward: 0.738 [0.677, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.496, 10.100], loss: 0.002067, mae: 0.047823, mean_q: 1.228827
 35397/100000: episode: 646, duration: 0.079s, episode steps: 16, steps per second: 201, episode reward: 10.106, mean reward: 0.632 [0.561, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.212, 10.100], loss: 0.001794, mae: 0.043789, mean_q: 1.221936
 35413/100000: episode: 647, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 10.408, mean reward: 0.651 [0.596, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.246, 10.100], loss: 0.001888, mae: 0.044874, mean_q: 1.226234
 35445/100000: episode: 648, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 23.744, mean reward: 0.742 [0.613, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.440], loss: 0.001803, mae: 0.045201, mean_q: 1.220255
 35461/100000: episode: 649, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 11.088, mean reward: 0.693 [0.651, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.504, 10.100], loss: 0.001579, mae: 0.041470, mean_q: 1.222260
 35493/100000: episode: 650, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 23.217, mean reward: 0.726 [0.595, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.200, 10.394], loss: 0.001753, mae: 0.044951, mean_q: 1.219547
 35509/100000: episode: 651, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 10.712, mean reward: 0.670 [0.610, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.217, 10.100], loss: 0.001683, mae: 0.043327, mean_q: 1.217689
 35525/100000: episode: 652, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 11.623, mean reward: 0.726 [0.673, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.138, 10.100], loss: 0.001641, mae: 0.043820, mean_q: 1.229867
 35541/100000: episode: 653, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 11.252, mean reward: 0.703 [0.662, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.064, 10.100], loss: 0.001737, mae: 0.043962, mean_q: 1.230073
 35557/100000: episode: 654, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.343, mean reward: 0.709 [0.644, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.314, 10.100], loss: 0.001515, mae: 0.042916, mean_q: 1.222870
 35573/100000: episode: 655, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 11.531, mean reward: 0.721 [0.675, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.311, 10.100], loss: 0.002056, mae: 0.044620, mean_q: 1.223175
 35589/100000: episode: 656, duration: 0.091s, episode steps: 16, steps per second: 177, episode reward: 10.695, mean reward: 0.668 [0.605, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.531, 10.100], loss: 0.002293, mae: 0.047600, mean_q: 1.210386
 35605/100000: episode: 657, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 11.317, mean reward: 0.707 [0.646, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.492, 10.100], loss: 0.001526, mae: 0.041166, mean_q: 1.215718
 35621/100000: episode: 658, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 11.587, mean reward: 0.724 [0.689, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.231, 10.100], loss: 0.001431, mae: 0.042501, mean_q: 1.241048
 35637/100000: episode: 659, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 11.376, mean reward: 0.711 [0.646, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.286, 10.100], loss: 0.001799, mae: 0.044334, mean_q: 1.225410
 35653/100000: episode: 660, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 11.099, mean reward: 0.694 [0.625, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.317, 10.100], loss: 0.002143, mae: 0.044423, mean_q: 1.229698
 35669/100000: episode: 661, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 10.545, mean reward: 0.659 [0.583, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.874, 10.100], loss: 0.001652, mae: 0.041540, mean_q: 1.219294
 35685/100000: episode: 662, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 10.915, mean reward: 0.682 [0.588, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.593, 10.100], loss: 0.001328, mae: 0.039992, mean_q: 1.228212
 35701/100000: episode: 663, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 10.856, mean reward: 0.678 [0.611, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.348, 10.100], loss: 0.001412, mae: 0.040474, mean_q: 1.233696
 35717/100000: episode: 664, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 10.821, mean reward: 0.676 [0.601, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.194, 10.100], loss: 0.001139, mae: 0.037522, mean_q: 1.234901
 35751/100000: episode: 665, duration: 0.198s, episode steps: 34, steps per second: 172, episode reward: 23.995, mean reward: 0.706 [0.631, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.837, 10.508], loss: 0.001631, mae: 0.042059, mean_q: 1.229095
 35767/100000: episode: 666, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 11.933, mean reward: 0.746 [0.684, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.986, 10.100], loss: 0.002294, mae: 0.047512, mean_q: 1.219174
 35783/100000: episode: 667, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 11.485, mean reward: 0.718 [0.663, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.378, 10.100], loss: 0.002750, mae: 0.051847, mean_q: 1.245466
 35799/100000: episode: 668, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.147, mean reward: 0.697 [0.641, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.392, 10.100], loss: 0.002378, mae: 0.052476, mean_q: 1.229591
 35815/100000: episode: 669, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 10.976, mean reward: 0.686 [0.645, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.255, 10.100], loss: 0.002238, mae: 0.047141, mean_q: 1.229993
 35831/100000: episode: 670, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 11.709, mean reward: 0.732 [0.685, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.351, 10.100], loss: 0.001854, mae: 0.046501, mean_q: 1.242299
 35863/100000: episode: 671, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 22.824, mean reward: 0.713 [0.628, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.651, 10.374], loss: 0.001774, mae: 0.043868, mean_q: 1.234352
 35879/100000: episode: 672, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 10.582, mean reward: 0.661 [0.613, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.301, 10.100], loss: 0.001758, mae: 0.042450, mean_q: 1.242195
 35895/100000: episode: 673, duration: 0.106s, episode steps: 16, steps per second: 151, episode reward: 12.046, mean reward: 0.753 [0.662, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.077, 10.100], loss: 0.001769, mae: 0.043584, mean_q: 1.242041
 35911/100000: episode: 674, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 11.710, mean reward: 0.732 [0.635, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.262, 10.100], loss: 0.001207, mae: 0.036546, mean_q: 1.238212
 35927/100000: episode: 675, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 12.297, mean reward: 0.769 [0.724, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.317, 10.100], loss: 0.001471, mae: 0.040853, mean_q: 1.225652
 35943/100000: episode: 676, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 10.970, mean reward: 0.686 [0.609, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.242, 10.100], loss: 0.001696, mae: 0.043383, mean_q: 1.234458
 35959/100000: episode: 677, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 11.886, mean reward: 0.743 [0.697, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.614, 10.100], loss: 0.001601, mae: 0.041417, mean_q: 1.240922
 35975/100000: episode: 678, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 11.483, mean reward: 0.718 [0.618, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.341, 10.100], loss: 0.001848, mae: 0.045722, mean_q: 1.241869
 36009/100000: episode: 679, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 21.809, mean reward: 0.641 [0.554, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.212, 10.267], loss: 0.001699, mae: 0.043887, mean_q: 1.241453
 36041/100000: episode: 680, duration: 0.155s, episode steps: 32, steps per second: 206, episode reward: 24.399, mean reward: 0.762 [0.670, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.476, 10.555], loss: 0.001572, mae: 0.043084, mean_q: 1.247593
 36057/100000: episode: 681, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 11.738, mean reward: 0.734 [0.696, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.374, 10.100], loss: 0.001329, mae: 0.040338, mean_q: 1.231243
 36073/100000: episode: 682, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 11.702, mean reward: 0.731 [0.681, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.235, 10.100], loss: 0.001572, mae: 0.042960, mean_q: 1.244918
 36089/100000: episode: 683, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 11.394, mean reward: 0.712 [0.630, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.955, 10.100], loss: 0.001784, mae: 0.045965, mean_q: 1.253579
 36105/100000: episode: 684, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 11.399, mean reward: 0.712 [0.654, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.620, 10.100], loss: 0.001601, mae: 0.043430, mean_q: 1.238897
 36121/100000: episode: 685, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 11.773, mean reward: 0.736 [0.614, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.936, 10.100], loss: 0.001582, mae: 0.042660, mean_q: 1.240292
 36137/100000: episode: 686, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 10.978, mean reward: 0.686 [0.608, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.304, 10.100], loss: 0.001823, mae: 0.042014, mean_q: 1.244482
 36153/100000: episode: 687, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 12.408, mean reward: 0.776 [0.736, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.002, 10.100], loss: 0.001658, mae: 0.043503, mean_q: 1.249097
 36169/100000: episode: 688, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 11.501, mean reward: 0.719 [0.677, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.464, 10.100], loss: 0.001520, mae: 0.040873, mean_q: 1.233697
 36185/100000: episode: 689, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 11.667, mean reward: 0.729 [0.685, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.143, 10.100], loss: 0.002487, mae: 0.052448, mean_q: 1.254696
 36201/100000: episode: 690, duration: 0.097s, episode steps: 16, steps per second: 166, episode reward: 11.226, mean reward: 0.702 [0.622, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.400, 10.100], loss: 0.001740, mae: 0.043976, mean_q: 1.254556
 36217/100000: episode: 691, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 11.419, mean reward: 0.714 [0.575, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.410, 10.100], loss: 0.001573, mae: 0.042966, mean_q: 1.254616
 36233/100000: episode: 692, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 11.531, mean reward: 0.721 [0.652, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.427, 10.100], loss: 0.002009, mae: 0.047527, mean_q: 1.245568
 36267/100000: episode: 693, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 23.333, mean reward: 0.686 [0.609, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.380, 10.390], loss: 0.001772, mae: 0.044239, mean_q: 1.243442
 36283/100000: episode: 694, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 10.883, mean reward: 0.680 [0.596, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.223, 10.100], loss: 0.001331, mae: 0.040533, mean_q: 1.253590
 36299/100000: episode: 695, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 10.693, mean reward: 0.668 [0.614, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.971, 10.100], loss: 0.002314, mae: 0.047920, mean_q: 1.244649
 36315/100000: episode: 696, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 11.481, mean reward: 0.718 [0.610, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.848, 10.100], loss: 0.001898, mae: 0.045284, mean_q: 1.253883
 36347/100000: episode: 697, duration: 0.161s, episode steps: 32, steps per second: 198, episode reward: 23.841, mean reward: 0.745 [0.635, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.417, 10.541], loss: 0.001676, mae: 0.042641, mean_q: 1.265775
 36363/100000: episode: 698, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 11.448, mean reward: 0.715 [0.640, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.236, 10.100], loss: 0.001681, mae: 0.043826, mean_q: 1.258377
 36379/100000: episode: 699, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 11.146, mean reward: 0.697 [0.670, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.912, 10.100], loss: 0.001502, mae: 0.042310, mean_q: 1.262570
[Info] 3-TH LEVEL FOUND: 1.5326027870178223, Considering 12/88 traces
 36411/100000: episode: 700, duration: 4.308s, episode steps: 32, steps per second: 7, episode reward: 21.163, mean reward: 0.661 [0.573, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.709, 10.326], loss: 0.001692, mae: 0.043769, mean_q: 1.262559
 36445/100000: episode: 701, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 24.154, mean reward: 0.710 [0.616, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.184, 10.395], loss: 0.001553, mae: 0.041957, mean_q: 1.260262
 36479/100000: episode: 702, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 26.705, mean reward: 0.785 [0.706, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.625, 10.517], loss: 0.001664, mae: 0.043431, mean_q: 1.262662
 36501/100000: episode: 703, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 16.737, mean reward: 0.761 [0.645, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.716, 10.408], loss: 0.002858, mae: 0.052567, mean_q: 1.266871
 36535/100000: episode: 704, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 24.177, mean reward: 0.711 [0.535, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.035, 10.306], loss: 0.002137, mae: 0.048125, mean_q: 1.261562
 36569/100000: episode: 705, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 20.925, mean reward: 0.615 [0.500, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.629, 10.100], loss: 0.002195, mae: 0.048125, mean_q: 1.252269
 36603/100000: episode: 706, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 25.891, mean reward: 0.762 [0.604, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.088, 10.414], loss: 0.001355, mae: 0.041001, mean_q: 1.252349
 36634/100000: episode: 707, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 21.314, mean reward: 0.688 [0.601, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.625, 10.478], loss: 0.001328, mae: 0.039853, mean_q: 1.271845
 36668/100000: episode: 708, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 26.434, mean reward: 0.777 [0.662, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.101, 10.490], loss: 0.001571, mae: 0.042960, mean_q: 1.264233
 36699/100000: episode: 709, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 22.617, mean reward: 0.730 [0.664, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.157, 10.408], loss: 0.002083, mae: 0.046951, mean_q: 1.266240
 36733/100000: episode: 710, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 22.066, mean reward: 0.649 [0.561, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.761, 10.237], loss: 0.001525, mae: 0.041590, mean_q: 1.274619
 36764/100000: episode: 711, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 22.608, mean reward: 0.729 [0.614, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.110, 10.389], loss: 0.001483, mae: 0.041692, mean_q: 1.266782
 36798/100000: episode: 712, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 22.393, mean reward: 0.659 [0.547, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.177, 10.276], loss: 0.001748, mae: 0.044714, mean_q: 1.268436
 36829/100000: episode: 713, duration: 0.159s, episode steps: 31, steps per second: 196, episode reward: 21.167, mean reward: 0.683 [0.549, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.326, 10.243], loss: 0.001393, mae: 0.040531, mean_q: 1.261971
 36863/100000: episode: 714, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 24.648, mean reward: 0.725 [0.642, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.265, 10.381], loss: 0.001376, mae: 0.039676, mean_q: 1.275591
 36897/100000: episode: 715, duration: 0.163s, episode steps: 34, steps per second: 208, episode reward: 25.821, mean reward: 0.759 [0.676, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.359, 10.362], loss: 0.001268, mae: 0.038631, mean_q: 1.270710
 36928/100000: episode: 716, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 23.159, mean reward: 0.747 [0.667, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-2.144, 10.397], loss: 0.001324, mae: 0.040374, mean_q: 1.272498
 36962/100000: episode: 717, duration: 0.165s, episode steps: 34, steps per second: 206, episode reward: 23.489, mean reward: 0.691 [0.568, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.091, 10.289], loss: 0.001314, mae: 0.039055, mean_q: 1.280962
 36993/100000: episode: 718, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 23.478, mean reward: 0.757 [0.620, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.002, 10.346], loss: 0.001443, mae: 0.041367, mean_q: 1.280402
 37023/100000: episode: 719, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 22.291, mean reward: 0.743 [0.556, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.188, 10.312], loss: 0.001940, mae: 0.046477, mean_q: 1.287366
 37057/100000: episode: 720, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 22.054, mean reward: 0.649 [0.545, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.148, 10.217], loss: 0.001733, mae: 0.044477, mean_q: 1.280143
 37091/100000: episode: 721, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 28.401, mean reward: 0.835 [0.747, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.419, 10.659], loss: 0.001675, mae: 0.043666, mean_q: 1.282981
 37122/100000: episode: 722, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 20.212, mean reward: 0.652 [0.533, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.161, 10.196], loss: 0.001405, mae: 0.040352, mean_q: 1.288534
 37156/100000: episode: 723, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 23.163, mean reward: 0.681 [0.592, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.547, 10.389], loss: 0.001345, mae: 0.040319, mean_q: 1.290642
 37186/100000: episode: 724, duration: 0.164s, episode steps: 30, steps per second: 182, episode reward: 24.407, mean reward: 0.814 [0.741, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.971, 10.597], loss: 0.001303, mae: 0.039405, mean_q: 1.295630
 37220/100000: episode: 725, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 23.955, mean reward: 0.705 [0.632, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-1.106, 10.349], loss: 0.001423, mae: 0.042335, mean_q: 1.308076
 37228/100000: episode: 726, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 5.947, mean reward: 0.743 [0.725, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.464, 10.100], loss: 0.001539, mae: 0.040573, mean_q: 1.289519
 37258/100000: episode: 727, duration: 0.170s, episode steps: 30, steps per second: 177, episode reward: 22.112, mean reward: 0.737 [0.613, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.171, 10.394], loss: 0.001636, mae: 0.044138, mean_q: 1.291309
 37292/100000: episode: 728, duration: 0.181s, episode steps: 34, steps per second: 187, episode reward: 24.343, mean reward: 0.716 [0.621, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.035, 10.432], loss: 0.001416, mae: 0.041177, mean_q: 1.280811
 37322/100000: episode: 729, duration: 0.173s, episode steps: 30, steps per second: 174, episode reward: 20.840, mean reward: 0.695 [0.588, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.176, 10.365], loss: 0.001295, mae: 0.039887, mean_q: 1.294547
 37356/100000: episode: 730, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 22.935, mean reward: 0.675 [0.624, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.985, 10.346], loss: 0.001750, mae: 0.045329, mean_q: 1.296430
 37390/100000: episode: 731, duration: 0.175s, episode steps: 34, steps per second: 195, episode reward: 25.599, mean reward: 0.753 [0.561, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.597, 10.346], loss: 0.001367, mae: 0.040622, mean_q: 1.286589
 37398/100000: episode: 732, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 5.851, mean reward: 0.731 [0.613, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.540, 10.100], loss: 0.001419, mae: 0.041376, mean_q: 1.292243
 37429/100000: episode: 733, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 21.771, mean reward: 0.702 [0.578, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.101, 10.256], loss: 0.001663, mae: 0.044036, mean_q: 1.292999
 37463/100000: episode: 734, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 22.930, mean reward: 0.674 [0.552, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.237, 10.276], loss: 0.002050, mae: 0.047304, mean_q: 1.289590
 37497/100000: episode: 735, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 21.959, mean reward: 0.646 [0.530, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.158, 10.197], loss: 0.001642, mae: 0.043911, mean_q: 1.295263
 37531/100000: episode: 736, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 22.226, mean reward: 0.654 [0.507, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.451, 10.100], loss: 0.001591, mae: 0.044157, mean_q: 1.305544
 37562/100000: episode: 737, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 21.335, mean reward: 0.688 [0.505, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.087, 10.189], loss: 0.001380, mae: 0.041075, mean_q: 1.304610
 37596/100000: episode: 738, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 26.061, mean reward: 0.767 [0.697, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.456], loss: 0.001146, mae: 0.037242, mean_q: 1.290522
 37630/100000: episode: 739, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 22.227, mean reward: 0.654 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.296, 10.144], loss: 0.001280, mae: 0.039809, mean_q: 1.294373
 37664/100000: episode: 740, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 25.382, mean reward: 0.747 [0.640, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.592, 10.387], loss: 0.001335, mae: 0.040615, mean_q: 1.309530
 37698/100000: episode: 741, duration: 0.196s, episode steps: 34, steps per second: 173, episode reward: 22.197, mean reward: 0.653 [0.504, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.099, 10.100], loss: 0.001439, mae: 0.042138, mean_q: 1.304502
 37732/100000: episode: 742, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 23.956, mean reward: 0.705 [0.599, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.786, 10.420], loss: 0.001450, mae: 0.042945, mean_q: 1.301126
 37740/100000: episode: 743, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 6.068, mean reward: 0.758 [0.711, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.648, 10.100], loss: 0.001157, mae: 0.038830, mean_q: 1.291160
 37774/100000: episode: 744, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 23.430, mean reward: 0.689 [0.596, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.124, 10.329], loss: 0.002245, mae: 0.048628, mean_q: 1.310817
 37808/100000: episode: 745, duration: 0.201s, episode steps: 34, steps per second: 169, episode reward: 21.185, mean reward: 0.623 [0.531, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.088, 10.223], loss: 0.001822, mae: 0.044935, mean_q: 1.320482
 37842/100000: episode: 746, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 22.969, mean reward: 0.676 [0.556, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.275, 10.293], loss: 0.001287, mae: 0.040446, mean_q: 1.318223
 37876/100000: episode: 747, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 23.248, mean reward: 0.684 [0.558, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.682, 10.150], loss: 0.002716, mae: 0.052162, mean_q: 1.300948
 37910/100000: episode: 748, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 24.602, mean reward: 0.724 [0.624, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.552, 10.543], loss: 0.001692, mae: 0.043811, mean_q: 1.320659
 37944/100000: episode: 749, duration: 0.168s, episode steps: 34, steps per second: 203, episode reward: 25.435, mean reward: 0.748 [0.654, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.807, 10.409], loss: 0.001313, mae: 0.039879, mean_q: 1.323270
 37975/100000: episode: 750, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 22.436, mean reward: 0.724 [0.618, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.213, 10.407], loss: 0.001366, mae: 0.041497, mean_q: 1.313894
 38005/100000: episode: 751, duration: 0.177s, episode steps: 30, steps per second: 169, episode reward: 21.801, mean reward: 0.727 [0.665, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.545, 10.476], loss: 0.001416, mae: 0.042061, mean_q: 1.313908
 38013/100000: episode: 752, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 5.683, mean reward: 0.710 [0.675, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.373, 10.100], loss: 0.001115, mae: 0.036851, mean_q: 1.295123
 38035/100000: episode: 753, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 16.204, mean reward: 0.737 [0.661, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.310, 10.416], loss: 0.001426, mae: 0.042206, mean_q: 1.324422
 38057/100000: episode: 754, duration: 0.128s, episode steps: 22, steps per second: 171, episode reward: 17.140, mean reward: 0.779 [0.724, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.534], loss: 0.001259, mae: 0.039679, mean_q: 1.322493
 38065/100000: episode: 755, duration: 0.056s, episode steps: 8, steps per second: 144, episode reward: 5.873, mean reward: 0.734 [0.677, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.405, 10.100], loss: 0.001250, mae: 0.038129, mean_q: 1.312916
 38095/100000: episode: 756, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 22.794, mean reward: 0.760 [0.703, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.411, 10.479], loss: 0.001280, mae: 0.039874, mean_q: 1.309800
 38117/100000: episode: 757, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 18.020, mean reward: 0.819 [0.724, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.202, 10.581], loss: 0.001536, mae: 0.042893, mean_q: 1.303089
 38151/100000: episode: 758, duration: 0.211s, episode steps: 34, steps per second: 161, episode reward: 26.921, mean reward: 0.792 [0.723, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.035, 10.477], loss: 0.001412, mae: 0.041655, mean_q: 1.309613
 38175/100000: episode: 759, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 17.445, mean reward: 0.727 [0.649, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.402], loss: 0.001264, mae: 0.039589, mean_q: 1.325831
 38206/100000: episode: 760, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 22.039, mean reward: 0.711 [0.611, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.662, 10.397], loss: 0.001339, mae: 0.040683, mean_q: 1.314877
 38240/100000: episode: 761, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 22.472, mean reward: 0.661 [0.553, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.779, 10.256], loss: 0.001238, mae: 0.038726, mean_q: 1.325024
 38274/100000: episode: 762, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 25.587, mean reward: 0.753 [0.680, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-1.258, 10.455], loss: 0.001316, mae: 0.040941, mean_q: 1.324476
 38308/100000: episode: 763, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 22.605, mean reward: 0.665 [0.525, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.419, 10.282], loss: 0.001297, mae: 0.039510, mean_q: 1.323498
 38342/100000: episode: 764, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 22.912, mean reward: 0.674 [0.583, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.742, 10.349], loss: 0.001593, mae: 0.042889, mean_q: 1.339053
 38376/100000: episode: 765, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 21.960, mean reward: 0.646 [0.572, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.162, 10.272], loss: 0.001440, mae: 0.042208, mean_q: 1.325933
 38384/100000: episode: 766, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 6.310, mean reward: 0.789 [0.753, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.463, 10.100], loss: 0.001313, mae: 0.040640, mean_q: 1.311547
 38418/100000: episode: 767, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 21.271, mean reward: 0.626 [0.518, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.590, 10.100], loss: 0.001738, mae: 0.043880, mean_q: 1.333943
 38449/100000: episode: 768, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 22.632, mean reward: 0.730 [0.623, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.035, 10.410], loss: 0.001495, mae: 0.042568, mean_q: 1.317955
 38483/100000: episode: 769, duration: 0.187s, episode steps: 34, steps per second: 181, episode reward: 25.006, mean reward: 0.735 [0.637, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.777, 10.434], loss: 0.001408, mae: 0.041554, mean_q: 1.317321
 38505/100000: episode: 770, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 14.579, mean reward: 0.663 [0.581, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.381], loss: 0.001194, mae: 0.038202, mean_q: 1.342570
 38529/100000: episode: 771, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 18.660, mean reward: 0.777 [0.696, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.760, 10.500], loss: 0.001434, mae: 0.041862, mean_q: 1.331793
 38553/100000: episode: 772, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 17.338, mean reward: 0.722 [0.626, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.587, 10.362], loss: 0.001297, mae: 0.040441, mean_q: 1.326640
 38587/100000: episode: 773, duration: 0.170s, episode steps: 34, steps per second: 200, episode reward: 25.329, mean reward: 0.745 [0.658, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.326, 10.445], loss: 0.001281, mae: 0.040122, mean_q: 1.330608
 38621/100000: episode: 774, duration: 0.194s, episode steps: 34, steps per second: 175, episode reward: 23.321, mean reward: 0.686 [0.548, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.294], loss: 0.001216, mae: 0.039155, mean_q: 1.328667
 38652/100000: episode: 775, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 21.828, mean reward: 0.704 [0.560, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.286], loss: 0.001226, mae: 0.038520, mean_q: 1.338225
 38686/100000: episode: 776, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 25.526, mean reward: 0.751 [0.688, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-1.108, 10.434], loss: 0.001336, mae: 0.040539, mean_q: 1.334573
 38708/100000: episode: 777, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 16.399, mean reward: 0.745 [0.642, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.420, 10.399], loss: 0.001457, mae: 0.041643, mean_q: 1.340532
 38742/100000: episode: 778, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 23.056, mean reward: 0.678 [0.578, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.593, 10.387], loss: 0.001295, mae: 0.039825, mean_q: 1.346421
 38776/100000: episode: 779, duration: 0.170s, episode steps: 34, steps per second: 200, episode reward: 24.658, mean reward: 0.725 [0.583, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.273, 10.366], loss: 0.001338, mae: 0.040226, mean_q: 1.330013
 38810/100000: episode: 780, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 26.014, mean reward: 0.765 [0.698, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.344, 10.500], loss: 0.001481, mae: 0.042882, mean_q: 1.344973
 38844/100000: episode: 781, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 22.909, mean reward: 0.674 [0.535, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.716, 10.262], loss: 0.001210, mae: 0.038345, mean_q: 1.332110
 38878/100000: episode: 782, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 26.325, mean reward: 0.774 [0.660, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.677, 10.488], loss: 0.001342, mae: 0.040205, mean_q: 1.338190
 38909/100000: episode: 783, duration: 0.175s, episode steps: 31, steps per second: 178, episode reward: 19.858, mean reward: 0.641 [0.517, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.461, 10.100], loss: 0.001592, mae: 0.043554, mean_q: 1.334721
 38940/100000: episode: 784, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 20.726, mean reward: 0.669 [0.554, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.270, 10.237], loss: 0.001226, mae: 0.039218, mean_q: 1.334911
[Info] FALSIFICATION!
 38954/100000: episode: 785, duration: 0.344s, episode steps: 14, steps per second: 41, episode reward: 11.552, mean reward: 0.825 [0.752, 1.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-1.025, 9.404], loss: 0.001249, mae: 0.039812, mean_q: 1.339795
 38985/100000: episode: 786, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 22.790, mean reward: 0.735 [0.537, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.522, 10.276], loss: 0.001639, mae: 0.043933, mean_q: 1.336862
 39019/100000: episode: 787, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 22.122, mean reward: 0.651 [0.541, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.629, 10.284], loss: 0.002107, mae: 0.047708, mean_q: 1.347889
[Info] Complete ISplit Iteration
[Info] Levels: [1.3803904, 1.4373296, 1.5326028, 1.6294403]
[Info] Cond. Prob: [0.1, 0.1, 0.12, 0.13]
[Info] Error Prob: 0.00015600000000000002

 39053/100000: episode: 788, duration: 4.548s, episode steps: 34, steps per second: 7, episode reward: 23.433, mean reward: 0.689 [0.615, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.062, 10.248], loss: 0.001319, mae: 0.040894, mean_q: 1.333738
 39153/100000: episode: 789, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.615, mean reward: 0.586 [0.502, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.245, 10.131], loss: 0.001280, mae: 0.039516, mean_q: 1.343948
 39253/100000: episode: 790, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.475, mean reward: 0.585 [0.511, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.257, 10.098], loss: 0.001287, mae: 0.039726, mean_q: 1.341227
 39353/100000: episode: 791, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.903, mean reward: 0.599 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.836, 10.204], loss: 0.001417, mae: 0.040541, mean_q: 1.341558
 39453/100000: episode: 792, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 56.912, mean reward: 0.569 [0.499, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.141, 10.098], loss: 0.001431, mae: 0.041268, mean_q: 1.343223
 39553/100000: episode: 793, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.227, mean reward: 0.582 [0.504, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.907, 10.246], loss: 0.001334, mae: 0.039919, mean_q: 1.339148
 39653/100000: episode: 794, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 60.222, mean reward: 0.602 [0.510, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.000, 10.098], loss: 0.001644, mae: 0.043372, mean_q: 1.335019
 39753/100000: episode: 795, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.192, mean reward: 0.592 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.438, 10.291], loss: 0.001898, mae: 0.046121, mean_q: 1.332721
 39853/100000: episode: 796, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 60.269, mean reward: 0.603 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.056, 10.251], loss: 0.001590, mae: 0.042532, mean_q: 1.333543
 39953/100000: episode: 797, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 57.618, mean reward: 0.576 [0.506, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.860, 10.343], loss: 0.001445, mae: 0.040008, mean_q: 1.326692
 40053/100000: episode: 798, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.952, mean reward: 0.580 [0.498, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.838, 10.210], loss: 0.001602, mae: 0.043339, mean_q: 1.321346
 40153/100000: episode: 799, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.652, mean reward: 0.587 [0.519, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.001, 10.098], loss: 0.001382, mae: 0.039900, mean_q: 1.322546
 40253/100000: episode: 800, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.999, mean reward: 0.590 [0.509, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.412, 10.099], loss: 0.001423, mae: 0.040531, mean_q: 1.314671
 40353/100000: episode: 801, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 60.383, mean reward: 0.604 [0.500, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.846, 10.098], loss: 0.001590, mae: 0.042970, mean_q: 1.309567
 40453/100000: episode: 802, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.272, mean reward: 0.583 [0.506, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.539, 10.098], loss: 0.001507, mae: 0.041371, mean_q: 1.309599
 40553/100000: episode: 803, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.809, mean reward: 0.588 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.232, 10.309], loss: 0.001324, mae: 0.039693, mean_q: 1.302161
 40653/100000: episode: 804, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 61.597, mean reward: 0.616 [0.501, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.784, 10.292], loss: 0.001609, mae: 0.042338, mean_q: 1.302747
 40753/100000: episode: 805, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.802, mean reward: 0.588 [0.509, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.789, 10.224], loss: 0.001592, mae: 0.042682, mean_q: 1.305403
 40853/100000: episode: 806, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.124, mean reward: 0.581 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.775, 10.350], loss: 0.001474, mae: 0.040937, mean_q: 1.290103
 40953/100000: episode: 807, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.961, mean reward: 0.590 [0.505, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.650, 10.152], loss: 0.001531, mae: 0.041758, mean_q: 1.284509
 41053/100000: episode: 808, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.272, mean reward: 0.593 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.554, 10.098], loss: 0.001483, mae: 0.042126, mean_q: 1.285773
 41153/100000: episode: 809, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 62.311, mean reward: 0.623 [0.518, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.585, 10.098], loss: 0.001543, mae: 0.041777, mean_q: 1.283392
 41253/100000: episode: 810, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.993, mean reward: 0.580 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.940, 10.098], loss: 0.001394, mae: 0.040188, mean_q: 1.277745
 41353/100000: episode: 811, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 61.412, mean reward: 0.614 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.710, 10.098], loss: 0.001449, mae: 0.041485, mean_q: 1.276344
 41453/100000: episode: 812, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.922, mean reward: 0.599 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.551, 10.098], loss: 0.001418, mae: 0.040624, mean_q: 1.271199
 41553/100000: episode: 813, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 61.290, mean reward: 0.613 [0.508, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.225, 10.400], loss: 0.001477, mae: 0.041099, mean_q: 1.271674
 41653/100000: episode: 814, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 60.168, mean reward: 0.602 [0.509, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.507, 10.098], loss: 0.001482, mae: 0.041228, mean_q: 1.261592
 41753/100000: episode: 815, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.272, mean reward: 0.583 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.687, 10.219], loss: 0.001379, mae: 0.040577, mean_q: 1.261819
 41853/100000: episode: 816, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.773, mean reward: 0.588 [0.504, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.598, 10.161], loss: 0.001546, mae: 0.042709, mean_q: 1.261048
 41953/100000: episode: 817, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.873, mean reward: 0.579 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.650, 10.098], loss: 0.001607, mae: 0.043450, mean_q: 1.252144
 42053/100000: episode: 818, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.628, mean reward: 0.586 [0.517, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.226, 10.396], loss: 0.001582, mae: 0.043337, mean_q: 1.252160
 42153/100000: episode: 819, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.424, mean reward: 0.584 [0.515, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.925, 10.261], loss: 0.001620, mae: 0.043081, mean_q: 1.247766
 42253/100000: episode: 820, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 61.050, mean reward: 0.611 [0.509, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.610, 10.098], loss: 0.001560, mae: 0.041971, mean_q: 1.236517
 42353/100000: episode: 821, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.198, mean reward: 0.592 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.512, 10.098], loss: 0.001553, mae: 0.042389, mean_q: 1.238654
 42453/100000: episode: 822, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.312, mean reward: 0.603 [0.512, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.855, 10.098], loss: 0.001657, mae: 0.043330, mean_q: 1.231670
 42553/100000: episode: 823, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.897, mean reward: 0.579 [0.499, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.348, 10.255], loss: 0.001672, mae: 0.044139, mean_q: 1.224331
 42653/100000: episode: 824, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.559, mean reward: 0.586 [0.506, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.914, 10.104], loss: 0.001586, mae: 0.042507, mean_q: 1.221529
 42753/100000: episode: 825, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.071, mean reward: 0.581 [0.497, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.508, 10.098], loss: 0.001493, mae: 0.042091, mean_q: 1.217586
 42853/100000: episode: 826, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 64.715, mean reward: 0.647 [0.506, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.942, 10.321], loss: 0.001618, mae: 0.042795, mean_q: 1.220835
 42953/100000: episode: 827, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.485, mean reward: 0.585 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.682, 10.136], loss: 0.001469, mae: 0.041917, mean_q: 1.214915
 43053/100000: episode: 828, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.605, mean reward: 0.576 [0.506, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.142, 10.104], loss: 0.001523, mae: 0.041872, mean_q: 1.211784
 43153/100000: episode: 829, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.304, mean reward: 0.583 [0.503, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.074, 10.159], loss: 0.001578, mae: 0.042899, mean_q: 1.206037
 43253/100000: episode: 830, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 61.543, mean reward: 0.615 [0.512, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.038, 10.098], loss: 0.001518, mae: 0.042359, mean_q: 1.202619
 43353/100000: episode: 831, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.821, mean reward: 0.588 [0.501, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.455, 10.098], loss: 0.001486, mae: 0.041948, mean_q: 1.197980
 43453/100000: episode: 832, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.636, mean reward: 0.576 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.693, 10.104], loss: 0.001548, mae: 0.042498, mean_q: 1.201484
 43553/100000: episode: 833, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 61.393, mean reward: 0.614 [0.510, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.389, 10.385], loss: 0.001478, mae: 0.042067, mean_q: 1.192560
 43653/100000: episode: 834, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.691, mean reward: 0.587 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.714, 10.098], loss: 0.001433, mae: 0.040826, mean_q: 1.188655
 43753/100000: episode: 835, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.344, mean reward: 0.593 [0.498, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.758, 10.187], loss: 0.001530, mae: 0.042949, mean_q: 1.183233
 43853/100000: episode: 836, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 59.022, mean reward: 0.590 [0.500, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.154, 10.098], loss: 0.001577, mae: 0.042905, mean_q: 1.179945
 43953/100000: episode: 837, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.783, mean reward: 0.598 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.795, 10.221], loss: 0.001408, mae: 0.041205, mean_q: 1.171728
 44053/100000: episode: 838, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.875, mean reward: 0.579 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.563, 10.125], loss: 0.001564, mae: 0.043151, mean_q: 1.171934
 44153/100000: episode: 839, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 60.955, mean reward: 0.610 [0.516, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.614, 10.137], loss: 0.001502, mae: 0.042352, mean_q: 1.172517
 44253/100000: episode: 840, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 62.565, mean reward: 0.626 [0.521, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.168, 10.287], loss: 0.001385, mae: 0.041295, mean_q: 1.176217
 44353/100000: episode: 841, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.123, mean reward: 0.591 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.243, 10.098], loss: 0.001428, mae: 0.041838, mean_q: 1.177758
 44453/100000: episode: 842, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.460, mean reward: 0.575 [0.505, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.782, 10.098], loss: 0.001351, mae: 0.040795, mean_q: 1.175645
 44553/100000: episode: 843, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.544, mean reward: 0.605 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.709, 10.098], loss: 0.001421, mae: 0.042082, mean_q: 1.175352
 44653/100000: episode: 844, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.812, mean reward: 0.598 [0.506, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.115, 10.340], loss: 0.001392, mae: 0.041031, mean_q: 1.176336
 44753/100000: episode: 845, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.891, mean reward: 0.589 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.768, 10.098], loss: 0.001380, mae: 0.040488, mean_q: 1.174173
 44853/100000: episode: 846, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.216, mean reward: 0.592 [0.508, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.853, 10.098], loss: 0.001419, mae: 0.041624, mean_q: 1.177370
 44953/100000: episode: 847, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 59.498, mean reward: 0.595 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.342, 10.329], loss: 0.001582, mae: 0.043419, mean_q: 1.175476
 45053/100000: episode: 848, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.175, mean reward: 0.582 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.486, 10.261], loss: 0.001404, mae: 0.041300, mean_q: 1.176884
 45153/100000: episode: 849, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.617, mean reward: 0.586 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.480, 10.280], loss: 0.001328, mae: 0.040369, mean_q: 1.174398
 45253/100000: episode: 850, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.309, mean reward: 0.573 [0.507, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.722, 10.267], loss: 0.001399, mae: 0.041160, mean_q: 1.173235
 45353/100000: episode: 851, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.376, mean reward: 0.574 [0.501, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.621, 10.098], loss: 0.001402, mae: 0.040926, mean_q: 1.173447
 45453/100000: episode: 852, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 60.448, mean reward: 0.604 [0.503, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.952, 10.098], loss: 0.001561, mae: 0.043392, mean_q: 1.173656
 45553/100000: episode: 853, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.233, mean reward: 0.582 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.689, 10.098], loss: 0.001324, mae: 0.040351, mean_q: 1.173259
 45653/100000: episode: 854, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 61.837, mean reward: 0.618 [0.509, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.424, 10.148], loss: 0.001428, mae: 0.041306, mean_q: 1.170732
 45753/100000: episode: 855, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 61.319, mean reward: 0.613 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.555, 10.098], loss: 0.001341, mae: 0.040234, mean_q: 1.174064
 45853/100000: episode: 856, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 56.957, mean reward: 0.570 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.317, 10.124], loss: 0.001306, mae: 0.039685, mean_q: 1.173707
 45953/100000: episode: 857, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.745, mean reward: 0.597 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.549, 10.353], loss: 0.001403, mae: 0.041148, mean_q: 1.175606
 46053/100000: episode: 858, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.789, mean reward: 0.588 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.421, 10.098], loss: 0.001370, mae: 0.041204, mean_q: 1.175520
 46153/100000: episode: 859, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 61.234, mean reward: 0.612 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.743, 10.098], loss: 0.001349, mae: 0.040731, mean_q: 1.173484
 46253/100000: episode: 860, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.022, mean reward: 0.590 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.510, 10.440], loss: 0.001362, mae: 0.040254, mean_q: 1.174851
 46353/100000: episode: 861, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.637, mean reward: 0.586 [0.510, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.094, 10.152], loss: 0.001244, mae: 0.038892, mean_q: 1.171755
 46453/100000: episode: 862, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.824, mean reward: 0.578 [0.499, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.924, 10.099], loss: 0.001485, mae: 0.041997, mean_q: 1.173904
 46553/100000: episode: 863, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 61.166, mean reward: 0.612 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.138, 10.098], loss: 0.001472, mae: 0.042241, mean_q: 1.173252
 46653/100000: episode: 864, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 59.183, mean reward: 0.592 [0.499, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.602, 10.190], loss: 0.001372, mae: 0.041050, mean_q: 1.173759
 46753/100000: episode: 865, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.777, mean reward: 0.588 [0.506, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.203, 10.098], loss: 0.001335, mae: 0.040546, mean_q: 1.172626
 46853/100000: episode: 866, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 56.956, mean reward: 0.570 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.301, 10.217], loss: 0.001522, mae: 0.042697, mean_q: 1.169953
 46953/100000: episode: 867, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.583, mean reward: 0.576 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.712, 10.184], loss: 0.001247, mae: 0.039284, mean_q: 1.171001
 47053/100000: episode: 868, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 60.087, mean reward: 0.601 [0.502, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.543, 10.343], loss: 0.001366, mae: 0.040785, mean_q: 1.169239
 47153/100000: episode: 869, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.487, mean reward: 0.585 [0.510, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.854, 10.098], loss: 0.001297, mae: 0.040034, mean_q: 1.174100
 47253/100000: episode: 870, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.542, mean reward: 0.585 [0.514, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.098, 10.294], loss: 0.001392, mae: 0.041381, mean_q: 1.172762
 47353/100000: episode: 871, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.738, mean reward: 0.587 [0.502, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.505, 10.182], loss: 0.001453, mae: 0.041699, mean_q: 1.172751
 47453/100000: episode: 872, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.413, mean reward: 0.574 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.907, 10.233], loss: 0.001462, mae: 0.041944, mean_q: 1.169233
 47553/100000: episode: 873, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 61.030, mean reward: 0.610 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.364, 10.098], loss: 0.001354, mae: 0.040331, mean_q: 1.169443
 47653/100000: episode: 874, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 61.264, mean reward: 0.613 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.761, 10.443], loss: 0.001355, mae: 0.040900, mean_q: 1.174170
 47753/100000: episode: 875, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.448, mean reward: 0.574 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.132, 10.121], loss: 0.001332, mae: 0.040421, mean_q: 1.174268
 47853/100000: episode: 876, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.436, mean reward: 0.574 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.962, 10.098], loss: 0.001301, mae: 0.039820, mean_q: 1.173307
 47953/100000: episode: 877, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 61.337, mean reward: 0.613 [0.507, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.205, 10.213], loss: 0.001454, mae: 0.041282, mean_q: 1.168959
 48053/100000: episode: 878, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.821, mean reward: 0.578 [0.501, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.595, 10.098], loss: 0.001341, mae: 0.039817, mean_q: 1.169975
 48153/100000: episode: 879, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 64.321, mean reward: 0.643 [0.508, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.353, 10.439], loss: 0.001276, mae: 0.039704, mean_q: 1.171643
 48253/100000: episode: 880, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.611, mean reward: 0.576 [0.502, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.371, 10.216], loss: 0.001277, mae: 0.039741, mean_q: 1.172603
 48353/100000: episode: 881, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.535, mean reward: 0.575 [0.508, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.467, 10.098], loss: 0.001322, mae: 0.040386, mean_q: 1.169720
 48453/100000: episode: 882, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 60.487, mean reward: 0.605 [0.520, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.490, 10.224], loss: 0.001371, mae: 0.040053, mean_q: 1.168218
 48553/100000: episode: 883, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.873, mean reward: 0.579 [0.499, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.537, 10.148], loss: 0.001495, mae: 0.042694, mean_q: 1.174253
 48653/100000: episode: 884, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 65.740, mean reward: 0.657 [0.512, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.982, 10.098], loss: 0.001240, mae: 0.038959, mean_q: 1.172210
 48753/100000: episode: 885, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.093, mean reward: 0.591 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.610, 10.098], loss: 0.001328, mae: 0.040719, mean_q: 1.173935
 48853/100000: episode: 886, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.312, mean reward: 0.593 [0.500, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.956, 10.384], loss: 0.001282, mae: 0.039315, mean_q: 1.174971
 48953/100000: episode: 887, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.543, mean reward: 0.585 [0.514, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.778, 10.292], loss: 0.001397, mae: 0.041554, mean_q: 1.173259
[Info] 1-TH LEVEL FOUND: 1.37153959274292, Considering 10/90 traces
 49053/100000: episode: 888, duration: 4.744s, episode steps: 100, steps per second: 21, episode reward: 58.005, mean reward: 0.580 [0.498, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.612, 10.098], loss: 0.001289, mae: 0.039965, mean_q: 1.176871
 49063/100000: episode: 889, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 6.998, mean reward: 0.700 [0.636, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.505], loss: 0.001319, mae: 0.040815, mean_q: 1.180662
 49106/100000: episode: 890, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 32.279, mean reward: 0.751 [0.651, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.370, 10.498], loss: 0.001265, mae: 0.039702, mean_q: 1.173513
 49167/100000: episode: 891, duration: 0.332s, episode steps: 61, steps per second: 184, episode reward: 41.150, mean reward: 0.675 [0.554, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.802 [-1.315, 10.403], loss: 0.001226, mae: 0.038402, mean_q: 1.181465
 49189/100000: episode: 892, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 15.684, mean reward: 0.713 [0.629, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.233, 10.424], loss: 0.001135, mae: 0.037176, mean_q: 1.173123
 49218/100000: episode: 893, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 20.186, mean reward: 0.696 [0.643, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.100, 10.541], loss: 0.001244, mae: 0.039242, mean_q: 1.176691
 49247/100000: episode: 894, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 21.727, mean reward: 0.749 [0.674, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.322, 10.459], loss: 0.001281, mae: 0.039322, mean_q: 1.177120
 49269/100000: episode: 895, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 16.168, mean reward: 0.735 [0.656, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.551], loss: 0.001321, mae: 0.040669, mean_q: 1.179325
 49319/100000: episode: 896, duration: 0.263s, episode steps: 50, steps per second: 190, episode reward: 36.349, mean reward: 0.727 [0.627, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.454, 10.459], loss: 0.001417, mae: 0.041291, mean_q: 1.182954
 49329/100000: episode: 897, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 6.853, mean reward: 0.685 [0.632, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-1.347, 10.451], loss: 0.001942, mae: 0.046016, mean_q: 1.174583
 49335/100000: episode: 898, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 4.883, mean reward: 0.814 [0.750, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.623], loss: 0.001394, mae: 0.041470, mean_q: 1.184871
 49378/100000: episode: 899, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 30.556, mean reward: 0.711 [0.632, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.211, 10.508], loss: 0.001493, mae: 0.040950, mean_q: 1.187603
 49439/100000: episode: 900, duration: 0.317s, episode steps: 61, steps per second: 192, episode reward: 37.388, mean reward: 0.613 [0.501, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.811 [-0.237, 10.151], loss: 0.001572, mae: 0.041557, mean_q: 1.185673
 49449/100000: episode: 901, duration: 0.064s, episode steps: 10, steps per second: 155, episode reward: 7.578, mean reward: 0.758 [0.651, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.326, 10.571], loss: 0.001697, mae: 0.044700, mean_q: 1.188332
 49455/100000: episode: 902, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 4.146, mean reward: 0.691 [0.650, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.466], loss: 0.001735, mae: 0.043704, mean_q: 1.193806
 49505/100000: episode: 903, duration: 0.284s, episode steps: 50, steps per second: 176, episode reward: 30.280, mean reward: 0.606 [0.516, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.374, 10.192], loss: 0.001889, mae: 0.046365, mean_q: 1.188408
 49515/100000: episode: 904, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 6.417, mean reward: 0.642 [0.606, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.319], loss: 0.001397, mae: 0.040599, mean_q: 1.181617
 49565/100000: episode: 905, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 38.383, mean reward: 0.768 [0.672, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.941, 10.430], loss: 0.001646, mae: 0.043106, mean_q: 1.190526
 49571/100000: episode: 906, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 3.831, mean reward: 0.639 [0.607, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.378], loss: 0.001756, mae: 0.044591, mean_q: 1.183042
 49621/100000: episode: 907, duration: 0.260s, episode steps: 50, steps per second: 192, episode reward: 34.319, mean reward: 0.686 [0.532, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.001, 10.283], loss: 0.001587, mae: 0.042885, mean_q: 1.190917
 49682/100000: episode: 908, duration: 0.327s, episode steps: 61, steps per second: 187, episode reward: 38.084, mean reward: 0.624 [0.520, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.812 [-0.522, 10.211], loss: 0.001596, mae: 0.043250, mean_q: 1.193799
 49711/100000: episode: 909, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 22.003, mean reward: 0.759 [0.627, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.533], loss: 0.001697, mae: 0.042685, mean_q: 1.189156
 49733/100000: episode: 910, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 16.280, mean reward: 0.740 [0.647, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.688, 10.376], loss: 0.001559, mae: 0.043465, mean_q: 1.189002
 49777/100000: episode: 911, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 26.980, mean reward: 0.613 [0.517, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.228, 10.311], loss: 0.001525, mae: 0.041448, mean_q: 1.193106
 49827/100000: episode: 912, duration: 0.251s, episode steps: 50, steps per second: 199, episode reward: 31.442, mean reward: 0.629 [0.516, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.571, 10.119], loss: 0.001622, mae: 0.042439, mean_q: 1.196591
 49837/100000: episode: 913, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 7.220, mean reward: 0.722 [0.627, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-1.057, 10.470], loss: 0.001618, mae: 0.044963, mean_q: 1.206052
 49859/100000: episode: 914, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 14.865, mean reward: 0.676 [0.551, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.756, 10.226], loss: 0.001534, mae: 0.042227, mean_q: 1.196742
 49874/100000: episode: 915, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 10.522, mean reward: 0.701 [0.646, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.321, 10.100], loss: 0.001530, mae: 0.042088, mean_q: 1.195255
 49884/100000: episode: 916, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 7.040, mean reward: 0.704 [0.621, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.432, 10.431], loss: 0.001898, mae: 0.046717, mean_q: 1.195682
 49899/100000: episode: 917, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 10.556, mean reward: 0.704 [0.668, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.395, 10.100], loss: 0.001644, mae: 0.043773, mean_q: 1.204791
 49905/100000: episode: 918, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 4.502, mean reward: 0.750 [0.675, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.460], loss: 0.001863, mae: 0.045429, mean_q: 1.201698
 49915/100000: episode: 919, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 7.413, mean reward: 0.741 [0.651, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.608], loss: 0.001538, mae: 0.043622, mean_q: 1.196190
 49970/100000: episode: 920, duration: 0.282s, episode steps: 55, steps per second: 195, episode reward: 37.786, mean reward: 0.687 [0.540, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-1.285, 10.200], loss: 0.001591, mae: 0.042806, mean_q: 1.204119
 49976/100000: episode: 921, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 4.000, mean reward: 0.667 [0.648, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.412], loss: 0.001523, mae: 0.040908, mean_q: 1.179680
 50026/100000: episode: 922, duration: 0.265s, episode steps: 50, steps per second: 188, episode reward: 30.296, mean reward: 0.606 [0.519, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.905, 10.100], loss: 0.001787, mae: 0.045013, mean_q: 1.199844
 50041/100000: episode: 923, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 10.623, mean reward: 0.708 [0.652, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.587, 10.100], loss: 0.001713, mae: 0.043523, mean_q: 1.196664
 50091/100000: episode: 924, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 31.137, mean reward: 0.623 [0.537, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.876, 10.218], loss: 0.001712, mae: 0.044694, mean_q: 1.203981
 50134/100000: episode: 925, duration: 0.235s, episode steps: 43, steps per second: 183, episode reward: 28.410, mean reward: 0.661 [0.584, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.882, 10.246], loss: 0.002042, mae: 0.047676, mean_q: 1.204966
 50177/100000: episode: 926, duration: 0.239s, episode steps: 43, steps per second: 180, episode reward: 29.895, mean reward: 0.695 [0.613, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.551, 10.397], loss: 0.001923, mae: 0.045673, mean_q: 1.201819
 50206/100000: episode: 927, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 21.268, mean reward: 0.733 [0.659, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.517], loss: 0.002111, mae: 0.048717, mean_q: 1.209371
 50261/100000: episode: 928, duration: 0.298s, episode steps: 55, steps per second: 185, episode reward: 33.339, mean reward: 0.606 [0.530, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.826, 10.100], loss: 0.001789, mae: 0.045234, mean_q: 1.208848
 50304/100000: episode: 929, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 27.107, mean reward: 0.630 [0.571, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.618, 10.305], loss: 0.001485, mae: 0.040923, mean_q: 1.205062
 50310/100000: episode: 930, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 3.960, mean reward: 0.660 [0.638, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.088, 10.329], loss: 0.001293, mae: 0.039715, mean_q: 1.208929
 50332/100000: episode: 931, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 16.608, mean reward: 0.755 [0.682, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.451], loss: 0.001280, mae: 0.038054, mean_q: 1.207364
 50347/100000: episode: 932, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 9.701, mean reward: 0.647 [0.580, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.739, 10.100], loss: 0.001744, mae: 0.044920, mean_q: 1.211210
 50397/100000: episode: 933, duration: 0.274s, episode steps: 50, steps per second: 182, episode reward: 31.978, mean reward: 0.640 [0.511, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.601, 10.384], loss: 0.001842, mae: 0.044199, mean_q: 1.209088
 50426/100000: episode: 934, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 19.494, mean reward: 0.672 [0.583, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.076, 10.398], loss: 0.001857, mae: 0.046919, mean_q: 1.217624
 50469/100000: episode: 935, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 28.522, mean reward: 0.663 [0.610, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.213, 10.455], loss: 0.001621, mae: 0.043306, mean_q: 1.215202
 50530/100000: episode: 936, duration: 0.315s, episode steps: 61, steps per second: 194, episode reward: 36.297, mean reward: 0.595 [0.509, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.805 [-0.670, 10.100], loss: 0.001622, mae: 0.042749, mean_q: 1.214131
 50545/100000: episode: 937, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 10.675, mean reward: 0.712 [0.652, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.496, 10.100], loss: 0.001408, mae: 0.041946, mean_q: 1.210307
 50600/100000: episode: 938, duration: 0.277s, episode steps: 55, steps per second: 199, episode reward: 34.774, mean reward: 0.632 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.838 [-0.349, 10.100], loss: 0.001612, mae: 0.042981, mean_q: 1.218934
 50629/100000: episode: 939, duration: 0.166s, episode steps: 29, steps per second: 174, episode reward: 21.660, mean reward: 0.747 [0.659, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.513], loss: 0.001681, mae: 0.044722, mean_q: 1.207139
 50673/100000: episode: 940, duration: 0.240s, episode steps: 44, steps per second: 183, episode reward: 29.207, mean reward: 0.664 [0.602, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-1.048, 10.404], loss: 0.001629, mae: 0.042984, mean_q: 1.209882
 50728/100000: episode: 941, duration: 0.294s, episode steps: 55, steps per second: 187, episode reward: 33.823, mean reward: 0.615 [0.512, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.483, 10.163], loss: 0.002044, mae: 0.047310, mean_q: 1.215697
 50783/100000: episode: 942, duration: 0.280s, episode steps: 55, steps per second: 197, episode reward: 35.492, mean reward: 0.645 [0.533, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.396, 10.194], loss: 0.001530, mae: 0.042387, mean_q: 1.219170
 50789/100000: episode: 943, duration: 0.041s, episode steps: 6, steps per second: 146, episode reward: 4.109, mean reward: 0.685 [0.662, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.419], loss: 0.001338, mae: 0.038994, mean_q: 1.208141
 50795/100000: episode: 944, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 4.257, mean reward: 0.709 [0.680, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.387], loss: 0.002071, mae: 0.044691, mean_q: 1.213426
 50856/100000: episode: 945, duration: 0.311s, episode steps: 61, steps per second: 196, episode reward: 37.135, mean reward: 0.609 [0.531, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.805 [-0.208, 10.100], loss: 0.001561, mae: 0.042738, mean_q: 1.217027
 50899/100000: episode: 946, duration: 0.236s, episode steps: 43, steps per second: 182, episode reward: 29.427, mean reward: 0.684 [0.556, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.213, 10.301], loss: 0.001692, mae: 0.044915, mean_q: 1.220362
 50954/100000: episode: 947, duration: 0.283s, episode steps: 55, steps per second: 195, episode reward: 42.792, mean reward: 0.778 [0.645, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-1.158, 10.526], loss: 0.001614, mae: 0.042754, mean_q: 1.218498
 51015/100000: episode: 948, duration: 0.334s, episode steps: 61, steps per second: 183, episode reward: 37.990, mean reward: 0.623 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-0.357, 10.239], loss: 0.001665, mae: 0.043906, mean_q: 1.222039
 51025/100000: episode: 949, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 6.637, mean reward: 0.664 [0.635, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.375], loss: 0.001780, mae: 0.043896, mean_q: 1.216250
 51086/100000: episode: 950, duration: 0.345s, episode steps: 61, steps per second: 177, episode reward: 39.833, mean reward: 0.653 [0.554, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.799 [-0.366, 10.310], loss: 0.001475, mae: 0.041427, mean_q: 1.233862
 51129/100000: episode: 951, duration: 0.233s, episode steps: 43, steps per second: 185, episode reward: 27.052, mean reward: 0.629 [0.512, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.224, 10.201], loss: 0.001715, mae: 0.043829, mean_q: 1.214679
 51173/100000: episode: 952, duration: 0.238s, episode steps: 44, steps per second: 185, episode reward: 30.319, mean reward: 0.689 [0.631, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.396, 10.389], loss: 0.001713, mae: 0.044195, mean_q: 1.224820
 51234/100000: episode: 953, duration: 0.335s, episode steps: 61, steps per second: 182, episode reward: 36.670, mean reward: 0.601 [0.504, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.803 [-0.307, 10.100], loss: 0.002753, mae: 0.052469, mean_q: 1.223224
 51284/100000: episode: 954, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 34.185, mean reward: 0.684 [0.585, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.366, 10.301], loss: 0.001589, mae: 0.042966, mean_q: 1.223244
 51327/100000: episode: 955, duration: 0.234s, episode steps: 43, steps per second: 184, episode reward: 29.661, mean reward: 0.690 [0.557, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.320, 10.224], loss: 0.001578, mae: 0.042796, mean_q: 1.225484
 51388/100000: episode: 956, duration: 0.327s, episode steps: 61, steps per second: 187, episode reward: 37.636, mean reward: 0.617 [0.500, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.813 [-0.967, 10.326], loss: 0.001767, mae: 0.043642, mean_q: 1.224694
 51431/100000: episode: 957, duration: 0.239s, episode steps: 43, steps per second: 180, episode reward: 30.019, mean reward: 0.698 [0.611, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.215, 10.400], loss: 0.001724, mae: 0.044559, mean_q: 1.222934
 51474/100000: episode: 958, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 28.360, mean reward: 0.660 [0.602, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.924, 10.368], loss: 0.001640, mae: 0.042012, mean_q: 1.233166
 51484/100000: episode: 959, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 7.010, mean reward: 0.701 [0.623, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.192, 10.540], loss: 0.001459, mae: 0.040867, mean_q: 1.232436
 51494/100000: episode: 960, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 7.277, mean reward: 0.728 [0.658, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.512], loss: 0.001686, mae: 0.043174, mean_q: 1.221907
 51549/100000: episode: 961, duration: 0.301s, episode steps: 55, steps per second: 183, episode reward: 34.469, mean reward: 0.627 [0.531, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.402, 10.181], loss: 0.001567, mae: 0.042552, mean_q: 1.225511
 51610/100000: episode: 962, duration: 0.317s, episode steps: 61, steps per second: 192, episode reward: 38.213, mean reward: 0.626 [0.501, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.802 [-0.877, 10.100], loss: 0.001647, mae: 0.043204, mean_q: 1.225384
 51639/100000: episode: 963, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 18.900, mean reward: 0.652 [0.561, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.378, 10.270], loss: 0.001771, mae: 0.045543, mean_q: 1.231922
 51654/100000: episode: 964, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 10.315, mean reward: 0.688 [0.637, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.328, 10.100], loss: 0.001525, mae: 0.041813, mean_q: 1.244925
 51660/100000: episode: 965, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 4.393, mean reward: 0.732 [0.669, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.369], loss: 0.001500, mae: 0.042534, mean_q: 1.240906
 51715/100000: episode: 966, duration: 0.299s, episode steps: 55, steps per second: 184, episode reward: 37.627, mean reward: 0.684 [0.598, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-1.728, 10.364], loss: 0.001503, mae: 0.041438, mean_q: 1.241210
 51758/100000: episode: 967, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 30.901, mean reward: 0.719 [0.644, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.264, 10.461], loss: 0.001336, mae: 0.039843, mean_q: 1.239457
 51773/100000: episode: 968, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 10.562, mean reward: 0.704 [0.626, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.507, 10.100], loss: 0.001832, mae: 0.043524, mean_q: 1.237990
 51823/100000: episode: 969, duration: 0.251s, episode steps: 50, steps per second: 199, episode reward: 31.800, mean reward: 0.636 [0.547, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-1.641, 10.296], loss: 0.001711, mae: 0.044767, mean_q: 1.236940
 51866/100000: episode: 970, duration: 0.237s, episode steps: 43, steps per second: 182, episode reward: 28.419, mean reward: 0.661 [0.594, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.047, 10.363], loss: 0.001554, mae: 0.042833, mean_q: 1.235078
 51881/100000: episode: 971, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 9.764, mean reward: 0.651 [0.586, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.530, 10.100], loss: 0.001762, mae: 0.044750, mean_q: 1.238149
 51910/100000: episode: 972, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 20.733, mean reward: 0.715 [0.604, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.858, 10.388], loss: 0.001763, mae: 0.043463, mean_q: 1.234803
 51954/100000: episode: 973, duration: 0.225s, episode steps: 44, steps per second: 195, episode reward: 29.846, mean reward: 0.678 [0.594, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.351, 10.318], loss: 0.001686, mae: 0.043342, mean_q: 1.237344
 51960/100000: episode: 974, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 4.337, mean reward: 0.723 [0.697, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.305], loss: 0.001308, mae: 0.041410, mean_q: 1.245034
 52021/100000: episode: 975, duration: 0.328s, episode steps: 61, steps per second: 186, episode reward: 38.943, mean reward: 0.638 [0.544, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.796 [-0.603, 10.100], loss: 0.001497, mae: 0.040935, mean_q: 1.244888
 52031/100000: episode: 976, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 6.786, mean reward: 0.679 [0.642, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.467], loss: 0.001530, mae: 0.040856, mean_q: 1.243307
 52075/100000: episode: 977, duration: 0.236s, episode steps: 44, steps per second: 186, episode reward: 27.056, mean reward: 0.615 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.468, 10.100], loss: 0.001557, mae: 0.042361, mean_q: 1.253689
[Info] 2-TH LEVEL FOUND: 1.5268561840057373, Considering 10/90 traces
 52081/100000: episode: 978, duration: 4.192s, episode steps: 6, steps per second: 1, episode reward: 3.956, mean reward: 0.659 [0.621, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.399], loss: 0.001365, mae: 0.041771, mean_q: 1.249004
 52112/100000: episode: 979, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 21.582, mean reward: 0.696 [0.641, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.975, 10.408], loss: 0.001326, mae: 0.039597, mean_q: 1.243230
 52149/100000: episode: 980, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 25.596, mean reward: 0.692 [0.594, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.106, 10.313], loss: 0.001591, mae: 0.042412, mean_q: 1.245721
 52186/100000: episode: 981, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 25.614, mean reward: 0.692 [0.607, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.112, 10.340], loss: 0.001514, mae: 0.041558, mean_q: 1.250830
 52227/100000: episode: 982, duration: 0.212s, episode steps: 41, steps per second: 193, episode reward: 28.549, mean reward: 0.696 [0.571, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.819, 10.367], loss: 0.001759, mae: 0.044532, mean_q: 1.251521
 52268/100000: episode: 983, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 26.441, mean reward: 0.645 [0.572, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.248, 10.310], loss: 0.001572, mae: 0.042620, mean_q: 1.255324
 52306/100000: episode: 984, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 26.514, mean reward: 0.698 [0.624, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.348, 10.334], loss: 0.001557, mae: 0.042110, mean_q: 1.255413
 52341/100000: episode: 985, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 25.540, mean reward: 0.730 [0.624, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.385, 10.383], loss: 0.001751, mae: 0.042971, mean_q: 1.259528
 52376/100000: episode: 986, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 27.174, mean reward: 0.776 [0.650, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.832, 10.547], loss: 0.001674, mae: 0.043657, mean_q: 1.247389
 52409/100000: episode: 987, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 24.013, mean reward: 0.728 [0.668, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.492, 10.427], loss: 0.001461, mae: 0.041049, mean_q: 1.254088
 52447/100000: episode: 988, duration: 0.214s, episode steps: 38, steps per second: 177, episode reward: 26.887, mean reward: 0.708 [0.592, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.093, 10.200], loss: 0.001840, mae: 0.044323, mean_q: 1.258053
 52482/100000: episode: 989, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 24.939, mean reward: 0.713 [0.634, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.064, 10.401], loss: 0.001792, mae: 0.044828, mean_q: 1.269135
 52501/100000: episode: 990, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 15.496, mean reward: 0.816 [0.775, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.611], loss: 0.002030, mae: 0.048166, mean_q: 1.266248
 52538/100000: episode: 991, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 26.556, mean reward: 0.718 [0.583, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.328, 10.277], loss: 0.001665, mae: 0.044211, mean_q: 1.256542
 52575/100000: episode: 992, duration: 0.196s, episode steps: 37, steps per second: 188, episode reward: 26.393, mean reward: 0.713 [0.580, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.630, 10.391], loss: 0.001411, mae: 0.040318, mean_q: 1.264125
 52608/100000: episode: 993, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 22.418, mean reward: 0.679 [0.614, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.182, 10.345], loss: 0.001398, mae: 0.041477, mean_q: 1.265008
 52641/100000: episode: 994, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 25.239, mean reward: 0.765 [0.685, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.035, 10.462], loss: 0.001314, mae: 0.038639, mean_q: 1.271846
 52682/100000: episode: 995, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 26.312, mean reward: 0.642 [0.564, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.131, 10.391], loss: 0.001679, mae: 0.044585, mean_q: 1.276191
 52708/100000: episode: 996, duration: 0.139s, episode steps: 26, steps per second: 186, episode reward: 19.970, mean reward: 0.768 [0.719, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.887, 10.467], loss: 0.001319, mae: 0.038598, mean_q: 1.267402
 52745/100000: episode: 997, duration: 0.189s, episode steps: 37, steps per second: 195, episode reward: 26.536, mean reward: 0.717 [0.596, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.123, 10.408], loss: 0.001315, mae: 0.039991, mean_q: 1.274480
 52786/100000: episode: 998, duration: 0.237s, episode steps: 41, steps per second: 173, episode reward: 29.426, mean reward: 0.718 [0.622, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.856, 10.513], loss: 0.001457, mae: 0.039915, mean_q: 1.275879
 52801/100000: episode: 999, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 12.336, mean reward: 0.822 [0.798, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.589], loss: 0.001515, mae: 0.040539, mean_q: 1.272684
 52816/100000: episode: 1000, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 12.612, mean reward: 0.841 [0.757, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.520], loss: 0.001833, mae: 0.041741, mean_q: 1.264679
 52851/100000: episode: 1001, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 23.520, mean reward: 0.672 [0.562, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.397, 10.267], loss: 0.001419, mae: 0.040879, mean_q: 1.287816
 52866/100000: episode: 1002, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 12.382, mean reward: 0.825 [0.749, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.499], loss: 0.001246, mae: 0.036903, mean_q: 1.271002
 52907/100000: episode: 1003, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 28.828, mean reward: 0.703 [0.605, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.568, 10.347], loss: 0.001407, mae: 0.040648, mean_q: 1.283769
 52945/100000: episode: 1004, duration: 0.212s, episode steps: 38, steps per second: 179, episode reward: 24.307, mean reward: 0.640 [0.514, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.530, 10.176], loss: 0.001538, mae: 0.041784, mean_q: 1.292643
 52982/100000: episode: 1005, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 28.353, mean reward: 0.766 [0.679, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.551, 10.577], loss: 0.001600, mae: 0.041775, mean_q: 1.280126
 53017/100000: episode: 1006, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 28.542, mean reward: 0.815 [0.697, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.795, 10.531], loss: 0.001416, mae: 0.039630, mean_q: 1.286865
 53032/100000: episode: 1007, duration: 0.086s, episode steps: 15, steps per second: 173, episode reward: 12.646, mean reward: 0.843 [0.811, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.601], loss: 0.001540, mae: 0.043236, mean_q: 1.294484
 53073/100000: episode: 1008, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 27.828, mean reward: 0.679 [0.521, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.563, 10.100], loss: 0.001627, mae: 0.043899, mean_q: 1.281951
 53106/100000: episode: 1009, duration: 0.162s, episode steps: 33, steps per second: 203, episode reward: 26.030, mean reward: 0.789 [0.711, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.316, 10.536], loss: 0.001585, mae: 0.043052, mean_q: 1.289334
 53137/100000: episode: 1010, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 21.913, mean reward: 0.707 [0.520, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.175], loss: 0.001338, mae: 0.040504, mean_q: 1.293133
 53156/100000: episode: 1011, duration: 0.104s, episode steps: 19, steps per second: 184, episode reward: 14.480, mean reward: 0.762 [0.699, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.467], loss: 0.001831, mae: 0.046416, mean_q: 1.295320
 53175/100000: episode: 1012, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 17.215, mean reward: 0.906 [0.743, 0.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.406, 10.653], loss: 0.001479, mae: 0.041196, mean_q: 1.293054
 53208/100000: episode: 1013, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 24.493, mean reward: 0.742 [0.655, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.346, 10.566], loss: 0.001568, mae: 0.043064, mean_q: 1.300131
 53234/100000: episode: 1014, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 20.876, mean reward: 0.803 [0.715, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.561], loss: 0.001583, mae: 0.042442, mean_q: 1.310576
 53271/100000: episode: 1015, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 25.840, mean reward: 0.698 [0.571, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.108, 10.248], loss: 0.001481, mae: 0.041866, mean_q: 1.296509
 53308/100000: episode: 1016, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 26.077, mean reward: 0.705 [0.604, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.721, 10.503], loss: 0.001346, mae: 0.040052, mean_q: 1.301738
 53339/100000: episode: 1017, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 23.713, mean reward: 0.765 [0.680, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.531], loss: 0.001266, mae: 0.038959, mean_q: 1.301922
 53365/100000: episode: 1018, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 19.408, mean reward: 0.746 [0.692, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.303, 10.514], loss: 0.001173, mae: 0.036981, mean_q: 1.307291
 53403/100000: episode: 1019, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 24.207, mean reward: 0.637 [0.533, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.235, 10.179], loss: 0.001406, mae: 0.041143, mean_q: 1.308007
 53438/100000: episode: 1020, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 26.215, mean reward: 0.749 [0.631, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.230, 10.516], loss: 0.001244, mae: 0.038514, mean_q: 1.311781
 53479/100000: episode: 1021, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 27.368, mean reward: 0.668 [0.578, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.280, 10.358], loss: 0.001513, mae: 0.042067, mean_q: 1.314082
 53494/100000: episode: 1022, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 11.907, mean reward: 0.794 [0.756, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.589], loss: 0.001447, mae: 0.041285, mean_q: 1.302721
 53525/100000: episode: 1023, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 24.284, mean reward: 0.783 [0.691, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.283, 10.508], loss: 0.001547, mae: 0.043053, mean_q: 1.305163
 53560/100000: episode: 1024, duration: 0.207s, episode steps: 35, steps per second: 169, episode reward: 23.503, mean reward: 0.672 [0.563, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.188, 10.288], loss: 0.001362, mae: 0.039908, mean_q: 1.314375
 53579/100000: episode: 1025, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 15.680, mean reward: 0.825 [0.700, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.976, 10.535], loss: 0.001373, mae: 0.040853, mean_q: 1.319442
 53598/100000: episode: 1026, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 14.257, mean reward: 0.750 [0.677, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.438], loss: 0.001313, mae: 0.039772, mean_q: 1.304714
 53613/100000: episode: 1027, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 12.765, mean reward: 0.851 [0.823, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.608], loss: 0.001497, mae: 0.043869, mean_q: 1.315504
 53644/100000: episode: 1028, duration: 0.188s, episode steps: 31, steps per second: 165, episode reward: 23.541, mean reward: 0.759 [0.670, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.554, 10.690], loss: 0.001590, mae: 0.044153, mean_q: 1.311780
 53670/100000: episode: 1029, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 19.586, mean reward: 0.753 [0.613, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.744, 10.360], loss: 0.001493, mae: 0.042757, mean_q: 1.333779
 53701/100000: episode: 1030, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 23.841, mean reward: 0.769 [0.665, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.305, 10.440], loss: 0.001340, mae: 0.039113, mean_q: 1.323247
 53738/100000: episode: 1031, duration: 0.239s, episode steps: 37, steps per second: 155, episode reward: 26.101, mean reward: 0.705 [0.602, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.669, 10.354], loss: 0.001632, mae: 0.042642, mean_q: 1.325570
 53780/100000: episode: 1032, duration: 0.249s, episode steps: 42, steps per second: 169, episode reward: 29.542, mean reward: 0.703 [0.607, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.260, 10.279], loss: 0.001450, mae: 0.041393, mean_q: 1.325958
 53817/100000: episode: 1033, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 27.358, mean reward: 0.739 [0.648, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.260, 10.514], loss: 0.001600, mae: 0.044307, mean_q: 1.331109
 53859/100000: episode: 1034, duration: 0.242s, episode steps: 42, steps per second: 174, episode reward: 29.085, mean reward: 0.692 [0.585, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.649, 10.338], loss: 0.001439, mae: 0.041811, mean_q: 1.335013
 53874/100000: episode: 1035, duration: 0.100s, episode steps: 15, steps per second: 150, episode reward: 12.011, mean reward: 0.801 [0.718, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.622, 10.543], loss: 0.001429, mae: 0.041887, mean_q: 1.329877
 53905/100000: episode: 1036, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 24.832, mean reward: 0.801 [0.688, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.849, 10.560], loss: 0.001538, mae: 0.043286, mean_q: 1.334621
[Info] FALSIFICATION!
 53914/100000: episode: 1037, duration: 0.213s, episode steps: 9, steps per second: 42, episode reward: 7.778, mean reward: 0.864 [0.772, 1.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.018, 9.404], loss: 0.001619, mae: 0.043037, mean_q: 1.335782
 53933/100000: episode: 1038, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 16.012, mean reward: 0.843 [0.723, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.112, 10.607], loss: 0.001384, mae: 0.037748, mean_q: 1.331993
 53966/100000: episode: 1039, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 23.561, mean reward: 0.714 [0.613, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.261, 10.398], loss: 0.001393, mae: 0.039387, mean_q: 1.330448
 53999/100000: episode: 1040, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 24.351, mean reward: 0.738 [0.672, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.035, 10.420], loss: 0.001288, mae: 0.039951, mean_q: 1.331782
 54014/100000: episode: 1041, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 12.092, mean reward: 0.806 [0.774, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.121, 10.540], loss: 0.001533, mae: 0.042922, mean_q: 1.350089
 54055/100000: episode: 1042, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 30.036, mean reward: 0.733 [0.586, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.142, 10.355], loss: 0.001318, mae: 0.040498, mean_q: 1.328655
 54097/100000: episode: 1043, duration: 0.215s, episode steps: 42, steps per second: 196, episode reward: 31.006, mean reward: 0.738 [0.615, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.282, 10.380], loss: 0.001505, mae: 0.041995, mean_q: 1.350150
 54112/100000: episode: 1044, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 12.317, mean reward: 0.821 [0.769, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.579, 10.536], loss: 0.001495, mae: 0.041860, mean_q: 1.322273
 54153/100000: episode: 1045, duration: 0.226s, episode steps: 41, steps per second: 181, episode reward: 25.456, mean reward: 0.621 [0.516, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.153, 10.359], loss: 0.001220, mae: 0.038644, mean_q: 1.333925
 54168/100000: episode: 1046, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 11.832, mean reward: 0.789 [0.739, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.500], loss: 0.001182, mae: 0.037395, mean_q: 1.320825
 54194/100000: episode: 1047, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 18.831, mean reward: 0.724 [0.646, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.315, 10.426], loss: 0.001266, mae: 0.038780, mean_q: 1.334239
 54220/100000: episode: 1048, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 20.605, mean reward: 0.792 [0.701, 0.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.509], loss: 0.001368, mae: 0.040987, mean_q: 1.327785
 54251/100000: episode: 1049, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 24.234, mean reward: 0.782 [0.665, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.925, 10.510], loss: 0.001453, mae: 0.041554, mean_q: 1.339562
 54284/100000: episode: 1050, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 23.746, mean reward: 0.720 [0.620, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.213, 10.302], loss: 0.001299, mae: 0.039458, mean_q: 1.324905
 54321/100000: episode: 1051, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 27.800, mean reward: 0.751 [0.565, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.210, 10.233], loss: 0.001594, mae: 0.044193, mean_q: 1.344859
 54340/100000: episode: 1052, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 12.126, mean reward: 0.638 [0.585, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.866, 10.273], loss: 0.001322, mae: 0.040133, mean_q: 1.339378
 54378/100000: episode: 1053, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 26.912, mean reward: 0.708 [0.618, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.644, 10.535], loss: 0.001293, mae: 0.039194, mean_q: 1.346761
 54411/100000: episode: 1054, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 23.479, mean reward: 0.711 [0.605, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.613, 10.427], loss: 0.001226, mae: 0.037817, mean_q: 1.331741
 54448/100000: episode: 1055, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 31.019, mean reward: 0.838 [0.740, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.116, 10.493], loss: 0.001302, mae: 0.039639, mean_q: 1.343477
 54467/100000: episode: 1056, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 15.080, mean reward: 0.794 [0.723, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.446], loss: 0.001181, mae: 0.038441, mean_q: 1.350153
 54502/100000: episode: 1057, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 26.680, mean reward: 0.762 [0.658, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.476, 10.414], loss: 0.001417, mae: 0.041611, mean_q: 1.335878
 54544/100000: episode: 1058, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 28.733, mean reward: 0.684 [0.581, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.259, 10.301], loss: 0.001415, mae: 0.041399, mean_q: 1.351694
 54586/100000: episode: 1059, duration: 0.221s, episode steps: 42, steps per second: 190, episode reward: 29.398, mean reward: 0.700 [0.616, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.674, 10.475], loss: 0.001688, mae: 0.044452, mean_q: 1.346063
 54617/100000: episode: 1060, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 22.064, mean reward: 0.712 [0.530, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.454, 10.144], loss: 0.001309, mae: 0.039636, mean_q: 1.350034
 54648/100000: episode: 1061, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 21.692, mean reward: 0.700 [0.595, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.612, 10.514], loss: 0.001226, mae: 0.038888, mean_q: 1.344051
 54667/100000: episode: 1062, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 16.605, mean reward: 0.874 [0.787, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.623, 10.616], loss: 0.001348, mae: 0.039469, mean_q: 1.348222
 54702/100000: episode: 1063, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 27.052, mean reward: 0.773 [0.700, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.070, 10.409], loss: 0.001370, mae: 0.040708, mean_q: 1.348985
 54735/100000: episode: 1064, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 25.434, mean reward: 0.771 [0.710, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.479, 10.531], loss: 0.001574, mae: 0.042421, mean_q: 1.352849
 54766/100000: episode: 1065, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 23.020, mean reward: 0.743 [0.625, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.218, 10.321], loss: 0.001546, mae: 0.043287, mean_q: 1.347465
 54804/100000: episode: 1066, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 28.451, mean reward: 0.749 [0.624, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.087, 10.390], loss: 0.001308, mae: 0.040001, mean_q: 1.355056
 54837/100000: episode: 1067, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 25.570, mean reward: 0.775 [0.628, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.278, 10.413], loss: 0.001202, mae: 0.038154, mean_q: 1.348842
[Info] Complete ISplit Iteration
[Info] Levels: [1.3715396, 1.5268562, 1.6166234]
[Info] Cond. Prob: [0.1, 0.1, 0.48]
[Info] Error Prob: 0.0048000000000000004

 54879/100000: episode: 1068, duration: 4.582s, episode steps: 42, steps per second: 9, episode reward: 28.388, mean reward: 0.676 [0.560, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.268, 10.275], loss: 0.001122, mae: 0.037366, mean_q: 1.350501
 54979/100000: episode: 1069, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 59.479, mean reward: 0.595 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.445, 10.273], loss: 0.001350, mae: 0.039540, mean_q: 1.355348
 55079/100000: episode: 1070, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 60.561, mean reward: 0.606 [0.511, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.908, 10.129], loss: 0.001203, mae: 0.038499, mean_q: 1.356689
 55179/100000: episode: 1071, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 64.212, mean reward: 0.642 [0.519, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.498, 10.397], loss: 0.001350, mae: 0.040163, mean_q: 1.357451
 55279/100000: episode: 1072, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 61.376, mean reward: 0.614 [0.510, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.530, 10.272], loss: 0.001298, mae: 0.039500, mean_q: 1.353177
 55379/100000: episode: 1073, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.193, mean reward: 0.602 [0.510, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.340, 10.098], loss: 0.001274, mae: 0.038763, mean_q: 1.351513
 55479/100000: episode: 1074, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.527, mean reward: 0.585 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.345, 10.098], loss: 0.001404, mae: 0.040391, mean_q: 1.353549
 55579/100000: episode: 1075, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 57.582, mean reward: 0.576 [0.503, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.267, 10.129], loss: 0.001398, mae: 0.040317, mean_q: 1.346757
 55679/100000: episode: 1076, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 64.585, mean reward: 0.646 [0.505, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.170, 10.098], loss: 0.001338, mae: 0.039754, mean_q: 1.341140
 55779/100000: episode: 1077, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 56.752, mean reward: 0.568 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.932, 10.098], loss: 0.001569, mae: 0.042636, mean_q: 1.344281
 55879/100000: episode: 1078, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.759, mean reward: 0.578 [0.508, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.919, 10.109], loss: 0.001444, mae: 0.041142, mean_q: 1.337921
 55979/100000: episode: 1079, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 62.188, mean reward: 0.622 [0.520, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.960, 10.305], loss: 0.001400, mae: 0.040626, mean_q: 1.342117
 56079/100000: episode: 1080, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.867, mean reward: 0.609 [0.515, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.731, 10.372], loss: 0.001336, mae: 0.039836, mean_q: 1.333365
 56179/100000: episode: 1081, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 57.997, mean reward: 0.580 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.437, 10.190], loss: 0.001444, mae: 0.041969, mean_q: 1.336816
 56279/100000: episode: 1082, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.323, mean reward: 0.573 [0.501, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.619, 10.149], loss: 0.001478, mae: 0.041571, mean_q: 1.331097
 56379/100000: episode: 1083, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 60.484, mean reward: 0.605 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.469, 10.303], loss: 0.001441, mae: 0.040720, mean_q: 1.330691
 56479/100000: episode: 1084, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 61.050, mean reward: 0.611 [0.513, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.209, 10.430], loss: 0.001525, mae: 0.041837, mean_q: 1.326404
 56579/100000: episode: 1085, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 56.829, mean reward: 0.568 [0.499, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.937, 10.180], loss: 0.001535, mae: 0.041663, mean_q: 1.326340
 56679/100000: episode: 1086, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 61.633, mean reward: 0.616 [0.527, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.889, 10.294], loss: 0.001488, mae: 0.041681, mean_q: 1.321510
 56779/100000: episode: 1087, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.502, mean reward: 0.595 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.648, 10.285], loss: 0.001555, mae: 0.041350, mean_q: 1.323645
 56879/100000: episode: 1088, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.178, mean reward: 0.592 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.198, 10.227], loss: 0.001464, mae: 0.041678, mean_q: 1.321251
 56979/100000: episode: 1089, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.525, mean reward: 0.595 [0.501, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.768, 10.098], loss: 0.001575, mae: 0.041515, mean_q: 1.317710
 57079/100000: episode: 1090, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.066, mean reward: 0.581 [0.499, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.159, 10.098], loss: 0.001841, mae: 0.046467, mean_q: 1.316972
 57179/100000: episode: 1091, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.897, mean reward: 0.589 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.290, 10.124], loss: 0.001521, mae: 0.042245, mean_q: 1.310185
 57279/100000: episode: 1092, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.369, mean reward: 0.584 [0.499, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.072, 10.098], loss: 0.001402, mae: 0.041111, mean_q: 1.309212
 57379/100000: episode: 1093, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 60.804, mean reward: 0.608 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.400, 10.302], loss: 0.001545, mae: 0.043069, mean_q: 1.306467
 57479/100000: episode: 1094, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.354, mean reward: 0.604 [0.521, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.412, 10.098], loss: 0.001594, mae: 0.042623, mean_q: 1.297964
 57579/100000: episode: 1095, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.992, mean reward: 0.590 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.935, 10.098], loss: 0.001474, mae: 0.041426, mean_q: 1.295158
 57679/100000: episode: 1096, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.735, mean reward: 0.587 [0.503, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.252, 10.098], loss: 0.001423, mae: 0.041650, mean_q: 1.298476
 57779/100000: episode: 1097, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.106, mean reward: 0.591 [0.502, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.903, 10.338], loss: 0.001572, mae: 0.042237, mean_q: 1.290313
 57879/100000: episode: 1098, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.271, mean reward: 0.573 [0.501, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.254, 10.182], loss: 0.001745, mae: 0.043138, mean_q: 1.280445
 57979/100000: episode: 1099, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.164, mean reward: 0.572 [0.500, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.508, 10.166], loss: 0.001484, mae: 0.040955, mean_q: 1.277105
 58079/100000: episode: 1100, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.036, mean reward: 0.590 [0.502, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.546, 10.098], loss: 0.001418, mae: 0.041738, mean_q: 1.267017
 58179/100000: episode: 1101, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.686, mean reward: 0.607 [0.510, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.910, 10.283], loss: 0.001586, mae: 0.041513, mean_q: 1.262069
 58279/100000: episode: 1102, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.098, mean reward: 0.591 [0.502, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.386, 10.193], loss: 0.001543, mae: 0.042166, mean_q: 1.254865
 58379/100000: episode: 1103, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.989, mean reward: 0.580 [0.499, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.751, 10.230], loss: 0.001614, mae: 0.042307, mean_q: 1.252747
 58479/100000: episode: 1104, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 60.062, mean reward: 0.601 [0.499, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.386, 10.098], loss: 0.001462, mae: 0.040832, mean_q: 1.244728
 58579/100000: episode: 1105, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.250, mean reward: 0.593 [0.515, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.846, 10.147], loss: 0.001614, mae: 0.043049, mean_q: 1.240033
 58679/100000: episode: 1106, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.483, mean reward: 0.585 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.559, 10.098], loss: 0.001374, mae: 0.040415, mean_q: 1.234986
 58779/100000: episode: 1107, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.930, mean reward: 0.609 [0.513, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.165, 10.098], loss: 0.001328, mae: 0.040092, mean_q: 1.237288
 58879/100000: episode: 1108, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 59.057, mean reward: 0.591 [0.500, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.917, 10.209], loss: 0.001563, mae: 0.042718, mean_q: 1.227090
 58979/100000: episode: 1109, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 64.870, mean reward: 0.649 [0.502, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.700, 10.431], loss: 0.001431, mae: 0.041508, mean_q: 1.225158
 59079/100000: episode: 1110, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.288, mean reward: 0.593 [0.497, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.679, 10.181], loss: 0.001400, mae: 0.040671, mean_q: 1.218311
 59179/100000: episode: 1111, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.492, mean reward: 0.595 [0.504, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.579, 10.214], loss: 0.001410, mae: 0.040702, mean_q: 1.215935
 59279/100000: episode: 1112, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 56.932, mean reward: 0.569 [0.507, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.097, 10.098], loss: 0.001379, mae: 0.040504, mean_q: 1.202048
 59379/100000: episode: 1113, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 62.063, mean reward: 0.621 [0.530, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.991, 10.098], loss: 0.001295, mae: 0.039616, mean_q: 1.205152
 59479/100000: episode: 1114, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.227, mean reward: 0.572 [0.500, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.602, 10.121], loss: 0.001261, mae: 0.038979, mean_q: 1.195431
 59579/100000: episode: 1115, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.764, mean reward: 0.578 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.558, 10.128], loss: 0.001373, mae: 0.040706, mean_q: 1.198252
 59679/100000: episode: 1116, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.763, mean reward: 0.578 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.399, 10.098], loss: 0.001421, mae: 0.041078, mean_q: 1.192177
 59779/100000: episode: 1117, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.487, mean reward: 0.575 [0.500, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.229, 10.098], loss: 0.001271, mae: 0.039396, mean_q: 1.180787
 59879/100000: episode: 1118, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.266, mean reward: 0.573 [0.499, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.863, 10.132], loss: 0.001342, mae: 0.040053, mean_q: 1.176539
 59979/100000: episode: 1119, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 59.175, mean reward: 0.592 [0.512, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.087, 10.306], loss: 0.001185, mae: 0.038438, mean_q: 1.176091
 60079/100000: episode: 1120, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.304, mean reward: 0.593 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.259, 10.099], loss: 0.001173, mae: 0.038173, mean_q: 1.172379
 60179/100000: episode: 1121, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.226, mean reward: 0.582 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.650, 10.240], loss: 0.001264, mae: 0.039083, mean_q: 1.173808
 60279/100000: episode: 1122, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 57.804, mean reward: 0.578 [0.501, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.982, 10.258], loss: 0.001232, mae: 0.038399, mean_q: 1.172409
 60379/100000: episode: 1123, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.450, mean reward: 0.584 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.556, 10.098], loss: 0.001293, mae: 0.039561, mean_q: 1.174181
 60479/100000: episode: 1124, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.669, mean reward: 0.607 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.163, 10.298], loss: 0.001329, mae: 0.040539, mean_q: 1.170659
 60579/100000: episode: 1125, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 63.210, mean reward: 0.632 [0.518, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.984, 10.098], loss: 0.001258, mae: 0.038890, mean_q: 1.172330
 60679/100000: episode: 1126, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 59.792, mean reward: 0.598 [0.507, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.397, 10.235], loss: 0.001266, mae: 0.039460, mean_q: 1.174422
 60779/100000: episode: 1127, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.314, mean reward: 0.583 [0.502, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.937, 10.098], loss: 0.001199, mae: 0.038056, mean_q: 1.170426
 60879/100000: episode: 1128, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 61.426, mean reward: 0.614 [0.516, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.916, 10.098], loss: 0.001155, mae: 0.037698, mean_q: 1.172046
 60979/100000: episode: 1129, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.894, mean reward: 0.579 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.629, 10.098], loss: 0.001252, mae: 0.038863, mean_q: 1.175269
 61079/100000: episode: 1130, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.285, mean reward: 0.573 [0.501, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.110, 10.200], loss: 0.001239, mae: 0.038650, mean_q: 1.176000
 61179/100000: episode: 1131, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.099, mean reward: 0.591 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.765, 10.098], loss: 0.001306, mae: 0.039972, mean_q: 1.172586
 61279/100000: episode: 1132, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.497, mean reward: 0.585 [0.509, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.259, 10.188], loss: 0.001268, mae: 0.039146, mean_q: 1.171707
 61379/100000: episode: 1133, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.480, mean reward: 0.575 [0.499, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.380, 10.098], loss: 0.001235, mae: 0.038813, mean_q: 1.173272
 61479/100000: episode: 1134, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.583, mean reward: 0.606 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.082, 10.394], loss: 0.001304, mae: 0.039365, mean_q: 1.169352
 61579/100000: episode: 1135, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.187, mean reward: 0.582 [0.510, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.755, 10.362], loss: 0.001200, mae: 0.038271, mean_q: 1.168535
 61679/100000: episode: 1136, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.502, mean reward: 0.575 [0.501, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.138], loss: 0.001130, mae: 0.037131, mean_q: 1.167221
 61779/100000: episode: 1137, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.961, mean reward: 0.580 [0.507, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.864, 10.213], loss: 0.001227, mae: 0.038509, mean_q: 1.165643
 61879/100000: episode: 1138, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.640, mean reward: 0.586 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.042, 10.098], loss: 0.001295, mae: 0.039413, mean_q: 1.167555
 61979/100000: episode: 1139, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.326, mean reward: 0.573 [0.501, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.777, 10.098], loss: 0.001216, mae: 0.037932, mean_q: 1.167542
 62079/100000: episode: 1140, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.148, mean reward: 0.581 [0.508, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.381, 10.117], loss: 0.001265, mae: 0.038928, mean_q: 1.167189
 62179/100000: episode: 1141, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 60.838, mean reward: 0.608 [0.508, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.839, 10.098], loss: 0.001168, mae: 0.037878, mean_q: 1.163644
 62279/100000: episode: 1142, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 61.259, mean reward: 0.613 [0.506, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.265, 10.263], loss: 0.001214, mae: 0.037889, mean_q: 1.165205
 62379/100000: episode: 1143, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.758, mean reward: 0.568 [0.508, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.245, 10.218], loss: 0.001235, mae: 0.038905, mean_q: 1.166474
 62479/100000: episode: 1144, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.859, mean reward: 0.589 [0.510, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.519, 10.166], loss: 0.001201, mae: 0.037633, mean_q: 1.164152
 62579/100000: episode: 1145, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.107, mean reward: 0.581 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.486, 10.169], loss: 0.001280, mae: 0.039727, mean_q: 1.163713
 62679/100000: episode: 1146, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.278, mean reward: 0.583 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.926, 10.197], loss: 0.001259, mae: 0.038534, mean_q: 1.165465
 62779/100000: episode: 1147, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 58.240, mean reward: 0.582 [0.508, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.586, 10.129], loss: 0.001308, mae: 0.039608, mean_q: 1.165403
 62879/100000: episode: 1148, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.331, mean reward: 0.583 [0.508, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.187, 10.256], loss: 0.001226, mae: 0.038578, mean_q: 1.166590
 62979/100000: episode: 1149, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 62.111, mean reward: 0.621 [0.510, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.498, 10.184], loss: 0.001272, mae: 0.039308, mean_q: 1.165005
 63079/100000: episode: 1150, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.738, mean reward: 0.597 [0.501, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.154, 10.098], loss: 0.001352, mae: 0.040070, mean_q: 1.168805
 63179/100000: episode: 1151, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 60.215, mean reward: 0.602 [0.502, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.009, 10.098], loss: 0.001154, mae: 0.037412, mean_q: 1.164327
 63279/100000: episode: 1152, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 59.240, mean reward: 0.592 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.832, 10.383], loss: 0.001271, mae: 0.039149, mean_q: 1.168168
 63379/100000: episode: 1153, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.775, mean reward: 0.578 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.172, 10.159], loss: 0.001269, mae: 0.039025, mean_q: 1.169660
 63479/100000: episode: 1154, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.182, mean reward: 0.602 [0.505, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.733, 10.126], loss: 0.001330, mae: 0.040163, mean_q: 1.169519
 63579/100000: episode: 1155, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.874, mean reward: 0.599 [0.508, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.716, 10.131], loss: 0.001347, mae: 0.039813, mean_q: 1.166947
 63679/100000: episode: 1156, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.713, mean reward: 0.597 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.032, 10.300], loss: 0.001263, mae: 0.039175, mean_q: 1.166430
 63779/100000: episode: 1157, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.935, mean reward: 0.589 [0.499, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.476, 10.098], loss: 0.001261, mae: 0.039446, mean_q: 1.165055
 63879/100000: episode: 1158, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.461, mean reward: 0.595 [0.513, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.881, 10.098], loss: 0.001251, mae: 0.039392, mean_q: 1.168061
 63979/100000: episode: 1159, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.846, mean reward: 0.588 [0.510, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.623, 10.098], loss: 0.001285, mae: 0.039767, mean_q: 1.167999
 64079/100000: episode: 1160, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.019, mean reward: 0.580 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.372, 10.098], loss: 0.001184, mae: 0.037492, mean_q: 1.161530
 64179/100000: episode: 1161, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.077, mean reward: 0.591 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.399, 10.188], loss: 0.001287, mae: 0.039135, mean_q: 1.166993
 64279/100000: episode: 1162, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.746, mean reward: 0.577 [0.506, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.840, 10.098], loss: 0.001165, mae: 0.037619, mean_q: 1.166274
 64379/100000: episode: 1163, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.156, mean reward: 0.592 [0.505, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.015, 10.098], loss: 0.001319, mae: 0.039769, mean_q: 1.164036
 64479/100000: episode: 1164, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 61.435, mean reward: 0.614 [0.520, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.024, 10.098], loss: 0.001392, mae: 0.041048, mean_q: 1.163199
 64579/100000: episode: 1165, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.558, mean reward: 0.576 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.587, 10.098], loss: 0.001336, mae: 0.040002, mean_q: 1.165885
 64679/100000: episode: 1166, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 59.669, mean reward: 0.597 [0.514, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.744, 10.098], loss: 0.001230, mae: 0.038575, mean_q: 1.163347
 64779/100000: episode: 1167, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.962, mean reward: 0.570 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.270, 10.187], loss: 0.001415, mae: 0.041070, mean_q: 1.164702
[Info] 1-TH LEVEL FOUND: 1.353394627571106, Considering 10/90 traces
 64879/100000: episode: 1168, duration: 4.636s, episode steps: 100, steps per second: 22, episode reward: 60.304, mean reward: 0.603 [0.501, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.773, 10.404], loss: 0.001337, mae: 0.040351, mean_q: 1.169441
 64923/100000: episode: 1169, duration: 0.263s, episode steps: 44, steps per second: 167, episode reward: 26.867, mean reward: 0.611 [0.515, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.704, 10.199], loss: 0.001381, mae: 0.041181, mean_q: 1.161000
 64943/100000: episode: 1170, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 13.675, mean reward: 0.684 [0.556, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.418, 10.266], loss: 0.001352, mae: 0.040664, mean_q: 1.164660
 64987/100000: episode: 1171, duration: 0.224s, episode steps: 44, steps per second: 196, episode reward: 28.079, mean reward: 0.638 [0.582, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.333, 10.357], loss: 0.001255, mae: 0.038682, mean_q: 1.171516
 65003/100000: episode: 1172, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.873, mean reward: 0.742 [0.645, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.484, 10.100], loss: 0.001373, mae: 0.040267, mean_q: 1.170919
 65023/100000: episode: 1173, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 14.234, mean reward: 0.712 [0.676, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.510], loss: 0.001241, mae: 0.037915, mean_q: 1.174397
 65043/100000: episode: 1174, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 12.768, mean reward: 0.638 [0.529, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.273], loss: 0.001607, mae: 0.042897, mean_q: 1.170478
 65062/100000: episode: 1175, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 14.315, mean reward: 0.753 [0.691, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.078, 10.501], loss: 0.001436, mae: 0.041986, mean_q: 1.167477
 65086/100000: episode: 1176, duration: 0.117s, episode steps: 24, steps per second: 206, episode reward: 16.918, mean reward: 0.705 [0.640, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.482], loss: 0.001436, mae: 0.042797, mean_q: 1.178061
 65128/100000: episode: 1177, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 27.649, mean reward: 0.658 [0.598, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.111, 10.332], loss: 0.001288, mae: 0.039502, mean_q: 1.173295
 65147/100000: episode: 1178, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 15.203, mean reward: 0.800 [0.680, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.474, 10.592], loss: 0.001531, mae: 0.042163, mean_q: 1.174409
 65194/100000: episode: 1179, duration: 0.239s, episode steps: 47, steps per second: 197, episode reward: 29.792, mean reward: 0.634 [0.541, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.461, 10.100], loss: 0.001447, mae: 0.041620, mean_q: 1.175261
 65214/100000: episode: 1180, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 13.436, mean reward: 0.672 [0.611, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.121, 10.402], loss: 0.001611, mae: 0.042851, mean_q: 1.179244
 65230/100000: episode: 1181, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 12.743, mean reward: 0.796 [0.667, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.472, 10.100], loss: 0.001587, mae: 0.041663, mean_q: 1.168529
 65272/100000: episode: 1182, duration: 0.216s, episode steps: 42, steps per second: 195, episode reward: 26.496, mean reward: 0.631 [0.531, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-1.208, 10.214], loss: 0.001318, mae: 0.039951, mean_q: 1.170963
 65291/100000: episode: 1183, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 13.003, mean reward: 0.684 [0.624, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.555], loss: 0.001463, mae: 0.041220, mean_q: 1.182055
 65333/100000: episode: 1184, duration: 0.209s, episode steps: 42, steps per second: 201, episode reward: 26.639, mean reward: 0.634 [0.512, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.275, 10.100], loss: 0.001489, mae: 0.040737, mean_q: 1.179474
 65352/100000: episode: 1185, duration: 0.093s, episode steps: 19, steps per second: 203, episode reward: 12.373, mean reward: 0.651 [0.559, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.129, 10.250], loss: 0.001943, mae: 0.046732, mean_q: 1.180452
 65395/100000: episode: 1186, duration: 0.232s, episode steps: 43, steps per second: 186, episode reward: 27.670, mean reward: 0.643 [0.585, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.364, 10.347], loss: 0.001707, mae: 0.042983, mean_q: 1.179301
 65417/100000: episode: 1187, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 14.522, mean reward: 0.660 [0.557, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.243, 10.100], loss: 0.001906, mae: 0.043833, mean_q: 1.175701
 65459/100000: episode: 1188, duration: 0.217s, episode steps: 42, steps per second: 193, episode reward: 26.245, mean reward: 0.625 [0.513, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-1.506, 10.396], loss: 0.001564, mae: 0.041502, mean_q: 1.182152
 65481/100000: episode: 1189, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 17.421, mean reward: 0.792 [0.694, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.296, 10.100], loss: 0.001843, mae: 0.043427, mean_q: 1.179002
 65525/100000: episode: 1190, duration: 0.229s, episode steps: 44, steps per second: 193, episode reward: 28.953, mean reward: 0.658 [0.555, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.186, 10.280], loss: 0.001569, mae: 0.042785, mean_q: 1.182469
 65545/100000: episode: 1191, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 14.000, mean reward: 0.700 [0.624, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.473, 10.444], loss: 0.001767, mae: 0.043731, mean_q: 1.182315
 65567/100000: episode: 1192, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 16.617, mean reward: 0.755 [0.713, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.431, 10.100], loss: 0.001868, mae: 0.044439, mean_q: 1.183368
 65609/100000: episode: 1193, duration: 0.220s, episode steps: 42, steps per second: 191, episode reward: 27.240, mean reward: 0.649 [0.558, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.189, 10.390], loss: 0.001764, mae: 0.044404, mean_q: 1.183610
 65633/100000: episode: 1194, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 15.524, mean reward: 0.647 [0.586, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.284, 10.339], loss: 0.001580, mae: 0.043335, mean_q: 1.180693
 65677/100000: episode: 1195, duration: 0.240s, episode steps: 44, steps per second: 183, episode reward: 26.540, mean reward: 0.603 [0.524, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.674, 10.100], loss: 0.001863, mae: 0.045187, mean_q: 1.184651
 65721/100000: episode: 1196, duration: 0.225s, episode steps: 44, steps per second: 196, episode reward: 28.157, mean reward: 0.640 [0.540, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.451, 10.183], loss: 0.001499, mae: 0.042164, mean_q: 1.181852
 65743/100000: episode: 1197, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 14.286, mean reward: 0.649 [0.577, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.727, 10.100], loss: 0.001869, mae: 0.045415, mean_q: 1.179079
 65786/100000: episode: 1198, duration: 0.221s, episode steps: 43, steps per second: 194, episode reward: 25.534, mean reward: 0.594 [0.510, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.659, 10.100], loss: 0.001645, mae: 0.042181, mean_q: 1.177104
 65810/100000: episode: 1199, duration: 0.120s, episode steps: 24, steps per second: 201, episode reward: 19.241, mean reward: 0.802 [0.697, 0.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.750], loss: 0.001561, mae: 0.041006, mean_q: 1.181970
 65834/100000: episode: 1200, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 15.191, mean reward: 0.633 [0.561, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.586, 10.300], loss: 0.001662, mae: 0.044080, mean_q: 1.188564
 65856/100000: episode: 1201, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 17.996, mean reward: 0.818 [0.762, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.439, 10.100], loss: 0.001731, mae: 0.045284, mean_q: 1.189056
 65898/100000: episode: 1202, duration: 0.214s, episode steps: 42, steps per second: 196, episode reward: 25.263, mean reward: 0.601 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.492, 10.100], loss: 0.001603, mae: 0.041620, mean_q: 1.190883
 65922/100000: episode: 1203, duration: 0.148s, episode steps: 24, steps per second: 163, episode reward: 17.735, mean reward: 0.739 [0.640, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.040, 10.384], loss: 0.001709, mae: 0.043905, mean_q: 1.185528
 65965/100000: episode: 1204, duration: 0.217s, episode steps: 43, steps per second: 198, episode reward: 27.517, mean reward: 0.640 [0.540, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.789, 10.257], loss: 0.001671, mae: 0.044797, mean_q: 1.194885
 66008/100000: episode: 1205, duration: 0.208s, episode steps: 43, steps per second: 207, episode reward: 26.357, mean reward: 0.613 [0.519, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.236, 10.100], loss: 0.002095, mae: 0.047805, mean_q: 1.195095
 66052/100000: episode: 1206, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 29.454, mean reward: 0.669 [0.566, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.181, 10.315], loss: 0.001701, mae: 0.043701, mean_q: 1.190568
 66076/100000: episode: 1207, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 18.099, mean reward: 0.754 [0.700, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.507], loss: 0.001597, mae: 0.042098, mean_q: 1.190992
 66119/100000: episode: 1208, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 26.159, mean reward: 0.608 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.211, 10.100], loss: 0.001669, mae: 0.042802, mean_q: 1.191636
 66162/100000: episode: 1209, duration: 0.229s, episode steps: 43, steps per second: 188, episode reward: 27.507, mean reward: 0.640 [0.527, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.979, 10.214], loss: 0.001728, mae: 0.044723, mean_q: 1.198144
 66182/100000: episode: 1210, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 12.696, mean reward: 0.635 [0.551, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.459, 10.256], loss: 0.001577, mae: 0.041658, mean_q: 1.179113
 66204/100000: episode: 1211, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 15.306, mean reward: 0.696 [0.631, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.635, 10.100], loss: 0.001832, mae: 0.043588, mean_q: 1.197968
 66228/100000: episode: 1212, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 17.642, mean reward: 0.735 [0.663, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.516, 10.544], loss: 0.001853, mae: 0.046142, mean_q: 1.190399
 66275/100000: episode: 1213, duration: 0.238s, episode steps: 47, steps per second: 198, episode reward: 29.710, mean reward: 0.632 [0.525, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.213, 10.290], loss: 0.002013, mae: 0.045948, mean_q: 1.194557
 66317/100000: episode: 1214, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 26.630, mean reward: 0.634 [0.558, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.125, 10.214], loss: 0.001808, mae: 0.043622, mean_q: 1.193840
 66361/100000: episode: 1215, duration: 0.223s, episode steps: 44, steps per second: 198, episode reward: 27.220, mean reward: 0.619 [0.527, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.182, 10.293], loss: 0.001758, mae: 0.043456, mean_q: 1.200128
 66404/100000: episode: 1216, duration: 0.215s, episode steps: 43, steps per second: 200, episode reward: 27.637, mean reward: 0.643 [0.523, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.098, 10.100], loss: 0.001898, mae: 0.045661, mean_q: 1.196660
 66426/100000: episode: 1217, duration: 0.123s, episode steps: 22, steps per second: 180, episode reward: 16.533, mean reward: 0.751 [0.705, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.368, 10.100], loss: 0.002234, mae: 0.047802, mean_q: 1.203912
 66470/100000: episode: 1218, duration: 0.245s, episode steps: 44, steps per second: 180, episode reward: 31.141, mean reward: 0.708 [0.639, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.193, 10.484], loss: 0.001764, mae: 0.044498, mean_q: 1.203024
 66492/100000: episode: 1219, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 14.994, mean reward: 0.682 [0.617, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.136, 10.100], loss: 0.001694, mae: 0.044226, mean_q: 1.200631
 66534/100000: episode: 1220, duration: 0.216s, episode steps: 42, steps per second: 194, episode reward: 24.589, mean reward: 0.585 [0.531, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.496, 10.216], loss: 0.001569, mae: 0.042591, mean_q: 1.208664
 66578/100000: episode: 1221, duration: 0.233s, episode steps: 44, steps per second: 189, episode reward: 26.410, mean reward: 0.600 [0.522, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.704, 10.112], loss: 0.001790, mae: 0.044219, mean_q: 1.200579
 66625/100000: episode: 1222, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 27.244, mean reward: 0.580 [0.499, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.464, 10.104], loss: 0.001683, mae: 0.043450, mean_q: 1.206412
 66649/100000: episode: 1223, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 17.603, mean reward: 0.733 [0.582, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.557, 10.281], loss: 0.002034, mae: 0.046014, mean_q: 1.209732
[Info] FALSIFICATION!
 66662/100000: episode: 1224, duration: 0.232s, episode steps: 13, steps per second: 56, episode reward: 11.032, mean reward: 0.849 [0.665, 1.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.550, 10.082], loss: 0.002006, mae: 0.048412, mean_q: 1.230976
 66709/100000: episode: 1225, duration: 0.252s, episode steps: 47, steps per second: 187, episode reward: 28.756, mean reward: 0.612 [0.509, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.204, 10.172], loss: 0.001690, mae: 0.042986, mean_q: 1.204526
 66751/100000: episode: 1226, duration: 0.214s, episode steps: 42, steps per second: 197, episode reward: 26.571, mean reward: 0.633 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.248, 10.100], loss: 0.001564, mae: 0.041739, mean_q: 1.206661
 66794/100000: episode: 1227, duration: 0.251s, episode steps: 43, steps per second: 172, episode reward: 25.716, mean reward: 0.598 [0.512, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.269, 10.100], loss: 0.001720, mae: 0.043598, mean_q: 1.206065
 66838/100000: episode: 1228, duration: 0.235s, episode steps: 44, steps per second: 187, episode reward: 26.850, mean reward: 0.610 [0.521, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-1.474, 10.262], loss: 0.001897, mae: 0.045395, mean_q: 1.210167
 66862/100000: episode: 1229, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 15.025, mean reward: 0.626 [0.520, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.231, 10.270], loss: 0.001343, mae: 0.039871, mean_q: 1.212088
 66906/100000: episode: 1230, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 29.169, mean reward: 0.663 [0.576, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.916, 10.378], loss: 0.001673, mae: 0.043741, mean_q: 1.208930
 66953/100000: episode: 1231, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 30.078, mean reward: 0.640 [0.576, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.289, 10.314], loss: 0.001707, mae: 0.043093, mean_q: 1.207139
 66973/100000: episode: 1232, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 12.861, mean reward: 0.643 [0.579, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.296], loss: 0.001580, mae: 0.042801, mean_q: 1.205952
 67016/100000: episode: 1233, duration: 0.222s, episode steps: 43, steps per second: 194, episode reward: 26.418, mean reward: 0.614 [0.502, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.756, 10.100], loss: 0.001582, mae: 0.042330, mean_q: 1.215937
 67040/100000: episode: 1234, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 16.272, mean reward: 0.678 [0.601, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.083, 10.372], loss: 0.001817, mae: 0.044790, mean_q: 1.206432
 67083/100000: episode: 1235, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 28.238, mean reward: 0.657 [0.541, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.602, 10.247], loss: 0.001752, mae: 0.044221, mean_q: 1.218904
 67130/100000: episode: 1236, duration: 0.231s, episode steps: 47, steps per second: 203, episode reward: 28.332, mean reward: 0.603 [0.523, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-1.866, 10.341], loss: 0.001623, mae: 0.043079, mean_q: 1.205952
 67172/100000: episode: 1237, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 30.297, mean reward: 0.721 [0.549, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.751, 10.348], loss: 0.001502, mae: 0.041415, mean_q: 1.215393
 67214/100000: episode: 1238, duration: 0.214s, episode steps: 42, steps per second: 196, episode reward: 24.929, mean reward: 0.594 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.239, 10.100], loss: 0.001480, mae: 0.040437, mean_q: 1.210130
 67257/100000: episode: 1239, duration: 0.228s, episode steps: 43, steps per second: 188, episode reward: 28.102, mean reward: 0.654 [0.524, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.879, 10.151], loss: 0.001626, mae: 0.042993, mean_q: 1.214834
 67276/100000: episode: 1240, duration: 0.095s, episode steps: 19, steps per second: 201, episode reward: 12.031, mean reward: 0.633 [0.555, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.095, 10.311], loss: 0.001409, mae: 0.040621, mean_q: 1.214323
 67292/100000: episode: 1241, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 11.506, mean reward: 0.719 [0.673, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.279, 10.100], loss: 0.002235, mae: 0.047513, mean_q: 1.193743
 67339/100000: episode: 1242, duration: 0.239s, episode steps: 47, steps per second: 196, episode reward: 30.772, mean reward: 0.655 [0.557, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.293, 10.429], loss: 0.001769, mae: 0.044772, mean_q: 1.212999
 67359/100000: episode: 1243, duration: 0.108s, episode steps: 20, steps per second: 184, episode reward: 13.871, mean reward: 0.694 [0.631, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.335], loss: 0.001616, mae: 0.043282, mean_q: 1.220028
 67403/100000: episode: 1244, duration: 0.246s, episode steps: 44, steps per second: 179, episode reward: 30.754, mean reward: 0.699 [0.589, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.781, 10.279], loss: 0.001595, mae: 0.042714, mean_q: 1.222561
 67450/100000: episode: 1245, duration: 0.244s, episode steps: 47, steps per second: 193, episode reward: 30.494, mean reward: 0.649 [0.527, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.546, 10.272], loss: 0.001525, mae: 0.041301, mean_q: 1.223011
 67469/100000: episode: 1246, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 11.994, mean reward: 0.631 [0.528, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.210], loss: 0.001606, mae: 0.042934, mean_q: 1.218983
 67512/100000: episode: 1247, duration: 0.213s, episode steps: 43, steps per second: 202, episode reward: 27.139, mean reward: 0.631 [0.522, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.171, 10.205], loss: 0.001763, mae: 0.044419, mean_q: 1.214476
 67532/100000: episode: 1248, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 14.073, mean reward: 0.704 [0.587, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.418, 10.305], loss: 0.001408, mae: 0.041089, mean_q: 1.221181
 67575/100000: episode: 1249, duration: 0.221s, episode steps: 43, steps per second: 195, episode reward: 27.903, mean reward: 0.649 [0.527, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.127, 10.262], loss: 0.001722, mae: 0.043172, mean_q: 1.223586
 67599/100000: episode: 1250, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 16.610, mean reward: 0.692 [0.636, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.144, 10.360], loss: 0.001548, mae: 0.042556, mean_q: 1.235087
 67619/100000: episode: 1251, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 13.228, mean reward: 0.661 [0.597, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.402], loss: 0.001568, mae: 0.042767, mean_q: 1.227670
 67662/100000: episode: 1252, duration: 0.226s, episode steps: 43, steps per second: 190, episode reward: 28.669, mean reward: 0.667 [0.575, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.970, 10.447], loss: 0.001654, mae: 0.043387, mean_q: 1.219684
 67706/100000: episode: 1253, duration: 0.223s, episode steps: 44, steps per second: 197, episode reward: 33.130, mean reward: 0.753 [0.616, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.026, 10.563], loss: 0.001359, mae: 0.040184, mean_q: 1.228814
 67748/100000: episode: 1254, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 29.532, mean reward: 0.703 [0.609, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.848, 10.465], loss: 0.001493, mae: 0.042127, mean_q: 1.237160
 67791/100000: episode: 1255, duration: 0.239s, episode steps: 43, steps per second: 180, episode reward: 28.639, mean reward: 0.666 [0.581, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.544, 10.350], loss: 0.001490, mae: 0.041256, mean_q: 1.234485
 67807/100000: episode: 1256, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 12.062, mean reward: 0.754 [0.671, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.570, 10.100], loss: 0.002070, mae: 0.044790, mean_q: 1.234736
 67851/100000: episode: 1257, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 26.588, mean reward: 0.604 [0.538, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.174, 10.242], loss: 0.001555, mae: 0.042391, mean_q: 1.236216
[Info] Complete ISplit Iteration
[Info] Levels: [1.3533946, 1.6178485]
[Info] Cond. Prob: [0.1, 0.03]
[Info] Error Prob: 0.003

 67894/100000: episode: 1258, duration: 4.396s, episode steps: 43, steps per second: 10, episode reward: 28.356, mean reward: 0.659 [0.580, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-1.224, 10.345], loss: 0.001288, mae: 0.038637, mean_q: 1.235473
 67994/100000: episode: 1259, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.104, mean reward: 0.591 [0.510, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.437, 10.098], loss: 0.001580, mae: 0.042148, mean_q: 1.233827
 68094/100000: episode: 1260, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.937, mean reward: 0.569 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.733, 10.098], loss: 0.001578, mae: 0.041578, mean_q: 1.233334
 68194/100000: episode: 1261, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.603, mean reward: 0.586 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.936, 10.098], loss: 0.001606, mae: 0.041890, mean_q: 1.223420
 68294/100000: episode: 1262, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.904, mean reward: 0.599 [0.514, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.107, 10.098], loss: 0.001448, mae: 0.040469, mean_q: 1.232690
 68394/100000: episode: 1263, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.819, mean reward: 0.598 [0.509, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.539, 10.276], loss: 0.001628, mae: 0.042387, mean_q: 1.226555
 68494/100000: episode: 1264, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.576, mean reward: 0.586 [0.507, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.559, 10.200], loss: 0.001547, mae: 0.040888, mean_q: 1.231143
 68594/100000: episode: 1265, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.521, mean reward: 0.595 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.940, 10.098], loss: 0.001479, mae: 0.041142, mean_q: 1.228807
 68694/100000: episode: 1266, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.847, mean reward: 0.588 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.101, 10.307], loss: 0.001464, mae: 0.040381, mean_q: 1.230976
 68794/100000: episode: 1267, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 59.408, mean reward: 0.594 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.511, 10.179], loss: 0.001603, mae: 0.042001, mean_q: 1.230676
 68894/100000: episode: 1268, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.140, mean reward: 0.591 [0.500, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.444, 10.117], loss: 0.001572, mae: 0.042124, mean_q: 1.238544
 68994/100000: episode: 1269, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.704, mean reward: 0.597 [0.507, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.598, 10.332], loss: 0.001415, mae: 0.040070, mean_q: 1.228637
 69094/100000: episode: 1270, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 60.730, mean reward: 0.607 [0.510, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.825, 10.254], loss: 0.001344, mae: 0.039823, mean_q: 1.233039
 69194/100000: episode: 1271, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.558, mean reward: 0.576 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.002, 10.098], loss: 0.001603, mae: 0.041742, mean_q: 1.228106
 69294/100000: episode: 1272, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 59.222, mean reward: 0.592 [0.499, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.077, 10.185], loss: 0.001579, mae: 0.040859, mean_q: 1.233015
 69394/100000: episode: 1273, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.842, mean reward: 0.578 [0.516, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.832, 10.281], loss: 0.001664, mae: 0.042882, mean_q: 1.232324
 69494/100000: episode: 1274, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 59.081, mean reward: 0.591 [0.500, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.840, 10.469], loss: 0.001534, mae: 0.041294, mean_q: 1.231841
 69594/100000: episode: 1275, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.427, mean reward: 0.574 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.914, 10.098], loss: 0.001410, mae: 0.040104, mean_q: 1.232454
 69694/100000: episode: 1276, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.866, mean reward: 0.579 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.790, 10.264], loss: 0.001523, mae: 0.041419, mean_q: 1.231648
 69794/100000: episode: 1277, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 56.590, mean reward: 0.566 [0.505, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.997, 10.176], loss: 0.001468, mae: 0.040944, mean_q: 1.228634
 69894/100000: episode: 1278, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.845, mean reward: 0.588 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.649, 10.265], loss: 0.001426, mae: 0.040116, mean_q: 1.229034
 69994/100000: episode: 1279, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.867, mean reward: 0.599 [0.505, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.468, 10.098], loss: 0.001587, mae: 0.042008, mean_q: 1.224663
 70094/100000: episode: 1280, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 61.131, mean reward: 0.611 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.040, 10.239], loss: 0.001471, mae: 0.041188, mean_q: 1.217666
 70194/100000: episode: 1281, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 59.913, mean reward: 0.599 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.867, 10.229], loss: 0.001439, mae: 0.041124, mean_q: 1.221593
 70294/100000: episode: 1282, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.657, mean reward: 0.607 [0.507, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.836, 10.321], loss: 0.001426, mae: 0.040472, mean_q: 1.222047
 70394/100000: episode: 1283, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 60.431, mean reward: 0.604 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.257, 10.114], loss: 0.001515, mae: 0.040773, mean_q: 1.214884
 70494/100000: episode: 1284, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 56.726, mean reward: 0.567 [0.505, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.423, 10.098], loss: 0.001686, mae: 0.043240, mean_q: 1.210702
 70594/100000: episode: 1285, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 59.316, mean reward: 0.593 [0.498, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.183, 10.099], loss: 0.001389, mae: 0.039903, mean_q: 1.213867
 70694/100000: episode: 1286, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 61.380, mean reward: 0.614 [0.510, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.644, 10.242], loss: 0.001535, mae: 0.041635, mean_q: 1.213880
 70794/100000: episode: 1287, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.949, mean reward: 0.579 [0.510, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.577, 10.098], loss: 0.001445, mae: 0.040632, mean_q: 1.205871
 70894/100000: episode: 1288, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 56.804, mean reward: 0.568 [0.512, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.588, 10.252], loss: 0.001436, mae: 0.040536, mean_q: 1.209738
 70994/100000: episode: 1289, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.557, mean reward: 0.576 [0.502, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.680, 10.098], loss: 0.001450, mae: 0.040384, mean_q: 1.203937
 71094/100000: episode: 1290, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.548, mean reward: 0.575 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.028, 10.147], loss: 0.001438, mae: 0.039758, mean_q: 1.202531
 71194/100000: episode: 1291, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.910, mean reward: 0.579 [0.513, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.700, 10.098], loss: 0.001536, mae: 0.041328, mean_q: 1.197376
 71294/100000: episode: 1292, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.374, mean reward: 0.594 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.978, 10.150], loss: 0.001565, mae: 0.041787, mean_q: 1.198363
 71394/100000: episode: 1293, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.693, mean reward: 0.597 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.562, 10.098], loss: 0.001435, mae: 0.040752, mean_q: 1.198426
 71494/100000: episode: 1294, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.857, mean reward: 0.579 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.825, 10.140], loss: 0.001521, mae: 0.041001, mean_q: 1.196078
 71594/100000: episode: 1295, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.960, mean reward: 0.600 [0.509, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.368, 10.355], loss: 0.001452, mae: 0.040666, mean_q: 1.197812
 71694/100000: episode: 1296, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 62.010, mean reward: 0.620 [0.516, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.317, 10.327], loss: 0.001516, mae: 0.041427, mean_q: 1.190335
 71794/100000: episode: 1297, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.686, mean reward: 0.577 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.542, 10.120], loss: 0.001489, mae: 0.041655, mean_q: 1.191633
 71894/100000: episode: 1298, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.128, mean reward: 0.581 [0.500, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.545, 10.098], loss: 0.001323, mae: 0.038928, mean_q: 1.185332
 71994/100000: episode: 1299, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.490, mean reward: 0.595 [0.501, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.722, 10.098], loss: 0.001330, mae: 0.039760, mean_q: 1.190332
 72094/100000: episode: 1300, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.669, mean reward: 0.587 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.779, 10.098], loss: 0.001352, mae: 0.039516, mean_q: 1.184751
 72194/100000: episode: 1301, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 63.554, mean reward: 0.636 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.506, 10.378], loss: 0.001511, mae: 0.040988, mean_q: 1.181601
 72294/100000: episode: 1302, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 60.460, mean reward: 0.605 [0.510, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.636, 10.098], loss: 0.001421, mae: 0.040110, mean_q: 1.182540
 72394/100000: episode: 1303, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.889, mean reward: 0.589 [0.508, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.414, 10.175], loss: 0.001456, mae: 0.040456, mean_q: 1.181942
 72494/100000: episode: 1304, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.102, mean reward: 0.571 [0.500, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.487, 10.225], loss: 0.001486, mae: 0.041734, mean_q: 1.180404
 72594/100000: episode: 1305, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.052, mean reward: 0.571 [0.502, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.535, 10.159], loss: 0.001311, mae: 0.039325, mean_q: 1.175881
 72694/100000: episode: 1306, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.719, mean reward: 0.587 [0.510, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.858, 10.114], loss: 0.001367, mae: 0.039861, mean_q: 1.173013
 72794/100000: episode: 1307, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 61.670, mean reward: 0.617 [0.511, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.641, 10.098], loss: 0.001354, mae: 0.040064, mean_q: 1.165669
 72894/100000: episode: 1308, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.139, mean reward: 0.581 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.903, 10.200], loss: 0.001262, mae: 0.038746, mean_q: 1.166553
 72994/100000: episode: 1309, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 61.853, mean reward: 0.619 [0.510, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.818, 10.254], loss: 0.001372, mae: 0.040953, mean_q: 1.167530
 73094/100000: episode: 1310, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 56.895, mean reward: 0.569 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.190, 10.277], loss: 0.001198, mae: 0.037926, mean_q: 1.167581
 73194/100000: episode: 1311, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.955, mean reward: 0.610 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.371, 10.284], loss: 0.001292, mae: 0.039353, mean_q: 1.167351
 73294/100000: episode: 1312, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.741, mean reward: 0.587 [0.512, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.717, 10.209], loss: 0.001202, mae: 0.038430, mean_q: 1.168344
 73394/100000: episode: 1313, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.451, mean reward: 0.575 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.506, 10.101], loss: 0.001203, mae: 0.038407, mean_q: 1.168291
 73494/100000: episode: 1314, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.739, mean reward: 0.577 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.189, 10.219], loss: 0.001184, mae: 0.038011, mean_q: 1.167149
 73594/100000: episode: 1315, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.470, mean reward: 0.575 [0.499, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.156, 10.244], loss: 0.001282, mae: 0.039878, mean_q: 1.167657
 73694/100000: episode: 1316, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 59.175, mean reward: 0.592 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.656, 10.098], loss: 0.001207, mae: 0.038377, mean_q: 1.168205
 73794/100000: episode: 1317, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.623, mean reward: 0.586 [0.508, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.406, 10.195], loss: 0.001216, mae: 0.037929, mean_q: 1.164477
 73894/100000: episode: 1318, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.143, mean reward: 0.601 [0.512, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.751, 10.371], loss: 0.001268, mae: 0.038849, mean_q: 1.168257
 73994/100000: episode: 1319, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.568, mean reward: 0.586 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.698, 10.098], loss: 0.001260, mae: 0.038951, mean_q: 1.167518
 74094/100000: episode: 1320, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 60.467, mean reward: 0.605 [0.505, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.098, 10.464], loss: 0.001215, mae: 0.038685, mean_q: 1.163749
 74194/100000: episode: 1321, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.810, mean reward: 0.598 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.514, 10.098], loss: 0.001238, mae: 0.038456, mean_q: 1.165185
 74294/100000: episode: 1322, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 57.109, mean reward: 0.571 [0.517, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.203, 10.152], loss: 0.001216, mae: 0.038399, mean_q: 1.165441
 74394/100000: episode: 1323, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 60.920, mean reward: 0.609 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.821, 10.186], loss: 0.001185, mae: 0.037856, mean_q: 1.165362
 74494/100000: episode: 1324, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 56.719, mean reward: 0.567 [0.499, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.258, 10.228], loss: 0.001212, mae: 0.038328, mean_q: 1.168205
 74594/100000: episode: 1325, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.574, mean reward: 0.586 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.548, 10.362], loss: 0.001185, mae: 0.038051, mean_q: 1.167655
 74694/100000: episode: 1326, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 62.239, mean reward: 0.622 [0.530, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.934, 10.310], loss: 0.001331, mae: 0.040309, mean_q: 1.167927
 74794/100000: episode: 1327, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.021, mean reward: 0.570 [0.507, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.778, 10.164], loss: 0.001205, mae: 0.038254, mean_q: 1.165805
 74894/100000: episode: 1328, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.013, mean reward: 0.570 [0.505, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.998, 10.120], loss: 0.001167, mae: 0.037780, mean_q: 1.167466
 74994/100000: episode: 1329, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.518, mean reward: 0.595 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.844, 10.336], loss: 0.001261, mae: 0.038856, mean_q: 1.169124
 75094/100000: episode: 1330, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 59.854, mean reward: 0.599 [0.506, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.121, 10.130], loss: 0.001292, mae: 0.039080, mean_q: 1.164461
 75194/100000: episode: 1331, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.693, mean reward: 0.577 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.478, 10.098], loss: 0.001186, mae: 0.038372, mean_q: 1.170590
 75294/100000: episode: 1332, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.895, mean reward: 0.579 [0.505, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.617, 10.118], loss: 0.001136, mae: 0.037262, mean_q: 1.164549
 75394/100000: episode: 1333, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 61.436, mean reward: 0.614 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.536, 10.098], loss: 0.001200, mae: 0.038576, mean_q: 1.166183
 75494/100000: episode: 1334, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.910, mean reward: 0.599 [0.514, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.379, 10.098], loss: 0.001186, mae: 0.037665, mean_q: 1.168441
 75594/100000: episode: 1335, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.150, mean reward: 0.602 [0.513, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.890, 10.098], loss: 0.001219, mae: 0.038353, mean_q: 1.167956
 75694/100000: episode: 1336, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.541, mean reward: 0.585 [0.510, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.250, 10.120], loss: 0.001203, mae: 0.038178, mean_q: 1.167781
 75794/100000: episode: 1337, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 62.962, mean reward: 0.630 [0.511, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.046, 10.341], loss: 0.001222, mae: 0.038433, mean_q: 1.167191
 75894/100000: episode: 1338, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 59.794, mean reward: 0.598 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.082, 10.151], loss: 0.001272, mae: 0.039001, mean_q: 1.169415
 75994/100000: episode: 1339, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 57.118, mean reward: 0.571 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.532, 10.115], loss: 0.001319, mae: 0.039850, mean_q: 1.169698
 76094/100000: episode: 1340, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 59.122, mean reward: 0.591 [0.507, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.898, 10.113], loss: 0.001274, mae: 0.039483, mean_q: 1.172557
 76194/100000: episode: 1341, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 59.716, mean reward: 0.597 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.088, 10.137], loss: 0.001304, mae: 0.040144, mean_q: 1.170308
 76294/100000: episode: 1342, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 56.909, mean reward: 0.569 [0.504, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.434, 10.241], loss: 0.001194, mae: 0.038257, mean_q: 1.170331
 76394/100000: episode: 1343, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.867, mean reward: 0.589 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.342, 10.216], loss: 0.001274, mae: 0.039344, mean_q: 1.170822
 76494/100000: episode: 1344, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.880, mean reward: 0.589 [0.509, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.913, 10.375], loss: 0.001197, mae: 0.038289, mean_q: 1.168531
 76594/100000: episode: 1345, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 59.903, mean reward: 0.599 [0.511, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.992, 10.098], loss: 0.001277, mae: 0.038970, mean_q: 1.170579
 76694/100000: episode: 1346, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 57.406, mean reward: 0.574 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.037, 10.098], loss: 0.001359, mae: 0.039782, mean_q: 1.168500
 76794/100000: episode: 1347, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.563, mean reward: 0.596 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.910, 10.098], loss: 0.001342, mae: 0.040209, mean_q: 1.168905
 76894/100000: episode: 1348, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.783, mean reward: 0.568 [0.500, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.022, 10.098], loss: 0.001310, mae: 0.039384, mean_q: 1.166860
 76994/100000: episode: 1349, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 59.236, mean reward: 0.592 [0.497, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.575, 10.098], loss: 0.001300, mae: 0.039671, mean_q: 1.167159
 77094/100000: episode: 1350, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.813, mean reward: 0.578 [0.497, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.329, 10.098], loss: 0.001281, mae: 0.039348, mean_q: 1.166127
 77194/100000: episode: 1351, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 60.579, mean reward: 0.606 [0.507, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.943, 10.098], loss: 0.001272, mae: 0.039260, mean_q: 1.164694
 77294/100000: episode: 1352, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.885, mean reward: 0.589 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.967, 10.210], loss: 0.001299, mae: 0.039614, mean_q: 1.167012
 77394/100000: episode: 1353, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 59.881, mean reward: 0.599 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.515, 10.098], loss: 0.001263, mae: 0.039168, mean_q: 1.162857
 77494/100000: episode: 1354, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.523, mean reward: 0.595 [0.511, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.290, 10.403], loss: 0.001342, mae: 0.040471, mean_q: 1.164400
 77594/100000: episode: 1355, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 59.114, mean reward: 0.591 [0.498, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.775, 10.416], loss: 0.001437, mae: 0.041484, mean_q: 1.166135
 77694/100000: episode: 1356, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.551, mean reward: 0.596 [0.509, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.587, 10.281], loss: 0.001181, mae: 0.038161, mean_q: 1.168876
 77794/100000: episode: 1357, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 61.242, mean reward: 0.612 [0.508, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.172, 10.098], loss: 0.001219, mae: 0.038430, mean_q: 1.168585
[Info] 1-TH LEVEL FOUND: 1.3575166463851929, Considering 10/90 traces
 77894/100000: episode: 1358, duration: 4.627s, episode steps: 100, steps per second: 22, episode reward: 58.067, mean reward: 0.581 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.521, 10.098], loss: 0.001276, mae: 0.039121, mean_q: 1.170864
 77932/100000: episode: 1359, duration: 0.223s, episode steps: 38, steps per second: 170, episode reward: 23.399, mean reward: 0.616 [0.510, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.150, 10.269], loss: 0.001261, mae: 0.039015, mean_q: 1.163515
 77978/100000: episode: 1360, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 31.518, mean reward: 0.685 [0.535, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.744, 10.219], loss: 0.001606, mae: 0.043326, mean_q: 1.167090
 78032/100000: episode: 1361, duration: 0.269s, episode steps: 54, steps per second: 200, episode reward: 34.274, mean reward: 0.635 [0.542, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.930, 10.100], loss: 0.001312, mae: 0.040491, mean_q: 1.168489
 78060/100000: episode: 1362, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 19.118, mean reward: 0.683 [0.592, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.440, 10.100], loss: 0.001169, mae: 0.037371, mean_q: 1.173269
 78114/100000: episode: 1363, duration: 0.275s, episode steps: 54, steps per second: 196, episode reward: 35.807, mean reward: 0.663 [0.588, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.434, 10.365], loss: 0.001317, mae: 0.039962, mean_q: 1.173900
 78152/100000: episode: 1364, duration: 0.238s, episode steps: 38, steps per second: 160, episode reward: 23.428, mean reward: 0.617 [0.505, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.318, 10.297], loss: 0.001221, mae: 0.038290, mean_q: 1.174886
 78190/100000: episode: 1365, duration: 0.246s, episode steps: 38, steps per second: 154, episode reward: 22.688, mean reward: 0.597 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.019, 10.100], loss: 0.001686, mae: 0.042401, mean_q: 1.168094
 78215/100000: episode: 1366, duration: 0.154s, episode steps: 25, steps per second: 163, episode reward: 18.704, mean reward: 0.748 [0.679, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.520, 10.519], loss: 0.001317, mae: 0.039467, mean_q: 1.173124
 78269/100000: episode: 1367, duration: 0.287s, episode steps: 54, steps per second: 188, episode reward: 35.476, mean reward: 0.657 [0.539, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.404, 10.340], loss: 0.001472, mae: 0.041179, mean_q: 1.176967
 78293/100000: episode: 1368, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 16.555, mean reward: 0.690 [0.599, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.664, 10.100], loss: 0.001595, mae: 0.041960, mean_q: 1.177570
 78331/100000: episode: 1369, duration: 0.212s, episode steps: 38, steps per second: 179, episode reward: 26.288, mean reward: 0.692 [0.612, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.161, 10.330], loss: 0.001494, mae: 0.040450, mean_q: 1.174654
 78385/100000: episode: 1370, duration: 0.288s, episode steps: 54, steps per second: 187, episode reward: 35.807, mean reward: 0.663 [0.557, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.396, 10.194], loss: 0.001520, mae: 0.041898, mean_q: 1.181540
 78413/100000: episode: 1371, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 18.665, mean reward: 0.667 [0.579, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.203, 10.100], loss: 0.001854, mae: 0.044762, mean_q: 1.180587
 78459/100000: episode: 1372, duration: 0.261s, episode steps: 46, steps per second: 176, episode reward: 33.046, mean reward: 0.718 [0.616, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.275, 10.371], loss: 0.001513, mae: 0.043043, mean_q: 1.181193
 78505/100000: episode: 1373, duration: 0.301s, episode steps: 46, steps per second: 153, episode reward: 29.246, mean reward: 0.636 [0.549, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.260, 10.372], loss: 0.001712, mae: 0.043077, mean_q: 1.182572
 78550/100000: episode: 1374, duration: 0.262s, episode steps: 45, steps per second: 172, episode reward: 28.690, mean reward: 0.638 [0.564, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.576, 10.314], loss: 0.001520, mae: 0.042656, mean_q: 1.183006
 78593/100000: episode: 1375, duration: 0.222s, episode steps: 43, steps per second: 194, episode reward: 28.457, mean reward: 0.662 [0.558, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.560, 10.466], loss: 0.001512, mae: 0.041301, mean_q: 1.184352
 78636/100000: episode: 1376, duration: 0.259s, episode steps: 43, steps per second: 166, episode reward: 26.283, mean reward: 0.611 [0.535, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.761, 10.368], loss: 0.001442, mae: 0.040997, mean_q: 1.182872
 78681/100000: episode: 1377, duration: 0.244s, episode steps: 45, steps per second: 185, episode reward: 27.262, mean reward: 0.606 [0.547, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.164, 10.187], loss: 0.001545, mae: 0.041227, mean_q: 1.185183
 78719/100000: episode: 1378, duration: 0.224s, episode steps: 38, steps per second: 170, episode reward: 25.676, mean reward: 0.676 [0.588, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.058, 10.349], loss: 0.001678, mae: 0.042908, mean_q: 1.189121
 78773/100000: episode: 1379, duration: 0.277s, episode steps: 54, steps per second: 195, episode reward: 38.026, mean reward: 0.704 [0.575, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.723, 10.279], loss: 0.001483, mae: 0.042128, mean_q: 1.189007
 78801/100000: episode: 1380, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 18.780, mean reward: 0.671 [0.575, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.190, 10.100], loss: 0.001440, mae: 0.041043, mean_q: 1.192480
 78826/100000: episode: 1381, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 16.013, mean reward: 0.641 [0.552, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.333, 10.505], loss: 0.001948, mae: 0.045391, mean_q: 1.194539
 78854/100000: episode: 1382, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 18.710, mean reward: 0.668 [0.560, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.369, 10.100], loss: 0.001469, mae: 0.040779, mean_q: 1.189229
 78899/100000: episode: 1383, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 30.413, mean reward: 0.676 [0.573, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-1.075, 10.263], loss: 0.001649, mae: 0.043647, mean_q: 1.190248
 78945/100000: episode: 1384, duration: 0.273s, episode steps: 46, steps per second: 169, episode reward: 31.228, mean reward: 0.679 [0.507, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.821, 10.100], loss: 0.001723, mae: 0.042247, mean_q: 1.196679
 78988/100000: episode: 1385, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 26.533, mean reward: 0.617 [0.510, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.975, 10.100], loss: 0.001583, mae: 0.042333, mean_q: 1.196936
 79034/100000: episode: 1386, duration: 0.252s, episode steps: 46, steps per second: 183, episode reward: 31.409, mean reward: 0.683 [0.577, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.435, 10.232], loss: 0.001721, mae: 0.043319, mean_q: 1.199519
 79062/100000: episode: 1387, duration: 0.167s, episode steps: 28, steps per second: 167, episode reward: 20.154, mean reward: 0.720 [0.610, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.905, 10.100], loss: 0.001603, mae: 0.043097, mean_q: 1.193344
 79090/100000: episode: 1388, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 19.463, mean reward: 0.695 [0.616, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.318, 10.100], loss: 0.001571, mae: 0.040763, mean_q: 1.197534
 79115/100000: episode: 1389, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 16.175, mean reward: 0.647 [0.581, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.538, 10.344], loss: 0.001580, mae: 0.042028, mean_q: 1.191431
 79153/100000: episode: 1390, duration: 0.195s, episode steps: 38, steps per second: 194, episode reward: 22.692, mean reward: 0.597 [0.510, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.300, 10.149], loss: 0.001575, mae: 0.041950, mean_q: 1.197093
 79199/100000: episode: 1391, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 30.140, mean reward: 0.655 [0.546, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.301, 10.233], loss: 0.001832, mae: 0.044324, mean_q: 1.196470
 79237/100000: episode: 1392, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 25.536, mean reward: 0.672 [0.600, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.318, 10.375], loss: 0.001670, mae: 0.042339, mean_q: 1.192576
 79275/100000: episode: 1393, duration: 0.219s, episode steps: 38, steps per second: 174, episode reward: 25.857, mean reward: 0.680 [0.590, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.436, 10.311], loss: 0.001647, mae: 0.042674, mean_q: 1.192245
 79300/100000: episode: 1394, duration: 0.145s, episode steps: 25, steps per second: 172, episode reward: 17.932, mean reward: 0.717 [0.625, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.501, 10.335], loss: 0.001785, mae: 0.045115, mean_q: 1.203110
 79338/100000: episode: 1395, duration: 0.206s, episode steps: 38, steps per second: 184, episode reward: 26.501, mean reward: 0.697 [0.622, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.322, 10.438], loss: 0.001852, mae: 0.044784, mean_q: 1.196150
 79376/100000: episode: 1396, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 26.945, mean reward: 0.709 [0.625, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.147, 10.509], loss: 0.001637, mae: 0.042624, mean_q: 1.206194
 79401/100000: episode: 1397, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 16.735, mean reward: 0.669 [0.549, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.257], loss: 0.001668, mae: 0.043253, mean_q: 1.207810
 79425/100000: episode: 1398, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 16.750, mean reward: 0.698 [0.617, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.265, 10.100], loss: 0.001570, mae: 0.042481, mean_q: 1.205719
 79470/100000: episode: 1399, duration: 0.239s, episode steps: 45, steps per second: 189, episode reward: 32.945, mean reward: 0.732 [0.631, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.854, 10.407], loss: 0.001752, mae: 0.043851, mean_q: 1.209404
 79494/100000: episode: 1400, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 16.692, mean reward: 0.695 [0.611, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.367, 10.100], loss: 0.001297, mae: 0.039190, mean_q: 1.216096
 79532/100000: episode: 1401, duration: 0.223s, episode steps: 38, steps per second: 171, episode reward: 29.764, mean reward: 0.783 [0.683, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.905, 10.475], loss: 0.001455, mae: 0.040775, mean_q: 1.212402
 79570/100000: episode: 1402, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 23.548, mean reward: 0.620 [0.519, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.035, 10.208], loss: 0.001565, mae: 0.041835, mean_q: 1.217954
 79616/100000: episode: 1403, duration: 0.254s, episode steps: 46, steps per second: 181, episode reward: 27.599, mean reward: 0.600 [0.513, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.242, 10.265], loss: 0.002008, mae: 0.045673, mean_q: 1.213586
 79654/100000: episode: 1404, duration: 0.192s, episode steps: 38, steps per second: 198, episode reward: 22.564, mean reward: 0.594 [0.512, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.561, 10.189], loss: 0.001597, mae: 0.043030, mean_q: 1.214212
 79682/100000: episode: 1405, duration: 0.173s, episode steps: 28, steps per second: 162, episode reward: 19.068, mean reward: 0.681 [0.578, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.069, 10.100], loss: 0.001972, mae: 0.046140, mean_q: 1.204875
 79725/100000: episode: 1406, duration: 0.238s, episode steps: 43, steps per second: 180, episode reward: 25.923, mean reward: 0.603 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.113, 10.378], loss: 0.001978, mae: 0.046078, mean_q: 1.213257
 79763/100000: episode: 1407, duration: 0.270s, episode steps: 38, steps per second: 141, episode reward: 24.126, mean reward: 0.635 [0.551, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.577, 10.277], loss: 0.001747, mae: 0.045272, mean_q: 1.214985
 79817/100000: episode: 1408, duration: 0.347s, episode steps: 54, steps per second: 156, episode reward: 39.349, mean reward: 0.729 [0.642, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-0.598, 10.403], loss: 0.001513, mae: 0.041638, mean_q: 1.214685
 79845/100000: episode: 1409, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 18.059, mean reward: 0.645 [0.579, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.230, 10.100], loss: 0.001728, mae: 0.043244, mean_q: 1.224155
 79891/100000: episode: 1410, duration: 0.254s, episode steps: 46, steps per second: 181, episode reward: 28.253, mean reward: 0.614 [0.521, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.501, 10.256], loss: 0.001579, mae: 0.040658, mean_q: 1.223408
 79916/100000: episode: 1411, duration: 0.145s, episode steps: 25, steps per second: 173, episode reward: 17.777, mean reward: 0.711 [0.646, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.544, 10.483], loss: 0.001598, mae: 0.041627, mean_q: 1.210951
 79962/100000: episode: 1412, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 31.008, mean reward: 0.674 [0.559, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.933, 10.252], loss: 0.001724, mae: 0.044053, mean_q: 1.219877
 80008/100000: episode: 1413, duration: 0.244s, episode steps: 46, steps per second: 188, episode reward: 29.297, mean reward: 0.637 [0.514, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.748, 10.212], loss: 0.001691, mae: 0.043436, mean_q: 1.220418
 80053/100000: episode: 1414, duration: 0.240s, episode steps: 45, steps per second: 187, episode reward: 28.164, mean reward: 0.626 [0.534, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.415, 10.315], loss: 0.001512, mae: 0.040793, mean_q: 1.228606
 80091/100000: episode: 1415, duration: 0.212s, episode steps: 38, steps per second: 179, episode reward: 26.344, mean reward: 0.693 [0.658, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.321, 10.371], loss: 0.001642, mae: 0.042915, mean_q: 1.214646
 80129/100000: episode: 1416, duration: 0.195s, episode steps: 38, steps per second: 195, episode reward: 24.702, mean reward: 0.650 [0.555, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.390, 10.295], loss: 0.001714, mae: 0.043081, mean_q: 1.222019
 80154/100000: episode: 1417, duration: 0.163s, episode steps: 25, steps per second: 153, episode reward: 17.507, mean reward: 0.700 [0.659, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.458], loss: 0.001579, mae: 0.043093, mean_q: 1.216478
 80179/100000: episode: 1418, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 18.291, mean reward: 0.732 [0.668, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.077, 10.462], loss: 0.001662, mae: 0.042754, mean_q: 1.234841
 80224/100000: episode: 1419, duration: 0.286s, episode steps: 45, steps per second: 157, episode reward: 26.272, mean reward: 0.584 [0.510, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.164, 10.215], loss: 0.001766, mae: 0.043978, mean_q: 1.228408
 80269/100000: episode: 1420, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 26.998, mean reward: 0.600 [0.510, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-1.019, 10.291], loss: 0.001607, mae: 0.042473, mean_q: 1.220180
 80323/100000: episode: 1421, duration: 0.270s, episode steps: 54, steps per second: 200, episode reward: 37.817, mean reward: 0.700 [0.525, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.618, 10.101], loss: 0.001734, mae: 0.042789, mean_q: 1.226977
 80366/100000: episode: 1422, duration: 0.220s, episode steps: 43, steps per second: 196, episode reward: 26.415, mean reward: 0.614 [0.524, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.316, 10.122], loss: 0.001671, mae: 0.043004, mean_q: 1.226609
 80390/100000: episode: 1423, duration: 0.144s, episode steps: 24, steps per second: 166, episode reward: 15.389, mean reward: 0.641 [0.569, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.763, 10.100], loss: 0.001672, mae: 0.043635, mean_q: 1.222253
 80428/100000: episode: 1424, duration: 0.234s, episode steps: 38, steps per second: 163, episode reward: 24.376, mean reward: 0.641 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.164, 10.119], loss: 0.001594, mae: 0.042844, mean_q: 1.225296
 80452/100000: episode: 1425, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 14.989, mean reward: 0.625 [0.557, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.137, 10.100], loss: 0.001769, mae: 0.044846, mean_q: 1.225241
 80497/100000: episode: 1426, duration: 0.277s, episode steps: 45, steps per second: 162, episode reward: 30.017, mean reward: 0.667 [0.566, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.156, 10.295], loss: 0.001401, mae: 0.041131, mean_q: 1.233254
 80535/100000: episode: 1427, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 23.618, mean reward: 0.622 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.039, 10.218], loss: 0.001516, mae: 0.042256, mean_q: 1.232277
 80578/100000: episode: 1428, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 29.875, mean reward: 0.695 [0.591, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.986, 10.591], loss: 0.001564, mae: 0.040893, mean_q: 1.233799
 80603/100000: episode: 1429, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 17.481, mean reward: 0.699 [0.588, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.377, 10.400], loss: 0.001343, mae: 0.040400, mean_q: 1.238802
 80628/100000: episode: 1430, duration: 0.156s, episode steps: 25, steps per second: 160, episode reward: 16.373, mean reward: 0.655 [0.555, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.062, 10.239], loss: 0.001427, mae: 0.039634, mean_q: 1.237904
 80671/100000: episode: 1431, duration: 0.219s, episode steps: 43, steps per second: 196, episode reward: 27.431, mean reward: 0.638 [0.501, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.183, 10.183], loss: 0.001789, mae: 0.044153, mean_q: 1.224498
 80725/100000: episode: 1432, duration: 0.294s, episode steps: 54, steps per second: 184, episode reward: 37.432, mean reward: 0.693 [0.504, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.658, 10.269], loss: 0.001659, mae: 0.042819, mean_q: 1.233106
 80768/100000: episode: 1433, duration: 0.229s, episode steps: 43, steps per second: 188, episode reward: 26.508, mean reward: 0.616 [0.512, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.325, 10.159], loss: 0.001891, mae: 0.044450, mean_q: 1.229830
 80814/100000: episode: 1434, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 32.105, mean reward: 0.698 [0.632, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.903, 10.389], loss: 0.001664, mae: 0.043699, mean_q: 1.228799
 80852/100000: episode: 1435, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 28.117, mean reward: 0.740 [0.656, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.033, 10.380], loss: 0.001548, mae: 0.041881, mean_q: 1.242450
 80890/100000: episode: 1436, duration: 0.257s, episode steps: 38, steps per second: 148, episode reward: 24.884, mean reward: 0.655 [0.568, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.391, 10.365], loss: 0.001441, mae: 0.040315, mean_q: 1.236563
 80914/100000: episode: 1437, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 16.915, mean reward: 0.705 [0.553, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.352, 10.100], loss: 0.001344, mae: 0.039666, mean_q: 1.250055
 80938/100000: episode: 1438, duration: 0.160s, episode steps: 24, steps per second: 150, episode reward: 15.127, mean reward: 0.630 [0.521, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.222, 10.100], loss: 0.001521, mae: 0.041671, mean_q: 1.243989
 80966/100000: episode: 1439, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 19.105, mean reward: 0.682 [0.593, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.621, 10.100], loss: 0.001366, mae: 0.041001, mean_q: 1.244053
 81020/100000: episode: 1440, duration: 0.268s, episode steps: 54, steps per second: 202, episode reward: 34.254, mean reward: 0.634 [0.500, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-1.377, 10.100], loss: 0.001578, mae: 0.042216, mean_q: 1.250794
 81074/100000: episode: 1441, duration: 0.280s, episode steps: 54, steps per second: 193, episode reward: 33.692, mean reward: 0.624 [0.508, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.396, 10.258], loss: 0.001718, mae: 0.042927, mean_q: 1.247586
 81099/100000: episode: 1442, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 17.149, mean reward: 0.686 [0.628, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.446, 10.277], loss: 0.001849, mae: 0.045036, mean_q: 1.241547
 81137/100000: episode: 1443, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 25.407, mean reward: 0.669 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-1.838, 10.100], loss: 0.001395, mae: 0.039948, mean_q: 1.251063
 81182/100000: episode: 1444, duration: 0.265s, episode steps: 45, steps per second: 170, episode reward: 29.219, mean reward: 0.649 [0.507, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.760, 10.115], loss: 0.001836, mae: 0.044325, mean_q: 1.248057
 81220/100000: episode: 1445, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 25.588, mean reward: 0.673 [0.610, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.447, 10.344], loss: 0.001353, mae: 0.040523, mean_q: 1.246170
 81248/100000: episode: 1446, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 19.000, mean reward: 0.679 [0.594, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.450, 10.100], loss: 0.001381, mae: 0.040297, mean_q: 1.240317
 81276/100000: episode: 1447, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 19.734, mean reward: 0.705 [0.635, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.288, 10.100], loss: 0.001213, mae: 0.038300, mean_q: 1.254764
[Info] 2-TH LEVEL FOUND: 1.5424937009811401, Considering 10/90 traces
 81322/100000: episode: 1448, duration: 4.592s, episode steps: 46, steps per second: 10, episode reward: 29.966, mean reward: 0.651 [0.564, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.295, 10.332], loss: 0.001604, mae: 0.042070, mean_q: 1.253009
 81339/100000: episode: 1449, duration: 0.115s, episode steps: 17, steps per second: 147, episode reward: 12.327, mean reward: 0.725 [0.616, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.433], loss: 0.001406, mae: 0.040527, mean_q: 1.267769
 81364/100000: episode: 1450, duration: 0.122s, episode steps: 25, steps per second: 206, episode reward: 19.304, mean reward: 0.772 [0.677, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.104, 10.450], loss: 0.001372, mae: 0.038933, mean_q: 1.251893
 81402/100000: episode: 1451, duration: 0.218s, episode steps: 38, steps per second: 175, episode reward: 27.157, mean reward: 0.715 [0.601, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.642, 10.502], loss: 0.001538, mae: 0.042584, mean_q: 1.254625
 81419/100000: episode: 1452, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 12.690, mean reward: 0.746 [0.671, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.523], loss: 0.001706, mae: 0.045091, mean_q: 1.260770
 81436/100000: episode: 1453, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 12.581, mean reward: 0.740 [0.658, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.457], loss: 0.001747, mae: 0.046140, mean_q: 1.246028
[Info] FALSIFICATION!
 81447/100000: episode: 1454, duration: 0.345s, episode steps: 11, steps per second: 32, episode reward: 9.654, mean reward: 0.878 [0.800, 1.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.681 [-0.686, 8.138], loss: 0.001670, mae: 0.043874, mean_q: 1.260008
 81477/100000: episode: 1455, duration: 0.216s, episode steps: 30, steps per second: 139, episode reward: 23.116, mean reward: 0.771 [0.699, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.590, 10.487], loss: 0.001780, mae: 0.045682, mean_q: 1.258234
 81502/100000: episode: 1456, duration: 0.157s, episode steps: 25, steps per second: 159, episode reward: 18.089, mean reward: 0.724 [0.651, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.180, 10.335], loss: 0.001698, mae: 0.043196, mean_q: 1.251984
 81527/100000: episode: 1457, duration: 0.181s, episode steps: 25, steps per second: 138, episode reward: 18.118, mean reward: 0.725 [0.651, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.242, 10.528], loss: 0.001478, mae: 0.040744, mean_q: 1.251948
 81554/100000: episode: 1458, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 18.622, mean reward: 0.690 [0.586, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.238, 10.314], loss: 0.001447, mae: 0.040533, mean_q: 1.263873
 81571/100000: episode: 1459, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 13.621, mean reward: 0.801 [0.728, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.290, 10.627], loss: 0.001693, mae: 0.042721, mean_q: 1.266558
 81601/100000: episode: 1460, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 22.801, mean reward: 0.760 [0.685, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.369, 10.605], loss: 0.001608, mae: 0.041991, mean_q: 1.260668
 81627/100000: episode: 1461, duration: 0.161s, episode steps: 26, steps per second: 161, episode reward: 21.001, mean reward: 0.808 [0.763, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.185, 10.558], loss: 0.001472, mae: 0.041209, mean_q: 1.266631
 81653/100000: episode: 1462, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 18.390, mean reward: 0.707 [0.617, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.079, 10.365], loss: 0.001570, mae: 0.039345, mean_q: 1.272209
 81679/100000: episode: 1463, duration: 0.170s, episode steps: 26, steps per second: 153, episode reward: 18.530, mean reward: 0.713 [0.653, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.462, 10.437], loss: 0.001251, mae: 0.039091, mean_q: 1.266677
 81719/100000: episode: 1464, duration: 0.246s, episode steps: 40, steps per second: 162, episode reward: 28.723, mean reward: 0.718 [0.647, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.163, 10.479], loss: 0.001233, mae: 0.038633, mean_q: 1.270692
 81736/100000: episode: 1465, duration: 0.150s, episode steps: 17, steps per second: 114, episode reward: 13.019, mean reward: 0.766 [0.703, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.566, 10.481], loss: 0.001477, mae: 0.041710, mean_q: 1.276377
 81753/100000: episode: 1466, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 13.070, mean reward: 0.769 [0.702, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.223, 10.538], loss: 0.001093, mae: 0.036483, mean_q: 1.275584
 81780/100000: episode: 1467, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 19.159, mean reward: 0.710 [0.604, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.277, 10.378], loss: 0.001383, mae: 0.040579, mean_q: 1.267867
 81820/100000: episode: 1468, duration: 0.266s, episode steps: 40, steps per second: 151, episode reward: 28.323, mean reward: 0.708 [0.596, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.155, 10.396], loss: 0.001405, mae: 0.040994, mean_q: 1.273857
 81845/100000: episode: 1469, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 20.947, mean reward: 0.838 [0.766, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.516, 10.541], loss: 0.001180, mae: 0.035679, mean_q: 1.281387
 81873/100000: episode: 1470, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 20.277, mean reward: 0.724 [0.603, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.353, 10.321], loss: 0.001273, mae: 0.038809, mean_q: 1.282865
 81901/100000: episode: 1471, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 18.020, mean reward: 0.644 [0.537, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.318], loss: 0.002178, mae: 0.045942, mean_q: 1.285402
 81944/100000: episode: 1472, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 29.809, mean reward: 0.693 [0.508, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.280, 10.200], loss: 0.001668, mae: 0.042566, mean_q: 1.283539
 81970/100000: episode: 1473, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 19.591, mean reward: 0.753 [0.636, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.099, 10.447], loss: 0.001217, mae: 0.038132, mean_q: 1.297132
 81997/100000: episode: 1474, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 17.779, mean reward: 0.658 [0.583, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.260, 10.291], loss: 0.001712, mae: 0.042065, mean_q: 1.291488
 82035/100000: episode: 1475, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 29.535, mean reward: 0.777 [0.694, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.717, 10.475], loss: 0.001451, mae: 0.040456, mean_q: 1.282002
 82061/100000: episode: 1476, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 18.277, mean reward: 0.703 [0.583, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.244, 10.317], loss: 0.001430, mae: 0.039382, mean_q: 1.289411
 82101/100000: episode: 1477, duration: 0.226s, episode steps: 40, steps per second: 177, episode reward: 28.279, mean reward: 0.707 [0.629, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.912, 10.476], loss: 0.001531, mae: 0.040614, mean_q: 1.285207
 82117/100000: episode: 1478, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 11.708, mean reward: 0.732 [0.630, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.768, 10.370], loss: 0.001288, mae: 0.037063, mean_q: 1.270894
[Info] FALSIFICATION!
 82131/100000: episode: 1479, duration: 0.398s, episode steps: 14, steps per second: 35, episode reward: 12.371, mean reward: 0.884 [0.817, 1.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-1.347, 10.578], loss: 0.001716, mae: 0.041689, mean_q: 1.295209
 82171/100000: episode: 1480, duration: 0.210s, episode steps: 40, steps per second: 191, episode reward: 26.203, mean reward: 0.655 [0.529, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.171, 10.100], loss: 0.001192, mae: 0.038202, mean_q: 1.289829
 82196/100000: episode: 1481, duration: 0.148s, episode steps: 25, steps per second: 168, episode reward: 17.610, mean reward: 0.704 [0.597, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.285], loss: 0.001604, mae: 0.039595, mean_q: 1.290371
 82222/100000: episode: 1482, duration: 0.161s, episode steps: 26, steps per second: 161, episode reward: 18.909, mean reward: 0.727 [0.609, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.133, 10.485], loss: 0.001431, mae: 0.041752, mean_q: 1.299060
 82248/100000: episode: 1483, duration: 0.165s, episode steps: 26, steps per second: 157, episode reward: 20.313, mean reward: 0.781 [0.734, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.405], loss: 0.001217, mae: 0.038465, mean_q: 1.292725
 82278/100000: episode: 1484, duration: 0.209s, episode steps: 30, steps per second: 144, episode reward: 23.127, mean reward: 0.771 [0.625, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.449], loss: 0.001556, mae: 0.042318, mean_q: 1.301152
 82305/100000: episode: 1485, duration: 0.216s, episode steps: 27, steps per second: 125, episode reward: 19.207, mean reward: 0.711 [0.612, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.016, 10.290], loss: 0.001512, mae: 0.042236, mean_q: 1.300389
 82343/100000: episode: 1486, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 25.621, mean reward: 0.674 [0.553, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.641, 10.245], loss: 0.001608, mae: 0.042583, mean_q: 1.295128
 82368/100000: episode: 1487, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 19.314, mean reward: 0.773 [0.677, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.550, 10.515], loss: 0.001364, mae: 0.038513, mean_q: 1.309180
 82396/100000: episode: 1488, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 19.183, mean reward: 0.685 [0.601, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.841, 10.323], loss: 0.001267, mae: 0.039677, mean_q: 1.306215
 82434/100000: episode: 1489, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 28.256, mean reward: 0.744 [0.665, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.157, 10.431], loss: 0.001225, mae: 0.038833, mean_q: 1.311838
 82472/100000: episode: 1490, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 25.876, mean reward: 0.681 [0.568, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-1.353, 10.210], loss: 0.001427, mae: 0.040192, mean_q: 1.306849
 82510/100000: episode: 1491, duration: 0.218s, episode steps: 38, steps per second: 174, episode reward: 27.367, mean reward: 0.720 [0.643, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.940, 10.363], loss: 0.001261, mae: 0.038419, mean_q: 1.297491
 82537/100000: episode: 1492, duration: 0.164s, episode steps: 27, steps per second: 165, episode reward: 20.239, mean reward: 0.750 [0.674, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.599], loss: 0.001311, mae: 0.039565, mean_q: 1.305482
 82563/100000: episode: 1493, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 18.295, mean reward: 0.704 [0.626, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.581], loss: 0.001445, mae: 0.040071, mean_q: 1.309631
 82588/100000: episode: 1494, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 18.517, mean reward: 0.741 [0.664, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.421, 10.502], loss: 0.001668, mae: 0.041740, mean_q: 1.312790
 82626/100000: episode: 1495, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 25.081, mean reward: 0.660 [0.536, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-1.311, 10.175], loss: 0.001457, mae: 0.040778, mean_q: 1.312732
[Info] FALSIFICATION!
 82651/100000: episode: 1496, duration: 0.299s, episode steps: 25, steps per second: 84, episode reward: 21.032, mean reward: 0.841 [0.723, 1.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.220, 10.828], loss: 0.001405, mae: 0.040504, mean_q: 1.307375
 82667/100000: episode: 1497, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 11.882, mean reward: 0.743 [0.663, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.456], loss: 0.001435, mae: 0.041441, mean_q: 1.314269
 82683/100000: episode: 1498, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 13.233, mean reward: 0.827 [0.767, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.557], loss: 0.001689, mae: 0.042914, mean_q: 1.310759
 82721/100000: episode: 1499, duration: 0.207s, episode steps: 38, steps per second: 184, episode reward: 26.052, mean reward: 0.686 [0.598, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.332, 10.307], loss: 0.001239, mae: 0.038446, mean_q: 1.313622
 82749/100000: episode: 1500, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 17.839, mean reward: 0.637 [0.497, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.096, 10.103], loss: 0.001316, mae: 0.037985, mean_q: 1.315345
 82766/100000: episode: 1501, duration: 0.117s, episode steps: 17, steps per second: 145, episode reward: 12.551, mean reward: 0.738 [0.696, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.547], loss: 0.001448, mae: 0.041250, mean_q: 1.302698
 82791/100000: episode: 1502, duration: 0.159s, episode steps: 25, steps per second: 158, episode reward: 18.240, mean reward: 0.730 [0.628, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.582, 10.357], loss: 0.001326, mae: 0.040721, mean_q: 1.321689
 82831/100000: episode: 1503, duration: 0.222s, episode steps: 40, steps per second: 180, episode reward: 25.980, mean reward: 0.650 [0.509, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.945, 10.185], loss: 0.001561, mae: 0.040831, mean_q: 1.321169
 82848/100000: episode: 1504, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 12.426, mean reward: 0.731 [0.680, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.443], loss: 0.001229, mae: 0.038344, mean_q: 1.322488
 82888/100000: episode: 1505, duration: 0.219s, episode steps: 40, steps per second: 183, episode reward: 29.405, mean reward: 0.735 [0.557, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.066, 10.203], loss: 0.001369, mae: 0.039073, mean_q: 1.320411
 82931/100000: episode: 1506, duration: 0.226s, episode steps: 43, steps per second: 190, episode reward: 33.249, mean reward: 0.773 [0.620, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.291, 10.419], loss: 0.001412, mae: 0.039344, mean_q: 1.325108
 82971/100000: episode: 1507, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 29.765, mean reward: 0.744 [0.666, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.470, 10.529], loss: 0.001444, mae: 0.039499, mean_q: 1.325386
 82987/100000: episode: 1508, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 11.558, mean reward: 0.722 [0.664, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.424], loss: 0.001110, mae: 0.036878, mean_q: 1.305527
 83012/100000: episode: 1509, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 18.773, mean reward: 0.751 [0.674, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.472], loss: 0.001150, mae: 0.036759, mean_q: 1.326521
 83038/100000: episode: 1510, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 18.344, mean reward: 0.706 [0.584, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.447, 10.311], loss: 0.001772, mae: 0.043195, mean_q: 1.335993
 83066/100000: episode: 1511, duration: 0.163s, episode steps: 28, steps per second: 171, episode reward: 20.172, mean reward: 0.720 [0.658, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.407], loss: 0.001197, mae: 0.038174, mean_q: 1.319785
 83096/100000: episode: 1512, duration: 0.187s, episode steps: 30, steps per second: 161, episode reward: 23.501, mean reward: 0.783 [0.732, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.835, 10.529], loss: 0.001264, mae: 0.038207, mean_q: 1.308040
 83122/100000: episode: 1513, duration: 0.171s, episode steps: 26, steps per second: 152, episode reward: 19.040, mean reward: 0.732 [0.643, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.043, 10.426], loss: 0.001266, mae: 0.039599, mean_q: 1.322589
 83148/100000: episode: 1514, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 19.262, mean reward: 0.741 [0.684, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.242, 10.581], loss: 0.001334, mae: 0.040124, mean_q: 1.341779
 83176/100000: episode: 1515, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 18.521, mean reward: 0.661 [0.535, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.530, 10.170], loss: 0.001341, mae: 0.040363, mean_q: 1.320700
 83204/100000: episode: 1516, duration: 0.189s, episode steps: 28, steps per second: 148, episode reward: 19.743, mean reward: 0.705 [0.605, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.415], loss: 0.001211, mae: 0.038944, mean_q: 1.336217
 83232/100000: episode: 1517, duration: 0.170s, episode steps: 28, steps per second: 165, episode reward: 17.971, mean reward: 0.642 [0.537, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.188], loss: 0.001376, mae: 0.039200, mean_q: 1.330887
 83272/100000: episode: 1518, duration: 0.212s, episode steps: 40, steps per second: 188, episode reward: 28.218, mean reward: 0.705 [0.553, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.158, 10.237], loss: 0.001278, mae: 0.038703, mean_q: 1.325013
 83300/100000: episode: 1519, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 21.009, mean reward: 0.750 [0.679, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.414, 10.519], loss: 0.001382, mae: 0.040077, mean_q: 1.338543
 83338/100000: episode: 1520, duration: 0.222s, episode steps: 38, steps per second: 171, episode reward: 28.888, mean reward: 0.760 [0.648, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.484, 10.385], loss: 0.001591, mae: 0.041325, mean_q: 1.334899
 83354/100000: episode: 1521, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 12.286, mean reward: 0.768 [0.714, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.498, 10.470], loss: 0.001239, mae: 0.037643, mean_q: 1.330219
 83394/100000: episode: 1522, duration: 0.246s, episode steps: 40, steps per second: 162, episode reward: 27.218, mean reward: 0.680 [0.575, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.614, 10.312], loss: 0.001601, mae: 0.041055, mean_q: 1.330545
 83424/100000: episode: 1523, duration: 0.194s, episode steps: 30, steps per second: 154, episode reward: 22.622, mean reward: 0.754 [0.663, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.181, 10.466], loss: 0.001593, mae: 0.040816, mean_q: 1.333211
 83452/100000: episode: 1524, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 18.992, mean reward: 0.678 [0.588, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.577, 10.275], loss: 0.001258, mae: 0.039560, mean_q: 1.327284
 83495/100000: episode: 1525, duration: 0.283s, episode steps: 43, steps per second: 152, episode reward: 34.624, mean reward: 0.805 [0.676, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.299, 10.482], loss: 0.001297, mae: 0.037883, mean_q: 1.334353
 83533/100000: episode: 1526, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 28.933, mean reward: 0.761 [0.662, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.375, 10.580], loss: 0.001439, mae: 0.041263, mean_q: 1.333540
 83549/100000: episode: 1527, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 12.207, mean reward: 0.763 [0.658, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.929, 10.414], loss: 0.001547, mae: 0.042653, mean_q: 1.335875
 83577/100000: episode: 1528, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 21.060, mean reward: 0.752 [0.681, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.594, 10.450], loss: 0.001546, mae: 0.038663, mean_q: 1.333331
 83603/100000: episode: 1529, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 18.484, mean reward: 0.711 [0.613, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.308], loss: 0.001516, mae: 0.041919, mean_q: 1.344540
 83629/100000: episode: 1530, duration: 0.184s, episode steps: 26, steps per second: 141, episode reward: 19.764, mean reward: 0.760 [0.693, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.171, 10.544], loss: 0.001195, mae: 0.038554, mean_q: 1.334828
 83654/100000: episode: 1531, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 20.139, mean reward: 0.806 [0.723, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.439, 10.547], loss: 0.001434, mae: 0.039215, mean_q: 1.348047
 83684/100000: episode: 1532, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 22.402, mean reward: 0.747 [0.627, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.396], loss: 0.001202, mae: 0.037062, mean_q: 1.343680
 83701/100000: episode: 1533, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 13.286, mean reward: 0.782 [0.740, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.294, 10.517], loss: 0.001091, mae: 0.036542, mean_q: 1.340145
 83718/100000: episode: 1534, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 13.226, mean reward: 0.778 [0.737, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.663, 10.461], loss: 0.001316, mae: 0.039534, mean_q: 1.348157
 83761/100000: episode: 1535, duration: 0.294s, episode steps: 43, steps per second: 146, episode reward: 33.294, mean reward: 0.774 [0.666, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.292, 10.399], loss: 0.001194, mae: 0.038150, mean_q: 1.342498
 83777/100000: episode: 1536, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 11.432, mean reward: 0.715 [0.620, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.937, 10.428], loss: 0.001265, mae: 0.039976, mean_q: 1.349357
 83807/100000: episode: 1537, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 20.504, mean reward: 0.683 [0.540, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.035, 10.212], loss: 0.001402, mae: 0.038818, mean_q: 1.341415
[Info] Complete ISplit Iteration
[Info] Levels: [1.3575166, 1.5424937, 1.6360751]
[Info] Cond. Prob: [0.1, 0.1, 0.26]
[Info] Error Prob: 0.0026000000000000007

 83837/100000: episode: 1538, duration: 4.440s, episode steps: 30, steps per second: 7, episode reward: 22.683, mean reward: 0.756 [0.678, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.404], loss: 0.001258, mae: 0.039692, mean_q: 1.354046
 83937/100000: episode: 1539, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 60.474, mean reward: 0.605 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.579, 10.137], loss: 0.001228, mae: 0.038643, mean_q: 1.357235
 84037/100000: episode: 1540, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.936, mean reward: 0.609 [0.509, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.692, 10.364], loss: 0.001416, mae: 0.039396, mean_q: 1.345109
 84137/100000: episode: 1541, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.732, mean reward: 0.597 [0.499, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.009, 10.254], loss: 0.001237, mae: 0.038584, mean_q: 1.344595
 84237/100000: episode: 1542, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.561, mean reward: 0.586 [0.502, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.514, 10.098], loss: 0.001252, mae: 0.038308, mean_q: 1.345022
 84337/100000: episode: 1543, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 62.676, mean reward: 0.627 [0.507, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.470, 10.532], loss: 0.001422, mae: 0.039954, mean_q: 1.343127
 84437/100000: episode: 1544, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.823, mean reward: 0.588 [0.512, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.170, 10.098], loss: 0.001369, mae: 0.039639, mean_q: 1.334127
 84537/100000: episode: 1545, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.517, mean reward: 0.585 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.380, 10.098], loss: 0.001357, mae: 0.039736, mean_q: 1.326689
 84637/100000: episode: 1546, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 60.488, mean reward: 0.605 [0.511, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.778, 10.150], loss: 0.001565, mae: 0.042632, mean_q: 1.332426
 84737/100000: episode: 1547, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.702, mean reward: 0.597 [0.512, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.557, 10.098], loss: 0.001406, mae: 0.040961, mean_q: 1.333642
 84837/100000: episode: 1548, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.181, mean reward: 0.592 [0.515, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.644, 10.329], loss: 0.001435, mae: 0.040111, mean_q: 1.331382
 84937/100000: episode: 1549, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 62.274, mean reward: 0.623 [0.517, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.780, 10.273], loss: 0.001538, mae: 0.041245, mean_q: 1.325155
 85037/100000: episode: 1550, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.709, mean reward: 0.597 [0.512, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.278, 10.255], loss: 0.001551, mae: 0.042278, mean_q: 1.326570
 85137/100000: episode: 1551, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.890, mean reward: 0.579 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.380, 10.098], loss: 0.001433, mae: 0.040233, mean_q: 1.321482
 85237/100000: episode: 1552, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 61.159, mean reward: 0.612 [0.511, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.846, 10.098], loss: 0.001670, mae: 0.042869, mean_q: 1.321164
 85337/100000: episode: 1553, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.756, mean reward: 0.588 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.697, 10.098], loss: 0.001432, mae: 0.040719, mean_q: 1.314934
 85437/100000: episode: 1554, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.769, mean reward: 0.578 [0.513, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.657, 10.146], loss: 0.001386, mae: 0.040113, mean_q: 1.319566
 85537/100000: episode: 1555, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.432, mean reward: 0.584 [0.509, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.306, 10.179], loss: 0.001568, mae: 0.041682, mean_q: 1.314800
 85637/100000: episode: 1556, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.042, mean reward: 0.580 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.552, 10.098], loss: 0.001453, mae: 0.040389, mean_q: 1.313616
 85737/100000: episode: 1557, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.018, mean reward: 0.590 [0.515, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.426, 10.098], loss: 0.001600, mae: 0.041667, mean_q: 1.310443
 85837/100000: episode: 1558, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 61.623, mean reward: 0.616 [0.505, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.020, 10.098], loss: 0.001790, mae: 0.043827, mean_q: 1.302744
 85937/100000: episode: 1559, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.013, mean reward: 0.590 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.758, 10.157], loss: 0.001622, mae: 0.042098, mean_q: 1.305572
 86037/100000: episode: 1560, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.969, mean reward: 0.600 [0.498, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.698, 10.202], loss: 0.001485, mae: 0.040991, mean_q: 1.305170
 86137/100000: episode: 1561, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.885, mean reward: 0.589 [0.510, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.380, 10.098], loss: 0.001611, mae: 0.041814, mean_q: 1.302821
 86237/100000: episode: 1562, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.467, mean reward: 0.575 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.643, 10.098], loss: 0.001617, mae: 0.042199, mean_q: 1.298769
 86337/100000: episode: 1563, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.234, mean reward: 0.582 [0.508, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.351, 10.098], loss: 0.001521, mae: 0.041650, mean_q: 1.289402
 86437/100000: episode: 1564, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.590, mean reward: 0.576 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.321, 10.219], loss: 0.001517, mae: 0.041350, mean_q: 1.281502
 86537/100000: episode: 1565, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 58.704, mean reward: 0.587 [0.510, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.466, 10.128], loss: 0.001566, mae: 0.041366, mean_q: 1.274182
 86637/100000: episode: 1566, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.176, mean reward: 0.582 [0.503, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.995, 10.098], loss: 0.001641, mae: 0.042888, mean_q: 1.274362
 86737/100000: episode: 1567, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.659, mean reward: 0.577 [0.504, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.026, 10.251], loss: 0.001609, mae: 0.041840, mean_q: 1.269804
 86837/100000: episode: 1568, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.995, mean reward: 0.590 [0.499, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.671, 10.384], loss: 0.001402, mae: 0.040467, mean_q: 1.268438
 86937/100000: episode: 1569, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.944, mean reward: 0.599 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.293, 10.098], loss: 0.001465, mae: 0.040445, mean_q: 1.259332
 87037/100000: episode: 1570, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.982, mean reward: 0.590 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.327, 10.184], loss: 0.001594, mae: 0.042759, mean_q: 1.262435
 87137/100000: episode: 1571, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 58.587, mean reward: 0.586 [0.514, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.470, 10.123], loss: 0.001573, mae: 0.042066, mean_q: 1.252443
 87237/100000: episode: 1572, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 57.203, mean reward: 0.572 [0.500, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.974, 10.162], loss: 0.001575, mae: 0.042310, mean_q: 1.250813
 87337/100000: episode: 1573, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.142, mean reward: 0.591 [0.499, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.176, 10.189], loss: 0.001753, mae: 0.043894, mean_q: 1.241958
 87437/100000: episode: 1574, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 57.902, mean reward: 0.579 [0.503, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.696, 10.294], loss: 0.001675, mae: 0.042517, mean_q: 1.239088
 87537/100000: episode: 1575, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.619, mean reward: 0.586 [0.503, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.750, 10.134], loss: 0.001553, mae: 0.041730, mean_q: 1.233016
 87637/100000: episode: 1576, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.877, mean reward: 0.579 [0.511, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.007, 10.098], loss: 0.001520, mae: 0.041346, mean_q: 1.229533
 87737/100000: episode: 1577, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 56.620, mean reward: 0.566 [0.503, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.882, 10.199], loss: 0.001435, mae: 0.040737, mean_q: 1.221369
 87837/100000: episode: 1578, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.896, mean reward: 0.589 [0.506, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.024, 10.098], loss: 0.001501, mae: 0.041888, mean_q: 1.220615
 87937/100000: episode: 1579, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 60.697, mean reward: 0.607 [0.502, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.644, 10.508], loss: 0.001505, mae: 0.041345, mean_q: 1.213681
 88037/100000: episode: 1580, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.656, mean reward: 0.607 [0.511, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.690, 10.312], loss: 0.001490, mae: 0.040906, mean_q: 1.212062
 88137/100000: episode: 1581, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.942, mean reward: 0.589 [0.503, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.627, 10.176], loss: 0.001536, mae: 0.041150, mean_q: 1.205432
 88237/100000: episode: 1582, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.528, mean reward: 0.575 [0.502, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.481, 10.294], loss: 0.001451, mae: 0.040085, mean_q: 1.195352
 88337/100000: episode: 1583, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 59.690, mean reward: 0.597 [0.503, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.277, 10.200], loss: 0.001435, mae: 0.040543, mean_q: 1.195046
 88437/100000: episode: 1584, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.216, mean reward: 0.582 [0.500, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.219, 10.350], loss: 0.001470, mae: 0.041879, mean_q: 1.189482
 88537/100000: episode: 1585, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.546, mean reward: 0.585 [0.515, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.062, 10.111], loss: 0.001434, mae: 0.041082, mean_q: 1.186896
 88637/100000: episode: 1586, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 56.941, mean reward: 0.569 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.641, 10.098], loss: 0.001474, mae: 0.041683, mean_q: 1.181417
 88737/100000: episode: 1587, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 60.623, mean reward: 0.606 [0.500, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.133, 10.098], loss: 0.001394, mae: 0.040475, mean_q: 1.172615
 88837/100000: episode: 1588, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.548, mean reward: 0.585 [0.500, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.459, 10.098], loss: 0.001359, mae: 0.039773, mean_q: 1.164738
 88937/100000: episode: 1589, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.915, mean reward: 0.599 [0.520, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.227, 10.179], loss: 0.001410, mae: 0.040491, mean_q: 1.165777
 89037/100000: episode: 1590, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 56.992, mean reward: 0.570 [0.502, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.138, 10.098], loss: 0.001389, mae: 0.040116, mean_q: 1.168839
 89137/100000: episode: 1591, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.966, mean reward: 0.590 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.201, 10.098], loss: 0.001462, mae: 0.041329, mean_q: 1.170484
 89237/100000: episode: 1592, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.737, mean reward: 0.577 [0.504, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.477, 10.339], loss: 0.001417, mae: 0.040557, mean_q: 1.167446
 89337/100000: episode: 1593, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.841, mean reward: 0.608 [0.516, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.596, 10.227], loss: 0.001379, mae: 0.040072, mean_q: 1.166637
 89437/100000: episode: 1594, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 60.491, mean reward: 0.605 [0.498, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.104, 10.098], loss: 0.001378, mae: 0.040067, mean_q: 1.169449
 89537/100000: episode: 1595, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.408, mean reward: 0.584 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.617, 10.098], loss: 0.001233, mae: 0.038535, mean_q: 1.169533
 89637/100000: episode: 1596, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 59.239, mean reward: 0.592 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.202, 10.427], loss: 0.001307, mae: 0.039060, mean_q: 1.166919
 89737/100000: episode: 1597, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 56.784, mean reward: 0.568 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.351, 10.225], loss: 0.001224, mae: 0.038281, mean_q: 1.163661
 89837/100000: episode: 1598, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 61.850, mean reward: 0.618 [0.514, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.870, 10.371], loss: 0.001266, mae: 0.038708, mean_q: 1.166310
 89937/100000: episode: 1599, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.937, mean reward: 0.589 [0.504, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.509, 10.098], loss: 0.001317, mae: 0.039128, mean_q: 1.165745
 90037/100000: episode: 1600, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.826, mean reward: 0.588 [0.500, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.720, 10.098], loss: 0.001275, mae: 0.038740, mean_q: 1.162566
 90137/100000: episode: 1601, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.337, mean reward: 0.583 [0.509, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.600, 10.098], loss: 0.001366, mae: 0.039940, mean_q: 1.165797
 90237/100000: episode: 1602, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 60.300, mean reward: 0.603 [0.527, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.140, 10.098], loss: 0.001200, mae: 0.037726, mean_q: 1.160672
 90337/100000: episode: 1603, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 63.080, mean reward: 0.631 [0.517, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.306, 10.234], loss: 0.001209, mae: 0.037495, mean_q: 1.163745
 90437/100000: episode: 1604, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.798, mean reward: 0.608 [0.515, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.583, 10.131], loss: 0.001275, mae: 0.039002, mean_q: 1.166672
 90537/100000: episode: 1605, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 60.330, mean reward: 0.603 [0.511, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.822, 10.419], loss: 0.001349, mae: 0.040306, mean_q: 1.165700
 90637/100000: episode: 1606, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.737, mean reward: 0.577 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.645, 10.205], loss: 0.001322, mae: 0.039607, mean_q: 1.166148
 90737/100000: episode: 1607, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.940, mean reward: 0.579 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.072, 10.318], loss: 0.001395, mae: 0.040432, mean_q: 1.166353
 90837/100000: episode: 1608, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 58.408, mean reward: 0.584 [0.501, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.357, 10.109], loss: 0.001345, mae: 0.039711, mean_q: 1.166070
 90937/100000: episode: 1609, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 59.605, mean reward: 0.596 [0.504, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.788, 10.130], loss: 0.001396, mae: 0.040528, mean_q: 1.163350
 91037/100000: episode: 1610, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.351, mean reward: 0.574 [0.504, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.590, 10.250], loss: 0.001292, mae: 0.039150, mean_q: 1.161580
 91137/100000: episode: 1611, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.155, mean reward: 0.572 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.295, 10.149], loss: 0.001385, mae: 0.040012, mean_q: 1.163312
 91237/100000: episode: 1612, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.589, mean reward: 0.576 [0.508, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.497, 10.179], loss: 0.001335, mae: 0.039877, mean_q: 1.164231
 91337/100000: episode: 1613, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 60.107, mean reward: 0.601 [0.512, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.071, 10.118], loss: 0.001377, mae: 0.040140, mean_q: 1.164653
 91437/100000: episode: 1614, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 58.060, mean reward: 0.581 [0.514, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.613, 10.098], loss: 0.001286, mae: 0.039224, mean_q: 1.161994
 91537/100000: episode: 1615, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 61.226, mean reward: 0.612 [0.515, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.315, 10.153], loss: 0.001325, mae: 0.039642, mean_q: 1.165689
 91637/100000: episode: 1616, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.340, mean reward: 0.583 [0.503, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.752, 10.098], loss: 0.001262, mae: 0.039044, mean_q: 1.165897
 91737/100000: episode: 1617, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.253, mean reward: 0.583 [0.501, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.446, 10.098], loss: 0.001284, mae: 0.039318, mean_q: 1.163794
 91837/100000: episode: 1618, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.950, mean reward: 0.580 [0.498, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.787, 10.186], loss: 0.001258, mae: 0.039021, mean_q: 1.165881
 91937/100000: episode: 1619, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 62.859, mean reward: 0.629 [0.503, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.425, 10.098], loss: 0.001346, mae: 0.040072, mean_q: 1.165400
 92037/100000: episode: 1620, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.997, mean reward: 0.580 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.973, 10.483], loss: 0.001450, mae: 0.041158, mean_q: 1.169117
 92137/100000: episode: 1621, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.711, mean reward: 0.577 [0.504, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.616, 10.295], loss: 0.001298, mae: 0.039899, mean_q: 1.167455
 92237/100000: episode: 1622, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.153, mean reward: 0.582 [0.503, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.601, 10.208], loss: 0.001476, mae: 0.040817, mean_q: 1.167220
 92337/100000: episode: 1623, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.310, mean reward: 0.593 [0.509, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.230, 10.098], loss: 0.001363, mae: 0.040576, mean_q: 1.164813
 92437/100000: episode: 1624, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.103, mean reward: 0.601 [0.503, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.630, 10.129], loss: 0.001239, mae: 0.038650, mean_q: 1.166319
 92537/100000: episode: 1625, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.072, mean reward: 0.591 [0.517, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.179, 10.120], loss: 0.001261, mae: 0.039004, mean_q: 1.163478
 92637/100000: episode: 1626, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 56.088, mean reward: 0.561 [0.501, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.477, 10.098], loss: 0.001323, mae: 0.039266, mean_q: 1.166293
 92737/100000: episode: 1627, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 60.693, mean reward: 0.607 [0.526, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.548, 10.098], loss: 0.001429, mae: 0.040901, mean_q: 1.169200
 92837/100000: episode: 1628, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.362, mean reward: 0.574 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.261, 10.112], loss: 0.001305, mae: 0.039581, mean_q: 1.169475
 92937/100000: episode: 1629, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 58.303, mean reward: 0.583 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.565, 10.098], loss: 0.001352, mae: 0.039961, mean_q: 1.168780
 93037/100000: episode: 1630, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 61.368, mean reward: 0.614 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.919, 10.098], loss: 0.001330, mae: 0.039707, mean_q: 1.169263
 93137/100000: episode: 1631, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 59.454, mean reward: 0.595 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.980, 10.098], loss: 0.001305, mae: 0.039417, mean_q: 1.167432
 93237/100000: episode: 1632, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.236, mean reward: 0.582 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.699, 10.107], loss: 0.001292, mae: 0.039414, mean_q: 1.166780
 93337/100000: episode: 1633, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 60.594, mean reward: 0.606 [0.510, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.792, 10.098], loss: 0.001254, mae: 0.039037, mean_q: 1.167941
 93437/100000: episode: 1634, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.647, mean reward: 0.586 [0.507, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.759, 10.268], loss: 0.001198, mae: 0.037864, mean_q: 1.164648
 93537/100000: episode: 1635, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 61.633, mean reward: 0.616 [0.520, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.428, 10.311], loss: 0.001278, mae: 0.039413, mean_q: 1.166392
 93637/100000: episode: 1636, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 62.431, mean reward: 0.624 [0.502, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.354, 10.098], loss: 0.001274, mae: 0.039024, mean_q: 1.168738
 93737/100000: episode: 1637, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 60.515, mean reward: 0.605 [0.499, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.187, 10.294], loss: 0.001225, mae: 0.038758, mean_q: 1.169100
[Info] 1-TH LEVEL FOUND: 1.3818167448043823, Considering 10/90 traces
 93837/100000: episode: 1638, duration: 4.784s, episode steps: 100, steps per second: 21, episode reward: 58.893, mean reward: 0.589 [0.501, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.071, 10.105], loss: 0.001383, mae: 0.040680, mean_q: 1.172928
 93855/100000: episode: 1639, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 12.014, mean reward: 0.667 [0.577, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.362, 10.220], loss: 0.001517, mae: 0.042572, mean_q: 1.176729
 93873/100000: episode: 1640, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 11.008, mean reward: 0.612 [0.531, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.111], loss: 0.001506, mae: 0.042624, mean_q: 1.178395
 93911/100000: episode: 1641, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 23.752, mean reward: 0.625 [0.518, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.569, 10.100], loss: 0.001238, mae: 0.038475, mean_q: 1.169745
 93929/100000: episode: 1642, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 12.321, mean reward: 0.684 [0.613, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.193, 10.100], loss: 0.001419, mae: 0.040213, mean_q: 1.174973
 93947/100000: episode: 1643, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 12.203, mean reward: 0.678 [0.618, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.306, 10.274], loss: 0.001276, mae: 0.039239, mean_q: 1.167054
 93980/100000: episode: 1644, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 22.748, mean reward: 0.689 [0.616, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.220, 10.100], loss: 0.001236, mae: 0.038740, mean_q: 1.169097
 94005/100000: episode: 1645, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 16.081, mean reward: 0.643 [0.519, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.277, 10.100], loss: 0.001186, mae: 0.037908, mean_q: 1.173940
 94036/100000: episode: 1646, duration: 0.180s, episode steps: 31, steps per second: 173, episode reward: 19.549, mean reward: 0.631 [0.563, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.212, 10.100], loss: 0.001287, mae: 0.038281, mean_q: 1.175427
 94050/100000: episode: 1647, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 10.267, mean reward: 0.733 [0.676, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.329, 10.546], loss: 0.001192, mae: 0.037567, mean_q: 1.174090
 94069/100000: episode: 1648, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 13.044, mean reward: 0.687 [0.618, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.261, 10.100], loss: 0.001249, mae: 0.038747, mean_q: 1.176013
 94087/100000: episode: 1649, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 11.096, mean reward: 0.616 [0.541, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.109, 10.100], loss: 0.001276, mae: 0.039368, mean_q: 1.179097
 94105/100000: episode: 1650, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 13.077, mean reward: 0.726 [0.675, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.485], loss: 0.001311, mae: 0.039583, mean_q: 1.177742
 94130/100000: episode: 1651, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 16.466, mean reward: 0.659 [0.610, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.749, 10.100], loss: 0.001202, mae: 0.037610, mean_q: 1.177688
 94146/100000: episode: 1652, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 10.646, mean reward: 0.665 [0.619, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.250, 10.100], loss: 0.001215, mae: 0.037960, mean_q: 1.174264
 94175/100000: episode: 1653, duration: 0.151s, episode steps: 29, steps per second: 193, episode reward: 20.946, mean reward: 0.722 [0.603, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.519, 10.100], loss: 0.001368, mae: 0.039927, mean_q: 1.175878
 94194/100000: episode: 1654, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 11.124, mean reward: 0.585 [0.518, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.123, 10.122], loss: 0.001512, mae: 0.042339, mean_q: 1.181963
 94225/100000: episode: 1655, duration: 0.165s, episode steps: 31, steps per second: 187, episode reward: 20.055, mean reward: 0.647 [0.520, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.705, 10.100], loss: 0.001423, mae: 0.040677, mean_q: 1.174201
 94250/100000: episode: 1656, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 15.526, mean reward: 0.621 [0.558, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.242, 10.100], loss: 0.001564, mae: 0.041798, mean_q: 1.186246
 94268/100000: episode: 1657, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 12.058, mean reward: 0.670 [0.621, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.445], loss: 0.001406, mae: 0.040241, mean_q: 1.186034
 94293/100000: episode: 1658, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 17.374, mean reward: 0.695 [0.637, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.959, 10.100], loss: 0.001562, mae: 0.042700, mean_q: 1.181056
 94312/100000: episode: 1659, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 13.082, mean reward: 0.689 [0.639, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.706, 10.100], loss: 0.001567, mae: 0.040956, mean_q: 1.180290
 94343/100000: episode: 1660, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 19.890, mean reward: 0.642 [0.513, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.695, 10.142], loss: 0.001609, mae: 0.041644, mean_q: 1.179803
 94374/100000: episode: 1661, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 18.661, mean reward: 0.602 [0.541, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.371, 10.100], loss: 0.001487, mae: 0.041260, mean_q: 1.183335
 94403/100000: episode: 1662, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 17.317, mean reward: 0.597 [0.537, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.460, 10.100], loss: 0.001748, mae: 0.043985, mean_q: 1.180313
 94428/100000: episode: 1663, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 16.574, mean reward: 0.663 [0.604, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.412, 10.100], loss: 0.001581, mae: 0.041340, mean_q: 1.186200
 94447/100000: episode: 1664, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 12.308, mean reward: 0.648 [0.593, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.280, 10.100], loss: 0.001925, mae: 0.044713, mean_q: 1.183500
 94472/100000: episode: 1665, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 15.903, mean reward: 0.636 [0.590, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.599, 10.100], loss: 0.001513, mae: 0.040904, mean_q: 1.186886
 94503/100000: episode: 1666, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 19.201, mean reward: 0.619 [0.524, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.864, 10.100], loss: 0.001809, mae: 0.044848, mean_q: 1.178870
 94519/100000: episode: 1667, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 10.587, mean reward: 0.662 [0.585, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.248, 10.100], loss: 0.001956, mae: 0.046729, mean_q: 1.179146
 94552/100000: episode: 1668, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 21.538, mean reward: 0.653 [0.567, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.070, 10.100], loss: 0.001779, mae: 0.044707, mean_q: 1.191593
 94566/100000: episode: 1669, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 9.679, mean reward: 0.691 [0.615, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.397], loss: 0.001389, mae: 0.039813, mean_q: 1.173498
 94591/100000: episode: 1670, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 16.904, mean reward: 0.676 [0.586, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.071, 10.100], loss: 0.001485, mae: 0.041171, mean_q: 1.178277
 94609/100000: episode: 1671, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 12.295, mean reward: 0.683 [0.631, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.385, 10.100], loss: 0.001461, mae: 0.040272, mean_q: 1.175762
 94640/100000: episode: 1672, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 20.271, mean reward: 0.654 [0.576, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.389, 10.100], loss: 0.001275, mae: 0.038865, mean_q: 1.180892
 94658/100000: episode: 1673, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 12.217, mean reward: 0.679 [0.628, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.243, 10.360], loss: 0.001485, mae: 0.040187, mean_q: 1.174381
 94687/100000: episode: 1674, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 19.951, mean reward: 0.688 [0.571, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.159, 10.100], loss: 0.001519, mae: 0.041236, mean_q: 1.181676
 94720/100000: episode: 1675, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 19.756, mean reward: 0.599 [0.501, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.231], loss: 0.001720, mae: 0.042575, mean_q: 1.184733
 94738/100000: episode: 1676, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 11.673, mean reward: 0.649 [0.569, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.196, 10.100], loss: 0.001690, mae: 0.043179, mean_q: 1.191186
 94771/100000: episode: 1677, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 22.857, mean reward: 0.693 [0.555, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.415, 10.100], loss: 0.001659, mae: 0.041927, mean_q: 1.189237
 94789/100000: episode: 1678, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 11.546, mean reward: 0.641 [0.598, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.445, 10.100], loss: 0.001266, mae: 0.039829, mean_q: 1.194539
 94808/100000: episode: 1679, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 12.081, mean reward: 0.636 [0.597, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.494, 10.100], loss: 0.001291, mae: 0.038682, mean_q: 1.187053
 94846/100000: episode: 1680, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 23.683, mean reward: 0.623 [0.566, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.603, 10.326], loss: 0.001712, mae: 0.044394, mean_q: 1.189106
 94884/100000: episode: 1681, duration: 0.194s, episode steps: 38, steps per second: 195, episode reward: 25.500, mean reward: 0.671 [0.558, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.143, 10.275], loss: 0.001579, mae: 0.041778, mean_q: 1.188477
 94913/100000: episode: 1682, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 20.690, mean reward: 0.713 [0.613, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.100, 10.100], loss: 0.001657, mae: 0.042539, mean_q: 1.191366
 94929/100000: episode: 1683, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 11.100, mean reward: 0.694 [0.646, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.160, 10.100], loss: 0.001510, mae: 0.041247, mean_q: 1.174803
 94954/100000: episode: 1684, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 16.890, mean reward: 0.676 [0.608, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.118, 10.100], loss: 0.001472, mae: 0.041823, mean_q: 1.199225
 94992/100000: episode: 1685, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 24.273, mean reward: 0.639 [0.552, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.393, 10.352], loss: 0.001884, mae: 0.044460, mean_q: 1.189505
 95011/100000: episode: 1686, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 13.861, mean reward: 0.730 [0.655, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.325, 10.100], loss: 0.001694, mae: 0.044018, mean_q: 1.190338
 95029/100000: episode: 1687, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 13.117, mean reward: 0.729 [0.634, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.427], loss: 0.001752, mae: 0.042287, mean_q: 1.194457
 95058/100000: episode: 1688, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 21.278, mean reward: 0.734 [0.658, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.492, 10.100], loss: 0.001940, mae: 0.045273, mean_q: 1.195257
 95091/100000: episode: 1689, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 21.637, mean reward: 0.656 [0.585, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.129, 10.100], loss: 0.001639, mae: 0.042241, mean_q: 1.186219
 95129/100000: episode: 1690, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 24.632, mean reward: 0.648 [0.556, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.323, 10.324], loss: 0.001550, mae: 0.041703, mean_q: 1.187546
 95148/100000: episode: 1691, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 13.014, mean reward: 0.685 [0.637, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.099, 10.100], loss: 0.001620, mae: 0.040669, mean_q: 1.196507
 95166/100000: episode: 1692, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 11.118, mean reward: 0.618 [0.573, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.161, 10.100], loss: 0.001670, mae: 0.041733, mean_q: 1.196736
 95195/100000: episode: 1693, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 18.727, mean reward: 0.646 [0.599, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.256, 10.100], loss: 0.001565, mae: 0.042143, mean_q: 1.199551
 95213/100000: episode: 1694, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 11.549, mean reward: 0.642 [0.531, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.459, 10.100], loss: 0.001526, mae: 0.040759, mean_q: 1.195677
 95244/100000: episode: 1695, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 22.333, mean reward: 0.720 [0.640, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.477, 10.100], loss: 0.001643, mae: 0.043796, mean_q: 1.195464
 95262/100000: episode: 1696, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 12.756, mean reward: 0.709 [0.646, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.661, 10.100], loss: 0.001337, mae: 0.038911, mean_q: 1.201067
 95295/100000: episode: 1697, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 20.836, mean reward: 0.631 [0.524, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.842, 10.202], loss: 0.001393, mae: 0.040226, mean_q: 1.201346
 95309/100000: episode: 1698, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 9.814, mean reward: 0.701 [0.635, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.081, 10.566], loss: 0.001601, mae: 0.040563, mean_q: 1.195685
 95347/100000: episode: 1699, duration: 0.213s, episode steps: 38, steps per second: 179, episode reward: 26.116, mean reward: 0.687 [0.575, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.035, 10.539], loss: 0.001557, mae: 0.040932, mean_q: 1.193549
 95380/100000: episode: 1700, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 21.848, mean reward: 0.662 [0.575, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.743, 10.100], loss: 0.001734, mae: 0.042912, mean_q: 1.202976
 95399/100000: episode: 1701, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 12.114, mean reward: 0.638 [0.521, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.488, 10.100], loss: 0.001607, mae: 0.040524, mean_q: 1.197204
 95430/100000: episode: 1702, duration: 0.183s, episode steps: 31, steps per second: 170, episode reward: 21.052, mean reward: 0.679 [0.636, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.441, 10.100], loss: 0.001592, mae: 0.043219, mean_q: 1.205532
 95444/100000: episode: 1703, duration: 0.072s, episode steps: 14, steps per second: 196, episode reward: 9.367, mean reward: 0.669 [0.620, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.258, 10.415], loss: 0.001624, mae: 0.043444, mean_q: 1.196521
 95458/100000: episode: 1704, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 9.879, mean reward: 0.706 [0.662, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.437], loss: 0.001495, mae: 0.040244, mean_q: 1.203440
 95476/100000: episode: 1705, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 12.024, mean reward: 0.668 [0.614, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.485, 10.100], loss: 0.001197, mae: 0.038623, mean_q: 1.196579
 95505/100000: episode: 1706, duration: 0.166s, episode steps: 29, steps per second: 174, episode reward: 19.140, mean reward: 0.660 [0.608, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.369, 10.100], loss: 0.001683, mae: 0.041798, mean_q: 1.191858
 95538/100000: episode: 1707, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 22.113, mean reward: 0.670 [0.587, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.435, 10.100], loss: 0.001630, mae: 0.043237, mean_q: 1.210470
 95556/100000: episode: 1708, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 11.898, mean reward: 0.661 [0.619, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.960, 10.100], loss: 0.001940, mae: 0.047191, mean_q: 1.215784
 95574/100000: episode: 1709, duration: 0.115s, episode steps: 18, steps per second: 156, episode reward: 12.724, mean reward: 0.707 [0.605, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.327, 10.272], loss: 0.001582, mae: 0.041747, mean_q: 1.196812
 95612/100000: episode: 1710, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 28.540, mean reward: 0.751 [0.654, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.737, 10.509], loss: 0.001601, mae: 0.043173, mean_q: 1.218451
 95628/100000: episode: 1711, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 11.016, mean reward: 0.689 [0.651, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.255, 10.100], loss: 0.001315, mae: 0.037770, mean_q: 1.211894
 95647/100000: episode: 1712, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 12.656, mean reward: 0.666 [0.614, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.916, 10.100], loss: 0.001699, mae: 0.042702, mean_q: 1.214310
 95666/100000: episode: 1713, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 14.102, mean reward: 0.742 [0.655, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.064, 10.100], loss: 0.001831, mae: 0.043258, mean_q: 1.202599
 95685/100000: episode: 1714, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 13.963, mean reward: 0.735 [0.645, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.471, 10.100], loss: 0.001888, mae: 0.044512, mean_q: 1.213519
 95699/100000: episode: 1715, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 10.023, mean reward: 0.716 [0.685, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.158, 10.474], loss: 0.002105, mae: 0.045484, mean_q: 1.211821
 95718/100000: episode: 1716, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 14.276, mean reward: 0.751 [0.683, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.398, 10.100], loss: 0.001479, mae: 0.041192, mean_q: 1.209736
 95732/100000: episode: 1717, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 9.819, mean reward: 0.701 [0.644, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.446, 10.397], loss: 0.001670, mae: 0.042307, mean_q: 1.199342
 95757/100000: episode: 1718, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 16.901, mean reward: 0.676 [0.611, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.548, 10.100], loss: 0.001956, mae: 0.043951, mean_q: 1.208334
 95795/100000: episode: 1719, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 25.819, mean reward: 0.679 [0.585, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.240, 10.356], loss: 0.001383, mae: 0.039839, mean_q: 1.215099
 95820/100000: episode: 1720, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 15.419, mean reward: 0.617 [0.536, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.660, 10.100], loss: 0.001438, mae: 0.039824, mean_q: 1.212966
 95853/100000: episode: 1721, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 23.591, mean reward: 0.715 [0.658, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.349, 10.100], loss: 0.001531, mae: 0.040184, mean_q: 1.216920
 95869/100000: episode: 1722, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 10.228, mean reward: 0.639 [0.594, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.267, 10.100], loss: 0.001616, mae: 0.042237, mean_q: 1.219973
 95907/100000: episode: 1723, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 23.985, mean reward: 0.631 [0.559, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.044, 10.263], loss: 0.001621, mae: 0.041941, mean_q: 1.210885
 95926/100000: episode: 1724, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 12.201, mean reward: 0.642 [0.576, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.257, 10.100], loss: 0.001524, mae: 0.040485, mean_q: 1.212567
 95942/100000: episode: 1725, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 11.286, mean reward: 0.705 [0.635, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.677, 10.100], loss: 0.001749, mae: 0.042199, mean_q: 1.213126
 95960/100000: episode: 1726, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 12.298, mean reward: 0.683 [0.582, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.095, 10.100], loss: 0.001864, mae: 0.042827, mean_q: 1.213451
 95979/100000: episode: 1727, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 13.209, mean reward: 0.695 [0.634, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.314, 10.100], loss: 0.001586, mae: 0.040297, mean_q: 1.211653
[Info] 2-TH LEVEL FOUND: 1.483345866203308, Considering 10/90 traces
 95995/100000: episode: 1728, duration: 4.223s, episode steps: 16, steps per second: 4, episode reward: 11.367, mean reward: 0.710 [0.649, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.350, 10.100], loss: 0.001625, mae: 0.042006, mean_q: 1.214659
 96009/100000: episode: 1729, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 9.439, mean reward: 0.674 [0.593, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.613, 10.100], loss: 0.001680, mae: 0.042223, mean_q: 1.194423
 96030/100000: episode: 1730, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 15.676, mean reward: 0.746 [0.693, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.023, 10.100], loss: 0.001686, mae: 0.039919, mean_q: 1.223548
 96044/100000: episode: 1731, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 9.489, mean reward: 0.678 [0.620, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.472, 10.100], loss: 0.001506, mae: 0.041419, mean_q: 1.213568
 96081/100000: episode: 1732, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: 23.106, mean reward: 0.624 [0.510, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.575, 10.476], loss: 0.001467, mae: 0.040239, mean_q: 1.217084
 96099/100000: episode: 1733, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 12.776, mean reward: 0.710 [0.627, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.097, 10.100], loss: 0.001947, mae: 0.042665, mean_q: 1.225321
 96125/100000: episode: 1734, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 20.243, mean reward: 0.779 [0.680, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.278, 10.100], loss: 0.001376, mae: 0.040090, mean_q: 1.222444
 96162/100000: episode: 1735, duration: 0.185s, episode steps: 37, steps per second: 200, episode reward: 24.161, mean reward: 0.653 [0.586, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-1.275, 10.447], loss: 0.001985, mae: 0.044272, mean_q: 1.232360
 96176/100000: episode: 1736, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 9.608, mean reward: 0.686 [0.618, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.265, 10.100], loss: 0.001583, mae: 0.043173, mean_q: 1.228517
 96190/100000: episode: 1737, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 9.267, mean reward: 0.662 [0.624, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.152, 10.100], loss: 0.001429, mae: 0.041410, mean_q: 1.229887
 96211/100000: episode: 1738, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 16.313, mean reward: 0.777 [0.703, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.771, 10.100], loss: 0.001232, mae: 0.038217, mean_q: 1.216241
 96248/100000: episode: 1739, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 23.758, mean reward: 0.642 [0.528, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.321, 10.318], loss: 0.001455, mae: 0.040625, mean_q: 1.223417
 96285/100000: episode: 1740, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 23.852, mean reward: 0.645 [0.568, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.035, 10.419], loss: 0.001417, mae: 0.038774, mean_q: 1.224808
 96297/100000: episode: 1741, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 7.980, mean reward: 0.665 [0.598, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.416, 10.100], loss: 0.001415, mae: 0.040258, mean_q: 1.230121
 96311/100000: episode: 1742, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 10.335, mean reward: 0.738 [0.683, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.307, 10.100], loss: 0.001378, mae: 0.038263, mean_q: 1.227304
 96348/100000: episode: 1743, duration: 0.185s, episode steps: 37, steps per second: 200, episode reward: 22.270, mean reward: 0.602 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.831, 10.100], loss: 0.001409, mae: 0.038512, mean_q: 1.230746
 96385/100000: episode: 1744, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 22.196, mean reward: 0.600 [0.512, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.479, 10.120], loss: 0.001431, mae: 0.039285, mean_q: 1.231732
 96406/100000: episode: 1745, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 15.023, mean reward: 0.715 [0.634, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.213, 10.100], loss: 0.001544, mae: 0.042107, mean_q: 1.232317
 96443/100000: episode: 1746, duration: 0.189s, episode steps: 37, steps per second: 195, episode reward: 22.661, mean reward: 0.612 [0.517, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.368, 10.153], loss: 0.001598, mae: 0.040631, mean_q: 1.229729
 96461/100000: episode: 1747, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 14.146, mean reward: 0.786 [0.708, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.540, 10.100], loss: 0.001603, mae: 0.042934, mean_q: 1.222857
 96498/100000: episode: 1748, duration: 0.216s, episode steps: 37, steps per second: 171, episode reward: 25.980, mean reward: 0.702 [0.632, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.154, 10.363], loss: 0.001569, mae: 0.040813, mean_q: 1.231486
 96524/100000: episode: 1749, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 20.394, mean reward: 0.784 [0.697, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.479, 10.100], loss: 0.001686, mae: 0.041655, mean_q: 1.234197
 96561/100000: episode: 1750, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 24.111, mean reward: 0.652 [0.553, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.165, 10.218], loss: 0.001496, mae: 0.041210, mean_q: 1.237671
 96597/100000: episode: 1751, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 24.476, mean reward: 0.680 [0.601, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.838, 10.334], loss: 0.001568, mae: 0.041031, mean_q: 1.232461
 96634/100000: episode: 1752, duration: 0.184s, episode steps: 37, steps per second: 202, episode reward: 23.528, mean reward: 0.636 [0.573, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.147, 10.295], loss: 0.001344, mae: 0.040271, mean_q: 1.224083
 96655/100000: episode: 1753, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 16.344, mean reward: 0.778 [0.709, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.427, 10.100], loss: 0.001665, mae: 0.042988, mean_q: 1.238296
 96681/100000: episode: 1754, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 18.831, mean reward: 0.724 [0.608, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.282, 10.100], loss: 0.001333, mae: 0.038577, mean_q: 1.237886
 96693/100000: episode: 1755, duration: 0.073s, episode steps: 12, steps per second: 163, episode reward: 8.553, mean reward: 0.713 [0.677, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.232, 10.100], loss: 0.001611, mae: 0.039432, mean_q: 1.240416
 96707/100000: episode: 1756, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 10.225, mean reward: 0.730 [0.616, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.877, 10.100], loss: 0.001574, mae: 0.043300, mean_q: 1.226785
 96733/100000: episode: 1757, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 19.553, mean reward: 0.752 [0.685, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.205, 10.100], loss: 0.001582, mae: 0.042560, mean_q: 1.239480
 96751/100000: episode: 1758, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 15.457, mean reward: 0.859 [0.795, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.440, 10.100], loss: 0.001466, mae: 0.040512, mean_q: 1.240932
 96769/100000: episode: 1759, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 13.149, mean reward: 0.730 [0.644, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.766, 10.100], loss: 0.001217, mae: 0.038209, mean_q: 1.255472
 96806/100000: episode: 1760, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 23.214, mean reward: 0.627 [0.521, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.590, 10.105], loss: 0.001478, mae: 0.040719, mean_q: 1.241525
 96824/100000: episode: 1761, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 13.137, mean reward: 0.730 [0.658, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.539, 10.100], loss: 0.001554, mae: 0.041202, mean_q: 1.243357
 96861/100000: episode: 1762, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 24.839, mean reward: 0.671 [0.543, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.625, 10.236], loss: 0.001617, mae: 0.042948, mean_q: 1.251927
 96898/100000: episode: 1763, duration: 0.183s, episode steps: 37, steps per second: 202, episode reward: 24.931, mean reward: 0.674 [0.581, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.474, 10.287], loss: 0.001704, mae: 0.042052, mean_q: 1.239947
 96935/100000: episode: 1764, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 22.712, mean reward: 0.614 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.144, 10.115], loss: 0.001537, mae: 0.042596, mean_q: 1.240436
 96971/100000: episode: 1765, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 23.346, mean reward: 0.649 [0.539, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.229, 10.236], loss: 0.001614, mae: 0.042278, mean_q: 1.246469
 96985/100000: episode: 1766, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 10.582, mean reward: 0.756 [0.667, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.666, 10.100], loss: 0.001287, mae: 0.040166, mean_q: 1.245843
 96997/100000: episode: 1767, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 7.885, mean reward: 0.657 [0.574, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.890, 10.100], loss: 0.001397, mae: 0.039347, mean_q: 1.243614
 97009/100000: episode: 1768, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 8.648, mean reward: 0.721 [0.656, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.431, 10.100], loss: 0.001599, mae: 0.042017, mean_q: 1.251345
 97035/100000: episode: 1769, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 18.903, mean reward: 0.727 [0.592, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.190, 10.100], loss: 0.001511, mae: 0.039806, mean_q: 1.254474
 97072/100000: episode: 1770, duration: 0.185s, episode steps: 37, steps per second: 201, episode reward: 23.657, mean reward: 0.639 [0.529, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.114, 10.108], loss: 0.001386, mae: 0.040300, mean_q: 1.257982
 97108/100000: episode: 1771, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 22.838, mean reward: 0.634 [0.534, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.035, 10.329], loss: 0.001338, mae: 0.039383, mean_q: 1.259483
 97145/100000: episode: 1772, duration: 0.193s, episode steps: 37, steps per second: 191, episode reward: 22.223, mean reward: 0.601 [0.509, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.186, 10.122], loss: 0.001259, mae: 0.039095, mean_q: 1.251739
 97166/100000: episode: 1773, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 14.462, mean reward: 0.689 [0.604, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.298, 10.100], loss: 0.001585, mae: 0.039519, mean_q: 1.249678
 97203/100000: episode: 1774, duration: 0.178s, episode steps: 37, steps per second: 208, episode reward: 23.862, mean reward: 0.645 [0.526, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.427, 10.239], loss: 0.001690, mae: 0.041919, mean_q: 1.249475
 97217/100000: episode: 1775, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 9.476, mean reward: 0.677 [0.623, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.344, 10.100], loss: 0.001162, mae: 0.037430, mean_q: 1.249956
 97254/100000: episode: 1776, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 23.094, mean reward: 0.624 [0.523, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.334, 10.110], loss: 0.001401, mae: 0.040162, mean_q: 1.250369
 97291/100000: episode: 1777, duration: 0.196s, episode steps: 37, steps per second: 188, episode reward: 26.583, mean reward: 0.718 [0.626, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.704, 10.534], loss: 0.001357, mae: 0.038610, mean_q: 1.256369
 97317/100000: episode: 1778, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 19.353, mean reward: 0.744 [0.620, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.050, 10.100], loss: 0.001397, mae: 0.040460, mean_q: 1.262992
 97331/100000: episode: 1779, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 9.370, mean reward: 0.669 [0.611, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.281, 10.100], loss: 0.001628, mae: 0.041766, mean_q: 1.261247
 97357/100000: episode: 1780, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 20.683, mean reward: 0.796 [0.726, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.451, 10.100], loss: 0.001579, mae: 0.042633, mean_q: 1.258921
 97375/100000: episode: 1781, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 12.166, mean reward: 0.676 [0.578, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.147, 10.100], loss: 0.001614, mae: 0.039457, mean_q: 1.264535
 97393/100000: episode: 1782, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 12.920, mean reward: 0.718 [0.679, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.216, 10.100], loss: 0.001154, mae: 0.037306, mean_q: 1.252761
 97419/100000: episode: 1783, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 20.008, mean reward: 0.770 [0.628, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.516, 10.100], loss: 0.001450, mae: 0.041358, mean_q: 1.256510
 97456/100000: episode: 1784, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 22.513, mean reward: 0.608 [0.524, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.629, 10.188], loss: 0.001366, mae: 0.039466, mean_q: 1.255736
 97477/100000: episode: 1785, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 16.574, mean reward: 0.789 [0.697, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.479, 10.100], loss: 0.001434, mae: 0.038997, mean_q: 1.263536
 97514/100000: episode: 1786, duration: 0.196s, episode steps: 37, steps per second: 188, episode reward: 22.619, mean reward: 0.611 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.893, 10.100], loss: 0.001378, mae: 0.040008, mean_q: 1.265321
 97528/100000: episode: 1787, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 10.560, mean reward: 0.754 [0.698, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.324, 10.100], loss: 0.001718, mae: 0.043417, mean_q: 1.264326
 97546/100000: episode: 1788, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 13.200, mean reward: 0.733 [0.634, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.249, 10.100], loss: 0.001314, mae: 0.038865, mean_q: 1.282069
 97558/100000: episode: 1789, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 8.092, mean reward: 0.674 [0.625, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.290, 10.100], loss: 0.001545, mae: 0.041691, mean_q: 1.273940
 97595/100000: episode: 1790, duration: 0.194s, episode steps: 37, steps per second: 190, episode reward: 23.793, mean reward: 0.643 [0.540, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.109, 10.223], loss: 0.001516, mae: 0.040954, mean_q: 1.268125
 97632/100000: episode: 1791, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 25.824, mean reward: 0.698 [0.596, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-1.705, 10.255], loss: 0.001445, mae: 0.039800, mean_q: 1.267527
 97668/100000: episode: 1792, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 23.142, mean reward: 0.643 [0.570, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.083, 10.216], loss: 0.001621, mae: 0.043628, mean_q: 1.267885
 97705/100000: episode: 1793, duration: 0.191s, episode steps: 37, steps per second: 193, episode reward: 24.751, mean reward: 0.669 [0.613, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.328, 10.496], loss: 0.001568, mae: 0.042054, mean_q: 1.274076
 97742/100000: episode: 1794, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 22.814, mean reward: 0.617 [0.507, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.541, 10.169], loss: 0.001400, mae: 0.041283, mean_q: 1.274048
 97754/100000: episode: 1795, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 8.473, mean reward: 0.706 [0.638, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.526, 10.100], loss: 0.001485, mae: 0.042530, mean_q: 1.266295
 97780/100000: episode: 1796, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 22.317, mean reward: 0.858 [0.760, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.840, 10.100], loss: 0.001223, mae: 0.039167, mean_q: 1.270622
 97816/100000: episode: 1797, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 25.322, mean reward: 0.703 [0.621, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.124, 10.483], loss: 0.001469, mae: 0.040664, mean_q: 1.276478
 97830/100000: episode: 1798, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 9.838, mean reward: 0.703 [0.640, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.808, 10.100], loss: 0.001287, mae: 0.039110, mean_q: 1.272872
 97842/100000: episode: 1799, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 8.544, mean reward: 0.712 [0.660, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.370, 10.100], loss: 0.001207, mae: 0.037206, mean_q: 1.265201
 97879/100000: episode: 1800, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 22.902, mean reward: 0.619 [0.510, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.571, 10.100], loss: 0.001400, mae: 0.041015, mean_q: 1.272463
 97900/100000: episode: 1801, duration: 0.120s, episode steps: 21, steps per second: 176, episode reward: 16.262, mean reward: 0.774 [0.726, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.783, 10.100], loss: 0.001464, mae: 0.041054, mean_q: 1.272548
 97918/100000: episode: 1802, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 13.576, mean reward: 0.754 [0.683, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.230, 10.100], loss: 0.001467, mae: 0.040297, mean_q: 1.260769
 97930/100000: episode: 1803, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 7.859, mean reward: 0.655 [0.581, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.334, 10.100], loss: 0.001395, mae: 0.039887, mean_q: 1.274265
 97966/100000: episode: 1804, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 25.647, mean reward: 0.712 [0.643, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.027, 10.343], loss: 0.001438, mae: 0.039962, mean_q: 1.276656
 97992/100000: episode: 1805, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 20.421, mean reward: 0.785 [0.716, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.076, 10.100], loss: 0.001309, mae: 0.039494, mean_q: 1.273206
 98018/100000: episode: 1806, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 18.121, mean reward: 0.697 [0.590, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.149, 10.100], loss: 0.001282, mae: 0.037868, mean_q: 1.292587
 98055/100000: episode: 1807, duration: 0.185s, episode steps: 37, steps per second: 200, episode reward: 26.254, mean reward: 0.710 [0.639, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.035, 10.509], loss: 0.001276, mae: 0.039315, mean_q: 1.291067
 98092/100000: episode: 1808, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 26.951, mean reward: 0.728 [0.651, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.036, 10.463], loss: 0.001273, mae: 0.039404, mean_q: 1.286528
 98106/100000: episode: 1809, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 9.370, mean reward: 0.669 [0.580, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.185, 10.100], loss: 0.001214, mae: 0.037771, mean_q: 1.303911
 98127/100000: episode: 1810, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 15.740, mean reward: 0.750 [0.658, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.370, 10.100], loss: 0.001415, mae: 0.040844, mean_q: 1.276398
 98164/100000: episode: 1811, duration: 0.189s, episode steps: 37, steps per second: 195, episode reward: 21.988, mean reward: 0.594 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.935, 10.203], loss: 0.001551, mae: 0.042227, mean_q: 1.288860
 98185/100000: episode: 1812, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 16.449, mean reward: 0.783 [0.701, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.316, 10.100], loss: 0.001305, mae: 0.039575, mean_q: 1.285064
 98222/100000: episode: 1813, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 24.180, mean reward: 0.654 [0.591, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.577, 10.346], loss: 0.001411, mae: 0.040717, mean_q: 1.286822
 98258/100000: episode: 1814, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 23.133, mean reward: 0.643 [0.535, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.035, 10.342], loss: 0.001405, mae: 0.040722, mean_q: 1.290854
 98284/100000: episode: 1815, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 18.513, mean reward: 0.712 [0.589, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.695, 10.100], loss: 0.001479, mae: 0.040864, mean_q: 1.285247
 98302/100000: episode: 1816, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 13.301, mean reward: 0.739 [0.675, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.043, 10.100], loss: 0.001372, mae: 0.040815, mean_q: 1.285213
 98339/100000: episode: 1817, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 25.290, mean reward: 0.684 [0.566, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.319, 10.255], loss: 0.001387, mae: 0.040677, mean_q: 1.297563
[Info] 3-TH LEVEL FOUND: 1.5918943881988525, Considering 10/90 traces
 98376/100000: episode: 1818, duration: 4.339s, episode steps: 37, steps per second: 9, episode reward: 25.956, mean reward: 0.702 [0.618, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.035, 10.410], loss: 0.001256, mae: 0.038373, mean_q: 1.287025
 98400/100000: episode: 1819, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 16.517, mean reward: 0.688 [0.633, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.600, 10.100], loss: 0.001412, mae: 0.039395, mean_q: 1.286144
 98416/100000: episode: 1820, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 12.116, mean reward: 0.757 [0.712, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.157, 10.100], loss: 0.001249, mae: 0.039711, mean_q: 1.292116
 98441/100000: episode: 1821, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 20.751, mean reward: 0.830 [0.750, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.393, 10.100], loss: 0.001368, mae: 0.041112, mean_q: 1.297089
 98464/100000: episode: 1822, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 19.108, mean reward: 0.831 [0.763, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.481, 10.100], loss: 0.001245, mae: 0.039096, mean_q: 1.292860
 98478/100000: episode: 1823, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 11.602, mean reward: 0.829 [0.758, 0.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.816, 10.100], loss: 0.001523, mae: 0.040895, mean_q: 1.288984
 98503/100000: episode: 1824, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 19.479, mean reward: 0.779 [0.686, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.323, 10.100], loss: 0.001388, mae: 0.039581, mean_q: 1.302801
 98526/100000: episode: 1825, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 19.160, mean reward: 0.833 [0.719, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.315, 10.100], loss: 0.001379, mae: 0.041240, mean_q: 1.295341
 98550/100000: episode: 1826, duration: 0.138s, episode steps: 24, steps per second: 173, episode reward: 19.562, mean reward: 0.815 [0.771, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.936, 10.100], loss: 0.001298, mae: 0.039419, mean_q: 1.297353
 98574/100000: episode: 1827, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 16.386, mean reward: 0.683 [0.613, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.453, 10.100], loss: 0.001331, mae: 0.039706, mean_q: 1.298287
 98598/100000: episode: 1828, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 18.950, mean reward: 0.790 [0.738, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.611, 10.100], loss: 0.001320, mae: 0.039950, mean_q: 1.296038
 98621/100000: episode: 1829, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 18.492, mean reward: 0.804 [0.719, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.416, 10.100], loss: 0.001269, mae: 0.039667, mean_q: 1.306523
 98645/100000: episode: 1830, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 17.919, mean reward: 0.747 [0.686, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.854, 10.100], loss: 0.001435, mae: 0.041914, mean_q: 1.306568
 98669/100000: episode: 1831, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 19.262, mean reward: 0.803 [0.698, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.492, 10.100], loss: 0.001371, mae: 0.042033, mean_q: 1.298284
 98693/100000: episode: 1832, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 16.724, mean reward: 0.697 [0.556, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.238, 10.100], loss: 0.001212, mae: 0.038466, mean_q: 1.306711
 98709/100000: episode: 1833, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 12.018, mean reward: 0.751 [0.684, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.328, 10.100], loss: 0.001338, mae: 0.040296, mean_q: 1.312444
 98732/100000: episode: 1834, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 17.721, mean reward: 0.770 [0.606, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.220, 10.100], loss: 0.001348, mae: 0.041503, mean_q: 1.310212
 98756/100000: episode: 1835, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 19.031, mean reward: 0.793 [0.705, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.823, 10.100], loss: 0.001242, mae: 0.039487, mean_q: 1.303635
 98778/100000: episode: 1836, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 16.739, mean reward: 0.761 [0.694, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.473, 10.100], loss: 0.001240, mae: 0.038987, mean_q: 1.317587
 98800/100000: episode: 1837, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 16.998, mean reward: 0.773 [0.676, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.689, 10.100], loss: 0.001208, mae: 0.039802, mean_q: 1.315546
 98814/100000: episode: 1838, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 10.497, mean reward: 0.750 [0.629, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.173, 10.100], loss: 0.001451, mae: 0.042499, mean_q: 1.309222
 98830/100000: episode: 1839, duration: 0.091s, episode steps: 16, steps per second: 177, episode reward: 13.271, mean reward: 0.829 [0.778, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.818, 10.100], loss: 0.001356, mae: 0.040606, mean_q: 1.312920
 98854/100000: episode: 1840, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 19.193, mean reward: 0.800 [0.725, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.213, 10.100], loss: 0.001166, mae: 0.037584, mean_q: 1.306231
 98878/100000: episode: 1841, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 17.683, mean reward: 0.737 [0.683, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.323, 10.100], loss: 0.001356, mae: 0.040375, mean_q: 1.314589
 98901/100000: episode: 1842, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 17.990, mean reward: 0.782 [0.718, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.318, 10.100], loss: 0.001189, mae: 0.038634, mean_q: 1.313914
 98926/100000: episode: 1843, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 20.554, mean reward: 0.822 [0.742, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.374, 10.100], loss: 0.001407, mae: 0.042147, mean_q: 1.318695
 98950/100000: episode: 1844, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 16.486, mean reward: 0.687 [0.593, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.341, 10.100], loss: 0.001288, mae: 0.039772, mean_q: 1.311375
 98975/100000: episode: 1845, duration: 0.151s, episode steps: 25, steps per second: 166, episode reward: 20.415, mean reward: 0.817 [0.765, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-2.169, 10.100], loss: 0.001544, mae: 0.042782, mean_q: 1.315132
 98989/100000: episode: 1846, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 11.247, mean reward: 0.803 [0.771, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.694, 10.100], loss: 0.001312, mae: 0.041772, mean_q: 1.322808
 99012/100000: episode: 1847, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 18.114, mean reward: 0.788 [0.656, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.457, 10.100], loss: 0.001434, mae: 0.042253, mean_q: 1.321816
 99026/100000: episode: 1848, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 10.820, mean reward: 0.773 [0.693, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.332, 10.100], loss: 0.001371, mae: 0.040934, mean_q: 1.330752
 99048/100000: episode: 1849, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 17.322, mean reward: 0.787 [0.730, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.524, 10.100], loss: 0.001404, mae: 0.041428, mean_q: 1.319601
 99072/100000: episode: 1850, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 20.988, mean reward: 0.874 [0.793, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.504, 10.100], loss: 0.001220, mae: 0.038289, mean_q: 1.330214
 99095/100000: episode: 1851, duration: 0.119s, episode steps: 23, steps per second: 194, episode reward: 17.988, mean reward: 0.782 [0.723, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.352, 10.100], loss: 0.001202, mae: 0.038405, mean_q: 1.334327
 99119/100000: episode: 1852, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 17.263, mean reward: 0.719 [0.640, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.947, 10.100], loss: 0.001441, mae: 0.042310, mean_q: 1.335065
 99141/100000: episode: 1853, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 16.481, mean reward: 0.749 [0.683, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.367, 10.100], loss: 0.001356, mae: 0.041064, mean_q: 1.322853
 99155/100000: episode: 1854, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 11.035, mean reward: 0.788 [0.701, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.295, 10.100], loss: 0.001261, mae: 0.039003, mean_q: 1.330936
 99179/100000: episode: 1855, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 18.547, mean reward: 0.773 [0.695, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.839, 10.100], loss: 0.001929, mae: 0.048921, mean_q: 1.316067
 99204/100000: episode: 1856, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 20.114, mean reward: 0.805 [0.697, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.541, 10.100], loss: 0.001529, mae: 0.043441, mean_q: 1.334508
 99228/100000: episode: 1857, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 18.944, mean reward: 0.789 [0.737, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.288, 10.100], loss: 0.001136, mae: 0.037672, mean_q: 1.335518
 99251/100000: episode: 1858, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 17.626, mean reward: 0.766 [0.701, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.573, 10.100], loss: 0.001152, mae: 0.037484, mean_q: 1.337312
 99275/100000: episode: 1859, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 18.690, mean reward: 0.779 [0.714, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.451, 10.100], loss: 0.001306, mae: 0.040333, mean_q: 1.335588
 99289/100000: episode: 1860, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 10.686, mean reward: 0.763 [0.664, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.300, 10.100], loss: 0.001278, mae: 0.040031, mean_q: 1.322115
 99311/100000: episode: 1861, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 17.278, mean reward: 0.785 [0.701, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.277, 10.100], loss: 0.001219, mae: 0.039101, mean_q: 1.338845
 99325/100000: episode: 1862, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 11.299, mean reward: 0.807 [0.735, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.238, 10.100], loss: 0.001367, mae: 0.039977, mean_q: 1.323091
 99335/100000: episode: 1863, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 7.453, mean reward: 0.745 [0.703, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.861, 10.100], loss: 0.001457, mae: 0.042454, mean_q: 1.331478
 99360/100000: episode: 1864, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 20.215, mean reward: 0.809 [0.701, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.671, 10.100], loss: 0.001285, mae: 0.039876, mean_q: 1.343307
 99383/100000: episode: 1865, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 19.873, mean reward: 0.864 [0.761, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.546, 10.100], loss: 0.001453, mae: 0.042346, mean_q: 1.337123
 99397/100000: episode: 1866, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 10.920, mean reward: 0.780 [0.635, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.349, 10.100], loss: 0.001062, mae: 0.035751, mean_q: 1.353267
 99419/100000: episode: 1867, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 16.775, mean reward: 0.762 [0.660, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.172, 10.100], loss: 0.001350, mae: 0.039618, mean_q: 1.346869
 99429/100000: episode: 1868, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 7.819, mean reward: 0.782 [0.716, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.298, 10.100], loss: 0.001270, mae: 0.037642, mean_q: 1.351684
 99454/100000: episode: 1869, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 17.800, mean reward: 0.712 [0.604, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.140, 10.100], loss: 0.001223, mae: 0.038611, mean_q: 1.347626
 99478/100000: episode: 1870, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 17.789, mean reward: 0.741 [0.618, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.134, 10.100], loss: 0.001340, mae: 0.040229, mean_q: 1.338010
 99503/100000: episode: 1871, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 19.922, mean reward: 0.797 [0.685, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.286, 10.100], loss: 0.001165, mae: 0.037865, mean_q: 1.342246
 99528/100000: episode: 1872, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 18.054, mean reward: 0.722 [0.654, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.327, 10.100], loss: 0.001380, mae: 0.041559, mean_q: 1.341468
 99542/100000: episode: 1873, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 11.066, mean reward: 0.790 [0.745, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.395, 10.100], loss: 0.001375, mae: 0.040984, mean_q: 1.349105
 99565/100000: episode: 1874, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 17.357, mean reward: 0.755 [0.679, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.396, 10.100], loss: 0.001638, mae: 0.044357, mean_q: 1.356380
 99590/100000: episode: 1875, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 19.152, mean reward: 0.766 [0.601, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.875, 10.100], loss: 0.001403, mae: 0.041562, mean_q: 1.344990
 99615/100000: episode: 1876, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 19.059, mean reward: 0.762 [0.672, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.885, 10.100], loss: 0.001316, mae: 0.040561, mean_q: 1.346961
 99629/100000: episode: 1877, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 10.929, mean reward: 0.781 [0.723, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.340, 10.100], loss: 0.001406, mae: 0.041619, mean_q: 1.362441
 99639/100000: episode: 1878, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 7.681, mean reward: 0.768 [0.693, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.431, 10.100], loss: 0.001476, mae: 0.042031, mean_q: 1.363642
 99664/100000: episode: 1879, duration: 0.122s, episode steps: 25, steps per second: 206, episode reward: 21.068, mean reward: 0.843 [0.800, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.378, 10.100], loss: 0.001543, mae: 0.043789, mean_q: 1.361128
 99674/100000: episode: 1880, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 7.635, mean reward: 0.763 [0.733, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.516, 10.100], loss: 0.001314, mae: 0.040419, mean_q: 1.342349
 99699/100000: episode: 1881, duration: 0.149s, episode steps: 25, steps per second: 167, episode reward: 19.245, mean reward: 0.770 [0.624, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.212, 10.100], loss: 0.001403, mae: 0.041557, mean_q: 1.366339
 99715/100000: episode: 1882, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 14.044, mean reward: 0.878 [0.819, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.921, 10.100], loss: 0.001339, mae: 0.041478, mean_q: 1.355583
 99740/100000: episode: 1883, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 20.455, mean reward: 0.818 [0.719, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.706, 10.100], loss: 0.001237, mae: 0.038713, mean_q: 1.341867
 99763/100000: episode: 1884, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 18.291, mean reward: 0.795 [0.723, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.344, 10.100], loss: 0.001397, mae: 0.041453, mean_q: 1.351006
 99786/100000: episode: 1885, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 17.507, mean reward: 0.761 [0.704, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.133, 10.100], loss: 0.001117, mae: 0.037618, mean_q: 1.361998
 99796/100000: episode: 1886, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 7.606, mean reward: 0.761 [0.721, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.346, 10.100], loss: 0.001287, mae: 0.039943, mean_q: 1.376805
 99819/100000: episode: 1887, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 19.491, mean reward: 0.847 [0.738, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.441, 10.100], loss: 0.001259, mae: 0.039184, mean_q: 1.370386
 99842/100000: episode: 1888, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 16.825, mean reward: 0.732 [0.649, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.222, 10.100], loss: 0.001302, mae: 0.039784, mean_q: 1.365070
 99867/100000: episode: 1889, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 20.650, mean reward: 0.826 [0.700, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.050, 10.100], loss: 0.001196, mae: 0.038629, mean_q: 1.359995
 99883/100000: episode: 1890, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 12.744, mean reward: 0.796 [0.717, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.298, 10.100], loss: 0.001121, mae: 0.037181, mean_q: 1.388922
 99906/100000: episode: 1891, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 17.964, mean reward: 0.781 [0.711, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.208, 10.100], loss: 0.001183, mae: 0.039228, mean_q: 1.352950
 99929/100000: episode: 1892, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 16.854, mean reward: 0.733 [0.664, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.316, 10.100], loss: 0.001348, mae: 0.040865, mean_q: 1.374152
 99945/100000: episode: 1893, duration: 0.078s, episode steps: 16, steps per second: 204, episode reward: 11.615, mean reward: 0.726 [0.686, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.223, 10.100], loss: 0.001203, mae: 0.038400, mean_q: 1.364843
 99968/100000: episode: 1894, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 17.742, mean reward: 0.771 [0.662, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.119, 10.100], loss: 0.001392, mae: 0.041067, mean_q: 1.368816
 99993/100000: episode: 1895, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 19.452, mean reward: 0.778 [0.625, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.426, 10.100], loss: 0.001353, mae: 0.040935, mean_q: 1.370896
done, took 592.908 seconds
[Info] End Importance Splitting. Falsification occurred 11 times.
