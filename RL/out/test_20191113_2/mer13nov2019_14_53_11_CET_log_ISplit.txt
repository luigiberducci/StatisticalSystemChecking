Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.172s, episode steps: 100, steps per second: 580, episode reward: 57.531, mean reward: 0.575 [0.507, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.050, 10.193], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.065s, episode steps: 100, steps per second: 1538, episode reward: 59.071, mean reward: 0.591 [0.508, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.201, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.071s, episode steps: 100, steps per second: 1416, episode reward: 58.797, mean reward: 0.588 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.421, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 59.941, mean reward: 0.599 [0.507, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.830, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.067s, episode steps: 100, steps per second: 1499, episode reward: 58.053, mean reward: 0.581 [0.519, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.172, 10.248], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.065s, episode steps: 100, steps per second: 1531, episode reward: 62.933, mean reward: 0.629 [0.497, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.599, 10.419], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.080s, episode steps: 100, steps per second: 1243, episode reward: 59.884, mean reward: 0.599 [0.513, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.068, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.082s, episode steps: 100, steps per second: 1215, episode reward: 57.971, mean reward: 0.580 [0.499, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.057, 10.165], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.066s, episode steps: 100, steps per second: 1520, episode reward: 61.793, mean reward: 0.618 [0.526, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.393, 10.171], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.078s, episode steps: 100, steps per second: 1282, episode reward: 60.178, mean reward: 0.602 [0.510, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.528, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.067s, episode steps: 100, steps per second: 1485, episode reward: 56.824, mean reward: 0.568 [0.499, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.656, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.076s, episode steps: 100, steps per second: 1310, episode reward: 59.274, mean reward: 0.593 [0.504, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.688, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.066s, episode steps: 100, steps per second: 1516, episode reward: 58.903, mean reward: 0.589 [0.507, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.131, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.065s, episode steps: 100, steps per second: 1529, episode reward: 58.689, mean reward: 0.587 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.807, 10.210], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.072s, episode steps: 100, steps per second: 1395, episode reward: 63.929, mean reward: 0.639 [0.509, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.107, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.066s, episode steps: 100, steps per second: 1526, episode reward: 59.330, mean reward: 0.593 [0.498, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.658, 10.325], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.081s, episode steps: 100, steps per second: 1242, episode reward: 56.978, mean reward: 0.570 [0.505, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.404, 10.230], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.079s, episode steps: 100, steps per second: 1262, episode reward: 57.415, mean reward: 0.574 [0.500, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.112, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.066s, episode steps: 100, steps per second: 1516, episode reward: 61.336, mean reward: 0.613 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.219, 10.401], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.065s, episode steps: 100, steps per second: 1536, episode reward: 58.283, mean reward: 0.583 [0.500, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.888, 10.165], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.072s, episode steps: 100, steps per second: 1386, episode reward: 58.267, mean reward: 0.583 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.364, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.066s, episode steps: 100, steps per second: 1522, episode reward: 58.469, mean reward: 0.585 [0.499, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.859, 10.229], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.066s, episode steps: 100, steps per second: 1511, episode reward: 58.518, mean reward: 0.585 [0.509, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.375, 10.178], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.066s, episode steps: 100, steps per second: 1511, episode reward: 60.566, mean reward: 0.606 [0.507, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.548, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.076s, episode steps: 100, steps per second: 1318, episode reward: 57.689, mean reward: 0.577 [0.500, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.470, 10.154], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.066s, episode steps: 100, steps per second: 1524, episode reward: 59.343, mean reward: 0.593 [0.520, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.403, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.065s, episode steps: 100, steps per second: 1528, episode reward: 59.756, mean reward: 0.598 [0.517, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.562, 10.324], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.066s, episode steps: 100, steps per second: 1522, episode reward: 58.477, mean reward: 0.585 [0.500, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.724, 10.305], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.071s, episode steps: 100, steps per second: 1408, episode reward: 58.904, mean reward: 0.589 [0.500, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.453, 10.178], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.065s, episode steps: 100, steps per second: 1529, episode reward: 61.622, mean reward: 0.616 [0.520, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.552, 10.358], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.073s, episode steps: 100, steps per second: 1361, episode reward: 56.740, mean reward: 0.567 [0.501, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.694, 10.269], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.073s, episode steps: 100, steps per second: 1377, episode reward: 58.673, mean reward: 0.587 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.750, 10.157], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.071s, episode steps: 100, steps per second: 1417, episode reward: 61.364, mean reward: 0.614 [0.504, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.845, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.067s, episode steps: 100, steps per second: 1498, episode reward: 60.306, mean reward: 0.603 [0.507, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.316, 10.310], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.067s, episode steps: 100, steps per second: 1500, episode reward: 59.265, mean reward: 0.593 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.299], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.066s, episode steps: 100, steps per second: 1511, episode reward: 58.657, mean reward: 0.587 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.592, 10.233], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.081s, episode steps: 100, steps per second: 1239, episode reward: 60.080, mean reward: 0.601 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.727, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.077s, episode steps: 100, steps per second: 1297, episode reward: 60.022, mean reward: 0.600 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.665, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.075s, episode steps: 100, steps per second: 1331, episode reward: 59.601, mean reward: 0.596 [0.508, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.734, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.065s, episode steps: 100, steps per second: 1530, episode reward: 58.975, mean reward: 0.590 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.196, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.077s, episode steps: 100, steps per second: 1296, episode reward: 61.729, mean reward: 0.617 [0.526, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.033, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.066s, episode steps: 100, steps per second: 1509, episode reward: 58.270, mean reward: 0.583 [0.511, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.554, 10.355], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.065s, episode steps: 100, steps per second: 1529, episode reward: 59.164, mean reward: 0.592 [0.499, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.179, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.065s, episode steps: 100, steps per second: 1529, episode reward: 62.761, mean reward: 0.628 [0.519, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.773, 10.557], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.079s, episode steps: 100, steps per second: 1263, episode reward: 58.238, mean reward: 0.582 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.952, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.072s, episode steps: 100, steps per second: 1388, episode reward: 60.917, mean reward: 0.609 [0.514, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.472, 10.315], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.080s, episode steps: 100, steps per second: 1258, episode reward: 56.872, mean reward: 0.569 [0.505, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.709, 10.211], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.066s, episode steps: 100, steps per second: 1524, episode reward: 62.910, mean reward: 0.629 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.561, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.079s, episode steps: 100, steps per second: 1265, episode reward: 58.414, mean reward: 0.584 [0.509, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.017, 10.153], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.072s, episode steps: 100, steps per second: 1396, episode reward: 61.563, mean reward: 0.616 [0.520, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.507, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.234s, episode steps: 100, steps per second: 81, episode reward: 56.787, mean reward: 0.568 [0.500, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.002, 10.144], loss: 0.024157, mae: 0.149494, mean_q: 0.584698
  5200/100000: episode: 52, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 58.704, mean reward: 0.587 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.891, 10.141], loss: 0.003820, mae: 0.065769, mean_q: 0.825973
  5300/100000: episode: 53, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.270, mean reward: 0.583 [0.515, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.639, 10.098], loss: 0.003467, mae: 0.060411, mean_q: 0.951495
  5400/100000: episode: 54, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 57.786, mean reward: 0.578 [0.498, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.534, 10.098], loss: 0.002788, mae: 0.054895, mean_q: 1.040164
  5500/100000: episode: 55, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.885, mean reward: 0.589 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.606, 10.098], loss: 0.002887, mae: 0.054595, mean_q: 1.093596
  5600/100000: episode: 56, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.319, mean reward: 0.583 [0.509, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.965, 10.120], loss: 0.003228, mae: 0.055690, mean_q: 1.123648
  5700/100000: episode: 57, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.530, mean reward: 0.585 [0.511, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.529, 10.198], loss: 0.002864, mae: 0.051994, mean_q: 1.144859
  5800/100000: episode: 58, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.051, mean reward: 0.591 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.825, 10.256], loss: 0.003235, mae: 0.056498, mean_q: 1.156019
  5900/100000: episode: 59, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.916, mean reward: 0.589 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.293, 10.098], loss: 0.003214, mae: 0.054922, mean_q: 1.161144
  6000/100000: episode: 60, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.569, mean reward: 0.586 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.292, 10.098], loss: 0.002975, mae: 0.053689, mean_q: 1.164169
  6100/100000: episode: 61, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.602, mean reward: 0.576 [0.502, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.887, 10.230], loss: 0.002854, mae: 0.053975, mean_q: 1.169798
  6200/100000: episode: 62, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 63.566, mean reward: 0.636 [0.505, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.695, 10.098], loss: 0.003212, mae: 0.055553, mean_q: 1.168921
  6300/100000: episode: 63, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.504, mean reward: 0.565 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.993, 10.190], loss: 0.002883, mae: 0.053963, mean_q: 1.172692
  6400/100000: episode: 64, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.258, mean reward: 0.593 [0.507, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.466, 10.172], loss: 0.002745, mae: 0.052023, mean_q: 1.174420
  6500/100000: episode: 65, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.594, mean reward: 0.586 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.812, 10.098], loss: 0.003176, mae: 0.054800, mean_q: 1.171145
  6600/100000: episode: 66, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 61.620, mean reward: 0.616 [0.498, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.184, 10.211], loss: 0.002766, mae: 0.052029, mean_q: 1.172380
  6700/100000: episode: 67, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 61.001, mean reward: 0.610 [0.508, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.628, 10.205], loss: 0.003133, mae: 0.054505, mean_q: 1.173401
  6800/100000: episode: 68, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.654, mean reward: 0.587 [0.516, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.021, 10.201], loss: 0.002535, mae: 0.050808, mean_q: 1.175917
  6900/100000: episode: 69, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.743, mean reward: 0.577 [0.508, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.460, 10.145], loss: 0.002837, mae: 0.051293, mean_q: 1.170553
  7000/100000: episode: 70, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.118, mean reward: 0.581 [0.510, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.174, 10.098], loss: 0.002460, mae: 0.049652, mean_q: 1.172668
  7100/100000: episode: 71, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.777, mean reward: 0.588 [0.506, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.522, 10.464], loss: 0.003070, mae: 0.053826, mean_q: 1.168760
  7200/100000: episode: 72, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.650, mean reward: 0.587 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.754, 10.098], loss: 0.002751, mae: 0.052150, mean_q: 1.169786
  7300/100000: episode: 73, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.080, mean reward: 0.601 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.153, 10.098], loss: 0.002790, mae: 0.052322, mean_q: 1.170233
  7400/100000: episode: 74, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.850, mean reward: 0.598 [0.507, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.953, 10.151], loss: 0.002701, mae: 0.051009, mean_q: 1.172501
  7500/100000: episode: 75, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.268, mean reward: 0.563 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.079, 10.176], loss: 0.002534, mae: 0.050454, mean_q: 1.171117
  7600/100000: episode: 76, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 59.347, mean reward: 0.593 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.389, 10.292], loss: 0.002909, mae: 0.053557, mean_q: 1.170955
  7700/100000: episode: 77, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.683, mean reward: 0.577 [0.506, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.445, 10.105], loss: 0.002513, mae: 0.049952, mean_q: 1.173121
  7800/100000: episode: 78, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 61.591, mean reward: 0.616 [0.521, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.442, 10.320], loss: 0.002790, mae: 0.052139, mean_q: 1.171884
  7900/100000: episode: 79, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.503, mean reward: 0.575 [0.510, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.507, 10.134], loss: 0.002653, mae: 0.051039, mean_q: 1.172275
  8000/100000: episode: 80, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 61.451, mean reward: 0.615 [0.516, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.023, 10.349], loss: 0.002719, mae: 0.053292, mean_q: 1.173057
  8100/100000: episode: 81, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.217, mean reward: 0.582 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.679, 10.263], loss: 0.002797, mae: 0.052764, mean_q: 1.174044
  8200/100000: episode: 82, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.318, mean reward: 0.573 [0.499, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.955, 10.098], loss: 0.002625, mae: 0.052266, mean_q: 1.172657
  8300/100000: episode: 83, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 58.110, mean reward: 0.581 [0.501, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.722, 10.282], loss: 0.002393, mae: 0.050124, mean_q: 1.173496
  8400/100000: episode: 84, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 60.045, mean reward: 0.600 [0.508, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.274, 10.102], loss: 0.002438, mae: 0.050336, mean_q: 1.174099
  8500/100000: episode: 85, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.955, mean reward: 0.580 [0.504, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.599, 10.098], loss: 0.002607, mae: 0.051640, mean_q: 1.173677
  8600/100000: episode: 86, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.586, mean reward: 0.586 [0.509, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.846, 10.098], loss: 0.002480, mae: 0.050302, mean_q: 1.172199
  8700/100000: episode: 87, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.096, mean reward: 0.591 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.903, 10.098], loss: 0.002358, mae: 0.049713, mean_q: 1.169766
  8800/100000: episode: 88, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.884, mean reward: 0.579 [0.509, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.249, 10.280], loss: 0.002199, mae: 0.048587, mean_q: 1.169641
  8900/100000: episode: 89, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 56.551, mean reward: 0.566 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.269, 10.098], loss: 0.002508, mae: 0.051349, mean_q: 1.169798
  9000/100000: episode: 90, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.919, mean reward: 0.589 [0.502, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.858, 10.170], loss: 0.002241, mae: 0.049083, mean_q: 1.171658
  9100/100000: episode: 91, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 63.449, mean reward: 0.634 [0.518, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.365, 10.286], loss: 0.002316, mae: 0.049442, mean_q: 1.167216
  9200/100000: episode: 92, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.764, mean reward: 0.588 [0.498, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.408, 10.158], loss: 0.002540, mae: 0.052149, mean_q: 1.169733
  9300/100000: episode: 93, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 56.798, mean reward: 0.568 [0.498, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.595, 10.285], loss: 0.002120, mae: 0.047589, mean_q: 1.171152
  9400/100000: episode: 94, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 61.046, mean reward: 0.610 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.486, 10.098], loss: 0.002433, mae: 0.051422, mean_q: 1.168748
  9500/100000: episode: 95, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.726, mean reward: 0.577 [0.505, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.359, 10.313], loss: 0.002283, mae: 0.050050, mean_q: 1.167384
  9600/100000: episode: 96, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.883, mean reward: 0.589 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.846, 10.098], loss: 0.002206, mae: 0.048826, mean_q: 1.167466
  9700/100000: episode: 97, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.090, mean reward: 0.581 [0.499, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.800, 10.164], loss: 0.002259, mae: 0.049712, mean_q: 1.164914
  9800/100000: episode: 98, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.936, mean reward: 0.579 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.881, 10.098], loss: 0.002185, mae: 0.048126, mean_q: 1.165201
  9900/100000: episode: 99, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.960, mean reward: 0.590 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.476, 10.098], loss: 0.002055, mae: 0.047996, mean_q: 1.166755
 10000/100000: episode: 100, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.870, mean reward: 0.599 [0.526, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.901, 10.285], loss: 0.001926, mae: 0.046224, mean_q: 1.165169
 10100/100000: episode: 101, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 59.110, mean reward: 0.591 [0.509, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.701, 10.321], loss: 0.001957, mae: 0.047001, mean_q: 1.164230
 10200/100000: episode: 102, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 57.675, mean reward: 0.577 [0.507, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.732, 10.122], loss: 0.001950, mae: 0.046297, mean_q: 1.166413
 10300/100000: episode: 103, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 56.762, mean reward: 0.568 [0.501, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.896, 10.102], loss: 0.001986, mae: 0.046999, mean_q: 1.164427
 10400/100000: episode: 104, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 56.662, mean reward: 0.567 [0.501, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.124, 10.098], loss: 0.001845, mae: 0.045933, mean_q: 1.166249
 10500/100000: episode: 105, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.906, mean reward: 0.579 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.143, 10.257], loss: 0.001940, mae: 0.046259, mean_q: 1.161757
 10600/100000: episode: 106, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.197, mean reward: 0.582 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.031, 10.098], loss: 0.002043, mae: 0.048273, mean_q: 1.162779
 10700/100000: episode: 107, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.905, mean reward: 0.609 [0.505, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.928, 10.098], loss: 0.001740, mae: 0.044492, mean_q: 1.162857
 10800/100000: episode: 108, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.183, mean reward: 0.582 [0.498, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.992, 10.098], loss: 0.002116, mae: 0.049248, mean_q: 1.167047
 10900/100000: episode: 109, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.655, mean reward: 0.587 [0.513, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.981, 10.157], loss: 0.001958, mae: 0.046687, mean_q: 1.163992
 11000/100000: episode: 110, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 61.251, mean reward: 0.613 [0.503, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.359, 10.098], loss: 0.001801, mae: 0.044615, mean_q: 1.164767
 11100/100000: episode: 111, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.029, mean reward: 0.590 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.664, 10.195], loss: 0.001906, mae: 0.046321, mean_q: 1.165564
 11200/100000: episode: 112, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 60.586, mean reward: 0.606 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.089, 10.482], loss: 0.001818, mae: 0.045414, mean_q: 1.166510
 11300/100000: episode: 113, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.516, mean reward: 0.585 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.911, 10.098], loss: 0.001922, mae: 0.046799, mean_q: 1.163747
 11400/100000: episode: 114, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 63.196, mean reward: 0.632 [0.505, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.990, 10.420], loss: 0.001895, mae: 0.046349, mean_q: 1.164014
 11500/100000: episode: 115, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.569, mean reward: 0.576 [0.497, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.220, 10.129], loss: 0.001793, mae: 0.045507, mean_q: 1.164487
 11600/100000: episode: 116, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 61.811, mean reward: 0.618 [0.512, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.517, 10.174], loss: 0.001776, mae: 0.045396, mean_q: 1.165840
 11700/100000: episode: 117, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.997, mean reward: 0.580 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.536, 10.098], loss: 0.001888, mae: 0.046451, mean_q: 1.164166
 11800/100000: episode: 118, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.795, mean reward: 0.588 [0.502, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.777, 10.098], loss: 0.001978, mae: 0.046817, mean_q: 1.165573
 11900/100000: episode: 119, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 60.211, mean reward: 0.602 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.476, 10.423], loss: 0.001801, mae: 0.045608, mean_q: 1.168905
 12000/100000: episode: 120, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.664, mean reward: 0.587 [0.507, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.586, 10.431], loss: 0.001823, mae: 0.046012, mean_q: 1.167209
 12100/100000: episode: 121, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.420, mean reward: 0.584 [0.508, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.602, 10.165], loss: 0.001945, mae: 0.047617, mean_q: 1.165344
 12200/100000: episode: 122, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.176, mean reward: 0.582 [0.511, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.453, 10.098], loss: 0.001871, mae: 0.046526, mean_q: 1.166280
 12300/100000: episode: 123, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 60.765, mean reward: 0.608 [0.527, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.606, 10.318], loss: 0.001967, mae: 0.047637, mean_q: 1.167343
 12400/100000: episode: 124, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.040, mean reward: 0.580 [0.506, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.658, 10.293], loss: 0.001855, mae: 0.046957, mean_q: 1.164768
 12500/100000: episode: 125, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.553, mean reward: 0.586 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.693, 10.098], loss: 0.001673, mae: 0.044616, mean_q: 1.166338
 12600/100000: episode: 126, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.084, mean reward: 0.581 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.869, 10.144], loss: 0.002092, mae: 0.048840, mean_q: 1.167078
 12700/100000: episode: 127, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.039, mean reward: 0.570 [0.502, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.627, 10.234], loss: 0.001812, mae: 0.046298, mean_q: 1.166458
 12800/100000: episode: 128, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.686, mean reward: 0.607 [0.506, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.454, 10.240], loss: 0.001800, mae: 0.045465, mean_q: 1.165596
 12900/100000: episode: 129, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.065, mean reward: 0.591 [0.500, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.267, 10.329], loss: 0.001737, mae: 0.045473, mean_q: 1.168090
 13000/100000: episode: 130, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.983, mean reward: 0.580 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.527, 10.101], loss: 0.001750, mae: 0.045442, mean_q: 1.164852
 13100/100000: episode: 131, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.099, mean reward: 0.591 [0.518, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.208, 10.133], loss: 0.001814, mae: 0.046517, mean_q: 1.165085
 13200/100000: episode: 132, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.594, mean reward: 0.586 [0.508, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.743, 10.098], loss: 0.001846, mae: 0.046589, mean_q: 1.167496
 13300/100000: episode: 133, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 62.391, mean reward: 0.624 [0.506, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.105, 10.098], loss: 0.001804, mae: 0.045450, mean_q: 1.163324
 13400/100000: episode: 134, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.896, mean reward: 0.589 [0.513, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.667, 10.098], loss: 0.001850, mae: 0.046366, mean_q: 1.164056
 13500/100000: episode: 135, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 61.349, mean reward: 0.613 [0.518, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.111, 10.098], loss: 0.001852, mae: 0.045827, mean_q: 1.163474
 13600/100000: episode: 136, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 57.090, mean reward: 0.571 [0.499, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.523, 10.193], loss: 0.001742, mae: 0.045341, mean_q: 1.166385
 13700/100000: episode: 137, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.270, mean reward: 0.573 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.292, 10.214], loss: 0.001659, mae: 0.044344, mean_q: 1.161923
 13800/100000: episode: 138, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.901, mean reward: 0.609 [0.513, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.571, 10.405], loss: 0.001727, mae: 0.044975, mean_q: 1.165568
 13900/100000: episode: 139, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.326, mean reward: 0.583 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.301, 10.098], loss: 0.001727, mae: 0.045448, mean_q: 1.169135
 14000/100000: episode: 140, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 60.038, mean reward: 0.600 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.450, 10.098], loss: 0.001731, mae: 0.045555, mean_q: 1.167743
 14100/100000: episode: 141, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.763, mean reward: 0.588 [0.505, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.283, 10.098], loss: 0.001659, mae: 0.044742, mean_q: 1.166981
 14200/100000: episode: 142, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.224, mean reward: 0.572 [0.505, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.868, 10.098], loss: 0.001904, mae: 0.046477, mean_q: 1.167329
 14300/100000: episode: 143, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.961, mean reward: 0.590 [0.512, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.451, 10.269], loss: 0.001692, mae: 0.044009, mean_q: 1.165480
 14400/100000: episode: 144, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.288, mean reward: 0.573 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.702, 10.098], loss: 0.001733, mae: 0.045514, mean_q: 1.166445
 14500/100000: episode: 145, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.716, mean reward: 0.587 [0.510, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.853, 10.124], loss: 0.001742, mae: 0.045486, mean_q: 1.166745
 14600/100000: episode: 146, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.896, mean reward: 0.579 [0.502, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.092, 10.098], loss: 0.001745, mae: 0.045568, mean_q: 1.164593
 14700/100000: episode: 147, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 62.989, mean reward: 0.630 [0.508, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.649, 10.163], loss: 0.001693, mae: 0.045109, mean_q: 1.168122
 14800/100000: episode: 148, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 56.261, mean reward: 0.563 [0.510, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.179, 10.098], loss: 0.001590, mae: 0.043805, mean_q: 1.166175
 14900/100000: episode: 149, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 61.624, mean reward: 0.616 [0.517, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.567, 10.098], loss: 0.001758, mae: 0.046183, mean_q: 1.168426
[Info] 1-TH LEVEL FOUND: 1.2172085046768188, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.198s, episode steps: 100, steps per second: 19, episode reward: 59.605, mean reward: 0.596 [0.505, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.510, 10.098], loss: 0.001769, mae: 0.046100, mean_q: 1.169651
 15022/100000: episode: 151, duration: 0.154s, episode steps: 22, steps per second: 143, episode reward: 15.709, mean reward: 0.714 [0.641, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.477, 10.407], loss: 0.001926, mae: 0.047854, mean_q: 1.176740
 15119/100000: episode: 152, duration: 0.543s, episode steps: 97, steps per second: 179, episode reward: 59.302, mean reward: 0.611 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-0.838, 10.100], loss: 0.001750, mae: 0.046017, mean_q: 1.169549
 15140/100000: episode: 153, duration: 0.168s, episode steps: 21, steps per second: 125, episode reward: 14.567, mean reward: 0.694 [0.626, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.455, 10.278], loss: 0.001614, mae: 0.044955, mean_q: 1.170798
 15157/100000: episode: 154, duration: 0.116s, episode steps: 17, steps per second: 146, episode reward: 9.805, mean reward: 0.577 [0.533, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.210], loss: 0.002122, mae: 0.050117, mean_q: 1.169591
 15174/100000: episode: 155, duration: 0.139s, episode steps: 17, steps per second: 122, episode reward: 10.448, mean reward: 0.615 [0.575, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.568, 10.100], loss: 0.001912, mae: 0.048176, mean_q: 1.174619
 15271/100000: episode: 156, duration: 0.559s, episode steps: 97, steps per second: 174, episode reward: 60.216, mean reward: 0.621 [0.508, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.674, 10.100], loss: 0.001976, mae: 0.048313, mean_q: 1.168320
 15369/100000: episode: 157, duration: 0.561s, episode steps: 98, steps per second: 175, episode reward: 58.664, mean reward: 0.599 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-1.476, 10.100], loss: 0.001849, mae: 0.046646, mean_q: 1.172627
 15465/100000: episode: 158, duration: 0.557s, episode steps: 96, steps per second: 172, episode reward: 58.307, mean reward: 0.607 [0.503, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-1.084, 10.297], loss: 0.001942, mae: 0.047859, mean_q: 1.172409
 15563/100000: episode: 159, duration: 0.569s, episode steps: 98, steps per second: 172, episode reward: 57.202, mean reward: 0.584 [0.508, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.897, 10.100], loss: 0.002007, mae: 0.047313, mean_q: 1.175408
 15658/100000: episode: 160, duration: 0.537s, episode steps: 95, steps per second: 177, episode reward: 58.325, mean reward: 0.614 [0.524, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.602, 10.174], loss: 0.002294, mae: 0.049978, mean_q: 1.174283
 15756/100000: episode: 161, duration: 0.539s, episode steps: 98, steps per second: 182, episode reward: 57.091, mean reward: 0.583 [0.502, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-0.636, 10.295], loss: 0.002217, mae: 0.049120, mean_q: 1.172572
 15854/100000: episode: 162, duration: 0.540s, episode steps: 98, steps per second: 181, episode reward: 57.072, mean reward: 0.582 [0.504, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.897, 10.183], loss: 0.002080, mae: 0.048117, mean_q: 1.175986
 15871/100000: episode: 163, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 10.085, mean reward: 0.593 [0.515, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.103, 10.117], loss: 0.001746, mae: 0.044547, mean_q: 1.174096
 15969/100000: episode: 164, duration: 0.554s, episode steps: 98, steps per second: 177, episode reward: 60.530, mean reward: 0.618 [0.513, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-0.561, 10.425], loss: 0.002146, mae: 0.049598, mean_q: 1.175838
 15986/100000: episode: 165, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 9.888, mean reward: 0.582 [0.513, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.127, 10.100], loss: 0.002172, mae: 0.048794, mean_q: 1.178792
 16003/100000: episode: 166, duration: 0.100s, episode steps: 17, steps per second: 169, episode reward: 9.682, mean reward: 0.570 [0.511, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.508, 10.100], loss: 0.002607, mae: 0.052852, mean_q: 1.168127
 16101/100000: episode: 167, duration: 0.543s, episode steps: 98, steps per second: 180, episode reward: 56.512, mean reward: 0.577 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-0.769, 10.100], loss: 0.002033, mae: 0.047938, mean_q: 1.172146
 16122/100000: episode: 168, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 14.683, mean reward: 0.699 [0.667, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.134, 10.347], loss: 0.001905, mae: 0.048023, mean_q: 1.176632
 16139/100000: episode: 169, duration: 0.108s, episode steps: 17, steps per second: 158, episode reward: 12.876, mean reward: 0.757 [0.703, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.719, 10.100], loss: 0.001959, mae: 0.047175, mean_q: 1.175108
 16161/100000: episode: 170, duration: 0.137s, episode steps: 22, steps per second: 160, episode reward: 15.039, mean reward: 0.684 [0.561, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.306], loss: 0.002156, mae: 0.048708, mean_q: 1.175670
 16256/100000: episode: 171, duration: 0.520s, episode steps: 95, steps per second: 183, episode reward: 56.977, mean reward: 0.600 [0.510, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-1.109, 10.171], loss: 0.002166, mae: 0.048702, mean_q: 1.173349
 16273/100000: episode: 172, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 12.160, mean reward: 0.715 [0.673, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.337, 10.100], loss: 0.001968, mae: 0.047362, mean_q: 1.180311
 16371/100000: episode: 173, duration: 0.565s, episode steps: 98, steps per second: 173, episode reward: 56.149, mean reward: 0.573 [0.497, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.476 [-0.639, 10.107], loss: 0.002106, mae: 0.049132, mean_q: 1.175803
 16469/100000: episode: 174, duration: 0.546s, episode steps: 98, steps per second: 179, episode reward: 60.357, mean reward: 0.616 [0.501, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.418, 10.100], loss: 0.002166, mae: 0.049768, mean_q: 1.174447
 16486/100000: episode: 175, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 10.082, mean reward: 0.593 [0.512, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.243, 10.127], loss: 0.002501, mae: 0.051361, mean_q: 1.170244
 16503/100000: episode: 176, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 12.396, mean reward: 0.729 [0.672, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.875, 10.100], loss: 0.002152, mae: 0.052264, mean_q: 1.172753
 16524/100000: episode: 177, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 14.022, mean reward: 0.668 [0.624, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.064, 10.410], loss: 0.001930, mae: 0.048245, mean_q: 1.176461
 16541/100000: episode: 178, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 11.684, mean reward: 0.687 [0.644, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.367, 10.100], loss: 0.001786, mae: 0.046687, mean_q: 1.175299
 16563/100000: episode: 179, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 15.384, mean reward: 0.699 [0.641, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.607, 10.338], loss: 0.001915, mae: 0.048921, mean_q: 1.180383
 16661/100000: episode: 180, duration: 0.559s, episode steps: 98, steps per second: 175, episode reward: 56.733, mean reward: 0.579 [0.505, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-1.154, 10.100], loss: 0.002219, mae: 0.049856, mean_q: 1.175436
 16682/100000: episode: 181, duration: 0.127s, episode steps: 21, steps per second: 166, episode reward: 12.948, mean reward: 0.617 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.265, 10.136], loss: 0.002166, mae: 0.051355, mean_q: 1.175421
 16704/100000: episode: 182, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 15.778, mean reward: 0.717 [0.663, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.445], loss: 0.002104, mae: 0.049233, mean_q: 1.170825
 16801/100000: episode: 183, duration: 0.553s, episode steps: 97, steps per second: 176, episode reward: 57.534, mean reward: 0.593 [0.508, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.612, 10.100], loss: 0.001949, mae: 0.048117, mean_q: 1.176612
 16814/100000: episode: 184, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 8.542, mean reward: 0.657 [0.611, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.357], loss: 0.002074, mae: 0.047281, mean_q: 1.168095
 16912/100000: episode: 185, duration: 0.563s, episode steps: 98, steps per second: 174, episode reward: 58.957, mean reward: 0.602 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.340, 10.100], loss: 0.002147, mae: 0.050136, mean_q: 1.178396
 16929/100000: episode: 186, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 12.431, mean reward: 0.731 [0.620, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.349, 10.100], loss: 0.001979, mae: 0.048001, mean_q: 1.168433
 17027/100000: episode: 187, duration: 0.554s, episode steps: 98, steps per second: 177, episode reward: 59.393, mean reward: 0.606 [0.511, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-0.650, 10.100], loss: 0.002061, mae: 0.049233, mean_q: 1.176162
 17048/100000: episode: 188, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 15.920, mean reward: 0.758 [0.709, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.490], loss: 0.002102, mae: 0.049429, mean_q: 1.177171
 17146/100000: episode: 189, duration: 0.558s, episode steps: 98, steps per second: 176, episode reward: 57.667, mean reward: 0.588 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.680, 10.100], loss: 0.002107, mae: 0.049572, mean_q: 1.179173
 17163/100000: episode: 190, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 11.047, mean reward: 0.650 [0.580, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.284, 10.100], loss: 0.002131, mae: 0.048598, mean_q: 1.178974
 17176/100000: episode: 191, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 7.859, mean reward: 0.605 [0.518, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.106, 10.225], loss: 0.002617, mae: 0.055322, mean_q: 1.188131
 17198/100000: episode: 192, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 15.664, mean reward: 0.712 [0.651, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.352, 10.337], loss: 0.002188, mae: 0.049948, mean_q: 1.181465
 17220/100000: episode: 193, duration: 0.133s, episode steps: 22, steps per second: 166, episode reward: 14.910, mean reward: 0.678 [0.554, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.079, 10.338], loss: 0.002199, mae: 0.048652, mean_q: 1.182173
 17318/100000: episode: 194, duration: 0.548s, episode steps: 98, steps per second: 179, episode reward: 56.253, mean reward: 0.574 [0.509, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-0.516, 10.100], loss: 0.002044, mae: 0.048619, mean_q: 1.181545
 17339/100000: episode: 195, duration: 0.130s, episode steps: 21, steps per second: 161, episode reward: 14.714, mean reward: 0.701 [0.661, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.721, 10.439], loss: 0.001838, mae: 0.046676, mean_q: 1.179042
 17352/100000: episode: 196, duration: 0.086s, episode steps: 13, steps per second: 150, episode reward: 9.318, mean reward: 0.717 [0.664, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.325, 10.450], loss: 0.002025, mae: 0.048920, mean_q: 1.185262
 17449/100000: episode: 197, duration: 0.540s, episode steps: 97, steps per second: 180, episode reward: 59.243, mean reward: 0.611 [0.507, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-0.344, 10.447], loss: 0.001911, mae: 0.046581, mean_q: 1.180336
 17545/100000: episode: 198, duration: 0.539s, episode steps: 96, steps per second: 178, episode reward: 58.047, mean reward: 0.605 [0.501, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.384, 10.115], loss: 0.002072, mae: 0.048669, mean_q: 1.184715
 17558/100000: episode: 199, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 8.906, mean reward: 0.685 [0.630, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.490, 10.404], loss: 0.002189, mae: 0.050624, mean_q: 1.187147
 17580/100000: episode: 200, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 14.682, mean reward: 0.667 [0.571, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.053, 10.337], loss: 0.002319, mae: 0.051077, mean_q: 1.176418
 17678/100000: episode: 201, duration: 0.543s, episode steps: 98, steps per second: 180, episode reward: 56.607, mean reward: 0.578 [0.512, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.432, 10.100], loss: 0.002113, mae: 0.049795, mean_q: 1.185522
 17695/100000: episode: 202, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 10.753, mean reward: 0.633 [0.558, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.548, 10.100], loss: 0.002221, mae: 0.048175, mean_q: 1.173276
 17791/100000: episode: 203, duration: 0.555s, episode steps: 96, steps per second: 173, episode reward: 53.603, mean reward: 0.558 [0.501, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.252, 10.266], loss: 0.001951, mae: 0.047667, mean_q: 1.182738
 17888/100000: episode: 204, duration: 0.560s, episode steps: 97, steps per second: 173, episode reward: 56.674, mean reward: 0.584 [0.507, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-0.890, 10.235], loss: 0.002012, mae: 0.047929, mean_q: 1.184280
 17986/100000: episode: 205, duration: 0.557s, episode steps: 98, steps per second: 176, episode reward: 58.532, mean reward: 0.597 [0.505, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-1.235, 10.167], loss: 0.002101, mae: 0.049588, mean_q: 1.183412
 18003/100000: episode: 206, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 11.685, mean reward: 0.687 [0.647, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.318, 10.100], loss: 0.001884, mae: 0.047742, mean_q: 1.184130
 18016/100000: episode: 207, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 9.069, mean reward: 0.698 [0.619, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-1.662, 10.390], loss: 0.002014, mae: 0.047925, mean_q: 1.195531
 18037/100000: episode: 208, duration: 0.125s, episode steps: 21, steps per second: 168, episode reward: 16.210, mean reward: 0.772 [0.709, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.794, 10.627], loss: 0.002119, mae: 0.050492, mean_q: 1.186314
 18054/100000: episode: 209, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 9.887, mean reward: 0.582 [0.507, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.083, 10.100], loss: 0.001794, mae: 0.046558, mean_q: 1.177423
 18075/100000: episode: 210, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 15.278, mean reward: 0.728 [0.661, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-1.184, 10.385], loss: 0.001872, mae: 0.045666, mean_q: 1.189297
 18173/100000: episode: 211, duration: 0.545s, episode steps: 98, steps per second: 180, episode reward: 56.861, mean reward: 0.580 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-1.255, 10.100], loss: 0.001894, mae: 0.046767, mean_q: 1.186643
 18195/100000: episode: 212, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 14.717, mean reward: 0.669 [0.599, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.788, 10.439], loss: 0.001819, mae: 0.046636, mean_q: 1.190236
 18208/100000: episode: 213, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 9.087, mean reward: 0.699 [0.650, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.371, 10.434], loss: 0.002110, mae: 0.048163, mean_q: 1.182509
 18305/100000: episode: 214, duration: 0.544s, episode steps: 97, steps per second: 178, episode reward: 56.160, mean reward: 0.579 [0.502, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-1.189, 10.174], loss: 0.001981, mae: 0.047209, mean_q: 1.184321
 18402/100000: episode: 215, duration: 0.546s, episode steps: 97, steps per second: 178, episode reward: 58.223, mean reward: 0.600 [0.499, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.742, 10.155], loss: 0.002226, mae: 0.050272, mean_q: 1.186387
 18499/100000: episode: 216, duration: 0.548s, episode steps: 97, steps per second: 177, episode reward: 57.686, mean reward: 0.595 [0.506, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.555, 10.408], loss: 0.001852, mae: 0.046660, mean_q: 1.186842
 18520/100000: episode: 217, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 15.132, mean reward: 0.721 [0.564, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.299], loss: 0.001936, mae: 0.046656, mean_q: 1.174291
 18616/100000: episode: 218, duration: 0.536s, episode steps: 96, steps per second: 179, episode reward: 59.040, mean reward: 0.615 [0.508, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.519, 10.358], loss: 0.001752, mae: 0.045634, mean_q: 1.184455
 18633/100000: episode: 219, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 12.020, mean reward: 0.707 [0.661, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.357, 10.100], loss: 0.001885, mae: 0.045987, mean_q: 1.194968
 18730/100000: episode: 220, duration: 0.538s, episode steps: 97, steps per second: 180, episode reward: 56.287, mean reward: 0.580 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-1.376, 10.198], loss: 0.001995, mae: 0.047490, mean_q: 1.187515
 18743/100000: episode: 221, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 8.479, mean reward: 0.652 [0.602, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.365], loss: 0.001643, mae: 0.045062, mean_q: 1.184035
 18760/100000: episode: 222, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 11.810, mean reward: 0.695 [0.616, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.147, 10.100], loss: 0.001603, mae: 0.044123, mean_q: 1.187171
 18856/100000: episode: 223, duration: 0.535s, episode steps: 96, steps per second: 180, episode reward: 57.733, mean reward: 0.601 [0.511, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-0.378, 10.100], loss: 0.001665, mae: 0.043872, mean_q: 1.183778
 18953/100000: episode: 224, duration: 0.538s, episode steps: 97, steps per second: 180, episode reward: 62.517, mean reward: 0.645 [0.515, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-1.874, 10.100], loss: 0.001751, mae: 0.045450, mean_q: 1.182111
 18970/100000: episode: 225, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 10.346, mean reward: 0.609 [0.523, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.342, 10.106], loss: 0.001533, mae: 0.042948, mean_q: 1.184976
 19068/100000: episode: 226, duration: 0.532s, episode steps: 98, steps per second: 184, episode reward: 57.144, mean reward: 0.583 [0.501, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.607, 10.100], loss: 0.001824, mae: 0.045998, mean_q: 1.188011
 19166/100000: episode: 227, duration: 0.546s, episode steps: 98, steps per second: 180, episode reward: 56.572, mean reward: 0.577 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-1.008, 10.100], loss: 0.001722, mae: 0.044919, mean_q: 1.194162
 19263/100000: episode: 228, duration: 0.542s, episode steps: 97, steps per second: 179, episode reward: 58.227, mean reward: 0.600 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-0.825, 10.100], loss: 0.001794, mae: 0.045728, mean_q: 1.195718
 19284/100000: episode: 229, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 15.742, mean reward: 0.750 [0.683, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.370, 10.527], loss: 0.001380, mae: 0.040716, mean_q: 1.189032
 19382/100000: episode: 230, duration: 0.538s, episode steps: 98, steps per second: 182, episode reward: 56.000, mean reward: 0.571 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.476 [-1.049, 10.226], loss: 0.002117, mae: 0.048401, mean_q: 1.195939
 19480/100000: episode: 231, duration: 0.559s, episode steps: 98, steps per second: 175, episode reward: 58.276, mean reward: 0.595 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.736, 10.100], loss: 0.001924, mae: 0.047111, mean_q: 1.193875
 19578/100000: episode: 232, duration: 0.553s, episode steps: 98, steps per second: 177, episode reward: 55.558, mean reward: 0.567 [0.499, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.497, 10.248], loss: 0.001671, mae: 0.044784, mean_q: 1.189377
 19595/100000: episode: 233, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 10.441, mean reward: 0.614 [0.532, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.174], loss: 0.001801, mae: 0.044190, mean_q: 1.186887
 19617/100000: episode: 234, duration: 0.134s, episode steps: 22, steps per second: 165, episode reward: 14.495, mean reward: 0.659 [0.557, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.461, 10.227], loss: 0.001762, mae: 0.044783, mean_q: 1.183092
 19712/100000: episode: 235, duration: 0.540s, episode steps: 95, steps per second: 176, episode reward: 53.612, mean reward: 0.564 [0.504, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.003, 10.288], loss: 0.001548, mae: 0.043379, mean_q: 1.193198
 19810/100000: episode: 236, duration: 0.554s, episode steps: 98, steps per second: 177, episode reward: 58.738, mean reward: 0.599 [0.502, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.658, 10.100], loss: 0.001721, mae: 0.044754, mean_q: 1.190186
 19827/100000: episode: 237, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 12.050, mean reward: 0.709 [0.625, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.061, 10.100], loss: 0.001487, mae: 0.042742, mean_q: 1.182189
 19849/100000: episode: 238, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 14.330, mean reward: 0.651 [0.586, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.825, 10.330], loss: 0.001408, mae: 0.039897, mean_q: 1.202656
 19866/100000: episode: 239, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 9.533, mean reward: 0.561 [0.524, 0.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.339, 10.194], loss: 0.001787, mae: 0.045550, mean_q: 1.200013
[Info] 2-TH LEVEL FOUND: 1.570723533630371, Considering 11/89 traces
 19962/100000: episode: 240, duration: 4.675s, episode steps: 96, steps per second: 21, episode reward: 56.050, mean reward: 0.584 [0.498, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-1.376, 10.100], loss: 0.001752, mae: 0.045805, mean_q: 1.192060
 19988/100000: episode: 241, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 18.886, mean reward: 0.726 [0.633, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.894, 10.403], loss: 0.001598, mae: 0.042768, mean_q: 1.182605
 20014/100000: episode: 242, duration: 0.147s, episode steps: 26, steps per second: 176, episode reward: 21.401, mean reward: 0.823 [0.718, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.117, 10.483], loss: 0.001518, mae: 0.042511, mean_q: 1.194540
 20040/100000: episode: 243, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 17.965, mean reward: 0.691 [0.584, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.206, 10.340], loss: 0.001462, mae: 0.041561, mean_q: 1.192505
 20066/100000: episode: 244, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 19.526, mean reward: 0.751 [0.657, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.498], loss: 0.001384, mae: 0.040848, mean_q: 1.196296
 20092/100000: episode: 245, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 19.995, mean reward: 0.769 [0.718, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.624, 10.354], loss: 0.001612, mae: 0.043324, mean_q: 1.198649
 20118/100000: episode: 246, duration: 0.147s, episode steps: 26, steps per second: 176, episode reward: 17.839, mean reward: 0.686 [0.552, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.076, 10.286], loss: 0.001621, mae: 0.043149, mean_q: 1.204217
 20144/100000: episode: 247, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 18.715, mean reward: 0.720 [0.633, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.669, 10.392], loss: 0.001347, mae: 0.040061, mean_q: 1.197068
 20170/100000: episode: 248, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 19.668, mean reward: 0.756 [0.670, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.350], loss: 0.001396, mae: 0.040199, mean_q: 1.188298
 20196/100000: episode: 249, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 20.298, mean reward: 0.781 [0.694, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.166, 10.492], loss: 0.001864, mae: 0.047114, mean_q: 1.199412
 20222/100000: episode: 250, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 19.839, mean reward: 0.763 [0.713, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.455, 10.446], loss: 0.001702, mae: 0.044274, mean_q: 1.203100
 20248/100000: episode: 251, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 20.879, mean reward: 0.803 [0.718, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.707, 10.467], loss: 0.001672, mae: 0.043997, mean_q: 1.200434
 20274/100000: episode: 252, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 20.758, mean reward: 0.798 [0.729, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.484, 10.451], loss: 0.001812, mae: 0.045397, mean_q: 1.203926
 20300/100000: episode: 253, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 20.660, mean reward: 0.795 [0.746, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.089, 10.567], loss: 0.001606, mae: 0.043382, mean_q: 1.208642
 20326/100000: episode: 254, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 19.808, mean reward: 0.762 [0.678, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.649, 10.540], loss: 0.001554, mae: 0.042145, mean_q: 1.205891
 20352/100000: episode: 255, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 20.178, mean reward: 0.776 [0.682, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.519], loss: 0.001354, mae: 0.040546, mean_q: 1.210363
 20378/100000: episode: 256, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 18.265, mean reward: 0.703 [0.619, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.354, 10.348], loss: 0.001553, mae: 0.042438, mean_q: 1.207671
 20404/100000: episode: 257, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 19.415, mean reward: 0.747 [0.623, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.508, 10.345], loss: 0.001620, mae: 0.042941, mean_q: 1.215366
 20430/100000: episode: 258, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 19.895, mean reward: 0.765 [0.677, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.624, 10.451], loss: 0.001716, mae: 0.045545, mean_q: 1.220949
 20456/100000: episode: 259, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 18.942, mean reward: 0.729 [0.638, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.743, 10.398], loss: 0.001521, mae: 0.042759, mean_q: 1.214982
 20482/100000: episode: 260, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 18.593, mean reward: 0.715 [0.606, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.295, 10.555], loss: 0.001639, mae: 0.044385, mean_q: 1.210810
 20508/100000: episode: 261, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 19.299, mean reward: 0.742 [0.610, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.066, 10.391], loss: 0.001450, mae: 0.042287, mean_q: 1.216077
 20534/100000: episode: 262, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 20.062, mean reward: 0.772 [0.615, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.419], loss: 0.001593, mae: 0.043702, mean_q: 1.225891
 20560/100000: episode: 263, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 18.439, mean reward: 0.709 [0.627, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.068, 10.312], loss: 0.001712, mae: 0.044830, mean_q: 1.221274
 20586/100000: episode: 264, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 20.182, mean reward: 0.776 [0.659, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.080, 10.428], loss: 0.001637, mae: 0.044401, mean_q: 1.232315
 20612/100000: episode: 265, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 18.229, mean reward: 0.701 [0.554, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.194, 10.257], loss: 0.001565, mae: 0.044061, mean_q: 1.209730
 20638/100000: episode: 266, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 18.205, mean reward: 0.700 [0.593, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.373, 10.379], loss: 0.001467, mae: 0.041621, mean_q: 1.231841
 20664/100000: episode: 267, duration: 0.157s, episode steps: 26, steps per second: 165, episode reward: 21.013, mean reward: 0.808 [0.736, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.591], loss: 0.001639, mae: 0.044310, mean_q: 1.211131
 20690/100000: episode: 268, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 20.167, mean reward: 0.776 [0.711, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.521, 10.404], loss: 0.001452, mae: 0.041943, mean_q: 1.214801
 20716/100000: episode: 269, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 18.182, mean reward: 0.699 [0.617, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.356], loss: 0.001697, mae: 0.045824, mean_q: 1.235713
 20742/100000: episode: 270, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 19.138, mean reward: 0.736 [0.658, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.509, 10.433], loss: 0.002181, mae: 0.050184, mean_q: 1.242303
 20768/100000: episode: 271, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 18.943, mean reward: 0.729 [0.613, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.292, 10.339], loss: 0.001712, mae: 0.044906, mean_q: 1.223771
[Info] FALSIFICATION!
 20787/100000: episode: 272, duration: 0.541s, episode steps: 19, steps per second: 35, episode reward: 17.000, mean reward: 0.895 [0.765, 1.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.614, 10.424], loss: 0.001514, mae: 0.043111, mean_q: 1.242400
 20813/100000: episode: 273, duration: 0.159s, episode steps: 26, steps per second: 163, episode reward: 18.495, mean reward: 0.711 [0.570, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.661, 10.280], loss: 0.002024, mae: 0.046348, mean_q: 1.238304
 20839/100000: episode: 274, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 20.485, mean reward: 0.788 [0.756, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.595], loss: 0.001803, mae: 0.045237, mean_q: 1.229354
 20865/100000: episode: 275, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 19.352, mean reward: 0.744 [0.686, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.720, 10.466], loss: 0.002531, mae: 0.050708, mean_q: 1.231733
 20891/100000: episode: 276, duration: 0.158s, episode steps: 26, steps per second: 164, episode reward: 22.403, mean reward: 0.862 [0.720, 0.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.796, 10.597], loss: 0.001444, mae: 0.041582, mean_q: 1.239825
 20917/100000: episode: 277, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 20.239, mean reward: 0.778 [0.722, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.399, 10.529], loss: 0.001530, mae: 0.043274, mean_q: 1.232836
 20943/100000: episode: 278, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 20.120, mean reward: 0.774 [0.696, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.565], loss: 0.002225, mae: 0.046636, mean_q: 1.230809
 20969/100000: episode: 279, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 18.781, mean reward: 0.722 [0.648, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.426], loss: 0.002056, mae: 0.049907, mean_q: 1.250011
 20995/100000: episode: 280, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 19.497, mean reward: 0.750 [0.679, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.444, 10.443], loss: 0.001702, mae: 0.045300, mean_q: 1.237000
 21021/100000: episode: 281, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 20.137, mean reward: 0.775 [0.675, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.554, 10.630], loss: 0.001713, mae: 0.043898, mean_q: 1.249104
 21047/100000: episode: 282, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 19.324, mean reward: 0.743 [0.680, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.175, 10.466], loss: 0.001662, mae: 0.045061, mean_q: 1.260062
 21073/100000: episode: 283, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 21.518, mean reward: 0.828 [0.742, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.284, 10.567], loss: 0.001909, mae: 0.047596, mean_q: 1.248620
 21099/100000: episode: 284, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 19.410, mean reward: 0.747 [0.667, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.446], loss: 0.001678, mae: 0.045192, mean_q: 1.245046
 21125/100000: episode: 285, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 19.125, mean reward: 0.736 [0.584, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.340, 10.309], loss: 0.001439, mae: 0.042217, mean_q: 1.254307
 21151/100000: episode: 286, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 18.778, mean reward: 0.722 [0.659, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.364, 10.369], loss: 0.001504, mae: 0.042436, mean_q: 1.256691
 21177/100000: episode: 287, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 19.740, mean reward: 0.759 [0.707, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.392, 10.508], loss: 0.001768, mae: 0.043647, mean_q: 1.255575
 21203/100000: episode: 288, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 19.037, mean reward: 0.732 [0.617, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.260], loss: 0.001433, mae: 0.042389, mean_q: 1.245970
 21229/100000: episode: 289, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 18.249, mean reward: 0.702 [0.572, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.773, 10.358], loss: 0.001306, mae: 0.040059, mean_q: 1.253582
 21255/100000: episode: 290, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 19.103, mean reward: 0.735 [0.639, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.642], loss: 0.001534, mae: 0.042948, mean_q: 1.253772
 21281/100000: episode: 291, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 18.471, mean reward: 0.710 [0.586, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.035, 10.379], loss: 0.001551, mae: 0.042972, mean_q: 1.256181
 21307/100000: episode: 292, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 20.278, mean reward: 0.780 [0.706, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.267, 10.540], loss: 0.001911, mae: 0.044716, mean_q: 1.261551
 21333/100000: episode: 293, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 20.930, mean reward: 0.805 [0.752, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.544], loss: 0.001663, mae: 0.044836, mean_q: 1.268433
 21359/100000: episode: 294, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 19.179, mean reward: 0.738 [0.642, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.402, 10.416], loss: 0.001477, mae: 0.042262, mean_q: 1.258129
 21385/100000: episode: 295, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 19.215, mean reward: 0.739 [0.573, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.447, 10.244], loss: 0.001693, mae: 0.043860, mean_q: 1.260874
 21411/100000: episode: 296, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 19.086, mean reward: 0.734 [0.630, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.873, 10.371], loss: 0.001698, mae: 0.046039, mean_q: 1.272248
 21437/100000: episode: 297, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 19.820, mean reward: 0.762 [0.680, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.087, 10.497], loss: 0.001342, mae: 0.040251, mean_q: 1.269839
 21463/100000: episode: 298, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 22.121, mean reward: 0.851 [0.775, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.469, 10.573], loss: 0.002182, mae: 0.047985, mean_q: 1.271794
 21489/100000: episode: 299, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 19.064, mean reward: 0.733 [0.652, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.273, 10.358], loss: 0.001424, mae: 0.041181, mean_q: 1.264384
 21515/100000: episode: 300, duration: 0.157s, episode steps: 26, steps per second: 165, episode reward: 19.154, mean reward: 0.737 [0.665, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.268, 10.459], loss: 0.001791, mae: 0.047208, mean_q: 1.265818
 21541/100000: episode: 301, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 19.115, mean reward: 0.735 [0.611, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.680, 10.416], loss: 0.002070, mae: 0.050729, mean_q: 1.268516
 21567/100000: episode: 302, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 20.790, mean reward: 0.800 [0.683, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.058, 10.394], loss: 0.001582, mae: 0.044515, mean_q: 1.259114
 21593/100000: episode: 303, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 20.013, mean reward: 0.770 [0.684, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.470, 10.448], loss: 0.001383, mae: 0.042023, mean_q: 1.287061
 21619/100000: episode: 304, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 18.460, mean reward: 0.710 [0.636, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.307, 10.326], loss: 0.001514, mae: 0.042964, mean_q: 1.286058
 21645/100000: episode: 305, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 19.623, mean reward: 0.755 [0.700, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.838, 10.519], loss: 0.001501, mae: 0.042778, mean_q: 1.266621
 21671/100000: episode: 306, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 18.995, mean reward: 0.731 [0.667, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.506], loss: 0.001565, mae: 0.042954, mean_q: 1.289705
 21697/100000: episode: 307, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 16.475, mean reward: 0.634 [0.525, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.701, 10.132], loss: 0.001863, mae: 0.044886, mean_q: 1.281482
 21723/100000: episode: 308, duration: 0.160s, episode steps: 26, steps per second: 162, episode reward: 18.544, mean reward: 0.713 [0.611, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.947, 10.344], loss: 0.001735, mae: 0.042003, mean_q: 1.281469
 21749/100000: episode: 309, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 21.095, mean reward: 0.811 [0.717, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.381, 10.640], loss: 0.001799, mae: 0.045491, mean_q: 1.278199
 21775/100000: episode: 310, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 17.449, mean reward: 0.671 [0.562, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.114, 10.234], loss: 0.001651, mae: 0.042466, mean_q: 1.276135
 21801/100000: episode: 311, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 19.751, mean reward: 0.760 [0.673, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.070, 10.532], loss: 0.001705, mae: 0.042606, mean_q: 1.277254
 21827/100000: episode: 312, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 16.927, mean reward: 0.651 [0.597, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.632, 10.375], loss: 0.001504, mae: 0.043598, mean_q: 1.276968
 21853/100000: episode: 313, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 18.077, mean reward: 0.695 [0.621, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.547, 10.355], loss: 0.001637, mae: 0.041959, mean_q: 1.288988
 21879/100000: episode: 314, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 23.255, mean reward: 0.894 [0.808, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.127, 10.680], loss: 0.001407, mae: 0.041568, mean_q: 1.282015
 21905/100000: episode: 315, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 18.557, mean reward: 0.714 [0.608, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.118, 10.358], loss: 0.001349, mae: 0.040628, mean_q: 1.285495
 21931/100000: episode: 316, duration: 0.161s, episode steps: 26, steps per second: 162, episode reward: 20.361, mean reward: 0.783 [0.667, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.436], loss: 0.001500, mae: 0.042919, mean_q: 1.284623
 21957/100000: episode: 317, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 19.878, mean reward: 0.765 [0.716, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.206, 10.522], loss: 0.001602, mae: 0.041281, mean_q: 1.287045
 21983/100000: episode: 318, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 18.969, mean reward: 0.730 [0.630, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.424], loss: 0.001223, mae: 0.038317, mean_q: 1.288605
 22009/100000: episode: 319, duration: 0.160s, episode steps: 26, steps per second: 163, episode reward: 18.090, mean reward: 0.696 [0.562, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.236, 10.244], loss: 0.001465, mae: 0.042747, mean_q: 1.286940
 22035/100000: episode: 320, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 18.749, mean reward: 0.721 [0.574, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.569, 10.307], loss: 0.001277, mae: 0.039441, mean_q: 1.286906
 22061/100000: episode: 321, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 18.590, mean reward: 0.715 [0.629, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.701, 10.468], loss: 0.001322, mae: 0.040166, mean_q: 1.279060
 22087/100000: episode: 322, duration: 0.155s, episode steps: 26, steps per second: 167, episode reward: 20.314, mean reward: 0.781 [0.624, 0.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.495, 10.525], loss: 0.001303, mae: 0.039775, mean_q: 1.287040
 22113/100000: episode: 323, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 20.138, mean reward: 0.775 [0.684, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.599, 10.440], loss: 0.001437, mae: 0.041061, mean_q: 1.291552
 22139/100000: episode: 324, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 17.968, mean reward: 0.691 [0.604, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.300, 10.289], loss: 0.001406, mae: 0.042016, mean_q: 1.296342
 22165/100000: episode: 325, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 17.080, mean reward: 0.657 [0.521, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.920, 10.111], loss: 0.001406, mae: 0.041889, mean_q: 1.302035
 22191/100000: episode: 326, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 19.098, mean reward: 0.735 [0.573, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.324], loss: 0.001386, mae: 0.040902, mean_q: 1.289785
 22217/100000: episode: 327, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 19.385, mean reward: 0.746 [0.674, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.542], loss: 0.001265, mae: 0.039905, mean_q: 1.298140
 22243/100000: episode: 328, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 18.778, mean reward: 0.722 [0.672, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.035, 10.498], loss: 0.001255, mae: 0.039374, mean_q: 1.293709
[Info] Complete ISplit Iteration
[Info] Levels: [1.2172085, 1.5707235, 1.811199]
[Info] Cond. Prob: [0.1, 0.11, 0.01]
[Info] Error Prob: 0.00011000000000000002

 22269/100000: episode: 329, duration: 4.459s, episode steps: 26, steps per second: 6, episode reward: 17.688, mean reward: 0.680 [0.565, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.058, 10.357], loss: 0.001424, mae: 0.042228, mean_q: 1.307470
 22369/100000: episode: 330, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.680, mean reward: 0.577 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.139, 10.107], loss: 0.001687, mae: 0.043872, mean_q: 1.289942
 22469/100000: episode: 331, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.827, mean reward: 0.578 [0.511, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.810, 10.098], loss: 0.001490, mae: 0.042274, mean_q: 1.297244
 22569/100000: episode: 332, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 59.947, mean reward: 0.599 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.689, 10.190], loss: 0.001452, mae: 0.040897, mean_q: 1.305457
 22669/100000: episode: 333, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.883, mean reward: 0.579 [0.498, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.850, 10.136], loss: 0.001521, mae: 0.041928, mean_q: 1.299544
 22769/100000: episode: 334, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 59.152, mean reward: 0.592 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.968, 10.098], loss: 0.001489, mae: 0.042403, mean_q: 1.300120
 22869/100000: episode: 335, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 61.633, mean reward: 0.616 [0.518, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.707, 10.098], loss: 0.001489, mae: 0.042110, mean_q: 1.300039
 22969/100000: episode: 336, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 56.776, mean reward: 0.568 [0.502, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.158, 10.101], loss: 0.001454, mae: 0.041647, mean_q: 1.301705
 23069/100000: episode: 337, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 59.352, mean reward: 0.594 [0.514, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.362, 10.197], loss: 0.001578, mae: 0.042196, mean_q: 1.297482
 23169/100000: episode: 338, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 59.937, mean reward: 0.599 [0.508, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.338, 10.098], loss: 0.001603, mae: 0.042815, mean_q: 1.303307
 23269/100000: episode: 339, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 58.138, mean reward: 0.581 [0.510, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.431, 10.271], loss: 0.001630, mae: 0.043713, mean_q: 1.291964
 23369/100000: episode: 340, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.049, mean reward: 0.580 [0.502, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.555, 10.300], loss: 0.002180, mae: 0.048925, mean_q: 1.296178
 23469/100000: episode: 341, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.353, mean reward: 0.594 [0.509, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.809, 10.098], loss: 0.001561, mae: 0.042679, mean_q: 1.294725
 23569/100000: episode: 342, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 56.619, mean reward: 0.566 [0.501, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.679, 10.098], loss: 0.001603, mae: 0.043552, mean_q: 1.298862
 23669/100000: episode: 343, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.321, mean reward: 0.583 [0.502, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.856, 10.166], loss: 0.001510, mae: 0.041816, mean_q: 1.296977
 23769/100000: episode: 344, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.700, mean reward: 0.587 [0.502, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.373, 10.098], loss: 0.001474, mae: 0.040914, mean_q: 1.299088
 23869/100000: episode: 345, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.819, mean reward: 0.578 [0.518, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.724, 10.098], loss: 0.001583, mae: 0.042754, mean_q: 1.292189
 23969/100000: episode: 346, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 56.424, mean reward: 0.564 [0.515, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.211, 10.160], loss: 0.001489, mae: 0.041528, mean_q: 1.290923
 24069/100000: episode: 347, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 59.261, mean reward: 0.593 [0.512, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.305, 10.138], loss: 0.001937, mae: 0.046814, mean_q: 1.286808
 24169/100000: episode: 348, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.016, mean reward: 0.570 [0.507, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.173, 10.148], loss: 0.001690, mae: 0.043055, mean_q: 1.284099
 24269/100000: episode: 349, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 60.857, mean reward: 0.609 [0.505, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.704, 10.378], loss: 0.001787, mae: 0.044058, mean_q: 1.293370
 24369/100000: episode: 350, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.122, mean reward: 0.581 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.637, 10.258], loss: 0.001522, mae: 0.041780, mean_q: 1.288743
 24469/100000: episode: 351, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.208, mean reward: 0.582 [0.515, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.270, 10.098], loss: 0.001709, mae: 0.044551, mean_q: 1.284542
 24569/100000: episode: 352, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 58.626, mean reward: 0.586 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.497, 10.098], loss: 0.001420, mae: 0.041486, mean_q: 1.288581
 24669/100000: episode: 353, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.421, mean reward: 0.584 [0.499, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.141, 10.098], loss: 0.001547, mae: 0.042549, mean_q: 1.287808
 24769/100000: episode: 354, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.287, mean reward: 0.583 [0.510, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.895, 10.295], loss: 0.001602, mae: 0.042434, mean_q: 1.289859
 24869/100000: episode: 355, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.638, mean reward: 0.596 [0.518, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.563, 10.148], loss: 0.001648, mae: 0.043607, mean_q: 1.284243
 24969/100000: episode: 356, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 66.098, mean reward: 0.661 [0.507, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.531, 10.290], loss: 0.001667, mae: 0.043723, mean_q: 1.281681
 25069/100000: episode: 357, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 61.137, mean reward: 0.611 [0.518, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.350, 10.098], loss: 0.001868, mae: 0.044704, mean_q: 1.282156
 25169/100000: episode: 358, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 58.129, mean reward: 0.581 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.770, 10.139], loss: 0.001462, mae: 0.041763, mean_q: 1.277594
 25269/100000: episode: 359, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 59.428, mean reward: 0.594 [0.516, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.449, 10.209], loss: 0.001716, mae: 0.043091, mean_q: 1.273792
 25369/100000: episode: 360, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.327, mean reward: 0.593 [0.507, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.731, 10.098], loss: 0.001547, mae: 0.043219, mean_q: 1.264949
 25469/100000: episode: 361, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 56.536, mean reward: 0.565 [0.498, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.613, 10.098], loss: 0.001576, mae: 0.042562, mean_q: 1.259628
 25569/100000: episode: 362, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.708, mean reward: 0.587 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.647, 10.098], loss: 0.001627, mae: 0.042963, mean_q: 1.252684
 25669/100000: episode: 363, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 57.801, mean reward: 0.578 [0.507, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.507, 10.148], loss: 0.001693, mae: 0.042957, mean_q: 1.251342
 25769/100000: episode: 364, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.282, mean reward: 0.603 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.312, 10.345], loss: 0.001936, mae: 0.045651, mean_q: 1.242831
 25869/100000: episode: 365, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.609, mean reward: 0.576 [0.499, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.521, 10.098], loss: 0.001358, mae: 0.040303, mean_q: 1.234692
 25969/100000: episode: 366, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.896, mean reward: 0.579 [0.504, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.027, 10.098], loss: 0.001509, mae: 0.041727, mean_q: 1.226813
 26069/100000: episode: 367, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.331, mean reward: 0.583 [0.505, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.901, 10.156], loss: 0.001553, mae: 0.042492, mean_q: 1.223823
 26169/100000: episode: 368, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 61.558, mean reward: 0.616 [0.526, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.083, 10.098], loss: 0.001601, mae: 0.043092, mean_q: 1.220219
 26269/100000: episode: 369, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.503, mean reward: 0.585 [0.513, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.111, 10.098], loss: 0.001387, mae: 0.040899, mean_q: 1.214909
 26369/100000: episode: 370, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.526, mean reward: 0.605 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.091, 10.098], loss: 0.001435, mae: 0.041038, mean_q: 1.202244
 26469/100000: episode: 371, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.750, mean reward: 0.588 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.184, 10.098], loss: 0.001395, mae: 0.040519, mean_q: 1.202056
 26569/100000: episode: 372, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.761, mean reward: 0.598 [0.507, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.890, 10.327], loss: 0.001438, mae: 0.041307, mean_q: 1.197597
 26669/100000: episode: 373, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.421, mean reward: 0.584 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.696, 10.098], loss: 0.001403, mae: 0.040991, mean_q: 1.193919
 26769/100000: episode: 374, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 56.658, mean reward: 0.567 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.594, 10.098], loss: 0.001387, mae: 0.040477, mean_q: 1.182896
 26869/100000: episode: 375, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 58.632, mean reward: 0.586 [0.511, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.183, 10.250], loss: 0.001356, mae: 0.040585, mean_q: 1.183676
 26969/100000: episode: 376, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.914, mean reward: 0.579 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.725, 10.098], loss: 0.001443, mae: 0.041238, mean_q: 1.172791
 27069/100000: episode: 377, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.992, mean reward: 0.600 [0.504, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.909, 10.098], loss: 0.001235, mae: 0.038593, mean_q: 1.167171
 27169/100000: episode: 378, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.756, mean reward: 0.588 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.406, 10.098], loss: 0.001358, mae: 0.040348, mean_q: 1.165968
 27269/100000: episode: 379, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.377, mean reward: 0.604 [0.519, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.722, 10.333], loss: 0.001319, mae: 0.039748, mean_q: 1.163153
 27369/100000: episode: 380, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.994, mean reward: 0.590 [0.515, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.116, 10.098], loss: 0.001361, mae: 0.040872, mean_q: 1.164984
 27469/100000: episode: 381, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 57.847, mean reward: 0.578 [0.499, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.826, 10.131], loss: 0.001334, mae: 0.040149, mean_q: 1.165345
 27569/100000: episode: 382, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.337, mean reward: 0.583 [0.505, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.138, 10.267], loss: 0.001281, mae: 0.039281, mean_q: 1.162193
 27669/100000: episode: 383, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.008, mean reward: 0.570 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.557, 10.177], loss: 0.001274, mae: 0.039347, mean_q: 1.162620
 27769/100000: episode: 384, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.725, mean reward: 0.597 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.163, 10.143], loss: 0.001284, mae: 0.040015, mean_q: 1.166446
 27869/100000: episode: 385, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.060, mean reward: 0.581 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.803, 10.197], loss: 0.001347, mae: 0.040187, mean_q: 1.164405
 27969/100000: episode: 386, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.639, mean reward: 0.596 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.691, 10.289], loss: 0.001330, mae: 0.039746, mean_q: 1.162900
 28069/100000: episode: 387, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 62.294, mean reward: 0.623 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.228, 10.365], loss: 0.001375, mae: 0.040644, mean_q: 1.164554
 28169/100000: episode: 388, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.901, mean reward: 0.599 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.499, 10.308], loss: 0.001395, mae: 0.040977, mean_q: 1.165338
 28269/100000: episode: 389, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 62.140, mean reward: 0.621 [0.501, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.441, 10.385], loss: 0.001375, mae: 0.040904, mean_q: 1.167927
 28369/100000: episode: 390, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.289, mean reward: 0.583 [0.513, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.414, 10.098], loss: 0.001392, mae: 0.041000, mean_q: 1.168422
 28469/100000: episode: 391, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.975, mean reward: 0.590 [0.501, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.224, 10.098], loss: 0.001349, mae: 0.040269, mean_q: 1.168428
 28569/100000: episode: 392, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.354, mean reward: 0.604 [0.506, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.465, 10.251], loss: 0.001386, mae: 0.040376, mean_q: 1.169490
 28669/100000: episode: 393, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.230, mean reward: 0.582 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.837, 10.098], loss: 0.001398, mae: 0.040691, mean_q: 1.166921
 28769/100000: episode: 394, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 57.800, mean reward: 0.578 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.426, 10.098], loss: 0.001432, mae: 0.041533, mean_q: 1.165430
 28869/100000: episode: 395, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.419, mean reward: 0.594 [0.508, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.176, 10.098], loss: 0.001399, mae: 0.041009, mean_q: 1.169080
 28969/100000: episode: 396, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.973, mean reward: 0.590 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.423, 10.098], loss: 0.001402, mae: 0.040983, mean_q: 1.166138
 29069/100000: episode: 397, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 60.441, mean reward: 0.604 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.103, 10.098], loss: 0.001411, mae: 0.041057, mean_q: 1.165084
 29169/100000: episode: 398, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 57.705, mean reward: 0.577 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.195, 10.276], loss: 0.001332, mae: 0.040150, mean_q: 1.166624
 29269/100000: episode: 399, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 56.836, mean reward: 0.568 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.141, 10.115], loss: 0.001466, mae: 0.042019, mean_q: 1.170768
 29369/100000: episode: 400, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.465, mean reward: 0.575 [0.508, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.347, 10.098], loss: 0.001361, mae: 0.040964, mean_q: 1.169672
 29469/100000: episode: 401, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.557, mean reward: 0.586 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.297, 10.165], loss: 0.001272, mae: 0.039486, mean_q: 1.169088
 29569/100000: episode: 402, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.588, mean reward: 0.586 [0.505, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.903, 10.258], loss: 0.001408, mae: 0.040630, mean_q: 1.169047
 29669/100000: episode: 403, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.914, mean reward: 0.579 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.404, 10.098], loss: 0.001550, mae: 0.042000, mean_q: 1.167913
 29769/100000: episode: 404, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 60.361, mean reward: 0.604 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.625, 10.098], loss: 0.001389, mae: 0.041093, mean_q: 1.169022
 29869/100000: episode: 405, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.596, mean reward: 0.586 [0.508, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.098, 10.098], loss: 0.001406, mae: 0.041182, mean_q: 1.168205
 29969/100000: episode: 406, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.342, mean reward: 0.573 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.218, 10.098], loss: 0.001327, mae: 0.039994, mean_q: 1.167557
 30069/100000: episode: 407, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 60.273, mean reward: 0.603 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.623, 10.425], loss: 0.001299, mae: 0.039361, mean_q: 1.165700
 30169/100000: episode: 408, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 58.237, mean reward: 0.582 [0.511, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.351, 10.192], loss: 0.001323, mae: 0.039902, mean_q: 1.164074
 30269/100000: episode: 409, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.306, mean reward: 0.583 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.211, 10.098], loss: 0.001293, mae: 0.039824, mean_q: 1.165642
 30369/100000: episode: 410, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 63.155, mean reward: 0.632 [0.509, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.045, 10.098], loss: 0.001401, mae: 0.041055, mean_q: 1.168693
 30469/100000: episode: 411, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.498, mean reward: 0.585 [0.512, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.189, 10.098], loss: 0.001432, mae: 0.041154, mean_q: 1.166788
 30569/100000: episode: 412, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.146, mean reward: 0.591 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.547, 10.098], loss: 0.001466, mae: 0.041935, mean_q: 1.165645
 30669/100000: episode: 413, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.713, mean reward: 0.587 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.567, 10.210], loss: 0.001518, mae: 0.042545, mean_q: 1.170029
 30769/100000: episode: 414, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 57.754, mean reward: 0.578 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.921, 10.098], loss: 0.001355, mae: 0.041082, mean_q: 1.165074
 30869/100000: episode: 415, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.167, mean reward: 0.582 [0.505, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.858, 10.100], loss: 0.001364, mae: 0.040984, mean_q: 1.166749
 30969/100000: episode: 416, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.140, mean reward: 0.571 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.826, 10.098], loss: 0.001557, mae: 0.042971, mean_q: 1.166086
 31069/100000: episode: 417, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.895, mean reward: 0.599 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.505, 10.098], loss: 0.001473, mae: 0.041915, mean_q: 1.165955
 31169/100000: episode: 418, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.215, mean reward: 0.602 [0.498, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.939, 10.098], loss: 0.001421, mae: 0.041577, mean_q: 1.164305
 31269/100000: episode: 419, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 60.759, mean reward: 0.608 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.517, 10.101], loss: 0.001364, mae: 0.040710, mean_q: 1.166641
 31369/100000: episode: 420, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 59.378, mean reward: 0.594 [0.503, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.660, 10.098], loss: 0.001276, mae: 0.039687, mean_q: 1.164738
 31469/100000: episode: 421, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.770, mean reward: 0.578 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.674, 10.318], loss: 0.001380, mae: 0.040883, mean_q: 1.165941
 31569/100000: episode: 422, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.772, mean reward: 0.578 [0.508, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.994, 10.098], loss: 0.001295, mae: 0.039921, mean_q: 1.163352
 31669/100000: episode: 423, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 59.095, mean reward: 0.591 [0.510, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.665, 10.322], loss: 0.001347, mae: 0.040329, mean_q: 1.163534
 31769/100000: episode: 424, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 61.075, mean reward: 0.611 [0.515, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.505, 10.098], loss: 0.001344, mae: 0.040024, mean_q: 1.164128
 31869/100000: episode: 425, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 59.448, mean reward: 0.594 [0.517, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.550, 10.233], loss: 0.001410, mae: 0.041406, mean_q: 1.164914
 31969/100000: episode: 426, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.518, mean reward: 0.575 [0.500, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.077, 10.116], loss: 0.001388, mae: 0.040899, mean_q: 1.166180
 32069/100000: episode: 427, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 60.700, mean reward: 0.607 [0.507, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.220, 10.098], loss: 0.001424, mae: 0.041471, mean_q: 1.167501
 32169/100000: episode: 428, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 62.302, mean reward: 0.623 [0.512, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.793, 10.098], loss: 0.001557, mae: 0.043125, mean_q: 1.170050
[Info] 1-TH LEVEL FOUND: 1.343430757522583, Considering 10/90 traces
 32269/100000: episode: 429, duration: 4.706s, episode steps: 100, steps per second: 21, episode reward: 61.568, mean reward: 0.616 [0.514, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.449, 10.443], loss: 0.001351, mae: 0.040480, mean_q: 1.168411
 32354/100000: episode: 430, duration: 0.472s, episode steps: 85, steps per second: 180, episode reward: 49.083, mean reward: 0.577 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.592 [-0.953, 10.100], loss: 0.001468, mae: 0.042557, mean_q: 1.168633
 32439/100000: episode: 431, duration: 0.465s, episode steps: 85, steps per second: 183, episode reward: 50.947, mean reward: 0.599 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.588 [-0.784, 10.100], loss: 0.001407, mae: 0.041861, mean_q: 1.171012
 32468/100000: episode: 432, duration: 0.168s, episode steps: 29, steps per second: 173, episode reward: 18.354, mean reward: 0.633 [0.529, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.404, 10.284], loss: 0.001494, mae: 0.042257, mean_q: 1.169843
 32518/100000: episode: 433, duration: 0.283s, episode steps: 50, steps per second: 176, episode reward: 36.993, mean reward: 0.740 [0.634, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.523, 10.533], loss: 0.001483, mae: 0.042192, mean_q: 1.173000
 32539/100000: episode: 434, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 13.748, mean reward: 0.655 [0.536, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.893, 10.367], loss: 0.001587, mae: 0.043111, mean_q: 1.169796
 32568/100000: episode: 435, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 18.624, mean reward: 0.642 [0.570, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.035, 10.355], loss: 0.001513, mae: 0.042615, mean_q: 1.172537
 32593/100000: episode: 436, duration: 0.153s, episode steps: 25, steps per second: 163, episode reward: 17.095, mean reward: 0.684 [0.594, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.145, 10.376], loss: 0.001541, mae: 0.041713, mean_q: 1.177557
 32618/100000: episode: 437, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 17.058, mean reward: 0.682 [0.612, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.443], loss: 0.001463, mae: 0.041633, mean_q: 1.172691
 32634/100000: episode: 438, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 11.489, mean reward: 0.718 [0.636, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.256, 10.100], loss: 0.001442, mae: 0.041830, mean_q: 1.178327
 32660/100000: episode: 439, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 18.555, mean reward: 0.714 [0.668, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.256, 10.374], loss: 0.001610, mae: 0.043838, mean_q: 1.179122
 32681/100000: episode: 440, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 14.389, mean reward: 0.685 [0.626, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.576, 10.318], loss: 0.001549, mae: 0.042229, mean_q: 1.180924
 32766/100000: episode: 441, duration: 0.481s, episode steps: 85, steps per second: 177, episode reward: 51.406, mean reward: 0.605 [0.509, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.600 [-0.783, 10.226], loss: 0.001717, mae: 0.044346, mean_q: 1.175938
 32792/100000: episode: 442, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 16.872, mean reward: 0.649 [0.571, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.196, 10.392], loss: 0.001537, mae: 0.042554, mean_q: 1.176725
 32818/100000: episode: 443, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 18.497, mean reward: 0.711 [0.649, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.254, 10.387], loss: 0.001425, mae: 0.040204, mean_q: 1.175611
 32843/100000: episode: 444, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 16.424, mean reward: 0.657 [0.574, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.003, 10.306], loss: 0.001601, mae: 0.042483, mean_q: 1.181921
 32855/100000: episode: 445, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 8.216, mean reward: 0.685 [0.648, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.194, 10.100], loss: 0.001606, mae: 0.040108, mean_q: 1.174259
 32867/100000: episode: 446, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 7.663, mean reward: 0.639 [0.593, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.204, 10.100], loss: 0.001579, mae: 0.043266, mean_q: 1.178291
 32896/100000: episode: 447, duration: 0.169s, episode steps: 29, steps per second: 172, episode reward: 17.530, mean reward: 0.604 [0.517, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.127, 10.100], loss: 0.001559, mae: 0.042639, mean_q: 1.183872
 32946/100000: episode: 448, duration: 0.285s, episode steps: 50, steps per second: 176, episode reward: 29.981, mean reward: 0.600 [0.509, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.372, 10.100], loss: 0.001523, mae: 0.043111, mean_q: 1.178341
 32967/100000: episode: 449, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 13.787, mean reward: 0.657 [0.616, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.323, 10.394], loss: 0.001622, mae: 0.043620, mean_q: 1.190880
 32996/100000: episode: 450, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 19.438, mean reward: 0.670 [0.613, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.425, 10.376], loss: 0.001534, mae: 0.043296, mean_q: 1.174596
 33012/100000: episode: 451, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 11.316, mean reward: 0.707 [0.630, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.336, 10.100], loss: 0.001544, mae: 0.043164, mean_q: 1.187531
 33033/100000: episode: 452, duration: 0.123s, episode steps: 21, steps per second: 170, episode reward: 14.965, mean reward: 0.713 [0.647, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.879, 10.524], loss: 0.001863, mae: 0.046129, mean_q: 1.182340
 33054/100000: episode: 453, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 14.925, mean reward: 0.711 [0.641, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.203, 10.277], loss: 0.001772, mae: 0.046206, mean_q: 1.178601
 33080/100000: episode: 454, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 19.205, mean reward: 0.739 [0.676, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.753, 10.509], loss: 0.001818, mae: 0.045911, mean_q: 1.181008
 33106/100000: episode: 455, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 18.178, mean reward: 0.699 [0.617, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.471], loss: 0.001830, mae: 0.046130, mean_q: 1.184592
 33122/100000: episode: 456, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 11.542, mean reward: 0.721 [0.694, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.288, 10.100], loss: 0.001890, mae: 0.047463, mean_q: 1.197555
 33207/100000: episode: 457, duration: 0.479s, episode steps: 85, steps per second: 178, episode reward: 49.979, mean reward: 0.588 [0.508, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.591 [-0.607, 10.100], loss: 0.001622, mae: 0.043490, mean_q: 1.182162
 33298/100000: episode: 458, duration: 0.517s, episode steps: 91, steps per second: 176, episode reward: 55.142, mean reward: 0.606 [0.512, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-0.870, 10.144], loss: 0.001648, mae: 0.043297, mean_q: 1.187764
 33389/100000: episode: 459, duration: 0.506s, episode steps: 91, steps per second: 180, episode reward: 54.422, mean reward: 0.598 [0.514, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.376, 10.100], loss: 0.001679, mae: 0.044008, mean_q: 1.184229
 33415/100000: episode: 460, duration: 0.156s, episode steps: 26, steps per second: 166, episode reward: 17.955, mean reward: 0.691 [0.640, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.430], loss: 0.001546, mae: 0.042898, mean_q: 1.189499
 33436/100000: episode: 461, duration: 0.131s, episode steps: 21, steps per second: 161, episode reward: 14.532, mean reward: 0.692 [0.651, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.373, 10.419], loss: 0.001427, mae: 0.041544, mean_q: 1.184941
 33448/100000: episode: 462, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 8.272, mean reward: 0.689 [0.643, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.448, 10.100], loss: 0.001717, mae: 0.042631, mean_q: 1.175936
 33469/100000: episode: 463, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 13.577, mean reward: 0.647 [0.563, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.312], loss: 0.001670, mae: 0.044818, mean_q: 1.186353
 33495/100000: episode: 464, duration: 0.155s, episode steps: 26, steps per second: 167, episode reward: 16.698, mean reward: 0.642 [0.559, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.305, 10.319], loss: 0.001518, mae: 0.041958, mean_q: 1.185990
 33545/100000: episode: 465, duration: 0.282s, episode steps: 50, steps per second: 177, episode reward: 31.647, mean reward: 0.633 [0.556, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.857, 10.216], loss: 0.001561, mae: 0.043209, mean_q: 1.183138
 33566/100000: episode: 466, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 15.275, mean reward: 0.727 [0.685, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.419, 10.473], loss: 0.001560, mae: 0.042229, mean_q: 1.187018
 33587/100000: episode: 467, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 13.199, mean reward: 0.629 [0.572, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.146, 10.304], loss: 0.001565, mae: 0.042481, mean_q: 1.185449
 33599/100000: episode: 468, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 7.920, mean reward: 0.660 [0.608, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.660, 10.100], loss: 0.001608, mae: 0.042918, mean_q: 1.198523
 33628/100000: episode: 469, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 19.686, mean reward: 0.679 [0.594, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.400], loss: 0.001346, mae: 0.039412, mean_q: 1.186386
 33654/100000: episode: 470, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 17.240, mean reward: 0.663 [0.579, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.805, 10.293], loss: 0.001569, mae: 0.042076, mean_q: 1.189865
 33680/100000: episode: 471, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 17.344, mean reward: 0.667 [0.555, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.632, 10.211], loss: 0.001512, mae: 0.040951, mean_q: 1.194634
 33701/100000: episode: 472, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 15.304, mean reward: 0.729 [0.669, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.042, 10.444], loss: 0.001931, mae: 0.045332, mean_q: 1.187849
 33726/100000: episode: 473, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 14.591, mean reward: 0.584 [0.507, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.592, 10.178], loss: 0.001501, mae: 0.040964, mean_q: 1.202804
 33747/100000: episode: 474, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 14.944, mean reward: 0.712 [0.601, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.186, 10.184], loss: 0.001680, mae: 0.044662, mean_q: 1.186908
 33797/100000: episode: 475, duration: 0.288s, episode steps: 50, steps per second: 174, episode reward: 35.352, mean reward: 0.707 [0.646, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.957, 10.349], loss: 0.001549, mae: 0.042061, mean_q: 1.193937
 33888/100000: episode: 476, duration: 0.513s, episode steps: 91, steps per second: 177, episode reward: 53.075, mean reward: 0.583 [0.504, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.535 [-0.819, 10.100], loss: 0.001500, mae: 0.042119, mean_q: 1.195277
 33979/100000: episode: 477, duration: 0.508s, episode steps: 91, steps per second: 179, episode reward: 53.594, mean reward: 0.589 [0.508, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.624, 10.190], loss: 0.001642, mae: 0.044351, mean_q: 1.192799
 34070/100000: episode: 478, duration: 0.545s, episode steps: 91, steps per second: 167, episode reward: 53.877, mean reward: 0.592 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-0.236, 10.236], loss: 0.001570, mae: 0.042684, mean_q: 1.191514
 34161/100000: episode: 479, duration: 0.522s, episode steps: 91, steps per second: 174, episode reward: 52.914, mean reward: 0.581 [0.499, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-1.785, 10.285], loss: 0.001649, mae: 0.043854, mean_q: 1.195735
 34187/100000: episode: 480, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 18.276, mean reward: 0.703 [0.615, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.307, 10.365], loss: 0.001467, mae: 0.040653, mean_q: 1.192124
 34203/100000: episode: 481, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 12.503, mean reward: 0.781 [0.688, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.465, 10.100], loss: 0.001879, mae: 0.045140, mean_q: 1.188881
 34219/100000: episode: 482, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 10.840, mean reward: 0.678 [0.633, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.510, 10.100], loss: 0.001927, mae: 0.045504, mean_q: 1.187644
 34245/100000: episode: 483, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 16.761, mean reward: 0.645 [0.507, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.204, 10.114], loss: 0.001621, mae: 0.042962, mean_q: 1.184811
 34266/100000: episode: 484, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 15.214, mean reward: 0.724 [0.665, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.658, 10.595], loss: 0.001824, mae: 0.046112, mean_q: 1.193438
 34278/100000: episode: 485, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 7.987, mean reward: 0.666 [0.627, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.365, 10.100], loss: 0.001654, mae: 0.045404, mean_q: 1.210426
 34294/100000: episode: 486, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 11.138, mean reward: 0.696 [0.655, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.983, 10.100], loss: 0.001706, mae: 0.044339, mean_q: 1.190256
 34310/100000: episode: 487, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 11.225, mean reward: 0.702 [0.660, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.296, 10.100], loss: 0.001458, mae: 0.041952, mean_q: 1.210433
 34322/100000: episode: 488, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 8.589, mean reward: 0.716 [0.636, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.466, 10.100], loss: 0.001638, mae: 0.041986, mean_q: 1.194781
 34348/100000: episode: 489, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 17.111, mean reward: 0.658 [0.555, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.322, 10.219], loss: 0.001611, mae: 0.042790, mean_q: 1.194875
 34398/100000: episode: 490, duration: 0.277s, episode steps: 50, steps per second: 180, episode reward: 30.165, mean reward: 0.603 [0.515, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.478, 10.100], loss: 0.001646, mae: 0.043926, mean_q: 1.194727
 34424/100000: episode: 491, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 18.877, mean reward: 0.726 [0.656, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.732, 10.422], loss: 0.001355, mae: 0.040825, mean_q: 1.195960
 34445/100000: episode: 492, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 13.480, mean reward: 0.642 [0.539, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.277], loss: 0.001347, mae: 0.040233, mean_q: 1.198507
 34495/100000: episode: 493, duration: 0.291s, episode steps: 50, steps per second: 172, episode reward: 33.192, mean reward: 0.664 [0.588, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.389, 10.369], loss: 0.001643, mae: 0.043678, mean_q: 1.200768
 34516/100000: episode: 494, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 14.551, mean reward: 0.693 [0.642, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.365], loss: 0.001687, mae: 0.044764, mean_q: 1.200559
 34528/100000: episode: 495, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 8.434, mean reward: 0.703 [0.642, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.417, 10.100], loss: 0.001495, mae: 0.044249, mean_q: 1.220418
 34544/100000: episode: 496, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 11.045, mean reward: 0.690 [0.620, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.200, 10.100], loss: 0.001544, mae: 0.043814, mean_q: 1.216259
 34594/100000: episode: 497, duration: 0.297s, episode steps: 50, steps per second: 168, episode reward: 36.366, mean reward: 0.727 [0.561, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-1.144, 10.602], loss: 0.001696, mae: 0.045248, mean_q: 1.211210
 34610/100000: episode: 498, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 11.871, mean reward: 0.742 [0.686, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.549, 10.100], loss: 0.001236, mae: 0.039331, mean_q: 1.206519
 34695/100000: episode: 499, duration: 0.479s, episode steps: 85, steps per second: 178, episode reward: 52.680, mean reward: 0.620 [0.499, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.600 [-1.193, 10.391], loss: 0.001631, mae: 0.043658, mean_q: 1.204904
 34707/100000: episode: 500, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 7.729, mean reward: 0.644 [0.621, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.225, 10.100], loss: 0.001662, mae: 0.045255, mean_q: 1.204409
 34728/100000: episode: 501, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 13.764, mean reward: 0.655 [0.600, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.155, 10.343], loss: 0.001772, mae: 0.044854, mean_q: 1.208068
 34819/100000: episode: 502, duration: 0.518s, episode steps: 91, steps per second: 176, episode reward: 53.889, mean reward: 0.592 [0.517, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.542 [-0.612, 10.264], loss: 0.001714, mae: 0.044337, mean_q: 1.210387
 34904/100000: episode: 503, duration: 0.487s, episode steps: 85, steps per second: 175, episode reward: 50.682, mean reward: 0.596 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.594 [-0.821, 10.295], loss: 0.001671, mae: 0.043679, mean_q: 1.208198
 34925/100000: episode: 504, duration: 0.118s, episode steps: 21, steps per second: 177, episode reward: 15.428, mean reward: 0.735 [0.678, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.539], loss: 0.001641, mae: 0.043705, mean_q: 1.210736
 34950/100000: episode: 505, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 16.070, mean reward: 0.643 [0.574, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.028, 10.424], loss: 0.001912, mae: 0.046309, mean_q: 1.206989
 34966/100000: episode: 506, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 11.340, mean reward: 0.709 [0.656, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.765, 10.100], loss: 0.001878, mae: 0.044778, mean_q: 1.220126
 35016/100000: episode: 507, duration: 0.290s, episode steps: 50, steps per second: 172, episode reward: 31.541, mean reward: 0.631 [0.551, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.376, 10.301], loss: 0.002007, mae: 0.047826, mean_q: 1.212468
 35107/100000: episode: 508, duration: 0.518s, episode steps: 91, steps per second: 176, episode reward: 57.050, mean reward: 0.627 [0.510, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.766, 10.100], loss: 0.001641, mae: 0.043313, mean_q: 1.207713
 35192/100000: episode: 509, duration: 0.483s, episode steps: 85, steps per second: 176, episode reward: 49.369, mean reward: 0.581 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.603 [-0.620, 10.100], loss: 0.001522, mae: 0.042761, mean_q: 1.218998
 35213/100000: episode: 510, duration: 0.123s, episode steps: 21, steps per second: 170, episode reward: 14.565, mean reward: 0.694 [0.648, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.437, 10.421], loss: 0.001490, mae: 0.041764, mean_q: 1.203235
 35263/100000: episode: 511, duration: 0.280s, episode steps: 50, steps per second: 178, episode reward: 36.929, mean reward: 0.739 [0.597, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.453, 10.298], loss: 0.001500, mae: 0.042215, mean_q: 1.217585
 35275/100000: episode: 512, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 7.882, mean reward: 0.657 [0.610, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.418, 10.100], loss: 0.001556, mae: 0.042164, mean_q: 1.208805
 35304/100000: episode: 513, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 19.779, mean reward: 0.682 [0.625, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.715, 10.386], loss: 0.001481, mae: 0.042637, mean_q: 1.220250
 35316/100000: episode: 514, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 8.229, mean reward: 0.686 [0.643, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.398, 10.100], loss: 0.001637, mae: 0.043966, mean_q: 1.216401
 35341/100000: episode: 515, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 15.931, mean reward: 0.637 [0.592, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.188, 10.345], loss: 0.001854, mae: 0.045760, mean_q: 1.220709
 35366/100000: episode: 516, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 16.232, mean reward: 0.649 [0.582, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.973, 10.334], loss: 0.001754, mae: 0.044122, mean_q: 1.227306
 35451/100000: episode: 517, duration: 0.481s, episode steps: 85, steps per second: 177, episode reward: 48.556, mean reward: 0.571 [0.499, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.596 [-0.301, 10.100], loss: 0.001667, mae: 0.043845, mean_q: 1.214963
 35463/100000: episode: 518, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 7.911, mean reward: 0.659 [0.623, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.235, 10.100], loss: 0.001390, mae: 0.041689, mean_q: 1.213999
[Info] 2-TH LEVEL FOUND: 1.4742462635040283, Considering 10/90 traces
 35548/100000: episode: 519, duration: 4.666s, episode steps: 85, steps per second: 18, episode reward: 51.644, mean reward: 0.608 [0.532, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.597 [-1.346, 10.262], loss: 0.001523, mae: 0.042321, mean_q: 1.219155
 35596/100000: episode: 520, duration: 0.270s, episode steps: 48, steps per second: 178, episode reward: 30.237, mean reward: 0.630 [0.544, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.511, 10.251], loss: 0.001676, mae: 0.043621, mean_q: 1.219124
 35645/100000: episode: 521, duration: 0.283s, episode steps: 49, steps per second: 173, episode reward: 33.706, mean reward: 0.688 [0.545, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.348, 10.409], loss: 0.001587, mae: 0.042692, mean_q: 1.221308
 35694/100000: episode: 522, duration: 0.285s, episode steps: 49, steps per second: 172, episode reward: 34.726, mean reward: 0.709 [0.613, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-1.061, 10.315], loss: 0.001464, mae: 0.041763, mean_q: 1.226981
 35743/100000: episode: 523, duration: 0.278s, episode steps: 49, steps per second: 176, episode reward: 32.966, mean reward: 0.673 [0.592, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.364, 10.307], loss: 0.001468, mae: 0.042359, mean_q: 1.223991
 35756/100000: episode: 524, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 8.934, mean reward: 0.687 [0.603, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.716, 10.349], loss: 0.001445, mae: 0.041812, mean_q: 1.207380
 35805/100000: episode: 525, duration: 0.277s, episode steps: 49, steps per second: 177, episode reward: 31.400, mean reward: 0.641 [0.522, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.835, 10.100], loss: 0.001605, mae: 0.042971, mean_q: 1.222100
 35853/100000: episode: 526, duration: 0.282s, episode steps: 48, steps per second: 170, episode reward: 31.182, mean reward: 0.650 [0.544, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-1.233, 10.276], loss: 0.001389, mae: 0.041079, mean_q: 1.223257
 35902/100000: episode: 527, duration: 0.277s, episode steps: 49, steps per second: 177, episode reward: 33.509, mean reward: 0.684 [0.569, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.891, 10.239], loss: 0.001421, mae: 0.041162, mean_q: 1.232990
 35951/100000: episode: 528, duration: 0.274s, episode steps: 49, steps per second: 179, episode reward: 36.552, mean reward: 0.746 [0.626, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.566, 10.456], loss: 0.001531, mae: 0.042894, mean_q: 1.233063
 36000/100000: episode: 529, duration: 0.284s, episode steps: 49, steps per second: 173, episode reward: 30.244, mean reward: 0.617 [0.515, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.574, 10.100], loss: 0.001716, mae: 0.044421, mean_q: 1.230681
 36049/100000: episode: 530, duration: 0.278s, episode steps: 49, steps per second: 177, episode reward: 30.082, mean reward: 0.614 [0.512, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.592, 10.100], loss: 0.001624, mae: 0.044097, mean_q: 1.234626
 36098/100000: episode: 531, duration: 0.274s, episode steps: 49, steps per second: 179, episode reward: 34.565, mean reward: 0.705 [0.577, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.393, 10.342], loss: 0.001408, mae: 0.041150, mean_q: 1.231770
 36147/100000: episode: 532, duration: 0.280s, episode steps: 49, steps per second: 175, episode reward: 33.284, mean reward: 0.679 [0.585, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.372, 10.309], loss: 0.001426, mae: 0.041398, mean_q: 1.232759
 36196/100000: episode: 533, duration: 0.273s, episode steps: 49, steps per second: 179, episode reward: 32.335, mean reward: 0.660 [0.549, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-1.063, 10.320], loss: 0.001576, mae: 0.042818, mean_q: 1.235185
 36245/100000: episode: 534, duration: 0.276s, episode steps: 49, steps per second: 178, episode reward: 30.080, mean reward: 0.614 [0.512, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.644, 10.100], loss: 0.001393, mae: 0.040753, mean_q: 1.239191
 36294/100000: episode: 535, duration: 0.273s, episode steps: 49, steps per second: 180, episode reward: 32.233, mean reward: 0.658 [0.548, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.967, 10.207], loss: 0.001544, mae: 0.043294, mean_q: 1.238668
 36343/100000: episode: 536, duration: 0.281s, episode steps: 49, steps per second: 175, episode reward: 32.251, mean reward: 0.658 [0.551, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.990, 10.282], loss: 0.001690, mae: 0.044318, mean_q: 1.232185
 36392/100000: episode: 537, duration: 0.286s, episode steps: 49, steps per second: 172, episode reward: 31.377, mean reward: 0.640 [0.521, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.003, 10.108], loss: 0.001432, mae: 0.041868, mean_q: 1.246610
 36441/100000: episode: 538, duration: 0.284s, episode steps: 49, steps per second: 173, episode reward: 30.644, mean reward: 0.625 [0.505, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.352, 10.103], loss: 0.001566, mae: 0.042906, mean_q: 1.242162
 36490/100000: episode: 539, duration: 0.282s, episode steps: 49, steps per second: 174, episode reward: 33.551, mean reward: 0.685 [0.605, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.664, 10.424], loss: 0.001532, mae: 0.041917, mean_q: 1.235532
 36539/100000: episode: 540, duration: 0.278s, episode steps: 49, steps per second: 176, episode reward: 30.758, mean reward: 0.628 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-1.075, 10.225], loss: 0.001694, mae: 0.044364, mean_q: 1.250580
 36552/100000: episode: 541, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 10.123, mean reward: 0.779 [0.730, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.573], loss: 0.001491, mae: 0.042815, mean_q: 1.248485
 36565/100000: episode: 542, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 10.300, mean reward: 0.792 [0.750, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.553], loss: 0.001740, mae: 0.043202, mean_q: 1.231071
 36614/100000: episode: 543, duration: 0.276s, episode steps: 49, steps per second: 177, episode reward: 33.497, mean reward: 0.684 [0.595, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.357, 10.394], loss: 0.001503, mae: 0.042341, mean_q: 1.249020
 36663/100000: episode: 544, duration: 0.270s, episode steps: 49, steps per second: 182, episode reward: 32.355, mean reward: 0.660 [0.510, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.391, 10.139], loss: 0.001433, mae: 0.041301, mean_q: 1.251642
 36711/100000: episode: 545, duration: 0.273s, episode steps: 48, steps per second: 176, episode reward: 33.537, mean reward: 0.699 [0.585, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.626, 10.267], loss: 0.001428, mae: 0.041388, mean_q: 1.251122
 36760/100000: episode: 546, duration: 0.275s, episode steps: 49, steps per second: 178, episode reward: 30.327, mean reward: 0.619 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-1.193, 10.125], loss: 0.001550, mae: 0.042528, mean_q: 1.253317
 36809/100000: episode: 547, duration: 0.285s, episode steps: 49, steps per second: 172, episode reward: 32.147, mean reward: 0.656 [0.556, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.104, 10.297], loss: 0.001603, mae: 0.043543, mean_q: 1.245466
 36858/100000: episode: 548, duration: 0.291s, episode steps: 49, steps per second: 168, episode reward: 36.249, mean reward: 0.740 [0.574, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.585, 10.366], loss: 0.001664, mae: 0.044755, mean_q: 1.251035
 36907/100000: episode: 549, duration: 0.269s, episode steps: 49, steps per second: 182, episode reward: 31.004, mean reward: 0.633 [0.532, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.360, 10.246], loss: 0.001396, mae: 0.041085, mean_q: 1.256260
 36956/100000: episode: 550, duration: 0.265s, episode steps: 49, steps per second: 185, episode reward: 36.925, mean reward: 0.754 [0.663, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.499, 10.496], loss: 0.001436, mae: 0.041531, mean_q: 1.256005
 37005/100000: episode: 551, duration: 0.282s, episode steps: 49, steps per second: 174, episode reward: 32.194, mean reward: 0.657 [0.536, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.375, 10.185], loss: 0.001788, mae: 0.046058, mean_q: 1.252713
 37054/100000: episode: 552, duration: 0.284s, episode steps: 49, steps per second: 172, episode reward: 33.791, mean reward: 0.690 [0.592, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.352, 10.340], loss: 0.001403, mae: 0.040743, mean_q: 1.255325
 37103/100000: episode: 553, duration: 0.269s, episode steps: 49, steps per second: 182, episode reward: 32.253, mean reward: 0.658 [0.531, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.681, 10.100], loss: 0.001338, mae: 0.040729, mean_q: 1.261114
 37152/100000: episode: 554, duration: 0.283s, episode steps: 49, steps per second: 173, episode reward: 33.619, mean reward: 0.686 [0.546, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.577, 10.399], loss: 0.001409, mae: 0.041181, mean_q: 1.271849
 37201/100000: episode: 555, duration: 0.281s, episode steps: 49, steps per second: 175, episode reward: 33.896, mean reward: 0.692 [0.581, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.897, 10.250], loss: 0.001441, mae: 0.041284, mean_q: 1.262264
 37250/100000: episode: 556, duration: 0.297s, episode steps: 49, steps per second: 165, episode reward: 34.183, mean reward: 0.698 [0.552, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.352, 10.223], loss: 0.001458, mae: 0.042167, mean_q: 1.260962
 37299/100000: episode: 557, duration: 0.275s, episode steps: 49, steps per second: 178, episode reward: 33.042, mean reward: 0.674 [0.592, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.872, 10.306], loss: 0.001613, mae: 0.044742, mean_q: 1.269382
 37348/100000: episode: 558, duration: 0.278s, episode steps: 49, steps per second: 177, episode reward: 29.732, mean reward: 0.607 [0.499, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.365, 10.263], loss: 0.001414, mae: 0.041167, mean_q: 1.260530
 37361/100000: episode: 559, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 9.088, mean reward: 0.699 [0.655, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.484], loss: 0.001344, mae: 0.039987, mean_q: 1.286597
 37410/100000: episode: 560, duration: 0.278s, episode steps: 49, steps per second: 176, episode reward: 31.590, mean reward: 0.645 [0.529, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.343, 10.307], loss: 0.001346, mae: 0.040952, mean_q: 1.266883
 37459/100000: episode: 561, duration: 0.276s, episode steps: 49, steps per second: 178, episode reward: 32.328, mean reward: 0.660 [0.581, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.624, 10.277], loss: 0.001420, mae: 0.041331, mean_q: 1.265274
 37507/100000: episode: 562, duration: 0.270s, episode steps: 48, steps per second: 178, episode reward: 30.419, mean reward: 0.634 [0.554, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.351, 10.299], loss: 0.001400, mae: 0.041850, mean_q: 1.272772
 37556/100000: episode: 563, duration: 0.275s, episode steps: 49, steps per second: 178, episode reward: 34.552, mean reward: 0.705 [0.583, 0.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.393, 10.348], loss: 0.001416, mae: 0.041491, mean_q: 1.262497
 37569/100000: episode: 564, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 8.907, mean reward: 0.685 [0.633, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.350], loss: 0.001249, mae: 0.038672, mean_q: 1.271180
 37617/100000: episode: 565, duration: 0.273s, episode steps: 48, steps per second: 176, episode reward: 32.742, mean reward: 0.682 [0.583, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.692, 10.281], loss: 0.001281, mae: 0.039530, mean_q: 1.265999
 37666/100000: episode: 566, duration: 0.285s, episode steps: 49, steps per second: 172, episode reward: 33.875, mean reward: 0.691 [0.562, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.836, 10.386], loss: 0.001301, mae: 0.039742, mean_q: 1.272386
 37715/100000: episode: 567, duration: 0.287s, episode steps: 49, steps per second: 171, episode reward: 30.780, mean reward: 0.628 [0.552, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.249, 10.255], loss: 0.001331, mae: 0.040201, mean_q: 1.263481
 37764/100000: episode: 568, duration: 0.270s, episode steps: 49, steps per second: 182, episode reward: 32.618, mean reward: 0.666 [0.553, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.341, 10.549], loss: 0.001455, mae: 0.041767, mean_q: 1.262993
 37813/100000: episode: 569, duration: 0.268s, episode steps: 49, steps per second: 183, episode reward: 30.527, mean reward: 0.623 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.618, 10.102], loss: 0.001425, mae: 0.041698, mean_q: 1.267798
 37830/100000: episode: 570, duration: 0.105s, episode steps: 17, steps per second: 161, episode reward: 12.528, mean reward: 0.737 [0.642, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.385, 10.378], loss: 0.001165, mae: 0.038296, mean_q: 1.272191
 37879/100000: episode: 571, duration: 0.276s, episode steps: 49, steps per second: 177, episode reward: 33.406, mean reward: 0.682 [0.583, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.386, 10.452], loss: 0.001353, mae: 0.040008, mean_q: 1.266421
 37927/100000: episode: 572, duration: 0.282s, episode steps: 48, steps per second: 170, episode reward: 31.958, mean reward: 0.666 [0.598, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.353, 10.301], loss: 0.001411, mae: 0.040577, mean_q: 1.267033
 37976/100000: episode: 573, duration: 0.279s, episode steps: 49, steps per second: 176, episode reward: 34.051, mean reward: 0.695 [0.624, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.639, 10.357], loss: 0.001438, mae: 0.041915, mean_q: 1.266755
 37993/100000: episode: 574, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 12.020, mean reward: 0.707 [0.667, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.356], loss: 0.001570, mae: 0.042578, mean_q: 1.277163
 38042/100000: episode: 575, duration: 0.282s, episode steps: 49, steps per second: 173, episode reward: 33.658, mean reward: 0.687 [0.559, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.577, 10.416], loss: 0.001450, mae: 0.042403, mean_q: 1.278103
 38091/100000: episode: 576, duration: 0.277s, episode steps: 49, steps per second: 177, episode reward: 31.217, mean reward: 0.637 [0.528, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-1.321, 10.100], loss: 0.001489, mae: 0.043147, mean_q: 1.270114
 38140/100000: episode: 577, duration: 0.279s, episode steps: 49, steps per second: 176, episode reward: 31.169, mean reward: 0.636 [0.508, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-1.801, 10.147], loss: 0.001414, mae: 0.041171, mean_q: 1.275781
 38188/100000: episode: 578, duration: 0.272s, episode steps: 48, steps per second: 176, episode reward: 36.945, mean reward: 0.770 [0.677, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-1.087, 10.612], loss: 0.001419, mae: 0.040666, mean_q: 1.275839
 38237/100000: episode: 579, duration: 0.275s, episode steps: 49, steps per second: 178, episode reward: 32.375, mean reward: 0.661 [0.556, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.098, 10.219], loss: 0.001310, mae: 0.039313, mean_q: 1.273190
 38250/100000: episode: 580, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 9.605, mean reward: 0.739 [0.688, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.606, 10.474], loss: 0.001268, mae: 0.038545, mean_q: 1.283393
 38267/100000: episode: 581, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 12.141, mean reward: 0.714 [0.662, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.565, 10.413], loss: 0.001503, mae: 0.042770, mean_q: 1.287203
 38316/100000: episode: 582, duration: 0.282s, episode steps: 49, steps per second: 174, episode reward: 31.294, mean reward: 0.639 [0.561, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.358, 10.226], loss: 0.001428, mae: 0.041786, mean_q: 1.278193
 38333/100000: episode: 583, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 12.245, mean reward: 0.720 [0.680, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.963, 10.520], loss: 0.001407, mae: 0.040397, mean_q: 1.285079
 38350/100000: episode: 584, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 12.502, mean reward: 0.735 [0.659, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.352, 10.520], loss: 0.001346, mae: 0.040629, mean_q: 1.296493
 38398/100000: episode: 585, duration: 0.275s, episode steps: 48, steps per second: 174, episode reward: 33.563, mean reward: 0.699 [0.627, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.347, 10.403], loss: 0.001344, mae: 0.040119, mean_q: 1.275104
 38446/100000: episode: 586, duration: 0.272s, episode steps: 48, steps per second: 176, episode reward: 33.027, mean reward: 0.688 [0.550, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.730, 10.255], loss: 0.001388, mae: 0.040738, mean_q: 1.281422
 38463/100000: episode: 587, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 12.354, mean reward: 0.727 [0.686, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-1.196, 10.578], loss: 0.001442, mae: 0.041265, mean_q: 1.271936
 38476/100000: episode: 588, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 9.399, mean reward: 0.723 [0.649, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.432], loss: 0.001496, mae: 0.042840, mean_q: 1.287573
 38525/100000: episode: 589, duration: 0.276s, episode steps: 49, steps per second: 177, episode reward: 30.134, mean reward: 0.615 [0.506, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.374, 10.219], loss: 0.001303, mae: 0.038857, mean_q: 1.277122
 38574/100000: episode: 590, duration: 0.276s, episode steps: 49, steps per second: 177, episode reward: 32.971, mean reward: 0.673 [0.525, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-1.035, 10.100], loss: 0.001365, mae: 0.040133, mean_q: 1.279379
 38623/100000: episode: 591, duration: 0.285s, episode steps: 49, steps per second: 172, episode reward: 36.600, mean reward: 0.747 [0.634, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-1.597, 10.393], loss: 0.001348, mae: 0.039583, mean_q: 1.279499
 38672/100000: episode: 592, duration: 0.279s, episode steps: 49, steps per second: 175, episode reward: 34.556, mean reward: 0.705 [0.599, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.612, 10.376], loss: 0.001326, mae: 0.039628, mean_q: 1.276218
 38721/100000: episode: 593, duration: 0.270s, episode steps: 49, steps per second: 181, episode reward: 32.574, mean reward: 0.665 [0.532, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.370, 10.337], loss: 0.001356, mae: 0.040933, mean_q: 1.279757
 38770/100000: episode: 594, duration: 0.277s, episode steps: 49, steps per second: 177, episode reward: 36.398, mean reward: 0.743 [0.639, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.491, 10.521], loss: 0.001422, mae: 0.041838, mean_q: 1.275496
 38819/100000: episode: 595, duration: 0.294s, episode steps: 49, steps per second: 167, episode reward: 32.332, mean reward: 0.660 [0.515, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.377, 10.100], loss: 0.001341, mae: 0.040570, mean_q: 1.286053
 38868/100000: episode: 596, duration: 0.273s, episode steps: 49, steps per second: 180, episode reward: 31.753, mean reward: 0.648 [0.558, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.367, 10.208], loss: 0.001725, mae: 0.045892, mean_q: 1.285269
 38917/100000: episode: 597, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 35.366, mean reward: 0.722 [0.643, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.345, 10.460], loss: 0.001508, mae: 0.042607, mean_q: 1.289920
 38966/100000: episode: 598, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 34.200, mean reward: 0.698 [0.554, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.849, 10.200], loss: 0.001204, mae: 0.038613, mean_q: 1.282869
 39014/100000: episode: 599, duration: 0.284s, episode steps: 48, steps per second: 169, episode reward: 30.493, mean reward: 0.635 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.352, 10.221], loss: 0.001292, mae: 0.039994, mean_q: 1.287072
 39063/100000: episode: 600, duration: 0.279s, episode steps: 49, steps per second: 176, episode reward: 32.193, mean reward: 0.657 [0.569, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.418, 10.369], loss: 0.001298, mae: 0.039197, mean_q: 1.291881
 39112/100000: episode: 601, duration: 0.289s, episode steps: 49, steps per second: 170, episode reward: 31.815, mean reward: 0.649 [0.510, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.366, 10.133], loss: 0.001340, mae: 0.040030, mean_q: 1.290596
 39161/100000: episode: 602, duration: 0.277s, episode steps: 49, steps per second: 177, episode reward: 30.222, mean reward: 0.617 [0.523, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.639, 10.174], loss: 0.001257, mae: 0.038631, mean_q: 1.294770
 39209/100000: episode: 603, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 34.550, mean reward: 0.720 [0.593, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.350, 10.393], loss: 0.001201, mae: 0.037829, mean_q: 1.297493
 39258/100000: episode: 604, duration: 0.274s, episode steps: 49, steps per second: 179, episode reward: 31.136, mean reward: 0.635 [0.519, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.340, 10.157], loss: 0.001325, mae: 0.039920, mean_q: 1.290659
 39275/100000: episode: 605, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 12.455, mean reward: 0.733 [0.642, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.447], loss: 0.001354, mae: 0.040046, mean_q: 1.294619
 39324/100000: episode: 606, duration: 0.283s, episode steps: 49, steps per second: 173, episode reward: 30.438, mean reward: 0.621 [0.532, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.476, 10.276], loss: 0.001421, mae: 0.040927, mean_q: 1.289450
 39373/100000: episode: 607, duration: 0.294s, episode steps: 49, steps per second: 167, episode reward: 35.435, mean reward: 0.723 [0.645, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.396, 10.484], loss: 0.001280, mae: 0.038924, mean_q: 1.293421
 39422/100000: episode: 608, duration: 0.272s, episode steps: 49, steps per second: 180, episode reward: 32.886, mean reward: 0.671 [0.553, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.363, 10.202], loss: 0.001400, mae: 0.041222, mean_q: 1.289912
[Info] 3-TH LEVEL FOUND: 1.5533314943313599, Considering 10/90 traces
 39471/100000: episode: 609, duration: 4.457s, episode steps: 49, steps per second: 11, episode reward: 32.419, mean reward: 0.662 [0.560, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.408, 10.319], loss: 0.001412, mae: 0.041501, mean_q: 1.290309
 39489/100000: episode: 610, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 15.578, mean reward: 0.865 [0.747, 0.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.739, 10.556], loss: 0.001294, mae: 0.040217, mean_q: 1.291039
 39535/100000: episode: 611, duration: 0.265s, episode steps: 46, steps per second: 173, episode reward: 33.087, mean reward: 0.719 [0.576, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.352, 10.293], loss: 0.001341, mae: 0.039996, mean_q: 1.290194
[Info] FALSIFICATION!
 39536/100000: episode: 612, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 1.052, mean reward: 1.052 [1.052, 1.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.638 [-0.252, 6.865], loss: 0.001132, mae: 0.038082, mean_q: 1.326757
 39580/100000: episode: 613, duration: 0.240s, episode steps: 44, steps per second: 183, episode reward: 29.697, mean reward: 0.675 [0.549, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.302, 10.273], loss: 0.001456, mae: 0.040414, mean_q: 1.291422
 39624/100000: episode: 614, duration: 0.262s, episode steps: 44, steps per second: 168, episode reward: 29.105, mean reward: 0.661 [0.547, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.799, 10.196], loss: 0.001319, mae: 0.040296, mean_q: 1.298930
 39663/100000: episode: 615, duration: 0.226s, episode steps: 39, steps per second: 172, episode reward: 30.143, mean reward: 0.773 [0.634, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-1.414, 10.405], loss: 0.001530, mae: 0.042134, mean_q: 1.289822
 39683/100000: episode: 616, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 15.543, mean reward: 0.777 [0.661, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.046, 10.482], loss: 0.001139, mae: 0.038048, mean_q: 1.295420
 39719/100000: episode: 617, duration: 0.211s, episode steps: 36, steps per second: 171, episode reward: 23.346, mean reward: 0.648 [0.553, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.154, 10.234], loss: 0.001283, mae: 0.039568, mean_q: 1.303584
 39737/100000: episode: 618, duration: 0.109s, episode steps: 18, steps per second: 166, episode reward: 13.487, mean reward: 0.749 [0.671, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.902, 10.519], loss: 0.001209, mae: 0.038334, mean_q: 1.296695
 39751/100000: episode: 619, duration: 0.090s, episode steps: 14, steps per second: 156, episode reward: 11.310, mean reward: 0.808 [0.753, 0.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.451, 10.549], loss: 0.001354, mae: 0.039949, mean_q: 1.296059
 39770/100000: episode: 620, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 14.441, mean reward: 0.760 [0.655, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.203, 10.379], loss: 0.001629, mae: 0.043539, mean_q: 1.299677
 39806/100000: episode: 621, duration: 0.219s, episode steps: 36, steps per second: 164, episode reward: 25.156, mean reward: 0.699 [0.575, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.705, 10.247], loss: 0.001360, mae: 0.040924, mean_q: 1.303041
 39842/100000: episode: 622, duration: 0.211s, episode steps: 36, steps per second: 171, episode reward: 29.318, mean reward: 0.814 [0.759, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.263, 10.584], loss: 0.001582, mae: 0.042454, mean_q: 1.303235
 39860/100000: episode: 623, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 15.230, mean reward: 0.846 [0.802, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.728, 10.524], loss: 0.001175, mae: 0.038933, mean_q: 1.312893
 39880/100000: episode: 624, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 16.730, mean reward: 0.837 [0.761, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.218, 10.566], loss: 0.001334, mae: 0.040803, mean_q: 1.306011
 39900/100000: episode: 625, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 14.536, mean reward: 0.727 [0.681, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.418], loss: 0.001432, mae: 0.039692, mean_q: 1.306869
 39939/100000: episode: 626, duration: 0.231s, episode steps: 39, steps per second: 169, episode reward: 28.376, mean reward: 0.728 [0.607, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-1.627, 10.370], loss: 0.001320, mae: 0.040051, mean_q: 1.310877
 39953/100000: episode: 627, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 10.581, mean reward: 0.756 [0.694, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.287, 10.430], loss: 0.001383, mae: 0.040055, mean_q: 1.308377
 39971/100000: episode: 628, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 15.087, mean reward: 0.838 [0.715, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.192, 10.656], loss: 0.001215, mae: 0.039161, mean_q: 1.311875
 40007/100000: episode: 629, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 26.465, mean reward: 0.735 [0.658, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.180, 10.475], loss: 0.001250, mae: 0.040062, mean_q: 1.310765
 40043/100000: episode: 630, duration: 0.209s, episode steps: 36, steps per second: 172, episode reward: 28.056, mean reward: 0.779 [0.647, 0.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.477, 10.381], loss: 0.001311, mae: 0.039643, mean_q: 1.307257
 40082/100000: episode: 631, duration: 0.224s, episode steps: 39, steps per second: 174, episode reward: 30.382, mean reward: 0.779 [0.708, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.886, 10.511], loss: 0.001251, mae: 0.039468, mean_q: 1.319968
 40122/100000: episode: 632, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 27.512, mean reward: 0.688 [0.508, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.171, 10.202], loss: 0.001244, mae: 0.038271, mean_q: 1.322168
 40142/100000: episode: 633, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 15.331, mean reward: 0.767 [0.691, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.827, 10.567], loss: 0.001324, mae: 0.040119, mean_q: 1.322758
 40160/100000: episode: 634, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 12.999, mean reward: 0.722 [0.660, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.439], loss: 0.001461, mae: 0.042109, mean_q: 1.301627
 40174/100000: episode: 635, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 10.651, mean reward: 0.761 [0.732, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.381, 10.462], loss: 0.001331, mae: 0.040614, mean_q: 1.319599
 40192/100000: episode: 636, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 13.479, mean reward: 0.749 [0.703, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.609, 10.540], loss: 0.001253, mae: 0.038801, mean_q: 1.326501
 40206/100000: episode: 637, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 11.128, mean reward: 0.795 [0.723, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.921, 10.586], loss: 0.001259, mae: 0.038770, mean_q: 1.324811
 40220/100000: episode: 638, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 11.586, mean reward: 0.828 [0.743, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.424, 10.465], loss: 0.001467, mae: 0.043029, mean_q: 1.313795
 40234/100000: episode: 639, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 10.383, mean reward: 0.742 [0.694, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.461], loss: 0.001413, mae: 0.042449, mean_q: 1.327243
 40252/100000: episode: 640, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 14.725, mean reward: 0.818 [0.747, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.665], loss: 0.001661, mae: 0.041727, mean_q: 1.311426
 40270/100000: episode: 641, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 13.519, mean reward: 0.751 [0.683, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.639, 10.510], loss: 0.001552, mae: 0.042920, mean_q: 1.315325
 40306/100000: episode: 642, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 25.473, mean reward: 0.708 [0.612, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.082, 10.359], loss: 0.001715, mae: 0.043588, mean_q: 1.329075
 40342/100000: episode: 643, duration: 0.183s, episode steps: 36, steps per second: 196, episode reward: 27.621, mean reward: 0.767 [0.548, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.106, 10.334], loss: 0.001307, mae: 0.039254, mean_q: 1.336434
 40388/100000: episode: 644, duration: 0.238s, episode steps: 46, steps per second: 194, episode reward: 33.845, mean reward: 0.736 [0.649, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.355, 10.452], loss: 0.001415, mae: 0.041856, mean_q: 1.327360
 40432/100000: episode: 645, duration: 0.220s, episode steps: 44, steps per second: 200, episode reward: 28.637, mean reward: 0.651 [0.520, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.309, 10.100], loss: 0.001331, mae: 0.040725, mean_q: 1.337615
 40451/100000: episode: 646, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 15.718, mean reward: 0.827 [0.697, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.538], loss: 0.001367, mae: 0.040433, mean_q: 1.330390
 40487/100000: episode: 647, duration: 0.188s, episode steps: 36, steps per second: 192, episode reward: 24.471, mean reward: 0.680 [0.580, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.455, 10.372], loss: 0.001170, mae: 0.037748, mean_q: 1.331637
 40527/100000: episode: 648, duration: 0.222s, episode steps: 40, steps per second: 180, episode reward: 26.978, mean reward: 0.674 [0.584, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.417, 10.261], loss: 0.001225, mae: 0.038182, mean_q: 1.328379
 40546/100000: episode: 649, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 14.464, mean reward: 0.761 [0.701, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.478], loss: 0.001823, mae: 0.041199, mean_q: 1.325457
 40566/100000: episode: 650, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 17.381, mean reward: 0.869 [0.782, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.308, 10.575], loss: 0.001703, mae: 0.043379, mean_q: 1.326876
 40585/100000: episode: 651, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 15.427, mean reward: 0.812 [0.764, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.057, 10.599], loss: 0.001597, mae: 0.044842, mean_q: 1.334240
 40621/100000: episode: 652, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 21.906, mean reward: 0.609 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.086, 10.100], loss: 0.001370, mae: 0.041412, mean_q: 1.342486
 40661/100000: episode: 653, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 29.076, mean reward: 0.727 [0.573, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-1.769, 10.310], loss: 0.001618, mae: 0.044776, mean_q: 1.333639
 40679/100000: episode: 654, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 12.679, mean reward: 0.704 [0.574, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.512, 10.400], loss: 0.001185, mae: 0.038241, mean_q: 1.343614
 40697/100000: episode: 655, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 13.177, mean reward: 0.732 [0.640, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.383], loss: 0.001323, mae: 0.039843, mean_q: 1.326886
 40711/100000: episode: 656, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 10.820, mean reward: 0.773 [0.737, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.516], loss: 0.001292, mae: 0.040311, mean_q: 1.323341
 40725/100000: episode: 657, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 10.124, mean reward: 0.723 [0.657, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.696, 10.429], loss: 0.001398, mae: 0.040706, mean_q: 1.337113
 40743/100000: episode: 658, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 12.636, mean reward: 0.702 [0.631, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.318], loss: 0.001342, mae: 0.039394, mean_q: 1.339384
 40761/100000: episode: 659, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 13.267, mean reward: 0.737 [0.690, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-1.528, 10.414], loss: 0.001185, mae: 0.037948, mean_q: 1.335026
 40775/100000: episode: 660, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 11.053, mean reward: 0.790 [0.753, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.693, 10.572], loss: 0.001674, mae: 0.044785, mean_q: 1.345194
 40789/100000: episode: 661, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 11.170, mean reward: 0.798 [0.709, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.242, 10.490], loss: 0.001386, mae: 0.040344, mean_q: 1.337989
 40807/100000: episode: 662, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 14.418, mean reward: 0.801 [0.733, 0.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.246, 10.487], loss: 0.001313, mae: 0.038973, mean_q: 1.341555
 40826/100000: episode: 663, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 14.708, mean reward: 0.774 [0.709, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.414], loss: 0.001403, mae: 0.040051, mean_q: 1.336740
 40840/100000: episode: 664, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 10.164, mean reward: 0.726 [0.652, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-1.181, 10.442], loss: 0.001389, mae: 0.041150, mean_q: 1.335386
 40880/100000: episode: 665, duration: 0.210s, episode steps: 40, steps per second: 191, episode reward: 25.272, mean reward: 0.632 [0.511, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.090, 10.217], loss: 0.001294, mae: 0.038799, mean_q: 1.345881
 40899/100000: episode: 666, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 15.696, mean reward: 0.826 [0.749, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.964, 10.527], loss: 0.001460, mae: 0.041884, mean_q: 1.345165
 40939/100000: episode: 667, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 28.145, mean reward: 0.704 [0.544, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.205, 10.360], loss: 0.001432, mae: 0.040058, mean_q: 1.344744
 40975/100000: episode: 668, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 27.266, mean reward: 0.757 [0.628, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-1.296, 10.272], loss: 0.001543, mae: 0.040811, mean_q: 1.345474
 41019/100000: episode: 669, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 28.429, mean reward: 0.646 [0.518, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.332, 10.173], loss: 0.001399, mae: 0.040814, mean_q: 1.338332
 41058/100000: episode: 670, duration: 0.209s, episode steps: 39, steps per second: 187, episode reward: 28.498, mean reward: 0.731 [0.609, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.710, 10.328], loss: 0.001378, mae: 0.040971, mean_q: 1.340212
 41076/100000: episode: 671, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 13.807, mean reward: 0.767 [0.690, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.592, 10.403], loss: 0.001535, mae: 0.043190, mean_q: 1.340406
 41122/100000: episode: 672, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 30.379, mean reward: 0.660 [0.540, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.955, 10.219], loss: 0.001534, mae: 0.041533, mean_q: 1.350861
 41142/100000: episode: 673, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 16.250, mean reward: 0.812 [0.661, 0.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.135, 10.390], loss: 0.001429, mae: 0.041390, mean_q: 1.335821
 41188/100000: episode: 674, duration: 0.225s, episode steps: 46, steps per second: 205, episode reward: 32.111, mean reward: 0.698 [0.544, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.509, 10.297], loss: 0.001277, mae: 0.039275, mean_q: 1.339850
 41224/100000: episode: 675, duration: 0.176s, episode steps: 36, steps per second: 205, episode reward: 27.989, mean reward: 0.777 [0.708, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.757, 10.492], loss: 0.001343, mae: 0.041018, mean_q: 1.345984
 41270/100000: episode: 676, duration: 0.262s, episode steps: 46, steps per second: 176, episode reward: 34.749, mean reward: 0.755 [0.670, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.347, 10.517], loss: 0.001479, mae: 0.041584, mean_q: 1.352668
 41290/100000: episode: 677, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 15.927, mean reward: 0.796 [0.751, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.412, 10.583], loss: 0.001262, mae: 0.039660, mean_q: 1.354414
 41308/100000: episode: 678, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 14.562, mean reward: 0.809 [0.734, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.579, 10.501], loss: 0.001544, mae: 0.044166, mean_q: 1.366764
 41348/100000: episode: 679, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 29.133, mean reward: 0.728 [0.602, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.719, 10.379], loss: 0.001314, mae: 0.040423, mean_q: 1.359989
 41384/100000: episode: 680, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 25.500, mean reward: 0.708 [0.586, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.328, 10.321], loss: 0.001250, mae: 0.038972, mean_q: 1.350748
 41420/100000: episode: 681, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 25.714, mean reward: 0.714 [0.564, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.304, 10.236], loss: 0.001215, mae: 0.037520, mean_q: 1.348157
 41466/100000: episode: 682, duration: 0.225s, episode steps: 46, steps per second: 205, episode reward: 29.674, mean reward: 0.645 [0.519, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.372, 10.332], loss: 0.001380, mae: 0.040940, mean_q: 1.353630
 41510/100000: episode: 683, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 31.166, mean reward: 0.708 [0.571, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.756, 10.554], loss: 0.001203, mae: 0.038964, mean_q: 1.353311
 41524/100000: episode: 684, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 10.912, mean reward: 0.779 [0.720, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.780, 10.545], loss: 0.001585, mae: 0.043024, mean_q: 1.355423
 41563/100000: episode: 685, duration: 0.205s, episode steps: 39, steps per second: 191, episode reward: 24.997, mean reward: 0.641 [0.521, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.021, 10.233], loss: 0.001265, mae: 0.039333, mean_q: 1.361383
 41583/100000: episode: 686, duration: 0.113s, episode steps: 20, steps per second: 178, episode reward: 15.080, mean reward: 0.754 [0.657, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.214, 10.338], loss: 0.001159, mae: 0.038147, mean_q: 1.367305
 41601/100000: episode: 687, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 12.963, mean reward: 0.720 [0.617, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.210, 10.402], loss: 0.001100, mae: 0.036886, mean_q: 1.374402
 41619/100000: episode: 688, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 14.915, mean reward: 0.829 [0.758, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.582, 10.537], loss: 0.001132, mae: 0.037592, mean_q: 1.365934
 41655/100000: episode: 689, duration: 0.179s, episode steps: 36, steps per second: 202, episode reward: 28.539, mean reward: 0.793 [0.728, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.277, 10.624], loss: 0.001330, mae: 0.040550, mean_q: 1.353976
[Info] FALSIFICATION!
 41666/100000: episode: 690, duration: 0.342s, episode steps: 11, steps per second: 32, episode reward: 9.910, mean reward: 0.901 [0.819, 1.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.196, 10.117], loss: 0.001624, mae: 0.043580, mean_q: 1.350294
[Info] FALSIFICATION!
 41676/100000: episode: 691, duration: 0.241s, episode steps: 10, steps per second: 41, episode reward: 9.165, mean reward: 0.917 [0.866, 1.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.204, 10.386], loss: 0.001435, mae: 0.042004, mean_q: 1.360880
 41720/100000: episode: 692, duration: 0.219s, episode steps: 44, steps per second: 201, episode reward: 32.041, mean reward: 0.728 [0.639, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.291, 10.452], loss: 0.001842, mae: 0.046187, mean_q: 1.356319
 41766/100000: episode: 693, duration: 0.240s, episode steps: 46, steps per second: 192, episode reward: 31.020, mean reward: 0.674 [0.587, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.525, 10.243], loss: 0.001390, mae: 0.041544, mean_q: 1.354793
 41806/100000: episode: 694, duration: 0.207s, episode steps: 40, steps per second: 194, episode reward: 27.059, mean reward: 0.676 [0.550, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.743, 10.252], loss: 0.001615, mae: 0.041210, mean_q: 1.363051
 41846/100000: episode: 695, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 31.765, mean reward: 0.794 [0.669, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.275, 10.525], loss: 0.001596, mae: 0.041103, mean_q: 1.348623
[Info] FALSIFICATION!
 41847/100000: episode: 696, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 1.002, mean reward: 1.002 [1.002, 1.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.698 [-0.148, 7.253], loss: 0.001546, mae: 0.045452, mean_q: 1.354272
 41887/100000: episode: 697, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 26.344, mean reward: 0.659 [0.585, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.408, 10.311], loss: 0.002031, mae: 0.043670, mean_q: 1.356649
 41905/100000: episode: 698, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 14.509, mean reward: 0.806 [0.749, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.077, 10.606], loss: 0.001960, mae: 0.040149, mean_q: 1.348982
[Info] Complete ISplit Iteration
[Info] Levels: [1.3434308, 1.4742463, 1.5533315, 1.5343935]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.93]
[Info] Error Prob: 0.0009300000000000003

 41945/100000: episode: 699, duration: 4.566s, episode steps: 40, steps per second: 9, episode reward: 31.608, mean reward: 0.790 [0.708, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.611, 10.521], loss: 0.002104, mae: 0.042952, mean_q: 1.362950
 42045/100000: episode: 700, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 59.569, mean reward: 0.596 [0.502, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.542, 10.207], loss: 0.001438, mae: 0.041157, mean_q: 1.361028
 42145/100000: episode: 701, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 56.968, mean reward: 0.570 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.693, 10.098], loss: 0.001619, mae: 0.041197, mean_q: 1.355174
 42245/100000: episode: 702, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.007, mean reward: 0.580 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.344, 10.144], loss: 0.001542, mae: 0.040454, mean_q: 1.351407
 42345/100000: episode: 703, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 63.359, mean reward: 0.634 [0.521, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.750, 10.098], loss: 0.001480, mae: 0.042222, mean_q: 1.350511
 42445/100000: episode: 704, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 61.213, mean reward: 0.612 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.590, 10.355], loss: 0.001747, mae: 0.044209, mean_q: 1.349053
 42545/100000: episode: 705, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 64.248, mean reward: 0.642 [0.508, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.248, 10.523], loss: 0.001708, mae: 0.041582, mean_q: 1.345419
 42645/100000: episode: 706, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.876, mean reward: 0.589 [0.502, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.177, 10.153], loss: 0.001816, mae: 0.043956, mean_q: 1.355348
 42745/100000: episode: 707, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.602, mean reward: 0.596 [0.512, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.270, 10.098], loss: 0.001612, mae: 0.041745, mean_q: 1.343503
 42845/100000: episode: 708, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.195, mean reward: 0.582 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.870, 10.151], loss: 0.001567, mae: 0.040370, mean_q: 1.341089
 42945/100000: episode: 709, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 58.732, mean reward: 0.587 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.576, 10.098], loss: 0.001567, mae: 0.041844, mean_q: 1.341578
 43045/100000: episode: 710, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.931, mean reward: 0.589 [0.509, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.405, 10.098], loss: 0.002032, mae: 0.044929, mean_q: 1.336451
 43145/100000: episode: 711, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.709, mean reward: 0.587 [0.500, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.586, 10.098], loss: 0.001901, mae: 0.044483, mean_q: 1.332312
 43245/100000: episode: 712, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 58.863, mean reward: 0.589 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.558, 10.183], loss: 0.001405, mae: 0.040825, mean_q: 1.334430
 43345/100000: episode: 713, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.129, mean reward: 0.581 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.217, 10.098], loss: 0.001477, mae: 0.041027, mean_q: 1.324721
 43445/100000: episode: 714, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 65.789, mean reward: 0.658 [0.502, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.291, 10.585], loss: 0.001947, mae: 0.043876, mean_q: 1.322975
 43545/100000: episode: 715, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.712, mean reward: 0.597 [0.514, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.559, 10.098], loss: 0.001829, mae: 0.043388, mean_q: 1.320640
 43645/100000: episode: 716, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 62.767, mean reward: 0.628 [0.507, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.378, 10.098], loss: 0.001759, mae: 0.044677, mean_q: 1.316892
 43745/100000: episode: 717, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.617, mean reward: 0.586 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.689, 10.491], loss: 0.001791, mae: 0.042400, mean_q: 1.313886
 43845/100000: episode: 718, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.495, mean reward: 0.585 [0.513, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.292, 10.265], loss: 0.002295, mae: 0.047324, mean_q: 1.308854
 43945/100000: episode: 719, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 59.533, mean reward: 0.595 [0.499, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.817, 10.098], loss: 0.001777, mae: 0.043726, mean_q: 1.310779
 44045/100000: episode: 720, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.974, mean reward: 0.580 [0.504, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.071, 10.098], loss: 0.001916, mae: 0.044784, mean_q: 1.301405
 44145/100000: episode: 721, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.538, mean reward: 0.585 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.765, 10.192], loss: 0.002024, mae: 0.044724, mean_q: 1.309213
 44245/100000: episode: 722, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.710, mean reward: 0.587 [0.505, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.710, 10.125], loss: 0.001605, mae: 0.043313, mean_q: 1.301341
 44345/100000: episode: 723, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 61.164, mean reward: 0.612 [0.510, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.862, 10.326], loss: 0.001723, mae: 0.043164, mean_q: 1.301886
 44445/100000: episode: 724, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.113, mean reward: 0.581 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.709, 10.162], loss: 0.001860, mae: 0.044353, mean_q: 1.297494
 44545/100000: episode: 725, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 58.246, mean reward: 0.582 [0.504, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.292, 10.098], loss: 0.001621, mae: 0.042725, mean_q: 1.288338
 44645/100000: episode: 726, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.296, mean reward: 0.583 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.095, 10.169], loss: 0.001599, mae: 0.042799, mean_q: 1.281888
 44745/100000: episode: 727, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.608, mean reward: 0.576 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.213, 10.114], loss: 0.001594, mae: 0.041708, mean_q: 1.283190
 44845/100000: episode: 728, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.104, mean reward: 0.581 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.270, 10.098], loss: 0.001747, mae: 0.042850, mean_q: 1.277176
 44945/100000: episode: 729, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.060, mean reward: 0.581 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.868, 10.098], loss: 0.001608, mae: 0.043276, mean_q: 1.270220
 45045/100000: episode: 730, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 57.756, mean reward: 0.578 [0.506, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.368, 10.166], loss: 0.001469, mae: 0.040609, mean_q: 1.265207
 45145/100000: episode: 731, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 58.321, mean reward: 0.583 [0.509, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.958, 10.098], loss: 0.001838, mae: 0.043207, mean_q: 1.263269
 45245/100000: episode: 732, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.393, mean reward: 0.594 [0.510, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.487, 10.422], loss: 0.001684, mae: 0.043313, mean_q: 1.248827
 45345/100000: episode: 733, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.744, mean reward: 0.587 [0.498, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.765, 10.098], loss: 0.001622, mae: 0.042808, mean_q: 1.244897
 45445/100000: episode: 734, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.717, mean reward: 0.587 [0.506, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.083, 10.098], loss: 0.001737, mae: 0.043411, mean_q: 1.240293
 45545/100000: episode: 735, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 62.285, mean reward: 0.623 [0.507, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.788, 10.479], loss: 0.001703, mae: 0.043083, mean_q: 1.233434
 45645/100000: episode: 736, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.998, mean reward: 0.590 [0.504, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.922, 10.126], loss: 0.001728, mae: 0.042557, mean_q: 1.229052
 45745/100000: episode: 737, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.010, mean reward: 0.600 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.951, 10.098], loss: 0.001458, mae: 0.041268, mean_q: 1.227097
 45845/100000: episode: 738, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 56.866, mean reward: 0.569 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.681, 10.098], loss: 0.001519, mae: 0.040593, mean_q: 1.227849
 45945/100000: episode: 739, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.606, mean reward: 0.576 [0.503, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.180, 10.365], loss: 0.001649, mae: 0.042244, mean_q: 1.212934
 46045/100000: episode: 740, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 59.421, mean reward: 0.594 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.815, 10.098], loss: 0.001443, mae: 0.040375, mean_q: 1.213694
 46145/100000: episode: 741, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 56.790, mean reward: 0.568 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.376, 10.098], loss: 0.001544, mae: 0.041282, mean_q: 1.213574
 46245/100000: episode: 742, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.318, mean reward: 0.573 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.252, 10.199], loss: 0.001662, mae: 0.042724, mean_q: 1.208239
 46345/100000: episode: 743, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 57.156, mean reward: 0.572 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.493, 10.148], loss: 0.001836, mae: 0.042605, mean_q: 1.201011
 46445/100000: episode: 744, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.371, mean reward: 0.574 [0.499, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.807, 10.198], loss: 0.001470, mae: 0.041296, mean_q: 1.199093
 46545/100000: episode: 745, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 60.142, mean reward: 0.601 [0.506, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.591, 10.098], loss: 0.001619, mae: 0.042352, mean_q: 1.191731
 46645/100000: episode: 746, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 58.056, mean reward: 0.581 [0.504, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.291, 10.118], loss: 0.001555, mae: 0.041443, mean_q: 1.184763
 46745/100000: episode: 747, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.414, mean reward: 0.584 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.171, 10.118], loss: 0.001387, mae: 0.040940, mean_q: 1.180858
 46845/100000: episode: 748, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.050, mean reward: 0.571 [0.503, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.842, 10.098], loss: 0.001355, mae: 0.040193, mean_q: 1.172776
 46945/100000: episode: 749, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 61.796, mean reward: 0.618 [0.507, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.588, 10.098], loss: 0.001424, mae: 0.041338, mean_q: 1.170331
 47045/100000: episode: 750, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 58.502, mean reward: 0.585 [0.514, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.781, 10.098], loss: 0.001243, mae: 0.038671, mean_q: 1.169510
 47145/100000: episode: 751, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 61.555, mean reward: 0.616 [0.519, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.586, 10.412], loss: 0.001181, mae: 0.038148, mean_q: 1.170393
 47245/100000: episode: 752, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.895, mean reward: 0.579 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.987, 10.098], loss: 0.001219, mae: 0.038746, mean_q: 1.171279
 47345/100000: episode: 753, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.712, mean reward: 0.587 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.756, 10.304], loss: 0.001185, mae: 0.037864, mean_q: 1.171091
 47445/100000: episode: 754, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 59.001, mean reward: 0.590 [0.518, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.435, 10.136], loss: 0.001204, mae: 0.038113, mean_q: 1.166575
 47545/100000: episode: 755, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.645, mean reward: 0.576 [0.508, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.264, 10.098], loss: 0.001159, mae: 0.037854, mean_q: 1.164345
 47645/100000: episode: 756, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 57.565, mean reward: 0.576 [0.499, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.316, 10.098], loss: 0.001173, mae: 0.038020, mean_q: 1.166158
 47745/100000: episode: 757, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.301, mean reward: 0.583 [0.504, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.791, 10.365], loss: 0.001192, mae: 0.037971, mean_q: 1.163180
 47845/100000: episode: 758, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.147, mean reward: 0.571 [0.499, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.083, 10.124], loss: 0.001150, mae: 0.037696, mean_q: 1.163371
 47945/100000: episode: 759, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.730, mean reward: 0.597 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.682, 10.299], loss: 0.001171, mae: 0.037579, mean_q: 1.163436
 48045/100000: episode: 760, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 56.967, mean reward: 0.570 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.785, 10.098], loss: 0.001261, mae: 0.039152, mean_q: 1.160771
 48145/100000: episode: 761, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.338, mean reward: 0.573 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.576, 10.286], loss: 0.001170, mae: 0.037692, mean_q: 1.159911
 48245/100000: episode: 762, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.395, mean reward: 0.594 [0.513, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.226, 10.336], loss: 0.001205, mae: 0.038624, mean_q: 1.162318
 48345/100000: episode: 763, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 56.546, mean reward: 0.565 [0.499, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.753, 10.147], loss: 0.001254, mae: 0.038609, mean_q: 1.164240
 48445/100000: episode: 764, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 63.727, mean reward: 0.637 [0.498, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.926, 10.098], loss: 0.001286, mae: 0.039420, mean_q: 1.162063
 48545/100000: episode: 765, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.576, mean reward: 0.596 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.864, 10.098], loss: 0.001272, mae: 0.038736, mean_q: 1.162142
 48645/100000: episode: 766, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.009, mean reward: 0.590 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.689, 10.119], loss: 0.001270, mae: 0.039316, mean_q: 1.160663
 48745/100000: episode: 767, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 59.732, mean reward: 0.597 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.370, 10.098], loss: 0.001268, mae: 0.039460, mean_q: 1.156951
 48845/100000: episode: 768, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 57.453, mean reward: 0.575 [0.506, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.471, 10.098], loss: 0.001148, mae: 0.037236, mean_q: 1.155908
 48945/100000: episode: 769, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.362, mean reward: 0.604 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.445, 10.193], loss: 0.001276, mae: 0.038898, mean_q: 1.159582
 49045/100000: episode: 770, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.378, mean reward: 0.574 [0.505, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.895, 10.142], loss: 0.001271, mae: 0.038851, mean_q: 1.162699
 49145/100000: episode: 771, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 58.314, mean reward: 0.583 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.597, 10.098], loss: 0.001249, mae: 0.038578, mean_q: 1.159442
 49245/100000: episode: 772, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.493, mean reward: 0.605 [0.507, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.976, 10.213], loss: 0.001231, mae: 0.038264, mean_q: 1.160009
 49345/100000: episode: 773, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 62.070, mean reward: 0.621 [0.502, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.665, 10.296], loss: 0.001243, mae: 0.038739, mean_q: 1.159521
 49445/100000: episode: 774, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.121, mean reward: 0.571 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.414, 10.098], loss: 0.001194, mae: 0.037991, mean_q: 1.160440
 49545/100000: episode: 775, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.775, mean reward: 0.588 [0.513, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.256, 10.350], loss: 0.001341, mae: 0.039188, mean_q: 1.161649
 49645/100000: episode: 776, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.744, mean reward: 0.587 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.689, 10.098], loss: 0.001282, mae: 0.039028, mean_q: 1.161265
 49745/100000: episode: 777, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 60.232, mean reward: 0.602 [0.503, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.404, 10.098], loss: 0.001270, mae: 0.039153, mean_q: 1.161520
 49845/100000: episode: 778, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 60.182, mean reward: 0.602 [0.507, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.152, 10.350], loss: 0.001178, mae: 0.037754, mean_q: 1.159514
 49945/100000: episode: 779, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.434, mean reward: 0.584 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.309, 10.098], loss: 0.001217, mae: 0.038298, mean_q: 1.163844
 50045/100000: episode: 780, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.386, mean reward: 0.594 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.027, 10.098], loss: 0.001244, mae: 0.038809, mean_q: 1.162716
 50145/100000: episode: 781, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.216, mean reward: 0.592 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.642, 10.109], loss: 0.001347, mae: 0.040333, mean_q: 1.163397
 50245/100000: episode: 782, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 57.417, mean reward: 0.574 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.217, 10.293], loss: 0.001291, mae: 0.039354, mean_q: 1.162817
 50345/100000: episode: 783, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 58.296, mean reward: 0.583 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.647, 10.098], loss: 0.001288, mae: 0.039672, mean_q: 1.160721
 50445/100000: episode: 784, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 59.976, mean reward: 0.600 [0.515, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.633, 10.167], loss: 0.001356, mae: 0.040102, mean_q: 1.162380
 50545/100000: episode: 785, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 59.087, mean reward: 0.591 [0.502, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.274, 10.138], loss: 0.001175, mae: 0.037952, mean_q: 1.162754
 50645/100000: episode: 786, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.521, mean reward: 0.595 [0.511, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.603, 10.098], loss: 0.001238, mae: 0.038978, mean_q: 1.161348
 50745/100000: episode: 787, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 56.503, mean reward: 0.565 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.936, 10.098], loss: 0.001227, mae: 0.038670, mean_q: 1.158993
 50845/100000: episode: 788, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.508, mean reward: 0.585 [0.512, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.963, 10.192], loss: 0.001303, mae: 0.040102, mean_q: 1.164016
 50945/100000: episode: 789, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.577, mean reward: 0.586 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.008, 10.098], loss: 0.001230, mae: 0.038580, mean_q: 1.160555
 51045/100000: episode: 790, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 56.643, mean reward: 0.566 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.672, 10.098], loss: 0.001171, mae: 0.037720, mean_q: 1.162218
 51145/100000: episode: 791, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.633, mean reward: 0.596 [0.504, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.756, 10.184], loss: 0.001315, mae: 0.039910, mean_q: 1.160403
 51245/100000: episode: 792, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.473, mean reward: 0.595 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.161, 10.259], loss: 0.001302, mae: 0.039391, mean_q: 1.162615
 51345/100000: episode: 793, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 58.595, mean reward: 0.586 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.225, 10.333], loss: 0.001318, mae: 0.039798, mean_q: 1.164620
 51445/100000: episode: 794, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 60.373, mean reward: 0.604 [0.508, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.072, 10.098], loss: 0.001243, mae: 0.039262, mean_q: 1.162857
 51545/100000: episode: 795, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 57.654, mean reward: 0.577 [0.501, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.677, 10.098], loss: 0.001246, mae: 0.038989, mean_q: 1.162301
 51645/100000: episode: 796, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 62.929, mean reward: 0.629 [0.517, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.369, 10.191], loss: 0.001199, mae: 0.038051, mean_q: 1.163368
 51745/100000: episode: 797, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 57.869, mean reward: 0.579 [0.503, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.484, 10.098], loss: 0.001333, mae: 0.040191, mean_q: 1.163889
 51845/100000: episode: 798, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.668, mean reward: 0.587 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.514, 10.192], loss: 0.001328, mae: 0.040328, mean_q: 1.165786
[Info] 1-TH LEVEL FOUND: 1.3752200603485107, Considering 10/90 traces
 51945/100000: episode: 799, duration: 4.648s, episode steps: 100, steps per second: 22, episode reward: 57.821, mean reward: 0.578 [0.512, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.021, 10.098], loss: 0.001324, mae: 0.039882, mean_q: 1.162594
 51973/100000: episode: 800, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 17.452, mean reward: 0.623 [0.531, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.196, 10.186], loss: 0.001484, mae: 0.042621, mean_q: 1.158017
 52000/100000: episode: 801, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 18.816, mean reward: 0.697 [0.639, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.075, 10.435], loss: 0.001352, mae: 0.040522, mean_q: 1.170362
 52029/100000: episode: 802, duration: 0.141s, episode steps: 29, steps per second: 206, episode reward: 18.084, mean reward: 0.624 [0.560, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.116, 10.277], loss: 0.001290, mae: 0.038958, mean_q: 1.162610
 52061/100000: episode: 803, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 23.402, mean reward: 0.731 [0.625, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.661, 10.435], loss: 0.001294, mae: 0.040158, mean_q: 1.165099
 52087/100000: episode: 804, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 18.293, mean reward: 0.704 [0.600, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.823, 10.566], loss: 0.001448, mae: 0.042959, mean_q: 1.167138
 52119/100000: episode: 805, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 22.720, mean reward: 0.710 [0.622, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.198, 10.549], loss: 0.001374, mae: 0.040999, mean_q: 1.165097
 52161/100000: episode: 806, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 27.467, mean reward: 0.654 [0.573, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.218, 10.335], loss: 0.001415, mae: 0.040991, mean_q: 1.164706
 52188/100000: episode: 807, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 17.880, mean reward: 0.662 [0.602, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.288, 10.290], loss: 0.001571, mae: 0.042902, mean_q: 1.166807
 52214/100000: episode: 808, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 17.873, mean reward: 0.687 [0.599, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.397], loss: 0.001336, mae: 0.039962, mean_q: 1.165950
 52246/100000: episode: 809, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 23.167, mean reward: 0.724 [0.674, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.284, 10.100], loss: 0.001409, mae: 0.040805, mean_q: 1.174486
 52285/100000: episode: 810, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 26.607, mean reward: 0.682 [0.555, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.128, 10.310], loss: 0.001464, mae: 0.041085, mean_q: 1.173073
 52312/100000: episode: 811, duration: 0.153s, episode steps: 27, steps per second: 177, episode reward: 17.688, mean reward: 0.655 [0.573, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.362], loss: 0.001307, mae: 0.039271, mean_q: 1.173652
 52339/100000: episode: 812, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 20.541, mean reward: 0.761 [0.638, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.119, 10.526], loss: 0.001318, mae: 0.040539, mean_q: 1.173689
 52368/100000: episode: 813, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 21.842, mean reward: 0.753 [0.669, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.242, 10.100], loss: 0.001481, mae: 0.041462, mean_q: 1.175082
 52395/100000: episode: 814, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 18.581, mean reward: 0.688 [0.628, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.062, 10.455], loss: 0.001483, mae: 0.041520, mean_q: 1.182058
 52423/100000: episode: 815, duration: 0.143s, episode steps: 28, steps per second: 195, episode reward: 18.342, mean reward: 0.655 [0.559, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.725, 10.319], loss: 0.001412, mae: 0.041005, mean_q: 1.179958
 52455/100000: episode: 816, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 22.728, mean reward: 0.710 [0.632, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.457, 10.445], loss: 0.001907, mae: 0.046398, mean_q: 1.175363
 52484/100000: episode: 817, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 18.018, mean reward: 0.621 [0.519, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.676, 10.179], loss: 0.001591, mae: 0.042862, mean_q: 1.177710
 52516/100000: episode: 818, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 21.395, mean reward: 0.669 [0.583, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.536, 10.329], loss: 0.001430, mae: 0.042425, mean_q: 1.181282
 52542/100000: episode: 819, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 18.614, mean reward: 0.716 [0.645, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.322, 10.590], loss: 0.001455, mae: 0.042187, mean_q: 1.185286
 52584/100000: episode: 820, duration: 0.214s, episode steps: 42, steps per second: 197, episode reward: 27.226, mean reward: 0.648 [0.515, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.736, 10.219], loss: 0.001576, mae: 0.043054, mean_q: 1.184476
 52613/100000: episode: 821, duration: 0.141s, episode steps: 29, steps per second: 205, episode reward: 19.651, mean reward: 0.678 [0.638, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.201, 10.401], loss: 0.001438, mae: 0.040225, mean_q: 1.192299
 52642/100000: episode: 822, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 21.325, mean reward: 0.735 [0.670, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.308, 10.100], loss: 0.001640, mae: 0.041640, mean_q: 1.184356
 52671/100000: episode: 823, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 19.939, mean reward: 0.688 [0.631, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.433, 10.412], loss: 0.001383, mae: 0.039973, mean_q: 1.187040
 52700/100000: episode: 824, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 19.800, mean reward: 0.683 [0.615, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.950, 10.405], loss: 0.001613, mae: 0.043239, mean_q: 1.198062
 52728/100000: episode: 825, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 21.766, mean reward: 0.777 [0.684, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.150, 10.611], loss: 0.001566, mae: 0.041704, mean_q: 1.189998
 52757/100000: episode: 826, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 18.859, mean reward: 0.650 [0.586, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.064, 10.299], loss: 0.001503, mae: 0.042491, mean_q: 1.185988
 52796/100000: episode: 827, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 28.393, mean reward: 0.728 [0.650, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.740, 10.389], loss: 0.001808, mae: 0.045323, mean_q: 1.192138
 52824/100000: episode: 828, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 18.529, mean reward: 0.662 [0.584, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.580, 10.330], loss: 0.001449, mae: 0.041817, mean_q: 1.206088
 52853/100000: episode: 829, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 19.797, mean reward: 0.683 [0.631, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.414, 10.100], loss: 0.001513, mae: 0.041430, mean_q: 1.191933
 52892/100000: episode: 830, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 25.747, mean reward: 0.660 [0.539, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.573, 10.136], loss: 0.001580, mae: 0.042494, mean_q: 1.193584
 52924/100000: episode: 831, duration: 0.175s, episode steps: 32, steps per second: 182, episode reward: 21.688, mean reward: 0.678 [0.597, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.133, 10.100], loss: 0.001856, mae: 0.046183, mean_q: 1.194794
 52952/100000: episode: 832, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 20.722, mean reward: 0.740 [0.669, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.458], loss: 0.001442, mae: 0.040515, mean_q: 1.198918
 52994/100000: episode: 833, duration: 0.214s, episode steps: 42, steps per second: 196, episode reward: 31.763, mean reward: 0.756 [0.597, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.315, 10.467], loss: 0.001428, mae: 0.041491, mean_q: 1.202779
 53021/100000: episode: 834, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 16.042, mean reward: 0.594 [0.514, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.562, 10.157], loss: 0.001628, mae: 0.041064, mean_q: 1.192805
 53053/100000: episode: 835, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 21.264, mean reward: 0.665 [0.589, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.353], loss: 0.001702, mae: 0.044557, mean_q: 1.214100
 53095/100000: episode: 836, duration: 0.216s, episode steps: 42, steps per second: 195, episode reward: 28.412, mean reward: 0.676 [0.546, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.213, 10.324], loss: 0.001543, mae: 0.042733, mean_q: 1.203449
 53122/100000: episode: 837, duration: 0.131s, episode steps: 27, steps per second: 206, episode reward: 18.291, mean reward: 0.677 [0.546, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.139, 10.341], loss: 0.001556, mae: 0.042893, mean_q: 1.211220
 53151/100000: episode: 838, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 20.453, mean reward: 0.705 [0.593, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.164, 10.100], loss: 0.001385, mae: 0.039951, mean_q: 1.207444
 53193/100000: episode: 839, duration: 0.216s, episode steps: 42, steps per second: 194, episode reward: 28.603, mean reward: 0.681 [0.573, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.221, 10.280], loss: 0.001508, mae: 0.041082, mean_q: 1.208575
 53222/100000: episode: 840, duration: 0.141s, episode steps: 29, steps per second: 206, episode reward: 19.932, mean reward: 0.687 [0.593, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.758, 10.100], loss: 0.001661, mae: 0.043388, mean_q: 1.209682
 53249/100000: episode: 841, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 19.257, mean reward: 0.713 [0.613, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.410, 10.380], loss: 0.001736, mae: 0.043869, mean_q: 1.209480
 53278/100000: episode: 842, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 20.389, mean reward: 0.703 [0.597, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.306], loss: 0.001536, mae: 0.043227, mean_q: 1.208723
 53305/100000: episode: 843, duration: 0.135s, episode steps: 27, steps per second: 201, episode reward: 17.214, mean reward: 0.638 [0.550, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.248], loss: 0.001443, mae: 0.041049, mean_q: 1.209377
 53334/100000: episode: 844, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 17.113, mean reward: 0.590 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.968, 10.283], loss: 0.001410, mae: 0.040882, mean_q: 1.205191
 53363/100000: episode: 845, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 21.106, mean reward: 0.728 [0.673, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.313, 10.100], loss: 0.001409, mae: 0.041577, mean_q: 1.210847
 53392/100000: episode: 846, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 20.981, mean reward: 0.723 [0.651, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.628, 10.100], loss: 0.001529, mae: 0.041304, mean_q: 1.217815
 53419/100000: episode: 847, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 17.802, mean reward: 0.659 [0.593, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.521, 10.471], loss: 0.001679, mae: 0.043293, mean_q: 1.203604
 53458/100000: episode: 848, duration: 0.188s, episode steps: 39, steps per second: 208, episode reward: 25.719, mean reward: 0.659 [0.563, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-1.035, 10.265], loss: 0.001653, mae: 0.043715, mean_q: 1.215461
 53487/100000: episode: 849, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 19.435, mean reward: 0.670 [0.572, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.590, 10.349], loss: 0.002316, mae: 0.050084, mean_q: 1.208557
 53516/100000: episode: 850, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 17.969, mean reward: 0.620 [0.536, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.248], loss: 0.001624, mae: 0.042344, mean_q: 1.219979
 53548/100000: episode: 851, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 22.577, mean reward: 0.706 [0.624, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.225, 10.400], loss: 0.001449, mae: 0.040838, mean_q: 1.212312
 53580/100000: episode: 852, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 21.726, mean reward: 0.679 [0.506, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.963, 10.117], loss: 0.001441, mae: 0.040553, mean_q: 1.215218
 53609/100000: episode: 853, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 21.784, mean reward: 0.751 [0.630, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.321, 10.100], loss: 0.001475, mae: 0.040986, mean_q: 1.205953
 53641/100000: episode: 854, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 25.153, mean reward: 0.786 [0.694, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.227, 10.100], loss: 0.001375, mae: 0.040401, mean_q: 1.229122
 53680/100000: episode: 855, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 27.966, mean reward: 0.717 [0.563, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.645, 10.308], loss: 0.001864, mae: 0.044145, mean_q: 1.216846
 53722/100000: episode: 856, duration: 0.203s, episode steps: 42, steps per second: 207, episode reward: 31.567, mean reward: 0.752 [0.627, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.561, 10.512], loss: 0.001792, mae: 0.044687, mean_q: 1.222890
 53761/100000: episode: 857, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 28.373, mean reward: 0.728 [0.663, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.296, 10.589], loss: 0.001636, mae: 0.042732, mean_q: 1.225962
 53793/100000: episode: 858, duration: 0.158s, episode steps: 32, steps per second: 203, episode reward: 20.971, mean reward: 0.655 [0.574, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.743, 10.100], loss: 0.001713, mae: 0.043868, mean_q: 1.224886
 53822/100000: episode: 859, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 19.841, mean reward: 0.684 [0.616, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.406], loss: 0.001600, mae: 0.043388, mean_q: 1.225695
 53851/100000: episode: 860, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 20.811, mean reward: 0.718 [0.642, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.645, 10.100], loss: 0.001592, mae: 0.043423, mean_q: 1.227169
 53883/100000: episode: 861, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 21.606, mean reward: 0.675 [0.599, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.299], loss: 0.001950, mae: 0.046378, mean_q: 1.231518
 53910/100000: episode: 862, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 16.585, mean reward: 0.614 [0.549, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.402, 10.215], loss: 0.001471, mae: 0.041656, mean_q: 1.243875
 53952/100000: episode: 863, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 25.873, mean reward: 0.616 [0.536, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.503, 10.306], loss: 0.001444, mae: 0.040998, mean_q: 1.235148
 53979/100000: episode: 864, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 18.615, mean reward: 0.689 [0.556, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.175, 10.335], loss: 0.001538, mae: 0.042069, mean_q: 1.222958
 54008/100000: episode: 865, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 17.173, mean reward: 0.592 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.846, 10.100], loss: 0.001640, mae: 0.042850, mean_q: 1.232036
 54050/100000: episode: 866, duration: 0.224s, episode steps: 42, steps per second: 187, episode reward: 30.941, mean reward: 0.737 [0.643, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.860, 10.421], loss: 0.001777, mae: 0.043961, mean_q: 1.232787
 54082/100000: episode: 867, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 21.327, mean reward: 0.666 [0.592, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.992, 10.292], loss: 0.001636, mae: 0.042613, mean_q: 1.232372
 54124/100000: episode: 868, duration: 0.216s, episode steps: 42, steps per second: 194, episode reward: 29.878, mean reward: 0.711 [0.597, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.305, 10.571], loss: 0.001457, mae: 0.040690, mean_q: 1.236590
 54152/100000: episode: 869, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 18.536, mean reward: 0.662 [0.599, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.857, 10.452], loss: 0.001657, mae: 0.043493, mean_q: 1.235646
 54194/100000: episode: 870, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 29.843, mean reward: 0.711 [0.618, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.458, 10.503], loss: 0.001612, mae: 0.043834, mean_q: 1.229785
 54223/100000: episode: 871, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 19.344, mean reward: 0.667 [0.550, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.569, 10.100], loss: 0.001869, mae: 0.044637, mean_q: 1.239410
 54252/100000: episode: 872, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 20.457, mean reward: 0.705 [0.600, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.986, 10.100], loss: 0.001475, mae: 0.042492, mean_q: 1.245010
 54279/100000: episode: 873, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 18.669, mean reward: 0.691 [0.565, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.308], loss: 0.001464, mae: 0.040305, mean_q: 1.239561
 54307/100000: episode: 874, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 17.563, mean reward: 0.627 [0.564, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.257, 10.340], loss: 0.001299, mae: 0.039305, mean_q: 1.237010
 54335/100000: episode: 875, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 20.744, mean reward: 0.741 [0.676, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.535], loss: 0.001562, mae: 0.042428, mean_q: 1.249896
 54361/100000: episode: 876, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 17.795, mean reward: 0.684 [0.629, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.223, 10.351], loss: 0.001687, mae: 0.043429, mean_q: 1.235658
 54390/100000: episode: 877, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 19.268, mean reward: 0.664 [0.519, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.154], loss: 0.001708, mae: 0.043192, mean_q: 1.243503
 54418/100000: episode: 878, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 18.471, mean reward: 0.660 [0.600, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.390], loss: 0.001288, mae: 0.038802, mean_q: 1.243808
 54460/100000: episode: 879, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 27.363, mean reward: 0.651 [0.526, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.858, 10.107], loss: 0.001657, mae: 0.043157, mean_q: 1.245353
 54487/100000: episode: 880, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 20.149, mean reward: 0.746 [0.665, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.395], loss: 0.001645, mae: 0.043021, mean_q: 1.249331
 54519/100000: episode: 881, duration: 0.188s, episode steps: 32, steps per second: 170, episode reward: 21.738, mean reward: 0.679 [0.616, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.724, 10.282], loss: 0.001701, mae: 0.043940, mean_q: 1.247792
 54546/100000: episode: 882, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 18.019, mean reward: 0.667 [0.576, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.290], loss: 0.001592, mae: 0.042879, mean_q: 1.238660
 54573/100000: episode: 883, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 19.868, mean reward: 0.736 [0.631, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.862, 10.519], loss: 0.001616, mae: 0.042407, mean_q: 1.259376
 54601/100000: episode: 884, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 18.235, mean reward: 0.651 [0.582, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.083, 10.264], loss: 0.001467, mae: 0.040667, mean_q: 1.254540
 54628/100000: episode: 885, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 17.469, mean reward: 0.647 [0.595, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.128, 10.390], loss: 0.001409, mae: 0.039963, mean_q: 1.244447
 54667/100000: episode: 886, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 23.398, mean reward: 0.600 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.847, 10.100], loss: 0.001482, mae: 0.042308, mean_q: 1.254210
 54709/100000: episode: 887, duration: 0.209s, episode steps: 42, steps per second: 201, episode reward: 29.086, mean reward: 0.693 [0.599, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.258, 10.454], loss: 0.001783, mae: 0.045153, mean_q: 1.250852
 54738/100000: episode: 888, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 18.637, mean reward: 0.643 [0.581, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.486, 10.416], loss: 0.001605, mae: 0.043326, mean_q: 1.244710
[Info] 2-TH LEVEL FOUND: 1.5719778537750244, Considering 10/90 traces
 54765/100000: episode: 889, duration: 4.401s, episode steps: 27, steps per second: 6, episode reward: 17.902, mean reward: 0.663 [0.582, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.462], loss: 0.001345, mae: 0.040799, mean_q: 1.252627
 54798/100000: episode: 890, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 27.342, mean reward: 0.829 [0.738, 0.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.499, 10.543], loss: 0.001666, mae: 0.043499, mean_q: 1.248373
 54805/100000: episode: 891, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 5.523, mean reward: 0.789 [0.732, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.471], loss: 0.002160, mae: 0.047550, mean_q: 1.243750
 54817/100000: episode: 892, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 10.058, mean reward: 0.838 [0.754, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.578, 10.100], loss: 0.001229, mae: 0.038634, mean_q: 1.241624
 54831/100000: episode: 893, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 9.600, mean reward: 0.686 [0.624, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.359, 10.100], loss: 0.002039, mae: 0.046034, mean_q: 1.249302
 54845/100000: episode: 894, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 11.118, mean reward: 0.794 [0.678, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.268, 10.100], loss: 0.001651, mae: 0.044397, mean_q: 1.253687
 54862/100000: episode: 895, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 13.123, mean reward: 0.772 [0.650, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.387, 10.536], loss: 0.001591, mae: 0.045636, mean_q: 1.244921
 54875/100000: episode: 896, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 10.970, mean reward: 0.844 [0.785, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.056, 10.533], loss: 0.001366, mae: 0.041940, mean_q: 1.269322
 54888/100000: episode: 897, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 11.102, mean reward: 0.854 [0.796, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.799, 10.100], loss: 0.001315, mae: 0.039353, mean_q: 1.267992
 54901/100000: episode: 898, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 10.913, mean reward: 0.839 [0.823, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.236, 10.100], loss: 0.001821, mae: 0.043650, mean_q: 1.262483
 54927/100000: episode: 899, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 21.569, mean reward: 0.830 [0.710, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.135, 10.522], loss: 0.001519, mae: 0.042522, mean_q: 1.272925
 54941/100000: episode: 900, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 11.111, mean reward: 0.794 [0.747, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.675, 10.100], loss: 0.001507, mae: 0.044097, mean_q: 1.268580
 54954/100000: episode: 901, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 10.005, mean reward: 0.770 [0.727, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.394, 10.100], loss: 0.001810, mae: 0.044100, mean_q: 1.244847
 54961/100000: episode: 902, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 5.741, mean reward: 0.820 [0.758, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.601], loss: 0.002003, mae: 0.049535, mean_q: 1.303950
 54974/100000: episode: 903, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 10.938, mean reward: 0.841 [0.718, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.285, 10.100], loss: 0.002024, mae: 0.050164, mean_q: 1.264922
 54981/100000: episode: 904, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.909, mean reward: 0.844 [0.826, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.582], loss: 0.001959, mae: 0.046604, mean_q: 1.247538
 54988/100000: episode: 905, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 5.625, mean reward: 0.804 [0.735, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.128, 10.587], loss: 0.001925, mae: 0.044619, mean_q: 1.247370
 54995/100000: episode: 906, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 5.523, mean reward: 0.789 [0.748, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.554], loss: 0.001277, mae: 0.038667, mean_q: 1.265490
[Info] FALSIFICATION!
 55002/100000: episode: 907, duration: 0.303s, episode steps: 7, steps per second: 23, episode reward: 6.415, mean reward: 0.916 [0.797, 1.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.774 [-2.149, 8.502], loss: 0.001573, mae: 0.041572, mean_q: 1.261500
 55016/100000: episode: 908, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 10.848, mean reward: 0.775 [0.714, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.318, 10.100], loss: 0.001523, mae: 0.041515, mean_q: 1.262178
[Info] FALSIFICATION!
 55024/100000: episode: 909, duration: 0.205s, episode steps: 8, steps per second: 39, episode reward: 7.243, mean reward: 0.905 [0.831, 1.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.020, 10.148], loss: 0.001473, mae: 0.040341, mean_q: 1.265131
 55037/100000: episode: 910, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 9.769, mean reward: 0.751 [0.679, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.354, 10.100], loss: 0.001177, mae: 0.038497, mean_q: 1.280727
 55070/100000: episode: 911, duration: 0.170s, episode steps: 33, steps per second: 195, episode reward: 25.107, mean reward: 0.761 [0.710, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.341, 10.531], loss: 0.001860, mae: 0.046005, mean_q: 1.267210
 55084/100000: episode: 912, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 10.050, mean reward: 0.718 [0.652, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.236, 10.100], loss: 0.001448, mae: 0.040940, mean_q: 1.281854
 55091/100000: episode: 913, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 5.980, mean reward: 0.854 [0.819, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.666], loss: 0.001785, mae: 0.046002, mean_q: 1.273498
 55120/100000: episode: 914, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 18.978, mean reward: 0.654 [0.541, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.285, 10.186], loss: 0.001770, mae: 0.042198, mean_q: 1.281647
 55133/100000: episode: 915, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 10.579, mean reward: 0.814 [0.765, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.415, 10.100], loss: 0.001258, mae: 0.039385, mean_q: 1.280298
 55162/100000: episode: 916, duration: 0.148s, episode steps: 29, steps per second: 195, episode reward: 22.552, mean reward: 0.778 [0.697, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.607, 10.464], loss: 0.001766, mae: 0.043090, mean_q: 1.270313
 55188/100000: episode: 917, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 19.388, mean reward: 0.746 [0.663, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.119, 10.448], loss: 0.001613, mae: 0.043309, mean_q: 1.280710
 55217/100000: episode: 918, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 22.700, mean reward: 0.783 [0.705, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.698, 10.509], loss: 0.001533, mae: 0.042042, mean_q: 1.273825
 55250/100000: episode: 919, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 21.159, mean reward: 0.641 [0.531, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.262, 10.212], loss: 0.001525, mae: 0.041377, mean_q: 1.279960
 55276/100000: episode: 920, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 22.002, mean reward: 0.846 [0.786, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.052, 10.602], loss: 0.001505, mae: 0.041974, mean_q: 1.280130
 55289/100000: episode: 921, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 9.980, mean reward: 0.768 [0.660, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.174, 10.100], loss: 0.001681, mae: 0.043812, mean_q: 1.292922
 55303/100000: episode: 922, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 10.876, mean reward: 0.777 [0.723, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.645, 10.100], loss: 0.001583, mae: 0.043415, mean_q: 1.285998
 55336/100000: episode: 923, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 24.658, mean reward: 0.747 [0.625, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.361, 10.327], loss: 0.001733, mae: 0.044490, mean_q: 1.293245
 55349/100000: episode: 924, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 10.792, mean reward: 0.830 [0.777, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.360, 10.593], loss: 0.001468, mae: 0.042301, mean_q: 1.284780
 55362/100000: episode: 925, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 11.430, mean reward: 0.879 [0.818, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.543, 10.100], loss: 0.001460, mae: 0.041495, mean_q: 1.296935
 55379/100000: episode: 926, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 12.195, mean reward: 0.717 [0.621, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.111, 10.382], loss: 0.001418, mae: 0.041515, mean_q: 1.293736
 55412/100000: episode: 927, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 27.354, mean reward: 0.829 [0.625, 0.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.270, 10.437], loss: 0.001353, mae: 0.039750, mean_q: 1.301797
 55425/100000: episode: 928, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 10.693, mean reward: 0.823 [0.768, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.394, 10.100], loss: 0.001641, mae: 0.043401, mean_q: 1.282550
 55442/100000: episode: 929, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 13.411, mean reward: 0.789 [0.702, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.909, 10.537], loss: 0.001518, mae: 0.041981, mean_q: 1.303540
 55455/100000: episode: 930, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 10.162, mean reward: 0.782 [0.755, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.675, 10.100], loss: 0.001491, mae: 0.041378, mean_q: 1.298141
 55481/100000: episode: 931, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 19.890, mean reward: 0.765 [0.689, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.443], loss: 0.001705, mae: 0.043538, mean_q: 1.296899
 55495/100000: episode: 932, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 11.252, mean reward: 0.804 [0.770, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.601, 10.100], loss: 0.001584, mae: 0.043633, mean_q: 1.280589
 55508/100000: episode: 933, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 10.507, mean reward: 0.808 [0.708, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.558], loss: 0.001294, mae: 0.039285, mean_q: 1.306661
 55525/100000: episode: 934, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 14.803, mean reward: 0.871 [0.811, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.606, 10.663], loss: 0.001621, mae: 0.040926, mean_q: 1.296583
 55538/100000: episode: 935, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 10.876, mean reward: 0.837 [0.760, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.764, 10.100], loss: 0.001255, mae: 0.039216, mean_q: 1.318125
 55545/100000: episode: 936, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 5.996, mean reward: 0.857 [0.814, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.344, 10.546], loss: 0.001504, mae: 0.040493, mean_q: 1.276931
 55552/100000: episode: 937, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 5.965, mean reward: 0.852 [0.759, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.641], loss: 0.002120, mae: 0.050333, mean_q: 1.318629
 55565/100000: episode: 938, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 10.707, mean reward: 0.824 [0.760, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.344, 10.100], loss: 0.001759, mae: 0.044276, mean_q: 1.315260
 55594/100000: episode: 939, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 23.678, mean reward: 0.816 [0.753, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.047, 10.558], loss: 0.001900, mae: 0.046482, mean_q: 1.299261
 55607/100000: episode: 940, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 10.150, mean reward: 0.781 [0.729, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.402, 10.100], loss: 0.001531, mae: 0.044028, mean_q: 1.331547
 55620/100000: episode: 941, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 10.784, mean reward: 0.830 [0.772, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.880, 10.534], loss: 0.002146, mae: 0.046667, mean_q: 1.316733
 55632/100000: episode: 942, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 9.784, mean reward: 0.815 [0.738, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.390, 10.100], loss: 0.001770, mae: 0.042882, mean_q: 1.343790
 55649/100000: episode: 943, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 13.623, mean reward: 0.801 [0.751, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.646], loss: 0.001815, mae: 0.042875, mean_q: 1.307050
 55662/100000: episode: 944, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 10.695, mean reward: 0.823 [0.746, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.539], loss: 0.001252, mae: 0.039023, mean_q: 1.324029
 55676/100000: episode: 945, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 10.694, mean reward: 0.764 [0.730, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.263, 10.100], loss: 0.001417, mae: 0.039130, mean_q: 1.320891
 55689/100000: episode: 946, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 10.318, mean reward: 0.794 [0.733, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.345, 10.100], loss: 0.001280, mae: 0.039835, mean_q: 1.307750
 55701/100000: episode: 947, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 9.390, mean reward: 0.782 [0.737, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.347, 10.100], loss: 0.001589, mae: 0.042354, mean_q: 1.315002
 55715/100000: episode: 948, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 10.312, mean reward: 0.737 [0.667, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.822, 10.100], loss: 0.001337, mae: 0.040814, mean_q: 1.336308
 55741/100000: episode: 949, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 20.683, mean reward: 0.795 [0.712, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.286, 10.523], loss: 0.001696, mae: 0.041999, mean_q: 1.321616
 55774/100000: episode: 950, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 26.961, mean reward: 0.817 [0.738, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.069, 10.501], loss: 0.001360, mae: 0.041023, mean_q: 1.324968
 55781/100000: episode: 951, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 5.986, mean reward: 0.855 [0.800, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.614], loss: 0.001303, mae: 0.040176, mean_q: 1.316172
 55794/100000: episode: 952, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 10.589, mean reward: 0.815 [0.795, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.416, 10.100], loss: 0.001509, mae: 0.042301, mean_q: 1.322457
 55811/100000: episode: 953, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 13.850, mean reward: 0.815 [0.756, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.106, 10.552], loss: 0.001765, mae: 0.042304, mean_q: 1.326022
 55840/100000: episode: 954, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 22.100, mean reward: 0.762 [0.693, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.152, 10.477], loss: 0.001812, mae: 0.044942, mean_q: 1.317219
 55857/100000: episode: 955, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 13.953, mean reward: 0.821 [0.739, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.574, 10.549], loss: 0.001626, mae: 0.042598, mean_q: 1.309718
 55864/100000: episode: 956, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 6.008, mean reward: 0.858 [0.771, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.660], loss: 0.001365, mae: 0.042903, mean_q: 1.342646
 55890/100000: episode: 957, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 19.093, mean reward: 0.734 [0.649, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.462], loss: 0.001353, mae: 0.039581, mean_q: 1.331729
 55904/100000: episode: 958, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 11.482, mean reward: 0.820 [0.765, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.510, 10.100], loss: 0.002347, mae: 0.047680, mean_q: 1.322264
 55918/100000: episode: 959, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 11.199, mean reward: 0.800 [0.734, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.645, 10.100], loss: 0.001305, mae: 0.039152, mean_q: 1.325941
 55931/100000: episode: 960, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 11.121, mean reward: 0.855 [0.727, 0.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.799, 10.100], loss: 0.001378, mae: 0.041316, mean_q: 1.326077
 55944/100000: episode: 961, duration: 0.084s, episode steps: 13, steps per second: 154, episode reward: 10.663, mean reward: 0.820 [0.747, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.390, 10.100], loss: 0.001833, mae: 0.044422, mean_q: 1.324800
 55957/100000: episode: 962, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 10.417, mean reward: 0.801 [0.718, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.504, 10.100], loss: 0.001604, mae: 0.043306, mean_q: 1.332514
 55974/100000: episode: 963, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 13.985, mean reward: 0.823 [0.729, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.790, 10.473], loss: 0.001235, mae: 0.039151, mean_q: 1.340592
 55987/100000: episode: 964, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 10.348, mean reward: 0.796 [0.728, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.260, 10.100], loss: 0.001670, mae: 0.045292, mean_q: 1.330169
 56020/100000: episode: 965, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 24.707, mean reward: 0.749 [0.597, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.824, 10.445], loss: 0.001361, mae: 0.039669, mean_q: 1.337678
 56033/100000: episode: 966, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 10.005, mean reward: 0.770 [0.694, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.507, 10.100], loss: 0.001598, mae: 0.044361, mean_q: 1.345320
 56059/100000: episode: 967, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 22.736, mean reward: 0.874 [0.726, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.374, 10.513], loss: 0.001271, mae: 0.039995, mean_q: 1.342179
 56092/100000: episode: 968, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 26.817, mean reward: 0.813 [0.714, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.316, 10.365], loss: 0.001643, mae: 0.044635, mean_q: 1.347523
 56105/100000: episode: 969, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 10.838, mean reward: 0.834 [0.795, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.171, 10.656], loss: 0.001268, mae: 0.040684, mean_q: 1.351010
 56134/100000: episode: 970, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 24.679, mean reward: 0.851 [0.776, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.126, 10.597], loss: 0.001662, mae: 0.043973, mean_q: 1.339409
 56163/100000: episode: 971, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 22.727, mean reward: 0.784 [0.706, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.510], loss: 0.001359, mae: 0.040742, mean_q: 1.348732
 56189/100000: episode: 972, duration: 0.139s, episode steps: 26, steps per second: 186, episode reward: 22.838, mean reward: 0.878 [0.782, 0.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.208, 10.570], loss: 0.001489, mae: 0.041064, mean_q: 1.344854
[Info] FALSIFICATION!
 56201/100000: episode: 973, duration: 0.231s, episode steps: 12, steps per second: 52, episode reward: 9.761, mean reward: 0.813 [0.720, 1.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.368, 10.098], loss: 0.001304, mae: 0.039806, mean_q: 1.359077
 56215/100000: episode: 974, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 10.595, mean reward: 0.757 [0.718, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.014, 10.100], loss: 0.001451, mae: 0.043213, mean_q: 1.360512
 56228/100000: episode: 975, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 10.667, mean reward: 0.821 [0.764, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.122, 10.100], loss: 0.001088, mae: 0.037658, mean_q: 1.352081
 56242/100000: episode: 976, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 10.160, mean reward: 0.726 [0.657, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.362, 10.100], loss: 0.001359, mae: 0.040449, mean_q: 1.351868
 56268/100000: episode: 977, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 18.849, mean reward: 0.725 [0.657, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.428], loss: 0.001218, mae: 0.038938, mean_q: 1.364233
 56294/100000: episode: 978, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 20.830, mean reward: 0.801 [0.660, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.498], loss: 0.001219, mae: 0.037737, mean_q: 1.351160
[Info] Complete ISplit Iteration
[Info] Levels: [1.3752201, 1.5719779, 1.5765469]
[Info] Cond. Prob: [0.1, 0.1, 0.9]
[Info] Error Prob: 0.009000000000000003

 56327/100000: episode: 979, duration: 4.561s, episode steps: 33, steps per second: 7, episode reward: 26.983, mean reward: 0.818 [0.705, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.226, 10.613], loss: 0.001288, mae: 0.038622, mean_q: 1.361173
 56427/100000: episode: 980, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.120, mean reward: 0.591 [0.505, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.552, 10.098], loss: 0.001558, mae: 0.041890, mean_q: 1.361917
 56527/100000: episode: 981, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.671, mean reward: 0.587 [0.511, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.752, 10.098], loss: 0.001525, mae: 0.041969, mean_q: 1.359757
 56627/100000: episode: 982, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 61.349, mean reward: 0.613 [0.532, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.285, 10.098], loss: 0.001459, mae: 0.041038, mean_q: 1.372218
 56727/100000: episode: 983, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.488, mean reward: 0.585 [0.512, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.429, 10.098], loss: 0.001638, mae: 0.042744, mean_q: 1.363330
 56827/100000: episode: 984, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.532, mean reward: 0.585 [0.503, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.596, 10.258], loss: 0.001550, mae: 0.041565, mean_q: 1.361855
 56927/100000: episode: 985, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 58.664, mean reward: 0.587 [0.505, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.577, 10.098], loss: 0.001399, mae: 0.041388, mean_q: 1.364363
 57027/100000: episode: 986, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: 57.300, mean reward: 0.573 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.466, 10.106], loss: 0.001471, mae: 0.041482, mean_q: 1.359856
 57127/100000: episode: 987, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 61.157, mean reward: 0.612 [0.512, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.743, 10.098], loss: 0.001438, mae: 0.041011, mean_q: 1.358376
 57227/100000: episode: 988, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.518, mean reward: 0.575 [0.499, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.266, 10.101], loss: 0.001443, mae: 0.041395, mean_q: 1.358600
 57327/100000: episode: 989, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 59.758, mean reward: 0.598 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.322, 10.202], loss: 0.001514, mae: 0.042756, mean_q: 1.352014
 57427/100000: episode: 990, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 61.479, mean reward: 0.615 [0.507, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.846, 10.527], loss: 0.001413, mae: 0.040689, mean_q: 1.337455
 57527/100000: episode: 991, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.078, mean reward: 0.571 [0.501, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.892, 10.098], loss: 0.001873, mae: 0.043445, mean_q: 1.343282
 57627/100000: episode: 992, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.366, mean reward: 0.594 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.026, 10.242], loss: 0.001729, mae: 0.044737, mean_q: 1.333450
 57727/100000: episode: 993, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.298, mean reward: 0.573 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.154, 10.201], loss: 0.001911, mae: 0.045622, mean_q: 1.333743
 57827/100000: episode: 994, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 60.596, mean reward: 0.606 [0.514, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.197, 10.098], loss: 0.001538, mae: 0.042221, mean_q: 1.333740
 57927/100000: episode: 995, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.519, mean reward: 0.605 [0.505, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.528, 10.285], loss: 0.001971, mae: 0.045296, mean_q: 1.335252
 58027/100000: episode: 996, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 59.615, mean reward: 0.596 [0.505, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.871, 10.098], loss: 0.001701, mae: 0.043292, mean_q: 1.318721
 58127/100000: episode: 997, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 59.646, mean reward: 0.596 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.793, 10.248], loss: 0.001706, mae: 0.043442, mean_q: 1.322396
 58227/100000: episode: 998, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.874, mean reward: 0.579 [0.514, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.709, 10.226], loss: 0.001541, mae: 0.043118, mean_q: 1.319861
 58327/100000: episode: 999, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.049, mean reward: 0.570 [0.499, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.985, 10.098], loss: 0.001433, mae: 0.040239, mean_q: 1.319824
 58427/100000: episode: 1000, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 56.535, mean reward: 0.565 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.425, 10.247], loss: 0.001502, mae: 0.042279, mean_q: 1.309675
 58527/100000: episode: 1001, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.300, mean reward: 0.583 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.963, 10.129], loss: 0.001569, mae: 0.042928, mean_q: 1.305323
 58627/100000: episode: 1002, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.696, mean reward: 0.597 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.093, 10.218], loss: 0.001857, mae: 0.044903, mean_q: 1.299413
 58727/100000: episode: 1003, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.997, mean reward: 0.580 [0.501, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.742, 10.157], loss: 0.001631, mae: 0.044056, mean_q: 1.293840
 58827/100000: episode: 1004, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 56.785, mean reward: 0.568 [0.498, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.623, 10.098], loss: 0.001798, mae: 0.043922, mean_q: 1.289890
 58927/100000: episode: 1005, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 61.592, mean reward: 0.616 [0.499, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.901, 10.098], loss: 0.001873, mae: 0.044837, mean_q: 1.298624
 59027/100000: episode: 1006, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 63.317, mean reward: 0.633 [0.502, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.181, 10.193], loss: 0.001609, mae: 0.043372, mean_q: 1.288204
 59127/100000: episode: 1007, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.672, mean reward: 0.577 [0.500, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.634, 10.108], loss: 0.001857, mae: 0.045256, mean_q: 1.288635
 59227/100000: episode: 1008, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 61.010, mean reward: 0.610 [0.512, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.399, 10.098], loss: 0.002004, mae: 0.047235, mean_q: 1.284917
 59327/100000: episode: 1009, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 61.049, mean reward: 0.610 [0.499, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.582, 10.098], loss: 0.001944, mae: 0.046273, mean_q: 1.283535
 59427/100000: episode: 1010, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 60.690, mean reward: 0.607 [0.506, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.151, 10.117], loss: 0.001722, mae: 0.043749, mean_q: 1.281343
 59527/100000: episode: 1011, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.376, mean reward: 0.574 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.538, 10.098], loss: 0.001747, mae: 0.044386, mean_q: 1.277317
 59627/100000: episode: 1012, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.426, mean reward: 0.594 [0.516, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.163, 10.331], loss: 0.001566, mae: 0.043544, mean_q: 1.272000
 59727/100000: episode: 1013, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.000, mean reward: 0.590 [0.500, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.705, 10.213], loss: 0.001514, mae: 0.042471, mean_q: 1.274802
 59827/100000: episode: 1014, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 63.296, mean reward: 0.633 [0.506, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.421, 10.098], loss: 0.001760, mae: 0.044085, mean_q: 1.264541
 59927/100000: episode: 1015, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.482, mean reward: 0.585 [0.507, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.172, 10.248], loss: 0.001647, mae: 0.043344, mean_q: 1.259661
 60027/100000: episode: 1016, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.876, mean reward: 0.579 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.518, 10.246], loss: 0.001587, mae: 0.043277, mean_q: 1.259044
 60127/100000: episode: 1017, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.353, mean reward: 0.604 [0.498, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.915, 10.128], loss: 0.001451, mae: 0.041974, mean_q: 1.252390
 60227/100000: episode: 1018, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.122, mean reward: 0.591 [0.508, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.627, 10.098], loss: 0.001708, mae: 0.044640, mean_q: 1.250029
 60327/100000: episode: 1019, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.295, mean reward: 0.583 [0.499, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.658, 10.098], loss: 0.001543, mae: 0.043143, mean_q: 1.247864
 60427/100000: episode: 1020, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 59.855, mean reward: 0.599 [0.505, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.033, 10.098], loss: 0.001475, mae: 0.041638, mean_q: 1.233934
 60527/100000: episode: 1021, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.822, mean reward: 0.588 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.163, 10.281], loss: 0.001413, mae: 0.041218, mean_q: 1.221011
 60627/100000: episode: 1022, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 60.338, mean reward: 0.603 [0.521, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.624, 10.098], loss: 0.001513, mae: 0.042709, mean_q: 1.218879
 60727/100000: episode: 1023, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 58.433, mean reward: 0.584 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.748, 10.098], loss: 0.001480, mae: 0.042021, mean_q: 1.211584
 60827/100000: episode: 1024, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.441, mean reward: 0.584 [0.507, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.203, 10.098], loss: 0.001530, mae: 0.043031, mean_q: 1.205536
 60927/100000: episode: 1025, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.439, mean reward: 0.594 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.916, 10.098], loss: 0.001414, mae: 0.041330, mean_q: 1.195912
 61027/100000: episode: 1026, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.336, mean reward: 0.593 [0.512, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.565, 10.258], loss: 0.001475, mae: 0.042365, mean_q: 1.195816
 61127/100000: episode: 1027, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.071, mean reward: 0.591 [0.506, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.535, 10.098], loss: 0.001484, mae: 0.042178, mean_q: 1.183565
 61227/100000: episode: 1028, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 61.057, mean reward: 0.611 [0.507, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.697, 10.577], loss: 0.001513, mae: 0.042762, mean_q: 1.181700
 61327/100000: episode: 1029, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.921, mean reward: 0.589 [0.500, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.143, 10.098], loss: 0.001455, mae: 0.041690, mean_q: 1.173721
 61427/100000: episode: 1030, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.617, mean reward: 0.586 [0.503, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.698, 10.131], loss: 0.001452, mae: 0.041749, mean_q: 1.172883
 61527/100000: episode: 1031, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.277, mean reward: 0.583 [0.509, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.788, 10.133], loss: 0.001391, mae: 0.041560, mean_q: 1.170869
 61627/100000: episode: 1032, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.194, mean reward: 0.572 [0.501, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.541, 10.151], loss: 0.001411, mae: 0.041556, mean_q: 1.169872
 61727/100000: episode: 1033, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 62.386, mean reward: 0.624 [0.512, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.667, 10.098], loss: 0.001404, mae: 0.041215, mean_q: 1.171969
 61827/100000: episode: 1034, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.213, mean reward: 0.582 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.602, 10.111], loss: 0.001343, mae: 0.040602, mean_q: 1.175923
 61927/100000: episode: 1035, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.039, mean reward: 0.580 [0.506, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.545, 10.197], loss: 0.001371, mae: 0.041010, mean_q: 1.174206
 62027/100000: episode: 1036, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 62.239, mean reward: 0.622 [0.506, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.879, 10.277], loss: 0.001369, mae: 0.040535, mean_q: 1.172109
 62127/100000: episode: 1037, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 59.183, mean reward: 0.592 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.667, 10.113], loss: 0.001377, mae: 0.040673, mean_q: 1.169297
 62227/100000: episode: 1038, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.586, mean reward: 0.596 [0.514, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.139, 10.209], loss: 0.001402, mae: 0.041005, mean_q: 1.171527
 62327/100000: episode: 1039, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 59.184, mean reward: 0.592 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.737, 10.225], loss: 0.001465, mae: 0.042555, mean_q: 1.174909
 62427/100000: episode: 1040, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.374, mean reward: 0.594 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.344, 10.416], loss: 0.001405, mae: 0.041602, mean_q: 1.175107
 62527/100000: episode: 1041, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.309, mean reward: 0.573 [0.504, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.346, 10.098], loss: 0.001444, mae: 0.041671, mean_q: 1.174873
 62627/100000: episode: 1042, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.308, mean reward: 0.583 [0.503, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.172, 10.098], loss: 0.001396, mae: 0.041062, mean_q: 1.170018
 62727/100000: episode: 1043, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.830, mean reward: 0.588 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.151, 10.173], loss: 0.001485, mae: 0.042713, mean_q: 1.173674
 62827/100000: episode: 1044, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 61.083, mean reward: 0.611 [0.521, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.550, 10.279], loss: 0.001346, mae: 0.040570, mean_q: 1.170627
 62927/100000: episode: 1045, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.983, mean reward: 0.600 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.703, 10.098], loss: 0.001412, mae: 0.041274, mean_q: 1.172421
 63027/100000: episode: 1046, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.142, mean reward: 0.581 [0.498, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.570, 10.133], loss: 0.001449, mae: 0.042413, mean_q: 1.175440
 63127/100000: episode: 1047, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.574, mean reward: 0.606 [0.498, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.443, 10.098], loss: 0.001393, mae: 0.041442, mean_q: 1.170080
 63227/100000: episode: 1048, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 61.809, mean reward: 0.618 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.728, 10.098], loss: 0.001350, mae: 0.040852, mean_q: 1.171338
 63327/100000: episode: 1049, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.774, mean reward: 0.588 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.396, 10.098], loss: 0.001311, mae: 0.040114, mean_q: 1.171373
 63427/100000: episode: 1050, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.232, mean reward: 0.582 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.000, 10.098], loss: 0.001389, mae: 0.040973, mean_q: 1.174540
 63527/100000: episode: 1051, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 56.823, mean reward: 0.568 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.519, 10.238], loss: 0.001367, mae: 0.040888, mean_q: 1.173459
 63627/100000: episode: 1052, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.856, mean reward: 0.589 [0.513, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.413, 10.200], loss: 0.001423, mae: 0.041528, mean_q: 1.173291
 63727/100000: episode: 1053, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.252, mean reward: 0.573 [0.502, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.778, 10.098], loss: 0.001377, mae: 0.040931, mean_q: 1.177894
 63827/100000: episode: 1054, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.957, mean reward: 0.570 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.529, 10.407], loss: 0.001394, mae: 0.040753, mean_q: 1.175376
 63927/100000: episode: 1055, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.989, mean reward: 0.580 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.890, 10.107], loss: 0.001399, mae: 0.041573, mean_q: 1.175649
 64027/100000: episode: 1056, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 56.033, mean reward: 0.560 [0.499, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.063, 10.098], loss: 0.001373, mae: 0.041077, mean_q: 1.169737
 64127/100000: episode: 1057, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.595, mean reward: 0.586 [0.507, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.315, 10.233], loss: 0.001302, mae: 0.039874, mean_q: 1.172461
 64227/100000: episode: 1058, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 61.002, mean reward: 0.610 [0.508, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.638, 10.332], loss: 0.001406, mae: 0.041319, mean_q: 1.170771
 64327/100000: episode: 1059, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.926, mean reward: 0.589 [0.511, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.426, 10.098], loss: 0.001427, mae: 0.041195, mean_q: 1.171182
 64427/100000: episode: 1060, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 60.272, mean reward: 0.603 [0.502, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.652, 10.137], loss: 0.001320, mae: 0.039663, mean_q: 1.170799
 64527/100000: episode: 1061, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.223, mean reward: 0.582 [0.500, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.253, 10.098], loss: 0.001289, mae: 0.039618, mean_q: 1.170063
 64627/100000: episode: 1062, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.998, mean reward: 0.600 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.787, 10.098], loss: 0.001292, mae: 0.039532, mean_q: 1.171252
 64727/100000: episode: 1063, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.135, mean reward: 0.591 [0.501, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.303, 10.126], loss: 0.001299, mae: 0.039949, mean_q: 1.173702
 64827/100000: episode: 1064, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.242, mean reward: 0.582 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.825, 10.098], loss: 0.001310, mae: 0.040106, mean_q: 1.171772
 64927/100000: episode: 1065, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.375, mean reward: 0.564 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.494, 10.098], loss: 0.001367, mae: 0.041064, mean_q: 1.167428
 65027/100000: episode: 1066, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 59.374, mean reward: 0.594 [0.509, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.364, 10.263], loss: 0.001326, mae: 0.039707, mean_q: 1.166475
 65127/100000: episode: 1067, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.222, mean reward: 0.592 [0.507, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.711, 10.374], loss: 0.001366, mae: 0.040888, mean_q: 1.167904
 65227/100000: episode: 1068, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.189, mean reward: 0.572 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.098], loss: 0.001310, mae: 0.040403, mean_q: 1.168345
 65327/100000: episode: 1069, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.171, mean reward: 0.582 [0.507, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.647, 10.199], loss: 0.001362, mae: 0.040250, mean_q: 1.165494
 65427/100000: episode: 1070, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.227, mean reward: 0.592 [0.511, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.155, 10.295], loss: 0.001281, mae: 0.039313, mean_q: 1.166087
 65527/100000: episode: 1071, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.949, mean reward: 0.589 [0.507, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.758, 10.204], loss: 0.001354, mae: 0.039732, mean_q: 1.165380
 65627/100000: episode: 1072, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.149, mean reward: 0.581 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.391, 10.098], loss: 0.001277, mae: 0.039249, mean_q: 1.164312
 65727/100000: episode: 1073, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.363, mean reward: 0.594 [0.511, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.099, 10.180], loss: 0.001288, mae: 0.040000, mean_q: 1.163543
 65827/100000: episode: 1074, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 60.171, mean reward: 0.602 [0.502, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.249, 10.459], loss: 0.001365, mae: 0.040655, mean_q: 1.167568
 65927/100000: episode: 1075, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.380, mean reward: 0.574 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.739, 10.137], loss: 0.001421, mae: 0.041341, mean_q: 1.168594
 66027/100000: episode: 1076, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.638, mean reward: 0.586 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.895, 10.098], loss: 0.001401, mae: 0.040944, mean_q: 1.161732
 66127/100000: episode: 1077, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.583, mean reward: 0.576 [0.502, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.910, 10.189], loss: 0.001328, mae: 0.040038, mean_q: 1.166411
 66227/100000: episode: 1078, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 59.253, mean reward: 0.593 [0.499, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.410, 10.098], loss: 0.001302, mae: 0.039030, mean_q: 1.160614
[Info] 1-TH LEVEL FOUND: 1.3874402046203613, Considering 10/90 traces
 66327/100000: episode: 1079, duration: 4.749s, episode steps: 100, steps per second: 21, episode reward: 60.413, mean reward: 0.604 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.111, 10.098], loss: 0.001363, mae: 0.040411, mean_q: 1.162971
 66351/100000: episode: 1080, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 15.363, mean reward: 0.640 [0.576, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.094, 10.277], loss: 0.001465, mae: 0.042219, mean_q: 1.165277
 66390/100000: episode: 1081, duration: 0.214s, episode steps: 39, steps per second: 183, episode reward: 26.566, mean reward: 0.681 [0.570, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.275, 10.100], loss: 0.001480, mae: 0.042321, mean_q: 1.161720
 66422/100000: episode: 1082, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 23.198, mean reward: 0.725 [0.648, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.058, 10.100], loss: 0.001321, mae: 0.040297, mean_q: 1.171743
 66429/100000: episode: 1083, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 4.712, mean reward: 0.673 [0.648, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.332, 10.100], loss: 0.001437, mae: 0.041614, mean_q: 1.165090
 66461/100000: episode: 1084, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 22.082, mean reward: 0.690 [0.629, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.599, 10.100], loss: 0.001281, mae: 0.039077, mean_q: 1.163406
 66551/100000: episode: 1085, duration: 0.455s, episode steps: 90, steps per second: 198, episode reward: 53.704, mean reward: 0.597 [0.519, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.546 [-0.952, 10.305], loss: 0.001274, mae: 0.039372, mean_q: 1.168832
 66590/100000: episode: 1086, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 29.367, mean reward: 0.753 [0.653, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.236, 10.100], loss: 0.001378, mae: 0.041242, mean_q: 1.169521
 66602/100000: episode: 1087, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 8.492, mean reward: 0.708 [0.639, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.831, 10.339], loss: 0.001610, mae: 0.043909, mean_q: 1.173808
 66614/100000: episode: 1088, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 8.140, mean reward: 0.678 [0.608, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.292, 10.423], loss: 0.001624, mae: 0.042856, mean_q: 1.183819
 66704/100000: episode: 1089, duration: 0.471s, episode steps: 90, steps per second: 191, episode reward: 52.539, mean reward: 0.584 [0.508, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.645, 10.233], loss: 0.001629, mae: 0.042897, mean_q: 1.170507
 66711/100000: episode: 1090, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 5.033, mean reward: 0.719 [0.686, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.352, 10.100], loss: 0.001364, mae: 0.041970, mean_q: 1.173666
 66723/100000: episode: 1091, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 8.135, mean reward: 0.678 [0.633, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.309, 10.417], loss: 0.001546, mae: 0.043060, mean_q: 1.172250
 66735/100000: episode: 1092, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 8.677, mean reward: 0.723 [0.644, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.322], loss: 0.001517, mae: 0.041713, mean_q: 1.168669
 66742/100000: episode: 1093, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 4.861, mean reward: 0.694 [0.657, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.452, 10.100], loss: 0.001488, mae: 0.041629, mean_q: 1.173867
 66749/100000: episode: 1094, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.753, mean reward: 0.679 [0.635, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.287, 10.100], loss: 0.001293, mae: 0.038193, mean_q: 1.155676
 66769/100000: episode: 1095, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 14.448, mean reward: 0.722 [0.648, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.355], loss: 0.001435, mae: 0.041909, mean_q: 1.169253
 66787/100000: episode: 1096, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 11.353, mean reward: 0.631 [0.528, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.283, 10.100], loss: 0.001486, mae: 0.042627, mean_q: 1.174113
 66807/100000: episode: 1097, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 14.561, mean reward: 0.728 [0.650, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.142, 10.385], loss: 0.001251, mae: 0.039151, mean_q: 1.175520
 66897/100000: episode: 1098, duration: 0.455s, episode steps: 90, steps per second: 198, episode reward: 52.879, mean reward: 0.588 [0.505, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-0.668, 10.100], loss: 0.001671, mae: 0.043265, mean_q: 1.167514
 66948/100000: episode: 1099, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 32.374, mean reward: 0.635 [0.505, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.590, 10.100], loss: 0.001664, mae: 0.044256, mean_q: 1.170602
 66996/100000: episode: 1100, duration: 0.248s, episode steps: 48, steps per second: 194, episode reward: 31.021, mean reward: 0.646 [0.510, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-2.577, 10.136], loss: 0.001432, mae: 0.040910, mean_q: 1.171229
 67014/100000: episode: 1101, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 11.994, mean reward: 0.666 [0.627, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.318, 10.100], loss: 0.001474, mae: 0.042226, mean_q: 1.170760
 67053/100000: episode: 1102, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 25.327, mean reward: 0.649 [0.558, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.613, 10.100], loss: 0.001584, mae: 0.042010, mean_q: 1.171483
 67071/100000: episode: 1103, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 11.808, mean reward: 0.656 [0.537, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.805, 10.100], loss: 0.001644, mae: 0.043353, mean_q: 1.174188
 67083/100000: episode: 1104, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 9.413, mean reward: 0.784 [0.714, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.537], loss: 0.001749, mae: 0.044680, mean_q: 1.168144
 67115/100000: episode: 1105, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 19.003, mean reward: 0.594 [0.519, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.680, 10.100], loss: 0.001476, mae: 0.041759, mean_q: 1.179437
 67133/100000: episode: 1106, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 11.413, mean reward: 0.634 [0.557, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.183, 10.100], loss: 0.001499, mae: 0.041983, mean_q: 1.167574
 67151/100000: episode: 1107, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 12.579, mean reward: 0.699 [0.644, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.353, 10.100], loss: 0.001468, mae: 0.041702, mean_q: 1.177218
 67171/100000: episode: 1108, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 13.836, mean reward: 0.692 [0.628, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.401], loss: 0.001475, mae: 0.041580, mean_q: 1.166860
 67222/100000: episode: 1109, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 33.735, mean reward: 0.661 [0.578, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.617, 10.301], loss: 0.001655, mae: 0.043983, mean_q: 1.180107
 67240/100000: episode: 1110, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 12.801, mean reward: 0.711 [0.660, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.271, 10.100], loss: 0.001584, mae: 0.043157, mean_q: 1.177813
 67279/100000: episode: 1111, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 25.035, mean reward: 0.642 [0.528, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.352, 10.255], loss: 0.001563, mae: 0.042551, mean_q: 1.171866
 67318/100000: episode: 1112, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 26.965, mean reward: 0.691 [0.579, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.946, 10.100], loss: 0.001859, mae: 0.045840, mean_q: 1.173842
 67342/100000: episode: 1113, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 16.775, mean reward: 0.699 [0.652, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.208, 10.454], loss: 0.001977, mae: 0.046677, mean_q: 1.178244
 67349/100000: episode: 1114, duration: 0.044s, episode steps: 7, steps per second: 157, episode reward: 5.004, mean reward: 0.715 [0.681, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.373, 10.100], loss: 0.001755, mae: 0.046924, mean_q: 1.179266
 67356/100000: episode: 1115, duration: 0.053s, episode steps: 7, steps per second: 132, episode reward: 4.833, mean reward: 0.690 [0.645, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.358, 10.100], loss: 0.001966, mae: 0.048481, mean_q: 1.186688
 67395/100000: episode: 1116, duration: 0.218s, episode steps: 39, steps per second: 179, episode reward: 26.699, mean reward: 0.685 [0.590, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.773, 10.100], loss: 0.001817, mae: 0.045250, mean_q: 1.178030
 67434/100000: episode: 1117, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 24.481, mean reward: 0.628 [0.516, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-1.282, 10.100], loss: 0.001626, mae: 0.044245, mean_q: 1.184053
 67452/100000: episode: 1118, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 13.580, mean reward: 0.754 [0.654, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.470, 10.100], loss: 0.001517, mae: 0.041442, mean_q: 1.178842
 67464/100000: episode: 1119, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 8.501, mean reward: 0.708 [0.622, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.336, 10.504], loss: 0.001748, mae: 0.045116, mean_q: 1.186636
 67554/100000: episode: 1120, duration: 0.492s, episode steps: 90, steps per second: 183, episode reward: 54.028, mean reward: 0.600 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.807, 10.222], loss: 0.001543, mae: 0.042408, mean_q: 1.187386
 67561/100000: episode: 1121, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 4.938, mean reward: 0.705 [0.676, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.283, 10.100], loss: 0.001892, mae: 0.045789, mean_q: 1.160612
 67593/100000: episode: 1122, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 19.319, mean reward: 0.604 [0.539, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.136, 10.100], loss: 0.001815, mae: 0.044927, mean_q: 1.180788
 67611/100000: episode: 1123, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 12.745, mean reward: 0.708 [0.670, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.268, 10.100], loss: 0.001745, mae: 0.044769, mean_q: 1.191445
 67659/100000: episode: 1124, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 33.149, mean reward: 0.691 [0.598, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.661, 10.288], loss: 0.001688, mae: 0.044033, mean_q: 1.182020
 67666/100000: episode: 1125, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 5.125, mean reward: 0.732 [0.684, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.424, 10.100], loss: 0.001529, mae: 0.041406, mean_q: 1.175074
 67717/100000: episode: 1126, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 33.792, mean reward: 0.663 [0.549, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.036, 10.345], loss: 0.001840, mae: 0.045303, mean_q: 1.190908
 67765/100000: episode: 1127, duration: 0.247s, episode steps: 48, steps per second: 194, episode reward: 29.771, mean reward: 0.620 [0.533, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-1.414, 10.222], loss: 0.001505, mae: 0.041217, mean_q: 1.183269
 67777/100000: episode: 1128, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 8.469, mean reward: 0.706 [0.647, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.518], loss: 0.001227, mae: 0.038100, mean_q: 1.185320
 67797/100000: episode: 1129, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 14.186, mean reward: 0.709 [0.616, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.824, 10.450], loss: 0.001544, mae: 0.042611, mean_q: 1.187641
 67809/100000: episode: 1130, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 8.339, mean reward: 0.695 [0.657, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.397], loss: 0.001542, mae: 0.042012, mean_q: 1.196698
 67833/100000: episode: 1131, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 15.464, mean reward: 0.644 [0.572, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.086, 10.316], loss: 0.001318, mae: 0.039622, mean_q: 1.192708
 67853/100000: episode: 1132, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 15.364, mean reward: 0.768 [0.667, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.045, 10.464], loss: 0.001378, mae: 0.039877, mean_q: 1.177489
 67904/100000: episode: 1133, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 36.849, mean reward: 0.723 [0.591, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.825, 10.324], loss: 0.001719, mae: 0.044444, mean_q: 1.196010
 67936/100000: episode: 1134, duration: 0.161s, episode steps: 32, steps per second: 198, episode reward: 22.616, mean reward: 0.707 [0.600, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.373, 10.100], loss: 0.001750, mae: 0.045293, mean_q: 1.198422
 67956/100000: episode: 1135, duration: 0.098s, episode steps: 20, steps per second: 203, episode reward: 16.009, mean reward: 0.800 [0.677, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.886, 10.581], loss: 0.001814, mae: 0.044101, mean_q: 1.186286
 68004/100000: episode: 1136, duration: 0.252s, episode steps: 48, steps per second: 190, episode reward: 30.351, mean reward: 0.632 [0.556, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-1.110, 10.236], loss: 0.001630, mae: 0.043364, mean_q: 1.197235
 68052/100000: episode: 1137, duration: 0.247s, episode steps: 48, steps per second: 194, episode reward: 32.248, mean reward: 0.672 [0.575, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.658, 10.410], loss: 0.001731, mae: 0.043719, mean_q: 1.199932
 68103/100000: episode: 1138, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 34.373, mean reward: 0.674 [0.576, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.085, 10.483], loss: 0.001857, mae: 0.045006, mean_q: 1.198287
 68110/100000: episode: 1139, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 5.001, mean reward: 0.714 [0.693, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.213, 10.100], loss: 0.002597, mae: 0.048346, mean_q: 1.190563
 68130/100000: episode: 1140, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 14.134, mean reward: 0.707 [0.658, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.790, 10.450], loss: 0.002154, mae: 0.048609, mean_q: 1.184742
 68150/100000: episode: 1141, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 13.301, mean reward: 0.665 [0.590, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.071, 10.369], loss: 0.001784, mae: 0.045325, mean_q: 1.201084
 68168/100000: episode: 1142, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 13.525, mean reward: 0.751 [0.691, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.731, 10.100], loss: 0.001778, mae: 0.045218, mean_q: 1.193453
 68180/100000: episode: 1143, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 8.337, mean reward: 0.695 [0.649, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.644, 10.448], loss: 0.001669, mae: 0.044020, mean_q: 1.208578
 68198/100000: episode: 1144, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 12.842, mean reward: 0.713 [0.667, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.260, 10.100], loss: 0.001482, mae: 0.042264, mean_q: 1.196717
 68222/100000: episode: 1145, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 14.589, mean reward: 0.608 [0.524, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.174], loss: 0.001778, mae: 0.044427, mean_q: 1.213467
 68229/100000: episode: 1146, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 5.124, mean reward: 0.732 [0.694, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.308, 10.100], loss: 0.001867, mae: 0.045167, mean_q: 1.217882
 68280/100000: episode: 1147, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 36.365, mean reward: 0.713 [0.636, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.390, 10.408], loss: 0.001754, mae: 0.045226, mean_q: 1.199364
 68370/100000: episode: 1148, duration: 0.465s, episode steps: 90, steps per second: 194, episode reward: 52.772, mean reward: 0.586 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.776, 10.100], loss: 0.001819, mae: 0.044905, mean_q: 1.206181
 68409/100000: episode: 1149, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 27.679, mean reward: 0.710 [0.544, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.307, 10.100], loss: 0.001507, mae: 0.040641, mean_q: 1.208864
 68421/100000: episode: 1150, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 8.119, mean reward: 0.677 [0.644, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.792, 10.359], loss: 0.001827, mae: 0.047022, mean_q: 1.208054
 68511/100000: episode: 1151, duration: 0.473s, episode steps: 90, steps per second: 190, episode reward: 53.995, mean reward: 0.600 [0.508, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-0.678, 10.100], loss: 0.001794, mae: 0.044516, mean_q: 1.204318
 68601/100000: episode: 1152, duration: 0.461s, episode steps: 90, steps per second: 195, episode reward: 53.131, mean reward: 0.590 [0.515, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-1.786, 10.100], loss: 0.001625, mae: 0.042577, mean_q: 1.210974
 68621/100000: episode: 1153, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 13.594, mean reward: 0.680 [0.630, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.316, 10.334], loss: 0.001708, mae: 0.044923, mean_q: 1.205362
 68633/100000: episode: 1154, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 7.696, mean reward: 0.641 [0.577, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.337], loss: 0.001980, mae: 0.045087, mean_q: 1.212876
 68653/100000: episode: 1155, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 13.798, mean reward: 0.690 [0.637, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.499], loss: 0.001784, mae: 0.044543, mean_q: 1.213345
 68673/100000: episode: 1156, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 14.279, mean reward: 0.714 [0.639, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.859, 10.421], loss: 0.001558, mae: 0.042868, mean_q: 1.203860
 68712/100000: episode: 1157, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 26.866, mean reward: 0.689 [0.572, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.360, 10.100], loss: 0.001542, mae: 0.041213, mean_q: 1.210959
 68736/100000: episode: 1158, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 15.316, mean reward: 0.638 [0.589, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.705, 10.444], loss: 0.001492, mae: 0.041170, mean_q: 1.211523
 68775/100000: episode: 1159, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 27.235, mean reward: 0.698 [0.618, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.544, 10.100], loss: 0.001433, mae: 0.040222, mean_q: 1.213698
 68795/100000: episode: 1160, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 13.994, mean reward: 0.700 [0.581, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.453], loss: 0.001438, mae: 0.041326, mean_q: 1.214773
 68843/100000: episode: 1161, duration: 0.257s, episode steps: 48, steps per second: 187, episode reward: 29.027, mean reward: 0.605 [0.534, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.896, 10.174], loss: 0.001481, mae: 0.042009, mean_q: 1.217049
 68867/100000: episode: 1162, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 17.002, mean reward: 0.708 [0.610, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.368, 10.376], loss: 0.001517, mae: 0.042755, mean_q: 1.219071
 68906/100000: episode: 1163, duration: 0.219s, episode steps: 39, steps per second: 178, episode reward: 27.772, mean reward: 0.712 [0.615, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.736, 10.100], loss: 0.001657, mae: 0.044156, mean_q: 1.213967
 68945/100000: episode: 1164, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 28.903, mean reward: 0.741 [0.524, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.068, 10.100], loss: 0.001791, mae: 0.044950, mean_q: 1.225218
 68965/100000: episode: 1165, duration: 0.125s, episode steps: 20, steps per second: 160, episode reward: 12.602, mean reward: 0.630 [0.548, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.444, 10.271], loss: 0.001966, mae: 0.045979, mean_q: 1.218435
 69004/100000: episode: 1166, duration: 0.217s, episode steps: 39, steps per second: 180, episode reward: 26.742, mean reward: 0.686 [0.550, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.189, 10.100], loss: 0.001774, mae: 0.045131, mean_q: 1.219263
 69055/100000: episode: 1167, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 31.523, mean reward: 0.618 [0.541, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.445, 10.254], loss: 0.001780, mae: 0.044552, mean_q: 1.224480
 69075/100000: episode: 1168, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 14.327, mean reward: 0.716 [0.629, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.429, 10.385], loss: 0.001718, mae: 0.043947, mean_q: 1.223836
[Info] 2-TH LEVEL FOUND: 1.5413480997085571, Considering 10/90 traces
 69126/100000: episode: 1169, duration: 4.392s, episode steps: 51, steps per second: 12, episode reward: 34.424, mean reward: 0.675 [0.576, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.377, 10.360], loss: 0.001612, mae: 0.042474, mean_q: 1.229864
 69146/100000: episode: 1170, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 16.047, mean reward: 0.802 [0.740, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.488, 10.100], loss: 0.001600, mae: 0.043301, mean_q: 1.226345
 69183/100000: episode: 1171, duration: 0.215s, episode steps: 37, steps per second: 172, episode reward: 25.476, mean reward: 0.689 [0.579, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.954, 10.289], loss: 0.001391, mae: 0.041020, mean_q: 1.234067
 69197/100000: episode: 1172, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 10.805, mean reward: 0.772 [0.702, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.648, 10.448], loss: 0.001619, mae: 0.043476, mean_q: 1.227748
 69211/100000: episode: 1173, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 11.014, mean reward: 0.787 [0.734, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.363, 10.555], loss: 0.001619, mae: 0.042319, mean_q: 1.228846
 69227/100000: episode: 1174, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 12.170, mean reward: 0.761 [0.677, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.614], loss: 0.001504, mae: 0.042906, mean_q: 1.240232
 69274/100000: episode: 1175, duration: 0.248s, episode steps: 47, steps per second: 189, episode reward: 29.199, mean reward: 0.621 [0.504, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.500, 10.100], loss: 0.001778, mae: 0.044939, mean_q: 1.227597
 69311/100000: episode: 1176, duration: 0.214s, episode steps: 37, steps per second: 173, episode reward: 22.455, mean reward: 0.607 [0.535, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.152, 10.100], loss: 0.001675, mae: 0.044761, mean_q: 1.219089
 69348/100000: episode: 1177, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 24.102, mean reward: 0.651 [0.530, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.035, 10.187], loss: 0.001710, mae: 0.044285, mean_q: 1.232711
 69364/100000: episode: 1178, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 11.624, mean reward: 0.727 [0.663, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.162, 10.449], loss: 0.001443, mae: 0.040808, mean_q: 1.227433
 69401/100000: episode: 1179, duration: 0.212s, episode steps: 37, steps per second: 175, episode reward: 25.287, mean reward: 0.683 [0.612, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.668, 10.401], loss: 0.001565, mae: 0.043208, mean_q: 1.233017
 69421/100000: episode: 1180, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 14.433, mean reward: 0.722 [0.678, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.152, 10.100], loss: 0.001434, mae: 0.041605, mean_q: 1.223018
 69450/100000: episode: 1181, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 20.586, mean reward: 0.710 [0.636, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.649, 10.100], loss: 0.001492, mae: 0.042482, mean_q: 1.241519
 69464/100000: episode: 1182, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 10.770, mean reward: 0.769 [0.677, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.607], loss: 0.001365, mae: 0.037991, mean_q: 1.219796
 69482/100000: episode: 1183, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 13.809, mean reward: 0.767 [0.648, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.518, 10.547], loss: 0.001770, mae: 0.043475, mean_q: 1.235290
 69500/100000: episode: 1184, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 14.850, mean reward: 0.825 [0.721, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.647, 10.656], loss: 0.001703, mae: 0.043628, mean_q: 1.229786
 69514/100000: episode: 1185, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 11.886, mean reward: 0.849 [0.733, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.832, 10.596], loss: 0.001689, mae: 0.043601, mean_q: 1.235821
 69560/100000: episode: 1186, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 31.397, mean reward: 0.683 [0.509, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.672, 10.197], loss: 0.001681, mae: 0.044626, mean_q: 1.244744
 69574/100000: episode: 1187, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 10.580, mean reward: 0.756 [0.662, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.507], loss: 0.001587, mae: 0.042807, mean_q: 1.221690
 69592/100000: episode: 1188, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 12.773, mean reward: 0.710 [0.605, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.053, 10.304], loss: 0.001775, mae: 0.045207, mean_q: 1.240110
 69610/100000: episode: 1189, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 13.644, mean reward: 0.758 [0.618, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.344], loss: 0.001572, mae: 0.042621, mean_q: 1.244956
 69656/100000: episode: 1190, duration: 0.238s, episode steps: 46, steps per second: 194, episode reward: 32.756, mean reward: 0.712 [0.633, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.707, 10.350], loss: 0.001567, mae: 0.042898, mean_q: 1.245717
 69676/100000: episode: 1191, duration: 0.121s, episode steps: 20, steps per second: 166, episode reward: 14.282, mean reward: 0.714 [0.650, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.269, 10.100], loss: 0.001493, mae: 0.042350, mean_q: 1.257594
 69713/100000: episode: 1192, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 25.330, mean reward: 0.685 [0.553, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-1.131, 10.263], loss: 0.001765, mae: 0.044681, mean_q: 1.250633
 69727/100000: episode: 1193, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 10.375, mean reward: 0.741 [0.699, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.780, 10.431], loss: 0.001680, mae: 0.044049, mean_q: 1.238422
 69756/100000: episode: 1194, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 20.157, mean reward: 0.695 [0.557, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.174, 10.100], loss: 0.001555, mae: 0.042683, mean_q: 1.239268
 69785/100000: episode: 1195, duration: 0.161s, episode steps: 29, steps per second: 181, episode reward: 19.291, mean reward: 0.665 [0.535, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.000, 10.100], loss: 0.001632, mae: 0.043064, mean_q: 1.239072
 69831/100000: episode: 1196, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 32.553, mean reward: 0.708 [0.619, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.381, 10.349], loss: 0.001849, mae: 0.046017, mean_q: 1.248572
 69868/100000: episode: 1197, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 27.197, mean reward: 0.735 [0.618, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.645, 10.388], loss: 0.001375, mae: 0.041209, mean_q: 1.264819
 69888/100000: episode: 1198, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 13.754, mean reward: 0.688 [0.570, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.100], loss: 0.001845, mae: 0.044562, mean_q: 1.256764
 69935/100000: episode: 1199, duration: 0.236s, episode steps: 47, steps per second: 199, episode reward: 28.826, mean reward: 0.613 [0.520, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.726, 10.100], loss: 0.001393, mae: 0.040813, mean_q: 1.256739
 69955/100000: episode: 1200, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 14.828, mean reward: 0.741 [0.632, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.244, 10.100], loss: 0.001366, mae: 0.039754, mean_q: 1.259733
 69973/100000: episode: 1201, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 12.873, mean reward: 0.715 [0.620, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.001, 10.403], loss: 0.001329, mae: 0.040450, mean_q: 1.262578
 70020/100000: episode: 1202, duration: 0.238s, episode steps: 47, steps per second: 198, episode reward: 29.212, mean reward: 0.622 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.908, 10.100], loss: 0.001662, mae: 0.043817, mean_q: 1.256515
 70067/100000: episode: 1203, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 28.616, mean reward: 0.609 [0.511, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.441, 10.100], loss: 0.001885, mae: 0.046557, mean_q: 1.258434
 70114/100000: episode: 1204, duration: 0.242s, episode steps: 47, steps per second: 194, episode reward: 29.168, mean reward: 0.621 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.289, 10.107], loss: 0.001492, mae: 0.041444, mean_q: 1.254987
 70143/100000: episode: 1205, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 21.928, mean reward: 0.756 [0.656, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.383, 10.100], loss: 0.001668, mae: 0.044377, mean_q: 1.257469
 70157/100000: episode: 1206, duration: 0.083s, episode steps: 14, steps per second: 170, episode reward: 11.518, mean reward: 0.823 [0.744, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.610], loss: 0.001598, mae: 0.041837, mean_q: 1.244505
 70177/100000: episode: 1207, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 15.708, mean reward: 0.785 [0.705, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.285, 10.100], loss: 0.001448, mae: 0.041459, mean_q: 1.247722
[Info] FALSIFICATION!
 70186/100000: episode: 1208, duration: 0.211s, episode steps: 9, steps per second: 43, episode reward: 8.288, mean reward: 0.921 [0.817, 1.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.718, 9.139], loss: 0.001589, mae: 0.042932, mean_q: 1.264408
 70215/100000: episode: 1209, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 21.545, mean reward: 0.743 [0.625, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-1.151, 10.100], loss: 0.001684, mae: 0.043783, mean_q: 1.253712
 70261/100000: episode: 1210, duration: 0.255s, episode steps: 46, steps per second: 180, episode reward: 30.864, mean reward: 0.671 [0.560, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.500, 10.293], loss: 0.001640, mae: 0.043051, mean_q: 1.268502
 70275/100000: episode: 1211, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 11.070, mean reward: 0.791 [0.735, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.531], loss: 0.001603, mae: 0.042015, mean_q: 1.267354
 70321/100000: episode: 1212, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 32.303, mean reward: 0.702 [0.563, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-1.476, 10.267], loss: 0.002058, mae: 0.046983, mean_q: 1.272022
 70358/100000: episode: 1213, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: 25.573, mean reward: 0.691 [0.625, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.046, 10.339], loss: 0.001547, mae: 0.042690, mean_q: 1.271541
 70378/100000: episode: 1214, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 15.574, mean reward: 0.779 [0.725, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.374, 10.100], loss: 0.001572, mae: 0.039634, mean_q: 1.280670
 70424/100000: episode: 1215, duration: 0.249s, episode steps: 46, steps per second: 185, episode reward: 35.863, mean reward: 0.780 [0.603, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.303, 10.317], loss: 0.001566, mae: 0.042209, mean_q: 1.277353
 70444/100000: episode: 1216, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 14.315, mean reward: 0.716 [0.627, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.274, 10.100], loss: 0.001577, mae: 0.040687, mean_q: 1.267893
 70460/100000: episode: 1217, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 10.868, mean reward: 0.679 [0.623, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.400], loss: 0.001544, mae: 0.043394, mean_q: 1.277426
 70506/100000: episode: 1218, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 30.552, mean reward: 0.664 [0.550, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.528, 10.309], loss: 0.001529, mae: 0.041519, mean_q: 1.268844
 70520/100000: episode: 1219, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 10.626, mean reward: 0.759 [0.693, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.533, 10.466], loss: 0.001431, mae: 0.040908, mean_q: 1.288543
 70567/100000: episode: 1220, duration: 0.235s, episode steps: 47, steps per second: 200, episode reward: 35.003, mean reward: 0.745 [0.620, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.574, 10.342], loss: 0.001445, mae: 0.041063, mean_q: 1.281435
 70596/100000: episode: 1221, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 19.342, mean reward: 0.667 [0.565, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.416, 10.100], loss: 0.001808, mae: 0.044419, mean_q: 1.286764
 70643/100000: episode: 1222, duration: 0.239s, episode steps: 47, steps per second: 197, episode reward: 32.152, mean reward: 0.684 [0.567, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-1.225, 10.272], loss: 0.001427, mae: 0.041204, mean_q: 1.290249
 70657/100000: episode: 1223, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 10.593, mean reward: 0.757 [0.667, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.101, 10.361], loss: 0.001516, mae: 0.043451, mean_q: 1.272640
 70704/100000: episode: 1224, duration: 0.239s, episode steps: 47, steps per second: 196, episode reward: 30.947, mean reward: 0.658 [0.523, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.262, 10.150], loss: 0.001527, mae: 0.041455, mean_q: 1.286580
 70720/100000: episode: 1225, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 12.612, mean reward: 0.788 [0.700, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-1.358, 10.495], loss: 0.001612, mae: 0.044487, mean_q: 1.295916
 70757/100000: episode: 1226, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 27.982, mean reward: 0.756 [0.660, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.727, 10.508], loss: 0.001279, mae: 0.039357, mean_q: 1.276851
 70803/100000: episode: 1227, duration: 0.254s, episode steps: 46, steps per second: 181, episode reward: 35.491, mean reward: 0.772 [0.687, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-1.058, 10.655], loss: 0.001391, mae: 0.040655, mean_q: 1.284143
 70849/100000: episode: 1228, duration: 0.242s, episode steps: 46, steps per second: 190, episode reward: 33.663, mean reward: 0.732 [0.534, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.297, 10.197], loss: 0.001330, mae: 0.039678, mean_q: 1.291943
 70867/100000: episode: 1229, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 13.392, mean reward: 0.744 [0.659, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.400, 10.384], loss: 0.001226, mae: 0.038948, mean_q: 1.292194
 70881/100000: episode: 1230, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 11.949, mean reward: 0.853 [0.755, 0.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.243, 10.588], loss: 0.001255, mae: 0.039299, mean_q: 1.292270
 70895/100000: episode: 1231, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 11.267, mean reward: 0.805 [0.703, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.527, 10.533], loss: 0.001391, mae: 0.041183, mean_q: 1.291603
 70941/100000: episode: 1232, duration: 0.248s, episode steps: 46, steps per second: 185, episode reward: 33.397, mean reward: 0.726 [0.615, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.876, 10.377], loss: 0.001362, mae: 0.040752, mean_q: 1.292881
 70988/100000: episode: 1233, duration: 0.246s, episode steps: 47, steps per second: 191, episode reward: 29.965, mean reward: 0.638 [0.535, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.293, 10.267], loss: 0.001526, mae: 0.041743, mean_q: 1.303077
 71017/100000: episode: 1234, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 23.533, mean reward: 0.811 [0.712, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.542, 10.100], loss: 0.001472, mae: 0.041462, mean_q: 1.308128
 71033/100000: episode: 1235, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 10.558, mean reward: 0.660 [0.558, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.362, 10.295], loss: 0.001388, mae: 0.042683, mean_q: 1.321760
 71079/100000: episode: 1236, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 31.281, mean reward: 0.680 [0.509, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.673, 10.100], loss: 0.001529, mae: 0.042810, mean_q: 1.309484
 71125/100000: episode: 1237, duration: 0.264s, episode steps: 46, steps per second: 174, episode reward: 29.522, mean reward: 0.642 [0.520, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.283, 10.100], loss: 0.001510, mae: 0.041099, mean_q: 1.300966
 71162/100000: episode: 1238, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 27.210, mean reward: 0.735 [0.670, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.820, 10.517], loss: 0.001321, mae: 0.040200, mean_q: 1.303171
 71199/100000: episode: 1239, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 24.285, mean reward: 0.656 [0.508, 0.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.035, 10.132], loss: 0.001637, mae: 0.041983, mean_q: 1.300069
 71213/100000: episode: 1240, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 10.959, mean reward: 0.783 [0.741, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.587], loss: 0.001528, mae: 0.038390, mean_q: 1.297133
 71250/100000: episode: 1241, duration: 0.204s, episode steps: 37, steps per second: 182, episode reward: 23.730, mean reward: 0.641 [0.527, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.981, 10.241], loss: 0.001371, mae: 0.040235, mean_q: 1.300279
 71279/100000: episode: 1242, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 20.213, mean reward: 0.697 [0.516, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.037, 10.100], loss: 0.001437, mae: 0.040233, mean_q: 1.303787
 71326/100000: episode: 1243, duration: 0.243s, episode steps: 47, steps per second: 194, episode reward: 31.762, mean reward: 0.676 [0.528, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.286, 10.177], loss: 0.001486, mae: 0.040786, mean_q: 1.311986
 71340/100000: episode: 1244, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 11.466, mean reward: 0.819 [0.749, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.104, 10.510], loss: 0.001308, mae: 0.039549, mean_q: 1.298771
 71356/100000: episode: 1245, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 11.869, mean reward: 0.742 [0.698, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.462], loss: 0.001743, mae: 0.044285, mean_q: 1.312719
 71385/100000: episode: 1246, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 22.607, mean reward: 0.780 [0.578, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-1.043, 10.100], loss: 0.001466, mae: 0.042505, mean_q: 1.313049
 71414/100000: episode: 1247, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 23.082, mean reward: 0.796 [0.734, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.950, 10.100], loss: 0.001436, mae: 0.040868, mean_q: 1.296897
 71432/100000: episode: 1248, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 15.030, mean reward: 0.835 [0.769, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.360, 10.590], loss: 0.001301, mae: 0.038804, mean_q: 1.311798
 71448/100000: episode: 1249, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 11.804, mean reward: 0.738 [0.650, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.260, 10.410], loss: 0.001256, mae: 0.039949, mean_q: 1.305388
 71466/100000: episode: 1250, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 12.041, mean reward: 0.669 [0.592, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.296], loss: 0.001359, mae: 0.040596, mean_q: 1.315292
 71495/100000: episode: 1251, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 19.726, mean reward: 0.680 [0.542, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.680, 10.100], loss: 0.001449, mae: 0.041955, mean_q: 1.319200
 71524/100000: episode: 1252, duration: 0.140s, episode steps: 29, steps per second: 207, episode reward: 24.904, mean reward: 0.859 [0.781, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-1.061, 10.100], loss: 0.001351, mae: 0.040783, mean_q: 1.315457
 71553/100000: episode: 1253, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 21.622, mean reward: 0.746 [0.662, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.281, 10.100], loss: 0.001319, mae: 0.039247, mean_q: 1.331937
 71599/100000: episode: 1254, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 31.255, mean reward: 0.679 [0.573, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-1.585, 10.575], loss: 0.001627, mae: 0.043971, mean_q: 1.324827
 71628/100000: episode: 1255, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 21.652, mean reward: 0.747 [0.672, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.207, 10.100], loss: 0.001262, mae: 0.038351, mean_q: 1.317955
 71648/100000: episode: 1256, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 16.494, mean reward: 0.825 [0.743, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.651, 10.100], loss: 0.001443, mae: 0.042685, mean_q: 1.334196
 71662/100000: episode: 1257, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 10.263, mean reward: 0.733 [0.697, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.121, 10.321], loss: 0.001291, mae: 0.038841, mean_q: 1.317208
 71680/100000: episode: 1258, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 14.895, mean reward: 0.828 [0.734, 0.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.117, 10.424], loss: 0.001498, mae: 0.042797, mean_q: 1.328628
[Info] Complete ISplit Iteration
[Info] Levels: [1.3874402, 1.5413481, 1.7219827]
[Info] Cond. Prob: [0.1, 0.1, 0.03]
[Info] Error Prob: 0.00030000000000000003

 71694/100000: episode: 1259, duration: 4.411s, episode steps: 14, steps per second: 3, episode reward: 10.900, mean reward: 0.779 [0.723, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.414], loss: 0.001167, mae: 0.037918, mean_q: 1.321612
 71794/100000: episode: 1260, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.882, mean reward: 0.599 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.934, 10.190], loss: 0.001446, mae: 0.041914, mean_q: 1.330457
 71894/100000: episode: 1261, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.191, mean reward: 0.572 [0.503, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.248, 10.197], loss: 0.001576, mae: 0.042622, mean_q: 1.323965
 71994/100000: episode: 1262, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.165, mean reward: 0.592 [0.504, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.614, 10.098], loss: 0.001429, mae: 0.041279, mean_q: 1.319580
 72094/100000: episode: 1263, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.809, mean reward: 0.588 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.884, 10.246], loss: 0.001572, mae: 0.042763, mean_q: 1.315928
 72194/100000: episode: 1264, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.384, mean reward: 0.574 [0.497, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.457, 10.179], loss: 0.001430, mae: 0.040744, mean_q: 1.316862
 72294/100000: episode: 1265, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.686, mean reward: 0.587 [0.503, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.424, 10.098], loss: 0.001550, mae: 0.042371, mean_q: 1.316666
 72394/100000: episode: 1266, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.675, mean reward: 0.577 [0.502, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.175, 10.318], loss: 0.001530, mae: 0.042434, mean_q: 1.308297
 72494/100000: episode: 1267, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.674, mean reward: 0.577 [0.504, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.095, 10.156], loss: 0.001453, mae: 0.041681, mean_q: 1.304803
 72594/100000: episode: 1268, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.257, mean reward: 0.583 [0.508, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.818, 10.239], loss: 0.001497, mae: 0.041529, mean_q: 1.302456
 72694/100000: episode: 1269, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 58.544, mean reward: 0.585 [0.504, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.120, 10.098], loss: 0.001554, mae: 0.042883, mean_q: 1.304181
 72794/100000: episode: 1270, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 56.630, mean reward: 0.566 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.128, 10.106], loss: 0.001724, mae: 0.044630, mean_q: 1.294707
 72894/100000: episode: 1271, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.607, mean reward: 0.586 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.660, 10.098], loss: 0.001614, mae: 0.042937, mean_q: 1.293194
 72994/100000: episode: 1272, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 56.835, mean reward: 0.568 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.526, 10.098], loss: 0.001535, mae: 0.041926, mean_q: 1.290908
 73094/100000: episode: 1273, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.149, mean reward: 0.581 [0.500, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.153, 10.098], loss: 0.001648, mae: 0.042187, mean_q: 1.289498
 73194/100000: episode: 1274, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.570, mean reward: 0.596 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.877, 10.098], loss: 0.001477, mae: 0.041011, mean_q: 1.289948
 73294/100000: episode: 1275, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 60.522, mean reward: 0.605 [0.513, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.426, 10.292], loss: 0.001471, mae: 0.041960, mean_q: 1.280090
 73394/100000: episode: 1276, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 63.268, mean reward: 0.633 [0.508, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.224, 10.530], loss: 0.001809, mae: 0.045849, mean_q: 1.284876
 73494/100000: episode: 1277, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 58.775, mean reward: 0.588 [0.510, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.271, 10.261], loss: 0.001659, mae: 0.042728, mean_q: 1.284895
 73594/100000: episode: 1278, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 63.470, mean reward: 0.635 [0.507, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.692, 10.624], loss: 0.001677, mae: 0.044407, mean_q: 1.281071
 73694/100000: episode: 1279, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 60.099, mean reward: 0.601 [0.505, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.719, 10.148], loss: 0.001729, mae: 0.044274, mean_q: 1.277960
 73794/100000: episode: 1280, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.008, mean reward: 0.580 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.979, 10.098], loss: 0.001555, mae: 0.042491, mean_q: 1.281800
 73894/100000: episode: 1281, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.744, mean reward: 0.567 [0.503, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.813, 10.204], loss: 0.001633, mae: 0.042710, mean_q: 1.272612
 73994/100000: episode: 1282, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 64.172, mean reward: 0.642 [0.521, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.114, 10.429], loss: 0.001771, mae: 0.043490, mean_q: 1.265183
 74094/100000: episode: 1283, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.296, mean reward: 0.563 [0.501, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.875, 10.098], loss: 0.001646, mae: 0.042818, mean_q: 1.275565
 74194/100000: episode: 1284, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 60.547, mean reward: 0.605 [0.498, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.454, 10.267], loss: 0.001678, mae: 0.043298, mean_q: 1.260761
 74294/100000: episode: 1285, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.074, mean reward: 0.591 [0.515, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.479, 10.148], loss: 0.001696, mae: 0.043656, mean_q: 1.264038
 74394/100000: episode: 1286, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.424, mean reward: 0.584 [0.519, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.072, 10.144], loss: 0.001624, mae: 0.042928, mean_q: 1.272218
 74494/100000: episode: 1287, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 60.257, mean reward: 0.603 [0.506, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.296, 10.098], loss: 0.001626, mae: 0.042696, mean_q: 1.265606
 74594/100000: episode: 1288, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 63.187, mean reward: 0.632 [0.505, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.948, 10.490], loss: 0.001736, mae: 0.043989, mean_q: 1.252542
 74694/100000: episode: 1289, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.733, mean reward: 0.577 [0.508, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.757, 10.203], loss: 0.001678, mae: 0.043686, mean_q: 1.253513
 74794/100000: episode: 1290, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.166, mean reward: 0.602 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.604, 10.098], loss: 0.001739, mae: 0.044749, mean_q: 1.253676
 74894/100000: episode: 1291, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.795, mean reward: 0.588 [0.505, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.176, 10.165], loss: 0.001629, mae: 0.042329, mean_q: 1.245005
 74994/100000: episode: 1292, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.047, mean reward: 0.580 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.699, 10.098], loss: 0.001626, mae: 0.042749, mean_q: 1.246686
 75094/100000: episode: 1293, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.284, mean reward: 0.573 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.156, 10.208], loss: 0.001931, mae: 0.045225, mean_q: 1.240808
 75194/100000: episode: 1294, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.785, mean reward: 0.578 [0.513, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.662, 10.180], loss: 0.001666, mae: 0.043349, mean_q: 1.235320
 75294/100000: episode: 1295, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.602, mean reward: 0.586 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.433, 10.366], loss: 0.001723, mae: 0.043842, mean_q: 1.222106
 75394/100000: episode: 1296, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.952, mean reward: 0.610 [0.507, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.331, 10.404], loss: 0.001711, mae: 0.043703, mean_q: 1.219782
 75494/100000: episode: 1297, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.212, mean reward: 0.582 [0.512, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.058, 10.348], loss: 0.001472, mae: 0.040988, mean_q: 1.222621
 75594/100000: episode: 1298, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.628, mean reward: 0.596 [0.507, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.948, 10.098], loss: 0.001569, mae: 0.042417, mean_q: 1.216855
 75694/100000: episode: 1299, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.943, mean reward: 0.589 [0.518, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.783, 10.098], loss: 0.001684, mae: 0.043627, mean_q: 1.211426
 75794/100000: episode: 1300, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.067, mean reward: 0.571 [0.505, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.170, 10.194], loss: 0.001657, mae: 0.043379, mean_q: 1.210854
 75894/100000: episode: 1301, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.339, mean reward: 0.583 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.666, 10.146], loss: 0.001631, mae: 0.042632, mean_q: 1.198028
 75994/100000: episode: 1302, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.998, mean reward: 0.580 [0.500, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.188, 10.150], loss: 0.001650, mae: 0.042655, mean_q: 1.191191
 76094/100000: episode: 1303, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 61.050, mean reward: 0.611 [0.506, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.687, 10.098], loss: 0.001576, mae: 0.042912, mean_q: 1.191113
 76194/100000: episode: 1304, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 63.569, mean reward: 0.636 [0.521, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.050, 10.321], loss: 0.001664, mae: 0.043387, mean_q: 1.192534
 76294/100000: episode: 1305, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.182, mean reward: 0.592 [0.510, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.810, 10.098], loss: 0.001630, mae: 0.043298, mean_q: 1.191422
 76394/100000: episode: 1306, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 61.152, mean reward: 0.612 [0.501, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.123, 10.098], loss: 0.001521, mae: 0.041850, mean_q: 1.188718
 76494/100000: episode: 1307, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.331, mean reward: 0.593 [0.510, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.234, 10.353], loss: 0.001434, mae: 0.040891, mean_q: 1.176383
 76594/100000: episode: 1308, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 57.913, mean reward: 0.579 [0.499, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.902, 10.098], loss: 0.001515, mae: 0.041977, mean_q: 1.173450
 76694/100000: episode: 1309, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.980, mean reward: 0.580 [0.511, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.761, 10.098], loss: 0.001467, mae: 0.041172, mean_q: 1.166636
 76794/100000: episode: 1310, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.961, mean reward: 0.580 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.436, 10.271], loss: 0.001372, mae: 0.040535, mean_q: 1.172387
 76894/100000: episode: 1311, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.525, mean reward: 0.585 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.734, 10.160], loss: 0.001439, mae: 0.040291, mean_q: 1.165473
 76994/100000: episode: 1312, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.231, mean reward: 0.592 [0.497, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.922, 10.098], loss: 0.001444, mae: 0.041833, mean_q: 1.169720
 77094/100000: episode: 1313, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.365, mean reward: 0.594 [0.512, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.000, 10.098], loss: 0.001412, mae: 0.041302, mean_q: 1.172080
 77194/100000: episode: 1314, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.853, mean reward: 0.579 [0.498, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.359, 10.112], loss: 0.001387, mae: 0.040564, mean_q: 1.165874
 77294/100000: episode: 1315, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.446, mean reward: 0.584 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.373, 10.098], loss: 0.001360, mae: 0.040248, mean_q: 1.167452
 77394/100000: episode: 1316, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 57.831, mean reward: 0.578 [0.504, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.504, 10.150], loss: 0.001485, mae: 0.041873, mean_q: 1.164591
 77494/100000: episode: 1317, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.253, mean reward: 0.593 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.895, 10.409], loss: 0.001375, mae: 0.040139, mean_q: 1.170352
 77594/100000: episode: 1318, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 59.185, mean reward: 0.592 [0.514, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.167, 10.098], loss: 0.001339, mae: 0.040053, mean_q: 1.172843
 77694/100000: episode: 1319, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 59.097, mean reward: 0.591 [0.501, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.381, 10.226], loss: 0.001346, mae: 0.039734, mean_q: 1.170949
 77794/100000: episode: 1320, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 59.346, mean reward: 0.593 [0.523, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.393, 10.098], loss: 0.001290, mae: 0.039261, mean_q: 1.170410
 77894/100000: episode: 1321, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 60.249, mean reward: 0.602 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.772, 10.283], loss: 0.001303, mae: 0.039360, mean_q: 1.170892
 77994/100000: episode: 1322, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 60.775, mean reward: 0.608 [0.505, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.105, 10.098], loss: 0.001324, mae: 0.039496, mean_q: 1.175168
 78094/100000: episode: 1323, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.489, mean reward: 0.595 [0.499, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.955, 10.098], loss: 0.001356, mae: 0.039874, mean_q: 1.175627
 78194/100000: episode: 1324, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.440, mean reward: 0.584 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.927, 10.098], loss: 0.001362, mae: 0.040097, mean_q: 1.173182
 78294/100000: episode: 1325, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 62.561, mean reward: 0.626 [0.522, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.073, 10.401], loss: 0.001352, mae: 0.040036, mean_q: 1.175533
 78394/100000: episode: 1326, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 60.319, mean reward: 0.603 [0.500, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.391, 10.281], loss: 0.001354, mae: 0.039680, mean_q: 1.172089
 78494/100000: episode: 1327, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 59.339, mean reward: 0.593 [0.509, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.605, 10.098], loss: 0.001435, mae: 0.041199, mean_q: 1.172376
 78594/100000: episode: 1328, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 60.736, mean reward: 0.607 [0.519, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.532, 10.098], loss: 0.001316, mae: 0.039308, mean_q: 1.167366
 78694/100000: episode: 1329, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.098, mean reward: 0.581 [0.516, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.834, 10.204], loss: 0.001242, mae: 0.038035, mean_q: 1.171795
 78794/100000: episode: 1330, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.151, mean reward: 0.582 [0.508, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.219, 10.098], loss: 0.001380, mae: 0.040351, mean_q: 1.167416
 78894/100000: episode: 1331, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 59.937, mean reward: 0.599 [0.512, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.716, 10.098], loss: 0.001393, mae: 0.039862, mean_q: 1.169918
 78994/100000: episode: 1332, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 59.112, mean reward: 0.591 [0.511, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.351, 10.252], loss: 0.001321, mae: 0.039776, mean_q: 1.168458
 79094/100000: episode: 1333, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 57.847, mean reward: 0.578 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.185, 10.098], loss: 0.001381, mae: 0.040499, mean_q: 1.170958
 79194/100000: episode: 1334, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.293, mean reward: 0.583 [0.513, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.970, 10.216], loss: 0.001270, mae: 0.038660, mean_q: 1.170862
 79294/100000: episode: 1335, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.189, mean reward: 0.572 [0.502, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.829, 10.098], loss: 0.001426, mae: 0.040932, mean_q: 1.171073
 79394/100000: episode: 1336, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.794, mean reward: 0.588 [0.512, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.302, 10.247], loss: 0.001379, mae: 0.039830, mean_q: 1.170364
 79494/100000: episode: 1337, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.562, mean reward: 0.566 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.545, 10.114], loss: 0.001335, mae: 0.039566, mean_q: 1.166950
 79594/100000: episode: 1338, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 62.569, mean reward: 0.626 [0.505, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.921, 10.098], loss: 0.001366, mae: 0.040161, mean_q: 1.167826
 79694/100000: episode: 1339, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 57.402, mean reward: 0.574 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.414, 10.098], loss: 0.001376, mae: 0.040113, mean_q: 1.168153
 79794/100000: episode: 1340, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.795, mean reward: 0.588 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.613, 10.131], loss: 0.001387, mae: 0.040125, mean_q: 1.165746
 79894/100000: episode: 1341, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 56.955, mean reward: 0.570 [0.503, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.153, 10.098], loss: 0.001410, mae: 0.041127, mean_q: 1.168509
 79994/100000: episode: 1342, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.160, mean reward: 0.582 [0.497, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.578, 10.252], loss: 0.001316, mae: 0.039325, mean_q: 1.164864
 80094/100000: episode: 1343, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.178, mean reward: 0.582 [0.513, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.563, 10.232], loss: 0.001469, mae: 0.041085, mean_q: 1.164298
 80194/100000: episode: 1344, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.981, mean reward: 0.580 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.119, 10.149], loss: 0.001411, mae: 0.040780, mean_q: 1.166609
 80294/100000: episode: 1345, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.230, mean reward: 0.582 [0.499, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.870, 10.149], loss: 0.001381, mae: 0.040253, mean_q: 1.165472
 80394/100000: episode: 1346, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.730, mean reward: 0.577 [0.515, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.428, 10.098], loss: 0.001395, mae: 0.040504, mean_q: 1.167058
 80494/100000: episode: 1347, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.284, mean reward: 0.583 [0.499, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.596, 10.098], loss: 0.001427, mae: 0.040874, mean_q: 1.168468
 80594/100000: episode: 1348, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 60.522, mean reward: 0.605 [0.515, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.709, 10.156], loss: 0.001459, mae: 0.040620, mean_q: 1.162818
 80694/100000: episode: 1349, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.235, mean reward: 0.582 [0.501, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.831, 10.098], loss: 0.001346, mae: 0.039899, mean_q: 1.165090
 80794/100000: episode: 1350, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.790, mean reward: 0.578 [0.516, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.070, 10.098], loss: 0.001444, mae: 0.040482, mean_q: 1.167146
 80894/100000: episode: 1351, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 58.430, mean reward: 0.584 [0.502, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.344, 10.177], loss: 0.001433, mae: 0.040747, mean_q: 1.164594
 80994/100000: episode: 1352, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.898, mean reward: 0.579 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.469, 10.098], loss: 0.001418, mae: 0.040736, mean_q: 1.165645
 81094/100000: episode: 1353, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.666, mean reward: 0.577 [0.504, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.222, 10.112], loss: 0.001504, mae: 0.041503, mean_q: 1.166850
 81194/100000: episode: 1354, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 56.654, mean reward: 0.567 [0.502, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.334, 10.391], loss: 0.001405, mae: 0.040989, mean_q: 1.164891
 81294/100000: episode: 1355, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 60.868, mean reward: 0.609 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.131, 10.308], loss: 0.001475, mae: 0.040977, mean_q: 1.161566
 81394/100000: episode: 1356, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 59.917, mean reward: 0.599 [0.501, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.122, 10.281], loss: 0.001371, mae: 0.040242, mean_q: 1.162846
 81494/100000: episode: 1357, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.469, mean reward: 0.605 [0.508, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.746, 10.098], loss: 0.001402, mae: 0.040261, mean_q: 1.161047
 81594/100000: episode: 1358, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.089, mean reward: 0.591 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.797, 10.098], loss: 0.001331, mae: 0.039510, mean_q: 1.163247
[Info] 1-TH LEVEL FOUND: 1.3619065284729004, Considering 10/90 traces
 81694/100000: episode: 1359, duration: 4.667s, episode steps: 100, steps per second: 21, episode reward: 58.102, mean reward: 0.581 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.004, 10.098], loss: 0.001462, mae: 0.040920, mean_q: 1.163667
 81726/100000: episode: 1360, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 21.562, mean reward: 0.674 [0.562, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.227, 10.287], loss: 0.001305, mae: 0.039616, mean_q: 1.165100
 81739/100000: episode: 1361, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 9.107, mean reward: 0.701 [0.677, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.315, 10.100], loss: 0.001518, mae: 0.041861, mean_q: 1.165921
 81771/100000: episode: 1362, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 20.562, mean reward: 0.643 [0.582, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.849, 10.305], loss: 0.001544, mae: 0.041914, mean_q: 1.166577
 81796/100000: episode: 1363, duration: 0.122s, episode steps: 25, steps per second: 204, episode reward: 16.658, mean reward: 0.666 [0.561, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.545, 10.356], loss: 0.001594, mae: 0.042990, mean_q: 1.167355
 81819/100000: episode: 1364, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 14.661, mean reward: 0.637 [0.579, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.140, 10.100], loss: 0.001284, mae: 0.038649, mean_q: 1.161926
 81833/100000: episode: 1365, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 9.984, mean reward: 0.713 [0.686, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.446], loss: 0.001577, mae: 0.042914, mean_q: 1.161493
 81868/100000: episode: 1366, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 22.385, mean reward: 0.640 [0.516, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.035, 10.105], loss: 0.001472, mae: 0.041042, mean_q: 1.167321
 81891/100000: episode: 1367, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 15.623, mean reward: 0.679 [0.581, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.156, 10.100], loss: 0.001444, mae: 0.040607, mean_q: 1.160739
 81905/100000: episode: 1368, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 9.760, mean reward: 0.697 [0.670, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.621, 10.400], loss: 0.001355, mae: 0.038834, mean_q: 1.169591
 81919/100000: episode: 1369, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 9.769, mean reward: 0.698 [0.652, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.446], loss: 0.001321, mae: 0.037756, mean_q: 1.158481
 81954/100000: episode: 1370, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 21.523, mean reward: 0.615 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.583, 10.111], loss: 0.001871, mae: 0.046018, mean_q: 1.167302
 81977/100000: episode: 1371, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 14.994, mean reward: 0.652 [0.599, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.668, 10.100], loss: 0.001565, mae: 0.042524, mean_q: 1.162547
 82000/100000: episode: 1372, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 15.921, mean reward: 0.692 [0.553, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.308], loss: 0.001508, mae: 0.041958, mean_q: 1.166315
 82025/100000: episode: 1373, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 18.776, mean reward: 0.751 [0.677, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.554, 10.593], loss: 0.001366, mae: 0.039370, mean_q: 1.170160
 82048/100000: episode: 1374, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 14.330, mean reward: 0.623 [0.555, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.575, 10.100], loss: 0.001719, mae: 0.043784, mean_q: 1.166701
 82071/100000: episode: 1375, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 14.819, mean reward: 0.644 [0.568, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.137, 10.100], loss: 0.001392, mae: 0.040726, mean_q: 1.173707
 82094/100000: episode: 1376, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 14.235, mean reward: 0.619 [0.559, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.583, 10.100], loss: 0.001585, mae: 0.041542, mean_q: 1.170253
 82117/100000: episode: 1377, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 14.338, mean reward: 0.623 [0.538, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.200, 10.100], loss: 0.001180, mae: 0.037993, mean_q: 1.170262
 82144/100000: episode: 1378, duration: 0.162s, episode steps: 27, steps per second: 166, episode reward: 17.955, mean reward: 0.665 [0.581, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.114, 10.368], loss: 0.001574, mae: 0.042340, mean_q: 1.167470
 82159/100000: episode: 1379, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 9.501, mean reward: 0.633 [0.537, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.711, 10.189], loss: 0.001752, mae: 0.044918, mean_q: 1.169406
 82186/100000: episode: 1380, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 19.236, mean reward: 0.712 [0.621, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.746, 10.336], loss: 0.001914, mae: 0.046152, mean_q: 1.166774
 82209/100000: episode: 1381, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 15.924, mean reward: 0.692 [0.619, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.100, 10.390], loss: 0.001595, mae: 0.041771, mean_q: 1.177323
 82244/100000: episode: 1382, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 25.230, mean reward: 0.721 [0.647, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.035, 10.414], loss: 0.001558, mae: 0.042524, mean_q: 1.172019
 82276/100000: episode: 1383, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 20.852, mean reward: 0.652 [0.583, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.526, 10.385], loss: 0.001715, mae: 0.042126, mean_q: 1.174648
 82301/100000: episode: 1384, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 16.960, mean reward: 0.678 [0.621, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.139, 10.373], loss: 0.001783, mae: 0.045047, mean_q: 1.176556
 82326/100000: episode: 1385, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 15.577, mean reward: 0.623 [0.520, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.075, 10.100], loss: 0.001486, mae: 0.041722, mean_q: 1.179474
 82361/100000: episode: 1386, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 22.880, mean reward: 0.654 [0.568, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.213, 10.357], loss: 0.001639, mae: 0.042316, mean_q: 1.170279
 82376/100000: episode: 1387, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 10.248, mean reward: 0.683 [0.657, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.108, 10.371], loss: 0.001678, mae: 0.043583, mean_q: 1.177375
 82408/100000: episode: 1388, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 19.987, mean reward: 0.625 [0.532, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.088, 10.100], loss: 0.001681, mae: 0.042636, mean_q: 1.183004
 82435/100000: episode: 1389, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 17.661, mean reward: 0.654 [0.588, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.910, 10.513], loss: 0.001706, mae: 0.043846, mean_q: 1.171777
 82448/100000: episode: 1390, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 10.015, mean reward: 0.770 [0.671, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.211, 10.100], loss: 0.001427, mae: 0.041332, mean_q: 1.174241
 82473/100000: episode: 1391, duration: 0.153s, episode steps: 25, steps per second: 163, episode reward: 17.148, mean reward: 0.686 [0.583, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.670, 10.505], loss: 0.001602, mae: 0.041873, mean_q: 1.182450
 82498/100000: episode: 1392, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 15.214, mean reward: 0.609 [0.511, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.240, 10.100], loss: 0.001617, mae: 0.041477, mean_q: 1.180915
 82523/100000: episode: 1393, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 17.318, mean reward: 0.693 [0.635, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.269, 10.436], loss: 0.001611, mae: 0.042160, mean_q: 1.178520
 82548/100000: episode: 1394, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 17.844, mean reward: 0.714 [0.584, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.352], loss: 0.001692, mae: 0.042526, mean_q: 1.177414
 82573/100000: episode: 1395, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 16.344, mean reward: 0.654 [0.583, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.122, 10.351], loss: 0.001697, mae: 0.043288, mean_q: 1.176742
 82608/100000: episode: 1396, duration: 0.176s, episode steps: 35, steps per second: 198, episode reward: 24.893, mean reward: 0.711 [0.636, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.213, 10.390], loss: 0.001514, mae: 0.041350, mean_q: 1.187079
 82643/100000: episode: 1397, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 23.570, mean reward: 0.673 [0.608, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.633, 10.386], loss: 0.001485, mae: 0.041692, mean_q: 1.187583
 82656/100000: episode: 1398, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 10.037, mean reward: 0.772 [0.723, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.333, 10.100], loss: 0.002085, mae: 0.046273, mean_q: 1.187129
 82681/100000: episode: 1399, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 16.075, mean reward: 0.643 [0.528, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.243], loss: 0.001978, mae: 0.046107, mean_q: 1.184691
 82706/100000: episode: 1400, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 17.922, mean reward: 0.717 [0.617, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.566, 10.357], loss: 0.001931, mae: 0.045482, mean_q: 1.190630
 82719/100000: episode: 1401, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 9.705, mean reward: 0.747 [0.673, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.204, 10.100], loss: 0.001783, mae: 0.043777, mean_q: 1.195260
 82746/100000: episode: 1402, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 17.301, mean reward: 0.641 [0.543, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.229], loss: 0.001755, mae: 0.043395, mean_q: 1.193300
 82769/100000: episode: 1403, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 15.191, mean reward: 0.660 [0.561, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.183, 10.100], loss: 0.001433, mae: 0.040399, mean_q: 1.191342
 82792/100000: episode: 1404, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 15.829, mean reward: 0.688 [0.609, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.550, 10.367], loss: 0.001926, mae: 0.046762, mean_q: 1.186497
 82824/100000: episode: 1405, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 21.525, mean reward: 0.673 [0.534, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.480, 10.236], loss: 0.001780, mae: 0.044202, mean_q: 1.186637
 82856/100000: episode: 1406, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 20.890, mean reward: 0.653 [0.560, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.335, 10.269], loss: 0.001605, mae: 0.041928, mean_q: 1.190347
 82871/100000: episode: 1407, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 10.376, mean reward: 0.692 [0.651, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.118, 10.413], loss: 0.001780, mae: 0.044027, mean_q: 1.194214
 82903/100000: episode: 1408, duration: 0.181s, episode steps: 32, steps per second: 177, episode reward: 22.084, mean reward: 0.690 [0.594, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.456, 10.425], loss: 0.001754, mae: 0.043596, mean_q: 1.186977
 82926/100000: episode: 1409, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 16.639, mean reward: 0.723 [0.640, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.177, 10.397], loss: 0.001555, mae: 0.042692, mean_q: 1.195688
 82951/100000: episode: 1410, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 17.257, mean reward: 0.690 [0.637, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.167, 10.357], loss: 0.001586, mae: 0.041466, mean_q: 1.188181
 82966/100000: episode: 1411, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 10.166, mean reward: 0.678 [0.573, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-1.343, 10.247], loss: 0.001986, mae: 0.046507, mean_q: 1.189217
 82981/100000: episode: 1412, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 10.274, mean reward: 0.685 [0.587, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.860, 10.244], loss: 0.001625, mae: 0.043571, mean_q: 1.193936
 83008/100000: episode: 1413, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 17.530, mean reward: 0.649 [0.551, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.326, 10.233], loss: 0.001559, mae: 0.041995, mean_q: 1.188769
 83043/100000: episode: 1414, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 24.371, mean reward: 0.696 [0.529, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.051, 10.263], loss: 0.001678, mae: 0.043187, mean_q: 1.191779
 83075/100000: episode: 1415, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 21.223, mean reward: 0.663 [0.516, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.162, 10.220], loss: 0.001598, mae: 0.041711, mean_q: 1.193059
 83088/100000: episode: 1416, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 8.503, mean reward: 0.654 [0.577, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.315, 10.100], loss: 0.001785, mae: 0.043985, mean_q: 1.186335
 83101/100000: episode: 1417, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 9.298, mean reward: 0.715 [0.691, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.294, 10.100], loss: 0.001615, mae: 0.041899, mean_q: 1.203249
 83124/100000: episode: 1418, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 15.569, mean reward: 0.677 [0.603, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.169, 10.100], loss: 0.001695, mae: 0.042580, mean_q: 1.190225
 83138/100000: episode: 1419, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 10.558, mean reward: 0.754 [0.707, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.530], loss: 0.001734, mae: 0.045666, mean_q: 1.199715
 83152/100000: episode: 1420, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 9.914, mean reward: 0.708 [0.664, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.519, 10.449], loss: 0.001968, mae: 0.047680, mean_q: 1.183175
 83167/100000: episode: 1421, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 10.342, mean reward: 0.689 [0.616, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.462], loss: 0.001980, mae: 0.046753, mean_q: 1.192278
 83192/100000: episode: 1422, duration: 0.143s, episode steps: 25, steps per second: 174, episode reward: 16.883, mean reward: 0.675 [0.595, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.343], loss: 0.001802, mae: 0.044715, mean_q: 1.188096
 83207/100000: episode: 1423, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 10.333, mean reward: 0.689 [0.630, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.886, 10.385], loss: 0.001751, mae: 0.045603, mean_q: 1.213684
 83242/100000: episode: 1424, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 24.650, mean reward: 0.704 [0.590, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.318, 10.418], loss: 0.001657, mae: 0.042730, mean_q: 1.203533
 83257/100000: episode: 1425, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 10.286, mean reward: 0.686 [0.647, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.366], loss: 0.001667, mae: 0.042294, mean_q: 1.187864
 83271/100000: episode: 1426, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 9.805, mean reward: 0.700 [0.664, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.474], loss: 0.001689, mae: 0.045619, mean_q: 1.205850
 83285/100000: episode: 1427, duration: 0.078s, episode steps: 14, steps per second: 178, episode reward: 9.574, mean reward: 0.684 [0.634, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.356], loss: 0.001674, mae: 0.044320, mean_q: 1.194712
 83312/100000: episode: 1428, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 19.070, mean reward: 0.706 [0.634, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.378, 10.446], loss: 0.001714, mae: 0.043895, mean_q: 1.197098
 83347/100000: episode: 1429, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 27.359, mean reward: 0.782 [0.665, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.645, 10.603], loss: 0.001715, mae: 0.044178, mean_q: 1.214139
 83372/100000: episode: 1430, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 16.843, mean reward: 0.674 [0.557, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.116, 10.321], loss: 0.001560, mae: 0.042726, mean_q: 1.211686
 83404/100000: episode: 1431, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 20.312, mean reward: 0.635 [0.526, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.530, 10.219], loss: 0.001986, mae: 0.045738, mean_q: 1.197388
 83417/100000: episode: 1432, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.513, mean reward: 0.732 [0.701, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.302, 10.100], loss: 0.001926, mae: 0.046404, mean_q: 1.221810
 83431/100000: episode: 1433, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.196, mean reward: 0.728 [0.685, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.556], loss: 0.001713, mae: 0.045881, mean_q: 1.205287
 83446/100000: episode: 1434, duration: 0.091s, episode steps: 15, steps per second: 166, episode reward: 10.607, mean reward: 0.707 [0.668, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.152, 10.363], loss: 0.001680, mae: 0.041511, mean_q: 1.200946
 83478/100000: episode: 1435, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 22.023, mean reward: 0.688 [0.638, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.035, 10.408], loss: 0.001690, mae: 0.044064, mean_q: 1.201649
 83503/100000: episode: 1436, duration: 0.154s, episode steps: 25, steps per second: 163, episode reward: 15.599, mean reward: 0.624 [0.527, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.127, 10.199], loss: 0.001597, mae: 0.042425, mean_q: 1.207669
 83526/100000: episode: 1437, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 14.782, mean reward: 0.643 [0.584, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.282], loss: 0.001690, mae: 0.043744, mean_q: 1.211646
 83549/100000: episode: 1438, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 16.483, mean reward: 0.717 [0.632, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.592, 10.461], loss: 0.001994, mae: 0.047306, mean_q: 1.209970
 83564/100000: episode: 1439, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 9.436, mean reward: 0.629 [0.512, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.395, 10.229], loss: 0.001823, mae: 0.046529, mean_q: 1.201541
 83599/100000: episode: 1440, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 27.354, mean reward: 0.782 [0.656, 0.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.325, 10.652], loss: 0.001686, mae: 0.044594, mean_q: 1.208830
 83622/100000: episode: 1441, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 16.146, mean reward: 0.702 [0.644, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.999, 10.100], loss: 0.001496, mae: 0.040860, mean_q: 1.211135
 83657/100000: episode: 1442, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 22.800, mean reward: 0.651 [0.566, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.789, 10.491], loss: 0.001543, mae: 0.041486, mean_q: 1.212763
 83682/100000: episode: 1443, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 16.976, mean reward: 0.679 [0.559, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.357], loss: 0.001436, mae: 0.041062, mean_q: 1.213159
 83707/100000: episode: 1444, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 17.706, mean reward: 0.708 [0.632, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.709, 10.364], loss: 0.001510, mae: 0.041245, mean_q: 1.214343
 83732/100000: episode: 1445, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 17.071, mean reward: 0.683 [0.644, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.276, 10.421], loss: 0.001513, mae: 0.042800, mean_q: 1.214826
 83747/100000: episode: 1446, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 10.415, mean reward: 0.694 [0.597, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.505, 10.439], loss: 0.001574, mae: 0.041471, mean_q: 1.208367
 83779/100000: episode: 1447, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 20.967, mean reward: 0.655 [0.584, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.310, 10.169], loss: 0.001794, mae: 0.043755, mean_q: 1.215014
 83804/100000: episode: 1448, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 16.557, mean reward: 0.662 [0.598, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.372], loss: 0.001492, mae: 0.041916, mean_q: 1.222228
[Info] 2-TH LEVEL FOUND: 1.512143850326538, Considering 10/90 traces
 83831/100000: episode: 1449, duration: 4.269s, episode steps: 27, steps per second: 6, episode reward: 18.500, mean reward: 0.685 [0.616, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.156, 10.360], loss: 0.001636, mae: 0.042786, mean_q: 1.217435
 83853/100000: episode: 1450, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 16.589, mean reward: 0.754 [0.648, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.225, 10.453], loss: 0.001599, mae: 0.042622, mean_q: 1.214168
 83881/100000: episode: 1451, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 19.779, mean reward: 0.706 [0.561, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.816, 10.340], loss: 0.001585, mae: 0.042685, mean_q: 1.224338
 83911/100000: episode: 1452, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 20.458, mean reward: 0.682 [0.524, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.391, 10.221], loss: 0.001597, mae: 0.041544, mean_q: 1.221827
 83933/100000: episode: 1453, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 14.610, mean reward: 0.664 [0.565, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.055, 10.301], loss: 0.001577, mae: 0.041497, mean_q: 1.224878
 83953/100000: episode: 1454, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 14.047, mean reward: 0.702 [0.616, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.841, 10.342], loss: 0.001851, mae: 0.044653, mean_q: 1.220936
 83983/100000: episode: 1455, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 23.958, mean reward: 0.799 [0.705, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.564], loss: 0.001570, mae: 0.042224, mean_q: 1.230519
 84005/100000: episode: 1456, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 14.066, mean reward: 0.639 [0.535, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.227, 10.253], loss: 0.001999, mae: 0.047429, mean_q: 1.218654
 84028/100000: episode: 1457, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 17.059, mean reward: 0.742 [0.653, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.775, 10.352], loss: 0.001853, mae: 0.045483, mean_q: 1.219682
 84054/100000: episode: 1458, duration: 0.126s, episode steps: 26, steps per second: 207, episode reward: 18.969, mean reward: 0.730 [0.612, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.796, 10.337], loss: 0.001465, mae: 0.040960, mean_q: 1.227875
 84074/100000: episode: 1459, duration: 0.110s, episode steps: 20, steps per second: 183, episode reward: 14.463, mean reward: 0.723 [0.677, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.533, 10.439], loss: 0.001378, mae: 0.040236, mean_q: 1.221837
 84104/100000: episode: 1460, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 22.905, mean reward: 0.764 [0.688, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.929, 10.488], loss: 0.001595, mae: 0.042596, mean_q: 1.229156
 84130/100000: episode: 1461, duration: 0.128s, episode steps: 26, steps per second: 202, episode reward: 18.125, mean reward: 0.697 [0.618, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.496, 10.347], loss: 0.001542, mae: 0.042102, mean_q: 1.226517
 84153/100000: episode: 1462, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 16.328, mean reward: 0.710 [0.658, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.116, 10.446], loss: 0.001278, mae: 0.038185, mean_q: 1.240778
 84183/100000: episode: 1463, duration: 0.170s, episode steps: 30, steps per second: 177, episode reward: 21.434, mean reward: 0.714 [0.649, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.989, 10.482], loss: 0.001440, mae: 0.040716, mean_q: 1.232340
 84206/100000: episode: 1464, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 15.860, mean reward: 0.690 [0.633, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.358], loss: 0.001589, mae: 0.042779, mean_q: 1.229760
 84215/100000: episode: 1465, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 6.618, mean reward: 0.735 [0.683, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.011, 10.100], loss: 0.001570, mae: 0.041697, mean_q: 1.209943
 84241/100000: episode: 1466, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 20.263, mean reward: 0.779 [0.628, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.139, 10.427], loss: 0.001373, mae: 0.040873, mean_q: 1.245480
 84267/100000: episode: 1467, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 20.264, mean reward: 0.779 [0.711, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.483], loss: 0.001672, mae: 0.043012, mean_q: 1.229189
 84281/100000: episode: 1468, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 10.758, mean reward: 0.768 [0.731, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.799, 10.539], loss: 0.001617, mae: 0.042840, mean_q: 1.227812
 84303/100000: episode: 1469, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 15.924, mean reward: 0.724 [0.616, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-1.187, 10.384], loss: 0.001465, mae: 0.041140, mean_q: 1.240863
 84326/100000: episode: 1470, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 17.904, mean reward: 0.778 [0.709, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.154, 10.503], loss: 0.001661, mae: 0.043458, mean_q: 1.239335
 84354/100000: episode: 1471, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 20.521, mean reward: 0.733 [0.687, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.749, 10.473], loss: 0.001521, mae: 0.041910, mean_q: 1.238388
 84376/100000: episode: 1472, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 16.444, mean reward: 0.747 [0.684, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.504], loss: 0.001861, mae: 0.045999, mean_q: 1.252227
 84398/100000: episode: 1473, duration: 0.107s, episode steps: 22, steps per second: 205, episode reward: 14.273, mean reward: 0.649 [0.539, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.903, 10.176], loss: 0.001341, mae: 0.039411, mean_q: 1.243770
 84418/100000: episode: 1474, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 12.944, mean reward: 0.647 [0.558, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.984, 10.244], loss: 0.001495, mae: 0.042235, mean_q: 1.253595
 84446/100000: episode: 1475, duration: 0.181s, episode steps: 28, steps per second: 155, episode reward: 20.497, mean reward: 0.732 [0.663, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.374, 10.419], loss: 0.001403, mae: 0.040696, mean_q: 1.248864
 84468/100000: episode: 1476, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 15.489, mean reward: 0.704 [0.633, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.063, 10.366], loss: 0.001332, mae: 0.039366, mean_q: 1.240345
 84488/100000: episode: 1477, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 14.161, mean reward: 0.708 [0.611, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.614, 10.455], loss: 0.001472, mae: 0.041737, mean_q: 1.239838
 84508/100000: episode: 1478, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 14.365, mean reward: 0.718 [0.605, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.443], loss: 0.001538, mae: 0.043711, mean_q: 1.237701
 84536/100000: episode: 1479, duration: 0.146s, episode steps: 28, steps per second: 191, episode reward: 23.789, mean reward: 0.850 [0.680, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.582], loss: 0.001306, mae: 0.039344, mean_q: 1.250499
 84545/100000: episode: 1480, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 7.203, mean reward: 0.800 [0.761, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.967, 10.100], loss: 0.002109, mae: 0.046010, mean_q: 1.234367
 84559/100000: episode: 1481, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 10.326, mean reward: 0.738 [0.671, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.467], loss: 0.001669, mae: 0.044764, mean_q: 1.254883
 84585/100000: episode: 1482, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 18.938, mean reward: 0.728 [0.645, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.282, 10.418], loss: 0.001405, mae: 0.040265, mean_q: 1.262941
 84613/100000: episode: 1483, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 19.241, mean reward: 0.687 [0.589, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.367, 10.325], loss: 0.001402, mae: 0.039788, mean_q: 1.248777
 84641/100000: episode: 1484, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 19.929, mean reward: 0.712 [0.587, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.446, 10.376], loss: 0.001580, mae: 0.042426, mean_q: 1.246696
 84669/100000: episode: 1485, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 20.538, mean reward: 0.734 [0.646, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.168, 10.509], loss: 0.001296, mae: 0.038585, mean_q: 1.257521
 84692/100000: episode: 1486, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 15.852, mean reward: 0.689 [0.612, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.396], loss: 0.001470, mae: 0.041152, mean_q: 1.265870
 84706/100000: episode: 1487, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 11.198, mean reward: 0.800 [0.734, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.880, 10.529], loss: 0.001660, mae: 0.043082, mean_q: 1.249974
 84736/100000: episode: 1488, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 22.687, mean reward: 0.756 [0.632, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.495, 10.412], loss: 0.001728, mae: 0.043200, mean_q: 1.258866
 84766/100000: episode: 1489, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 20.171, mean reward: 0.672 [0.564, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.652, 10.280], loss: 0.001628, mae: 0.043296, mean_q: 1.265931
 84788/100000: episode: 1490, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 15.509, mean reward: 0.705 [0.642, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.258, 10.476], loss: 0.001281, mae: 0.038594, mean_q: 1.268951
 84810/100000: episode: 1491, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 17.109, mean reward: 0.778 [0.729, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.195, 10.560], loss: 0.001520, mae: 0.040351, mean_q: 1.250738
 84833/100000: episode: 1492, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 16.882, mean reward: 0.734 [0.679, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.570], loss: 0.001514, mae: 0.041467, mean_q: 1.264217
 84853/100000: episode: 1493, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 14.287, mean reward: 0.714 [0.623, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-1.200, 10.372], loss: 0.001692, mae: 0.045076, mean_q: 1.278632
 84883/100000: episode: 1494, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 20.904, mean reward: 0.697 [0.601, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.323], loss: 0.001388, mae: 0.040697, mean_q: 1.260024
 84905/100000: episode: 1495, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 16.360, mean reward: 0.744 [0.621, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.388, 10.279], loss: 0.001422, mae: 0.040068, mean_q: 1.275112
 84935/100000: episode: 1496, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 23.069, mean reward: 0.769 [0.661, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.354, 10.601], loss: 0.001511, mae: 0.042438, mean_q: 1.267432
 84944/100000: episode: 1497, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 7.178, mean reward: 0.798 [0.741, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.427, 10.100], loss: 0.001567, mae: 0.044148, mean_q: 1.295373
 84958/100000: episode: 1498, duration: 0.095s, episode steps: 14, steps per second: 148, episode reward: 10.235, mean reward: 0.731 [0.676, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-1.032, 10.442], loss: 0.001502, mae: 0.040241, mean_q: 1.277855
 84980/100000: episode: 1499, duration: 0.110s, episode steps: 22, steps per second: 199, episode reward: 17.716, mean reward: 0.805 [0.738, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-1.501, 10.628], loss: 0.001756, mae: 0.042900, mean_q: 1.286382
 85002/100000: episode: 1500, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 15.715, mean reward: 0.714 [0.605, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.647, 10.371], loss: 0.001637, mae: 0.042511, mean_q: 1.271568
 85011/100000: episode: 1501, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 6.888, mean reward: 0.765 [0.704, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.383, 10.100], loss: 0.002029, mae: 0.049335, mean_q: 1.305763
 85037/100000: episode: 1502, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 18.185, mean reward: 0.699 [0.626, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.643, 10.335], loss: 0.001658, mae: 0.043352, mean_q: 1.275849
 85067/100000: episode: 1503, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 19.360, mean reward: 0.645 [0.531, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.827, 10.263], loss: 0.001537, mae: 0.041888, mean_q: 1.276003
 85097/100000: episode: 1504, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 20.438, mean reward: 0.681 [0.560, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.035, 10.224], loss: 0.001640, mae: 0.042969, mean_q: 1.265878
 85106/100000: episode: 1505, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 7.398, mean reward: 0.822 [0.763, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.460, 10.100], loss: 0.001382, mae: 0.040832, mean_q: 1.264040
 85132/100000: episode: 1506, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 21.123, mean reward: 0.812 [0.726, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.251, 10.729], loss: 0.001509, mae: 0.042332, mean_q: 1.289483
 85154/100000: episode: 1507, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 15.360, mean reward: 0.698 [0.576, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.077, 10.274], loss: 0.001197, mae: 0.038031, mean_q: 1.271034
 85184/100000: episode: 1508, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 21.249, mean reward: 0.708 [0.637, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.458, 10.384], loss: 0.001502, mae: 0.041757, mean_q: 1.272951
 85207/100000: episode: 1509, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 16.061, mean reward: 0.698 [0.611, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.094, 10.398], loss: 0.001390, mae: 0.041228, mean_q: 1.289801
 85237/100000: episode: 1510, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 20.782, mean reward: 0.693 [0.605, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.500, 10.207], loss: 0.001304, mae: 0.039420, mean_q: 1.287352
 85257/100000: episode: 1511, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 13.527, mean reward: 0.676 [0.548, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.233], loss: 0.001435, mae: 0.039365, mean_q: 1.284542
 85279/100000: episode: 1512, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 14.441, mean reward: 0.656 [0.547, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.403, 10.373], loss: 0.001419, mae: 0.041225, mean_q: 1.275962
 85288/100000: episode: 1513, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 7.450, mean reward: 0.828 [0.785, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.665, 10.100], loss: 0.001365, mae: 0.041055, mean_q: 1.295186
 85311/100000: episode: 1514, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 16.105, mean reward: 0.700 [0.637, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.441], loss: 0.001323, mae: 0.040057, mean_q: 1.293134
 85325/100000: episode: 1515, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 10.709, mean reward: 0.765 [0.727, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.757, 10.556], loss: 0.001670, mae: 0.042586, mean_q: 1.290664
 85334/100000: episode: 1516, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 6.918, mean reward: 0.769 [0.726, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.397, 10.100], loss: 0.001509, mae: 0.042375, mean_q: 1.295557
 85360/100000: episode: 1517, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 19.622, mean reward: 0.755 [0.689, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.149, 10.418], loss: 0.001442, mae: 0.040472, mean_q: 1.297587
 85382/100000: episode: 1518, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 15.637, mean reward: 0.711 [0.626, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.241, 10.425], loss: 0.001382, mae: 0.039689, mean_q: 1.296457
 85405/100000: episode: 1519, duration: 0.119s, episode steps: 23, steps per second: 194, episode reward: 17.682, mean reward: 0.769 [0.700, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.388, 10.430], loss: 0.001473, mae: 0.041163, mean_q: 1.295097
 85435/100000: episode: 1520, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 20.339, mean reward: 0.678 [0.567, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.156, 10.296], loss: 0.001416, mae: 0.040952, mean_q: 1.294499
 85465/100000: episode: 1521, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 19.382, mean reward: 0.646 [0.504, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.405, 10.187], loss: 0.001221, mae: 0.038276, mean_q: 1.297316
 85487/100000: episode: 1522, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 16.144, mean reward: 0.734 [0.684, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.887, 10.542], loss: 0.001443, mae: 0.040151, mean_q: 1.296424
 85513/100000: episode: 1523, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 18.956, mean reward: 0.729 [0.655, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.114, 10.401], loss: 0.001722, mae: 0.046295, mean_q: 1.306259
 85541/100000: episode: 1524, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 21.264, mean reward: 0.759 [0.701, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.832, 10.471], loss: 0.001331, mae: 0.039974, mean_q: 1.301279
 85567/100000: episode: 1525, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 20.585, mean reward: 0.792 [0.707, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.677, 10.527], loss: 0.001294, mae: 0.038448, mean_q: 1.303177
 85595/100000: episode: 1526, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 20.884, mean reward: 0.746 [0.606, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.229, 10.415], loss: 0.001326, mae: 0.038740, mean_q: 1.312477
 85604/100000: episode: 1527, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 7.408, mean reward: 0.823 [0.775, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.337, 10.100], loss: 0.001333, mae: 0.038952, mean_q: 1.310234
 85634/100000: episode: 1528, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 20.801, mean reward: 0.693 [0.640, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.171, 10.365], loss: 0.001222, mae: 0.037800, mean_q: 1.302258
 85656/100000: episode: 1529, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 16.478, mean reward: 0.749 [0.630, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.404], loss: 0.001334, mae: 0.037999, mean_q: 1.310316
 85686/100000: episode: 1530, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 22.545, mean reward: 0.751 [0.687, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.983, 10.466], loss: 0.001546, mae: 0.042906, mean_q: 1.311584
 85714/100000: episode: 1531, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 20.783, mean reward: 0.742 [0.640, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.088, 10.420], loss: 0.001433, mae: 0.042100, mean_q: 1.312767
 85744/100000: episode: 1532, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 18.333, mean reward: 0.611 [0.521, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.254, 10.136], loss: 0.001333, mae: 0.038617, mean_q: 1.303467
 85758/100000: episode: 1533, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 10.547, mean reward: 0.753 [0.690, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.287, 10.454], loss: 0.001294, mae: 0.039185, mean_q: 1.317789
 85788/100000: episode: 1534, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 19.886, mean reward: 0.663 [0.597, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.362, 10.319], loss: 0.001406, mae: 0.039729, mean_q: 1.314038
 85818/100000: episode: 1535, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 22.389, mean reward: 0.746 [0.680, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.614], loss: 0.001231, mae: 0.038065, mean_q: 1.307976
 85841/100000: episode: 1536, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 17.507, mean reward: 0.761 [0.653, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.517], loss: 0.001304, mae: 0.039465, mean_q: 1.312820
 85863/100000: episode: 1537, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 14.953, mean reward: 0.680 [0.582, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.022, 10.292], loss: 0.001439, mae: 0.041499, mean_q: 1.312553
 85893/100000: episode: 1538, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 19.431, mean reward: 0.648 [0.509, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.041, 10.175], loss: 0.001551, mae: 0.043933, mean_q: 1.325636
[Info] 3-TH LEVEL FOUND: 1.63873291015625, Considering 10/90 traces
 85913/100000: episode: 1539, duration: 4.284s, episode steps: 20, steps per second: 5, episode reward: 14.899, mean reward: 0.745 [0.601, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.320, 10.314], loss: 0.001100, mae: 0.036547, mean_q: 1.322453
 85934/100000: episode: 1540, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 17.158, mean reward: 0.817 [0.758, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.782, 10.535], loss: 0.001624, mae: 0.042420, mean_q: 1.309662
 85952/100000: episode: 1541, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 13.319, mean reward: 0.740 [0.645, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.237, 10.408], loss: 0.001456, mae: 0.041439, mean_q: 1.301905
 85969/100000: episode: 1542, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 13.283, mean reward: 0.781 [0.714, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.611], loss: 0.001304, mae: 0.038848, mean_q: 1.317194
 85986/100000: episode: 1543, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 14.383, mean reward: 0.846 [0.772, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.582], loss: 0.001291, mae: 0.039601, mean_q: 1.328044
 86006/100000: episode: 1544, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 16.312, mean reward: 0.816 [0.735, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-1.335, 10.613], loss: 0.001415, mae: 0.040297, mean_q: 1.335825
 86023/100000: episode: 1545, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 13.255, mean reward: 0.780 [0.698, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.604, 10.461], loss: 0.001241, mae: 0.038548, mean_q: 1.325636
 86040/100000: episode: 1546, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 12.829, mean reward: 0.755 [0.603, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.451], loss: 0.001268, mae: 0.039367, mean_q: 1.329168
 86056/100000: episode: 1547, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 11.553, mean reward: 0.722 [0.646, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.411], loss: 0.001459, mae: 0.039756, mean_q: 1.326750
 86073/100000: episode: 1548, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 13.344, mean reward: 0.785 [0.724, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.511, 10.504], loss: 0.001405, mae: 0.040923, mean_q: 1.331055
 86093/100000: episode: 1549, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 14.806, mean reward: 0.740 [0.621, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.341], loss: 0.001639, mae: 0.043965, mean_q: 1.329045
 86111/100000: episode: 1550, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 13.667, mean reward: 0.759 [0.676, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.482], loss: 0.001159, mae: 0.037812, mean_q: 1.328089
 86134/100000: episode: 1551, duration: 0.147s, episode steps: 23, steps per second: 156, episode reward: 17.672, mean reward: 0.768 [0.728, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.305, 10.533], loss: 0.001133, mae: 0.036457, mean_q: 1.323711
 86151/100000: episode: 1552, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 13.863, mean reward: 0.815 [0.777, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-1.104, 10.570], loss: 0.001367, mae: 0.041704, mean_q: 1.347400
 86174/100000: episode: 1553, duration: 0.140s, episode steps: 23, steps per second: 165, episode reward: 19.136, mean reward: 0.832 [0.674, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.334, 10.513], loss: 0.001344, mae: 0.040145, mean_q: 1.340239
 86192/100000: episode: 1554, duration: 0.110s, episode steps: 18, steps per second: 163, episode reward: 13.825, mean reward: 0.768 [0.672, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.482], loss: 0.001093, mae: 0.036686, mean_q: 1.343955
 86209/100000: episode: 1555, duration: 0.113s, episode steps: 17, steps per second: 150, episode reward: 13.615, mean reward: 0.801 [0.747, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.443], loss: 0.001338, mae: 0.039342, mean_q: 1.332304
 86225/100000: episode: 1556, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 11.242, mean reward: 0.703 [0.623, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.781, 10.302], loss: 0.001181, mae: 0.038512, mean_q: 1.351922
 86242/100000: episode: 1557, duration: 0.134s, episode steps: 17, steps per second: 127, episode reward: 14.879, mean reward: 0.875 [0.815, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.291, 10.565], loss: 0.001479, mae: 0.040054, mean_q: 1.325449
 86258/100000: episode: 1558, duration: 0.140s, episode steps: 16, steps per second: 114, episode reward: 12.371, mean reward: 0.773 [0.690, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.452], loss: 0.001263, mae: 0.039810, mean_q: 1.338711
 86275/100000: episode: 1559, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 13.335, mean reward: 0.784 [0.689, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.496], loss: 0.001180, mae: 0.037981, mean_q: 1.343007
 86296/100000: episode: 1560, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 16.539, mean reward: 0.788 [0.710, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.458, 10.394], loss: 0.001268, mae: 0.038013, mean_q: 1.340550
 86313/100000: episode: 1561, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 13.461, mean reward: 0.792 [0.734, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.452, 10.443], loss: 0.001180, mae: 0.038863, mean_q: 1.361774
 86331/100000: episode: 1562, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 14.383, mean reward: 0.799 [0.721, 0.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.534, 10.584], loss: 0.001253, mae: 0.038307, mean_q: 1.352720
 86348/100000: episode: 1563, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 14.107, mean reward: 0.830 [0.746, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.075, 10.717], loss: 0.001465, mae: 0.042328, mean_q: 1.347070
 86366/100000: episode: 1564, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 14.618, mean reward: 0.812 [0.749, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.595], loss: 0.001514, mae: 0.041537, mean_q: 1.338068
 86387/100000: episode: 1565, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 15.534, mean reward: 0.740 [0.594, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.312], loss: 0.001143, mae: 0.037717, mean_q: 1.350456
 86410/100000: episode: 1566, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 19.223, mean reward: 0.836 [0.795, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.513, 10.670], loss: 0.001276, mae: 0.039332, mean_q: 1.351893
 86428/100000: episode: 1567, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 13.829, mean reward: 0.768 [0.706, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.059, 10.577], loss: 0.001078, mae: 0.037023, mean_q: 1.345303
 86444/100000: episode: 1568, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 12.127, mean reward: 0.758 [0.696, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-1.019, 10.540], loss: 0.001067, mae: 0.035855, mean_q: 1.357257
 86461/100000: episode: 1569, duration: 0.115s, episode steps: 17, steps per second: 148, episode reward: 15.127, mean reward: 0.890 [0.820, 0.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.055, 10.562], loss: 0.001094, mae: 0.036876, mean_q: 1.362291
 86477/100000: episode: 1570, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 13.234, mean reward: 0.827 [0.743, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.869, 10.614], loss: 0.001091, mae: 0.037013, mean_q: 1.361494
 86495/100000: episode: 1571, duration: 0.115s, episode steps: 18, steps per second: 156, episode reward: 15.017, mean reward: 0.834 [0.761, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.939, 10.510], loss: 0.001152, mae: 0.037681, mean_q: 1.352514
 86512/100000: episode: 1572, duration: 0.124s, episode steps: 17, steps per second: 137, episode reward: 13.581, mean reward: 0.799 [0.734, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.070, 10.520], loss: 0.001425, mae: 0.042966, mean_q: 1.339527
 86528/100000: episode: 1573, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 12.314, mean reward: 0.770 [0.667, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.307, 10.406], loss: 0.001289, mae: 0.039410, mean_q: 1.357674
 86544/100000: episode: 1574, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 12.648, mean reward: 0.790 [0.737, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.497], loss: 0.001240, mae: 0.039381, mean_q: 1.360007
 86561/100000: episode: 1575, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 13.605, mean reward: 0.800 [0.749, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.479], loss: 0.001036, mae: 0.035828, mean_q: 1.349218
 86582/100000: episode: 1576, duration: 0.169s, episode steps: 21, steps per second: 124, episode reward: 17.211, mean reward: 0.820 [0.763, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.145, 10.554], loss: 0.001110, mae: 0.035262, mean_q: 1.361402
 86599/100000: episode: 1577, duration: 0.134s, episode steps: 17, steps per second: 127, episode reward: 12.460, mean reward: 0.733 [0.669, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.141, 10.450], loss: 0.001123, mae: 0.037509, mean_q: 1.355796
 86616/100000: episode: 1578, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 13.510, mean reward: 0.795 [0.757, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.546], loss: 0.001109, mae: 0.036618, mean_q: 1.366073
 86632/100000: episode: 1579, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 13.558, mean reward: 0.847 [0.744, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.125, 10.657], loss: 0.001313, mae: 0.040621, mean_q: 1.354500
[Info] FALSIFICATION!
 86652/100000: episode: 1580, duration: 0.496s, episode steps: 20, steps per second: 40, episode reward: 17.266, mean reward: 0.863 [0.817, 1.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.116, 10.789], loss: 0.001253, mae: 0.039150, mean_q: 1.366610
 86669/100000: episode: 1581, duration: 0.115s, episode steps: 17, steps per second: 148, episode reward: 12.417, mean reward: 0.730 [0.608, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.419], loss: 0.001137, mae: 0.037134, mean_q: 1.376403
 86685/100000: episode: 1582, duration: 0.105s, episode steps: 16, steps per second: 153, episode reward: 13.356, mean reward: 0.835 [0.777, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.609], loss: 0.001187, mae: 0.037739, mean_q: 1.366119
 86703/100000: episode: 1583, duration: 0.118s, episode steps: 18, steps per second: 153, episode reward: 14.142, mean reward: 0.786 [0.672, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.404], loss: 0.001291, mae: 0.039867, mean_q: 1.366157
[Info] FALSIFICATION!
 86716/100000: episode: 1584, duration: 0.372s, episode steps: 13, steps per second: 35, episode reward: 11.758, mean reward: 0.904 [0.765, 1.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.019, 10.712], loss: 0.001485, mae: 0.042882, mean_q: 1.368780
 86736/100000: episode: 1585, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 15.858, mean reward: 0.793 [0.728, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.504], loss: 0.001186, mae: 0.037971, mean_q: 1.363746
 86759/100000: episode: 1586, duration: 0.134s, episode steps: 23, steps per second: 171, episode reward: 18.041, mean reward: 0.784 [0.703, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.370], loss: 0.001220, mae: 0.038793, mean_q: 1.378088
 86775/100000: episode: 1587, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 11.971, mean reward: 0.748 [0.676, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.270, 10.458], loss: 0.001230, mae: 0.038555, mean_q: 1.365409
 86792/100000: episode: 1588, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 12.873, mean reward: 0.757 [0.676, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.111, 10.468], loss: 0.001072, mae: 0.036549, mean_q: 1.376668
[Info] FALSIFICATION!
 86809/100000: episode: 1589, duration: 0.411s, episode steps: 17, steps per second: 41, episode reward: 14.499, mean reward: 0.853 [0.799, 1.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.670, 10.487], loss: 0.001061, mae: 0.036756, mean_q: 1.366282
 86829/100000: episode: 1590, duration: 0.124s, episode steps: 20, steps per second: 162, episode reward: 15.121, mean reward: 0.756 [0.699, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.256, 10.483], loss: 0.001175, mae: 0.038298, mean_q: 1.354620
 86847/100000: episode: 1591, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 13.031, mean reward: 0.724 [0.644, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.900, 10.486], loss: 0.001082, mae: 0.036475, mean_q: 1.387126
 86865/100000: episode: 1592, duration: 0.125s, episode steps: 18, steps per second: 144, episode reward: 14.061, mean reward: 0.781 [0.730, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.386, 10.527], loss: 0.001375, mae: 0.040763, mean_q: 1.396251
[Info] FALSIFICATION!
 86872/100000: episode: 1593, duration: 0.351s, episode steps: 7, steps per second: 20, episode reward: 6.239, mean reward: 0.891 [0.796, 1.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.094, 10.450], loss: 0.001043, mae: 0.035502, mean_q: 1.388042
 86893/100000: episode: 1594, duration: 0.221s, episode steps: 21, steps per second: 95, episode reward: 16.745, mean reward: 0.797 [0.726, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.705, 10.449], loss: 0.001540, mae: 0.038125, mean_q: 1.366311
 86914/100000: episode: 1595, duration: 0.222s, episode steps: 21, steps per second: 94, episode reward: 15.332, mean reward: 0.730 [0.601, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.393], loss: 0.001297, mae: 0.039755, mean_q: 1.376710
 86932/100000: episode: 1596, duration: 0.138s, episode steps: 18, steps per second: 130, episode reward: 14.235, mean reward: 0.791 [0.675, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.628, 10.504], loss: 0.001188, mae: 0.037348, mean_q: 1.373365
 86948/100000: episode: 1597, duration: 0.199s, episode steps: 16, steps per second: 80, episode reward: 12.232, mean reward: 0.764 [0.709, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.491, 10.474], loss: 0.001176, mae: 0.037761, mean_q: 1.377006
 86964/100000: episode: 1598, duration: 0.223s, episode steps: 16, steps per second: 72, episode reward: 12.086, mean reward: 0.755 [0.669, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.458], loss: 0.001684, mae: 0.038770, mean_q: 1.380733
 86981/100000: episode: 1599, duration: 0.122s, episode steps: 17, steps per second: 140, episode reward: 14.247, mean reward: 0.838 [0.765, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.972, 10.519], loss: 0.001241, mae: 0.038931, mean_q: 1.361894
 86998/100000: episode: 1600, duration: 0.213s, episode steps: 17, steps per second: 80, episode reward: 13.595, mean reward: 0.800 [0.734, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.565], loss: 0.001067, mae: 0.035875, mean_q: 1.387732
 87014/100000: episode: 1601, duration: 0.147s, episode steps: 16, steps per second: 109, episode reward: 13.377, mean reward: 0.836 [0.759, 0.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.059, 10.668], loss: 0.001275, mae: 0.039682, mean_q: 1.376050
 87037/100000: episode: 1602, duration: 0.205s, episode steps: 23, steps per second: 112, episode reward: 17.754, mean reward: 0.772 [0.692, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.200, 10.477], loss: 0.001145, mae: 0.037475, mean_q: 1.385366
 87054/100000: episode: 1603, duration: 0.148s, episode steps: 17, steps per second: 115, episode reward: 13.652, mean reward: 0.803 [0.745, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.681, 10.538], loss: 0.001742, mae: 0.040465, mean_q: 1.376891
 87072/100000: episode: 1604, duration: 0.129s, episode steps: 18, steps per second: 140, episode reward: 13.643, mean reward: 0.758 [0.697, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.484, 10.520], loss: 0.001386, mae: 0.042794, mean_q: 1.368567
 87089/100000: episode: 1605, duration: 0.123s, episode steps: 17, steps per second: 138, episode reward: 12.773, mean reward: 0.751 [0.654, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.356, 10.497], loss: 0.001617, mae: 0.039001, mean_q: 1.391013
 87106/100000: episode: 1606, duration: 0.172s, episode steps: 17, steps per second: 99, episode reward: 15.315, mean reward: 0.901 [0.795, 0.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.666], loss: 0.001290, mae: 0.040798, mean_q: 1.384655
 87123/100000: episode: 1607, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 14.085, mean reward: 0.829 [0.748, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.180, 10.578], loss: 0.001215, mae: 0.038643, mean_q: 1.388805
 87143/100000: episode: 1608, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 16.499, mean reward: 0.825 [0.764, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.586, 10.565], loss: 0.001097, mae: 0.036603, mean_q: 1.379586
 87159/100000: episode: 1609, duration: 0.136s, episode steps: 16, steps per second: 118, episode reward: 12.277, mean reward: 0.767 [0.704, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.487], loss: 0.001365, mae: 0.040931, mean_q: 1.394634
 87182/100000: episode: 1610, duration: 0.190s, episode steps: 23, steps per second: 121, episode reward: 20.363, mean reward: 0.885 [0.779, 0.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.308, 10.525], loss: 0.001074, mae: 0.036742, mean_q: 1.390138
 87198/100000: episode: 1611, duration: 0.112s, episode steps: 16, steps per second: 142, episode reward: 12.798, mean reward: 0.800 [0.749, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.647, 10.590], loss: 0.001239, mae: 0.039072, mean_q: 1.406041
 87215/100000: episode: 1612, duration: 0.167s, episode steps: 17, steps per second: 102, episode reward: 14.731, mean reward: 0.867 [0.820, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.595, 10.543], loss: 0.001267, mae: 0.039134, mean_q: 1.387563
 87233/100000: episode: 1613, duration: 0.145s, episode steps: 18, steps per second: 124, episode reward: 14.116, mean reward: 0.784 [0.678, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.536, 10.424], loss: 0.001390, mae: 0.041269, mean_q: 1.392981
 87256/100000: episode: 1614, duration: 0.329s, episode steps: 23, steps per second: 70, episode reward: 19.191, mean reward: 0.834 [0.768, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.660], loss: 0.001146, mae: 0.038254, mean_q: 1.397668
 87276/100000: episode: 1615, duration: 0.277s, episode steps: 20, steps per second: 72, episode reward: 15.472, mean reward: 0.774 [0.660, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.603, 10.412], loss: 0.001291, mae: 0.039944, mean_q: 1.400554
 87297/100000: episode: 1616, duration: 0.320s, episode steps: 21, steps per second: 66, episode reward: 17.410, mean reward: 0.829 [0.781, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.552, 10.500], loss: 0.001452, mae: 0.041987, mean_q: 1.392932
 87314/100000: episode: 1617, duration: 0.210s, episode steps: 17, steps per second: 81, episode reward: 13.753, mean reward: 0.809 [0.743, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.693, 10.523], loss: 0.001271, mae: 0.040161, mean_q: 1.400986
 87331/100000: episode: 1618, duration: 0.179s, episode steps: 17, steps per second: 95, episode reward: 12.945, mean reward: 0.761 [0.569, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.607, 10.309], loss: 0.001857, mae: 0.040131, mean_q: 1.400669
 87347/100000: episode: 1619, duration: 0.297s, episode steps: 16, steps per second: 54, episode reward: 11.736, mean reward: 0.733 [0.670, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.072, 10.391], loss: 0.001768, mae: 0.039255, mean_q: 1.389210
 87364/100000: episode: 1620, duration: 0.373s, episode steps: 17, steps per second: 46, episode reward: 13.287, mean reward: 0.782 [0.713, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.511], loss: 0.001106, mae: 0.037201, mean_q: 1.394413
 87381/100000: episode: 1621, duration: 0.326s, episode steps: 17, steps per second: 52, episode reward: 13.593, mean reward: 0.800 [0.763, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.281, 10.495], loss: 0.001226, mae: 0.039541, mean_q: 1.415988
 87398/100000: episode: 1622, duration: 0.297s, episode steps: 17, steps per second: 57, episode reward: 14.716, mean reward: 0.866 [0.738, 0.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.788, 10.623], loss: 0.001114, mae: 0.036464, mean_q: 1.411238
[Info] FALSIFICATION!
 87412/100000: episode: 1623, duration: 0.518s, episode steps: 14, steps per second: 27, episode reward: 11.557, mean reward: 0.826 [0.740, 1.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.120, 10.624], loss: 0.001763, mae: 0.038127, mean_q: 1.403087
 87430/100000: episode: 1624, duration: 0.228s, episode steps: 18, steps per second: 79, episode reward: 13.773, mean reward: 0.765 [0.623, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.450], loss: 0.002256, mae: 0.044383, mean_q: 1.386985
 87446/100000: episode: 1625, duration: 0.297s, episode steps: 16, steps per second: 54, episode reward: 13.034, mean reward: 0.815 [0.721, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.545], loss: 0.001237, mae: 0.039759, mean_q: 1.398007
 87463/100000: episode: 1626, duration: 0.250s, episode steps: 17, steps per second: 68, episode reward: 13.784, mean reward: 0.811 [0.733, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.298, 10.450], loss: 0.001722, mae: 0.040749, mean_q: 1.417029
 87480/100000: episode: 1627, duration: 0.248s, episode steps: 17, steps per second: 69, episode reward: 13.491, mean reward: 0.794 [0.700, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.419], loss: 0.001249, mae: 0.039466, mean_q: 1.414293
 87497/100000: episode: 1628, duration: 0.240s, episode steps: 17, steps per second: 71, episode reward: 13.550, mean reward: 0.797 [0.757, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.556], loss: 0.001127, mae: 0.036766, mean_q: 1.393912
[Info] Complete ISplit Iteration
[Info] Levels: [1.3619065, 1.5121439, 1.6387329, 1.6416111]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.58]
[Info] Error Prob: 0.0005800000000000001

 87518/100000: episode: 1629, duration: 8.468s, episode steps: 21, steps per second: 2, episode reward: 16.503, mean reward: 0.786 [0.699, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.430], loss: 0.001246, mae: 0.038898, mean_q: 1.392932
 87618/100000: episode: 1630, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 57.434, mean reward: 0.574 [0.498, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.202, 10.098], loss: 0.001453, mae: 0.040380, mean_q: 1.405282
 87718/100000: episode: 1631, duration: 1.423s, episode steps: 100, steps per second: 70, episode reward: 60.205, mean reward: 0.602 [0.502, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.072, 10.477], loss: 0.001205, mae: 0.038419, mean_q: 1.398452
 87818/100000: episode: 1632, duration: 1.265s, episode steps: 100, steps per second: 79, episode reward: 58.694, mean reward: 0.587 [0.509, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.435, 10.131], loss: 0.001375, mae: 0.039756, mean_q: 1.391659
 87918/100000: episode: 1633, duration: 0.964s, episode steps: 100, steps per second: 104, episode reward: 56.906, mean reward: 0.569 [0.498, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.878, 10.098], loss: 0.001423, mae: 0.038887, mean_q: 1.394118
 88018/100000: episode: 1634, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: 59.026, mean reward: 0.590 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.526, 10.204], loss: 0.001200, mae: 0.037748, mean_q: 1.386022
 88118/100000: episode: 1635, duration: 1.230s, episode steps: 100, steps per second: 81, episode reward: 58.466, mean reward: 0.585 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.947, 10.098], loss: 0.001488, mae: 0.040665, mean_q: 1.393831
 88218/100000: episode: 1636, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 58.027, mean reward: 0.580 [0.503, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.122, 10.156], loss: 0.001475, mae: 0.040159, mean_q: 1.379143
 88318/100000: episode: 1637, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.525, mean reward: 0.585 [0.498, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.951, 10.216], loss: 0.001568, mae: 0.040921, mean_q: 1.381126
 88418/100000: episode: 1638, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 61.758, mean reward: 0.618 [0.502, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-2.240, 10.098], loss: 0.001717, mae: 0.041381, mean_q: 1.374905
 88518/100000: episode: 1639, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.392, mean reward: 0.584 [0.508, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.715, 10.098], loss: 0.001403, mae: 0.039787, mean_q: 1.373824
 88618/100000: episode: 1640, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: 60.696, mean reward: 0.607 [0.507, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.485, 10.291], loss: 0.001520, mae: 0.040383, mean_q: 1.368669
 88718/100000: episode: 1641, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 59.759, mean reward: 0.598 [0.508, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.448, 10.359], loss: 0.001639, mae: 0.041897, mean_q: 1.368970
 88818/100000: episode: 1642, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 60.813, mean reward: 0.608 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.795, 10.138], loss: 0.001446, mae: 0.040319, mean_q: 1.375197
 88918/100000: episode: 1643, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 59.684, mean reward: 0.597 [0.509, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.883, 10.147], loss: 0.001323, mae: 0.039200, mean_q: 1.354936
 89018/100000: episode: 1644, duration: 0.815s, episode steps: 100, steps per second: 123, episode reward: 60.214, mean reward: 0.602 [0.516, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.148, 10.098], loss: 0.001604, mae: 0.042283, mean_q: 1.360825
 89118/100000: episode: 1645, duration: 0.834s, episode steps: 100, steps per second: 120, episode reward: 60.085, mean reward: 0.601 [0.505, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.631, 10.098], loss: 0.001721, mae: 0.041938, mean_q: 1.355142
 89218/100000: episode: 1646, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 57.670, mean reward: 0.577 [0.506, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.326, 10.098], loss: 0.001520, mae: 0.040647, mean_q: 1.344159
 89318/100000: episode: 1647, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 57.337, mean reward: 0.573 [0.508, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.299, 10.162], loss: 0.001661, mae: 0.042241, mean_q: 1.343977
 89418/100000: episode: 1648, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 57.454, mean reward: 0.575 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.657, 10.098], loss: 0.001555, mae: 0.041749, mean_q: 1.332530
 89518/100000: episode: 1649, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 57.194, mean reward: 0.572 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.389, 10.183], loss: 0.001527, mae: 0.041440, mean_q: 1.329386
 89618/100000: episode: 1650, duration: 0.651s, episode steps: 100, steps per second: 154, episode reward: 58.081, mean reward: 0.581 [0.500, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.652, 10.098], loss: 0.001612, mae: 0.041621, mean_q: 1.332356
 89718/100000: episode: 1651, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.308, mean reward: 0.603 [0.507, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.397, 10.474], loss: 0.001881, mae: 0.044110, mean_q: 1.328068
 89818/100000: episode: 1652, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.301, mean reward: 0.583 [0.503, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.421, 10.168], loss: 0.001721, mae: 0.042298, mean_q: 1.322377
 89918/100000: episode: 1653, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.660, mean reward: 0.587 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.086, 10.340], loss: 0.001566, mae: 0.041719, mean_q: 1.306449
 90018/100000: episode: 1654, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 63.445, mean reward: 0.634 [0.511, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.996, 10.244], loss: 0.001824, mae: 0.042484, mean_q: 1.312175
 90118/100000: episode: 1655, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 56.540, mean reward: 0.565 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.206, 10.098], loss: 0.001434, mae: 0.041318, mean_q: 1.304943
 90218/100000: episode: 1656, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.039, mean reward: 0.590 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.308, 10.189], loss: 0.001975, mae: 0.043945, mean_q: 1.304263
 90318/100000: episode: 1657, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.559, mean reward: 0.586 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.887, 10.332], loss: 0.001728, mae: 0.043537, mean_q: 1.303400
 90418/100000: episode: 1658, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 60.667, mean reward: 0.607 [0.498, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.015, 10.243], loss: 0.001483, mae: 0.041553, mean_q: 1.291285
 90518/100000: episode: 1659, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 56.431, mean reward: 0.564 [0.502, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.779, 10.098], loss: 0.001748, mae: 0.042381, mean_q: 1.283164
 90618/100000: episode: 1660, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 59.592, mean reward: 0.596 [0.503, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.673, 10.098], loss: 0.001728, mae: 0.043734, mean_q: 1.284744
 90718/100000: episode: 1661, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: 57.080, mean reward: 0.571 [0.503, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.571, 10.192], loss: 0.001814, mae: 0.044373, mean_q: 1.282421
 90818/100000: episode: 1662, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: 61.039, mean reward: 0.610 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.843, 10.309], loss: 0.001764, mae: 0.043114, mean_q: 1.275637
 90918/100000: episode: 1663, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 57.875, mean reward: 0.579 [0.505, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.424, 10.189], loss: 0.001675, mae: 0.043320, mean_q: 1.274051
 91018/100000: episode: 1664, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 57.622, mean reward: 0.576 [0.511, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.879, 10.098], loss: 0.001530, mae: 0.041613, mean_q: 1.267942
 91118/100000: episode: 1665, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 62.624, mean reward: 0.626 [0.510, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.231, 10.098], loss: 0.001611, mae: 0.042202, mean_q: 1.263290
 91218/100000: episode: 1666, duration: 0.897s, episode steps: 100, steps per second: 111, episode reward: 58.385, mean reward: 0.584 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.422, 10.098], loss: 0.001774, mae: 0.043172, mean_q: 1.256088
 91318/100000: episode: 1667, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 59.420, mean reward: 0.594 [0.499, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.808, 10.211], loss: 0.001648, mae: 0.041711, mean_q: 1.247728
 91418/100000: episode: 1668, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 58.824, mean reward: 0.588 [0.502, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.483, 10.128], loss: 0.001873, mae: 0.042559, mean_q: 1.236439
 91518/100000: episode: 1669, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: 60.675, mean reward: 0.607 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.611, 10.098], loss: 0.001635, mae: 0.042398, mean_q: 1.234679
 91618/100000: episode: 1670, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.664, mean reward: 0.577 [0.504, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.167, 10.098], loss: 0.001829, mae: 0.043483, mean_q: 1.221130
 91718/100000: episode: 1671, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 57.778, mean reward: 0.578 [0.517, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.231, 10.098], loss: 0.001555, mae: 0.042173, mean_q: 1.220827
 91818/100000: episode: 1672, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.062, mean reward: 0.571 [0.501, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.184, 10.176], loss: 0.001777, mae: 0.042728, mean_q: 1.207446
 91918/100000: episode: 1673, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 58.778, mean reward: 0.588 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.638, 10.098], loss: 0.001650, mae: 0.043163, mean_q: 1.207100
 92018/100000: episode: 1674, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.676, mean reward: 0.587 [0.503, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.662, 10.194], loss: 0.001571, mae: 0.041983, mean_q: 1.201647
 92118/100000: episode: 1675, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 62.919, mean reward: 0.629 [0.510, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.103, 10.359], loss: 0.001669, mae: 0.043229, mean_q: 1.192783
 92218/100000: episode: 1676, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.342, mean reward: 0.573 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.494, 10.099], loss: 0.001500, mae: 0.041746, mean_q: 1.187999
 92318/100000: episode: 1677, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.756, mean reward: 0.598 [0.510, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.173, 10.254], loss: 0.001423, mae: 0.041244, mean_q: 1.183713
 92418/100000: episode: 1678, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.121, mean reward: 0.571 [0.499, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.457, 10.098], loss: 0.001579, mae: 0.042444, mean_q: 1.176493
 92518/100000: episode: 1679, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 57.713, mean reward: 0.577 [0.503, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.362, 10.151], loss: 0.001396, mae: 0.040672, mean_q: 1.166017
 92618/100000: episode: 1680, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.183, mean reward: 0.582 [0.498, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.133, 10.272], loss: 0.001412, mae: 0.040197, mean_q: 1.163979
 92718/100000: episode: 1681, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.698, mean reward: 0.587 [0.502, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.177, 10.098], loss: 0.001382, mae: 0.040510, mean_q: 1.168042
 92818/100000: episode: 1682, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.519, mean reward: 0.585 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.722, 10.120], loss: 0.001313, mae: 0.039686, mean_q: 1.169126
 92918/100000: episode: 1683, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 60.280, mean reward: 0.603 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.853, 10.098], loss: 0.001360, mae: 0.039870, mean_q: 1.165478
 93018/100000: episode: 1684, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 60.090, mean reward: 0.601 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.214, 10.098], loss: 0.001274, mae: 0.038549, mean_q: 1.168178
 93118/100000: episode: 1685, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 60.417, mean reward: 0.604 [0.509, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.546, 10.323], loss: 0.001442, mae: 0.041053, mean_q: 1.167710
 93218/100000: episode: 1686, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.409, mean reward: 0.574 [0.497, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.284, 10.098], loss: 0.001394, mae: 0.040366, mean_q: 1.168820
 93318/100000: episode: 1687, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.436, mean reward: 0.594 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.344, 10.336], loss: 0.001402, mae: 0.040661, mean_q: 1.168876
 93418/100000: episode: 1688, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.875, mean reward: 0.569 [0.499, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.730, 10.102], loss: 0.001353, mae: 0.040610, mean_q: 1.166667
 93518/100000: episode: 1689, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.839, mean reward: 0.598 [0.500, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.805, 10.098], loss: 0.001364, mae: 0.040206, mean_q: 1.166078
 93618/100000: episode: 1690, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.044, mean reward: 0.590 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.771, 10.366], loss: 0.001352, mae: 0.040411, mean_q: 1.166377
 93718/100000: episode: 1691, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 61.105, mean reward: 0.611 [0.503, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.171, 10.098], loss: 0.001392, mae: 0.040138, mean_q: 1.162575
 93818/100000: episode: 1692, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 60.001, mean reward: 0.600 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.184, 10.399], loss: 0.001425, mae: 0.040834, mean_q: 1.165193
 93918/100000: episode: 1693, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.040, mean reward: 0.580 [0.503, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.942, 10.098], loss: 0.001324, mae: 0.039294, mean_q: 1.167617
 94018/100000: episode: 1694, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.139, mean reward: 0.591 [0.513, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.711, 10.150], loss: 0.001361, mae: 0.039883, mean_q: 1.162691
 94118/100000: episode: 1695, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.873, mean reward: 0.579 [0.506, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.911, 10.205], loss: 0.001291, mae: 0.038792, mean_q: 1.161763
 94218/100000: episode: 1696, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.274, mean reward: 0.573 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.467, 10.098], loss: 0.001293, mae: 0.039025, mean_q: 1.162122
 94318/100000: episode: 1697, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.136, mean reward: 0.581 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.603, 10.098], loss: 0.001257, mae: 0.038728, mean_q: 1.161017
 94418/100000: episode: 1698, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.784, mean reward: 0.578 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.361, 10.098], loss: 0.001257, mae: 0.039186, mean_q: 1.162156
 94518/100000: episode: 1699, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 58.321, mean reward: 0.583 [0.500, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.883, 10.098], loss: 0.001348, mae: 0.040160, mean_q: 1.167445
 94618/100000: episode: 1700, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.672, mean reward: 0.587 [0.507, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.505, 10.151], loss: 0.001209, mae: 0.038459, mean_q: 1.166394
 94718/100000: episode: 1701, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 63.405, mean reward: 0.634 [0.508, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.673, 10.098], loss: 0.001237, mae: 0.038461, mean_q: 1.167577
 94818/100000: episode: 1702, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.875, mean reward: 0.589 [0.506, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.794, 10.098], loss: 0.001357, mae: 0.039829, mean_q: 1.172051
 94918/100000: episode: 1703, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.477, mean reward: 0.585 [0.502, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.858, 10.098], loss: 0.001323, mae: 0.039436, mean_q: 1.165608
 95018/100000: episode: 1704, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 59.239, mean reward: 0.592 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.486, 10.098], loss: 0.001292, mae: 0.039326, mean_q: 1.164801
 95118/100000: episode: 1705, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: 58.952, mean reward: 0.590 [0.513, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.901, 10.098], loss: 0.001453, mae: 0.041804, mean_q: 1.164711
 95218/100000: episode: 1706, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.042, mean reward: 0.580 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.979, 10.259], loss: 0.001296, mae: 0.039258, mean_q: 1.162906
 95318/100000: episode: 1707, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.463, mean reward: 0.575 [0.512, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.984, 10.137], loss: 0.001323, mae: 0.039847, mean_q: 1.163499
 95418/100000: episode: 1708, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 61.679, mean reward: 0.617 [0.501, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.084, 10.266], loss: 0.001302, mae: 0.039474, mean_q: 1.164328
 95518/100000: episode: 1709, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.966, mean reward: 0.610 [0.504, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.497, 10.138], loss: 0.001393, mae: 0.041466, mean_q: 1.169325
 95618/100000: episode: 1710, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 61.649, mean reward: 0.616 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.743, 10.147], loss: 0.001332, mae: 0.039491, mean_q: 1.169501
 95718/100000: episode: 1711, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.997, mean reward: 0.580 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.171, 10.102], loss: 0.001355, mae: 0.040261, mean_q: 1.174050
 95818/100000: episode: 1712, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 56.773, mean reward: 0.568 [0.505, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.797, 10.187], loss: 0.001403, mae: 0.041379, mean_q: 1.166302
 95918/100000: episode: 1713, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.593, mean reward: 0.596 [0.508, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.442, 10.098], loss: 0.001295, mae: 0.039797, mean_q: 1.166345
 96018/100000: episode: 1714, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.340, mean reward: 0.593 [0.510, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.043, 10.098], loss: 0.001289, mae: 0.039692, mean_q: 1.166527
 96118/100000: episode: 1715, duration: 0.907s, episode steps: 100, steps per second: 110, episode reward: 57.662, mean reward: 0.577 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.367, 10.189], loss: 0.001376, mae: 0.040680, mean_q: 1.169378
 96218/100000: episode: 1716, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 59.588, mean reward: 0.596 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.238, 10.217], loss: 0.001320, mae: 0.040135, mean_q: 1.168280
 96318/100000: episode: 1717, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 57.236, mean reward: 0.572 [0.513, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.267, 10.098], loss: 0.001273, mae: 0.039327, mean_q: 1.163875
 96418/100000: episode: 1718, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 60.595, mean reward: 0.606 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.888, 10.098], loss: 0.001343, mae: 0.040487, mean_q: 1.161632
 96518/100000: episode: 1719, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 59.694, mean reward: 0.597 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.128, 10.098], loss: 0.001334, mae: 0.039996, mean_q: 1.165509
 96618/100000: episode: 1720, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 59.581, mean reward: 0.596 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.760, 10.252], loss: 0.001382, mae: 0.040756, mean_q: 1.164359
 96718/100000: episode: 1721, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 58.369, mean reward: 0.584 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.950, 10.098], loss: 0.001373, mae: 0.040568, mean_q: 1.166484
 96818/100000: episode: 1722, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 59.938, mean reward: 0.599 [0.512, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.814, 10.197], loss: 0.001367, mae: 0.040034, mean_q: 1.166781
 96918/100000: episode: 1723, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 58.399, mean reward: 0.584 [0.509, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.405, 10.209], loss: 0.001363, mae: 0.040209, mean_q: 1.166677
 97018/100000: episode: 1724, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 56.602, mean reward: 0.566 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.363, 10.098], loss: 0.001375, mae: 0.040209, mean_q: 1.166177
 97118/100000: episode: 1725, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 57.550, mean reward: 0.575 [0.506, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.698, 10.260], loss: 0.001445, mae: 0.041310, mean_q: 1.166587
 97218/100000: episode: 1726, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 59.028, mean reward: 0.590 [0.513, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.322, 10.098], loss: 0.001455, mae: 0.041482, mean_q: 1.166544
 97318/100000: episode: 1727, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 60.012, mean reward: 0.600 [0.503, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.318, 10.098], loss: 0.001415, mae: 0.041243, mean_q: 1.165428
 97418/100000: episode: 1728, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 57.217, mean reward: 0.572 [0.499, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.471, 10.098], loss: 0.001381, mae: 0.040795, mean_q: 1.163301
[Info] 1-TH LEVEL FOUND: 1.3855947256088257, Considering 10/90 traces
 97518/100000: episode: 1729, duration: 5.853s, episode steps: 100, steps per second: 17, episode reward: 56.956, mean reward: 0.570 [0.504, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.488, 10.098], loss: 0.001392, mae: 0.040989, mean_q: 1.165790
 97615/100000: episode: 1730, duration: 0.551s, episode steps: 97, steps per second: 176, episode reward: 57.328, mean reward: 0.591 [0.499, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-0.844, 10.171], loss: 0.001343, mae: 0.040368, mean_q: 1.166742
 97641/100000: episode: 1731, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 15.850, mean reward: 0.610 [0.514, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.405, 10.100], loss: 0.001499, mae: 0.041827, mean_q: 1.165321
 97679/100000: episode: 1732, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 25.441, mean reward: 0.670 [0.518, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.889, 10.128], loss: 0.001372, mae: 0.040859, mean_q: 1.161492
 97732/100000: episode: 1733, duration: 0.295s, episode steps: 53, steps per second: 179, episode reward: 31.808, mean reward: 0.600 [0.506, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.922, 10.244], loss: 0.001318, mae: 0.039773, mean_q: 1.165773
 97777/100000: episode: 1734, duration: 0.255s, episode steps: 45, steps per second: 176, episode reward: 26.417, mean reward: 0.587 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.600, 10.239], loss: 0.001250, mae: 0.038850, mean_q: 1.168387
 97874/100000: episode: 1735, duration: 0.503s, episode steps: 97, steps per second: 193, episode reward: 57.376, mean reward: 0.592 [0.503, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-1.526, 10.100], loss: 0.001424, mae: 0.041205, mean_q: 1.165948
 97912/100000: episode: 1736, duration: 0.299s, episode steps: 38, steps per second: 127, episode reward: 29.223, mean reward: 0.769 [0.684, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.708, 10.100], loss: 0.001357, mae: 0.040130, mean_q: 1.171562
 97965/100000: episode: 1737, duration: 0.270s, episode steps: 53, steps per second: 196, episode reward: 33.700, mean reward: 0.636 [0.511, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.909, 10.156], loss: 0.001402, mae: 0.041049, mean_q: 1.172428
 98010/100000: episode: 1738, duration: 0.250s, episode steps: 45, steps per second: 180, episode reward: 27.649, mean reward: 0.614 [0.519, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.581, 10.238], loss: 0.001369, mae: 0.040330, mean_q: 1.167519
 98036/100000: episode: 1739, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 14.959, mean reward: 0.575 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.161, 10.125], loss: 0.001384, mae: 0.040108, mean_q: 1.166184
 98062/100000: episode: 1740, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 17.736, mean reward: 0.682 [0.641, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.269, 10.100], loss: 0.001408, mae: 0.040019, mean_q: 1.163775
 98132/100000: episode: 1741, duration: 0.377s, episode steps: 70, steps per second: 186, episode reward: 42.034, mean reward: 0.600 [0.500, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.737 [-2.019, 10.285], loss: 0.001468, mae: 0.041702, mean_q: 1.169339
 98177/100000: episode: 1742, duration: 0.229s, episode steps: 45, steps per second: 196, episode reward: 29.769, mean reward: 0.662 [0.574, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-1.576, 10.298], loss: 0.001530, mae: 0.042411, mean_q: 1.165648
 98230/100000: episode: 1743, duration: 0.293s, episode steps: 53, steps per second: 181, episode reward: 32.636, mean reward: 0.616 [0.532, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.135, 10.100], loss: 0.001462, mae: 0.041447, mean_q: 1.172461
 98238/100000: episode: 1744, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 5.418, mean reward: 0.677 [0.661, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.375], loss: 0.001528, mae: 0.041293, mean_q: 1.170196
 98291/100000: episode: 1745, duration: 0.280s, episode steps: 53, steps per second: 189, episode reward: 34.454, mean reward: 0.650 [0.549, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.898, 10.278], loss: 0.001505, mae: 0.041558, mean_q: 1.172219
 98361/100000: episode: 1746, duration: 0.355s, episode steps: 70, steps per second: 197, episode reward: 44.727, mean reward: 0.639 [0.511, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-1.280, 10.100], loss: 0.001522, mae: 0.042418, mean_q: 1.178070
 98369/100000: episode: 1747, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 6.003, mean reward: 0.750 [0.688, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.595], loss: 0.001120, mae: 0.036535, mean_q: 1.163049
 98383/100000: episode: 1748, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.841, mean reward: 0.774 [0.703, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.405, 10.607], loss: 0.001538, mae: 0.040625, mean_q: 1.162047
 98409/100000: episode: 1749, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 19.162, mean reward: 0.737 [0.643, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.315, 10.100], loss: 0.001414, mae: 0.041332, mean_q: 1.178931
 98471/100000: episode: 1750, duration: 0.312s, episode steps: 62, steps per second: 199, episode reward: 40.107, mean reward: 0.647 [0.577, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.798 [-0.159, 10.100], loss: 0.001509, mae: 0.042342, mean_q: 1.178062
 98568/100000: episode: 1751, duration: 0.524s, episode steps: 97, steps per second: 185, episode reward: 61.454, mean reward: 0.634 [0.505, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.734, 10.100], loss: 0.001505, mae: 0.042229, mean_q: 1.172943
 98594/100000: episode: 1752, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 17.515, mean reward: 0.674 [0.617, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.492, 10.100], loss: 0.001375, mae: 0.041116, mean_q: 1.182450
 98656/100000: episode: 1753, duration: 0.343s, episode steps: 62, steps per second: 181, episode reward: 41.244, mean reward: 0.665 [0.584, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.800 [-0.634, 10.100], loss: 0.001497, mae: 0.042605, mean_q: 1.181205
 98701/100000: episode: 1754, duration: 0.239s, episode steps: 45, steps per second: 188, episode reward: 26.525, mean reward: 0.589 [0.509, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.163, 10.181], loss: 0.001569, mae: 0.042324, mean_q: 1.183719
 98754/100000: episode: 1755, duration: 0.265s, episode steps: 53, steps per second: 200, episode reward: 35.820, mean reward: 0.676 [0.602, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.917, 10.100], loss: 0.001695, mae: 0.044044, mean_q: 1.181127
 98780/100000: episode: 1756, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 17.801, mean reward: 0.685 [0.600, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.378, 10.100], loss: 0.001760, mae: 0.044129, mean_q: 1.175894
 98833/100000: episode: 1757, duration: 0.268s, episode steps: 53, steps per second: 197, episode reward: 35.943, mean reward: 0.678 [0.605, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.962, 10.100], loss: 0.001578, mae: 0.041961, mean_q: 1.183211
 98878/100000: episode: 1758, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 33.307, mean reward: 0.740 [0.647, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.642, 10.556], loss: 0.001834, mae: 0.045086, mean_q: 1.187296
 98923/100000: episode: 1759, duration: 0.250s, episode steps: 45, steps per second: 180, episode reward: 26.843, mean reward: 0.597 [0.510, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.567, 10.146], loss: 0.001857, mae: 0.046475, mean_q: 1.187811
 98968/100000: episode: 1760, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 26.076, mean reward: 0.579 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.176, 10.156], loss: 0.001754, mae: 0.045302, mean_q: 1.181205
 99013/100000: episode: 1761, duration: 0.232s, episode steps: 45, steps per second: 194, episode reward: 28.952, mean reward: 0.643 [0.569, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.845, 10.328], loss: 0.001687, mae: 0.042883, mean_q: 1.183001
 99028/100000: episode: 1762, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 10.670, mean reward: 0.711 [0.627, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.510], loss: 0.001503, mae: 0.041469, mean_q: 1.179586
 99054/100000: episode: 1763, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 17.715, mean reward: 0.681 [0.572, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.349, 10.100], loss: 0.001725, mae: 0.043160, mean_q: 1.191938
 99124/100000: episode: 1764, duration: 0.366s, episode steps: 70, steps per second: 191, episode reward: 44.186, mean reward: 0.631 [0.512, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.720 [-1.946, 10.172], loss: 0.001809, mae: 0.044748, mean_q: 1.187314
 99186/100000: episode: 1765, duration: 0.349s, episode steps: 62, steps per second: 178, episode reward: 36.703, mean reward: 0.592 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-0.637, 10.100], loss: 0.001667, mae: 0.043066, mean_q: 1.190653
 99194/100000: episode: 1766, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 6.420, mean reward: 0.803 [0.739, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.472], loss: 0.001666, mae: 0.044728, mean_q: 1.185676
 99256/100000: episode: 1767, duration: 0.330s, episode steps: 62, steps per second: 188, episode reward: 40.823, mean reward: 0.658 [0.536, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-0.399, 10.100], loss: 0.001767, mae: 0.045068, mean_q: 1.193834
 99270/100000: episode: 1768, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 10.383, mean reward: 0.742 [0.665, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.060, 10.483], loss: 0.002007, mae: 0.045521, mean_q: 1.183017
 99296/100000: episode: 1769, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 16.712, mean reward: 0.643 [0.526, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.383, 10.169], loss: 0.001831, mae: 0.046100, mean_q: 1.194098
 99310/100000: episode: 1770, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 10.313, mean reward: 0.737 [0.656, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.231, 10.471], loss: 0.001376, mae: 0.040745, mean_q: 1.206802
 99348/100000: episode: 1771, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 26.382, mean reward: 0.694 [0.648, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.953, 10.100], loss: 0.001627, mae: 0.042312, mean_q: 1.191409
 99401/100000: episode: 1772, duration: 0.290s, episode steps: 53, steps per second: 183, episode reward: 33.439, mean reward: 0.631 [0.564, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.326, 10.100], loss: 0.001529, mae: 0.042185, mean_q: 1.191534
 99439/100000: episode: 1773, duration: 0.211s, episode steps: 38, steps per second: 181, episode reward: 25.197, mean reward: 0.663 [0.567, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.565, 10.100], loss: 0.001437, mae: 0.040341, mean_q: 1.201009
 99484/100000: episode: 1774, duration: 0.240s, episode steps: 45, steps per second: 188, episode reward: 28.335, mean reward: 0.630 [0.562, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.407, 10.409], loss: 0.001564, mae: 0.042168, mean_q: 1.196613
 99498/100000: episode: 1775, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 9.914, mean reward: 0.708 [0.653, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.657, 10.435], loss: 0.001711, mae: 0.044392, mean_q: 1.193790
 99543/100000: episode: 1776, duration: 0.235s, episode steps: 45, steps per second: 192, episode reward: 28.789, mean reward: 0.640 [0.526, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.014, 10.197], loss: 0.001664, mae: 0.043345, mean_q: 1.206884
 99569/100000: episode: 1777, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 17.284, mean reward: 0.665 [0.591, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.473, 10.100], loss: 0.001837, mae: 0.045207, mean_q: 1.201364
 99607/100000: episode: 1778, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 25.539, mean reward: 0.672 [0.563, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.264, 10.100], loss: 0.001631, mae: 0.044301, mean_q: 1.198535
 99622/100000: episode: 1779, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 10.617, mean reward: 0.708 [0.671, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.480], loss: 0.001646, mae: 0.042874, mean_q: 1.192658
 99648/100000: episode: 1780, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 17.437, mean reward: 0.671 [0.583, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.644, 10.100], loss: 0.001630, mae: 0.042742, mean_q: 1.187045
 99701/100000: episode: 1781, duration: 0.294s, episode steps: 53, steps per second: 181, episode reward: 32.489, mean reward: 0.613 [0.516, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.933, 10.234], loss: 0.001760, mae: 0.043513, mean_q: 1.199601
 99727/100000: episode: 1782, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 17.297, mean reward: 0.665 [0.592, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.121, 10.100], loss: 0.001465, mae: 0.041021, mean_q: 1.204384
 99797/100000: episode: 1783, duration: 0.366s, episode steps: 70, steps per second: 191, episode reward: 45.467, mean reward: 0.650 [0.542, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-0.897, 10.283], loss: 0.001828, mae: 0.045382, mean_q: 1.200320
 99894/100000: episode: 1784, duration: 0.496s, episode steps: 97, steps per second: 195, episode reward: 58.438, mean reward: 0.602 [0.516, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-0.264, 10.159], loss: 0.001788, mae: 0.045105, mean_q: 1.202163
 99908/100000: episode: 1785, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 10.433, mean reward: 0.745 [0.708, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.409, 10.430], loss: 0.001650, mae: 0.041753, mean_q: 1.212295
 99923/100000: episode: 1786, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 11.322, mean reward: 0.755 [0.727, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.548, 10.435], loss: 0.001383, mae: 0.041462, mean_q: 1.209345
 99985/100000: episode: 1787, duration: 0.336s, episode steps: 62, steps per second: 185, episode reward: 38.770, mean reward: 0.625 [0.522, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-0.684, 10.179], loss: 0.001588, mae: 0.043182, mean_q: 1.209594
done, took 616.248 seconds
[Info] End Importance Splitting. Falsification occurred 14 times.
