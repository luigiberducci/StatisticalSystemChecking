Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.185s, episode steps: 100, steps per second: 542, episode reward: 59.182, mean reward: 0.592 [0.508, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.439, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.072s, episode steps: 100, steps per second: 1397, episode reward: 59.092, mean reward: 0.591 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.992, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.062s, episode steps: 100, steps per second: 1611, episode reward: 57.164, mean reward: 0.572 [0.504, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.887, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.062s, episode steps: 100, steps per second: 1615, episode reward: 57.973, mean reward: 0.580 [0.507, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.780, 10.164], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1605, episode reward: 57.999, mean reward: 0.580 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.772, 10.143], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.062s, episode steps: 100, steps per second: 1616, episode reward: 66.277, mean reward: 0.663 [0.522, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.895, 10.309], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.062s, episode steps: 100, steps per second: 1615, episode reward: 59.799, mean reward: 0.598 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.849, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.074s, episode steps: 100, steps per second: 1360, episode reward: 61.639, mean reward: 0.616 [0.523, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.381, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.062s, episode steps: 100, steps per second: 1615, episode reward: 56.950, mean reward: 0.569 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.994, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.062s, episode steps: 100, steps per second: 1624, episode reward: 58.051, mean reward: 0.581 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.878, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.066s, episode steps: 100, steps per second: 1512, episode reward: 57.867, mean reward: 0.579 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.327, 10.132], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.062s, episode steps: 100, steps per second: 1617, episode reward: 58.049, mean reward: 0.580 [0.506, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.943, 10.177], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.062s, episode steps: 100, steps per second: 1622, episode reward: 59.098, mean reward: 0.591 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.786, 10.274], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.062s, episode steps: 100, steps per second: 1623, episode reward: 59.409, mean reward: 0.594 [0.510, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.933, 10.318], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 59.354, mean reward: 0.594 [0.503, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.299, 10.135], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.073s, episode steps: 100, steps per second: 1373, episode reward: 58.171, mean reward: 0.582 [0.500, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.474, 10.145], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.062s, episode steps: 100, steps per second: 1608, episode reward: 61.720, mean reward: 0.617 [0.524, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.579, 10.199], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.062s, episode steps: 100, steps per second: 1610, episode reward: 58.690, mean reward: 0.587 [0.511, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.485, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 58.250, mean reward: 0.583 [0.513, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.371, 10.182], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.066s, episode steps: 100, steps per second: 1515, episode reward: 57.317, mean reward: 0.573 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.207, 10.221], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 64.334, mean reward: 0.643 [0.521, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.015, 10.426], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.062s, episode steps: 100, steps per second: 1606, episode reward: 59.507, mean reward: 0.595 [0.518, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.791, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.063s, episode steps: 100, steps per second: 1599, episode reward: 57.421, mean reward: 0.574 [0.504, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.952, 10.245], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.062s, episode steps: 100, steps per second: 1618, episode reward: 58.407, mean reward: 0.584 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.792, 10.116], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.062s, episode steps: 100, steps per second: 1620, episode reward: 59.367, mean reward: 0.594 [0.509, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.200, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.062s, episode steps: 100, steps per second: 1617, episode reward: 58.257, mean reward: 0.583 [0.507, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.307, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.062s, episode steps: 100, steps per second: 1612, episode reward: 59.350, mean reward: 0.593 [0.513, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.393, 10.282], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.062s, episode steps: 100, steps per second: 1622, episode reward: 60.585, mean reward: 0.606 [0.510, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.664, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.062s, episode steps: 100, steps per second: 1605, episode reward: 57.999, mean reward: 0.580 [0.503, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.948, 10.099], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 58.068, mean reward: 0.581 [0.508, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.896, 10.124], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 59.318, mean reward: 0.593 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.580, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.082s, episode steps: 100, steps per second: 1220, episode reward: 62.619, mean reward: 0.626 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.610, 10.128], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.062s, episode steps: 100, steps per second: 1614, episode reward: 59.894, mean reward: 0.599 [0.504, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.742, 10.500], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.062s, episode steps: 100, steps per second: 1617, episode reward: 58.312, mean reward: 0.583 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.286, 10.155], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.062s, episode steps: 100, steps per second: 1623, episode reward: 62.343, mean reward: 0.623 [0.511, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.277, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.062s, episode steps: 100, steps per second: 1603, episode reward: 58.347, mean reward: 0.583 [0.503, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.063, 10.167], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.062s, episode steps: 100, steps per second: 1606, episode reward: 59.066, mean reward: 0.591 [0.510, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.060, 10.138], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.062s, episode steps: 100, steps per second: 1620, episode reward: 57.676, mean reward: 0.577 [0.499, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.937, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 60.013, mean reward: 0.600 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.720, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.062s, episode steps: 100, steps per second: 1613, episode reward: 63.372, mean reward: 0.634 [0.528, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.817, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.062s, episode steps: 100, steps per second: 1618, episode reward: 58.593, mean reward: 0.586 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.724, 10.187], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.063s, episode steps: 100, steps per second: 1597, episode reward: 59.050, mean reward: 0.591 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.982, 10.173], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.062s, episode steps: 100, steps per second: 1616, episode reward: 59.235, mean reward: 0.592 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.661, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.062s, episode steps: 100, steps per second: 1617, episode reward: 59.052, mean reward: 0.591 [0.508, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.061, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.062s, episode steps: 100, steps per second: 1601, episode reward: 59.699, mean reward: 0.597 [0.520, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.097, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.062s, episode steps: 100, steps per second: 1614, episode reward: 58.668, mean reward: 0.587 [0.508, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.934, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.063s, episode steps: 100, steps per second: 1591, episode reward: 58.034, mean reward: 0.580 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.674, 10.215], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 59.497, mean reward: 0.595 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.621, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.062s, episode steps: 100, steps per second: 1610, episode reward: 58.936, mean reward: 0.589 [0.498, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.674, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.063s, episode steps: 100, steps per second: 1586, episode reward: 59.516, mean reward: 0.595 [0.512, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.006, 10.216], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.238s, episode steps: 100, steps per second: 81, episode reward: 59.795, mean reward: 0.598 [0.503, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.538, 10.249], loss: 0.011953, mae: 0.095211, mean_q: 1.015705
  5200/100000: episode: 52, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.678, mean reward: 0.587 [0.500, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.590, 10.098], loss: 0.005242, mae: 0.067597, mean_q: 1.086876
  5300/100000: episode: 53, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.584, mean reward: 0.596 [0.498, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.619, 10.157], loss: 0.004732, mae: 0.065589, mean_q: 1.119704
  5400/100000: episode: 54, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 60.453, mean reward: 0.605 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.623, 10.098], loss: 0.003783, mae: 0.057756, mean_q: 1.141001
  5500/100000: episode: 55, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.441, mean reward: 0.574 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.436, 10.219], loss: 0.004267, mae: 0.063711, mean_q: 1.155506
  5600/100000: episode: 56, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.693, mean reward: 0.577 [0.501, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.329, 10.307], loss: 0.004369, mae: 0.065865, mean_q: 1.157132
  5700/100000: episode: 57, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.403, mean reward: 0.604 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.865, 10.098], loss: 0.002843, mae: 0.051903, mean_q: 1.163143
  5800/100000: episode: 58, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 61.902, mean reward: 0.619 [0.528, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.387, 10.443], loss: 0.003222, mae: 0.055317, mean_q: 1.167309
  5900/100000: episode: 59, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 56.736, mean reward: 0.567 [0.502, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.255, 10.109], loss: 0.003059, mae: 0.054141, mean_q: 1.170739
  6000/100000: episode: 60, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.527, mean reward: 0.585 [0.509, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.710, 10.133], loss: 0.002632, mae: 0.051038, mean_q: 1.169669
  6100/100000: episode: 61, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.088, mean reward: 0.601 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.966, 10.098], loss: 0.002768, mae: 0.053793, mean_q: 1.173622
  6200/100000: episode: 62, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.625, mean reward: 0.576 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.652, 10.182], loss: 0.002998, mae: 0.054255, mean_q: 1.171564
  6300/100000: episode: 63, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.463, mean reward: 0.595 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.098], loss: 0.003117, mae: 0.055061, mean_q: 1.171346
  6400/100000: episode: 64, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.644, mean reward: 0.586 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.311, 10.098], loss: 0.003259, mae: 0.056572, mean_q: 1.170191
  6500/100000: episode: 65, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 57.774, mean reward: 0.578 [0.504, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.972, 10.098], loss: 0.002807, mae: 0.054624, mean_q: 1.176051
  6600/100000: episode: 66, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.520, mean reward: 0.575 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.752, 10.098], loss: 0.003054, mae: 0.054128, mean_q: 1.172087
  6700/100000: episode: 67, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.449, mean reward: 0.594 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.291, 10.098], loss: 0.002822, mae: 0.053001, mean_q: 1.169254
  6800/100000: episode: 68, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 60.607, mean reward: 0.606 [0.502, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.760, 10.098], loss: 0.002909, mae: 0.052829, mean_q: 1.169025
  6900/100000: episode: 69, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.230, mean reward: 0.592 [0.509, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.831, 10.098], loss: 0.002706, mae: 0.053616, mean_q: 1.171835
  7000/100000: episode: 70, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.513, mean reward: 0.575 [0.507, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.302, 10.279], loss: 0.003246, mae: 0.057247, mean_q: 1.172302
  7100/100000: episode: 71, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.009, mean reward: 0.590 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.880, 10.098], loss: 0.002944, mae: 0.053888, mean_q: 1.172675
  7200/100000: episode: 72, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.387, mean reward: 0.594 [0.506, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.893, 10.098], loss: 0.002922, mae: 0.054192, mean_q: 1.170469
  7300/100000: episode: 73, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.877, mean reward: 0.599 [0.504, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.610, 10.338], loss: 0.002702, mae: 0.052347, mean_q: 1.171803
  7400/100000: episode: 74, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.529, mean reward: 0.575 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.040, 10.140], loss: 0.002950, mae: 0.054439, mean_q: 1.175404
  7500/100000: episode: 75, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.718, mean reward: 0.607 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.911, 10.331], loss: 0.002245, mae: 0.049344, mean_q: 1.177029
  7600/100000: episode: 76, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.577, mean reward: 0.596 [0.512, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.593, 10.098], loss: 0.002760, mae: 0.052909, mean_q: 1.172369
  7700/100000: episode: 77, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 60.177, mean reward: 0.602 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.632, 10.098], loss: 0.002543, mae: 0.051986, mean_q: 1.175184
  7800/100000: episode: 78, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 57.263, mean reward: 0.573 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.855, 10.119], loss: 0.002967, mae: 0.054578, mean_q: 1.169375
  7900/100000: episode: 79, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.459, mean reward: 0.595 [0.510, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.048, 10.098], loss: 0.002572, mae: 0.051792, mean_q: 1.170575
  8000/100000: episode: 80, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 61.071, mean reward: 0.611 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.862, 10.098], loss: 0.002737, mae: 0.052699, mean_q: 1.171967
  8100/100000: episode: 81, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.129, mean reward: 0.581 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.780, 10.260], loss: 0.002821, mae: 0.053642, mean_q: 1.171006
  8200/100000: episode: 82, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.614, mean reward: 0.576 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.158, 10.098], loss: 0.002657, mae: 0.052104, mean_q: 1.171959
  8300/100000: episode: 83, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.903, mean reward: 0.609 [0.502, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.768, 10.393], loss: 0.002667, mae: 0.052699, mean_q: 1.169085
  8400/100000: episode: 84, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 62.300, mean reward: 0.623 [0.524, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.935, 10.098], loss: 0.002619, mae: 0.051888, mean_q: 1.170640
  8500/100000: episode: 85, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.384, mean reward: 0.584 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.015, 10.165], loss: 0.002383, mae: 0.050013, mean_q: 1.170562
  8600/100000: episode: 86, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 61.035, mean reward: 0.610 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.524, 10.098], loss: 0.002625, mae: 0.052144, mean_q: 1.168184
  8700/100000: episode: 87, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.420, mean reward: 0.574 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.743, 10.098], loss: 0.002532, mae: 0.051636, mean_q: 1.168968
  8800/100000: episode: 88, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 56.803, mean reward: 0.568 [0.510, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.473, 10.109], loss: 0.002444, mae: 0.051411, mean_q: 1.168958
  8900/100000: episode: 89, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.614, mean reward: 0.576 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.804, 10.260], loss: 0.002129, mae: 0.049072, mean_q: 1.171834
  9000/100000: episode: 90, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.211, mean reward: 0.592 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.401, 10.260], loss: 0.002399, mae: 0.051203, mean_q: 1.172157
  9100/100000: episode: 91, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 61.289, mean reward: 0.613 [0.506, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.928, 10.098], loss: 0.002203, mae: 0.048823, mean_q: 1.170841
  9200/100000: episode: 92, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 60.361, mean reward: 0.604 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.648, 10.415], loss: 0.002273, mae: 0.048983, mean_q: 1.170946
  9300/100000: episode: 93, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.417, mean reward: 0.584 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.072, 10.098], loss: 0.002352, mae: 0.050789, mean_q: 1.170819
  9400/100000: episode: 94, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.663, mean reward: 0.577 [0.508, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.812, 10.320], loss: 0.002737, mae: 0.053960, mean_q: 1.170048
  9500/100000: episode: 95, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 60.468, mean reward: 0.605 [0.514, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.760, 10.260], loss: 0.002231, mae: 0.049907, mean_q: 1.170514
  9600/100000: episode: 96, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.944, mean reward: 0.579 [0.510, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.009, 10.248], loss: 0.002362, mae: 0.050557, mean_q: 1.170622
  9700/100000: episode: 97, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.524, mean reward: 0.585 [0.509, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.799, 10.246], loss: 0.002274, mae: 0.049779, mean_q: 1.166469
  9800/100000: episode: 98, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 59.502, mean reward: 0.595 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.695, 10.306], loss: 0.001956, mae: 0.047140, mean_q: 1.169805
  9900/100000: episode: 99, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 57.618, mean reward: 0.576 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.608, 10.376], loss: 0.002745, mae: 0.053797, mean_q: 1.166111
 10000/100000: episode: 100, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 63.207, mean reward: 0.632 [0.513, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.112, 10.098], loss: 0.002448, mae: 0.051872, mean_q: 1.165312
 10100/100000: episode: 101, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.662, mean reward: 0.587 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.464, 10.098], loss: 0.002160, mae: 0.047958, mean_q: 1.164758
 10200/100000: episode: 102, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.884, mean reward: 0.589 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.931, 10.098], loss: 0.002313, mae: 0.050613, mean_q: 1.169758
 10300/100000: episode: 103, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 60.631, mean reward: 0.606 [0.510, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.626, 10.098], loss: 0.002054, mae: 0.048461, mean_q: 1.171201
 10400/100000: episode: 104, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.218, mean reward: 0.572 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.288, 10.116], loss: 0.002145, mae: 0.049602, mean_q: 1.169030
 10500/100000: episode: 105, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.666, mean reward: 0.587 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.190, 10.098], loss: 0.002024, mae: 0.047324, mean_q: 1.169149
 10600/100000: episode: 106, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.492, mean reward: 0.605 [0.509, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.264, 10.113], loss: 0.002044, mae: 0.047397, mean_q: 1.168725
 10700/100000: episode: 107, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.985, mean reward: 0.590 [0.502, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.395, 10.195], loss: 0.002066, mae: 0.047945, mean_q: 1.169136
 10800/100000: episode: 108, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.317, mean reward: 0.573 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.042, 10.116], loss: 0.002503, mae: 0.052522, mean_q: 1.168409
 10900/100000: episode: 109, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.565, mean reward: 0.576 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.521, 10.098], loss: 0.002007, mae: 0.047967, mean_q: 1.169703
 11000/100000: episode: 110, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.852, mean reward: 0.589 [0.504, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.529, 10.222], loss: 0.002038, mae: 0.048188, mean_q: 1.170998
 11100/100000: episode: 111, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.673, mean reward: 0.597 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.659, 10.143], loss: 0.001929, mae: 0.047132, mean_q: 1.169883
 11200/100000: episode: 112, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.479, mean reward: 0.585 [0.499, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.792, 10.098], loss: 0.001868, mae: 0.046906, mean_q: 1.169645
 11300/100000: episode: 113, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.801, mean reward: 0.598 [0.513, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.013, 10.137], loss: 0.002101, mae: 0.049657, mean_q: 1.165994
 11400/100000: episode: 114, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 58.321, mean reward: 0.583 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.733, 10.098], loss: 0.001853, mae: 0.046888, mean_q: 1.169176
 11500/100000: episode: 115, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.556, mean reward: 0.596 [0.509, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.674, 10.098], loss: 0.001921, mae: 0.047437, mean_q: 1.169183
 11600/100000: episode: 116, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.922, mean reward: 0.589 [0.498, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.423, 10.126], loss: 0.001825, mae: 0.046329, mean_q: 1.167084
 11700/100000: episode: 117, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 62.754, mean reward: 0.628 [0.499, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.915, 10.437], loss: 0.001913, mae: 0.047292, mean_q: 1.168956
 11800/100000: episode: 118, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 61.006, mean reward: 0.610 [0.510, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.362, 10.278], loss: 0.002194, mae: 0.050010, mean_q: 1.170230
 11900/100000: episode: 119, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.029, mean reward: 0.590 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.782, 10.098], loss: 0.002175, mae: 0.050895, mean_q: 1.169462
 12000/100000: episode: 120, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 64.238, mean reward: 0.642 [0.499, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.972, 10.343], loss: 0.001970, mae: 0.048428, mean_q: 1.172718
 12100/100000: episode: 121, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 59.403, mean reward: 0.594 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.939, 10.339], loss: 0.001878, mae: 0.047286, mean_q: 1.169949
 12200/100000: episode: 122, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 56.973, mean reward: 0.570 [0.498, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.591, 10.179], loss: 0.002117, mae: 0.049755, mean_q: 1.173354
 12300/100000: episode: 123, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.793, mean reward: 0.568 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.611, 10.114], loss: 0.001983, mae: 0.049025, mean_q: 1.175435
 12400/100000: episode: 124, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.821, mean reward: 0.588 [0.505, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.935, 10.139], loss: 0.001898, mae: 0.047935, mean_q: 1.173098
 12500/100000: episode: 125, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.956, mean reward: 0.590 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.678, 10.156], loss: 0.002011, mae: 0.048432, mean_q: 1.172976
 12600/100000: episode: 126, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.265, mean reward: 0.573 [0.509, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.751, 10.098], loss: 0.002154, mae: 0.050488, mean_q: 1.172534
 12700/100000: episode: 127, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.802, mean reward: 0.598 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.558, 10.098], loss: 0.001965, mae: 0.048443, mean_q: 1.169547
 12800/100000: episode: 128, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.976, mean reward: 0.580 [0.500, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.554, 10.123], loss: 0.001784, mae: 0.046085, mean_q: 1.168752
 12900/100000: episode: 129, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.424, mean reward: 0.594 [0.507, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.473, 10.192], loss: 0.001868, mae: 0.047228, mean_q: 1.171553
 13000/100000: episode: 130, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 59.685, mean reward: 0.597 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.597, 10.098], loss: 0.001966, mae: 0.048487, mean_q: 1.170353
 13100/100000: episode: 131, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 59.842, mean reward: 0.598 [0.508, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.881, 10.098], loss: 0.002097, mae: 0.049915, mean_q: 1.172465
 13200/100000: episode: 132, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.897, mean reward: 0.589 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.506, 10.124], loss: 0.001821, mae: 0.046623, mean_q: 1.170352
 13300/100000: episode: 133, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 61.074, mean reward: 0.611 [0.517, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.380, 10.098], loss: 0.001948, mae: 0.048474, mean_q: 1.172714
 13400/100000: episode: 134, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.096, mean reward: 0.591 [0.506, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.919, 10.161], loss: 0.001806, mae: 0.046771, mean_q: 1.172307
 13500/100000: episode: 135, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 56.676, mean reward: 0.567 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.641, 10.098], loss: 0.001960, mae: 0.047932, mean_q: 1.169854
 13600/100000: episode: 136, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 55.890, mean reward: 0.559 [0.504, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.404, 10.218], loss: 0.001905, mae: 0.047466, mean_q: 1.169185
 13700/100000: episode: 137, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.521, mean reward: 0.585 [0.498, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.788, 10.154], loss: 0.001811, mae: 0.046305, mean_q: 1.167984
 13800/100000: episode: 138, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 56.285, mean reward: 0.563 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.137, 10.098], loss: 0.002011, mae: 0.048434, mean_q: 1.168071
 13900/100000: episode: 139, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.490, mean reward: 0.595 [0.514, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.725, 10.318], loss: 0.001773, mae: 0.045976, mean_q: 1.168924
 14000/100000: episode: 140, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.657, mean reward: 0.587 [0.517, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.930, 10.098], loss: 0.001923, mae: 0.048015, mean_q: 1.169722
 14100/100000: episode: 141, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.035, mean reward: 0.580 [0.497, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.116, 10.147], loss: 0.001808, mae: 0.046658, mean_q: 1.166328
 14200/100000: episode: 142, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.076, mean reward: 0.571 [0.510, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.136, 10.260], loss: 0.001795, mae: 0.046487, mean_q: 1.163637
 14300/100000: episode: 143, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.942, mean reward: 0.579 [0.505, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.705, 10.215], loss: 0.001693, mae: 0.045387, mean_q: 1.167402
 14400/100000: episode: 144, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.858, mean reward: 0.599 [0.506, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.997, 10.198], loss: 0.002313, mae: 0.052451, mean_q: 1.166929
 14500/100000: episode: 145, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.853, mean reward: 0.599 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.861, 10.231], loss: 0.001731, mae: 0.045358, mean_q: 1.163774
 14600/100000: episode: 146, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 59.428, mean reward: 0.594 [0.508, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.962, 10.098], loss: 0.001871, mae: 0.047745, mean_q: 1.166806
 14700/100000: episode: 147, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 61.128, mean reward: 0.611 [0.521, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.728, 10.098], loss: 0.001927, mae: 0.047969, mean_q: 1.165210
 14800/100000: episode: 148, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.937, mean reward: 0.579 [0.509, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.647, 10.299], loss: 0.001803, mae: 0.046831, mean_q: 1.167280
 14900/100000: episode: 149, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.757, mean reward: 0.578 [0.501, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.686, 10.098], loss: 0.001923, mae: 0.047492, mean_q: 1.166665
[Info] 1-TH LEVEL FOUND: 1.2743631601333618, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.155s, episode steps: 100, steps per second: 19, episode reward: 59.531, mean reward: 0.595 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.786, 10.211], loss: 0.001790, mae: 0.046385, mean_q: 1.164883
 15044/100000: episode: 151, duration: 0.278s, episode steps: 44, steps per second: 158, episode reward: 31.839, mean reward: 0.724 [0.658, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.247, 10.431], loss: 0.001713, mae: 0.046107, mean_q: 1.167568
 15072/100000: episode: 152, duration: 0.170s, episode steps: 28, steps per second: 164, episode reward: 17.205, mean reward: 0.614 [0.514, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.139, 10.182], loss: 0.001682, mae: 0.044982, mean_q: 1.168594
 15097/100000: episode: 153, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 16.487, mean reward: 0.659 [0.581, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.198, 10.236], loss: 0.001724, mae: 0.045360, mean_q: 1.165809
 15125/100000: episode: 154, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 18.178, mean reward: 0.649 [0.551, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.218, 10.100], loss: 0.001884, mae: 0.046969, mean_q: 1.166917
 15150/100000: episode: 155, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 15.813, mean reward: 0.633 [0.552, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.446, 10.182], loss: 0.002165, mae: 0.050737, mean_q: 1.173385
 15176/100000: episode: 156, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 18.747, mean reward: 0.721 [0.666, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.859, 10.438], loss: 0.002189, mae: 0.050369, mean_q: 1.159859
 15203/100000: episode: 157, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 17.799, mean reward: 0.659 [0.565, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.035, 10.340], loss: 0.002163, mae: 0.050046, mean_q: 1.174822
 15230/100000: episode: 158, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 15.907, mean reward: 0.589 [0.518, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.533, 10.165], loss: 0.002014, mae: 0.049695, mean_q: 1.170247
 15256/100000: episode: 159, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 16.860, mean reward: 0.648 [0.572, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.343, 10.407], loss: 0.001798, mae: 0.046813, mean_q: 1.170230
 15283/100000: episode: 160, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 19.138, mean reward: 0.709 [0.607, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.146, 10.346], loss: 0.001799, mae: 0.045994, mean_q: 1.172842
 15327/100000: episode: 161, duration: 0.241s, episode steps: 44, steps per second: 183, episode reward: 29.757, mean reward: 0.676 [0.559, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.431, 10.450], loss: 0.001804, mae: 0.046541, mean_q: 1.167857
 15355/100000: episode: 162, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 19.368, mean reward: 0.692 [0.641, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.604, 10.389], loss: 0.001895, mae: 0.048248, mean_q: 1.177236
 15383/100000: episode: 163, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 17.979, mean reward: 0.642 [0.587, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.109, 10.301], loss: 0.001896, mae: 0.048115, mean_q: 1.179896
 15410/100000: episode: 164, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 19.544, mean reward: 0.724 [0.625, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.634, 10.453], loss: 0.001848, mae: 0.047046, mean_q: 1.173210
 15444/100000: episode: 165, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 21.515, mean reward: 0.633 [0.562, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.348, 10.244], loss: 0.002156, mae: 0.049838, mean_q: 1.175063
 15470/100000: episode: 166, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 16.299, mean reward: 0.627 [0.560, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.149, 10.288], loss: 0.002110, mae: 0.049541, mean_q: 1.175423
 15498/100000: episode: 167, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 19.916, mean reward: 0.711 [0.626, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.358, 10.382], loss: 0.002380, mae: 0.053261, mean_q: 1.170538
 15519/100000: episode: 168, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 14.330, mean reward: 0.682 [0.620, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.828, 10.418], loss: 0.002408, mae: 0.052389, mean_q: 1.172844
 15563/100000: episode: 169, duration: 0.242s, episode steps: 44, steps per second: 182, episode reward: 28.640, mean reward: 0.651 [0.591, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.435, 10.337], loss: 0.002153, mae: 0.049508, mean_q: 1.178225
 15589/100000: episode: 170, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 14.903, mean reward: 0.573 [0.502, 0.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.322, 10.118], loss: 0.001885, mae: 0.047788, mean_q: 1.181830
 15616/100000: episode: 171, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 15.665, mean reward: 0.580 [0.529, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.637, 10.100], loss: 0.001852, mae: 0.047774, mean_q: 1.180388
 15642/100000: episode: 172, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 16.602, mean reward: 0.639 [0.581, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.595, 10.358], loss: 0.001844, mae: 0.046773, mean_q: 1.183085
 15667/100000: episode: 173, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 16.123, mean reward: 0.645 [0.551, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.189, 10.165], loss: 0.002168, mae: 0.051748, mean_q: 1.180333
 15693/100000: episode: 174, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 15.749, mean reward: 0.606 [0.508, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.635, 10.155], loss: 0.002304, mae: 0.051524, mean_q: 1.182342
 15714/100000: episode: 175, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 13.553, mean reward: 0.645 [0.556, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.079, 10.378], loss: 0.001997, mae: 0.048287, mean_q: 1.182208
 15742/100000: episode: 176, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 17.808, mean reward: 0.636 [0.514, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.390, 10.154], loss: 0.002167, mae: 0.048799, mean_q: 1.174158
 15768/100000: episode: 177, duration: 0.160s, episode steps: 26, steps per second: 163, episode reward: 17.088, mean reward: 0.657 [0.543, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.110, 10.343], loss: 0.002230, mae: 0.050497, mean_q: 1.180231
 15802/100000: episode: 178, duration: 0.204s, episode steps: 34, steps per second: 167, episode reward: 22.222, mean reward: 0.654 [0.557, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.170, 10.317], loss: 0.002326, mae: 0.052716, mean_q: 1.182888
 15828/100000: episode: 179, duration: 0.201s, episode steps: 26, steps per second: 129, episode reward: 17.210, mean reward: 0.662 [0.537, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.902, 10.296], loss: 0.001980, mae: 0.047358, mean_q: 1.175853
 15855/100000: episode: 180, duration: 0.163s, episode steps: 27, steps per second: 166, episode reward: 16.616, mean reward: 0.615 [0.579, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.296], loss: 0.001945, mae: 0.048032, mean_q: 1.185451
 15880/100000: episode: 181, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 16.476, mean reward: 0.659 [0.558, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.356, 10.276], loss: 0.002540, mae: 0.052988, mean_q: 1.179113
 15908/100000: episode: 182, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 19.097, mean reward: 0.682 [0.586, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.046, 10.513], loss: 0.002439, mae: 0.053025, mean_q: 1.177106
 15935/100000: episode: 183, duration: 0.166s, episode steps: 27, steps per second: 163, episode reward: 16.173, mean reward: 0.599 [0.516, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.378, 10.100], loss: 0.002149, mae: 0.049461, mean_q: 1.181293
 15963/100000: episode: 184, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 17.877, mean reward: 0.638 [0.515, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.249, 10.100], loss: 0.001821, mae: 0.046259, mean_q: 1.176168
 15984/100000: episode: 185, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 14.150, mean reward: 0.674 [0.572, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.095, 10.245], loss: 0.001899, mae: 0.046412, mean_q: 1.178066
 16028/100000: episode: 186, duration: 0.248s, episode steps: 44, steps per second: 178, episode reward: 27.567, mean reward: 0.627 [0.550, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.132, 10.288], loss: 0.001859, mae: 0.047813, mean_q: 1.180740
 16062/100000: episode: 187, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 22.684, mean reward: 0.667 [0.529, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.449, 10.275], loss: 0.001920, mae: 0.047462, mean_q: 1.183747
 16088/100000: episode: 188, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 16.697, mean reward: 0.642 [0.597, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.394], loss: 0.001906, mae: 0.048365, mean_q: 1.192387
 16116/100000: episode: 189, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 19.152, mean reward: 0.684 [0.620, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.293, 10.300], loss: 0.002166, mae: 0.050932, mean_q: 1.183726
 16142/100000: episode: 190, duration: 0.152s, episode steps: 26, steps per second: 172, episode reward: 15.615, mean reward: 0.601 [0.535, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.195, 10.100], loss: 0.002008, mae: 0.048504, mean_q: 1.183827
 16169/100000: episode: 191, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 17.533, mean reward: 0.649 [0.538, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.252, 10.100], loss: 0.001680, mae: 0.044382, mean_q: 1.183183
 16195/100000: episode: 192, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 17.282, mean reward: 0.665 [0.607, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.825, 10.292], loss: 0.002053, mae: 0.048685, mean_q: 1.193159
 16220/100000: episode: 193, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 16.541, mean reward: 0.662 [0.597, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.426, 10.299], loss: 0.002184, mae: 0.051144, mean_q: 1.182725
 16264/100000: episode: 194, duration: 0.250s, episode steps: 44, steps per second: 176, episode reward: 27.351, mean reward: 0.622 [0.547, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.360, 10.323], loss: 0.002051, mae: 0.049591, mean_q: 1.185338
 16298/100000: episode: 195, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 22.666, mean reward: 0.667 [0.602, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.389, 10.326], loss: 0.002162, mae: 0.050829, mean_q: 1.185329
 16326/100000: episode: 196, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 17.702, mean reward: 0.632 [0.552, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.416, 10.306], loss: 0.001911, mae: 0.048410, mean_q: 1.193589
 16353/100000: episode: 197, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 18.867, mean reward: 0.699 [0.646, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.821, 10.474], loss: 0.001900, mae: 0.047177, mean_q: 1.190603
 16374/100000: episode: 198, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 15.263, mean reward: 0.727 [0.644, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.400, 10.516], loss: 0.001956, mae: 0.047083, mean_q: 1.182075
 16401/100000: episode: 199, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 18.948, mean reward: 0.702 [0.637, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.410], loss: 0.001892, mae: 0.048337, mean_q: 1.189059
 16435/100000: episode: 200, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 23.192, mean reward: 0.682 [0.585, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.518, 10.444], loss: 0.001990, mae: 0.048428, mean_q: 1.191693
 16462/100000: episode: 201, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 17.233, mean reward: 0.638 [0.527, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.674, 10.100], loss: 0.001938, mae: 0.046613, mean_q: 1.199843
 16489/100000: episode: 202, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 17.882, mean reward: 0.662 [0.552, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.313], loss: 0.002182, mae: 0.049296, mean_q: 1.192244
 16517/100000: episode: 203, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 17.168, mean reward: 0.613 [0.559, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.581, 10.262], loss: 0.001892, mae: 0.046242, mean_q: 1.188550
 16545/100000: episode: 204, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 18.833, mean reward: 0.673 [0.601, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.338, 10.390], loss: 0.002059, mae: 0.049125, mean_q: 1.188759
 16579/100000: episode: 205, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 24.536, mean reward: 0.722 [0.553, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.268], loss: 0.002113, mae: 0.049827, mean_q: 1.197128
 16623/100000: episode: 206, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 28.028, mean reward: 0.637 [0.513, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.209, 10.174], loss: 0.002529, mae: 0.054056, mean_q: 1.195155
 16649/100000: episode: 207, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 16.373, mean reward: 0.630 [0.524, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.535, 10.100], loss: 0.002589, mae: 0.053894, mean_q: 1.195346
 16676/100000: episode: 208, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 16.846, mean reward: 0.624 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.089, 10.150], loss: 0.003016, mae: 0.060437, mean_q: 1.194961
 16704/100000: episode: 209, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 17.881, mean reward: 0.639 [0.577, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.283], loss: 0.002041, mae: 0.049673, mean_q: 1.203223
 16738/100000: episode: 210, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 20.524, mean reward: 0.604 [0.513, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.552, 10.269], loss: 0.001741, mae: 0.044865, mean_q: 1.199783
 16782/100000: episode: 211, duration: 0.236s, episode steps: 44, steps per second: 186, episode reward: 28.201, mean reward: 0.641 [0.513, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.435, 10.100], loss: 0.002366, mae: 0.051159, mean_q: 1.189243
 16809/100000: episode: 212, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 17.717, mean reward: 0.656 [0.564, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.539, 10.474], loss: 0.002117, mae: 0.049244, mean_q: 1.196137
 16837/100000: episode: 213, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 18.646, mean reward: 0.666 [0.525, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.533, 10.170], loss: 0.001970, mae: 0.047553, mean_q: 1.198869
 16865/100000: episode: 214, duration: 0.166s, episode steps: 28, steps per second: 169, episode reward: 17.798, mean reward: 0.636 [0.515, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.415, 10.119], loss: 0.002122, mae: 0.048277, mean_q: 1.204153
 16899/100000: episode: 215, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 19.805, mean reward: 0.583 [0.521, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.418, 10.178], loss: 0.001779, mae: 0.046405, mean_q: 1.198143
 16925/100000: episode: 216, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 16.643, mean reward: 0.640 [0.546, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.035, 10.271], loss: 0.002287, mae: 0.051362, mean_q: 1.196896
 16946/100000: episode: 217, duration: 0.131s, episode steps: 21, steps per second: 160, episode reward: 14.322, mean reward: 0.682 [0.615, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.072, 10.451], loss: 0.002050, mae: 0.047993, mean_q: 1.202309
 16974/100000: episode: 218, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 17.982, mean reward: 0.642 [0.528, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.046, 10.277], loss: 0.002453, mae: 0.052896, mean_q: 1.191475
 17008/100000: episode: 219, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 22.362, mean reward: 0.658 [0.522, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.303, 10.212], loss: 0.002001, mae: 0.047727, mean_q: 1.196361
 17036/100000: episode: 220, duration: 0.153s, episode steps: 28, steps per second: 182, episode reward: 17.387, mean reward: 0.621 [0.547, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.264, 10.204], loss: 0.001996, mae: 0.047532, mean_q: 1.195842
 17061/100000: episode: 221, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 16.126, mean reward: 0.645 [0.563, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.344, 10.248], loss: 0.002015, mae: 0.049273, mean_q: 1.190939
 17082/100000: episode: 222, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 14.047, mean reward: 0.669 [0.621, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.053, 10.352], loss: 0.001806, mae: 0.045187, mean_q: 1.201904
 17108/100000: episode: 223, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 17.549, mean reward: 0.675 [0.613, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.249, 10.416], loss: 0.001914, mae: 0.045953, mean_q: 1.203210
 17142/100000: episode: 224, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 22.718, mean reward: 0.668 [0.604, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.700, 10.331], loss: 0.002192, mae: 0.049764, mean_q: 1.192760
 17167/100000: episode: 225, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 16.226, mean reward: 0.649 [0.586, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.632, 10.429], loss: 0.002316, mae: 0.050867, mean_q: 1.195528
 17192/100000: episode: 226, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 17.076, mean reward: 0.683 [0.597, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.328, 10.387], loss: 0.001993, mae: 0.048497, mean_q: 1.203618
 17220/100000: episode: 227, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 19.563, mean reward: 0.699 [0.544, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.567, 10.306], loss: 0.002064, mae: 0.048844, mean_q: 1.199546
 17247/100000: episode: 228, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 16.649, mean reward: 0.617 [0.522, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.191, 10.207], loss: 0.002003, mae: 0.047784, mean_q: 1.209849
 17291/100000: episode: 229, duration: 0.242s, episode steps: 44, steps per second: 182, episode reward: 29.272, mean reward: 0.665 [0.602, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.777, 10.379], loss: 0.001854, mae: 0.045376, mean_q: 1.201958
 17325/100000: episode: 230, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 22.269, mean reward: 0.655 [0.520, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.419], loss: 0.002331, mae: 0.052979, mean_q: 1.205231
 17351/100000: episode: 231, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 16.517, mean reward: 0.635 [0.541, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.513, 10.235], loss: 0.002126, mae: 0.049543, mean_q: 1.202313
 17395/100000: episode: 232, duration: 0.245s, episode steps: 44, steps per second: 180, episode reward: 28.984, mean reward: 0.659 [0.511, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.232, 10.301], loss: 0.001923, mae: 0.047097, mean_q: 1.212609
 17439/100000: episode: 233, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 28.309, mean reward: 0.643 [0.529, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.315, 10.167], loss: 0.002162, mae: 0.050550, mean_q: 1.207490
 17466/100000: episode: 234, duration: 0.158s, episode steps: 27, steps per second: 170, episode reward: 17.364, mean reward: 0.643 [0.584, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.346, 10.286], loss: 0.002243, mae: 0.050905, mean_q: 1.211044
 17494/100000: episode: 235, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 18.962, mean reward: 0.677 [0.613, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.678, 10.382], loss: 0.002264, mae: 0.050378, mean_q: 1.215274
 17519/100000: episode: 236, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 15.511, mean reward: 0.620 [0.542, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.179], loss: 0.002099, mae: 0.048799, mean_q: 1.211469
 17547/100000: episode: 237, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 16.804, mean reward: 0.600 [0.508, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.040, 10.152], loss: 0.002022, mae: 0.048245, mean_q: 1.217734
 17573/100000: episode: 238, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 15.527, mean reward: 0.597 [0.539, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.441, 10.203], loss: 0.002247, mae: 0.051733, mean_q: 1.212355
 17601/100000: episode: 239, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 18.839, mean reward: 0.673 [0.587, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.415, 10.469], loss: 0.001872, mae: 0.046506, mean_q: 1.211703
[Info] 2-TH LEVEL FOUND: 1.4194135665893555, Considering 10/90 traces
 17626/100000: episode: 240, duration: 17.485s, episode steps: 25, steps per second: 1, episode reward: 17.003, mean reward: 0.680 [0.640, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.180, 10.303], loss: 0.001979, mae: 0.047700, mean_q: 1.217746
 17644/100000: episode: 241, duration: 0.141s, episode steps: 18, steps per second: 128, episode reward: 12.406, mean reward: 0.689 [0.656, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.200, 10.435], loss: 0.001880, mae: 0.046457, mean_q: 1.197160
 17673/100000: episode: 242, duration: 0.180s, episode steps: 29, steps per second: 161, episode reward: 18.959, mean reward: 0.654 [0.599, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.274, 10.254], loss: 0.001739, mae: 0.043980, mean_q: 1.212923
 17702/100000: episode: 243, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 24.434, mean reward: 0.843 [0.698, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.841, 10.552], loss: 0.001868, mae: 0.047444, mean_q: 1.211545
 17732/100000: episode: 244, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 21.190, mean reward: 0.706 [0.569, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.541, 10.410], loss: 0.001771, mae: 0.043779, mean_q: 1.218156
 17767/100000: episode: 245, duration: 0.204s, episode steps: 35, steps per second: 172, episode reward: 23.413, mean reward: 0.669 [0.557, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.048, 10.355], loss: 0.001803, mae: 0.044855, mean_q: 1.221557
 17797/100000: episode: 246, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 19.131, mean reward: 0.638 [0.536, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.393, 10.176], loss: 0.001866, mae: 0.045816, mean_q: 1.223247
 17830/100000: episode: 247, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 22.004, mean reward: 0.667 [0.573, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.047, 10.382], loss: 0.001987, mae: 0.047809, mean_q: 1.213948
 17859/100000: episode: 248, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 22.369, mean reward: 0.771 [0.667, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.144, 10.404], loss: 0.001742, mae: 0.045229, mean_q: 1.221470
 17892/100000: episode: 249, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 20.412, mean reward: 0.619 [0.524, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.771, 10.100], loss: 0.002290, mae: 0.051630, mean_q: 1.215154
 17922/100000: episode: 250, duration: 0.173s, episode steps: 30, steps per second: 174, episode reward: 19.743, mean reward: 0.658 [0.551, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.305], loss: 0.001568, mae: 0.043805, mean_q: 1.216235
 17951/100000: episode: 251, duration: 0.200s, episode steps: 29, steps per second: 145, episode reward: 20.168, mean reward: 0.695 [0.649, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.339, 10.414], loss: 0.001941, mae: 0.046721, mean_q: 1.225765
 17984/100000: episode: 252, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 21.811, mean reward: 0.661 [0.543, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.423, 10.599], loss: 0.002139, mae: 0.049217, mean_q: 1.220342
 18002/100000: episode: 253, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 12.411, mean reward: 0.690 [0.646, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.753, 10.480], loss: 0.002046, mae: 0.047923, mean_q: 1.207172
 18035/100000: episode: 254, duration: 0.193s, episode steps: 33, steps per second: 171, episode reward: 21.480, mean reward: 0.651 [0.558, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.086, 10.269], loss: 0.001990, mae: 0.048053, mean_q: 1.218187
 18053/100000: episode: 255, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 15.108, mean reward: 0.839 [0.767, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.640], loss: 0.001759, mae: 0.045106, mean_q: 1.215773
 18083/100000: episode: 256, duration: 0.176s, episode steps: 30, steps per second: 171, episode reward: 19.765, mean reward: 0.659 [0.546, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.459, 10.277], loss: 0.001714, mae: 0.044116, mean_q: 1.223689
 18113/100000: episode: 257, duration: 0.174s, episode steps: 30, steps per second: 172, episode reward: 19.489, mean reward: 0.650 [0.522, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.894, 10.300], loss: 0.001981, mae: 0.048341, mean_q: 1.230597
 18142/100000: episode: 258, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 19.383, mean reward: 0.668 [0.600, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.810, 10.422], loss: 0.002007, mae: 0.047115, mean_q: 1.229898
 18174/100000: episode: 259, duration: 0.173s, episode steps: 32, steps per second: 184, episode reward: 21.101, mean reward: 0.659 [0.573, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.524, 10.263], loss: 0.001949, mae: 0.047385, mean_q: 1.229748
 18203/100000: episode: 260, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 19.281, mean reward: 0.665 [0.518, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.845, 10.248], loss: 0.001568, mae: 0.042957, mean_q: 1.222121
 18221/100000: episode: 261, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 12.783, mean reward: 0.710 [0.637, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.730, 10.365], loss: 0.002146, mae: 0.050329, mean_q: 1.230708
 18239/100000: episode: 262, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 13.058, mean reward: 0.725 [0.632, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.114, 10.382], loss: 0.001744, mae: 0.045897, mean_q: 1.230437
 18269/100000: episode: 263, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 18.863, mean reward: 0.629 [0.550, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.250], loss: 0.001737, mae: 0.044663, mean_q: 1.239827
 18302/100000: episode: 264, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 21.083, mean reward: 0.639 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.796, 10.146], loss: 0.001582, mae: 0.042191, mean_q: 1.230707
 18320/100000: episode: 265, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 14.765, mean reward: 0.820 [0.739, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.983, 10.598], loss: 0.002266, mae: 0.051623, mean_q: 1.238120
 18350/100000: episode: 266, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 17.525, mean reward: 0.584 [0.526, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.050, 10.100], loss: 0.002163, mae: 0.050247, mean_q: 1.235121
 18380/100000: episode: 267, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 19.355, mean reward: 0.645 [0.576, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.690, 10.314], loss: 0.001864, mae: 0.046809, mean_q: 1.226685
 18412/100000: episode: 268, duration: 0.188s, episode steps: 32, steps per second: 170, episode reward: 21.141, mean reward: 0.661 [0.533, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-1.052, 10.220], loss: 0.001853, mae: 0.045234, mean_q: 1.233397
 18442/100000: episode: 269, duration: 0.167s, episode steps: 30, steps per second: 179, episode reward: 20.370, mean reward: 0.679 [0.528, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.035, 10.119], loss: 0.002491, mae: 0.054142, mean_q: 1.235388
 18471/100000: episode: 270, duration: 0.164s, episode steps: 29, steps per second: 176, episode reward: 19.346, mean reward: 0.667 [0.590, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.496, 10.466], loss: 0.002171, mae: 0.049710, mean_q: 1.244691
 18489/100000: episode: 271, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 13.410, mean reward: 0.745 [0.695, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.438, 10.499], loss: 0.001805, mae: 0.045187, mean_q: 1.237091
 18519/100000: episode: 272, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 22.820, mean reward: 0.761 [0.695, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.462, 10.544], loss: 0.002309, mae: 0.051157, mean_q: 1.244633
 18548/100000: episode: 273, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 22.003, mean reward: 0.759 [0.664, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-2.683, 10.476], loss: 0.001973, mae: 0.047857, mean_q: 1.240261
 18578/100000: episode: 274, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 19.853, mean reward: 0.662 [0.580, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.169, 10.306], loss: 0.001652, mae: 0.043594, mean_q: 1.248302
 18613/100000: episode: 275, duration: 0.205s, episode steps: 35, steps per second: 170, episode reward: 23.366, mean reward: 0.668 [0.572, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.461, 10.276], loss: 0.001588, mae: 0.042235, mean_q: 1.241067
 18648/100000: episode: 276, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 23.753, mean reward: 0.679 [0.550, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.381, 10.256], loss: 0.001886, mae: 0.046170, mean_q: 1.240259
 18678/100000: episode: 277, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 22.836, mean reward: 0.761 [0.702, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.316, 10.530], loss: 0.001683, mae: 0.043734, mean_q: 1.246591
 18707/100000: episode: 278, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 21.039, mean reward: 0.725 [0.678, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.436, 10.475], loss: 0.001973, mae: 0.047218, mean_q: 1.243522
 18739/100000: episode: 279, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 20.405, mean reward: 0.638 [0.519, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.568, 10.165], loss: 0.001909, mae: 0.046984, mean_q: 1.249981
 18769/100000: episode: 280, duration: 0.176s, episode steps: 30, steps per second: 171, episode reward: 24.072, mean reward: 0.802 [0.714, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.451], loss: 0.001572, mae: 0.044008, mean_q: 1.252245
 18802/100000: episode: 281, duration: 0.187s, episode steps: 33, steps per second: 177, episode reward: 21.986, mean reward: 0.666 [0.589, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.511, 10.350], loss: 0.001848, mae: 0.045710, mean_q: 1.247823
 18820/100000: episode: 282, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 13.718, mean reward: 0.762 [0.675, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.278, 10.496], loss: 0.001517, mae: 0.042509, mean_q: 1.262018
 18852/100000: episode: 283, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 20.826, mean reward: 0.651 [0.547, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.124, 10.215], loss: 0.001804, mae: 0.044466, mean_q: 1.250428
 18884/100000: episode: 284, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 21.986, mean reward: 0.687 [0.604, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.309, 10.325], loss: 0.001644, mae: 0.043134, mean_q: 1.259568
 18914/100000: episode: 285, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 21.889, mean reward: 0.730 [0.628, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.130, 10.507], loss: 0.001859, mae: 0.045078, mean_q: 1.251209
 18932/100000: episode: 286, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 14.542, mean reward: 0.808 [0.722, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.210, 10.514], loss: 0.001703, mae: 0.045223, mean_q: 1.264056
 18950/100000: episode: 287, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 13.981, mean reward: 0.777 [0.687, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.246, 10.433], loss: 0.001667, mae: 0.043459, mean_q: 1.254518
 18980/100000: episode: 288, duration: 0.164s, episode steps: 30, steps per second: 182, episode reward: 22.339, mean reward: 0.745 [0.654, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.217, 10.462], loss: 0.001757, mae: 0.045116, mean_q: 1.252334
 19010/100000: episode: 289, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 21.670, mean reward: 0.722 [0.653, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.649, 10.469], loss: 0.001947, mae: 0.046491, mean_q: 1.250495
 19039/100000: episode: 290, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 21.476, mean reward: 0.741 [0.634, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.553], loss: 0.001575, mae: 0.043531, mean_q: 1.257967
 19057/100000: episode: 291, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 12.923, mean reward: 0.718 [0.623, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.370], loss: 0.002502, mae: 0.054480, mean_q: 1.267191
 19087/100000: episode: 292, duration: 0.176s, episode steps: 30, steps per second: 170, episode reward: 17.927, mean reward: 0.598 [0.515, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.346, 10.112], loss: 0.002098, mae: 0.049732, mean_q: 1.266837
 19116/100000: episode: 293, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 19.688, mean reward: 0.679 [0.515, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.356, 10.261], loss: 0.001927, mae: 0.048133, mean_q: 1.259295
 19146/100000: episode: 294, duration: 0.182s, episode steps: 30, steps per second: 165, episode reward: 21.234, mean reward: 0.708 [0.634, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.118, 10.478], loss: 0.002027, mae: 0.047239, mean_q: 1.264699
 19164/100000: episode: 295, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 13.535, mean reward: 0.752 [0.703, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.092, 10.507], loss: 0.001611, mae: 0.044028, mean_q: 1.264442
 19182/100000: episode: 296, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 13.888, mean reward: 0.772 [0.703, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.282, 10.385], loss: 0.001665, mae: 0.044181, mean_q: 1.263307
 19214/100000: episode: 297, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 21.090, mean reward: 0.659 [0.545, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.177], loss: 0.002195, mae: 0.052107, mean_q: 1.268815
 19244/100000: episode: 298, duration: 0.173s, episode steps: 30, steps per second: 174, episode reward: 19.634, mean reward: 0.654 [0.559, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.020, 10.204], loss: 0.001959, mae: 0.046834, mean_q: 1.266481
 19279/100000: episode: 299, duration: 0.202s, episode steps: 35, steps per second: 173, episode reward: 23.633, mean reward: 0.675 [0.577, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.147, 10.302], loss: 0.001924, mae: 0.048753, mean_q: 1.280282
 19311/100000: episode: 300, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 22.086, mean reward: 0.690 [0.588, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.741, 10.282], loss: 0.001561, mae: 0.041752, mean_q: 1.276293
 19346/100000: episode: 301, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 22.752, mean reward: 0.650 [0.550, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.467, 10.421], loss: 0.001709, mae: 0.044374, mean_q: 1.277802
 19364/100000: episode: 302, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 12.286, mean reward: 0.683 [0.609, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.909, 10.351], loss: 0.001495, mae: 0.041424, mean_q: 1.273092
 19393/100000: episode: 303, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 20.558, mean reward: 0.709 [0.633, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.381], loss: 0.001585, mae: 0.042931, mean_q: 1.270152
 19411/100000: episode: 304, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 12.941, mean reward: 0.719 [0.643, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.488, 10.543], loss: 0.001387, mae: 0.040086, mean_q: 1.276793
 19441/100000: episode: 305, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 21.926, mean reward: 0.731 [0.572, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.333, 10.362], loss: 0.001918, mae: 0.046168, mean_q: 1.258128
 19471/100000: episode: 306, duration: 0.186s, episode steps: 30, steps per second: 162, episode reward: 20.998, mean reward: 0.700 [0.587, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.177, 10.356], loss: 0.002125, mae: 0.049495, mean_q: 1.275162
 19500/100000: episode: 307, duration: 0.170s, episode steps: 29, steps per second: 171, episode reward: 21.622, mean reward: 0.746 [0.659, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.512, 10.399], loss: 0.002084, mae: 0.049509, mean_q: 1.278034
 19518/100000: episode: 308, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 12.455, mean reward: 0.692 [0.642, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.426], loss: 0.002275, mae: 0.053346, mean_q: 1.292050
 19536/100000: episode: 309, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 12.064, mean reward: 0.670 [0.637, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.344], loss: 0.002462, mae: 0.053744, mean_q: 1.270277
 19554/100000: episode: 310, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 13.456, mean reward: 0.748 [0.680, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.439, 10.437], loss: 0.001423, mae: 0.040872, mean_q: 1.277176
 19586/100000: episode: 311, duration: 0.186s, episode steps: 32, steps per second: 172, episode reward: 20.257, mean reward: 0.633 [0.553, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.049, 10.355], loss: 0.001612, mae: 0.042855, mean_q: 1.289750
 19621/100000: episode: 312, duration: 0.203s, episode steps: 35, steps per second: 173, episode reward: 23.008, mean reward: 0.657 [0.573, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.229, 10.288], loss: 0.001585, mae: 0.042655, mean_q: 1.276271
 19653/100000: episode: 313, duration: 0.181s, episode steps: 32, steps per second: 177, episode reward: 22.946, mean reward: 0.717 [0.594, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.090, 10.342], loss: 0.001798, mae: 0.044425, mean_q: 1.268794
 19683/100000: episode: 314, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 21.547, mean reward: 0.718 [0.608, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.663, 10.298], loss: 0.001633, mae: 0.042607, mean_q: 1.285920
 19718/100000: episode: 315, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 23.812, mean reward: 0.680 [0.603, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.134, 10.331], loss: 0.001504, mae: 0.042300, mean_q: 1.288109
 19747/100000: episode: 316, duration: 0.170s, episode steps: 29, steps per second: 170, episode reward: 20.917, mean reward: 0.721 [0.664, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.390, 10.439], loss: 0.001856, mae: 0.047127, mean_q: 1.283991
 19779/100000: episode: 317, duration: 0.167s, episode steps: 32, steps per second: 191, episode reward: 22.731, mean reward: 0.710 [0.642, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.771, 10.595], loss: 0.001753, mae: 0.045427, mean_q: 1.293546
 19814/100000: episode: 318, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 27.555, mean reward: 0.787 [0.643, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.761, 10.510], loss: 0.001575, mae: 0.042393, mean_q: 1.283555
 19844/100000: episode: 319, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 21.627, mean reward: 0.721 [0.629, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.317], loss: 0.001650, mae: 0.042651, mean_q: 1.283759
 19873/100000: episode: 320, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 18.295, mean reward: 0.631 [0.522, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.252, 10.242], loss: 0.001574, mae: 0.042177, mean_q: 1.285571
 19908/100000: episode: 321, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 23.337, mean reward: 0.667 [0.605, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.410, 10.346], loss: 0.001899, mae: 0.047213, mean_q: 1.285628
 19938/100000: episode: 322, duration: 0.190s, episode steps: 30, steps per second: 158, episode reward: 21.407, mean reward: 0.714 [0.616, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.654, 10.330], loss: 0.001681, mae: 0.044136, mean_q: 1.289961
 19971/100000: episode: 323, duration: 0.220s, episode steps: 33, steps per second: 150, episode reward: 20.222, mean reward: 0.613 [0.519, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.669, 10.100], loss: 0.001552, mae: 0.041681, mean_q: 1.292148
 20000/100000: episode: 324, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 18.615, mean reward: 0.642 [0.549, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.367], loss: 0.001674, mae: 0.043703, mean_q: 1.299216
 20032/100000: episode: 325, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 20.545, mean reward: 0.642 [0.580, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.440, 10.378], loss: 0.001829, mae: 0.045968, mean_q: 1.286193
 20061/100000: episode: 326, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 22.648, mean reward: 0.781 [0.706, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.695, 10.641], loss: 0.001668, mae: 0.044077, mean_q: 1.287250
 20094/100000: episode: 327, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 20.293, mean reward: 0.615 [0.539, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.204], loss: 0.001743, mae: 0.046430, mean_q: 1.308143
 20124/100000: episode: 328, duration: 0.175s, episode steps: 30, steps per second: 172, episode reward: 20.353, mean reward: 0.678 [0.612, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.458, 10.312], loss: 0.001623, mae: 0.042893, mean_q: 1.283995
 20154/100000: episode: 329, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 23.881, mean reward: 0.796 [0.696, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.049, 10.484], loss: 0.001531, mae: 0.042537, mean_q: 1.294247
[Info] 3-TH LEVEL FOUND: 1.5723916292190552, Considering 10/90 traces
 20184/100000: episode: 330, duration: 17.356s, episode steps: 30, steps per second: 2, episode reward: 18.576, mean reward: 0.619 [0.500, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.484, 10.100], loss: 0.001605, mae: 0.043509, mean_q: 1.281223
 20202/100000: episode: 331, duration: 0.141s, episode steps: 18, steps per second: 127, episode reward: 12.878, mean reward: 0.715 [0.638, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.706, 10.313], loss: 0.002049, mae: 0.047997, mean_q: 1.296308
 20217/100000: episode: 332, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 11.957, mean reward: 0.797 [0.713, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.685], loss: 0.001678, mae: 0.043520, mean_q: 1.282228
 20236/100000: episode: 333, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 15.213, mean reward: 0.801 [0.754, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.927, 10.432], loss: 0.001580, mae: 0.043357, mean_q: 1.295037
 20256/100000: episode: 334, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 16.559, mean reward: 0.828 [0.777, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-1.173, 10.610], loss: 0.001589, mae: 0.042412, mean_q: 1.294493
 20270/100000: episode: 335, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 11.599, mean reward: 0.828 [0.784, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.581], loss: 0.001368, mae: 0.039848, mean_q: 1.299019
 20289/100000: episode: 336, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 15.032, mean reward: 0.791 [0.705, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.543], loss: 0.001471, mae: 0.040489, mean_q: 1.289816
 20309/100000: episode: 337, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 15.515, mean reward: 0.776 [0.673, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.759, 10.458], loss: 0.001620, mae: 0.044686, mean_q: 1.310781
[Info] FALSIFICATION!
 20322/100000: episode: 338, duration: 0.540s, episode steps: 13, steps per second: 24, episode reward: 11.421, mean reward: 0.879 [0.741, 1.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.053, 10.633], loss: 0.001648, mae: 0.043916, mean_q: 1.320182
 20342/100000: episode: 339, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 15.143, mean reward: 0.757 [0.693, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.813, 10.486], loss: 0.001642, mae: 0.043698, mean_q: 1.304937
 20360/100000: episode: 340, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 14.216, mean reward: 0.790 [0.702, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.451, 10.439], loss: 0.001956, mae: 0.049726, mean_q: 1.308180
 20380/100000: episode: 341, duration: 0.122s, episode steps: 20, steps per second: 163, episode reward: 14.767, mean reward: 0.738 [0.616, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.348], loss: 0.001681, mae: 0.043065, mean_q: 1.298086
 20402/100000: episode: 342, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 17.316, mean reward: 0.787 [0.736, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.247, 10.492], loss: 0.001583, mae: 0.042742, mean_q: 1.303936
 20421/100000: episode: 343, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 14.315, mean reward: 0.753 [0.667, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.888, 10.467], loss: 0.001837, mae: 0.047254, mean_q: 1.309261
 20436/100000: episode: 344, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 11.709, mean reward: 0.781 [0.738, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.515], loss: 0.002136, mae: 0.051058, mean_q: 1.292469
 20456/100000: episode: 345, duration: 0.124s, episode steps: 20, steps per second: 162, episode reward: 16.059, mean reward: 0.803 [0.727, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.556], loss: 0.001512, mae: 0.042075, mean_q: 1.317493
 20478/100000: episode: 346, duration: 0.137s, episode steps: 22, steps per second: 160, episode reward: 18.418, mean reward: 0.837 [0.713, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.611], loss: 0.001620, mae: 0.042966, mean_q: 1.295028
 20498/100000: episode: 347, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 15.602, mean reward: 0.780 [0.673, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.063, 10.431], loss: 0.001781, mae: 0.045438, mean_q: 1.317280
 20517/100000: episode: 348, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 14.388, mean reward: 0.757 [0.628, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.616, 10.372], loss: 0.001528, mae: 0.042833, mean_q: 1.317368
 20537/100000: episode: 349, duration: 0.127s, episode steps: 20, steps per second: 158, episode reward: 16.107, mean reward: 0.805 [0.691, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.337, 10.447], loss: 0.001682, mae: 0.044209, mean_q: 1.310627
 20559/100000: episode: 350, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 15.683, mean reward: 0.713 [0.656, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.407], loss: 0.001588, mae: 0.042758, mean_q: 1.306954
 20573/100000: episode: 351, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 10.956, mean reward: 0.783 [0.745, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.503], loss: 0.001426, mae: 0.041796, mean_q: 1.301865
 20591/100000: episode: 352, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 15.155, mean reward: 0.842 [0.734, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.124, 10.582], loss: 0.001429, mae: 0.041883, mean_q: 1.300550
 20615/100000: episode: 353, duration: 0.157s, episode steps: 24, steps per second: 153, episode reward: 19.763, mean reward: 0.823 [0.776, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.088, 10.644], loss: 0.001782, mae: 0.044893, mean_q: 1.317086
 20639/100000: episode: 354, duration: 0.163s, episode steps: 24, steps per second: 148, episode reward: 19.602, mean reward: 0.817 [0.742, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.233, 10.530], loss: 0.001869, mae: 0.046169, mean_q: 1.310618
 20659/100000: episode: 355, duration: 0.147s, episode steps: 20, steps per second: 136, episode reward: 15.466, mean reward: 0.773 [0.727, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.516], loss: 0.001602, mae: 0.043386, mean_q: 1.326444
 20678/100000: episode: 356, duration: 0.123s, episode steps: 19, steps per second: 154, episode reward: 15.035, mean reward: 0.791 [0.630, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.464], loss: 0.001456, mae: 0.040877, mean_q: 1.332065
 20693/100000: episode: 357, duration: 0.108s, episode steps: 15, steps per second: 139, episode reward: 11.911, mean reward: 0.794 [0.738, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-1.033, 10.547], loss: 0.001471, mae: 0.041070, mean_q: 1.302280
 20715/100000: episode: 358, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 16.112, mean reward: 0.732 [0.662, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.947, 10.453], loss: 0.001810, mae: 0.046149, mean_q: 1.317782
 20737/100000: episode: 359, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 17.564, mean reward: 0.798 [0.748, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.209, 10.444], loss: 0.001807, mae: 0.047153, mean_q: 1.313474
 20757/100000: episode: 360, duration: 0.128s, episode steps: 20, steps per second: 156, episode reward: 16.914, mean reward: 0.846 [0.805, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.730, 10.645], loss: 0.001676, mae: 0.044762, mean_q: 1.315004
 20776/100000: episode: 361, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 13.862, mean reward: 0.730 [0.661, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.403], loss: 0.001671, mae: 0.044759, mean_q: 1.310715
 20798/100000: episode: 362, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 16.748, mean reward: 0.761 [0.602, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.474, 10.325], loss: 0.001875, mae: 0.046201, mean_q: 1.318669
 20816/100000: episode: 363, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 13.705, mean reward: 0.761 [0.705, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.453], loss: 0.001768, mae: 0.044910, mean_q: 1.329225
 20834/100000: episode: 364, duration: 0.121s, episode steps: 18, steps per second: 149, episode reward: 13.716, mean reward: 0.762 [0.713, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.696, 10.474], loss: 0.001688, mae: 0.045180, mean_q: 1.317977
 20849/100000: episode: 365, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 12.455, mean reward: 0.830 [0.737, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-1.266, 10.506], loss: 0.001768, mae: 0.046287, mean_q: 1.323236
 20873/100000: episode: 366, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 18.261, mean reward: 0.761 [0.695, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.450], loss: 0.001875, mae: 0.049141, mean_q: 1.330320
 20895/100000: episode: 367, duration: 0.158s, episode steps: 22, steps per second: 139, episode reward: 17.026, mean reward: 0.774 [0.668, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.423], loss: 0.001551, mae: 0.043047, mean_q: 1.333065
 20915/100000: episode: 368, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 17.213, mean reward: 0.861 [0.784, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.675], loss: 0.001423, mae: 0.040745, mean_q: 1.335337
 20929/100000: episode: 369, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 12.418, mean reward: 0.887 [0.824, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.566], loss: 0.001188, mae: 0.038024, mean_q: 1.333142
 20943/100000: episode: 370, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 11.129, mean reward: 0.795 [0.752, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.266, 10.591], loss: 0.001650, mae: 0.043762, mean_q: 1.326523
 20963/100000: episode: 371, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 16.991, mean reward: 0.850 [0.781, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.541], loss: 0.001745, mae: 0.045644, mean_q: 1.322564
 20987/100000: episode: 372, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 19.491, mean reward: 0.812 [0.750, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.342, 10.451], loss: 0.001779, mae: 0.046611, mean_q: 1.333725
 21001/100000: episode: 373, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 11.735, mean reward: 0.838 [0.735, 0.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.199, 10.474], loss: 0.001891, mae: 0.046786, mean_q: 1.335773
 21016/100000: episode: 374, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 12.117, mean reward: 0.808 [0.731, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.518], loss: 0.001644, mae: 0.043258, mean_q: 1.334215
 21040/100000: episode: 375, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 18.747, mean reward: 0.781 [0.714, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.081, 10.460], loss: 0.002108, mae: 0.051877, mean_q: 1.340615
 21058/100000: episode: 376, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 14.751, mean reward: 0.819 [0.759, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.067, 10.626], loss: 0.001894, mae: 0.046980, mean_q: 1.347408
 21072/100000: episode: 377, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 11.033, mean reward: 0.788 [0.747, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.596, 10.552], loss: 0.001919, mae: 0.046555, mean_q: 1.332103
 21087/100000: episode: 378, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 11.903, mean reward: 0.794 [0.707, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-1.057, 10.423], loss: 0.001541, mae: 0.042959, mean_q: 1.361099
 21105/100000: episode: 379, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 13.388, mean reward: 0.744 [0.655, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.378], loss: 0.002214, mae: 0.050668, mean_q: 1.347377
 21125/100000: episode: 380, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 16.043, mean reward: 0.802 [0.749, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.262, 10.483], loss: 0.001961, mae: 0.048296, mean_q: 1.345473
 21139/100000: episode: 381, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 11.208, mean reward: 0.801 [0.728, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.585], loss: 0.001901, mae: 0.048373, mean_q: 1.346452
 21158/100000: episode: 382, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 14.453, mean reward: 0.761 [0.656, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-1.086, 10.483], loss: 0.002296, mae: 0.050994, mean_q: 1.345497
 21178/100000: episode: 383, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 13.383, mean reward: 0.669 [0.564, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.130, 10.233], loss: 0.001614, mae: 0.044650, mean_q: 1.348643
 21192/100000: episode: 384, duration: 0.083s, episode steps: 14, steps per second: 170, episode reward: 10.827, mean reward: 0.773 [0.719, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.508], loss: 0.001707, mae: 0.043344, mean_q: 1.330281
 21211/100000: episode: 385, duration: 0.111s, episode steps: 19, steps per second: 170, episode reward: 13.678, mean reward: 0.720 [0.643, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.464, 10.329], loss: 0.001879, mae: 0.048117, mean_q: 1.361238
 21229/100000: episode: 386, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 12.453, mean reward: 0.692 [0.603, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.362], loss: 0.001589, mae: 0.043211, mean_q: 1.345343
 21249/100000: episode: 387, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 15.714, mean reward: 0.786 [0.694, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.504], loss: 0.001634, mae: 0.042446, mean_q: 1.338126
 21269/100000: episode: 388, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 16.633, mean reward: 0.832 [0.755, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.414, 10.579], loss: 0.002008, mae: 0.049272, mean_q: 1.354842
 21289/100000: episode: 389, duration: 0.115s, episode steps: 20, steps per second: 173, episode reward: 17.542, mean reward: 0.877 [0.794, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.210, 10.642], loss: 0.002115, mae: 0.049290, mean_q: 1.335273
 21303/100000: episode: 390, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 11.645, mean reward: 0.832 [0.766, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-1.033, 10.655], loss: 0.001747, mae: 0.043343, mean_q: 1.360506
 21318/100000: episode: 391, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 11.320, mean reward: 0.755 [0.663, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.432, 10.482], loss: 0.001572, mae: 0.044088, mean_q: 1.361054
 21338/100000: episode: 392, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 14.357, mean reward: 0.718 [0.657, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.546, 10.374], loss: 0.001850, mae: 0.045740, mean_q: 1.349623
 21358/100000: episode: 393, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 15.567, mean reward: 0.778 [0.711, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.526, 10.508], loss: 0.001760, mae: 0.045148, mean_q: 1.342044
 21378/100000: episode: 394, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 14.463, mean reward: 0.723 [0.663, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.483, 10.407], loss: 0.001822, mae: 0.046985, mean_q: 1.349891
 21392/100000: episode: 395, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.715, mean reward: 0.765 [0.716, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.984, 10.332], loss: 0.001891, mae: 0.047625, mean_q: 1.357535
 21410/100000: episode: 396, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 13.468, mean reward: 0.748 [0.716, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.521], loss: 0.001528, mae: 0.041975, mean_q: 1.348140
 21424/100000: episode: 397, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 11.509, mean reward: 0.822 [0.772, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.584], loss: 0.001635, mae: 0.044355, mean_q: 1.352737
 21439/100000: episode: 398, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 12.643, mean reward: 0.843 [0.761, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.441, 10.503], loss: 0.001585, mae: 0.043713, mean_q: 1.364265
 21457/100000: episode: 399, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 15.563, mean reward: 0.865 [0.799, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.267, 10.519], loss: 0.002133, mae: 0.050297, mean_q: 1.347070
 21475/100000: episode: 400, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 14.453, mean reward: 0.803 [0.765, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.478, 10.574], loss: 0.001484, mae: 0.041915, mean_q: 1.360420
 21493/100000: episode: 401, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 13.829, mean reward: 0.768 [0.696, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.591, 10.580], loss: 0.001493, mae: 0.041743, mean_q: 1.345268
 21507/100000: episode: 402, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 10.504, mean reward: 0.750 [0.702, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.050, 10.463], loss: 0.001920, mae: 0.045738, mean_q: 1.334552
 21531/100000: episode: 403, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 18.750, mean reward: 0.781 [0.745, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.436, 10.410], loss: 0.001498, mae: 0.041397, mean_q: 1.365182
 21545/100000: episode: 404, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 10.765, mean reward: 0.769 [0.727, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.772, 10.538], loss: 0.001483, mae: 0.042338, mean_q: 1.356909
 21560/100000: episode: 405, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 13.379, mean reward: 0.892 [0.820, 0.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.604], loss: 0.001636, mae: 0.043178, mean_q: 1.365032
[Info] FALSIFICATION!
 21566/100000: episode: 406, duration: 3.953s, episode steps: 6, steps per second: 2, episode reward: 5.250, mean reward: 0.875 [0.791, 1.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.212, 9.507], loss: 0.001361, mae: 0.041800, mean_q: 1.371668
 21580/100000: episode: 407, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 11.394, mean reward: 0.814 [0.767, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.597], loss: 0.001656, mae: 0.045268, mean_q: 1.378835
 21602/100000: episode: 408, duration: 0.132s, episode steps: 22, steps per second: 166, episode reward: 17.867, mean reward: 0.812 [0.737, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.533], loss: 0.001968, mae: 0.044328, mean_q: 1.371382
 21616/100000: episode: 409, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 11.397, mean reward: 0.814 [0.765, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.595], loss: 0.001833, mae: 0.046752, mean_q: 1.356684
 21640/100000: episode: 410, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 17.423, mean reward: 0.726 [0.570, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.323], loss: 0.001745, mae: 0.043931, mean_q: 1.375886
 21660/100000: episode: 411, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 15.937, mean reward: 0.797 [0.680, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.524], loss: 0.002144, mae: 0.052082, mean_q: 1.342977
 21682/100000: episode: 412, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 16.603, mean reward: 0.755 [0.679, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.752, 10.473], loss: 0.001853, mae: 0.047592, mean_q: 1.359772
[Info] FALSIFICATION!
 21687/100000: episode: 413, duration: 0.313s, episode steps: 5, steps per second: 16, episode reward: 4.533, mean reward: 0.907 [0.825, 1.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.020, 9.847], loss: 0.001742, mae: 0.045946, mean_q: 1.355792
 21702/100000: episode: 414, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 12.336, mean reward: 0.822 [0.767, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.828, 10.611], loss: 0.002001, mae: 0.047282, mean_q: 1.376338
 21716/100000: episode: 415, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 11.242, mean reward: 0.803 [0.642, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-1.059, 10.440], loss: 0.002166, mae: 0.050398, mean_q: 1.356581
 21731/100000: episode: 416, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 11.776, mean reward: 0.785 [0.738, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.530], loss: 0.002302, mae: 0.049397, mean_q: 1.378385
 21750/100000: episode: 417, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 15.866, mean reward: 0.835 [0.762, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.645], loss: 0.001811, mae: 0.045002, mean_q: 1.375919
 21764/100000: episode: 418, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 11.302, mean reward: 0.807 [0.737, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.618], loss: 0.001841, mae: 0.047018, mean_q: 1.383408
 21779/100000: episode: 419, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 12.115, mean reward: 0.808 [0.755, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.504, 10.517], loss: 0.001424, mae: 0.041286, mean_q: 1.371458
[Info] Complete ISplit Iteration
[Info] Levels: [1.2743632, 1.4194136, 1.5723916, 1.6626943]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.29]
[Info] Error Prob: 0.00029000000000000006

 21797/100000: episode: 420, duration: 4.392s, episode steps: 18, steps per second: 4, episode reward: 14.334, mean reward: 0.796 [0.694, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.649], loss: 0.001692, mae: 0.044691, mean_q: 1.372899
 21897/100000: episode: 421, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 56.969, mean reward: 0.570 [0.504, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.809, 10.176], loss: 0.001911, mae: 0.046655, mean_q: 1.367510
 21997/100000: episode: 422, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 60.910, mean reward: 0.609 [0.508, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.264, 10.098], loss: 0.002003, mae: 0.045588, mean_q: 1.376701
 22097/100000: episode: 423, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.983, mean reward: 0.590 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.035, 10.415], loss: 0.002007, mae: 0.047825, mean_q: 1.361076
 22197/100000: episode: 424, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 60.249, mean reward: 0.602 [0.509, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.447, 10.098], loss: 0.001728, mae: 0.044516, mean_q: 1.355472
 22297/100000: episode: 425, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 57.334, mean reward: 0.573 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.255, 10.153], loss: 0.001808, mae: 0.044874, mean_q: 1.366090
 22397/100000: episode: 426, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 57.673, mean reward: 0.577 [0.511, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.488, 10.098], loss: 0.001608, mae: 0.042825, mean_q: 1.361558
 22497/100000: episode: 427, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.037, mean reward: 0.580 [0.516, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.125, 10.098], loss: 0.001919, mae: 0.045868, mean_q: 1.352802
 22597/100000: episode: 428, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 59.834, mean reward: 0.598 [0.510, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.623, 10.319], loss: 0.001901, mae: 0.045206, mean_q: 1.352612
 22697/100000: episode: 429, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.920, mean reward: 0.579 [0.509, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.206, 10.098], loss: 0.001695, mae: 0.044666, mean_q: 1.361533
 22797/100000: episode: 430, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.220, mean reward: 0.572 [0.504, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.146, 10.170], loss: 0.001680, mae: 0.043999, mean_q: 1.352223
 22897/100000: episode: 431, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 58.155, mean reward: 0.582 [0.509, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.137, 10.098], loss: 0.001760, mae: 0.043336, mean_q: 1.352675
 22997/100000: episode: 432, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 62.978, mean reward: 0.630 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.748, 10.098], loss: 0.002117, mae: 0.048436, mean_q: 1.346053
 23097/100000: episode: 433, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.776, mean reward: 0.588 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.693, 10.098], loss: 0.001714, mae: 0.044961, mean_q: 1.341169
 23197/100000: episode: 434, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.460, mean reward: 0.585 [0.505, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.792, 10.098], loss: 0.001684, mae: 0.044117, mean_q: 1.334773
 23297/100000: episode: 435, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.152, mean reward: 0.582 [0.507, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.038, 10.098], loss: 0.001900, mae: 0.045400, mean_q: 1.337463
 23397/100000: episode: 436, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 62.287, mean reward: 0.623 [0.502, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.252, 10.236], loss: 0.001925, mae: 0.047921, mean_q: 1.340991
 23497/100000: episode: 437, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.232, mean reward: 0.592 [0.517, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.502, 10.098], loss: 0.001790, mae: 0.044677, mean_q: 1.331948
 23597/100000: episode: 438, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.603, mean reward: 0.586 [0.510, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.024, 10.469], loss: 0.001819, mae: 0.046574, mean_q: 1.325420
 23697/100000: episode: 439, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.618, mean reward: 0.586 [0.511, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.699, 10.098], loss: 0.001872, mae: 0.046578, mean_q: 1.321119
 23797/100000: episode: 440, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.560, mean reward: 0.576 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.665, 10.098], loss: 0.001855, mae: 0.045880, mean_q: 1.321728
 23897/100000: episode: 441, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.359, mean reward: 0.584 [0.506, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.937, 10.098], loss: 0.001724, mae: 0.043903, mean_q: 1.306968
 23997/100000: episode: 442, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.105, mean reward: 0.581 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.979, 10.098], loss: 0.001924, mae: 0.047201, mean_q: 1.301139
 24097/100000: episode: 443, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.876, mean reward: 0.589 [0.506, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.076, 10.098], loss: 0.001779, mae: 0.045460, mean_q: 1.300548
 24197/100000: episode: 444, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.738, mean reward: 0.597 [0.502, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.742, 10.098], loss: 0.001636, mae: 0.044309, mean_q: 1.299215
 24297/100000: episode: 445, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 62.524, mean reward: 0.625 [0.511, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.622, 10.290], loss: 0.002181, mae: 0.049852, mean_q: 1.291916
 24397/100000: episode: 446, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.985, mean reward: 0.580 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.013, 10.098], loss: 0.001652, mae: 0.043789, mean_q: 1.303072
 24497/100000: episode: 447, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.497, mean reward: 0.585 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.520, 10.098], loss: 0.001864, mae: 0.047576, mean_q: 1.296462
 24597/100000: episode: 448, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.959, mean reward: 0.590 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.184, 10.098], loss: 0.001942, mae: 0.047135, mean_q: 1.294977
 24697/100000: episode: 449, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.751, mean reward: 0.578 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.763, 10.238], loss: 0.001887, mae: 0.046500, mean_q: 1.286767
 24797/100000: episode: 450, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.359, mean reward: 0.584 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.183], loss: 0.001835, mae: 0.045449, mean_q: 1.289367
 24897/100000: episode: 451, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.078, mean reward: 0.581 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.844, 10.205], loss: 0.001771, mae: 0.046217, mean_q: 1.279138
 24997/100000: episode: 452, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 59.774, mean reward: 0.598 [0.505, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.355, 10.131], loss: 0.001971, mae: 0.047032, mean_q: 1.280363
 25097/100000: episode: 453, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.047, mean reward: 0.580 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.074, 10.351], loss: 0.001951, mae: 0.046529, mean_q: 1.269195
 25197/100000: episode: 454, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 59.127, mean reward: 0.591 [0.514, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.569, 10.098], loss: 0.001893, mae: 0.045629, mean_q: 1.256689
 25297/100000: episode: 455, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.766, mean reward: 0.598 [0.505, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.356, 10.098], loss: 0.001674, mae: 0.043759, mean_q: 1.260773
 25397/100000: episode: 456, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.105, mean reward: 0.581 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.311, 10.340], loss: 0.001847, mae: 0.043965, mean_q: 1.263263
 25497/100000: episode: 457, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.771, mean reward: 0.598 [0.507, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.723, 10.282], loss: 0.001648, mae: 0.043985, mean_q: 1.243390
 25597/100000: episode: 458, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 56.951, mean reward: 0.570 [0.500, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.932, 10.106], loss: 0.001829, mae: 0.044636, mean_q: 1.237582
 25697/100000: episode: 459, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.221, mean reward: 0.582 [0.520, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.669, 10.265], loss: 0.001873, mae: 0.045211, mean_q: 1.235517
 25797/100000: episode: 460, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 61.499, mean reward: 0.615 [0.510, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.623, 10.098], loss: 0.001686, mae: 0.044227, mean_q: 1.227034
 25897/100000: episode: 461, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 60.501, mean reward: 0.605 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.885, 10.309], loss: 0.001743, mae: 0.044779, mean_q: 1.219705
 25997/100000: episode: 462, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.486, mean reward: 0.585 [0.509, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.713, 10.261], loss: 0.001624, mae: 0.044012, mean_q: 1.215471
 26097/100000: episode: 463, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 60.529, mean reward: 0.605 [0.503, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.581, 10.098], loss: 0.001768, mae: 0.044768, mean_q: 1.208813
 26197/100000: episode: 464, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 60.718, mean reward: 0.607 [0.500, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.630, 10.356], loss: 0.001651, mae: 0.043782, mean_q: 1.208608
 26297/100000: episode: 465, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 56.967, mean reward: 0.570 [0.500, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.043, 10.177], loss: 0.001587, mae: 0.043476, mean_q: 1.198417
 26397/100000: episode: 466, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 59.177, mean reward: 0.592 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.565, 10.098], loss: 0.001664, mae: 0.044416, mean_q: 1.191851
 26497/100000: episode: 467, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.920, mean reward: 0.579 [0.507, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.659, 10.120], loss: 0.001810, mae: 0.044971, mean_q: 1.186392
 26597/100000: episode: 468, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.127, mean reward: 0.581 [0.501, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.833, 10.131], loss: 0.001675, mae: 0.043242, mean_q: 1.175743
 26697/100000: episode: 469, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 60.054, mean reward: 0.601 [0.504, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.669, 10.307], loss: 0.001494, mae: 0.042417, mean_q: 1.173275
 26797/100000: episode: 470, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 56.964, mean reward: 0.570 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.080, 10.098], loss: 0.001393, mae: 0.041139, mean_q: 1.167302
 26897/100000: episode: 471, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.779, mean reward: 0.588 [0.510, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.113, 10.393], loss: 0.001645, mae: 0.044642, mean_q: 1.167536
 26997/100000: episode: 472, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.347, mean reward: 0.603 [0.506, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.842, 10.098], loss: 0.001410, mae: 0.041361, mean_q: 1.165437
 27097/100000: episode: 473, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.053, mean reward: 0.591 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.356, 10.223], loss: 0.001394, mae: 0.040786, mean_q: 1.164199
 27197/100000: episode: 474, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 60.844, mean reward: 0.608 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.484, 10.313], loss: 0.001584, mae: 0.043361, mean_q: 1.166851
 27297/100000: episode: 475, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 57.742, mean reward: 0.577 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.580, 10.098], loss: 0.001519, mae: 0.042546, mean_q: 1.166633
 27397/100000: episode: 476, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.954, mean reward: 0.580 [0.500, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.161, 10.098], loss: 0.001463, mae: 0.041887, mean_q: 1.166098
 27497/100000: episode: 477, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.554, mean reward: 0.586 [0.505, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.441, 10.207], loss: 0.001586, mae: 0.043483, mean_q: 1.164459
 27597/100000: episode: 478, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 56.642, mean reward: 0.566 [0.508, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.405, 10.098], loss: 0.001534, mae: 0.042923, mean_q: 1.168122
 27697/100000: episode: 479, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.572, mean reward: 0.586 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.149, 10.098], loss: 0.001526, mae: 0.042777, mean_q: 1.167438
 27797/100000: episode: 480, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 61.720, mean reward: 0.617 [0.500, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.909, 10.156], loss: 0.001642, mae: 0.044468, mean_q: 1.167954
 27897/100000: episode: 481, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 62.004, mean reward: 0.620 [0.506, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.818, 10.438], loss: 0.001382, mae: 0.041387, mean_q: 1.168985
 27997/100000: episode: 482, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 61.988, mean reward: 0.620 [0.513, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.362, 10.098], loss: 0.001647, mae: 0.044700, mean_q: 1.167920
 28097/100000: episode: 483, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.825, mean reward: 0.598 [0.507, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.029, 10.380], loss: 0.001476, mae: 0.042512, mean_q: 1.171938
 28197/100000: episode: 484, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.736, mean reward: 0.587 [0.514, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.803, 10.215], loss: 0.001523, mae: 0.042960, mean_q: 1.169296
 28297/100000: episode: 485, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 56.559, mean reward: 0.566 [0.501, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.042, 10.098], loss: 0.001533, mae: 0.042854, mean_q: 1.167771
 28397/100000: episode: 486, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 56.714, mean reward: 0.567 [0.505, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.001, 10.098], loss: 0.001476, mae: 0.042005, mean_q: 1.166752
 28497/100000: episode: 487, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 57.880, mean reward: 0.579 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.133, 10.156], loss: 0.001612, mae: 0.043977, mean_q: 1.164807
 28597/100000: episode: 488, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 59.823, mean reward: 0.598 [0.507, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.955, 10.182], loss: 0.001427, mae: 0.041596, mean_q: 1.165243
 28697/100000: episode: 489, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 64.224, mean reward: 0.642 [0.511, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.975, 10.098], loss: 0.001510, mae: 0.042504, mean_q: 1.163549
 28797/100000: episode: 490, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 61.341, mean reward: 0.613 [0.513, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.231, 10.098], loss: 0.001594, mae: 0.043620, mean_q: 1.169334
 28897/100000: episode: 491, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.846, mean reward: 0.578 [0.507, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.926, 10.307], loss: 0.001444, mae: 0.042058, mean_q: 1.168768
 28997/100000: episode: 492, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 60.529, mean reward: 0.605 [0.516, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.062, 10.098], loss: 0.001499, mae: 0.042496, mean_q: 1.171121
 29097/100000: episode: 493, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 58.959, mean reward: 0.590 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.485, 10.098], loss: 0.001574, mae: 0.042842, mean_q: 1.174687
 29197/100000: episode: 494, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.333, mean reward: 0.583 [0.502, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.977, 10.260], loss: 0.001433, mae: 0.041732, mean_q: 1.173931
 29297/100000: episode: 495, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 61.333, mean reward: 0.613 [0.503, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.171, 10.112], loss: 0.001528, mae: 0.042389, mean_q: 1.168646
 29397/100000: episode: 496, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.799, mean reward: 0.578 [0.511, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.465, 10.256], loss: 0.001536, mae: 0.042558, mean_q: 1.169459
 29497/100000: episode: 497, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 56.702, mean reward: 0.567 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.828, 10.146], loss: 0.001479, mae: 0.042426, mean_q: 1.168003
 29597/100000: episode: 498, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 60.024, mean reward: 0.600 [0.505, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.884, 10.098], loss: 0.001556, mae: 0.042617, mean_q: 1.169478
 29697/100000: episode: 499, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.558, mean reward: 0.586 [0.504, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.224, 10.098], loss: 0.001611, mae: 0.043543, mean_q: 1.170303
 29797/100000: episode: 500, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.702, mean reward: 0.587 [0.501, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.770, 10.258], loss: 0.001565, mae: 0.042935, mean_q: 1.172627
 29897/100000: episode: 501, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.618, mean reward: 0.586 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.453, 10.098], loss: 0.001501, mae: 0.041865, mean_q: 1.168643
 29997/100000: episode: 502, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.899, mean reward: 0.589 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.553, 10.098], loss: 0.001556, mae: 0.043049, mean_q: 1.166258
 30097/100000: episode: 503, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 56.453, mean reward: 0.565 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.986, 10.199], loss: 0.001428, mae: 0.040924, mean_q: 1.169252
 30197/100000: episode: 504, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 56.581, mean reward: 0.566 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.463, 10.137], loss: 0.001481, mae: 0.042037, mean_q: 1.168306
 30297/100000: episode: 505, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 57.554, mean reward: 0.576 [0.510, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.484, 10.098], loss: 0.001646, mae: 0.043869, mean_q: 1.168589
 30397/100000: episode: 506, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 57.826, mean reward: 0.578 [0.501, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.198, 10.247], loss: 0.001669, mae: 0.044759, mean_q: 1.171675
 30497/100000: episode: 507, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 59.166, mean reward: 0.592 [0.508, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.632, 10.119], loss: 0.001619, mae: 0.044009, mean_q: 1.170356
 30597/100000: episode: 508, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 59.321, mean reward: 0.593 [0.509, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.850, 10.106], loss: 0.001524, mae: 0.041879, mean_q: 1.165107
 30697/100000: episode: 509, duration: 0.668s, episode steps: 100, steps per second: 150, episode reward: 58.637, mean reward: 0.586 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.191, 10.098], loss: 0.001677, mae: 0.044603, mean_q: 1.165750
 30797/100000: episode: 510, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 57.496, mean reward: 0.575 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.485, 10.098], loss: 0.001521, mae: 0.042476, mean_q: 1.165272
 30897/100000: episode: 511, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 62.094, mean reward: 0.621 [0.508, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.551, 10.098], loss: 0.001599, mae: 0.043562, mean_q: 1.162614
 30997/100000: episode: 512, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 56.385, mean reward: 0.564 [0.500, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.298, 10.098], loss: 0.001581, mae: 0.042516, mean_q: 1.164168
 31097/100000: episode: 513, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 57.919, mean reward: 0.579 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.646, 10.098], loss: 0.001393, mae: 0.041114, mean_q: 1.165582
 31197/100000: episode: 514, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 57.739, mean reward: 0.577 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.824, 10.159], loss: 0.001745, mae: 0.045525, mean_q: 1.167254
 31297/100000: episode: 515, duration: 0.688s, episode steps: 100, steps per second: 145, episode reward: 58.383, mean reward: 0.584 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.854, 10.113], loss: 0.001549, mae: 0.042987, mean_q: 1.165786
 31397/100000: episode: 516, duration: 0.692s, episode steps: 100, steps per second: 145, episode reward: 59.120, mean reward: 0.591 [0.508, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.294, 10.391], loss: 0.001516, mae: 0.042445, mean_q: 1.164707
 31497/100000: episode: 517, duration: 0.717s, episode steps: 100, steps per second: 140, episode reward: 57.689, mean reward: 0.577 [0.500, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.879, 10.098], loss: 0.001555, mae: 0.042507, mean_q: 1.163724
 31597/100000: episode: 518, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 59.580, mean reward: 0.596 [0.501, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.793, 10.550], loss: 0.001549, mae: 0.042295, mean_q: 1.164668
 31697/100000: episode: 519, duration: 0.983s, episode steps: 100, steps per second: 102, episode reward: 57.038, mean reward: 0.570 [0.513, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.566, 10.098], loss: 0.001520, mae: 0.042352, mean_q: 1.164173
[Info] 1-TH LEVEL FOUND: 1.3631137609481812, Considering 10/90 traces
 31797/100000: episode: 520, duration: 786.141s, episode steps: 100, steps per second: 0, episode reward: 61.576, mean reward: 0.616 [0.497, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.148, 10.098], loss: 0.001531, mae: 0.042401, mean_q: 1.163260
 31837/100000: episode: 521, duration: 0.262s, episode steps: 40, steps per second: 153, episode reward: 25.899, mean reward: 0.647 [0.535, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.228, 10.318], loss: 0.001493, mae: 0.041926, mean_q: 1.166329
 31849/100000: episode: 522, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 9.113, mean reward: 0.759 [0.685, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.669], loss: 0.001470, mae: 0.043901, mean_q: 1.165813
 31867/100000: episode: 523, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 13.033, mean reward: 0.724 [0.629, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.141, 10.501], loss: 0.001475, mae: 0.040621, mean_q: 1.162668
 31889/100000: episode: 524, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 16.135, mean reward: 0.733 [0.612, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.388, 10.453], loss: 0.001556, mae: 0.043698, mean_q: 1.164543
 31912/100000: episode: 525, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 15.185, mean reward: 0.660 [0.618, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.221, 10.383], loss: 0.001417, mae: 0.041219, mean_q: 1.173274
 31935/100000: episode: 526, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 14.818, mean reward: 0.644 [0.577, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.264], loss: 0.001641, mae: 0.043534, mean_q: 1.171484
 31979/100000: episode: 527, duration: 0.262s, episode steps: 44, steps per second: 168, episode reward: 30.745, mean reward: 0.699 [0.657, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.128, 10.415], loss: 0.001583, mae: 0.043439, mean_q: 1.166388
 31991/100000: episode: 528, duration: 0.078s, episode steps: 12, steps per second: 155, episode reward: 7.970, mean reward: 0.664 [0.615, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.410, 10.371], loss: 0.001991, mae: 0.047788, mean_q: 1.171824
 32008/100000: episode: 529, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 12.838, mean reward: 0.755 [0.644, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.843, 10.100], loss: 0.001660, mae: 0.043673, mean_q: 1.173384
 32052/100000: episode: 530, duration: 0.238s, episode steps: 44, steps per second: 185, episode reward: 27.532, mean reward: 0.626 [0.510, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-1.686, 10.123], loss: 0.001614, mae: 0.043082, mean_q: 1.171486
 32075/100000: episode: 531, duration: 0.115s, episode steps: 23, steps per second: 199, episode reward: 14.796, mean reward: 0.643 [0.605, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.714, 10.296], loss: 0.001633, mae: 0.043862, mean_q: 1.167122
 32102/100000: episode: 532, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 16.329, mean reward: 0.605 [0.536, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.329, 10.239], loss: 0.001698, mae: 0.045106, mean_q: 1.175464
 32124/100000: episode: 533, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 15.419, mean reward: 0.701 [0.639, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.775, 10.512], loss: 0.001981, mae: 0.047744, mean_q: 1.172821
 32142/100000: episode: 534, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 12.212, mean reward: 0.678 [0.607, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-1.328, 10.339], loss: 0.001512, mae: 0.042190, mean_q: 1.162620
 32182/100000: episode: 535, duration: 0.231s, episode steps: 40, steps per second: 173, episode reward: 24.950, mean reward: 0.624 [0.514, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.093, 10.243], loss: 0.001667, mae: 0.043983, mean_q: 1.178676
 32200/100000: episode: 536, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 14.183, mean reward: 0.788 [0.630, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.073, 10.529], loss: 0.001818, mae: 0.045720, mean_q: 1.176182
 32218/100000: episode: 537, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 13.218, mean reward: 0.734 [0.661, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-1.251, 10.460], loss: 0.001475, mae: 0.041926, mean_q: 1.164898
 32253/100000: episode: 538, duration: 0.203s, episode steps: 35, steps per second: 172, episode reward: 25.633, mean reward: 0.732 [0.649, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.518, 10.100], loss: 0.001774, mae: 0.044158, mean_q: 1.168919
 32275/100000: episode: 539, duration: 0.114s, episode steps: 22, steps per second: 194, episode reward: 14.724, mean reward: 0.669 [0.597, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.485, 10.457], loss: 0.001449, mae: 0.041852, mean_q: 1.178063
 32297/100000: episode: 540, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 15.245, mean reward: 0.693 [0.599, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.097, 10.420], loss: 0.001992, mae: 0.048229, mean_q: 1.173887
 32314/100000: episode: 541, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 11.669, mean reward: 0.686 [0.628, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.811, 10.100], loss: 0.001836, mae: 0.046207, mean_q: 1.166587
 32326/100000: episode: 542, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 7.927, mean reward: 0.661 [0.590, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.710, 10.296], loss: 0.001695, mae: 0.042683, mean_q: 1.174055
 32343/100000: episode: 543, duration: 0.129s, episode steps: 17, steps per second: 132, episode reward: 11.838, mean reward: 0.696 [0.596, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.330, 10.100], loss: 0.001764, mae: 0.044012, mean_q: 1.177292
 32360/100000: episode: 544, duration: 0.116s, episode steps: 17, steps per second: 147, episode reward: 11.763, mean reward: 0.692 [0.607, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.442, 10.100], loss: 0.001543, mae: 0.041983, mean_q: 1.185710
 32377/100000: episode: 545, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 11.715, mean reward: 0.689 [0.626, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.403, 10.100], loss: 0.001794, mae: 0.046914, mean_q: 1.182494
 32394/100000: episode: 546, duration: 0.108s, episode steps: 17, steps per second: 158, episode reward: 11.709, mean reward: 0.689 [0.614, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.878, 10.100], loss: 0.002171, mae: 0.048266, mean_q: 1.180531
 32429/100000: episode: 547, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 23.410, mean reward: 0.669 [0.519, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.400, 10.110], loss: 0.002032, mae: 0.048104, mean_q: 1.184012
 32446/100000: episode: 548, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 11.407, mean reward: 0.671 [0.625, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.190, 10.100], loss: 0.002408, mae: 0.054469, mean_q: 1.180869
 32481/100000: episode: 549, duration: 0.240s, episode steps: 35, steps per second: 146, episode reward: 27.188, mean reward: 0.777 [0.686, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.441, 10.100], loss: 0.001767, mae: 0.044575, mean_q: 1.175485
 32498/100000: episode: 550, duration: 0.149s, episode steps: 17, steps per second: 114, episode reward: 11.486, mean reward: 0.676 [0.625, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.276, 10.100], loss: 0.001898, mae: 0.046818, mean_q: 1.178256
 32538/100000: episode: 551, duration: 0.245s, episode steps: 40, steps per second: 163, episode reward: 26.539, mean reward: 0.663 [0.565, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.187, 10.219], loss: 0.001806, mae: 0.044638, mean_q: 1.175671
 32582/100000: episode: 552, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 28.363, mean reward: 0.645 [0.537, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.268, 10.430], loss: 0.002168, mae: 0.048687, mean_q: 1.187576
 32612/100000: episode: 553, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 22.805, mean reward: 0.760 [0.663, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.629, 10.100], loss: 0.001562, mae: 0.043132, mean_q: 1.193159
 32635/100000: episode: 554, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 17.204, mean reward: 0.748 [0.636, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-1.041, 10.581], loss: 0.002046, mae: 0.046250, mean_q: 1.187923
 32679/100000: episode: 555, duration: 0.226s, episode steps: 44, steps per second: 194, episode reward: 28.181, mean reward: 0.640 [0.545, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.474, 10.284], loss: 0.001932, mae: 0.046794, mean_q: 1.192176
 32696/100000: episode: 556, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 12.526, mean reward: 0.737 [0.640, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.221, 10.100], loss: 0.001907, mae: 0.044270, mean_q: 1.181102
 32714/100000: episode: 557, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 13.115, mean reward: 0.729 [0.681, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.513], loss: 0.001639, mae: 0.043925, mean_q: 1.184981
 32758/100000: episode: 558, duration: 0.265s, episode steps: 44, steps per second: 166, episode reward: 26.939, mean reward: 0.612 [0.503, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.782, 10.100], loss: 0.001893, mae: 0.045585, mean_q: 1.190133
 32785/100000: episode: 559, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 19.398, mean reward: 0.718 [0.664, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.311, 10.399], loss: 0.001835, mae: 0.045088, mean_q: 1.195758
 32829/100000: episode: 560, duration: 0.233s, episode steps: 44, steps per second: 189, episode reward: 27.667, mean reward: 0.629 [0.532, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.480, 10.206], loss: 0.002155, mae: 0.048439, mean_q: 1.191230
 32873/100000: episode: 561, duration: 0.422s, episode steps: 44, steps per second: 104, episode reward: 28.557, mean reward: 0.649 [0.519, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.581, 10.191], loss: 0.001906, mae: 0.046161, mean_q: 1.197138
 32913/100000: episode: 562, duration: 0.310s, episode steps: 40, steps per second: 129, episode reward: 28.067, mean reward: 0.702 [0.583, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.353, 10.287], loss: 0.002155, mae: 0.048897, mean_q: 1.188424
 32953/100000: episode: 563, duration: 0.415s, episode steps: 40, steps per second: 96, episode reward: 27.830, mean reward: 0.696 [0.568, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.338, 10.275], loss: 0.001720, mae: 0.043918, mean_q: 1.193816
 32993/100000: episode: 564, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 26.054, mean reward: 0.651 [0.576, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.057, 10.445], loss: 0.002006, mae: 0.047051, mean_q: 1.193384
 33023/100000: episode: 565, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 19.703, mean reward: 0.657 [0.572, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.553, 10.100], loss: 0.001740, mae: 0.045016, mean_q: 1.199355
 33067/100000: episode: 566, duration: 0.216s, episode steps: 44, steps per second: 204, episode reward: 27.313, mean reward: 0.621 [0.526, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.537, 10.100], loss: 0.001736, mae: 0.044383, mean_q: 1.199981
 33090/100000: episode: 567, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 15.760, mean reward: 0.685 [0.629, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.508, 10.389], loss: 0.001988, mae: 0.047817, mean_q: 1.207318
 33130/100000: episode: 568, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 27.071, mean reward: 0.677 [0.579, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.064, 10.282], loss: 0.001761, mae: 0.044578, mean_q: 1.201972
 33148/100000: episode: 569, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 12.221, mean reward: 0.679 [0.629, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.371], loss: 0.001598, mae: 0.042873, mean_q: 1.201381
 33170/100000: episode: 570, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 14.433, mean reward: 0.656 [0.610, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-2.144, 10.389], loss: 0.001962, mae: 0.046644, mean_q: 1.200671
 33193/100000: episode: 571, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 16.222, mean reward: 0.705 [0.652, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.246, 10.339], loss: 0.001599, mae: 0.043539, mean_q: 1.197614
 33228/100000: episode: 572, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 25.415, mean reward: 0.726 [0.624, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.257, 10.100], loss: 0.001908, mae: 0.046356, mean_q: 1.203457
 33272/100000: episode: 573, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 26.976, mean reward: 0.613 [0.532, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.138, 10.261], loss: 0.001827, mae: 0.045839, mean_q: 1.210348
 33316/100000: episode: 574, duration: 0.246s, episode steps: 44, steps per second: 179, episode reward: 28.368, mean reward: 0.645 [0.515, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.591, 10.344], loss: 0.002118, mae: 0.050049, mean_q: 1.203459
 33333/100000: episode: 575, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 12.119, mean reward: 0.713 [0.637, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.665, 10.100], loss: 0.001761, mae: 0.044941, mean_q: 1.204383
 33368/100000: episode: 576, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 25.254, mean reward: 0.722 [0.652, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-1.296, 10.100], loss: 0.001829, mae: 0.045465, mean_q: 1.210355
 33403/100000: episode: 577, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 24.164, mean reward: 0.690 [0.514, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.323, 10.100], loss: 0.001707, mae: 0.043144, mean_q: 1.214187
 33420/100000: episode: 578, duration: 0.121s, episode steps: 17, steps per second: 140, episode reward: 11.731, mean reward: 0.690 [0.647, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.399, 10.100], loss: 0.001579, mae: 0.043191, mean_q: 1.214997
 33464/100000: episode: 579, duration: 0.295s, episode steps: 44, steps per second: 149, episode reward: 27.386, mean reward: 0.622 [0.531, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.588, 10.290], loss: 0.001757, mae: 0.044329, mean_q: 1.203449
 33508/100000: episode: 580, duration: 0.246s, episode steps: 44, steps per second: 179, episode reward: 27.183, mean reward: 0.618 [0.512, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.234, 10.274], loss: 0.002076, mae: 0.047572, mean_q: 1.208255
 33548/100000: episode: 581, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 27.020, mean reward: 0.675 [0.634, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.337, 10.441], loss: 0.001695, mae: 0.044245, mean_q: 1.208722
 33560/100000: episode: 582, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 7.729, mean reward: 0.644 [0.624, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.349], loss: 0.002325, mae: 0.047719, mean_q: 1.205501
 33595/100000: episode: 583, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 26.352, mean reward: 0.753 [0.641, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-1.150, 10.100], loss: 0.001683, mae: 0.043865, mean_q: 1.208663
 33622/100000: episode: 584, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 17.411, mean reward: 0.645 [0.560, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.624, 10.208], loss: 0.001885, mae: 0.045450, mean_q: 1.217130
 33640/100000: episode: 585, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 11.672, mean reward: 0.648 [0.587, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.244, 10.333], loss: 0.001721, mae: 0.044446, mean_q: 1.215732
 33652/100000: episode: 586, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 8.245, mean reward: 0.687 [0.624, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.179, 10.450], loss: 0.001636, mae: 0.044029, mean_q: 1.215666
 33670/100000: episode: 587, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 13.205, mean reward: 0.734 [0.669, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.355, 10.555], loss: 0.001785, mae: 0.043684, mean_q: 1.206077
 33692/100000: episode: 588, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 14.544, mean reward: 0.661 [0.539, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.365, 10.306], loss: 0.001981, mae: 0.046285, mean_q: 1.207357
 33736/100000: episode: 589, duration: 0.286s, episode steps: 44, steps per second: 154, episode reward: 29.740, mean reward: 0.676 [0.591, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.215, 10.252], loss: 0.002040, mae: 0.047543, mean_q: 1.211529
 33748/100000: episode: 590, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 8.904, mean reward: 0.742 [0.681, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.599], loss: 0.001590, mae: 0.042105, mean_q: 1.223258
 33775/100000: episode: 591, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 16.534, mean reward: 0.612 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.545, 10.100], loss: 0.001498, mae: 0.042055, mean_q: 1.229188
 33805/100000: episode: 592, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 22.928, mean reward: 0.764 [0.705, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.347, 10.100], loss: 0.001823, mae: 0.045260, mean_q: 1.223867
 33840/100000: episode: 593, duration: 0.189s, episode steps: 35, steps per second: 186, episode reward: 23.721, mean reward: 0.678 [0.623, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.219, 10.100], loss: 0.001968, mae: 0.046148, mean_q: 1.217413
 33862/100000: episode: 594, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 14.390, mean reward: 0.654 [0.602, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.452, 10.333], loss: 0.001827, mae: 0.046281, mean_q: 1.216757
 33880/100000: episode: 595, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 14.035, mean reward: 0.780 [0.635, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.352, 10.602], loss: 0.002176, mae: 0.049017, mean_q: 1.208927
 33907/100000: episode: 596, duration: 0.162s, episode steps: 27, steps per second: 167, episode reward: 17.910, mean reward: 0.663 [0.597, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.703, 10.377], loss: 0.001869, mae: 0.045338, mean_q: 1.227018
 33929/100000: episode: 597, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 15.926, mean reward: 0.724 [0.660, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.345, 10.372], loss: 0.001804, mae: 0.044277, mean_q: 1.228609
 33947/100000: episode: 598, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 13.517, mean reward: 0.751 [0.673, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.680, 10.554], loss: 0.002144, mae: 0.047894, mean_q: 1.221035
 33987/100000: episode: 599, duration: 0.235s, episode steps: 40, steps per second: 170, episode reward: 25.016, mean reward: 0.625 [0.511, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-2.195, 10.166], loss: 0.001670, mae: 0.042832, mean_q: 1.221846
 34031/100000: episode: 600, duration: 0.277s, episode steps: 44, steps per second: 159, episode reward: 31.217, mean reward: 0.709 [0.632, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.964, 10.368], loss: 0.001598, mae: 0.042752, mean_q: 1.220957
 34043/100000: episode: 601, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 7.553, mean reward: 0.629 [0.589, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.408, 10.298], loss: 0.001727, mae: 0.044498, mean_q: 1.230360
 34065/100000: episode: 602, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 16.164, mean reward: 0.735 [0.659, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.616, 10.496], loss: 0.001831, mae: 0.047438, mean_q: 1.214052
 34087/100000: episode: 603, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 16.168, mean reward: 0.735 [0.677, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.887, 10.544], loss: 0.002185, mae: 0.051477, mean_q: 1.214908
 34099/100000: episode: 604, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 8.659, mean reward: 0.722 [0.669, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.521], loss: 0.001720, mae: 0.046456, mean_q: 1.243240
 34134/100000: episode: 605, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 25.769, mean reward: 0.736 [0.632, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.206, 10.100], loss: 0.001648, mae: 0.042716, mean_q: 1.230621
 34146/100000: episode: 606, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 8.456, mean reward: 0.705 [0.662, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.458], loss: 0.001683, mae: 0.045264, mean_q: 1.233560
 34181/100000: episode: 607, duration: 0.199s, episode steps: 35, steps per second: 175, episode reward: 27.693, mean reward: 0.791 [0.676, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.538, 10.100], loss: 0.001631, mae: 0.043697, mean_q: 1.229603
 34203/100000: episode: 608, duration: 0.131s, episode steps: 22, steps per second: 169, episode reward: 15.528, mean reward: 0.706 [0.641, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.479], loss: 0.001634, mae: 0.043751, mean_q: 1.237763
 34243/100000: episode: 609, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 25.662, mean reward: 0.642 [0.539, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.370, 10.123], loss: 0.001903, mae: 0.045981, mean_q: 1.235313
[Info] 2-TH LEVEL FOUND: 1.5606170892715454, Considering 10/90 traces
 34283/100000: episode: 610, duration: 4.480s, episode steps: 40, steps per second: 9, episode reward: 27.294, mean reward: 0.682 [0.560, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.444, 10.281], loss: 0.001759, mae: 0.044897, mean_q: 1.230673
 34300/100000: episode: 611, duration: 0.112s, episode steps: 17, steps per second: 152, episode reward: 12.035, mean reward: 0.708 [0.600, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.117, 10.100], loss: 0.001825, mae: 0.046438, mean_q: 1.240223
 34316/100000: episode: 612, duration: 0.104s, episode steps: 16, steps per second: 153, episode reward: 12.508, mean reward: 0.782 [0.681, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.500, 10.100], loss: 0.001660, mae: 0.043677, mean_q: 1.227613
 34339/100000: episode: 613, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 17.727, mean reward: 0.771 [0.642, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.188, 10.100], loss: 0.001482, mae: 0.042009, mean_q: 1.237556
 34366/100000: episode: 614, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 20.101, mean reward: 0.744 [0.684, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.252, 10.100], loss: 0.001752, mae: 0.043828, mean_q: 1.234689
 34393/100000: episode: 615, duration: 0.159s, episode steps: 27, steps per second: 169, episode reward: 19.792, mean reward: 0.733 [0.609, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.973, 10.100], loss: 0.002038, mae: 0.049439, mean_q: 1.238650
 34421/100000: episode: 616, duration: 0.157s, episode steps: 28, steps per second: 179, episode reward: 22.057, mean reward: 0.788 [0.706, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.353, 10.100], loss: 0.001651, mae: 0.043172, mean_q: 1.241598
 34444/100000: episode: 617, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 15.709, mean reward: 0.683 [0.550, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.751, 10.100], loss: 0.001984, mae: 0.047478, mean_q: 1.241477
 34465/100000: episode: 618, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 15.877, mean reward: 0.756 [0.665, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.295, 10.100], loss: 0.001724, mae: 0.045106, mean_q: 1.243410
 34473/100000: episode: 619, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 6.166, mean reward: 0.771 [0.732, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-1.259, 10.100], loss: 0.001782, mae: 0.046794, mean_q: 1.254401
 34500/100000: episode: 620, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 20.255, mean reward: 0.750 [0.660, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.270, 10.100], loss: 0.002280, mae: 0.049517, mean_q: 1.237759
 34523/100000: episode: 621, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 15.440, mean reward: 0.671 [0.575, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.104, 10.100], loss: 0.001783, mae: 0.044873, mean_q: 1.243800
 34550/100000: episode: 622, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 19.684, mean reward: 0.729 [0.634, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.335, 10.100], loss: 0.001536, mae: 0.041911, mean_q: 1.237671
 34578/100000: episode: 623, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 22.416, mean reward: 0.801 [0.731, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.652, 10.100], loss: 0.001898, mae: 0.046293, mean_q: 1.256792
 34604/100000: episode: 624, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 19.947, mean reward: 0.767 [0.692, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.544, 10.100], loss: 0.001782, mae: 0.045811, mean_q: 1.251492
 34627/100000: episode: 625, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 15.770, mean reward: 0.686 [0.613, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.295, 10.100], loss: 0.001886, mae: 0.045926, mean_q: 1.248951
 34635/100000: episode: 626, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 6.088, mean reward: 0.761 [0.716, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.641, 10.100], loss: 0.001699, mae: 0.044780, mean_q: 1.268321
 34659/100000: episode: 627, duration: 0.135s, episode steps: 24, steps per second: 177, episode reward: 18.064, mean reward: 0.753 [0.704, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.625, 10.100], loss: 0.002201, mae: 0.051696, mean_q: 1.245968
 34680/100000: episode: 628, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 16.567, mean reward: 0.789 [0.713, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.373, 10.100], loss: 0.002079, mae: 0.049306, mean_q: 1.251323
 34708/100000: episode: 629, duration: 0.169s, episode steps: 28, steps per second: 165, episode reward: 20.760, mean reward: 0.741 [0.628, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.215, 10.100], loss: 0.001846, mae: 0.045473, mean_q: 1.243432
 34729/100000: episode: 630, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 14.469, mean reward: 0.689 [0.573, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.124, 10.100], loss: 0.001971, mae: 0.046699, mean_q: 1.272038
 34753/100000: episode: 631, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 19.151, mean reward: 0.798 [0.727, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.947, 10.100], loss: 0.001794, mae: 0.044292, mean_q: 1.256557
 34769/100000: episode: 632, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 12.420, mean reward: 0.776 [0.676, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-1.206, 10.100], loss: 0.002164, mae: 0.049917, mean_q: 1.252493
 34790/100000: episode: 633, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 14.706, mean reward: 0.700 [0.564, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.230, 10.100], loss: 0.002207, mae: 0.049196, mean_q: 1.259994
 34811/100000: episode: 634, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 16.835, mean reward: 0.802 [0.750, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.402, 10.100], loss: 0.001716, mae: 0.044806, mean_q: 1.259669
 34838/100000: episode: 635, duration: 0.161s, episode steps: 27, steps per second: 167, episode reward: 20.298, mean reward: 0.752 [0.672, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.160, 10.100], loss: 0.001774, mae: 0.045077, mean_q: 1.268217
 34846/100000: episode: 636, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 6.050, mean reward: 0.756 [0.646, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.356, 10.100], loss: 0.001699, mae: 0.043765, mean_q: 1.285842
 34862/100000: episode: 637, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 10.898, mean reward: 0.681 [0.597, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.161, 10.100], loss: 0.001822, mae: 0.045169, mean_q: 1.262827
 34885/100000: episode: 638, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 17.798, mean reward: 0.774 [0.713, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.527, 10.100], loss: 0.002116, mae: 0.048675, mean_q: 1.270991
 34901/100000: episode: 639, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 11.480, mean reward: 0.718 [0.659, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.771, 10.100], loss: 0.002090, mae: 0.050150, mean_q: 1.265979
 34909/100000: episode: 640, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 6.311, mean reward: 0.789 [0.740, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.311, 10.100], loss: 0.001675, mae: 0.042710, mean_q: 1.274728
 34935/100000: episode: 641, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 21.186, mean reward: 0.815 [0.742, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.927, 10.100], loss: 0.001857, mae: 0.046107, mean_q: 1.272929
 34943/100000: episode: 642, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 6.206, mean reward: 0.776 [0.718, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.232, 10.100], loss: 0.001783, mae: 0.044657, mean_q: 1.273708
 34959/100000: episode: 643, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 11.180, mean reward: 0.699 [0.627, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.529, 10.100], loss: 0.001467, mae: 0.040372, mean_q: 1.280009
 34982/100000: episode: 644, duration: 0.142s, episode steps: 23, steps per second: 163, episode reward: 17.247, mean reward: 0.750 [0.611, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.287, 10.100], loss: 0.001415, mae: 0.041127, mean_q: 1.282667
 35008/100000: episode: 645, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 19.085, mean reward: 0.734 [0.623, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.581, 10.100], loss: 0.001904, mae: 0.044143, mean_q: 1.274745
 35016/100000: episode: 646, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 6.699, mean reward: 0.837 [0.772, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.488, 10.100], loss: 0.001450, mae: 0.041949, mean_q: 1.276171
 35032/100000: episode: 647, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 12.250, mean reward: 0.766 [0.693, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.382, 10.100], loss: 0.001827, mae: 0.044064, mean_q: 1.272343
 35055/100000: episode: 648, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 16.279, mean reward: 0.708 [0.577, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.165, 10.100], loss: 0.001797, mae: 0.044342, mean_q: 1.278426
 35082/100000: episode: 649, duration: 0.132s, episode steps: 27, steps per second: 204, episode reward: 20.688, mean reward: 0.766 [0.691, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.538, 10.100], loss: 0.001770, mae: 0.045020, mean_q: 1.275130
 35099/100000: episode: 650, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 12.507, mean reward: 0.736 [0.665, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.077, 10.100], loss: 0.001677, mae: 0.043983, mean_q: 1.272436
 35123/100000: episode: 651, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 18.525, mean reward: 0.772 [0.712, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.345, 10.100], loss: 0.001946, mae: 0.047634, mean_q: 1.278925
 35146/100000: episode: 652, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 16.364, mean reward: 0.711 [0.621, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.668, 10.100], loss: 0.001615, mae: 0.043350, mean_q: 1.275708
 35154/100000: episode: 653, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 6.308, mean reward: 0.789 [0.748, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.374, 10.100], loss: 0.001900, mae: 0.047219, mean_q: 1.283245
 35181/100000: episode: 654, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 20.344, mean reward: 0.753 [0.683, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.269, 10.100], loss: 0.002036, mae: 0.047130, mean_q: 1.285958
 35204/100000: episode: 655, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 17.477, mean reward: 0.760 [0.711, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.234, 10.100], loss: 0.002184, mae: 0.049592, mean_q: 1.293481
 35227/100000: episode: 656, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 16.159, mean reward: 0.703 [0.643, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.588, 10.100], loss: 0.001784, mae: 0.046998, mean_q: 1.289299
 35250/100000: episode: 657, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 18.347, mean reward: 0.798 [0.711, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.538, 10.100], loss: 0.002223, mae: 0.051443, mean_q: 1.293666
 35266/100000: episode: 658, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 11.924, mean reward: 0.745 [0.714, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.491, 10.100], loss: 0.001498, mae: 0.043963, mean_q: 1.280207
 35282/100000: episode: 659, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.507, mean reward: 0.719 [0.677, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.258, 10.100], loss: 0.001568, mae: 0.042178, mean_q: 1.276065
 35290/100000: episode: 660, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 6.220, mean reward: 0.778 [0.731, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.321, 10.100], loss: 0.001619, mae: 0.043722, mean_q: 1.314309
 35298/100000: episode: 661, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 5.999, mean reward: 0.750 [0.714, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.331, 10.100], loss: 0.001712, mae: 0.043944, mean_q: 1.277761
 35322/100000: episode: 662, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 20.536, mean reward: 0.856 [0.770, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.377, 10.100], loss: 0.001725, mae: 0.044675, mean_q: 1.296681
 35343/100000: episode: 663, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 16.619, mean reward: 0.791 [0.731, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.337, 10.100], loss: 0.001597, mae: 0.044373, mean_q: 1.299571
 35364/100000: episode: 664, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 16.727, mean reward: 0.797 [0.669, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.359, 10.100], loss: 0.001755, mae: 0.046087, mean_q: 1.291743
 35372/100000: episode: 665, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 6.261, mean reward: 0.783 [0.720, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.370, 10.100], loss: 0.001555, mae: 0.042771, mean_q: 1.291511
 35400/100000: episode: 666, duration: 0.158s, episode steps: 28, steps per second: 178, episode reward: 24.282, mean reward: 0.867 [0.719, 0.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.442, 10.100], loss: 0.001829, mae: 0.045086, mean_q: 1.308517
 35416/100000: episode: 667, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 11.827, mean reward: 0.739 [0.684, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.322, 10.100], loss: 0.001825, mae: 0.045863, mean_q: 1.298292
 35432/100000: episode: 668, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 11.652, mean reward: 0.728 [0.664, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.264, 10.100], loss: 0.001892, mae: 0.047218, mean_q: 1.313328
 35440/100000: episode: 669, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 6.942, mean reward: 0.868 [0.834, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.482, 10.100], loss: 0.001474, mae: 0.040838, mean_q: 1.299078
 35463/100000: episode: 670, duration: 0.136s, episode steps: 23, steps per second: 170, episode reward: 16.566, mean reward: 0.720 [0.661, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.131, 10.100], loss: 0.002013, mae: 0.045948, mean_q: 1.301584
 35486/100000: episode: 671, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 15.306, mean reward: 0.665 [0.596, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.280, 10.100], loss: 0.002471, mae: 0.053912, mean_q: 1.306989
 35512/100000: episode: 672, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 20.420, mean reward: 0.785 [0.720, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.782, 10.100], loss: 0.001913, mae: 0.047477, mean_q: 1.306767
 35528/100000: episode: 673, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 11.381, mean reward: 0.711 [0.605, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.380, 10.100], loss: 0.001824, mae: 0.046751, mean_q: 1.285142
 35555/100000: episode: 674, duration: 0.150s, episode steps: 27, steps per second: 179, episode reward: 20.301, mean reward: 0.752 [0.704, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.498, 10.100], loss: 0.001705, mae: 0.045430, mean_q: 1.306554
 35563/100000: episode: 675, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 6.472, mean reward: 0.809 [0.753, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.589, 10.100], loss: 0.001537, mae: 0.041659, mean_q: 1.300487
 35589/100000: episode: 676, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 20.022, mean reward: 0.770 [0.694, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.406, 10.100], loss: 0.001852, mae: 0.047162, mean_q: 1.311477
 35615/100000: episode: 677, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 18.466, mean reward: 0.710 [0.656, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.343, 10.100], loss: 0.001801, mae: 0.045454, mean_q: 1.305853
 35641/100000: episode: 678, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 18.940, mean reward: 0.728 [0.632, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.426, 10.100], loss: 0.001674, mae: 0.043501, mean_q: 1.313071
 35667/100000: episode: 679, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 17.856, mean reward: 0.687 [0.618, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.268, 10.100], loss: 0.002075, mae: 0.049502, mean_q: 1.316445
 35694/100000: episode: 680, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 22.341, mean reward: 0.827 [0.726, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.466, 10.100], loss: 0.001717, mae: 0.045334, mean_q: 1.321035
 35702/100000: episode: 681, duration: 0.059s, episode steps: 8, steps per second: 136, episode reward: 6.623, mean reward: 0.828 [0.736, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.769, 10.100], loss: 0.001841, mae: 0.046637, mean_q: 1.327233
 35726/100000: episode: 682, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 17.899, mean reward: 0.746 [0.685, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.320, 10.100], loss: 0.001639, mae: 0.042709, mean_q: 1.336810
 35754/100000: episode: 683, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 20.663, mean reward: 0.738 [0.694, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.700, 10.100], loss: 0.001765, mae: 0.045873, mean_q: 1.323910
 35762/100000: episode: 684, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 6.500, mean reward: 0.812 [0.676, 0.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.378, 10.100], loss: 0.001369, mae: 0.041072, mean_q: 1.317841
 35790/100000: episode: 685, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 18.954, mean reward: 0.677 [0.618, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.297, 10.100], loss: 0.001736, mae: 0.044896, mean_q: 1.320450
 35811/100000: episode: 686, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 16.851, mean reward: 0.802 [0.738, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.407, 10.100], loss: 0.001718, mae: 0.044525, mean_q: 1.338128
 35839/100000: episode: 687, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 20.321, mean reward: 0.726 [0.658, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.515, 10.100], loss: 0.002143, mae: 0.047430, mean_q: 1.326150
 35865/100000: episode: 688, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 18.759, mean reward: 0.722 [0.614, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.586, 10.100], loss: 0.001749, mae: 0.045995, mean_q: 1.316844
 35882/100000: episode: 689, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 11.088, mean reward: 0.652 [0.499, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.165], loss: 0.001910, mae: 0.048824, mean_q: 1.336221
 35890/100000: episode: 690, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 6.095, mean reward: 0.762 [0.722, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.330, 10.100], loss: 0.001481, mae: 0.041773, mean_q: 1.311912
 35913/100000: episode: 691, duration: 0.127s, episode steps: 23, steps per second: 180, episode reward: 16.639, mean reward: 0.723 [0.604, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.515, 10.100], loss: 0.001638, mae: 0.045275, mean_q: 1.345204
 35934/100000: episode: 692, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 17.050, mean reward: 0.812 [0.664, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.544, 10.100], loss: 0.002022, mae: 0.046692, mean_q: 1.336679
 35962/100000: episode: 693, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 21.372, mean reward: 0.763 [0.710, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.267, 10.100], loss: 0.001578, mae: 0.043406, mean_q: 1.339372
 35970/100000: episode: 694, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 6.044, mean reward: 0.755 [0.670, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.323, 10.100], loss: 0.001913, mae: 0.044436, mean_q: 1.317378
 35993/100000: episode: 695, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 17.352, mean reward: 0.754 [0.636, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.149, 10.100], loss: 0.001935, mae: 0.046920, mean_q: 1.337127
 36014/100000: episode: 696, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 16.035, mean reward: 0.764 [0.637, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.200, 10.100], loss: 0.001633, mae: 0.044429, mean_q: 1.333539
 36022/100000: episode: 697, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 6.295, mean reward: 0.787 [0.741, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.256, 10.100], loss: 0.001670, mae: 0.043999, mean_q: 1.332280
 36048/100000: episode: 698, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 19.235, mean reward: 0.740 [0.656, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.444, 10.100], loss: 0.001567, mae: 0.042478, mean_q: 1.340784
 36065/100000: episode: 699, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 13.117, mean reward: 0.772 [0.697, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.582, 10.100], loss: 0.001568, mae: 0.043947, mean_q: 1.332101
[Info] 3-TH LEVEL FOUND: 1.6390002965927124, Considering 10/90 traces
 36089/100000: episode: 700, duration: 4.321s, episode steps: 24, steps per second: 6, episode reward: 18.290, mean reward: 0.762 [0.670, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.676, 10.100], loss: 0.001724, mae: 0.044990, mean_q: 1.343280
 36109/100000: episode: 701, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 17.308, mean reward: 0.865 [0.815, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.448, 10.100], loss: 0.001340, mae: 0.040743, mean_q: 1.330623
 36120/100000: episode: 702, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 9.488, mean reward: 0.863 [0.816, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.521, 10.100], loss: 0.001389, mae: 0.041901, mean_q: 1.339087
 36142/100000: episode: 703, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 18.614, mean reward: 0.846 [0.759, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.964, 10.100], loss: 0.001928, mae: 0.047397, mean_q: 1.325525
 36161/100000: episode: 704, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 15.943, mean reward: 0.839 [0.731, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.510, 10.100], loss: 0.001680, mae: 0.045676, mean_q: 1.335016
 36176/100000: episode: 705, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 13.607, mean reward: 0.907 [0.857, 0.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.373, 10.100], loss: 0.001922, mae: 0.048103, mean_q: 1.335070
 36198/100000: episode: 706, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 17.502, mean reward: 0.796 [0.706, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.372, 10.100], loss: 0.001773, mae: 0.044673, mean_q: 1.348751
 36220/100000: episode: 707, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 17.425, mean reward: 0.792 [0.727, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.472, 10.100], loss: 0.001647, mae: 0.044657, mean_q: 1.348917
 36238/100000: episode: 708, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 13.366, mean reward: 0.743 [0.705, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.307, 10.100], loss: 0.001747, mae: 0.046006, mean_q: 1.353079
 36253/100000: episode: 709, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 12.766, mean reward: 0.851 [0.808, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.494, 10.100], loss: 0.001641, mae: 0.044598, mean_q: 1.359194
 36275/100000: episode: 710, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 19.079, mean reward: 0.867 [0.770, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.551, 10.100], loss: 0.001472, mae: 0.042625, mean_q: 1.344475
 36293/100000: episode: 711, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 14.250, mean reward: 0.792 [0.694, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.438, 10.100], loss: 0.001657, mae: 0.044717, mean_q: 1.362738
 36312/100000: episode: 712, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 14.419, mean reward: 0.759 [0.711, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.384, 10.100], loss: 0.001570, mae: 0.043513, mean_q: 1.356164
 36334/100000: episode: 713, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 16.333, mean reward: 0.742 [0.638, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.280, 10.100], loss: 0.001790, mae: 0.045753, mean_q: 1.348170
 36345/100000: episode: 714, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 9.220, mean reward: 0.838 [0.809, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.490, 10.100], loss: 0.001508, mae: 0.043392, mean_q: 1.373040
 36356/100000: episode: 715, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 9.025, mean reward: 0.820 [0.685, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.408, 10.100], loss: 0.001276, mae: 0.038408, mean_q: 1.366457
 36374/100000: episode: 716, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 15.307, mean reward: 0.850 [0.758, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.383, 10.100], loss: 0.002009, mae: 0.047585, mean_q: 1.360204
 36389/100000: episode: 717, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 13.511, mean reward: 0.901 [0.863, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.413, 10.100], loss: 0.001331, mae: 0.039821, mean_q: 1.380599
 36411/100000: episode: 718, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 18.830, mean reward: 0.856 [0.771, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.916, 10.100], loss: 0.001470, mae: 0.042965, mean_q: 1.364260
 36432/100000: episode: 719, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 15.353, mean reward: 0.731 [0.647, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.012, 10.100], loss: 0.001600, mae: 0.044498, mean_q: 1.378003
 36454/100000: episode: 720, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 17.855, mean reward: 0.812 [0.697, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.340, 10.100], loss: 0.001847, mae: 0.047321, mean_q: 1.349509
 36475/100000: episode: 721, duration: 0.125s, episode steps: 21, steps per second: 169, episode reward: 16.246, mean reward: 0.774 [0.649, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.342, 10.100], loss: 0.001565, mae: 0.044017, mean_q: 1.368967
 36493/100000: episode: 722, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 14.799, mean reward: 0.822 [0.789, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.702, 10.100], loss: 0.001736, mae: 0.046640, mean_q: 1.378721
 36508/100000: episode: 723, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 12.836, mean reward: 0.856 [0.763, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.383, 10.100], loss: 0.001632, mae: 0.044890, mean_q: 1.370163
 36528/100000: episode: 724, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 16.288, mean reward: 0.814 [0.746, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.312, 10.100], loss: 0.001697, mae: 0.045182, mean_q: 1.383278
 36550/100000: episode: 725, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 16.262, mean reward: 0.739 [0.658, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.408, 10.100], loss: 0.001789, mae: 0.047716, mean_q: 1.376362
 36561/100000: episode: 726, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 8.873, mean reward: 0.807 [0.739, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.441, 10.100], loss: 0.001551, mae: 0.043551, mean_q: 1.367675
 36576/100000: episode: 727, duration: 0.102s, episode steps: 15, steps per second: 147, episode reward: 12.687, mean reward: 0.846 [0.805, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.446, 10.100], loss: 0.002089, mae: 0.049595, mean_q: 1.375259
 36587/100000: episode: 728, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 9.338, mean reward: 0.849 [0.799, 0.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.554, 10.100], loss: 0.001823, mae: 0.046712, mean_q: 1.377664
 36605/100000: episode: 729, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 13.836, mean reward: 0.769 [0.688, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.932, 10.100], loss: 0.001915, mae: 0.047644, mean_q: 1.376604
 36620/100000: episode: 730, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 12.101, mean reward: 0.807 [0.750, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.263, 10.100], loss: 0.001362, mae: 0.042140, mean_q: 1.376255
 36640/100000: episode: 731, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 14.989, mean reward: 0.749 [0.623, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.231, 10.100], loss: 0.001829, mae: 0.048584, mean_q: 1.374217
 36658/100000: episode: 732, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 15.305, mean reward: 0.850 [0.794, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.737, 10.100], loss: 0.001717, mae: 0.046788, mean_q: 1.371344
 36673/100000: episode: 733, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 12.229, mean reward: 0.815 [0.722, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.236, 10.100], loss: 0.001510, mae: 0.043365, mean_q: 1.379167
 36691/100000: episode: 734, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 15.608, mean reward: 0.867 [0.790, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.341, 10.100], loss: 0.001525, mae: 0.043900, mean_q: 1.374773
[Info] FALSIFICATION!
 36695/100000: episode: 735, duration: 0.195s, episode steps: 4, steps per second: 21, episode reward: 3.577, mean reward: 0.894 [0.810, 1.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.037, 9.629], loss: 0.001792, mae: 0.045968, mean_q: 1.374566
 36714/100000: episode: 736, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 16.228, mean reward: 0.854 [0.771, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.217, 10.100], loss: 0.002123, mae: 0.051939, mean_q: 1.377270
 36733/100000: episode: 737, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 17.042, mean reward: 0.897 [0.811, 0.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.650, 10.100], loss: 0.001809, mae: 0.047246, mean_q: 1.380003
 36755/100000: episode: 738, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 17.561, mean reward: 0.798 [0.698, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.486, 10.100], loss: 0.001526, mae: 0.043101, mean_q: 1.377105
 36777/100000: episode: 739, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 18.718, mean reward: 0.851 [0.784, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.306, 10.100], loss: 0.001511, mae: 0.042871, mean_q: 1.379048
[Info] FALSIFICATION!
 36790/100000: episode: 740, duration: 0.251s, episode steps: 13, steps per second: 52, episode reward: 11.756, mean reward: 0.904 [0.822, 1.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.775, 10.093], loss: 0.002914, mae: 0.048646, mean_q: 1.380420
 36812/100000: episode: 741, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 18.893, mean reward: 0.859 [0.720, 0.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.402, 10.100], loss: 0.001718, mae: 0.046233, mean_q: 1.383021
 36823/100000: episode: 742, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 9.552, mean reward: 0.868 [0.819, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.324, 10.100], loss: 0.001592, mae: 0.044481, mean_q: 1.397779
 36845/100000: episode: 743, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 19.168, mean reward: 0.871 [0.788, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.293, 10.100], loss: 0.001508, mae: 0.043431, mean_q: 1.390460
 36867/100000: episode: 744, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 16.482, mean reward: 0.749 [0.681, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.404, 10.100], loss: 0.001478, mae: 0.041955, mean_q: 1.400833
 36889/100000: episode: 745, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 16.396, mean reward: 0.745 [0.621, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.687, 10.100], loss: 0.001562, mae: 0.043886, mean_q: 1.392460
 36910/100000: episode: 746, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 15.918, mean reward: 0.758 [0.631, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.239, 10.100], loss: 0.001377, mae: 0.041333, mean_q: 1.389203
 36932/100000: episode: 747, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 15.868, mean reward: 0.721 [0.604, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.317, 10.100], loss: 0.001899, mae: 0.044745, mean_q: 1.400263
 36950/100000: episode: 748, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 14.339, mean reward: 0.797 [0.665, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.534, 10.100], loss: 0.001536, mae: 0.042657, mean_q: 1.388936
 36965/100000: episode: 749, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 13.152, mean reward: 0.877 [0.826, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.304, 10.100], loss: 0.002041, mae: 0.049599, mean_q: 1.404212
 36980/100000: episode: 750, duration: 0.080s, episode steps: 15, steps per second: 186, episode reward: 12.828, mean reward: 0.855 [0.727, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.105, 10.100], loss: 0.001567, mae: 0.044436, mean_q: 1.408880
[Info] FALSIFICATION!
 36989/100000: episode: 751, duration: 0.221s, episode steps: 9, steps per second: 41, episode reward: 7.568, mean reward: 0.841 [0.770, 1.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.084, 9.911], loss: 0.001496, mae: 0.043713, mean_q: 1.385799
 37008/100000: episode: 752, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 16.245, mean reward: 0.855 [0.746, 0.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.253, 10.100], loss: 0.001763, mae: 0.046432, mean_q: 1.386552
 37026/100000: episode: 753, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 14.778, mean reward: 0.821 [0.688, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.368, 10.100], loss: 0.001849, mae: 0.048494, mean_q: 1.413461
 37044/100000: episode: 754, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 13.997, mean reward: 0.778 [0.694, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.784, 10.100], loss: 0.002408, mae: 0.049074, mean_q: 1.373920
 37065/100000: episode: 755, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 17.138, mean reward: 0.816 [0.779, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.506, 10.100], loss: 0.001834, mae: 0.047362, mean_q: 1.408971
 37083/100000: episode: 756, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 14.937, mean reward: 0.830 [0.726, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.712, 10.100], loss: 0.001307, mae: 0.039826, mean_q: 1.410551
 37094/100000: episode: 757, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 8.707, mean reward: 0.792 [0.719, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.576, 10.100], loss: 0.001464, mae: 0.042013, mean_q: 1.406682
 37109/100000: episode: 758, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 13.168, mean reward: 0.878 [0.835, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.481, 10.100], loss: 0.001983, mae: 0.047605, mean_q: 1.396906
 37128/100000: episode: 759, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 15.921, mean reward: 0.838 [0.796, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.265, 10.100], loss: 0.002252, mae: 0.049756, mean_q: 1.398035
 37146/100000: episode: 760, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 13.650, mean reward: 0.758 [0.687, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.455, 10.100], loss: 0.002289, mae: 0.053552, mean_q: 1.408555
 37168/100000: episode: 761, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 16.745, mean reward: 0.761 [0.603, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.303, 10.100], loss: 0.002364, mae: 0.048056, mean_q: 1.415220
 37190/100000: episode: 762, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 18.367, mean reward: 0.835 [0.735, 0.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.466, 10.100], loss: 0.001873, mae: 0.047407, mean_q: 1.413614
 37208/100000: episode: 763, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 15.690, mean reward: 0.872 [0.796, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.364, 10.100], loss: 0.002249, mae: 0.050071, mean_q: 1.400963
 37228/100000: episode: 764, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 17.180, mean reward: 0.859 [0.774, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.406, 10.100], loss: 0.001603, mae: 0.044538, mean_q: 1.407273
 37250/100000: episode: 765, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 18.221, mean reward: 0.828 [0.736, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.269, 10.100], loss: 0.002040, mae: 0.050012, mean_q: 1.394698
 37272/100000: episode: 766, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 18.770, mean reward: 0.853 [0.751, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.413, 10.100], loss: 0.002372, mae: 0.052181, mean_q: 1.411827
 37294/100000: episode: 767, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 17.227, mean reward: 0.783 [0.692, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.220, 10.100], loss: 0.001630, mae: 0.044881, mean_q: 1.425231
 37313/100000: episode: 768, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 14.832, mean reward: 0.781 [0.760, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.575, 10.100], loss: 0.001478, mae: 0.041719, mean_q: 1.408626
 37334/100000: episode: 769, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 16.873, mean reward: 0.803 [0.708, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.919, 10.100], loss: 0.001649, mae: 0.045107, mean_q: 1.430781
 37352/100000: episode: 770, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 16.113, mean reward: 0.895 [0.772, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.649, 10.100], loss: 0.001647, mae: 0.044795, mean_q: 1.421525
 37371/100000: episode: 771, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 15.956, mean reward: 0.840 [0.762, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.422, 10.100], loss: 0.002066, mae: 0.049551, mean_q: 1.432115
 37389/100000: episode: 772, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 14.209, mean reward: 0.789 [0.734, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.330, 10.100], loss: 0.001650, mae: 0.043894, mean_q: 1.418317
 37407/100000: episode: 773, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 15.358, mean reward: 0.853 [0.807, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.446, 10.100], loss: 0.001687, mae: 0.044833, mean_q: 1.409078
 37425/100000: episode: 774, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 14.058, mean reward: 0.781 [0.692, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.611, 10.100], loss: 0.001605, mae: 0.045033, mean_q: 1.426341
[Info] FALSIFICATION!
 37441/100000: episode: 775, duration: 0.304s, episode steps: 16, steps per second: 53, episode reward: 13.487, mean reward: 0.843 [0.790, 1.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.160, 10.093], loss: 0.001869, mae: 0.044216, mean_q: 1.429368
 37459/100000: episode: 776, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 14.948, mean reward: 0.830 [0.781, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.403, 10.100], loss: 0.001600, mae: 0.044956, mean_q: 1.421558
 37481/100000: episode: 777, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 16.218, mean reward: 0.737 [0.640, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.360, 10.100], loss: 0.001410, mae: 0.041866, mean_q: 1.425082
 37499/100000: episode: 778, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 16.443, mean reward: 0.913 [0.803, 0.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.592, 10.100], loss: 0.001425, mae: 0.042874, mean_q: 1.428396
 37521/100000: episode: 779, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 19.294, mean reward: 0.877 [0.830, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.733, 10.100], loss: 0.001727, mae: 0.042232, mean_q: 1.422356
 37539/100000: episode: 780, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 14.200, mean reward: 0.789 [0.737, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.380, 10.100], loss: 0.001934, mae: 0.046869, mean_q: 1.434095
 37559/100000: episode: 781, duration: 0.115s, episode steps: 20, steps per second: 175, episode reward: 16.846, mean reward: 0.842 [0.753, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.083, 10.100], loss: 0.001787, mae: 0.047506, mean_q: 1.422076
 37574/100000: episode: 782, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 12.117, mean reward: 0.808 [0.684, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.450, 10.100], loss: 0.001528, mae: 0.044185, mean_q: 1.430324
 37596/100000: episode: 783, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 16.399, mean reward: 0.745 [0.662, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.679, 10.100], loss: 0.002111, mae: 0.050407, mean_q: 1.437849
 37614/100000: episode: 784, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 14.911, mean reward: 0.828 [0.674, 0.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.473, 10.100], loss: 0.002280, mae: 0.050953, mean_q: 1.426246
 37636/100000: episode: 785, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 16.587, mean reward: 0.754 [0.718, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.281, 10.100], loss: 0.001821, mae: 0.044333, mean_q: 1.426227
 37655/100000: episode: 786, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 15.598, mean reward: 0.821 [0.740, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.167, 10.100], loss: 0.001725, mae: 0.046425, mean_q: 1.424252
 37677/100000: episode: 787, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 16.628, mean reward: 0.756 [0.675, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.302, 10.100], loss: 0.001328, mae: 0.040982, mean_q: 1.423856
 37695/100000: episode: 788, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 14.243, mean reward: 0.791 [0.709, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.479, 10.100], loss: 0.001557, mae: 0.044313, mean_q: 1.440607
 37714/100000: episode: 789, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 14.285, mean reward: 0.752 [0.612, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.320, 10.100], loss: 0.001483, mae: 0.043712, mean_q: 1.445559
[Info] Complete ISplit Iteration
[Info] Levels: [1.3631138, 1.5606171, 1.6390003, 1.7067994]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.49]
[Info] Error Prob: 0.0004900000000000001

 37736/100000: episode: 790, duration: 4.601s, episode steps: 22, steps per second: 5, episode reward: 18.035, mean reward: 0.820 [0.732, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.343, 10.100], loss: 0.001753, mae: 0.045740, mean_q: 1.427149
 37836/100000: episode: 791, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 61.189, mean reward: 0.612 [0.508, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.366, 10.326], loss: 0.001570, mae: 0.042770, mean_q: 1.434112
 37936/100000: episode: 792, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 56.686, mean reward: 0.567 [0.501, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.335, 10.104], loss: 0.001924, mae: 0.046998, mean_q: 1.435286
 38036/100000: episode: 793, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.445, mean reward: 0.574 [0.508, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.901, 10.098], loss: 0.001817, mae: 0.045895, mean_q: 1.429965
 38136/100000: episode: 794, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 61.982, mean reward: 0.620 [0.504, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.349, 10.336], loss: 0.001700, mae: 0.044447, mean_q: 1.419906
 38236/100000: episode: 795, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 60.147, mean reward: 0.601 [0.505, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.305, 10.253], loss: 0.001690, mae: 0.044744, mean_q: 1.423510
 38336/100000: episode: 796, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 60.277, mean reward: 0.603 [0.507, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.104, 10.121], loss: 0.001663, mae: 0.044588, mean_q: 1.425893
 38436/100000: episode: 797, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 57.569, mean reward: 0.576 [0.509, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.560, 10.139], loss: 0.001871, mae: 0.044964, mean_q: 1.426750
 38536/100000: episode: 798, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.713, mean reward: 0.587 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.065, 10.234], loss: 0.001981, mae: 0.046840, mean_q: 1.420424
 38636/100000: episode: 799, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: 61.616, mean reward: 0.616 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.436, 10.290], loss: 0.001731, mae: 0.043350, mean_q: 1.414541
 38736/100000: episode: 800, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.251, mean reward: 0.593 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.312, 10.329], loss: 0.001712, mae: 0.043851, mean_q: 1.409395
 38836/100000: episode: 801, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 61.251, mean reward: 0.613 [0.516, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.428, 10.098], loss: 0.001793, mae: 0.045621, mean_q: 1.406599
 38936/100000: episode: 802, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 59.788, mean reward: 0.598 [0.504, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.640, 10.269], loss: 0.001719, mae: 0.044677, mean_q: 1.406841
 39036/100000: episode: 803, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 58.633, mean reward: 0.586 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.785, 10.162], loss: 0.001715, mae: 0.044618, mean_q: 1.401245
 39136/100000: episode: 804, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.792, mean reward: 0.598 [0.514, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.150, 10.130], loss: 0.002136, mae: 0.047195, mean_q: 1.395782
 39236/100000: episode: 805, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 59.238, mean reward: 0.592 [0.505, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.700, 10.259], loss: 0.001820, mae: 0.045730, mean_q: 1.391602
 39336/100000: episode: 806, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.144, mean reward: 0.591 [0.507, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.377, 10.111], loss: 0.001978, mae: 0.046503, mean_q: 1.386652
 39436/100000: episode: 807, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.558, mean reward: 0.596 [0.510, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.335, 10.098], loss: 0.001949, mae: 0.046218, mean_q: 1.386290
 39536/100000: episode: 808, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.062, mean reward: 0.591 [0.509, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.401, 10.165], loss: 0.001736, mae: 0.045161, mean_q: 1.380539
 39636/100000: episode: 809, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 62.376, mean reward: 0.624 [0.512, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.311, 10.098], loss: 0.001708, mae: 0.043686, mean_q: 1.375948
 39736/100000: episode: 810, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.947, mean reward: 0.599 [0.515, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.239, 10.382], loss: 0.001999, mae: 0.047051, mean_q: 1.370139
 39836/100000: episode: 811, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.070, mean reward: 0.591 [0.507, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.491, 10.376], loss: 0.001874, mae: 0.045217, mean_q: 1.359160
 39936/100000: episode: 812, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.809, mean reward: 0.578 [0.505, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.166, 10.191], loss: 0.001696, mae: 0.044567, mean_q: 1.355792
 40036/100000: episode: 813, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.438, mean reward: 0.584 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.347, 10.110], loss: 0.001481, mae: 0.041813, mean_q: 1.357056
 40136/100000: episode: 814, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 58.651, mean reward: 0.587 [0.511, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.783, 10.128], loss: 0.001776, mae: 0.044304, mean_q: 1.341054
 40236/100000: episode: 815, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.253, mean reward: 0.593 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.876, 10.098], loss: 0.001556, mae: 0.042688, mean_q: 1.340923
 40336/100000: episode: 816, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 58.398, mean reward: 0.584 [0.506, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.868, 10.254], loss: 0.001893, mae: 0.046003, mean_q: 1.330285
 40436/100000: episode: 817, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.366, mean reward: 0.584 [0.512, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.000, 10.134], loss: 0.001737, mae: 0.044148, mean_q: 1.327502
 40536/100000: episode: 818, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.158, mean reward: 0.582 [0.499, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.756, 10.122], loss: 0.002066, mae: 0.045135, mean_q: 1.317976
 40636/100000: episode: 819, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 57.686, mean reward: 0.577 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.776, 10.124], loss: 0.001757, mae: 0.044692, mean_q: 1.317757
 40736/100000: episode: 820, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 60.079, mean reward: 0.601 [0.513, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.876, 10.263], loss: 0.001719, mae: 0.043136, mean_q: 1.310745
 40836/100000: episode: 821, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 65.892, mean reward: 0.659 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.232, 10.098], loss: 0.001634, mae: 0.044154, mean_q: 1.308864
 40936/100000: episode: 822, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 60.024, mean reward: 0.600 [0.506, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.777, 10.098], loss: 0.001671, mae: 0.044989, mean_q: 1.297190
 41036/100000: episode: 823, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.113, mean reward: 0.591 [0.509, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.664, 10.098], loss: 0.001754, mae: 0.045399, mean_q: 1.293464
 41136/100000: episode: 824, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.884, mean reward: 0.599 [0.511, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.702, 10.098], loss: 0.001521, mae: 0.042654, mean_q: 1.294554
 41236/100000: episode: 825, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.181, mean reward: 0.592 [0.502, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.732, 10.227], loss: 0.001651, mae: 0.043512, mean_q: 1.285780
 41336/100000: episode: 826, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 59.167, mean reward: 0.592 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.348, 10.098], loss: 0.001995, mae: 0.045649, mean_q: 1.274630
 41436/100000: episode: 827, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 61.139, mean reward: 0.611 [0.497, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.942, 10.098], loss: 0.001804, mae: 0.044653, mean_q: 1.271460
 41536/100000: episode: 828, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 61.810, mean reward: 0.618 [0.518, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.825, 10.098], loss: 0.001670, mae: 0.043032, mean_q: 1.267627
 41636/100000: episode: 829, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 60.029, mean reward: 0.600 [0.521, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.954, 10.280], loss: 0.001604, mae: 0.044301, mean_q: 1.259439
 41736/100000: episode: 830, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.786, mean reward: 0.588 [0.513, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.645, 10.098], loss: 0.001718, mae: 0.044301, mean_q: 1.246980
 41836/100000: episode: 831, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.087, mean reward: 0.591 [0.505, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.219, 10.098], loss: 0.001493, mae: 0.042512, mean_q: 1.245733
 41936/100000: episode: 832, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 56.690, mean reward: 0.567 [0.498, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.210, 10.098], loss: 0.001642, mae: 0.044156, mean_q: 1.237041
 42036/100000: episode: 833, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.822, mean reward: 0.578 [0.509, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.634, 10.284], loss: 0.001662, mae: 0.044216, mean_q: 1.231726
 42136/100000: episode: 834, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.398, mean reward: 0.584 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.903, 10.098], loss: 0.001646, mae: 0.043343, mean_q: 1.220728
 42236/100000: episode: 835, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.876, mean reward: 0.579 [0.508, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.222, 10.329], loss: 0.001480, mae: 0.041406, mean_q: 1.208048
 42336/100000: episode: 836, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: 57.536, mean reward: 0.575 [0.501, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.550, 10.098], loss: 0.001490, mae: 0.042139, mean_q: 1.200895
 42436/100000: episode: 837, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 61.168, mean reward: 0.612 [0.514, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.316, 10.098], loss: 0.001539, mae: 0.042504, mean_q: 1.186930
 42536/100000: episode: 838, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 56.681, mean reward: 0.567 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.418, 10.179], loss: 0.001519, mae: 0.042449, mean_q: 1.184541
 42636/100000: episode: 839, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 58.736, mean reward: 0.587 [0.498, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.236, 10.098], loss: 0.001597, mae: 0.043902, mean_q: 1.178266
 42736/100000: episode: 840, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.212, mean reward: 0.602 [0.510, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.955, 10.098], loss: 0.001521, mae: 0.042605, mean_q: 1.177029
 42836/100000: episode: 841, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 60.676, mean reward: 0.607 [0.519, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.484, 10.098], loss: 0.001420, mae: 0.041189, mean_q: 1.176261
 42936/100000: episode: 842, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 61.259, mean reward: 0.613 [0.502, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.917, 10.431], loss: 0.001450, mae: 0.041695, mean_q: 1.173515
 43036/100000: episode: 843, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.141, mean reward: 0.581 [0.502, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.345, 10.098], loss: 0.001595, mae: 0.043592, mean_q: 1.179298
 43136/100000: episode: 844, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 57.843, mean reward: 0.578 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.545, 10.142], loss: 0.001484, mae: 0.042054, mean_q: 1.176859
 43236/100000: episode: 845, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.971, mean reward: 0.580 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.675, 10.190], loss: 0.001535, mae: 0.042797, mean_q: 1.170535
 43336/100000: episode: 846, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 56.041, mean reward: 0.560 [0.503, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.504, 10.117], loss: 0.001536, mae: 0.043117, mean_q: 1.175340
 43436/100000: episode: 847, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 57.077, mean reward: 0.571 [0.499, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.305, 10.110], loss: 0.001471, mae: 0.041940, mean_q: 1.171715
 43536/100000: episode: 848, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 59.766, mean reward: 0.598 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.102, 10.098], loss: 0.001517, mae: 0.042740, mean_q: 1.172926
 43636/100000: episode: 849, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.740, mean reward: 0.587 [0.504, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.540, 10.269], loss: 0.001548, mae: 0.042831, mean_q: 1.173316
 43736/100000: episode: 850, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.796, mean reward: 0.578 [0.499, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.577, 10.098], loss: 0.001448, mae: 0.041698, mean_q: 1.169786
 43836/100000: episode: 851, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.077, mean reward: 0.571 [0.504, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.950, 10.209], loss: 0.001484, mae: 0.042256, mean_q: 1.169328
 43936/100000: episode: 852, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.701, mean reward: 0.587 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.180, 10.332], loss: 0.001272, mae: 0.039308, mean_q: 1.170447
 44036/100000: episode: 853, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.316, mean reward: 0.593 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.740, 10.103], loss: 0.001528, mae: 0.042560, mean_q: 1.167948
 44136/100000: episode: 854, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.936, mean reward: 0.599 [0.506, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.496, 10.124], loss: 0.001458, mae: 0.041757, mean_q: 1.170433
 44236/100000: episode: 855, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.475, mean reward: 0.605 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.716, 10.380], loss: 0.001486, mae: 0.042020, mean_q: 1.167883
 44336/100000: episode: 856, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 62.265, mean reward: 0.623 [0.501, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.932, 10.193], loss: 0.001364, mae: 0.040719, mean_q: 1.168743
 44436/100000: episode: 857, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.702, mean reward: 0.577 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.160, 10.146], loss: 0.001372, mae: 0.040769, mean_q: 1.169092
 44536/100000: episode: 858, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.489, mean reward: 0.585 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.469, 10.098], loss: 0.001520, mae: 0.042633, mean_q: 1.168845
 44636/100000: episode: 859, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.967, mean reward: 0.580 [0.497, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.973, 10.098], loss: 0.001415, mae: 0.041388, mean_q: 1.168103
 44736/100000: episode: 860, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.799, mean reward: 0.588 [0.504, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.506, 10.176], loss: 0.001547, mae: 0.042864, mean_q: 1.165821
 44836/100000: episode: 861, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.247, mean reward: 0.592 [0.510, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.613, 10.186], loss: 0.001583, mae: 0.043490, mean_q: 1.169001
 44936/100000: episode: 862, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.503, mean reward: 0.595 [0.514, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.757, 10.260], loss: 0.001497, mae: 0.042446, mean_q: 1.168752
 45036/100000: episode: 863, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.738, mean reward: 0.587 [0.511, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.975, 10.098], loss: 0.001322, mae: 0.040033, mean_q: 1.167580
 45136/100000: episode: 864, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.756, mean reward: 0.568 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.763, 10.127], loss: 0.001556, mae: 0.042782, mean_q: 1.167744
 45236/100000: episode: 865, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.353, mean reward: 0.594 [0.501, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.628, 10.098], loss: 0.001493, mae: 0.042541, mean_q: 1.166703
 45336/100000: episode: 866, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.736, mean reward: 0.577 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.785, 10.098], loss: 0.001466, mae: 0.041730, mean_q: 1.170075
 45436/100000: episode: 867, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.488, mean reward: 0.575 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.638, 10.236], loss: 0.001530, mae: 0.042617, mean_q: 1.166777
 45536/100000: episode: 868, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.487, mean reward: 0.595 [0.509, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.272, 10.104], loss: 0.001541, mae: 0.042640, mean_q: 1.164459
 45636/100000: episode: 869, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 62.144, mean reward: 0.621 [0.501, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.461, 10.098], loss: 0.001472, mae: 0.042047, mean_q: 1.171514
 45736/100000: episode: 870, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 60.154, mean reward: 0.602 [0.501, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.663, 10.363], loss: 0.001479, mae: 0.042358, mean_q: 1.166887
 45836/100000: episode: 871, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.482, mean reward: 0.595 [0.507, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.480, 10.224], loss: 0.001480, mae: 0.042177, mean_q: 1.167549
 45936/100000: episode: 872, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.237, mean reward: 0.582 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.116, 10.098], loss: 0.001637, mae: 0.044437, mean_q: 1.169227
 46036/100000: episode: 873, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 58.277, mean reward: 0.583 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.914, 10.225], loss: 0.001451, mae: 0.041570, mean_q: 1.164776
 46136/100000: episode: 874, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.157, mean reward: 0.582 [0.497, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.779, 10.256], loss: 0.001448, mae: 0.041330, mean_q: 1.162738
 46236/100000: episode: 875, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.522, mean reward: 0.575 [0.509, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.515, 10.169], loss: 0.001437, mae: 0.041271, mean_q: 1.165337
 46336/100000: episode: 876, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 61.652, mean reward: 0.617 [0.504, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.515, 10.431], loss: 0.001608, mae: 0.043772, mean_q: 1.161772
 46436/100000: episode: 877, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.953, mean reward: 0.590 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.496, 10.098], loss: 0.001474, mae: 0.041539, mean_q: 1.161805
 46536/100000: episode: 878, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.066, mean reward: 0.591 [0.509, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.488, 10.219], loss: 0.001492, mae: 0.041924, mean_q: 1.164156
 46636/100000: episode: 879, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.937, mean reward: 0.589 [0.502, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.709, 10.116], loss: 0.001603, mae: 0.043620, mean_q: 1.160944
 46736/100000: episode: 880, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.260, mean reward: 0.593 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.499, 10.098], loss: 0.001604, mae: 0.043880, mean_q: 1.164673
 46836/100000: episode: 881, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.277, mean reward: 0.593 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.098], loss: 0.001366, mae: 0.040436, mean_q: 1.162281
 46936/100000: episode: 882, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.797, mean reward: 0.598 [0.503, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.005, 10.212], loss: 0.001433, mae: 0.040935, mean_q: 1.162808
 47036/100000: episode: 883, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 60.106, mean reward: 0.601 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.315, 10.098], loss: 0.001453, mae: 0.041431, mean_q: 1.165738
 47136/100000: episode: 884, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.503, mean reward: 0.585 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.591, 10.098], loss: 0.001435, mae: 0.041068, mean_q: 1.166302
 47236/100000: episode: 885, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.674, mean reward: 0.577 [0.500, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.075, 10.098], loss: 0.001453, mae: 0.041861, mean_q: 1.168738
 47336/100000: episode: 886, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.093, mean reward: 0.581 [0.508, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.268, 10.121], loss: 0.001454, mae: 0.041427, mean_q: 1.167134
 47436/100000: episode: 887, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.294, mean reward: 0.573 [0.502, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.764, 10.098], loss: 0.001400, mae: 0.041019, mean_q: 1.166850
 47536/100000: episode: 888, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.982, mean reward: 0.590 [0.513, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.531, 10.114], loss: 0.001391, mae: 0.040616, mean_q: 1.162657
 47636/100000: episode: 889, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.902, mean reward: 0.589 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.883, 10.098], loss: 0.001595, mae: 0.042431, mean_q: 1.166240
[Info] 1-TH LEVEL FOUND: 1.3976683616638184, Considering 10/90 traces
 47736/100000: episode: 890, duration: 4.776s, episode steps: 100, steps per second: 21, episode reward: 57.618, mean reward: 0.576 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.957, 10.098], loss: 0.001365, mae: 0.040936, mean_q: 1.164804
 47815/100000: episode: 891, duration: 0.415s, episode steps: 79, steps per second: 190, episode reward: 45.975, mean reward: 0.582 [0.504, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.657 [-1.220, 10.249], loss: 0.001367, mae: 0.040537, mean_q: 1.163408
 47894/100000: episode: 892, duration: 0.416s, episode steps: 79, steps per second: 190, episode reward: 50.015, mean reward: 0.633 [0.528, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.652 [-0.731, 10.328], loss: 0.001394, mae: 0.040677, mean_q: 1.159269
 47913/100000: episode: 893, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 12.664, mean reward: 0.667 [0.601, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.101, 10.100], loss: 0.001450, mae: 0.041167, mean_q: 1.161415
 47933/100000: episode: 894, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 13.839, mean reward: 0.692 [0.633, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.624, 10.100], loss: 0.001539, mae: 0.042986, mean_q: 1.164217
 48029/100000: episode: 895, duration: 0.509s, episode steps: 96, steps per second: 189, episode reward: 56.171, mean reward: 0.585 [0.507, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-1.037, 10.337], loss: 0.001462, mae: 0.040537, mean_q: 1.161621
 48048/100000: episode: 896, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 13.419, mean reward: 0.706 [0.630, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.378, 10.100], loss: 0.001461, mae: 0.041950, mean_q: 1.160781
 48068/100000: episode: 897, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 13.531, mean reward: 0.677 [0.546, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.169, 10.100], loss: 0.001277, mae: 0.038893, mean_q: 1.162045
 48087/100000: episode: 898, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 13.171, mean reward: 0.693 [0.577, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.923, 10.100], loss: 0.001395, mae: 0.041558, mean_q: 1.166833
 48186/100000: episode: 899, duration: 0.504s, episode steps: 99, steps per second: 196, episode reward: 62.065, mean reward: 0.627 [0.516, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.852, 10.100], loss: 0.001481, mae: 0.041510, mean_q: 1.163504
 48206/100000: episode: 900, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 13.448, mean reward: 0.672 [0.606, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.385, 10.100], loss: 0.001623, mae: 0.043594, mean_q: 1.166650
 48226/100000: episode: 901, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 15.730, mean reward: 0.786 [0.654, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.772, 10.100], loss: 0.001496, mae: 0.042388, mean_q: 1.168507
 48246/100000: episode: 902, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 12.747, mean reward: 0.637 [0.532, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.619, 10.100], loss: 0.001730, mae: 0.043066, mean_q: 1.172046
 48265/100000: episode: 903, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 12.383, mean reward: 0.652 [0.582, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.590, 10.100], loss: 0.001463, mae: 0.041102, mean_q: 1.167032
 48284/100000: episode: 904, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 12.106, mean reward: 0.637 [0.579, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.265, 10.100], loss: 0.001370, mae: 0.040463, mean_q: 1.168779
 48303/100000: episode: 905, duration: 0.094s, episode steps: 19, steps per second: 201, episode reward: 13.284, mean reward: 0.699 [0.622, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.070, 10.100], loss: 0.001574, mae: 0.044493, mean_q: 1.176823
 48320/100000: episode: 906, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 11.278, mean reward: 0.663 [0.622, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.157, 10.100], loss: 0.001549, mae: 0.041123, mean_q: 1.170985
 48340/100000: episode: 907, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 12.731, mean reward: 0.637 [0.562, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.100], loss: 0.001342, mae: 0.040590, mean_q: 1.172670
 48359/100000: episode: 908, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 11.662, mean reward: 0.614 [0.543, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.138, 10.100], loss: 0.001613, mae: 0.042261, mean_q: 1.168699
 48379/100000: episode: 909, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 14.271, mean reward: 0.714 [0.640, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.109, 10.100], loss: 0.001677, mae: 0.045136, mean_q: 1.176703
 48398/100000: episode: 910, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 12.280, mean reward: 0.646 [0.572, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.042, 10.100], loss: 0.001662, mae: 0.043235, mean_q: 1.175058
 48430/100000: episode: 911, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 20.523, mean reward: 0.641 [0.553, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.060, 10.100], loss: 0.001708, mae: 0.044307, mean_q: 1.171048
 48509/100000: episode: 912, duration: 0.403s, episode steps: 79, steps per second: 196, episode reward: 51.285, mean reward: 0.649 [0.522, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.651 [-0.746, 10.100], loss: 0.001526, mae: 0.041849, mean_q: 1.173777
 48541/100000: episode: 913, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 21.686, mean reward: 0.678 [0.547, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.363, 10.114], loss: 0.001407, mae: 0.040598, mean_q: 1.180419
 48640/100000: episode: 914, duration: 0.524s, episode steps: 99, steps per second: 189, episode reward: 59.238, mean reward: 0.598 [0.498, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.428, 10.100], loss: 0.001594, mae: 0.043262, mean_q: 1.175685
 48672/100000: episode: 915, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 21.558, mean reward: 0.674 [0.601, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.277, 10.100], loss: 0.001585, mae: 0.043321, mean_q: 1.180450
 48687/100000: episode: 916, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 10.852, mean reward: 0.723 [0.664, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.226, 10.100], loss: 0.001410, mae: 0.039873, mean_q: 1.181208
 48786/100000: episode: 917, duration: 0.509s, episode steps: 99, steps per second: 195, episode reward: 59.105, mean reward: 0.597 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-0.740, 10.100], loss: 0.001562, mae: 0.042608, mean_q: 1.175769
 48806/100000: episode: 918, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 13.890, mean reward: 0.695 [0.631, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.392, 10.100], loss: 0.001348, mae: 0.039794, mean_q: 1.173600
 48820/100000: episode: 919, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 9.859, mean reward: 0.704 [0.646, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.784, 10.100], loss: 0.001889, mae: 0.045767, mean_q: 1.184596
 48835/100000: episode: 920, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 10.802, mean reward: 0.720 [0.653, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.202, 10.100], loss: 0.001785, mae: 0.043911, mean_q: 1.180245
 48914/100000: episode: 921, duration: 0.417s, episode steps: 79, steps per second: 189, episode reward: 48.431, mean reward: 0.613 [0.516, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.643 [-0.347, 10.100], loss: 0.001565, mae: 0.042506, mean_q: 1.181828
 48934/100000: episode: 922, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 13.493, mean reward: 0.675 [0.629, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.273, 10.100], loss: 0.001536, mae: 0.041561, mean_q: 1.179270
 48966/100000: episode: 923, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 22.790, mean reward: 0.712 [0.609, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.421, 10.100], loss: 0.001535, mae: 0.042288, mean_q: 1.176645
 48983/100000: episode: 924, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 10.948, mean reward: 0.644 [0.569, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.527, 10.100], loss: 0.001483, mae: 0.042222, mean_q: 1.179713
 49079/100000: episode: 925, duration: 0.495s, episode steps: 96, steps per second: 194, episode reward: 55.235, mean reward: 0.575 [0.508, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-1.903, 10.101], loss: 0.001682, mae: 0.043495, mean_q: 1.185737
 49094/100000: episode: 926, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 10.552, mean reward: 0.703 [0.652, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.097, 10.100], loss: 0.001379, mae: 0.040659, mean_q: 1.194181
 49173/100000: episode: 927, duration: 0.421s, episode steps: 79, steps per second: 188, episode reward: 50.717, mean reward: 0.642 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.648 [-0.575, 10.164], loss: 0.001416, mae: 0.041148, mean_q: 1.188480
 49192/100000: episode: 928, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 12.787, mean reward: 0.673 [0.616, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.147, 10.100], loss: 0.001500, mae: 0.040974, mean_q: 1.189735
 49212/100000: episode: 929, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 13.753, mean reward: 0.688 [0.570, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.564, 10.100], loss: 0.001122, mae: 0.037023, mean_q: 1.184447
 49308/100000: episode: 930, duration: 0.486s, episode steps: 96, steps per second: 198, episode reward: 59.434, mean reward: 0.619 [0.499, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-1.660, 10.100], loss: 0.001866, mae: 0.046079, mean_q: 1.184936
 49328/100000: episode: 931, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 13.678, mean reward: 0.684 [0.623, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.357, 10.100], loss: 0.001472, mae: 0.040656, mean_q: 1.180535
 49342/100000: episode: 932, duration: 0.094s, episode steps: 14, steps per second: 148, episode reward: 8.490, mean reward: 0.606 [0.540, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.308, 10.100], loss: 0.001583, mae: 0.042293, mean_q: 1.198106
 49356/100000: episode: 933, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 8.960, mean reward: 0.640 [0.558, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.167, 10.100], loss: 0.001613, mae: 0.044380, mean_q: 1.199510
 49455/100000: episode: 934, duration: 0.518s, episode steps: 99, steps per second: 191, episode reward: 58.802, mean reward: 0.594 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-1.124, 10.100], loss: 0.001523, mae: 0.042109, mean_q: 1.189596
 49475/100000: episode: 935, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 14.805, mean reward: 0.740 [0.613, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.647, 10.100], loss: 0.001545, mae: 0.042658, mean_q: 1.188299
 49489/100000: episode: 936, duration: 0.100s, episode steps: 14, steps per second: 140, episode reward: 9.783, mean reward: 0.699 [0.658, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.283, 10.100], loss: 0.001577, mae: 0.043039, mean_q: 1.198038
 49521/100000: episode: 937, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 21.094, mean reward: 0.659 [0.555, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.574, 10.100], loss: 0.001544, mae: 0.042652, mean_q: 1.187767
 49536/100000: episode: 938, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 10.174, mean reward: 0.678 [0.622, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.225, 10.100], loss: 0.001692, mae: 0.041906, mean_q: 1.200258
 49551/100000: episode: 939, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 9.997, mean reward: 0.666 [0.622, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.778, 10.100], loss: 0.001747, mae: 0.042898, mean_q: 1.188755
 49630/100000: episode: 940, duration: 0.426s, episode steps: 79, steps per second: 186, episode reward: 46.127, mean reward: 0.584 [0.509, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.650 [-0.624, 10.220], loss: 0.001742, mae: 0.044451, mean_q: 1.190625
 49726/100000: episode: 941, duration: 0.491s, episode steps: 96, steps per second: 196, episode reward: 55.963, mean reward: 0.583 [0.515, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-1.434, 10.150], loss: 0.001500, mae: 0.041483, mean_q: 1.192669
 49805/100000: episode: 942, duration: 0.456s, episode steps: 79, steps per second: 173, episode reward: 48.944, mean reward: 0.620 [0.505, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.647 [-0.382, 10.110], loss: 0.001779, mae: 0.044268, mean_q: 1.187895
 49824/100000: episode: 943, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 12.172, mean reward: 0.641 [0.576, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.425, 10.100], loss: 0.001238, mae: 0.038206, mean_q: 1.196873
 49843/100000: episode: 944, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 13.028, mean reward: 0.686 [0.599, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.455, 10.100], loss: 0.001498, mae: 0.041945, mean_q: 1.197566
 49858/100000: episode: 945, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 10.124, mean reward: 0.675 [0.616, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.101, 10.100], loss: 0.001324, mae: 0.039863, mean_q: 1.189034
 49878/100000: episode: 946, duration: 0.123s, episode steps: 20, steps per second: 163, episode reward: 15.155, mean reward: 0.758 [0.678, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.258, 10.100], loss: 0.001328, mae: 0.039056, mean_q: 1.201316
 49897/100000: episode: 947, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 12.428, mean reward: 0.654 [0.578, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.806, 10.100], loss: 0.001446, mae: 0.039856, mean_q: 1.188737
 49911/100000: episode: 948, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 9.481, mean reward: 0.677 [0.630, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.101, 10.100], loss: 0.001602, mae: 0.042201, mean_q: 1.201256
 50007/100000: episode: 949, duration: 0.509s, episode steps: 96, steps per second: 189, episode reward: 59.886, mean reward: 0.624 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-1.474, 10.415], loss: 0.001545, mae: 0.041873, mean_q: 1.193527
 50021/100000: episode: 950, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 10.266, mean reward: 0.733 [0.661, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.200, 10.100], loss: 0.001784, mae: 0.045459, mean_q: 1.198798
 50035/100000: episode: 951, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 9.996, mean reward: 0.714 [0.667, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.249, 10.100], loss: 0.001713, mae: 0.044454, mean_q: 1.193451
 50067/100000: episode: 952, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 23.763, mean reward: 0.743 [0.670, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.598, 10.100], loss: 0.001707, mae: 0.044249, mean_q: 1.187578
 50081/100000: episode: 953, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 9.301, mean reward: 0.664 [0.609, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.170, 10.100], loss: 0.001825, mae: 0.044397, mean_q: 1.208641
 50177/100000: episode: 954, duration: 0.501s, episode steps: 96, steps per second: 192, episode reward: 57.439, mean reward: 0.598 [0.515, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.237, 10.261], loss: 0.001752, mae: 0.043487, mean_q: 1.196939
 50194/100000: episode: 955, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 11.142, mean reward: 0.655 [0.537, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.044, 10.100], loss: 0.001330, mae: 0.039974, mean_q: 1.191772
 50211/100000: episode: 956, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 10.573, mean reward: 0.622 [0.500, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.040, 10.143], loss: 0.001587, mae: 0.041508, mean_q: 1.199130
 50231/100000: episode: 957, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 12.598, mean reward: 0.630 [0.590, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.068, 10.100], loss: 0.001384, mae: 0.040575, mean_q: 1.196437
 50246/100000: episode: 958, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 9.973, mean reward: 0.665 [0.619, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.217, 10.100], loss: 0.001476, mae: 0.041051, mean_q: 1.197671
 50325/100000: episode: 959, duration: 0.401s, episode steps: 79, steps per second: 197, episode reward: 47.869, mean reward: 0.606 [0.507, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.643 [-0.980, 10.100], loss: 0.001539, mae: 0.042202, mean_q: 1.206350
 50357/100000: episode: 960, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 20.387, mean reward: 0.637 [0.527, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.682, 10.100], loss: 0.001433, mae: 0.041161, mean_q: 1.208732
 50436/100000: episode: 961, duration: 0.413s, episode steps: 79, steps per second: 191, episode reward: 48.222, mean reward: 0.610 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.658 [-0.821, 10.230], loss: 0.001552, mae: 0.042915, mean_q: 1.208749
 50455/100000: episode: 962, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 13.230, mean reward: 0.696 [0.621, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.276, 10.100], loss: 0.002001, mae: 0.046870, mean_q: 1.191484
 50551/100000: episode: 963, duration: 0.527s, episode steps: 96, steps per second: 182, episode reward: 56.620, mean reward: 0.590 [0.508, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-1.037, 10.163], loss: 0.001493, mae: 0.040860, mean_q: 1.193183
 50571/100000: episode: 964, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 13.257, mean reward: 0.663 [0.594, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.467, 10.100], loss: 0.001421, mae: 0.041185, mean_q: 1.208691
 50586/100000: episode: 965, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 10.473, mean reward: 0.698 [0.628, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.416, 10.100], loss: 0.001757, mae: 0.042576, mean_q: 1.198400
 50665/100000: episode: 966, duration: 0.422s, episode steps: 79, steps per second: 187, episode reward: 48.887, mean reward: 0.619 [0.519, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.645 [-0.719, 10.131], loss: 0.001456, mae: 0.041055, mean_q: 1.203735
 50764/100000: episode: 967, duration: 0.510s, episode steps: 99, steps per second: 194, episode reward: 70.477, mean reward: 0.712 [0.550, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.477, 10.544], loss: 0.001481, mae: 0.042134, mean_q: 1.205696
 50863/100000: episode: 968, duration: 0.528s, episode steps: 99, steps per second: 187, episode reward: 60.127, mean reward: 0.607 [0.529, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-1.318, 10.100], loss: 0.001568, mae: 0.042803, mean_q: 1.204651
 50895/100000: episode: 969, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 19.858, mean reward: 0.621 [0.562, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.175, 10.100], loss: 0.001559, mae: 0.042974, mean_q: 1.209275
 50912/100000: episode: 970, duration: 0.105s, episode steps: 17, steps per second: 161, episode reward: 12.385, mean reward: 0.729 [0.658, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.343, 10.100], loss: 0.001398, mae: 0.039769, mean_q: 1.207532
 50931/100000: episode: 971, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 12.049, mean reward: 0.634 [0.565, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.216, 10.100], loss: 0.001429, mae: 0.040716, mean_q: 1.201073
 50951/100000: episode: 972, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 13.610, mean reward: 0.680 [0.614, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.317, 10.100], loss: 0.001408, mae: 0.040919, mean_q: 1.200473
 50966/100000: episode: 973, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 10.434, mean reward: 0.696 [0.607, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.240, 10.100], loss: 0.001762, mae: 0.045076, mean_q: 1.200540
 50985/100000: episode: 974, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 12.170, mean reward: 0.641 [0.554, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.928, 10.100], loss: 0.001675, mae: 0.044906, mean_q: 1.205388
 51000/100000: episode: 975, duration: 0.100s, episode steps: 15, steps per second: 150, episode reward: 10.890, mean reward: 0.726 [0.682, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.655, 10.100], loss: 0.001433, mae: 0.041292, mean_q: 1.222245
 51096/100000: episode: 976, duration: 0.509s, episode steps: 96, steps per second: 188, episode reward: 56.147, mean reward: 0.585 [0.505, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.973, 10.186], loss: 0.001442, mae: 0.040722, mean_q: 1.209934
 51116/100000: episode: 977, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 12.539, mean reward: 0.627 [0.581, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.353, 10.100], loss: 0.001626, mae: 0.043518, mean_q: 1.206848
 51195/100000: episode: 978, duration: 0.415s, episode steps: 79, steps per second: 190, episode reward: 49.205, mean reward: 0.623 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.661 [-0.639, 10.258], loss: 0.001602, mae: 0.042855, mean_q: 1.206293
 51291/100000: episode: 979, duration: 0.501s, episode steps: 96, steps per second: 192, episode reward: 57.767, mean reward: 0.602 [0.512, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-0.782, 10.204], loss: 0.001560, mae: 0.042459, mean_q: 1.208923
[Info] 2-TH LEVEL FOUND: 1.4587517976760864, Considering 10/90 traces
 51305/100000: episode: 980, duration: 4.295s, episode steps: 14, steps per second: 3, episode reward: 9.639, mean reward: 0.688 [0.636, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.315, 10.100], loss: 0.001574, mae: 0.042383, mean_q: 1.203348
 51315/100000: episode: 981, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 6.867, mean reward: 0.687 [0.627, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.203, 10.100], loss: 0.001614, mae: 0.044103, mean_q: 1.227790
 51325/100000: episode: 982, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 7.419, mean reward: 0.742 [0.646, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.293, 10.100], loss: 0.001657, mae: 0.042278, mean_q: 1.220082
 51346/100000: episode: 983, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 14.995, mean reward: 0.714 [0.634, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.819, 10.100], loss: 0.001504, mae: 0.042182, mean_q: 1.209212
 51359/100000: episode: 984, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 9.083, mean reward: 0.699 [0.652, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.464, 10.100], loss: 0.001523, mae: 0.041630, mean_q: 1.220167
 51373/100000: episode: 985, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 9.534, mean reward: 0.681 [0.647, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.905, 10.100], loss: 0.001334, mae: 0.039410, mean_q: 1.216553
 51380/100000: episode: 986, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 5.134, mean reward: 0.733 [0.677, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.462, 10.100], loss: 0.001291, mae: 0.038191, mean_q: 1.213443
 51389/100000: episode: 987, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 7.015, mean reward: 0.779 [0.736, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-1.227, 10.100], loss: 0.001432, mae: 0.039835, mean_q: 1.226201
 51396/100000: episode: 988, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 4.738, mean reward: 0.677 [0.647, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.477, 10.100], loss: 0.001148, mae: 0.036914, mean_q: 1.223541
 51417/100000: episode: 989, duration: 0.113s, episode steps: 21, steps per second: 187, episode reward: 15.664, mean reward: 0.746 [0.694, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.493, 10.100], loss: 0.001616, mae: 0.043564, mean_q: 1.200376
 51424/100000: episode: 990, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 5.253, mean reward: 0.750 [0.712, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.348, 10.100], loss: 0.001591, mae: 0.042938, mean_q: 1.222988
 51440/100000: episode: 991, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.074, mean reward: 0.692 [0.596, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.172, 10.100], loss: 0.001488, mae: 0.042208, mean_q: 1.214657
 51494/100000: episode: 992, duration: 0.297s, episode steps: 54, steps per second: 182, episode reward: 33.207, mean reward: 0.615 [0.513, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.534, 10.100], loss: 0.001383, mae: 0.040617, mean_q: 1.224005
 51548/100000: episode: 993, duration: 0.291s, episode steps: 54, steps per second: 186, episode reward: 36.707, mean reward: 0.680 [0.577, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-1.437, 10.353], loss: 0.001272, mae: 0.039097, mean_q: 1.225581
 51568/100000: episode: 994, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 13.327, mean reward: 0.666 [0.568, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.652, 10.100], loss: 0.001148, mae: 0.037645, mean_q: 1.223169
 51581/100000: episode: 995, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 8.712, mean reward: 0.670 [0.621, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.247, 10.100], loss: 0.001389, mae: 0.039772, mean_q: 1.208838
 51588/100000: episode: 996, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 5.438, mean reward: 0.777 [0.736, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.464, 10.100], loss: 0.001293, mae: 0.037830, mean_q: 1.208838
 51604/100000: episode: 997, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 12.041, mean reward: 0.753 [0.702, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.367, 10.100], loss: 0.001782, mae: 0.044272, mean_q: 1.212957
 51658/100000: episode: 998, duration: 0.298s, episode steps: 54, steps per second: 181, episode reward: 41.474, mean reward: 0.768 [0.588, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.940, 10.326], loss: 0.001361, mae: 0.039546, mean_q: 1.218390
 51712/100000: episode: 999, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 42.560, mean reward: 0.788 [0.699, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-1.613, 10.468], loss: 0.001729, mae: 0.045111, mean_q: 1.223833
 51721/100000: episode: 1000, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 6.502, mean reward: 0.722 [0.696, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.246, 10.100], loss: 0.001495, mae: 0.040415, mean_q: 1.210259
 51741/100000: episode: 1001, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 13.711, mean reward: 0.686 [0.637, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.280, 10.100], loss: 0.001400, mae: 0.041621, mean_q: 1.238584
 51795/100000: episode: 1002, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 36.682, mean reward: 0.679 [0.595, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-1.165, 10.290], loss: 0.001633, mae: 0.043730, mean_q: 1.222673
 51849/100000: episode: 1003, duration: 0.287s, episode steps: 54, steps per second: 188, episode reward: 37.718, mean reward: 0.698 [0.514, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.681, 10.173], loss: 0.001304, mae: 0.039643, mean_q: 1.240069
 51856/100000: episode: 1004, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 5.385, mean reward: 0.769 [0.725, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.399, 10.100], loss: 0.001398, mae: 0.041979, mean_q: 1.233761
 51865/100000: episode: 1005, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 6.773, mean reward: 0.753 [0.684, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.296, 10.100], loss: 0.001800, mae: 0.046020, mean_q: 1.236605
 51881/100000: episode: 1006, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 11.742, mean reward: 0.734 [0.624, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.963, 10.100], loss: 0.001250, mae: 0.039454, mean_q: 1.247011
 51897/100000: episode: 1007, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 11.952, mean reward: 0.747 [0.692, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.671, 10.100], loss: 0.001941, mae: 0.047287, mean_q: 1.229166
 51913/100000: episode: 1008, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 12.002, mean reward: 0.750 [0.724, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.318, 10.100], loss: 0.001754, mae: 0.045766, mean_q: 1.217794
 51923/100000: episode: 1009, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 7.225, mean reward: 0.723 [0.647, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.236, 10.100], loss: 0.001752, mae: 0.046093, mean_q: 1.255765
 51932/100000: episode: 1010, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 6.531, mean reward: 0.726 [0.655, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.294, 10.100], loss: 0.001382, mae: 0.040913, mean_q: 1.229315
 51939/100000: episode: 1011, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 5.283, mean reward: 0.755 [0.696, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.510, 10.100], loss: 0.001876, mae: 0.047984, mean_q: 1.227429
 51949/100000: episode: 1012, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 7.285, mean reward: 0.729 [0.688, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.869, 10.100], loss: 0.001474, mae: 0.040696, mean_q: 1.215821
 51965/100000: episode: 1013, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 11.693, mean reward: 0.731 [0.654, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.220, 10.100], loss: 0.001317, mae: 0.041010, mean_q: 1.236980
 51978/100000: episode: 1014, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 9.232, mean reward: 0.710 [0.655, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.411, 10.100], loss: 0.001342, mae: 0.040135, mean_q: 1.225148
 51999/100000: episode: 1015, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 14.886, mean reward: 0.709 [0.662, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.114, 10.100], loss: 0.001214, mae: 0.038862, mean_q: 1.246083
 52008/100000: episode: 1016, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 7.017, mean reward: 0.780 [0.703, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.119, 10.100], loss: 0.001178, mae: 0.038866, mean_q: 1.250565
 52024/100000: episode: 1017, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 12.263, mean reward: 0.766 [0.655, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.365, 10.100], loss: 0.001480, mae: 0.040741, mean_q: 1.248795
 52078/100000: episode: 1018, duration: 0.290s, episode steps: 54, steps per second: 186, episode reward: 36.213, mean reward: 0.671 [0.504, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.755, 10.123], loss: 0.001226, mae: 0.038458, mean_q: 1.239817
 52087/100000: episode: 1019, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 7.204, mean reward: 0.800 [0.757, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.305, 10.100], loss: 0.001689, mae: 0.042773, mean_q: 1.232854
 52101/100000: episode: 1020, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 10.070, mean reward: 0.719 [0.639, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.562, 10.100], loss: 0.001385, mae: 0.041296, mean_q: 1.245161
 52121/100000: episode: 1021, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 14.414, mean reward: 0.721 [0.669, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.250, 10.100], loss: 0.001463, mae: 0.040815, mean_q: 1.239281
 52141/100000: episode: 1022, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 13.675, mean reward: 0.684 [0.625, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.223, 10.100], loss: 0.001802, mae: 0.044247, mean_q: 1.249269
 52157/100000: episode: 1023, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 12.232, mean reward: 0.765 [0.721, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.311, 10.100], loss: 0.001486, mae: 0.041717, mean_q: 1.247859
 52173/100000: episode: 1024, duration: 0.116s, episode steps: 16, steps per second: 138, episode reward: 12.219, mean reward: 0.764 [0.730, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.930, 10.100], loss: 0.001386, mae: 0.040843, mean_q: 1.250451
 52183/100000: episode: 1025, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 7.226, mean reward: 0.723 [0.657, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.569, 10.100], loss: 0.001510, mae: 0.042624, mean_q: 1.262171
 52199/100000: episode: 1026, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 11.782, mean reward: 0.736 [0.690, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.540, 10.100], loss: 0.001460, mae: 0.040592, mean_q: 1.236334
 52206/100000: episode: 1027, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 5.201, mean reward: 0.743 [0.671, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.420, 10.100], loss: 0.001285, mae: 0.040439, mean_q: 1.252586
 52227/100000: episode: 1028, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 16.164, mean reward: 0.770 [0.715, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.057, 10.100], loss: 0.001261, mae: 0.038600, mean_q: 1.249984
 52237/100000: episode: 1029, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 7.545, mean reward: 0.754 [0.706, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.071, 10.100], loss: 0.001364, mae: 0.041371, mean_q: 1.252147
 52257/100000: episode: 1030, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 14.381, mean reward: 0.719 [0.645, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.106, 10.100], loss: 0.001148, mae: 0.037655, mean_q: 1.252320
 52270/100000: episode: 1031, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 8.988, mean reward: 0.691 [0.660, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.279, 10.100], loss: 0.001155, mae: 0.038745, mean_q: 1.259051
 52286/100000: episode: 1032, duration: 0.097s, episode steps: 16, steps per second: 166, episode reward: 11.626, mean reward: 0.727 [0.667, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.278, 10.100], loss: 0.001509, mae: 0.043212, mean_q: 1.270699
 52300/100000: episode: 1033, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 9.552, mean reward: 0.682 [0.607, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.225, 10.100], loss: 0.001548, mae: 0.041795, mean_q: 1.248432
 52320/100000: episode: 1034, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 14.697, mean reward: 0.735 [0.672, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.230, 10.100], loss: 0.001338, mae: 0.040258, mean_q: 1.256741
 52327/100000: episode: 1035, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 5.177, mean reward: 0.740 [0.705, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.479, 10.100], loss: 0.001748, mae: 0.047813, mean_q: 1.246399
 52347/100000: episode: 1036, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 14.899, mean reward: 0.745 [0.651, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.280, 10.100], loss: 0.001556, mae: 0.042618, mean_q: 1.247220
 52360/100000: episode: 1037, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 9.190, mean reward: 0.707 [0.643, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.232, 10.100], loss: 0.001492, mae: 0.041878, mean_q: 1.259317
 52370/100000: episode: 1038, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 6.914, mean reward: 0.691 [0.648, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.239, 10.100], loss: 0.001203, mae: 0.039103, mean_q: 1.244577
 52380/100000: episode: 1039, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 8.003, mean reward: 0.800 [0.729, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.705, 10.100], loss: 0.001506, mae: 0.040733, mean_q: 1.246388
 52393/100000: episode: 1040, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 8.841, mean reward: 0.680 [0.590, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.265, 10.100], loss: 0.001464, mae: 0.041911, mean_q: 1.249855
 52447/100000: episode: 1041, duration: 0.286s, episode steps: 54, steps per second: 189, episode reward: 40.692, mean reward: 0.754 [0.649, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-1.160, 10.515], loss: 0.001253, mae: 0.038458, mean_q: 1.255015
 52460/100000: episode: 1042, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 9.553, mean reward: 0.735 [0.643, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.347, 10.100], loss: 0.001230, mae: 0.039187, mean_q: 1.261379
 52470/100000: episode: 1043, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 7.689, mean reward: 0.769 [0.700, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.290, 10.100], loss: 0.001385, mae: 0.040454, mean_q: 1.258362
 52484/100000: episode: 1044, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 10.514, mean reward: 0.751 [0.680, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.364, 10.100], loss: 0.001300, mae: 0.040382, mean_q: 1.255981
 52494/100000: episode: 1045, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 7.123, mean reward: 0.712 [0.631, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.296, 10.100], loss: 0.001323, mae: 0.041182, mean_q: 1.262228
 52503/100000: episode: 1046, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 6.667, mean reward: 0.741 [0.696, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.438, 10.100], loss: 0.001199, mae: 0.038173, mean_q: 1.261877
 52512/100000: episode: 1047, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 6.727, mean reward: 0.747 [0.653, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.199, 10.100], loss: 0.001422, mae: 0.039118, mean_q: 1.265984
 52525/100000: episode: 1048, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 8.978, mean reward: 0.691 [0.630, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.345, 10.100], loss: 0.001211, mae: 0.037594, mean_q: 1.248071
 52535/100000: episode: 1049, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 7.314, mean reward: 0.731 [0.703, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.866, 10.100], loss: 0.001518, mae: 0.044574, mean_q: 1.264174
 52545/100000: episode: 1050, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 7.583, mean reward: 0.758 [0.678, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.280, 10.100], loss: 0.001380, mae: 0.042097, mean_q: 1.270381
[Info] FALSIFICATION!
 52562/100000: episode: 1051, duration: 0.263s, episode steps: 17, steps per second: 65, episode reward: 13.270, mean reward: 0.781 [0.688, 1.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.961, 7.052], loss: 0.001263, mae: 0.039908, mean_q: 1.254691
 52582/100000: episode: 1052, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 13.912, mean reward: 0.696 [0.615, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.309, 10.100], loss: 0.001336, mae: 0.040503, mean_q: 1.254275
 52636/100000: episode: 1053, duration: 0.289s, episode steps: 54, steps per second: 187, episode reward: 33.583, mean reward: 0.622 [0.507, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.380, 10.294], loss: 0.001401, mae: 0.040777, mean_q: 1.256819
 52643/100000: episode: 1054, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.236, mean reward: 0.748 [0.662, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.405, 10.100], loss: 0.002162, mae: 0.047130, mean_q: 1.272269
 52652/100000: episode: 1055, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 6.561, mean reward: 0.729 [0.667, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.201, 10.100], loss: 0.001208, mae: 0.037781, mean_q: 1.252405
 52666/100000: episode: 1056, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 10.063, mean reward: 0.719 [0.586, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.241, 10.100], loss: 0.001578, mae: 0.042568, mean_q: 1.250999
 52680/100000: episode: 1057, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 9.229, mean reward: 0.659 [0.599, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.123, 10.100], loss: 0.001440, mae: 0.041969, mean_q: 1.271666
 52701/100000: episode: 1058, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 14.595, mean reward: 0.695 [0.616, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.555, 10.100], loss: 0.001200, mae: 0.038780, mean_q: 1.265899
 52722/100000: episode: 1059, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 15.832, mean reward: 0.754 [0.663, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.394, 10.100], loss: 0.001590, mae: 0.044472, mean_q: 1.268903
 52735/100000: episode: 1060, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 9.553, mean reward: 0.735 [0.633, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.149, 10.100], loss: 0.001327, mae: 0.041108, mean_q: 1.286745
 52742/100000: episode: 1061, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 4.580, mean reward: 0.654 [0.633, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.607, 10.100], loss: 0.001347, mae: 0.041465, mean_q: 1.268345
 52752/100000: episode: 1062, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 6.895, mean reward: 0.689 [0.623, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.241, 10.100], loss: 0.001112, mae: 0.036841, mean_q: 1.265201
 52762/100000: episode: 1063, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 6.918, mean reward: 0.692 [0.626, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.275, 10.100], loss: 0.001187, mae: 0.038257, mean_q: 1.271554
 52772/100000: episode: 1064, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 7.481, mean reward: 0.748 [0.701, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.404, 10.100], loss: 0.001688, mae: 0.041259, mean_q: 1.256047
 52788/100000: episode: 1065, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 13.096, mean reward: 0.819 [0.773, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.303, 10.100], loss: 0.001431, mae: 0.041842, mean_q: 1.270751
 52804/100000: episode: 1066, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 12.870, mean reward: 0.804 [0.687, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.272, 10.100], loss: 0.001339, mae: 0.041187, mean_q: 1.280879
 52814/100000: episode: 1067, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 7.378, mean reward: 0.738 [0.698, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.481, 10.100], loss: 0.001189, mae: 0.037986, mean_q: 1.263832
 52835/100000: episode: 1068, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 16.982, mean reward: 0.809 [0.742, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.404, 10.100], loss: 0.001259, mae: 0.039629, mean_q: 1.263591
 52844/100000: episode: 1069, duration: 0.063s, episode steps: 9, steps per second: 143, episode reward: 7.162, mean reward: 0.796 [0.736, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.479, 10.100], loss: 0.001471, mae: 0.041988, mean_q: 1.268694
[Info] Complete ISplit Iteration
[Info] Levels: [1.3976684, 1.4587518, 1.5640109]
[Info] Cond. Prob: [0.1, 0.1, 0.06]
[Info] Error Prob: 0.0006000000000000001

 52865/100000: episode: 1070, duration: 4.410s, episode steps: 21, steps per second: 5, episode reward: 15.388, mean reward: 0.733 [0.684, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.313, 10.100], loss: 0.001448, mae: 0.040522, mean_q: 1.275814
 52965/100000: episode: 1071, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 61.768, mean reward: 0.618 [0.501, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.710, 10.098], loss: 0.001334, mae: 0.040054, mean_q: 1.271568
 53065/100000: episode: 1072, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.671, mean reward: 0.597 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.030, 10.167], loss: 0.001418, mae: 0.041815, mean_q: 1.271658
 53165/100000: episode: 1073, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.342, mean reward: 0.593 [0.512, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.819, 10.192], loss: 0.001502, mae: 0.043069, mean_q: 1.263455
 53265/100000: episode: 1074, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 61.687, mean reward: 0.617 [0.523, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.602, 10.098], loss: 0.001390, mae: 0.040453, mean_q: 1.266779
 53365/100000: episode: 1075, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.280, mean reward: 0.583 [0.513, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.051, 10.098], loss: 0.001365, mae: 0.040314, mean_q: 1.270705
 53465/100000: episode: 1076, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.251, mean reward: 0.593 [0.508, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.346, 10.098], loss: 0.001451, mae: 0.042238, mean_q: 1.261412
 53565/100000: episode: 1077, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 56.486, mean reward: 0.565 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.204, 10.098], loss: 0.001501, mae: 0.042383, mean_q: 1.263381
 53665/100000: episode: 1078, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 57.364, mean reward: 0.574 [0.508, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.316, 10.098], loss: 0.001439, mae: 0.041168, mean_q: 1.264495
 53765/100000: episode: 1079, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.879, mean reward: 0.579 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.775, 10.098], loss: 0.001622, mae: 0.044138, mean_q: 1.260271
 53865/100000: episode: 1080, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.926, mean reward: 0.589 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.167, 10.098], loss: 0.001443, mae: 0.041842, mean_q: 1.259959
 53965/100000: episode: 1081, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.851, mean reward: 0.579 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.734, 10.153], loss: 0.001600, mae: 0.043742, mean_q: 1.262123
 54065/100000: episode: 1082, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.235, mean reward: 0.572 [0.497, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.102, 10.098], loss: 0.001724, mae: 0.044906, mean_q: 1.251712
 54165/100000: episode: 1083, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.871, mean reward: 0.579 [0.514, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.897, 10.144], loss: 0.001513, mae: 0.042159, mean_q: 1.254404
 54265/100000: episode: 1084, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.028, mean reward: 0.580 [0.506, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.408, 10.167], loss: 0.001601, mae: 0.042352, mean_q: 1.247222
 54365/100000: episode: 1085, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.240, mean reward: 0.582 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.254, 10.098], loss: 0.001497, mae: 0.041252, mean_q: 1.252222
 54465/100000: episode: 1086, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.590, mean reward: 0.596 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.454, 10.098], loss: 0.001554, mae: 0.043302, mean_q: 1.247745
 54565/100000: episode: 1087, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.279, mean reward: 0.593 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.647, 10.326], loss: 0.001605, mae: 0.043567, mean_q: 1.247414
 54665/100000: episode: 1088, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.241, mean reward: 0.582 [0.512, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.744, 10.149], loss: 0.001676, mae: 0.044605, mean_q: 1.250263
 54765/100000: episode: 1089, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.959, mean reward: 0.590 [0.503, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.898, 10.141], loss: 0.001716, mae: 0.044944, mean_q: 1.242965
 54865/100000: episode: 1090, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.962, mean reward: 0.570 [0.511, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.508, 10.129], loss: 0.001453, mae: 0.041860, mean_q: 1.241729
 54965/100000: episode: 1091, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 60.590, mean reward: 0.606 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.794, 10.098], loss: 0.001625, mae: 0.043822, mean_q: 1.238154
 55065/100000: episode: 1092, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.286, mean reward: 0.593 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.234, 10.226], loss: 0.001655, mae: 0.043554, mean_q: 1.242534
 55165/100000: episode: 1093, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.958, mean reward: 0.580 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.352, 10.362], loss: 0.001738, mae: 0.044302, mean_q: 1.232818
 55265/100000: episode: 1094, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 60.657, mean reward: 0.607 [0.512, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.966, 10.098], loss: 0.001645, mae: 0.043863, mean_q: 1.236218
 55365/100000: episode: 1095, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.066, mean reward: 0.581 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.441, 10.218], loss: 0.001672, mae: 0.043545, mean_q: 1.237573
 55465/100000: episode: 1096, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.037, mean reward: 0.580 [0.508, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.122, 10.162], loss: 0.001741, mae: 0.044282, mean_q: 1.238376
 55565/100000: episode: 1097, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.088, mean reward: 0.581 [0.508, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.058, 10.098], loss: 0.001553, mae: 0.042688, mean_q: 1.235994
 55665/100000: episode: 1098, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 61.459, mean reward: 0.615 [0.509, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.513, 10.098], loss: 0.001693, mae: 0.043932, mean_q: 1.232363
 55765/100000: episode: 1099, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.227, mean reward: 0.582 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.265, 10.205], loss: 0.001642, mae: 0.043191, mean_q: 1.233820
 55865/100000: episode: 1100, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.886, mean reward: 0.589 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.812, 10.256], loss: 0.001670, mae: 0.043535, mean_q: 1.225614
 55965/100000: episode: 1101, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.215, mean reward: 0.572 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.871, 10.118], loss: 0.001581, mae: 0.042596, mean_q: 1.219161
 56065/100000: episode: 1102, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.032, mean reward: 0.570 [0.508, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.828, 10.098], loss: 0.001672, mae: 0.043866, mean_q: 1.228501
 56165/100000: episode: 1103, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 56.577, mean reward: 0.566 [0.508, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.528, 10.134], loss: 0.001667, mae: 0.042990, mean_q: 1.223261
 56265/100000: episode: 1104, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.780, mean reward: 0.598 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.000, 10.174], loss: 0.001609, mae: 0.043052, mean_q: 1.223248
 56365/100000: episode: 1105, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.759, mean reward: 0.578 [0.500, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.457, 10.102], loss: 0.001579, mae: 0.042604, mean_q: 1.216769
 56465/100000: episode: 1106, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.412, mean reward: 0.584 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.756, 10.265], loss: 0.001604, mae: 0.042974, mean_q: 1.218560
 56565/100000: episode: 1107, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.913, mean reward: 0.579 [0.505, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.999, 10.181], loss: 0.001551, mae: 0.042122, mean_q: 1.214341
 56665/100000: episode: 1108, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.860, mean reward: 0.609 [0.520, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.935, 10.404], loss: 0.001769, mae: 0.044793, mean_q: 1.206247
 56765/100000: episode: 1109, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 56.520, mean reward: 0.565 [0.502, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.526, 10.173], loss: 0.001508, mae: 0.042137, mean_q: 1.206913
 56865/100000: episode: 1110, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.370, mean reward: 0.604 [0.526, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.823, 10.217], loss: 0.001582, mae: 0.042989, mean_q: 1.198187
 56965/100000: episode: 1111, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.006, mean reward: 0.600 [0.507, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.202, 10.281], loss: 0.001481, mae: 0.041808, mean_q: 1.193912
 57065/100000: episode: 1112, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 63.314, mean reward: 0.633 [0.530, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.112, 10.495], loss: 0.001571, mae: 0.042792, mean_q: 1.189720
 57165/100000: episode: 1113, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.594, mean reward: 0.576 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.669, 10.166], loss: 0.001637, mae: 0.042888, mean_q: 1.191485
 57265/100000: episode: 1114, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.205, mean reward: 0.582 [0.507, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.219, 10.259], loss: 0.001613, mae: 0.042902, mean_q: 1.186627
 57365/100000: episode: 1115, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.429, mean reward: 0.584 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.605, 10.098], loss: 0.001483, mae: 0.041828, mean_q: 1.180032
 57465/100000: episode: 1116, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.311, mean reward: 0.583 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.064, 10.240], loss: 0.001422, mae: 0.040883, mean_q: 1.175567
 57565/100000: episode: 1117, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 56.667, mean reward: 0.567 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.390, 10.216], loss: 0.001476, mae: 0.041594, mean_q: 1.172013
 57665/100000: episode: 1118, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.018, mean reward: 0.580 [0.500, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.786, 10.331], loss: 0.001464, mae: 0.041662, mean_q: 1.173374
 57765/100000: episode: 1119, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.682, mean reward: 0.587 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.151, 10.098], loss: 0.001344, mae: 0.040182, mean_q: 1.167980
 57865/100000: episode: 1120, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 63.443, mean reward: 0.634 [0.512, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.287, 10.098], loss: 0.001304, mae: 0.039848, mean_q: 1.164379
 57965/100000: episode: 1121, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.196, mean reward: 0.572 [0.504, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.402, 10.123], loss: 0.001346, mae: 0.039606, mean_q: 1.162371
 58065/100000: episode: 1122, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.691, mean reward: 0.577 [0.507, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.328, 10.098], loss: 0.001399, mae: 0.040924, mean_q: 1.161100
 58165/100000: episode: 1123, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.003, mean reward: 0.590 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.642, 10.098], loss: 0.001349, mae: 0.040313, mean_q: 1.164380
 58265/100000: episode: 1124, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.217, mean reward: 0.582 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.551, 10.156], loss: 0.001255, mae: 0.038628, mean_q: 1.158973
 58365/100000: episode: 1125, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.191, mean reward: 0.592 [0.507, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.459, 10.098], loss: 0.001389, mae: 0.041472, mean_q: 1.156172
 58465/100000: episode: 1126, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.910, mean reward: 0.579 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.504, 10.098], loss: 0.001168, mae: 0.037299, mean_q: 1.154576
 58565/100000: episode: 1127, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 60.289, mean reward: 0.603 [0.512, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.152, 10.161], loss: 0.001311, mae: 0.039788, mean_q: 1.160763
 58665/100000: episode: 1128, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 59.873, mean reward: 0.599 [0.500, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.765, 10.203], loss: 0.001315, mae: 0.039798, mean_q: 1.157542
 58765/100000: episode: 1129, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.488, mean reward: 0.595 [0.507, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.288, 10.098], loss: 0.001253, mae: 0.038816, mean_q: 1.159968
 58865/100000: episode: 1130, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.408, mean reward: 0.584 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.042, 10.269], loss: 0.001286, mae: 0.038966, mean_q: 1.162944
 58965/100000: episode: 1131, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 62.209, mean reward: 0.622 [0.514, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.380, 10.098], loss: 0.001365, mae: 0.040622, mean_q: 1.164299
 59065/100000: episode: 1132, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 59.923, mean reward: 0.599 [0.513, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.160, 10.098], loss: 0.001280, mae: 0.039293, mean_q: 1.161801
 59165/100000: episode: 1133, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 62.614, mean reward: 0.626 [0.509, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.859, 10.098], loss: 0.001351, mae: 0.040347, mean_q: 1.164198
 59265/100000: episode: 1134, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 59.297, mean reward: 0.593 [0.506, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.561, 10.129], loss: 0.001364, mae: 0.040353, mean_q: 1.167383
 59365/100000: episode: 1135, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.473, mean reward: 0.585 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.733, 10.167], loss: 0.001410, mae: 0.041292, mean_q: 1.169143
 59465/100000: episode: 1136, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.965, mean reward: 0.590 [0.497, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.386, 10.320], loss: 0.001285, mae: 0.039224, mean_q: 1.167083
 59565/100000: episode: 1137, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.045, mean reward: 0.580 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.086, 10.250], loss: 0.001341, mae: 0.039984, mean_q: 1.164917
 59665/100000: episode: 1138, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.436, mean reward: 0.584 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.050, 10.130], loss: 0.001359, mae: 0.040401, mean_q: 1.167606
 59765/100000: episode: 1139, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 56.981, mean reward: 0.570 [0.506, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.424, 10.098], loss: 0.001339, mae: 0.040313, mean_q: 1.163916
 59865/100000: episode: 1140, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 56.451, mean reward: 0.565 [0.503, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.530, 10.099], loss: 0.001350, mae: 0.040531, mean_q: 1.164405
 59965/100000: episode: 1141, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.392, mean reward: 0.584 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.021, 10.174], loss: 0.001311, mae: 0.039373, mean_q: 1.161856
 60065/100000: episode: 1142, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 58.288, mean reward: 0.583 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.128, 10.212], loss: 0.001290, mae: 0.038916, mean_q: 1.163727
 60165/100000: episode: 1143, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 61.828, mean reward: 0.618 [0.510, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.635, 10.098], loss: 0.001400, mae: 0.040976, mean_q: 1.163033
 60265/100000: episode: 1144, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.522, mean reward: 0.595 [0.502, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.464, 10.375], loss: 0.001318, mae: 0.039941, mean_q: 1.162780
 60365/100000: episode: 1145, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.698, mean reward: 0.587 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.586, 10.098], loss: 0.001289, mae: 0.039148, mean_q: 1.164279
 60465/100000: episode: 1146, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.399, mean reward: 0.584 [0.501, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.178, 10.127], loss: 0.001332, mae: 0.039953, mean_q: 1.162639
 60565/100000: episode: 1147, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 60.530, mean reward: 0.605 [0.507, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.810, 10.098], loss: 0.001284, mae: 0.039055, mean_q: 1.164705
 60665/100000: episode: 1148, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 59.948, mean reward: 0.599 [0.506, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.767, 10.098], loss: 0.001507, mae: 0.041744, mean_q: 1.168095
 60765/100000: episode: 1149, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.051, mean reward: 0.571 [0.501, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.192, 10.150], loss: 0.001279, mae: 0.039025, mean_q: 1.167435
 60865/100000: episode: 1150, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.238, mean reward: 0.582 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.676, 10.098], loss: 0.001290, mae: 0.039053, mean_q: 1.168949
 60965/100000: episode: 1151, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 60.351, mean reward: 0.604 [0.505, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.130, 10.098], loss: 0.001194, mae: 0.037421, mean_q: 1.166167
 61065/100000: episode: 1152, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 63.817, mean reward: 0.638 [0.531, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.018, 10.098], loss: 0.001254, mae: 0.039047, mean_q: 1.164412
 61165/100000: episode: 1153, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 59.653, mean reward: 0.597 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.700, 10.098], loss: 0.001333, mae: 0.039993, mean_q: 1.170114
 61265/100000: episode: 1154, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 58.915, mean reward: 0.589 [0.512, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.398, 10.098], loss: 0.001393, mae: 0.040422, mean_q: 1.168850
 61365/100000: episode: 1155, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.158, mean reward: 0.592 [0.511, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.966, 10.098], loss: 0.001390, mae: 0.040811, mean_q: 1.167851
 61465/100000: episode: 1156, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 59.398, mean reward: 0.594 [0.504, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.241, 10.098], loss: 0.001381, mae: 0.040348, mean_q: 1.170912
 61565/100000: episode: 1157, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.545, mean reward: 0.595 [0.524, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.776, 10.098], loss: 0.001402, mae: 0.041053, mean_q: 1.172456
 61665/100000: episode: 1158, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 61.524, mean reward: 0.615 [0.505, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.662, 10.298], loss: 0.001358, mae: 0.040160, mean_q: 1.169056
 61765/100000: episode: 1159, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 60.376, mean reward: 0.604 [0.505, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.627, 10.098], loss: 0.001417, mae: 0.041156, mean_q: 1.172257
 61865/100000: episode: 1160, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.234, mean reward: 0.572 [0.498, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.926, 10.098], loss: 0.001555, mae: 0.042618, mean_q: 1.173451
 61965/100000: episode: 1161, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 57.133, mean reward: 0.571 [0.499, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.098], loss: 0.001365, mae: 0.040824, mean_q: 1.172188
 62065/100000: episode: 1162, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.816, mean reward: 0.578 [0.502, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.453, 10.098], loss: 0.001331, mae: 0.040237, mean_q: 1.170630
 62165/100000: episode: 1163, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 60.677, mean reward: 0.607 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.696, 10.098], loss: 0.001349, mae: 0.040158, mean_q: 1.171752
 62265/100000: episode: 1164, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.482, mean reward: 0.585 [0.504, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.882, 10.098], loss: 0.001405, mae: 0.041448, mean_q: 1.170600
 62365/100000: episode: 1165, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 57.176, mean reward: 0.572 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.764, 10.098], loss: 0.001390, mae: 0.040407, mean_q: 1.169957
 62465/100000: episode: 1166, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 63.402, mean reward: 0.634 [0.518, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.361, 10.282], loss: 0.001282, mae: 0.039519, mean_q: 1.168669
 62565/100000: episode: 1167, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.921, mean reward: 0.589 [0.512, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.588, 10.326], loss: 0.001421, mae: 0.041406, mean_q: 1.174566
 62665/100000: episode: 1168, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 59.541, mean reward: 0.595 [0.508, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.165, 10.190], loss: 0.001381, mae: 0.041006, mean_q: 1.171463
 62765/100000: episode: 1169, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.278, mean reward: 0.573 [0.498, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.827, 10.098], loss: 0.001377, mae: 0.040940, mean_q: 1.177030
[Info] 1-TH LEVEL FOUND: 1.3938908576965332, Considering 10/90 traces
 62865/100000: episode: 1170, duration: 4.744s, episode steps: 100, steps per second: 21, episode reward: 58.121, mean reward: 0.581 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.610, 10.098], loss: 0.001390, mae: 0.041051, mean_q: 1.175151
 62878/100000: episode: 1171, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 8.763, mean reward: 0.674 [0.595, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.092, 10.100], loss: 0.001233, mae: 0.038620, mean_q: 1.165715
 62907/100000: episode: 1172, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 18.841, mean reward: 0.650 [0.606, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.921, 10.299], loss: 0.001423, mae: 0.041370, mean_q: 1.169441
 62936/100000: episode: 1173, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 18.170, mean reward: 0.627 [0.511, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.232, 10.255], loss: 0.001366, mae: 0.040993, mean_q: 1.172240
 62975/100000: episode: 1174, duration: 0.228s, episode steps: 39, steps per second: 171, episode reward: 27.586, mean reward: 0.707 [0.645, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.541, 10.100], loss: 0.001351, mae: 0.039887, mean_q: 1.170638
 63004/100000: episode: 1175, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 19.650, mean reward: 0.678 [0.588, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.666, 10.289], loss: 0.001492, mae: 0.041171, mean_q: 1.176003
 63034/100000: episode: 1176, duration: 0.176s, episode steps: 30, steps per second: 171, episode reward: 19.426, mean reward: 0.648 [0.575, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.104, 10.315], loss: 0.001270, mae: 0.039839, mean_q: 1.173289
 63043/100000: episode: 1177, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 6.149, mean reward: 0.683 [0.645, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.744, 10.100], loss: 0.001412, mae: 0.041792, mean_q: 1.178643
 63069/100000: episode: 1178, duration: 0.127s, episode steps: 26, steps per second: 204, episode reward: 17.391, mean reward: 0.669 [0.577, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.413, 10.100], loss: 0.001378, mae: 0.040268, mean_q: 1.173029
 63082/100000: episode: 1179, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 9.311, mean reward: 0.716 [0.681, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.271, 10.100], loss: 0.002357, mae: 0.052329, mean_q: 1.165407
 63130/100000: episode: 1180, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 32.697, mean reward: 0.681 [0.588, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.465, 10.424], loss: 0.001494, mae: 0.042566, mean_q: 1.176420
 63143/100000: episode: 1181, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 9.565, mean reward: 0.736 [0.677, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.298, 10.100], loss: 0.001532, mae: 0.042810, mean_q: 1.180575
 63172/100000: episode: 1182, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 19.306, mean reward: 0.666 [0.541, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.319, 10.286], loss: 0.001531, mae: 0.041857, mean_q: 1.180218
 63204/100000: episode: 1183, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 21.573, mean reward: 0.674 [0.571, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.180, 10.100], loss: 0.001409, mae: 0.040317, mean_q: 1.179333
 63233/100000: episode: 1184, duration: 0.171s, episode steps: 29, steps per second: 170, episode reward: 18.403, mean reward: 0.635 [0.570, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.056, 10.320], loss: 0.001834, mae: 0.044220, mean_q: 1.183226
 63262/100000: episode: 1185, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 19.016, mean reward: 0.656 [0.581, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.629, 10.331], loss: 0.001374, mae: 0.041011, mean_q: 1.180143
 63288/100000: episode: 1186, duration: 0.136s, episode steps: 26, steps per second: 190, episode reward: 19.732, mean reward: 0.759 [0.657, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.600, 10.100], loss: 0.001251, mae: 0.039109, mean_q: 1.184349
 63297/100000: episode: 1187, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 6.190, mean reward: 0.688 [0.641, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.718, 10.100], loss: 0.001694, mae: 0.044212, mean_q: 1.188337
 63329/100000: episode: 1188, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 22.083, mean reward: 0.690 [0.587, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.288, 10.100], loss: 0.001471, mae: 0.041064, mean_q: 1.181561
 63358/100000: episode: 1189, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 19.406, mean reward: 0.669 [0.609, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.148, 10.424], loss: 0.001758, mae: 0.044838, mean_q: 1.187611
 63387/100000: episode: 1190, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 18.979, mean reward: 0.654 [0.573, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.943, 10.318], loss: 0.001608, mae: 0.044133, mean_q: 1.179198
 63417/100000: episode: 1191, duration: 0.177s, episode steps: 30, steps per second: 169, episode reward: 20.621, mean reward: 0.687 [0.591, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.723, 10.356], loss: 0.001573, mae: 0.041152, mean_q: 1.189112
 63430/100000: episode: 1192, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 9.292, mean reward: 0.715 [0.667, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.289, 10.100], loss: 0.001742, mae: 0.045122, mean_q: 1.188251
 63462/100000: episode: 1193, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 20.000, mean reward: 0.625 [0.512, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.671, 10.100], loss: 0.001528, mae: 0.042118, mean_q: 1.178526
 63510/100000: episode: 1194, duration: 0.238s, episode steps: 48, steps per second: 202, episode reward: 30.893, mean reward: 0.644 [0.556, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.337, 10.148], loss: 0.001621, mae: 0.043207, mean_q: 1.185787
 63539/100000: episode: 1195, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 19.608, mean reward: 0.676 [0.574, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.701, 10.320], loss: 0.001705, mae: 0.043516, mean_q: 1.183169
 63562/100000: episode: 1196, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 14.871, mean reward: 0.647 [0.585, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.211, 10.421], loss: 0.001258, mae: 0.038289, mean_q: 1.183566
 63610/100000: episode: 1197, duration: 0.248s, episode steps: 48, steps per second: 193, episode reward: 33.131, mean reward: 0.690 [0.605, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.271, 10.399], loss: 0.001522, mae: 0.041534, mean_q: 1.185155
 63649/100000: episode: 1198, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 25.980, mean reward: 0.666 [0.601, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.393, 10.100], loss: 0.001690, mae: 0.044235, mean_q: 1.192840
 63675/100000: episode: 1199, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 18.495, mean reward: 0.711 [0.648, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.392, 10.100], loss: 0.001671, mae: 0.044159, mean_q: 1.195732
 63698/100000: episode: 1200, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 14.330, mean reward: 0.623 [0.558, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.431, 10.265], loss: 0.001592, mae: 0.042700, mean_q: 1.190678
 63721/100000: episode: 1201, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 15.982, mean reward: 0.695 [0.618, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.384, 10.507], loss: 0.001338, mae: 0.039497, mean_q: 1.196220
 63750/100000: episode: 1202, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 20.440, mean reward: 0.705 [0.669, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.403], loss: 0.001480, mae: 0.042228, mean_q: 1.195571
 63779/100000: episode: 1203, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 20.000, mean reward: 0.690 [0.649, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.406], loss: 0.001694, mae: 0.043611, mean_q: 1.195516
 63827/100000: episode: 1204, duration: 0.241s, episode steps: 48, steps per second: 199, episode reward: 28.079, mean reward: 0.585 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.236, 10.100], loss: 0.001470, mae: 0.041551, mean_q: 1.196676
 63875/100000: episode: 1205, duration: 0.247s, episode steps: 48, steps per second: 194, episode reward: 29.431, mean reward: 0.613 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.238, 10.100], loss: 0.001555, mae: 0.042136, mean_q: 1.193745
 63904/100000: episode: 1206, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 18.228, mean reward: 0.629 [0.555, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.217, 10.343], loss: 0.001333, mae: 0.039870, mean_q: 1.198525
 63917/100000: episode: 1207, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 10.286, mean reward: 0.791 [0.702, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.714, 10.100], loss: 0.001466, mae: 0.041919, mean_q: 1.186109
 63940/100000: episode: 1208, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 15.863, mean reward: 0.690 [0.626, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.429], loss: 0.001974, mae: 0.046740, mean_q: 1.194746
 63963/100000: episode: 1209, duration: 0.144s, episode steps: 23, steps per second: 160, episode reward: 14.573, mean reward: 0.634 [0.535, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.225, 10.173], loss: 0.001437, mae: 0.041534, mean_q: 1.196845
 64011/100000: episode: 1210, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 28.351, mean reward: 0.591 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.537, 10.160], loss: 0.001523, mae: 0.042279, mean_q: 1.195563
 64034/100000: episode: 1211, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 14.761, mean reward: 0.642 [0.551, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.111, 10.251], loss: 0.001756, mae: 0.045493, mean_q: 1.193485
 64047/100000: episode: 1212, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 9.378, mean reward: 0.721 [0.677, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.235, 10.100], loss: 0.001752, mae: 0.045007, mean_q: 1.180743
 64060/100000: episode: 1213, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.608, mean reward: 0.739 [0.672, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.265, 10.100], loss: 0.001764, mae: 0.044475, mean_q: 1.190192
 64099/100000: episode: 1214, duration: 0.236s, episode steps: 39, steps per second: 165, episode reward: 26.369, mean reward: 0.676 [0.582, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.286, 10.100], loss: 0.001543, mae: 0.042943, mean_q: 1.196542
 64131/100000: episode: 1215, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 22.299, mean reward: 0.697 [0.630, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.195, 10.100], loss: 0.001874, mae: 0.045533, mean_q: 1.195481
 64161/100000: episode: 1216, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 22.319, mean reward: 0.744 [0.665, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.371, 10.446], loss: 0.001487, mae: 0.041532, mean_q: 1.192338
 64209/100000: episode: 1217, duration: 0.271s, episode steps: 48, steps per second: 177, episode reward: 29.130, mean reward: 0.607 [0.516, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-1.864, 10.171], loss: 0.001572, mae: 0.042824, mean_q: 1.196266
 64222/100000: episode: 1218, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.358, mean reward: 0.720 [0.649, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.343, 10.100], loss: 0.001640, mae: 0.041935, mean_q: 1.192605
 64261/100000: episode: 1219, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 26.004, mean reward: 0.667 [0.570, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.844, 10.100], loss: 0.001437, mae: 0.041275, mean_q: 1.194225
 64287/100000: episode: 1220, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 17.574, mean reward: 0.676 [0.582, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.596, 10.100], loss: 0.001571, mae: 0.042403, mean_q: 1.205117
 64313/100000: episode: 1221, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 17.057, mean reward: 0.656 [0.573, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.204, 10.100], loss: 0.001523, mae: 0.041892, mean_q: 1.193271
 64322/100000: episode: 1222, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 5.585, mean reward: 0.621 [0.553, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.300, 10.100], loss: 0.001697, mae: 0.044226, mean_q: 1.198464
 64345/100000: episode: 1223, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 14.814, mean reward: 0.644 [0.573, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.085, 10.264], loss: 0.001546, mae: 0.040804, mean_q: 1.199357
 64377/100000: episode: 1224, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 22.346, mean reward: 0.698 [0.637, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.117, 10.100], loss: 0.001441, mae: 0.040742, mean_q: 1.204331
 64400/100000: episode: 1225, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 14.880, mean reward: 0.647 [0.580, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.250], loss: 0.001370, mae: 0.040136, mean_q: 1.208531
 64423/100000: episode: 1226, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 14.892, mean reward: 0.647 [0.602, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.901, 10.326], loss: 0.001435, mae: 0.041252, mean_q: 1.209672
 64446/100000: episode: 1227, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 15.603, mean reward: 0.678 [0.616, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.029, 10.458], loss: 0.001555, mae: 0.042314, mean_q: 1.199792
 64476/100000: episode: 1228, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 21.433, mean reward: 0.714 [0.653, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.490, 10.412], loss: 0.001350, mae: 0.040293, mean_q: 1.202979
 64508/100000: episode: 1229, duration: 0.188s, episode steps: 32, steps per second: 170, episode reward: 23.088, mean reward: 0.722 [0.648, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.700, 10.100], loss: 0.001615, mae: 0.043822, mean_q: 1.208767
 64521/100000: episode: 1230, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 8.863, mean reward: 0.682 [0.617, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.158, 10.100], loss: 0.001500, mae: 0.040946, mean_q: 1.201921
 64530/100000: episode: 1231, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 5.829, mean reward: 0.648 [0.603, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.229, 10.100], loss: 0.001329, mae: 0.040819, mean_q: 1.217230
 64543/100000: episode: 1232, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 9.503, mean reward: 0.731 [0.656, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.293, 10.100], loss: 0.001309, mae: 0.038853, mean_q: 1.208507
 64552/100000: episode: 1233, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 5.771, mean reward: 0.641 [0.616, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.192, 10.100], loss: 0.001786, mae: 0.045858, mean_q: 1.194381
 64582/100000: episode: 1234, duration: 0.167s, episode steps: 30, steps per second: 179, episode reward: 21.859, mean reward: 0.729 [0.650, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.464, 10.416], loss: 0.001632, mae: 0.043432, mean_q: 1.208427
 64591/100000: episode: 1235, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 6.340, mean reward: 0.704 [0.657, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.250, 10.100], loss: 0.002006, mae: 0.046349, mean_q: 1.209547
 64614/100000: episode: 1236, duration: 0.136s, episode steps: 23, steps per second: 170, episode reward: 14.667, mean reward: 0.638 [0.586, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.356, 10.338], loss: 0.001653, mae: 0.042978, mean_q: 1.221863
 64623/100000: episode: 1237, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 6.275, mean reward: 0.697 [0.658, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.310, 10.100], loss: 0.001324, mae: 0.040732, mean_q: 1.220969
 64632/100000: episode: 1238, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 6.356, mean reward: 0.706 [0.663, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.427, 10.100], loss: 0.001331, mae: 0.038578, mean_q: 1.219051
 64661/100000: episode: 1239, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 19.128, mean reward: 0.660 [0.551, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.480, 10.344], loss: 0.001383, mae: 0.040526, mean_q: 1.213964
 64684/100000: episode: 1240, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 16.123, mean reward: 0.701 [0.610, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.645, 10.432], loss: 0.001607, mae: 0.041742, mean_q: 1.212784
 64723/100000: episode: 1241, duration: 0.207s, episode steps: 39, steps per second: 188, episode reward: 27.966, mean reward: 0.717 [0.617, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.540, 10.100], loss: 0.001325, mae: 0.039906, mean_q: 1.207387
 64762/100000: episode: 1242, duration: 0.209s, episode steps: 39, steps per second: 186, episode reward: 25.623, mean reward: 0.657 [0.550, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.493, 10.100], loss: 0.001567, mae: 0.042449, mean_q: 1.215565
 64775/100000: episode: 1243, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 8.884, mean reward: 0.683 [0.654, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.223, 10.100], loss: 0.001489, mae: 0.041136, mean_q: 1.208755
 64805/100000: episode: 1244, duration: 0.161s, episode steps: 30, steps per second: 187, episode reward: 20.518, mean reward: 0.684 [0.606, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.619, 10.357], loss: 0.001332, mae: 0.038812, mean_q: 1.215517
 64834/100000: episode: 1245, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 23.930, mean reward: 0.825 [0.722, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.449, 10.574], loss: 0.001186, mae: 0.038426, mean_q: 1.219748
 64863/100000: episode: 1246, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 19.644, mean reward: 0.677 [0.564, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.393, 10.293], loss: 0.001433, mae: 0.040577, mean_q: 1.221108
 64893/100000: episode: 1247, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 19.595, mean reward: 0.653 [0.601, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.398, 10.362], loss: 0.001470, mae: 0.042380, mean_q: 1.221522
 64932/100000: episode: 1248, duration: 0.225s, episode steps: 39, steps per second: 173, episode reward: 27.887, mean reward: 0.715 [0.563, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.517, 10.100], loss: 0.001704, mae: 0.044433, mean_q: 1.220554
 64955/100000: episode: 1249, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 15.136, mean reward: 0.658 [0.581, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.211, 10.361], loss: 0.001167, mae: 0.038086, mean_q: 1.213830
 64987/100000: episode: 1250, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 20.485, mean reward: 0.640 [0.545, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.118, 10.100], loss: 0.001320, mae: 0.040168, mean_q: 1.232794
 65000/100000: episode: 1251, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 8.920, mean reward: 0.686 [0.646, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.192, 10.100], loss: 0.001107, mae: 0.037167, mean_q: 1.228458
 65013/100000: episode: 1252, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 9.405, mean reward: 0.723 [0.674, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.310, 10.100], loss: 0.001476, mae: 0.040550, mean_q: 1.225266
 65026/100000: episode: 1253, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 9.956, mean reward: 0.766 [0.714, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.300, 10.100], loss: 0.001471, mae: 0.038816, mean_q: 1.222753
 65039/100000: episode: 1254, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.284, mean reward: 0.714 [0.650, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.233, 10.100], loss: 0.001094, mae: 0.036797, mean_q: 1.229286
 65071/100000: episode: 1255, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 23.140, mean reward: 0.723 [0.603, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.367, 10.100], loss: 0.001371, mae: 0.041132, mean_q: 1.236613
 65100/100000: episode: 1256, duration: 0.172s, episode steps: 29, steps per second: 168, episode reward: 19.127, mean reward: 0.660 [0.552, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.291, 10.239], loss: 0.001637, mae: 0.043082, mean_q: 1.224962
 65129/100000: episode: 1257, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 17.423, mean reward: 0.601 [0.526, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.436], loss: 0.001377, mae: 0.041241, mean_q: 1.226691
 65177/100000: episode: 1258, duration: 0.268s, episode steps: 48, steps per second: 179, episode reward: 33.032, mean reward: 0.688 [0.598, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.592, 10.608], loss: 0.001522, mae: 0.042117, mean_q: 1.227653
 65203/100000: episode: 1259, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 16.600, mean reward: 0.638 [0.572, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.251, 10.100], loss: 0.001239, mae: 0.037878, mean_q: 1.231974
[Info] 2-TH LEVEL FOUND: 1.519929051399231, Considering 10/90 traces
 65232/100000: episode: 1260, duration: 4.349s, episode steps: 29, steps per second: 7, episode reward: 18.296, mean reward: 0.631 [0.565, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.060, 10.241], loss: 0.001525, mae: 0.042584, mean_q: 1.224830
 65260/100000: episode: 1261, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 20.327, mean reward: 0.726 [0.611, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.255, 10.100], loss: 0.001652, mae: 0.043579, mean_q: 1.232817
 65271/100000: episode: 1262, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 8.067, mean reward: 0.733 [0.644, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.195, 10.100], loss: 0.001189, mae: 0.039126, mean_q: 1.225226
 65299/100000: episode: 1263, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 18.221, mean reward: 0.651 [0.511, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.054, 10.100], loss: 0.001477, mae: 0.041993, mean_q: 1.228730
 65327/100000: episode: 1264, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 20.360, mean reward: 0.727 [0.552, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.190, 10.100], loss: 0.001365, mae: 0.040100, mean_q: 1.235316
 65362/100000: episode: 1265, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 23.852, mean reward: 0.681 [0.550, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.035, 10.345], loss: 0.001373, mae: 0.039722, mean_q: 1.229598
 65371/100000: episode: 1266, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 6.724, mean reward: 0.747 [0.714, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.296, 10.100], loss: 0.001292, mae: 0.038033, mean_q: 1.233832
 65393/100000: episode: 1267, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 15.929, mean reward: 0.724 [0.630, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.930, 10.351], loss: 0.001312, mae: 0.039310, mean_q: 1.239724
 65411/100000: episode: 1268, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 13.914, mean reward: 0.773 [0.716, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.521], loss: 0.001492, mae: 0.041609, mean_q: 1.229761
 65420/100000: episode: 1269, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 7.030, mean reward: 0.781 [0.741, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.439, 10.100], loss: 0.001901, mae: 0.044079, mean_q: 1.238488
 65446/100000: episode: 1270, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 19.104, mean reward: 0.735 [0.592, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.135, 10.299], loss: 0.001541, mae: 0.041416, mean_q: 1.233255
 65455/100000: episode: 1271, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 6.739, mean reward: 0.749 [0.736, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.379, 10.100], loss: 0.001276, mae: 0.037003, mean_q: 1.231676
 65490/100000: episode: 1272, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 23.198, mean reward: 0.663 [0.509, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.358, 10.175], loss: 0.001396, mae: 0.040754, mean_q: 1.244505
 65518/100000: episode: 1273, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 20.720, mean reward: 0.740 [0.637, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.858, 10.100], loss: 0.001263, mae: 0.038877, mean_q: 1.234337
 65536/100000: episode: 1274, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 12.991, mean reward: 0.722 [0.658, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.621, 10.429], loss: 0.001273, mae: 0.039843, mean_q: 1.254642
 65555/100000: episode: 1275, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 13.429, mean reward: 0.707 [0.652, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.415], loss: 0.001383, mae: 0.040603, mean_q: 1.246653
 65566/100000: episode: 1276, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 8.504, mean reward: 0.773 [0.703, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.411, 10.100], loss: 0.001123, mae: 0.036585, mean_q: 1.260559
 65585/100000: episode: 1277, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 14.987, mean reward: 0.789 [0.747, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.698, 10.565], loss: 0.001380, mae: 0.039385, mean_q: 1.242760
 65611/100000: episode: 1278, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 18.664, mean reward: 0.718 [0.589, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.294, 10.320], loss: 0.001638, mae: 0.042922, mean_q: 1.249756
 65631/100000: episode: 1279, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 14.016, mean reward: 0.701 [0.633, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.611, 10.100], loss: 0.001490, mae: 0.042713, mean_q: 1.254506
 65651/100000: episode: 1280, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 15.655, mean reward: 0.783 [0.694, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.315, 10.100], loss: 0.001364, mae: 0.039061, mean_q: 1.257718
 65669/100000: episode: 1281, duration: 0.100s, episode steps: 18, steps per second: 179, episode reward: 13.321, mean reward: 0.740 [0.690, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.174, 10.544], loss: 0.001566, mae: 0.044016, mean_q: 1.251474
 65691/100000: episode: 1282, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 17.244, mean reward: 0.784 [0.742, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.586], loss: 0.001473, mae: 0.042155, mean_q: 1.248102
 65726/100000: episode: 1283, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 26.494, mean reward: 0.757 [0.567, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.588, 10.295], loss: 0.001510, mae: 0.041851, mean_q: 1.258883
 65745/100000: episode: 1284, duration: 0.093s, episode steps: 19, steps per second: 203, episode reward: 13.559, mean reward: 0.714 [0.625, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-1.085, 10.352], loss: 0.001602, mae: 0.044127, mean_q: 1.254464
 65767/100000: episode: 1285, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 15.989, mean reward: 0.727 [0.649, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.707, 10.402], loss: 0.001723, mae: 0.043702, mean_q: 1.252553
 65793/100000: episode: 1286, duration: 0.128s, episode steps: 26, steps per second: 204, episode reward: 18.682, mean reward: 0.719 [0.604, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.773, 10.312], loss: 0.001551, mae: 0.043372, mean_q: 1.255109
 65821/100000: episode: 1287, duration: 0.163s, episode steps: 28, steps per second: 171, episode reward: 20.634, mean reward: 0.737 [0.623, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.628, 10.100], loss: 0.001611, mae: 0.043724, mean_q: 1.253225
 65843/100000: episode: 1288, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 14.923, mean reward: 0.678 [0.626, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.041, 10.325], loss: 0.001450, mae: 0.041881, mean_q: 1.254073
 65865/100000: episode: 1289, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 17.664, mean reward: 0.803 [0.718, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.262, 10.642], loss: 0.001558, mae: 0.043270, mean_q: 1.262283
 65874/100000: episode: 1290, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 6.785, mean reward: 0.754 [0.722, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.257, 10.100], loss: 0.001517, mae: 0.040176, mean_q: 1.240237
 65893/100000: episode: 1291, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 14.167, mean reward: 0.746 [0.702, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.573, 10.459], loss: 0.001580, mae: 0.042767, mean_q: 1.262231
 65904/100000: episode: 1292, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 8.548, mean reward: 0.777 [0.734, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.440, 10.100], loss: 0.001431, mae: 0.040509, mean_q: 1.256502
 65915/100000: episode: 1293, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 8.522, mean reward: 0.775 [0.739, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.582, 10.100], loss: 0.001288, mae: 0.039042, mean_q: 1.274283
 65937/100000: episode: 1294, duration: 0.128s, episode steps: 22, steps per second: 171, episode reward: 14.746, mean reward: 0.670 [0.620, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.421], loss: 0.001446, mae: 0.040723, mean_q: 1.255263
 65956/100000: episode: 1295, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 14.881, mean reward: 0.783 [0.692, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.252, 10.461], loss: 0.001875, mae: 0.045991, mean_q: 1.264907
 65965/100000: episode: 1296, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 6.609, mean reward: 0.734 [0.686, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.422, 10.100], loss: 0.001895, mae: 0.045950, mean_q: 1.265727
 65987/100000: episode: 1297, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 16.383, mean reward: 0.745 [0.704, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.403, 10.516], loss: 0.001476, mae: 0.040808, mean_q: 1.251346
 65996/100000: episode: 1298, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 6.400, mean reward: 0.711 [0.650, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.262, 10.100], loss: 0.001421, mae: 0.040056, mean_q: 1.252133
 66007/100000: episode: 1299, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 8.366, mean reward: 0.761 [0.714, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.269, 10.100], loss: 0.001315, mae: 0.040902, mean_q: 1.259314
 66035/100000: episode: 1300, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 21.979, mean reward: 0.785 [0.677, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.622, 10.100], loss: 0.001312, mae: 0.038546, mean_q: 1.257931
 66070/100000: episode: 1301, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 24.034, mean reward: 0.687 [0.568, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.429, 10.199], loss: 0.001316, mae: 0.038517, mean_q: 1.255102
 66096/100000: episode: 1302, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 17.756, mean reward: 0.683 [0.588, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.729, 10.282], loss: 0.001588, mae: 0.044010, mean_q: 1.279252
 66118/100000: episode: 1303, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 16.157, mean reward: 0.734 [0.652, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.633, 10.391], loss: 0.001429, mae: 0.041338, mean_q: 1.266994
 66127/100000: episode: 1304, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 7.530, mean reward: 0.837 [0.764, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.365, 10.100], loss: 0.001370, mae: 0.041613, mean_q: 1.267435
 66147/100000: episode: 1305, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 14.677, mean reward: 0.734 [0.669, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.923, 10.100], loss: 0.001537, mae: 0.041215, mean_q: 1.267228
 66156/100000: episode: 1306, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 6.698, mean reward: 0.744 [0.714, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.331, 10.100], loss: 0.001315, mae: 0.038031, mean_q: 1.258263
 66175/100000: episode: 1307, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 14.685, mean reward: 0.773 [0.696, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.546], loss: 0.001607, mae: 0.043358, mean_q: 1.269197
 66184/100000: episode: 1308, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 6.529, mean reward: 0.725 [0.665, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.900, 10.100], loss: 0.001283, mae: 0.038866, mean_q: 1.275815
 66193/100000: episode: 1309, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 7.000, mean reward: 0.778 [0.730, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.715, 10.100], loss: 0.001505, mae: 0.043182, mean_q: 1.273947
 66212/100000: episode: 1310, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 13.945, mean reward: 0.734 [0.652, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.420], loss: 0.001142, mae: 0.037710, mean_q: 1.273436
 66234/100000: episode: 1311, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 16.246, mean reward: 0.738 [0.655, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.496, 10.497], loss: 0.001217, mae: 0.037371, mean_q: 1.276548
 66245/100000: episode: 1312, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 8.651, mean reward: 0.786 [0.761, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.295, 10.100], loss: 0.001270, mae: 0.038827, mean_q: 1.273473
 66280/100000: episode: 1313, duration: 0.199s, episode steps: 35, steps per second: 175, episode reward: 25.694, mean reward: 0.734 [0.690, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.035, 10.449], loss: 0.001359, mae: 0.039547, mean_q: 1.270546
 66308/100000: episode: 1314, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 21.026, mean reward: 0.751 [0.680, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.257, 10.100], loss: 0.001481, mae: 0.041396, mean_q: 1.287613
 66326/100000: episode: 1315, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 13.964, mean reward: 0.776 [0.638, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.477], loss: 0.001482, mae: 0.042295, mean_q: 1.255980
 66332/100000: episode: 1316, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 4.596, mean reward: 0.766 [0.693, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.324, 10.100], loss: 0.001242, mae: 0.040200, mean_q: 1.265290
 66367/100000: episode: 1317, duration: 0.170s, episode steps: 35, steps per second: 205, episode reward: 24.382, mean reward: 0.697 [0.623, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.035, 10.486], loss: 0.001562, mae: 0.042155, mean_q: 1.281013
 66385/100000: episode: 1318, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 13.448, mean reward: 0.747 [0.696, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.474], loss: 0.001316, mae: 0.039613, mean_q: 1.265521
 66407/100000: episode: 1319, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 18.820, mean reward: 0.855 [0.760, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.219, 10.664], loss: 0.001541, mae: 0.041491, mean_q: 1.277114
 66416/100000: episode: 1320, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 6.616, mean reward: 0.735 [0.674, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.286, 10.100], loss: 0.002000, mae: 0.050215, mean_q: 1.271027
 66427/100000: episode: 1321, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 9.094, mean reward: 0.827 [0.748, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-2.221, 10.100], loss: 0.001572, mae: 0.043847, mean_q: 1.276696
 66433/100000: episode: 1322, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 4.657, mean reward: 0.776 [0.708, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.389, 10.100], loss: 0.001824, mae: 0.043874, mean_q: 1.286014
 66444/100000: episode: 1323, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 8.159, mean reward: 0.742 [0.715, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.335, 10.100], loss: 0.001530, mae: 0.043300, mean_q: 1.277689
 66450/100000: episode: 1324, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 4.674, mean reward: 0.779 [0.756, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.599, 10.100], loss: 0.001400, mae: 0.039198, mean_q: 1.264931
 66470/100000: episode: 1325, duration: 0.098s, episode steps: 20, steps per second: 203, episode reward: 13.294, mean reward: 0.665 [0.542, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.124, 10.100], loss: 0.001311, mae: 0.040616, mean_q: 1.291301
 66481/100000: episode: 1326, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 9.205, mean reward: 0.837 [0.729, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.444, 10.100], loss: 0.001199, mae: 0.037138, mean_q: 1.278288
 66501/100000: episode: 1327, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 14.925, mean reward: 0.746 [0.704, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.458, 10.100], loss: 0.001283, mae: 0.038817, mean_q: 1.281301
 66510/100000: episode: 1328, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 6.610, mean reward: 0.734 [0.682, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.217, 10.100], loss: 0.001504, mae: 0.040442, mean_q: 1.292771
 66528/100000: episode: 1329, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 13.719, mean reward: 0.762 [0.704, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.049, 10.507], loss: 0.001195, mae: 0.037706, mean_q: 1.288056
 66537/100000: episode: 1330, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 7.132, mean reward: 0.792 [0.748, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.416, 10.100], loss: 0.001146, mae: 0.037663, mean_q: 1.298275
 66559/100000: episode: 1331, duration: 0.132s, episode steps: 22, steps per second: 166, episode reward: 15.853, mean reward: 0.721 [0.656, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.443, 10.421], loss: 0.001257, mae: 0.038565, mean_q: 1.288431
 66578/100000: episode: 1332, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 13.447, mean reward: 0.708 [0.642, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.276, 10.330], loss: 0.001220, mae: 0.039293, mean_q: 1.271288
 66589/100000: episode: 1333, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 8.066, mean reward: 0.733 [0.679, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.340, 10.100], loss: 0.001586, mae: 0.046496, mean_q: 1.293581
 66598/100000: episode: 1334, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 6.618, mean reward: 0.735 [0.683, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-1.877, 10.100], loss: 0.001713, mae: 0.044616, mean_q: 1.301003
 66604/100000: episode: 1335, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 4.473, mean reward: 0.746 [0.730, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.295, 10.100], loss: 0.001413, mae: 0.043066, mean_q: 1.332572
 66613/100000: episode: 1336, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 6.869, mean reward: 0.763 [0.743, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.373, 10.100], loss: 0.002186, mae: 0.049788, mean_q: 1.290694
 66622/100000: episode: 1337, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 7.423, mean reward: 0.825 [0.776, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.661, 10.100], loss: 0.001242, mae: 0.038996, mean_q: 1.291906
 66628/100000: episode: 1338, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 4.499, mean reward: 0.750 [0.698, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.347, 10.100], loss: 0.001112, mae: 0.038735, mean_q: 1.306565
 66654/100000: episode: 1339, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 18.266, mean reward: 0.703 [0.656, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.755, 10.450], loss: 0.001215, mae: 0.037707, mean_q: 1.281544
 66674/100000: episode: 1340, duration: 0.100s, episode steps: 20, steps per second: 199, episode reward: 13.813, mean reward: 0.691 [0.593, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.131, 10.100], loss: 0.001177, mae: 0.037438, mean_q: 1.295555
 66693/100000: episode: 1341, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 15.065, mean reward: 0.793 [0.748, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.867, 10.578], loss: 0.001498, mae: 0.041000, mean_q: 1.284188
 66721/100000: episode: 1342, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 21.462, mean reward: 0.766 [0.638, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.164, 10.100], loss: 0.001336, mae: 0.040024, mean_q: 1.288887
 66730/100000: episode: 1343, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 6.801, mean reward: 0.756 [0.735, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.372, 10.100], loss: 0.001631, mae: 0.044065, mean_q: 1.260346
 66739/100000: episode: 1344, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 6.910, mean reward: 0.768 [0.740, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.408, 10.100], loss: 0.001189, mae: 0.037604, mean_q: 1.285882
 66757/100000: episode: 1345, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 13.978, mean reward: 0.777 [0.729, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.426], loss: 0.001241, mae: 0.038735, mean_q: 1.298043
 66783/100000: episode: 1346, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 17.374, mean reward: 0.668 [0.566, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.257, 10.295], loss: 0.001229, mae: 0.038614, mean_q: 1.300834
 66801/100000: episode: 1347, duration: 0.115s, episode steps: 18, steps per second: 156, episode reward: 14.170, mean reward: 0.787 [0.743, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.582], loss: 0.001224, mae: 0.038812, mean_q: 1.321474
 66810/100000: episode: 1348, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 6.718, mean reward: 0.746 [0.716, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.329, 10.100], loss: 0.001399, mae: 0.038290, mean_q: 1.288798
 66845/100000: episode: 1349, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 26.709, mean reward: 0.763 [0.562, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.280], loss: 0.001214, mae: 0.038819, mean_q: 1.305900
[Info] 3-TH LEVEL FOUND: 1.628711223602295, Considering 10/90 traces
 66856/100000: episode: 1350, duration: 4.323s, episode steps: 11, steps per second: 3, episode reward: 8.626, mean reward: 0.784 [0.755, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.327, 10.100], loss: 0.001420, mae: 0.042795, mean_q: 1.312807
 66887/100000: episode: 1351, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 22.491, mean reward: 0.726 [0.539, 0.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.861, 10.284], loss: 0.001524, mae: 0.042869, mean_q: 1.306513
 66917/100000: episode: 1352, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 20.983, mean reward: 0.699 [0.555, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.585, 10.354], loss: 0.001317, mae: 0.040000, mean_q: 1.302627
 66949/100000: episode: 1353, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 23.612, mean reward: 0.738 [0.663, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.676, 10.523], loss: 0.001487, mae: 0.042320, mean_q: 1.313818
 66962/100000: episode: 1354, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 9.860, mean reward: 0.758 [0.682, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.510], loss: 0.001232, mae: 0.039401, mean_q: 1.299748
 66979/100000: episode: 1355, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 13.673, mean reward: 0.804 [0.705, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.731, 10.519], loss: 0.001459, mae: 0.040139, mean_q: 1.311630
 66996/100000: episode: 1356, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 13.510, mean reward: 0.795 [0.656, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.421], loss: 0.001694, mae: 0.044331, mean_q: 1.301621
 67029/100000: episode: 1357, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 24.776, mean reward: 0.751 [0.659, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.404, 10.351], loss: 0.001294, mae: 0.039855, mean_q: 1.312523
 67060/100000: episode: 1358, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 21.892, mean reward: 0.706 [0.601, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.245, 10.298], loss: 0.001165, mae: 0.037925, mean_q: 1.313650
 67094/100000: episode: 1359, duration: 0.203s, episode steps: 34, steps per second: 167, episode reward: 25.483, mean reward: 0.749 [0.645, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.293, 10.453], loss: 0.001200, mae: 0.037715, mean_q: 1.294459
 67127/100000: episode: 1360, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 21.881, mean reward: 0.663 [0.603, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.035, 10.325], loss: 0.001366, mae: 0.040217, mean_q: 1.312523
 67157/100000: episode: 1361, duration: 0.170s, episode steps: 30, steps per second: 177, episode reward: 20.797, mean reward: 0.693 [0.572, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.155, 10.290], loss: 0.001449, mae: 0.039958, mean_q: 1.314701
 67174/100000: episode: 1362, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 12.266, mean reward: 0.722 [0.568, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.237, 10.263], loss: 0.001201, mae: 0.037737, mean_q: 1.325922
 67192/100000: episode: 1363, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 14.616, mean reward: 0.812 [0.746, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.523], loss: 0.001290, mae: 0.040645, mean_q: 1.317986
 67224/100000: episode: 1364, duration: 0.181s, episode steps: 32, steps per second: 177, episode reward: 23.042, mean reward: 0.720 [0.642, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.103, 10.490], loss: 0.001299, mae: 0.040317, mean_q: 1.315204
 67255/100000: episode: 1365, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 24.356, mean reward: 0.786 [0.719, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.044, 10.536], loss: 0.001219, mae: 0.037841, mean_q: 1.328982
 67287/100000: episode: 1366, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 23.103, mean reward: 0.722 [0.624, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.035, 10.467], loss: 0.001467, mae: 0.042744, mean_q: 1.328395
 67318/100000: episode: 1367, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 22.920, mean reward: 0.739 [0.671, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.146, 10.493], loss: 0.001258, mae: 0.039169, mean_q: 1.324016
 67335/100000: episode: 1368, duration: 0.108s, episode steps: 17, steps per second: 158, episode reward: 14.442, mean reward: 0.850 [0.808, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.630], loss: 0.001286, mae: 0.039306, mean_q: 1.328344
 67348/100000: episode: 1369, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 9.115, mean reward: 0.701 [0.640, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-1.379, 10.375], loss: 0.001090, mae: 0.036823, mean_q: 1.338557
 67380/100000: episode: 1370, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 22.600, mean reward: 0.706 [0.633, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.363], loss: 0.001202, mae: 0.038087, mean_q: 1.324213
 67414/100000: episode: 1371, duration: 0.166s, episode steps: 34, steps per second: 204, episode reward: 21.681, mean reward: 0.638 [0.517, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.035, 10.208], loss: 0.001099, mae: 0.037016, mean_q: 1.337664
 67427/100000: episode: 1372, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.704, mean reward: 0.746 [0.678, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.575], loss: 0.001078, mae: 0.037302, mean_q: 1.331373
 67460/100000: episode: 1373, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 26.810, mean reward: 0.812 [0.740, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.546, 10.570], loss: 0.001251, mae: 0.038723, mean_q: 1.317948
 67492/100000: episode: 1374, duration: 0.188s, episode steps: 32, steps per second: 171, episode reward: 22.275, mean reward: 0.696 [0.586, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.495, 10.303], loss: 0.001221, mae: 0.038725, mean_q: 1.332980
 67526/100000: episode: 1375, duration: 0.175s, episode steps: 34, steps per second: 195, episode reward: 25.330, mean reward: 0.745 [0.624, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.372], loss: 0.001496, mae: 0.043141, mean_q: 1.344532
 67544/100000: episode: 1376, duration: 0.101s, episode steps: 18, steps per second: 177, episode reward: 14.530, mean reward: 0.807 [0.687, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.349, 10.575], loss: 0.001132, mae: 0.037627, mean_q: 1.335361
 67576/100000: episode: 1377, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 25.330, mean reward: 0.792 [0.717, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.335, 10.527], loss: 0.001196, mae: 0.036799, mean_q: 1.336370
 67589/100000: episode: 1378, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 10.735, mean reward: 0.826 [0.777, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.180, 10.519], loss: 0.001237, mae: 0.039697, mean_q: 1.349712
 67606/100000: episode: 1379, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 13.592, mean reward: 0.800 [0.729, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.534], loss: 0.001353, mae: 0.040060, mean_q: 1.338632
 67638/100000: episode: 1380, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 22.912, mean reward: 0.716 [0.588, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.713, 10.328], loss: 0.001180, mae: 0.037364, mean_q: 1.349820
 67655/100000: episode: 1381, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 13.695, mean reward: 0.806 [0.736, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.502], loss: 0.001112, mae: 0.036975, mean_q: 1.350888
 67686/100000: episode: 1382, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 22.999, mean reward: 0.742 [0.603, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.310, 10.348], loss: 0.001194, mae: 0.038541, mean_q: 1.354587
 67703/100000: episode: 1383, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 12.147, mean reward: 0.715 [0.615, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.246, 10.310], loss: 0.001401, mae: 0.041955, mean_q: 1.360500
 67720/100000: episode: 1384, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 13.191, mean reward: 0.776 [0.653, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.046, 10.392], loss: 0.001144, mae: 0.037967, mean_q: 1.364792
 67737/100000: episode: 1385, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 13.062, mean reward: 0.768 [0.727, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.094, 10.524], loss: 0.001147, mae: 0.038921, mean_q: 1.339773
 67770/100000: episode: 1386, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 22.750, mean reward: 0.689 [0.580, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.638, 10.201], loss: 0.001267, mae: 0.038998, mean_q: 1.350319
 67801/100000: episode: 1387, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 23.714, mean reward: 0.765 [0.680, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.684, 10.510], loss: 0.001379, mae: 0.041588, mean_q: 1.352733
 67831/100000: episode: 1388, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 20.179, mean reward: 0.673 [0.561, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.360, 10.236], loss: 0.001170, mae: 0.037775, mean_q: 1.348897
 67864/100000: episode: 1389, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 22.266, mean reward: 0.675 [0.603, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.324], loss: 0.001249, mae: 0.038462, mean_q: 1.361916
 67898/100000: episode: 1390, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 23.063, mean reward: 0.678 [0.547, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.211, 10.161], loss: 0.001298, mae: 0.039518, mean_q: 1.348802
 67931/100000: episode: 1391, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 22.196, mean reward: 0.673 [0.538, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-1.176, 10.352], loss: 0.001470, mae: 0.042859, mean_q: 1.346085
 67948/100000: episode: 1392, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 13.354, mean reward: 0.786 [0.737, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.237, 10.619], loss: 0.001163, mae: 0.038144, mean_q: 1.333969
 67978/100000: episode: 1393, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 20.981, mean reward: 0.699 [0.509, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.192, 10.139], loss: 0.001154, mae: 0.037799, mean_q: 1.345014
 68008/100000: episode: 1394, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 20.970, mean reward: 0.699 [0.576, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.344, 10.313], loss: 0.001214, mae: 0.038730, mean_q: 1.350107
 68042/100000: episode: 1395, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 23.883, mean reward: 0.702 [0.541, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.332, 10.146], loss: 0.001270, mae: 0.039344, mean_q: 1.361119
 68076/100000: episode: 1396, duration: 0.181s, episode steps: 34, steps per second: 187, episode reward: 23.891, mean reward: 0.703 [0.539, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.355, 10.244], loss: 0.001357, mae: 0.040897, mean_q: 1.336077
 68093/100000: episode: 1397, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 13.414, mean reward: 0.789 [0.652, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.340], loss: 0.001277, mae: 0.039859, mean_q: 1.364968
 68126/100000: episode: 1398, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 21.787, mean reward: 0.660 [0.589, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.355, 10.418], loss: 0.001346, mae: 0.039889, mean_q: 1.363569
 68144/100000: episode: 1399, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 14.277, mean reward: 0.793 [0.747, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.523], loss: 0.001001, mae: 0.035315, mean_q: 1.360347
 68174/100000: episode: 1400, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 23.439, mean reward: 0.781 [0.680, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.423], loss: 0.001231, mae: 0.038149, mean_q: 1.358477
 68191/100000: episode: 1401, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 13.457, mean reward: 0.792 [0.714, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.600, 10.412], loss: 0.001089, mae: 0.036681, mean_q: 1.359776
 68223/100000: episode: 1402, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 23.317, mean reward: 0.729 [0.646, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.411], loss: 0.001179, mae: 0.037781, mean_q: 1.355603
 68256/100000: episode: 1403, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 22.350, mean reward: 0.677 [0.554, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.096, 10.307], loss: 0.001393, mae: 0.041185, mean_q: 1.367749
 68288/100000: episode: 1404, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 22.464, mean reward: 0.702 [0.636, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.184, 10.398], loss: 0.001186, mae: 0.037862, mean_q: 1.364075
 68305/100000: episode: 1405, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 13.921, mean reward: 0.819 [0.776, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.241, 10.514], loss: 0.001069, mae: 0.035887, mean_q: 1.358951
 68322/100000: episode: 1406, duration: 0.105s, episode steps: 17, steps per second: 161, episode reward: 14.037, mean reward: 0.826 [0.780, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.430, 10.577], loss: 0.001160, mae: 0.037098, mean_q: 1.344891
 68339/100000: episode: 1407, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 13.111, mean reward: 0.771 [0.696, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.106, 10.463], loss: 0.001247, mae: 0.040163, mean_q: 1.365156
 68372/100000: episode: 1408, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 23.713, mean reward: 0.719 [0.606, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.035, 10.286], loss: 0.001106, mae: 0.036040, mean_q: 1.369832
 68406/100000: episode: 1409, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 23.883, mean reward: 0.702 [0.579, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.555, 10.247], loss: 0.001220, mae: 0.038742, mean_q: 1.362203
 68436/100000: episode: 1410, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 19.784, mean reward: 0.659 [0.587, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.035, 10.272], loss: 0.001243, mae: 0.039497, mean_q: 1.361337
 68454/100000: episode: 1411, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 13.837, mean reward: 0.769 [0.730, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.522], loss: 0.001443, mae: 0.042204, mean_q: 1.353726
 68487/100000: episode: 1412, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 25.551, mean reward: 0.774 [0.696, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.214, 10.403], loss: 0.001146, mae: 0.037297, mean_q: 1.353356
 68504/100000: episode: 1413, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 14.045, mean reward: 0.826 [0.742, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.603], loss: 0.001042, mae: 0.034866, mean_q: 1.351686
 68522/100000: episode: 1414, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 14.965, mean reward: 0.831 [0.756, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.753, 10.654], loss: 0.001130, mae: 0.037382, mean_q: 1.363031
 68556/100000: episode: 1415, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 23.190, mean reward: 0.682 [0.565, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.035, 10.268], loss: 0.001246, mae: 0.038036, mean_q: 1.370804
 68587/100000: episode: 1416, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 23.092, mean reward: 0.745 [0.664, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.829, 10.431], loss: 0.001299, mae: 0.040227, mean_q: 1.369278
 68604/100000: episode: 1417, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 11.651, mean reward: 0.685 [0.589, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.506, 10.372], loss: 0.001272, mae: 0.039675, mean_q: 1.364784
 68634/100000: episode: 1418, duration: 0.171s, episode steps: 30, steps per second: 176, episode reward: 22.225, mean reward: 0.741 [0.661, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.324, 10.388], loss: 0.001275, mae: 0.039228, mean_q: 1.369792
 68666/100000: episode: 1419, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 22.098, mean reward: 0.691 [0.531, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.218], loss: 0.001188, mae: 0.038596, mean_q: 1.369267
 68684/100000: episode: 1420, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 13.727, mean reward: 0.763 [0.624, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.125, 10.446], loss: 0.001243, mae: 0.038836, mean_q: 1.374098
 68718/100000: episode: 1421, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 23.997, mean reward: 0.706 [0.588, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.206, 10.304], loss: 0.001351, mae: 0.040980, mean_q: 1.356398
 68752/100000: episode: 1422, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 24.143, mean reward: 0.710 [0.586, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.185, 10.335], loss: 0.001141, mae: 0.036801, mean_q: 1.364694
 68784/100000: episode: 1423, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 24.054, mean reward: 0.752 [0.686, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.035, 10.473], loss: 0.001196, mae: 0.037732, mean_q: 1.368499
 68801/100000: episode: 1424, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 14.974, mean reward: 0.881 [0.822, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.100, 10.673], loss: 0.001302, mae: 0.040265, mean_q: 1.372131
 68832/100000: episode: 1425, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 23.901, mean reward: 0.771 [0.635, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.122, 10.278], loss: 0.001407, mae: 0.041699, mean_q: 1.387721
 68863/100000: episode: 1426, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 21.404, mean reward: 0.690 [0.565, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.299], loss: 0.001213, mae: 0.038336, mean_q: 1.377912
 68897/100000: episode: 1427, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 23.296, mean reward: 0.685 [0.539, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.395, 10.316], loss: 0.001150, mae: 0.037782, mean_q: 1.370342
 68930/100000: episode: 1428, duration: 0.186s, episode steps: 33, steps per second: 178, episode reward: 22.630, mean reward: 0.686 [0.601, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.944, 10.368], loss: 0.001370, mae: 0.040406, mean_q: 1.380509
 68947/100000: episode: 1429, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 14.216, mean reward: 0.836 [0.687, 0.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.578], loss: 0.001516, mae: 0.044125, mean_q: 1.391777
 68978/100000: episode: 1430, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 24.800, mean reward: 0.800 [0.697, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.422, 10.518], loss: 0.001288, mae: 0.039400, mean_q: 1.386348
 69010/100000: episode: 1431, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 23.425, mean reward: 0.732 [0.664, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.805, 10.479], loss: 0.001161, mae: 0.037143, mean_q: 1.371993
 69043/100000: episode: 1432, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 22.281, mean reward: 0.675 [0.552, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.316, 10.372], loss: 0.001174, mae: 0.037892, mean_q: 1.380388
 69061/100000: episode: 1433, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 14.482, mean reward: 0.805 [0.740, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.784, 10.597], loss: 0.001052, mae: 0.036311, mean_q: 1.382380
 69094/100000: episode: 1434, duration: 0.179s, episode steps: 33, steps per second: 185, episode reward: 24.482, mean reward: 0.742 [0.656, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.750, 10.380], loss: 0.001186, mae: 0.037974, mean_q: 1.383999
 69125/100000: episode: 1435, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 21.053, mean reward: 0.679 [0.571, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.035, 10.328], loss: 0.001178, mae: 0.037822, mean_q: 1.373152
 69158/100000: episode: 1436, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 22.549, mean reward: 0.683 [0.578, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.610, 10.290], loss: 0.001126, mae: 0.037256, mean_q: 1.373768
 69175/100000: episode: 1437, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 13.579, mean reward: 0.799 [0.737, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.528], loss: 0.001237, mae: 0.037995, mean_q: 1.389936
 69192/100000: episode: 1438, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 14.125, mean reward: 0.831 [0.789, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.493, 10.568], loss: 0.001171, mae: 0.037696, mean_q: 1.383610
 69224/100000: episode: 1439, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 22.779, mean reward: 0.712 [0.518, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.289], loss: 0.001206, mae: 0.038589, mean_q: 1.377145
[Info] 4-TH LEVEL FOUND: 1.7092561721801758, Considering 10/90 traces
 69257/100000: episode: 1440, duration: 4.426s, episode steps: 33, steps per second: 7, episode reward: 23.060, mean reward: 0.699 [0.592, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.035, 10.404], loss: 0.001233, mae: 0.039491, mean_q: 1.387168
 69283/100000: episode: 1441, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 19.448, mean reward: 0.748 [0.672, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.440, 10.439], loss: 0.001303, mae: 0.040189, mean_q: 1.375290
 69297/100000: episode: 1442, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 11.391, mean reward: 0.814 [0.726, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.481, 10.475], loss: 0.001599, mae: 0.045239, mean_q: 1.409314
 69305/100000: episode: 1443, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 6.801, mean reward: 0.850 [0.829, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.495], loss: 0.001353, mae: 0.038992, mean_q: 1.399542
 69335/100000: episode: 1444, duration: 0.181s, episode steps: 30, steps per second: 166, episode reward: 21.080, mean reward: 0.703 [0.551, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.224], loss: 0.001163, mae: 0.036962, mean_q: 1.390524
 69365/100000: episode: 1445, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 26.616, mean reward: 0.887 [0.690, 0.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.837, 10.631], loss: 0.001320, mae: 0.039993, mean_q: 1.392138
 69395/100000: episode: 1446, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 21.298, mean reward: 0.710 [0.621, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.749, 10.402], loss: 0.001289, mae: 0.038576, mean_q: 1.374541
[Info] FALSIFICATION!
 69398/100000: episode: 1447, duration: 0.306s, episode steps: 3, steps per second: 10, episode reward: 2.876, mean reward: 0.959 [0.902, 1.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.489, 10.303], loss: 0.001011, mae: 0.035322, mean_q: 1.335763
 69412/100000: episode: 1448, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 12.218, mean reward: 0.873 [0.799, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.593], loss: 0.001253, mae: 0.038596, mean_q: 1.406001
 69420/100000: episode: 1449, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 6.706, mean reward: 0.838 [0.808, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.630], loss: 0.001255, mae: 0.039417, mean_q: 1.391464
 69449/100000: episode: 1450, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 25.401, mean reward: 0.876 [0.769, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.308, 10.554], loss: 0.001099, mae: 0.035860, mean_q: 1.377044
 69465/100000: episode: 1451, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 12.702, mean reward: 0.794 [0.681, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.179, 10.431], loss: 0.001229, mae: 0.037976, mean_q: 1.392034
 69495/100000: episode: 1452, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 23.481, mean reward: 0.783 [0.645, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.300, 10.442], loss: 0.001215, mae: 0.037674, mean_q: 1.392433
 69525/100000: episode: 1453, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 21.056, mean reward: 0.702 [0.577, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.609, 10.299], loss: 0.001188, mae: 0.037572, mean_q: 1.398082
 69555/100000: episode: 1454, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 21.257, mean reward: 0.709 [0.551, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.208], loss: 0.001240, mae: 0.038382, mean_q: 1.395882
 69585/100000: episode: 1455, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 21.712, mean reward: 0.724 [0.597, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.387, 10.249], loss: 0.001277, mae: 0.039806, mean_q: 1.393732
 69593/100000: episode: 1456, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 6.497, mean reward: 0.812 [0.747, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.124, 10.546], loss: 0.001337, mae: 0.040084, mean_q: 1.387698
 69603/100000: episode: 1457, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 9.447, mean reward: 0.945 [0.890, 0.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.965, 10.750], loss: 0.001306, mae: 0.040143, mean_q: 1.412730
 69632/100000: episode: 1458, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 21.164, mean reward: 0.730 [0.650, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.636, 10.402], loss: 0.001474, mae: 0.041485, mean_q: 1.401819
 69662/100000: episode: 1459, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 21.128, mean reward: 0.704 [0.607, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.355, 10.484], loss: 0.001211, mae: 0.038861, mean_q: 1.405818
 69691/100000: episode: 1460, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 21.257, mean reward: 0.733 [0.583, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.422, 10.357], loss: 0.001148, mae: 0.037149, mean_q: 1.386139
 69721/100000: episode: 1461, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 22.950, mean reward: 0.765 [0.616, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.458, 10.319], loss: 0.001294, mae: 0.039290, mean_q: 1.402372
 69747/100000: episode: 1462, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 18.953, mean reward: 0.729 [0.573, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.153, 10.331], loss: 0.001323, mae: 0.039152, mean_q: 1.400938
 69777/100000: episode: 1463, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 21.544, mean reward: 0.718 [0.619, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.041, 10.356], loss: 0.001321, mae: 0.040773, mean_q: 1.390632
 69785/100000: episode: 1464, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 7.551, mean reward: 0.944 [0.902, 0.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.081, 10.735], loss: 0.002095, mae: 0.041537, mean_q: 1.412108
 69799/100000: episode: 1465, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 11.996, mean reward: 0.857 [0.798, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-1.212, 10.547], loss: 0.001288, mae: 0.039357, mean_q: 1.397446
 69809/100000: episode: 1466, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 8.420, mean reward: 0.842 [0.792, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.113, 10.583], loss: 0.001323, mae: 0.039591, mean_q: 1.384429
 69835/100000: episode: 1467, duration: 0.147s, episode steps: 26, steps per second: 176, episode reward: 19.685, mean reward: 0.757 [0.569, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.431, 10.277], loss: 0.001224, mae: 0.038224, mean_q: 1.391654
 69845/100000: episode: 1468, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 8.803, mean reward: 0.880 [0.825, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.650], loss: 0.001172, mae: 0.038423, mean_q: 1.394760
 69871/100000: episode: 1469, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 19.773, mean reward: 0.761 [0.691, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.543, 10.398], loss: 0.001392, mae: 0.039984, mean_q: 1.410162
 69881/100000: episode: 1470, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 8.297, mean reward: 0.830 [0.792, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.376, 10.623], loss: 0.001353, mae: 0.040635, mean_q: 1.396706
 69895/100000: episode: 1471, duration: 0.097s, episode steps: 14, steps per second: 144, episode reward: 11.989, mean reward: 0.856 [0.722, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.149, 10.536], loss: 0.001041, mae: 0.036070, mean_q: 1.383481
 69903/100000: episode: 1472, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 6.633, mean reward: 0.829 [0.802, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.532], loss: 0.001416, mae: 0.043427, mean_q: 1.424030
 69913/100000: episode: 1473, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 9.092, mean reward: 0.909 [0.888, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.454, 10.607], loss: 0.001309, mae: 0.039594, mean_q: 1.420759
 69921/100000: episode: 1474, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 6.217, mean reward: 0.777 [0.691, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.437], loss: 0.001297, mae: 0.038143, mean_q: 1.415706
 69951/100000: episode: 1475, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 20.071, mean reward: 0.669 [0.502, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.479, 10.138], loss: 0.001243, mae: 0.037695, mean_q: 1.416401
 69977/100000: episode: 1476, duration: 0.130s, episode steps: 26, steps per second: 199, episode reward: 20.496, mean reward: 0.788 [0.692, 0.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.489, 10.446], loss: 0.001822, mae: 0.044152, mean_q: 1.395161
 69993/100000: episode: 1477, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 11.990, mean reward: 0.749 [0.585, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.439], loss: 0.001523, mae: 0.043194, mean_q: 1.402109
 70023/100000: episode: 1478, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 20.323, mean reward: 0.677 [0.588, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.368, 10.350], loss: 0.001291, mae: 0.039907, mean_q: 1.393761
 70052/100000: episode: 1479, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 22.914, mean reward: 0.790 [0.718, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.084, 10.552], loss: 0.001371, mae: 0.041243, mean_q: 1.403469
 70062/100000: episode: 1480, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 8.794, mean reward: 0.879 [0.779, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.390, 10.564], loss: 0.003196, mae: 0.052126, mean_q: 1.417207
 70092/100000: episode: 1481, duration: 0.146s, episode steps: 30, steps per second: 205, episode reward: 22.701, mean reward: 0.757 [0.705, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.368, 10.568], loss: 0.001570, mae: 0.044448, mean_q: 1.402193
 70108/100000: episode: 1482, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 12.731, mean reward: 0.796 [0.728, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.448], loss: 0.001243, mae: 0.039041, mean_q: 1.417675
 70120/100000: episode: 1483, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 9.611, mean reward: 0.801 [0.738, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.463], loss: 0.001187, mae: 0.039244, mean_q: 1.387949
 70134/100000: episode: 1484, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 11.684, mean reward: 0.835 [0.736, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.119, 10.506], loss: 0.001334, mae: 0.040332, mean_q: 1.416360
 70164/100000: episode: 1485, duration: 0.185s, episode steps: 30, steps per second: 163, episode reward: 21.550, mean reward: 0.718 [0.645, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.410], loss: 0.001416, mae: 0.041685, mean_q: 1.407181
 70176/100000: episode: 1486, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 10.226, mean reward: 0.852 [0.791, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.568], loss: 0.001591, mae: 0.043745, mean_q: 1.410261
 70186/100000: episode: 1487, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 8.710, mean reward: 0.871 [0.849, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.573], loss: 0.001530, mae: 0.043495, mean_q: 1.384897
 70194/100000: episode: 1488, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 7.315, mean reward: 0.914 [0.895, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.457], loss: 0.001305, mae: 0.038513, mean_q: 1.416047
[Info] FALSIFICATION!
 70195/100000: episode: 1489, duration: 0.278s, episode steps: 1, steps per second: 4, episode reward: 1.011, mean reward: 1.011 [1.011, 1.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.024, 9.135], loss: 0.001192, mae: 0.040840, mean_q: 1.396311
 70225/100000: episode: 1490, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 23.038, mean reward: 0.768 [0.703, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.463], loss: 0.001422, mae: 0.038848, mean_q: 1.414097
 70237/100000: episode: 1491, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 10.318, mean reward: 0.860 [0.785, 0.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.606], loss: 0.001257, mae: 0.038663, mean_q: 1.408044
 70251/100000: episode: 1492, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 11.978, mean reward: 0.856 [0.737, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.600], loss: 0.001317, mae: 0.040279, mean_q: 1.402814
 70265/100000: episode: 1493, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 11.087, mean reward: 0.792 [0.657, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.412], loss: 0.001415, mae: 0.042328, mean_q: 1.407772
 70275/100000: episode: 1494, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 8.497, mean reward: 0.850 [0.781, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.277, 10.629], loss: 0.001623, mae: 0.044261, mean_q: 1.401261
 70287/100000: episode: 1495, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 9.977, mean reward: 0.831 [0.785, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.158, 10.627], loss: 0.001566, mae: 0.044712, mean_q: 1.394622
 70317/100000: episode: 1496, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 21.777, mean reward: 0.726 [0.644, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.491], loss: 0.001266, mae: 0.038978, mean_q: 1.411268
[Info] FALSIFICATION!
 70318/100000: episode: 1497, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 1.012, mean reward: 1.012 [1.012, 1.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.009, 9.813], loss: 0.001662, mae: 0.045871, mean_q: 1.395269
 70347/100000: episode: 1498, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 22.835, mean reward: 0.787 [0.720, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.925, 10.535], loss: 0.001323, mae: 0.040063, mean_q: 1.418553
[Info] FALSIFICATION!
 70349/100000: episode: 1499, duration: 0.197s, episode steps: 2, steps per second: 10, episode reward: 1.942, mean reward: 0.971 [0.926, 1.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.009, 10.035], loss: 0.002033, mae: 0.048754, mean_q: 1.349076
 70379/100000: episode: 1500, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 23.433, mean reward: 0.781 [0.668, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.741, 10.481], loss: 0.001480, mae: 0.042203, mean_q: 1.404853
 70389/100000: episode: 1501, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 8.359, mean reward: 0.836 [0.776, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.065, 10.536], loss: 0.001354, mae: 0.040089, mean_q: 1.387319
 70418/100000: episode: 1502, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 21.859, mean reward: 0.754 [0.675, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.489, 10.484], loss: 0.001471, mae: 0.041509, mean_q: 1.417915
 70448/100000: episode: 1503, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 21.530, mean reward: 0.718 [0.649, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.052, 10.409], loss: 0.001871, mae: 0.044852, mean_q: 1.408435
 70477/100000: episode: 1504, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 22.361, mean reward: 0.771 [0.614, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.189, 10.410], loss: 0.001803, mae: 0.044613, mean_q: 1.419596
 70503/100000: episode: 1505, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 19.653, mean reward: 0.756 [0.634, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.568, 10.392], loss: 0.001637, mae: 0.041078, mean_q: 1.413710
 70529/100000: episode: 1506, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 17.256, mean reward: 0.664 [0.526, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.489, 10.151], loss: 0.001475, mae: 0.039027, mean_q: 1.418814
 70543/100000: episode: 1507, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 12.032, mean reward: 0.859 [0.745, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.241, 10.466], loss: 0.001109, mae: 0.036424, mean_q: 1.419254
 70557/100000: episode: 1508, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 11.375, mean reward: 0.812 [0.725, 0.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-1.168, 10.552], loss: 0.001335, mae: 0.040185, mean_q: 1.414014
 70571/100000: episode: 1509, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 11.806, mean reward: 0.843 [0.753, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.598], loss: 0.001814, mae: 0.040487, mean_q: 1.414513
 70579/100000: episode: 1510, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 6.832, mean reward: 0.854 [0.822, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.682], loss: 0.001483, mae: 0.042535, mean_q: 1.396497
 70609/100000: episode: 1511, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 21.702, mean reward: 0.723 [0.594, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.377, 10.276], loss: 0.001985, mae: 0.044903, mean_q: 1.418726
 70639/100000: episode: 1512, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 21.570, mean reward: 0.719 [0.540, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.287, 10.206], loss: 0.001737, mae: 0.044280, mean_q: 1.415263
[Info] FALSIFICATION!
 70644/100000: episode: 1513, duration: 0.221s, episode steps: 5, steps per second: 23, episode reward: 4.704, mean reward: 0.941 [0.851, 1.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.795 [-0.113, 8.617], loss: 0.001526, mae: 0.043059, mean_q: 1.360525
 70656/100000: episode: 1514, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 9.836, mean reward: 0.820 [0.732, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.526], loss: 0.002251, mae: 0.047504, mean_q: 1.397694
 70672/100000: episode: 1515, duration: 0.108s, episode steps: 16, steps per second: 148, episode reward: 13.150, mean reward: 0.822 [0.690, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.437], loss: 0.001339, mae: 0.040941, mean_q: 1.423117
 70682/100000: episode: 1516, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 7.959, mean reward: 0.796 [0.688, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.048, 10.500], loss: 0.001507, mae: 0.042999, mean_q: 1.416759
 70696/100000: episode: 1517, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 12.178, mean reward: 0.870 [0.765, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.137, 10.567], loss: 0.001993, mae: 0.043896, mean_q: 1.426263
 70726/100000: episode: 1518, duration: 0.183s, episode steps: 30, steps per second: 164, episode reward: 20.518, mean reward: 0.684 [0.594, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.701, 10.364], loss: 0.001481, mae: 0.042276, mean_q: 1.414216
 70738/100000: episode: 1519, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 9.850, mean reward: 0.821 [0.792, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.741, 10.500], loss: 0.001957, mae: 0.041986, mean_q: 1.419440
 70754/100000: episode: 1520, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 13.448, mean reward: 0.841 [0.803, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.285, 10.593], loss: 0.001376, mae: 0.039635, mean_q: 1.426966
 70784/100000: episode: 1521, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 21.394, mean reward: 0.713 [0.594, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.035, 10.323], loss: 0.001619, mae: 0.039932, mean_q: 1.400703
 70814/100000: episode: 1522, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 23.441, mean reward: 0.781 [0.692, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.451, 10.558], loss: 0.002347, mae: 0.044588, mean_q: 1.408354
 70828/100000: episode: 1523, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 11.769, mean reward: 0.841 [0.760, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.728, 10.479], loss: 0.002078, mae: 0.044550, mean_q: 1.419644
 70836/100000: episode: 1524, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 6.376, mean reward: 0.797 [0.762, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.492], loss: 0.001840, mae: 0.039244, mean_q: 1.437109
 70866/100000: episode: 1525, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 24.140, mean reward: 0.805 [0.731, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.492], loss: 0.001538, mae: 0.040038, mean_q: 1.406855
 70896/100000: episode: 1526, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 22.334, mean reward: 0.744 [0.624, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.396], loss: 0.001603, mae: 0.039826, mean_q: 1.429630
 70926/100000: episode: 1527, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 25.472, mean reward: 0.849 [0.726, 0.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.069, 10.544], loss: 0.001643, mae: 0.042681, mean_q: 1.431048
[Info] FALSIFICATION!
 70933/100000: episode: 1528, duration: 0.241s, episode steps: 7, steps per second: 29, episode reward: 6.581, mean reward: 0.940 [0.899, 1.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.434, 10.155], loss: 0.001346, mae: 0.041276, mean_q: 1.396415
 70963/100000: episode: 1529, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 22.078, mean reward: 0.736 [0.567, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.514, 10.393], loss: 0.002208, mae: 0.044186, mean_q: 1.421982
[Info] Complete ISplit Iteration
[Info] Levels: [1.3938909, 1.519929, 1.6287112, 1.7092562, 1.6598301]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 1.0]
[Info] Error Prob: 0.00010000000000000003

 70993/100000: episode: 1530, duration: 4.589s, episode steps: 30, steps per second: 7, episode reward: 21.849, mean reward: 0.728 [0.598, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.555, 10.235], loss: 0.001943, mae: 0.043517, mean_q: 1.417329
 71093/100000: episode: 1531, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 60.567, mean reward: 0.606 [0.509, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.688, 10.262], loss: 0.001664, mae: 0.041720, mean_q: 1.421809
 71193/100000: episode: 1532, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.007, mean reward: 0.590 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.065, 10.236], loss: 0.001455, mae: 0.039756, mean_q: 1.418740
 71293/100000: episode: 1533, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.803, mean reward: 0.578 [0.507, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.613, 10.098], loss: 0.001977, mae: 0.043491, mean_q: 1.406960
 71393/100000: episode: 1534, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.176, mean reward: 0.582 [0.516, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.267, 10.098], loss: 0.001695, mae: 0.041278, mean_q: 1.412884
 71493/100000: episode: 1535, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.282, mean reward: 0.583 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.454, 10.098], loss: 0.001812, mae: 0.042244, mean_q: 1.403892
 71593/100000: episode: 1536, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.031, mean reward: 0.580 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.584, 10.282], loss: 0.001757, mae: 0.042177, mean_q: 1.395290
 71693/100000: episode: 1537, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.610, mean reward: 0.586 [0.509, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.980, 10.108], loss: 0.001439, mae: 0.039647, mean_q: 1.392464
 71793/100000: episode: 1538, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 62.232, mean reward: 0.622 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.341, 10.203], loss: 0.001666, mae: 0.041197, mean_q: 1.379279
 71893/100000: episode: 1539, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.326, mean reward: 0.583 [0.511, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.830, 10.098], loss: 0.001671, mae: 0.041591, mean_q: 1.381559
 71993/100000: episode: 1540, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 58.492, mean reward: 0.585 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.809, 10.160], loss: 0.001552, mae: 0.040719, mean_q: 1.378549
 72093/100000: episode: 1541, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.026, mean reward: 0.570 [0.504, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.644, 10.158], loss: 0.001782, mae: 0.041845, mean_q: 1.372175
 72193/100000: episode: 1542, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 56.669, mean reward: 0.567 [0.504, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.493, 10.098], loss: 0.001706, mae: 0.042993, mean_q: 1.360255
 72293/100000: episode: 1543, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.044, mean reward: 0.600 [0.509, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.206, 10.098], loss: 0.001799, mae: 0.041159, mean_q: 1.359733
 72393/100000: episode: 1544, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.438, mean reward: 0.594 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.751, 10.353], loss: 0.001621, mae: 0.041381, mean_q: 1.358982
 72493/100000: episode: 1545, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 56.745, mean reward: 0.567 [0.505, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.939, 10.100], loss: 0.001738, mae: 0.042348, mean_q: 1.345097
 72593/100000: episode: 1546, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 62.760, mean reward: 0.628 [0.511, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.204, 10.098], loss: 0.002040, mae: 0.043144, mean_q: 1.336302
 72693/100000: episode: 1547, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.911, mean reward: 0.589 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.163, 10.132], loss: 0.001490, mae: 0.041193, mean_q: 1.343178
 72793/100000: episode: 1548, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 60.264, mean reward: 0.603 [0.508, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.384, 10.230], loss: 0.001877, mae: 0.043022, mean_q: 1.327873
 72893/100000: episode: 1549, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.770, mean reward: 0.598 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.775, 10.230], loss: 0.001998, mae: 0.043773, mean_q: 1.329904
 72993/100000: episode: 1550, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.424, mean reward: 0.574 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.298, 10.111], loss: 0.001681, mae: 0.041212, mean_q: 1.336243
 73093/100000: episode: 1551, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.389, mean reward: 0.594 [0.511, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.820, 10.194], loss: 0.001796, mae: 0.041605, mean_q: 1.316128
 73193/100000: episode: 1552, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.079, mean reward: 0.591 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.831, 10.103], loss: 0.001891, mae: 0.043790, mean_q: 1.318651
 73293/100000: episode: 1553, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.968, mean reward: 0.590 [0.499, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.790, 10.098], loss: 0.001688, mae: 0.042605, mean_q: 1.313842
 73393/100000: episode: 1554, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.340, mean reward: 0.593 [0.500, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.133, 10.386], loss: 0.001630, mae: 0.041316, mean_q: 1.298424
 73493/100000: episode: 1555, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.771, mean reward: 0.578 [0.512, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.630, 10.289], loss: 0.001855, mae: 0.044291, mean_q: 1.296463
 73593/100000: episode: 1556, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.064, mean reward: 0.591 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.221, 10.098], loss: 0.001894, mae: 0.043301, mean_q: 1.294626
 73693/100000: episode: 1557, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.824, mean reward: 0.588 [0.498, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.902, 10.102], loss: 0.001760, mae: 0.040895, mean_q: 1.294523
 73793/100000: episode: 1558, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.281, mean reward: 0.583 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.448, 10.124], loss: 0.002066, mae: 0.043866, mean_q: 1.284025
 73893/100000: episode: 1559, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.923, mean reward: 0.599 [0.517, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.618, 10.184], loss: 0.001955, mae: 0.045395, mean_q: 1.280374
 73993/100000: episode: 1560, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 57.071, mean reward: 0.571 [0.500, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.588, 10.207], loss: 0.001751, mae: 0.041497, mean_q: 1.273938
 74093/100000: episode: 1561, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.846, mean reward: 0.598 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.987, 10.262], loss: 0.001835, mae: 0.043120, mean_q: 1.272262
 74193/100000: episode: 1562, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 59.586, mean reward: 0.596 [0.500, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.617, 10.579], loss: 0.002027, mae: 0.044478, mean_q: 1.267468
 74293/100000: episode: 1563, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 62.695, mean reward: 0.627 [0.510, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.577, 10.098], loss: 0.001827, mae: 0.044220, mean_q: 1.259838
 74393/100000: episode: 1564, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.452, mean reward: 0.595 [0.517, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.589, 10.419], loss: 0.002221, mae: 0.046404, mean_q: 1.259127
 74493/100000: episode: 1565, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 56.861, mean reward: 0.569 [0.511, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.474, 10.098], loss: 0.002108, mae: 0.044830, mean_q: 1.254110
 74593/100000: episode: 1566, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.080, mean reward: 0.581 [0.506, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.086, 10.126], loss: 0.001581, mae: 0.041096, mean_q: 1.247232
 74693/100000: episode: 1567, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.481, mean reward: 0.585 [0.499, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.647, 10.185], loss: 0.001909, mae: 0.044750, mean_q: 1.237973
 74793/100000: episode: 1568, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.669, mean reward: 0.587 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.417, 10.164], loss: 0.001684, mae: 0.043092, mean_q: 1.233259
 74893/100000: episode: 1569, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 56.124, mean reward: 0.561 [0.501, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.777, 10.223], loss: 0.002344, mae: 0.044813, mean_q: 1.230260
 74993/100000: episode: 1570, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 56.639, mean reward: 0.566 [0.498, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.134, 10.107], loss: 0.001610, mae: 0.041077, mean_q: 1.225283
 75093/100000: episode: 1571, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 61.409, mean reward: 0.614 [0.514, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.429, 10.098], loss: 0.001817, mae: 0.042463, mean_q: 1.215614
 75193/100000: episode: 1572, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.468, mean reward: 0.575 [0.504, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.729, 10.220], loss: 0.001454, mae: 0.039904, mean_q: 1.214707
 75293/100000: episode: 1573, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.084, mean reward: 0.581 [0.508, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.542, 10.239], loss: 0.001666, mae: 0.042029, mean_q: 1.201816
 75393/100000: episode: 1574, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.752, mean reward: 0.578 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.642, 10.193], loss: 0.001444, mae: 0.039872, mean_q: 1.198419
 75493/100000: episode: 1575, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.795, mean reward: 0.598 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.950, 10.280], loss: 0.001613, mae: 0.041803, mean_q: 1.190134
 75593/100000: episode: 1576, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 62.191, mean reward: 0.622 [0.514, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.854, 10.098], loss: 0.001487, mae: 0.040334, mean_q: 1.186423
 75693/100000: episode: 1577, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.422, mean reward: 0.574 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.211, 10.098], loss: 0.001503, mae: 0.041764, mean_q: 1.185823
 75793/100000: episode: 1578, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.518, mean reward: 0.585 [0.509, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.000, 10.125], loss: 0.001542, mae: 0.040740, mean_q: 1.175022
 75893/100000: episode: 1579, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 60.517, mean reward: 0.605 [0.510, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.907, 10.255], loss: 0.001427, mae: 0.040785, mean_q: 1.171078
 75993/100000: episode: 1580, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.595, mean reward: 0.576 [0.510, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.244, 10.098], loss: 0.001276, mae: 0.038953, mean_q: 1.164308
 76093/100000: episode: 1581, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 61.147, mean reward: 0.611 [0.516, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.911, 10.393], loss: 0.001336, mae: 0.039502, mean_q: 1.162153
 76193/100000: episode: 1582, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.183, mean reward: 0.582 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.865, 10.098], loss: 0.001298, mae: 0.039275, mean_q: 1.166606
 76293/100000: episode: 1583, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.845, mean reward: 0.588 [0.503, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.993, 10.098], loss: 0.001313, mae: 0.039263, mean_q: 1.164545
 76393/100000: episode: 1584, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.762, mean reward: 0.588 [0.509, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.583, 10.098], loss: 0.001256, mae: 0.038485, mean_q: 1.164965
 76493/100000: episode: 1585, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.601, mean reward: 0.576 [0.499, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.060, 10.153], loss: 0.001318, mae: 0.039047, mean_q: 1.162765
 76593/100000: episode: 1586, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.533, mean reward: 0.575 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.405, 10.271], loss: 0.001264, mae: 0.038729, mean_q: 1.163167
 76693/100000: episode: 1587, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.293, mean reward: 0.583 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.231, 10.098], loss: 0.001390, mae: 0.040375, mean_q: 1.163696
 76793/100000: episode: 1588, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.069, mean reward: 0.571 [0.501, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.656, 10.118], loss: 0.001358, mae: 0.039877, mean_q: 1.161148
 76893/100000: episode: 1589, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.875, mean reward: 0.599 [0.502, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.985, 10.123], loss: 0.001383, mae: 0.039863, mean_q: 1.162794
 76993/100000: episode: 1590, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.019, mean reward: 0.590 [0.511, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.722, 10.191], loss: 0.001294, mae: 0.039144, mean_q: 1.160070
 77093/100000: episode: 1591, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.499, mean reward: 0.585 [0.518, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.520, 10.098], loss: 0.001479, mae: 0.041625, mean_q: 1.163185
 77193/100000: episode: 1592, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.926, mean reward: 0.579 [0.510, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.486, 10.122], loss: 0.001406, mae: 0.040433, mean_q: 1.166663
 77293/100000: episode: 1593, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.111, mean reward: 0.581 [0.500, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.631, 10.098], loss: 0.001279, mae: 0.038992, mean_q: 1.164097
 77393/100000: episode: 1594, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.675, mean reward: 0.577 [0.509, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.320, 10.162], loss: 0.001466, mae: 0.041646, mean_q: 1.165858
 77493/100000: episode: 1595, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.898, mean reward: 0.599 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.162, 10.098], loss: 0.001475, mae: 0.041095, mean_q: 1.161525
 77593/100000: episode: 1596, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.331, mean reward: 0.583 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.592, 10.158], loss: 0.001384, mae: 0.040363, mean_q: 1.163388
 77693/100000: episode: 1597, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.545, mean reward: 0.595 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.413, 10.351], loss: 0.001410, mae: 0.040823, mean_q: 1.161990
 77793/100000: episode: 1598, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.333, mean reward: 0.573 [0.501, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.304, 10.153], loss: 0.001363, mae: 0.039672, mean_q: 1.162665
 77893/100000: episode: 1599, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.763, mean reward: 0.598 [0.505, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.633, 10.098], loss: 0.001384, mae: 0.040315, mean_q: 1.159161
 77993/100000: episode: 1600, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.181, mean reward: 0.592 [0.515, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.271, 10.154], loss: 0.001294, mae: 0.039370, mean_q: 1.160790
 78093/100000: episode: 1601, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 62.163, mean reward: 0.622 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.771, 10.111], loss: 0.001438, mae: 0.040930, mean_q: 1.159166
 78193/100000: episode: 1602, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.093, mean reward: 0.591 [0.511, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.511, 10.225], loss: 0.001341, mae: 0.040035, mean_q: 1.162135
 78293/100000: episode: 1603, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 62.388, mean reward: 0.624 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.428, 10.449], loss: 0.001459, mae: 0.041374, mean_q: 1.165690
 78393/100000: episode: 1604, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.239, mean reward: 0.582 [0.505, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.737, 10.134], loss: 0.001353, mae: 0.040705, mean_q: 1.168096
 78493/100000: episode: 1605, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.334, mean reward: 0.583 [0.501, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.861, 10.139], loss: 0.001457, mae: 0.041134, mean_q: 1.165960
 78593/100000: episode: 1606, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.212, mean reward: 0.592 [0.499, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.237, 10.227], loss: 0.001460, mae: 0.041043, mean_q: 1.164780
 78693/100000: episode: 1607, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.305, mean reward: 0.583 [0.500, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.619, 10.206], loss: 0.001467, mae: 0.041521, mean_q: 1.165657
 78793/100000: episode: 1608, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.166, mean reward: 0.572 [0.515, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.244, 10.212], loss: 0.001878, mae: 0.046302, mean_q: 1.160301
 78893/100000: episode: 1609, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.521, mean reward: 0.575 [0.502, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.930, 10.285], loss: 0.001414, mae: 0.040516, mean_q: 1.161172
 78993/100000: episode: 1610, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.352, mean reward: 0.594 [0.513, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.183, 10.219], loss: 0.001360, mae: 0.040147, mean_q: 1.161015
 79093/100000: episode: 1611, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 57.283, mean reward: 0.573 [0.500, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.907, 10.098], loss: 0.001319, mae: 0.039462, mean_q: 1.161353
 79193/100000: episode: 1612, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 58.114, mean reward: 0.581 [0.510, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.105, 10.098], loss: 0.001422, mae: 0.040915, mean_q: 1.162017
 79293/100000: episode: 1613, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.136, mean reward: 0.581 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.259, 10.129], loss: 0.001427, mae: 0.040459, mean_q: 1.164584
 79393/100000: episode: 1614, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 57.700, mean reward: 0.577 [0.505, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.733, 10.156], loss: 0.001322, mae: 0.040095, mean_q: 1.157717
 79493/100000: episode: 1615, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 60.684, mean reward: 0.607 [0.500, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.215, 10.098], loss: 0.001321, mae: 0.039471, mean_q: 1.160690
 79593/100000: episode: 1616, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.121, mean reward: 0.581 [0.521, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.469, 10.302], loss: 0.001305, mae: 0.039581, mean_q: 1.157921
 79693/100000: episode: 1617, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 61.605, mean reward: 0.616 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.624, 10.182], loss: 0.001336, mae: 0.040186, mean_q: 1.159548
 79793/100000: episode: 1618, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.109, mean reward: 0.591 [0.511, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.204, 10.098], loss: 0.001405, mae: 0.040887, mean_q: 1.158367
 79893/100000: episode: 1619, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.580, mean reward: 0.576 [0.505, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.099, 10.140], loss: 0.001333, mae: 0.039948, mean_q: 1.162271
 79993/100000: episode: 1620, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.169, mean reward: 0.572 [0.498, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.750, 10.176], loss: 0.001357, mae: 0.040166, mean_q: 1.159597
 80093/100000: episode: 1621, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.336, mean reward: 0.603 [0.511, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.751, 10.098], loss: 0.001381, mae: 0.041133, mean_q: 1.165094
 80193/100000: episode: 1622, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.552, mean reward: 0.586 [0.511, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.564, 10.237], loss: 0.001392, mae: 0.040960, mean_q: 1.163235
 80293/100000: episode: 1623, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.468, mean reward: 0.575 [0.507, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.521, 10.098], loss: 0.001431, mae: 0.041424, mean_q: 1.164657
 80393/100000: episode: 1624, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.733, mean reward: 0.607 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.267, 10.343], loss: 0.001433, mae: 0.040883, mean_q: 1.163995
 80493/100000: episode: 1625, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 62.311, mean reward: 0.623 [0.501, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.810, 10.098], loss: 0.001423, mae: 0.041444, mean_q: 1.164880
 80593/100000: episode: 1626, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 60.751, mean reward: 0.608 [0.528, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.207, 10.223], loss: 0.001300, mae: 0.039456, mean_q: 1.161853
 80693/100000: episode: 1627, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.279, mean reward: 0.583 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.086, 10.098], loss: 0.001403, mae: 0.041194, mean_q: 1.164872
 80793/100000: episode: 1628, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.467, mean reward: 0.585 [0.506, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.441, 10.098], loss: 0.001253, mae: 0.039112, mean_q: 1.157317
 80893/100000: episode: 1629, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.159, mean reward: 0.582 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.056, 10.098], loss: 0.001421, mae: 0.040979, mean_q: 1.163221
[Info] 1-TH LEVEL FOUND: 1.3630927801132202, Considering 10/90 traces
 80993/100000: episode: 1630, duration: 4.831s, episode steps: 100, steps per second: 21, episode reward: 60.363, mean reward: 0.604 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.636, 10.202], loss: 0.001440, mae: 0.041226, mean_q: 1.165148
 81014/100000: episode: 1631, duration: 0.126s, episode steps: 21, steps per second: 166, episode reward: 13.508, mean reward: 0.643 [0.545, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.064, 10.100], loss: 0.001325, mae: 0.038960, mean_q: 1.158854
 81053/100000: episode: 1632, duration: 0.227s, episode steps: 39, steps per second: 172, episode reward: 26.316, mean reward: 0.675 [0.504, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.264, 10.162], loss: 0.001680, mae: 0.044497, mean_q: 1.167641
 81073/100000: episode: 1633, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 13.178, mean reward: 0.659 [0.585, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.408, 10.100], loss: 0.001658, mae: 0.044574, mean_q: 1.168259
 81161/100000: episode: 1634, duration: 0.478s, episode steps: 88, steps per second: 184, episode reward: 52.073, mean reward: 0.592 [0.516, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.568 [-0.601, 10.100], loss: 0.001396, mae: 0.040856, mean_q: 1.164033
 81249/100000: episode: 1635, duration: 0.458s, episode steps: 88, steps per second: 192, episode reward: 52.790, mean reward: 0.600 [0.510, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.572 [-0.829, 10.205], loss: 0.001357, mae: 0.039577, mean_q: 1.164142
 81276/100000: episode: 1636, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 17.786, mean reward: 0.659 [0.599, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.595, 10.100], loss: 0.001583, mae: 0.042992, mean_q: 1.168780
 81296/100000: episode: 1637, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 14.082, mean reward: 0.704 [0.619, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.717, 10.100], loss: 0.001636, mae: 0.043492, mean_q: 1.166790
 81335/100000: episode: 1638, duration: 0.207s, episode steps: 39, steps per second: 189, episode reward: 24.137, mean reward: 0.619 [0.518, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.746, 10.351], loss: 0.001373, mae: 0.039850, mean_q: 1.168885
 81374/100000: episode: 1639, duration: 0.220s, episode steps: 39, steps per second: 177, episode reward: 24.343, mean reward: 0.624 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.388, 10.100], loss: 0.001475, mae: 0.041646, mean_q: 1.170298
 81462/100000: episode: 1640, duration: 0.461s, episode steps: 88, steps per second: 191, episode reward: 52.977, mean reward: 0.602 [0.516, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.576 [-0.444, 10.223], loss: 0.001471, mae: 0.041808, mean_q: 1.165451
 81489/100000: episode: 1641, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 18.915, mean reward: 0.701 [0.625, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.366, 10.100], loss: 0.001476, mae: 0.041730, mean_q: 1.172268
 81577/100000: episode: 1642, duration: 0.468s, episode steps: 88, steps per second: 188, episode reward: 52.236, mean reward: 0.594 [0.519, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.571 [-1.383, 10.224], loss: 0.001455, mae: 0.041458, mean_q: 1.169810
 81665/100000: episode: 1643, duration: 0.464s, episode steps: 88, steps per second: 190, episode reward: 54.944, mean reward: 0.624 [0.527, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.573 [-0.398, 10.372], loss: 0.001602, mae: 0.042375, mean_q: 1.166446
 81692/100000: episode: 1644, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 19.242, mean reward: 0.713 [0.513, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.292, 10.100], loss: 0.001651, mae: 0.044489, mean_q: 1.176323
 81716/100000: episode: 1645, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 16.231, mean reward: 0.676 [0.565, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.107, 10.122], loss: 0.001568, mae: 0.043075, mean_q: 1.178859
 81735/100000: episode: 1646, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 12.194, mean reward: 0.642 [0.584, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.346, 10.100], loss: 0.001587, mae: 0.042168, mean_q: 1.171843
 81755/100000: episode: 1647, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 13.050, mean reward: 0.653 [0.601, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.317, 10.100], loss: 0.001764, mae: 0.044367, mean_q: 1.174847
 81794/100000: episode: 1648, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 24.910, mean reward: 0.639 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.927, 10.158], loss: 0.001415, mae: 0.040377, mean_q: 1.173774
 81882/100000: episode: 1649, duration: 0.462s, episode steps: 88, steps per second: 190, episode reward: 52.787, mean reward: 0.600 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.574 [-0.746, 10.457], loss: 0.001507, mae: 0.042131, mean_q: 1.179904
 81906/100000: episode: 1650, duration: 0.142s, episode steps: 24, steps per second: 168, episode reward: 15.945, mean reward: 0.664 [0.540, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.225, 10.100], loss: 0.001465, mae: 0.041615, mean_q: 1.176921
 81933/100000: episode: 1651, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 18.683, mean reward: 0.692 [0.620, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.737, 10.100], loss: 0.001465, mae: 0.041661, mean_q: 1.183363
 81952/100000: episode: 1652, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 12.565, mean reward: 0.661 [0.613, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.594, 10.100], loss: 0.001517, mae: 0.041083, mean_q: 1.178750
 81971/100000: episode: 1653, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 12.193, mean reward: 0.642 [0.568, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.207, 10.100], loss: 0.001612, mae: 0.043094, mean_q: 1.180884
 81995/100000: episode: 1654, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 17.761, mean reward: 0.740 [0.647, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.472, 10.100], loss: 0.001565, mae: 0.040331, mean_q: 1.171181
[Info] FALSIFICATION!
 82006/100000: episode: 1655, duration: 0.210s, episode steps: 11, steps per second: 52, episode reward: 8.121, mean reward: 0.738 [0.669, 1.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.007, 9.484], loss: 0.001696, mae: 0.044228, mean_q: 1.177748
 82024/100000: episode: 1656, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 12.140, mean reward: 0.674 [0.609, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.194, 10.100], loss: 0.001456, mae: 0.041143, mean_q: 1.180192
 82048/100000: episode: 1657, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 17.889, mean reward: 0.745 [0.684, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.569, 10.100], loss: 0.001733, mae: 0.045077, mean_q: 1.178894
 82136/100000: episode: 1658, duration: 0.465s, episode steps: 88, steps per second: 189, episode reward: 52.450, mean reward: 0.596 [0.505, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.565 [-0.374, 10.114], loss: 0.001630, mae: 0.042486, mean_q: 1.175581
 82224/100000: episode: 1659, duration: 0.474s, episode steps: 88, steps per second: 186, episode reward: 51.135, mean reward: 0.581 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.576 [-0.390, 10.100], loss: 0.001695, mae: 0.043073, mean_q: 1.180213
 82312/100000: episode: 1660, duration: 0.449s, episode steps: 88, steps per second: 196, episode reward: 55.036, mean reward: 0.625 [0.503, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.579 [-0.812, 10.298], loss: 0.001645, mae: 0.042763, mean_q: 1.185893
 82339/100000: episode: 1661, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 16.910, mean reward: 0.626 [0.513, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.798, 10.100], loss: 0.001751, mae: 0.045620, mean_q: 1.181097
 82357/100000: episode: 1662, duration: 0.101s, episode steps: 18, steps per second: 177, episode reward: 13.316, mean reward: 0.740 [0.676, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.704, 10.100], loss: 0.001655, mae: 0.042998, mean_q: 1.190314
 82381/100000: episode: 1663, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 16.378, mean reward: 0.682 [0.615, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.524, 10.100], loss: 0.001739, mae: 0.043447, mean_q: 1.187746
 82469/100000: episode: 1664, duration: 0.467s, episode steps: 88, steps per second: 188, episode reward: 54.868, mean reward: 0.624 [0.523, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.566 [-1.445, 10.100], loss: 0.001724, mae: 0.043254, mean_q: 1.185467
 82557/100000: episode: 1665, duration: 0.466s, episode steps: 88, steps per second: 189, episode reward: 51.785, mean reward: 0.588 [0.509, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.571 [-0.562, 10.290], loss: 0.001728, mae: 0.043794, mean_q: 1.190947
 82645/100000: episode: 1666, duration: 0.447s, episode steps: 88, steps per second: 197, episode reward: 51.786, mean reward: 0.588 [0.513, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.565 [-0.481, 10.160], loss: 0.001593, mae: 0.042943, mean_q: 1.188992
 82666/100000: episode: 1667, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 14.908, mean reward: 0.710 [0.654, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.683, 10.100], loss: 0.001358, mae: 0.040359, mean_q: 1.180617
 82693/100000: episode: 1668, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 17.416, mean reward: 0.645 [0.558, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.411, 10.100], loss: 0.001665, mae: 0.042702, mean_q: 1.188300
 82713/100000: episode: 1669, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 13.824, mean reward: 0.691 [0.634, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.366, 10.100], loss: 0.001917, mae: 0.045589, mean_q: 1.179037
 82731/100000: episode: 1670, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 13.424, mean reward: 0.746 [0.675, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.229, 10.100], loss: 0.002259, mae: 0.049429, mean_q: 1.196840
 82758/100000: episode: 1671, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 20.302, mean reward: 0.752 [0.659, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.807, 10.100], loss: 0.001663, mae: 0.042410, mean_q: 1.197875
 82777/100000: episode: 1672, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 13.203, mean reward: 0.695 [0.653, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.918, 10.100], loss: 0.001533, mae: 0.042888, mean_q: 1.196731
 82798/100000: episode: 1673, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 13.878, mean reward: 0.661 [0.584, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.318, 10.100], loss: 0.001589, mae: 0.040972, mean_q: 1.190518
 82886/100000: episode: 1674, duration: 0.448s, episode steps: 88, steps per second: 196, episode reward: 57.317, mean reward: 0.651 [0.510, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.568 [-1.119, 10.100], loss: 0.001622, mae: 0.042527, mean_q: 1.190443
 82974/100000: episode: 1675, duration: 0.452s, episode steps: 88, steps per second: 195, episode reward: 53.359, mean reward: 0.606 [0.510, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.573 [-0.393, 10.100], loss: 0.001768, mae: 0.044713, mean_q: 1.192194
 82992/100000: episode: 1676, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 12.397, mean reward: 0.689 [0.604, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.104, 10.100], loss: 0.001530, mae: 0.042776, mean_q: 1.195931
 83016/100000: episode: 1677, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 15.902, mean reward: 0.663 [0.583, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.792, 10.100], loss: 0.001574, mae: 0.042570, mean_q: 1.194619
 83036/100000: episode: 1678, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 14.434, mean reward: 0.722 [0.657, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.348, 10.100], loss: 0.001432, mae: 0.040451, mean_q: 1.196625
 83075/100000: episode: 1679, duration: 0.219s, episode steps: 39, steps per second: 178, episode reward: 25.455, mean reward: 0.653 [0.558, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-1.253, 10.205], loss: 0.001686, mae: 0.044442, mean_q: 1.192376
 83093/100000: episode: 1680, duration: 0.103s, episode steps: 18, steps per second: 176, episode reward: 12.492, mean reward: 0.694 [0.613, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.401, 10.100], loss: 0.002097, mae: 0.045553, mean_q: 1.195433
 83117/100000: episode: 1681, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 16.891, mean reward: 0.704 [0.641, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.295, 10.100], loss: 0.001575, mae: 0.040470, mean_q: 1.193781
 83137/100000: episode: 1682, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 13.781, mean reward: 0.689 [0.576, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.224, 10.100], loss: 0.001718, mae: 0.046852, mean_q: 1.212703
 83225/100000: episode: 1683, duration: 0.479s, episode steps: 88, steps per second: 184, episode reward: 52.009, mean reward: 0.591 [0.513, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.565 [-0.954, 10.100], loss: 0.001824, mae: 0.044965, mean_q: 1.192527
 83255/100000: episode: 1684, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 18.856, mean reward: 0.629 [0.538, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.511, 10.100], loss: 0.002004, mae: 0.048101, mean_q: 1.188417
 83275/100000: episode: 1685, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 13.506, mean reward: 0.675 [0.618, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.201, 10.100], loss: 0.001885, mae: 0.043935, mean_q: 1.185524
 83305/100000: episode: 1686, duration: 0.161s, episode steps: 30, steps per second: 187, episode reward: 21.132, mean reward: 0.704 [0.631, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-1.209, 10.100], loss: 0.001808, mae: 0.045129, mean_q: 1.188888
 83323/100000: episode: 1687, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 12.385, mean reward: 0.688 [0.619, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.384, 10.100], loss: 0.002030, mae: 0.046922, mean_q: 1.185924
 83347/100000: episode: 1688, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 15.350, mean reward: 0.640 [0.512, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.042, 10.100], loss: 0.001789, mae: 0.043909, mean_q: 1.190551
 83377/100000: episode: 1689, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 20.570, mean reward: 0.686 [0.629, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.776, 10.100], loss: 0.001838, mae: 0.045568, mean_q: 1.194886
 83397/100000: episode: 1690, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 12.776, mean reward: 0.639 [0.551, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.205, 10.100], loss: 0.001527, mae: 0.040379, mean_q: 1.186855
 83427/100000: episode: 1691, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 19.796, mean reward: 0.660 [0.578, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.090, 10.100], loss: 0.001645, mae: 0.042352, mean_q: 1.198769
 83454/100000: episode: 1692, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 19.543, mean reward: 0.724 [0.632, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.350, 10.100], loss: 0.001566, mae: 0.040951, mean_q: 1.196194
 83478/100000: episode: 1693, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 16.098, mean reward: 0.671 [0.599, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.134, 10.100], loss: 0.001700, mae: 0.043712, mean_q: 1.202191
 83502/100000: episode: 1694, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 15.700, mean reward: 0.654 [0.566, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.491, 10.100], loss: 0.001719, mae: 0.044103, mean_q: 1.201396
 83522/100000: episode: 1695, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 12.412, mean reward: 0.621 [0.551, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.407, 10.100], loss: 0.001689, mae: 0.044913, mean_q: 1.203847
 83549/100000: episode: 1696, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 19.074, mean reward: 0.706 [0.607, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.344, 10.100], loss: 0.001759, mae: 0.045500, mean_q: 1.194106
 83637/100000: episode: 1697, duration: 0.491s, episode steps: 88, steps per second: 179, episode reward: 51.789, mean reward: 0.589 [0.498, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.570 [-1.353, 10.266], loss: 0.001677, mae: 0.043357, mean_q: 1.201285
 83664/100000: episode: 1698, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 19.118, mean reward: 0.708 [0.633, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-1.058, 10.100], loss: 0.001460, mae: 0.040210, mean_q: 1.202422
 83683/100000: episode: 1699, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 12.701, mean reward: 0.668 [0.602, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.288, 10.100], loss: 0.001644, mae: 0.041791, mean_q: 1.207546
 83701/100000: episode: 1700, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 13.470, mean reward: 0.748 [0.686, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.649, 10.100], loss: 0.001627, mae: 0.043615, mean_q: 1.209572
 83728/100000: episode: 1701, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 19.765, mean reward: 0.732 [0.628, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.201, 10.100], loss: 0.001454, mae: 0.040670, mean_q: 1.204874
 83755/100000: episode: 1702, duration: 0.146s, episode steps: 27, steps per second: 186, episode reward: 19.213, mean reward: 0.712 [0.650, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.398, 10.100], loss: 0.001871, mae: 0.044762, mean_q: 1.209213
 83774/100000: episode: 1703, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 13.932, mean reward: 0.733 [0.640, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.564, 10.100], loss: 0.002157, mae: 0.048289, mean_q: 1.213120
 83801/100000: episode: 1704, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 18.643, mean reward: 0.690 [0.589, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.303, 10.100], loss: 0.001896, mae: 0.046712, mean_q: 1.202611
 83840/100000: episode: 1705, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 23.997, mean reward: 0.615 [0.516, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.239, 10.354], loss: 0.001727, mae: 0.043095, mean_q: 1.213284
 83864/100000: episode: 1706, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 16.711, mean reward: 0.696 [0.640, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.317, 10.100], loss: 0.001671, mae: 0.043138, mean_q: 1.207765
 83903/100000: episode: 1707, duration: 0.205s, episode steps: 39, steps per second: 191, episode reward: 26.973, mean reward: 0.692 [0.626, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.681, 10.461], loss: 0.001570, mae: 0.041958, mean_q: 1.216374
 83942/100000: episode: 1708, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 28.725, mean reward: 0.737 [0.631, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.387, 10.411], loss: 0.001709, mae: 0.043751, mean_q: 1.213813
 83981/100000: episode: 1709, duration: 0.201s, episode steps: 39, steps per second: 194, episode reward: 24.746, mean reward: 0.635 [0.584, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.889, 10.231], loss: 0.001601, mae: 0.043136, mean_q: 1.214646
 84000/100000: episode: 1710, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 13.378, mean reward: 0.704 [0.633, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.630, 10.100], loss: 0.001752, mae: 0.044205, mean_q: 1.214632
 84030/100000: episode: 1711, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 21.517, mean reward: 0.717 [0.652, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.607, 10.100], loss: 0.001419, mae: 0.041570, mean_q: 1.219432
 84048/100000: episode: 1712, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 12.421, mean reward: 0.690 [0.622, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.125, 10.100], loss: 0.001628, mae: 0.042458, mean_q: 1.220825
 84087/100000: episode: 1713, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 26.818, mean reward: 0.688 [0.580, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.056, 10.380], loss: 0.001579, mae: 0.043195, mean_q: 1.219305
 84106/100000: episode: 1714, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 13.443, mean reward: 0.708 [0.645, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.243, 10.100], loss: 0.001914, mae: 0.043029, mean_q: 1.219018
 84194/100000: episode: 1715, duration: 0.457s, episode steps: 88, steps per second: 192, episode reward: 53.537, mean reward: 0.608 [0.507, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.571 [-1.207, 10.319], loss: 0.001672, mae: 0.043448, mean_q: 1.225678
 84215/100000: episode: 1716, duration: 0.122s, episode steps: 21, steps per second: 171, episode reward: 14.928, mean reward: 0.711 [0.653, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.540, 10.100], loss: 0.001481, mae: 0.042763, mean_q: 1.221894
 84236/100000: episode: 1717, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 14.623, mean reward: 0.696 [0.625, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.112, 10.100], loss: 0.001289, mae: 0.038227, mean_q: 1.220085
 84275/100000: episode: 1718, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 25.132, mean reward: 0.644 [0.580, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.169, 10.310], loss: 0.001723, mae: 0.042880, mean_q: 1.223067
 84293/100000: episode: 1719, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 12.627, mean reward: 0.702 [0.583, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.176, 10.100], loss: 0.001718, mae: 0.042844, mean_q: 1.226372
[Info] Complete ISplit Iteration
[Info] Levels: [1.3630928, 1.5078049]
[Info] Cond. Prob: [0.1, 0.17]
[Info] Error Prob: 0.017

 84381/100000: episode: 1720, duration: 4.844s, episode steps: 88, steps per second: 18, episode reward: 53.027, mean reward: 0.603 [0.505, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.575 [-0.384, 10.372], loss: 0.001523, mae: 0.041516, mean_q: 1.228217
 84481/100000: episode: 1721, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 60.352, mean reward: 0.604 [0.505, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.500, 10.195], loss: 0.001579, mae: 0.042688, mean_q: 1.222485
 84581/100000: episode: 1722, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 61.701, mean reward: 0.617 [0.503, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.251, 10.247], loss: 0.001808, mae: 0.044776, mean_q: 1.228346
 84681/100000: episode: 1723, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 61.458, mean reward: 0.615 [0.509, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.339, 10.340], loss: 0.001815, mae: 0.045560, mean_q: 1.224907
 84781/100000: episode: 1724, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 57.741, mean reward: 0.577 [0.499, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.644, 10.110], loss: 0.001596, mae: 0.043260, mean_q: 1.225609
 84881/100000: episode: 1725, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.639, mean reward: 0.576 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.893, 10.098], loss: 0.001569, mae: 0.042478, mean_q: 1.228926
 84981/100000: episode: 1726, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 57.720, mean reward: 0.577 [0.498, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.615, 10.098], loss: 0.001620, mae: 0.042907, mean_q: 1.228272
 85081/100000: episode: 1727, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 60.138, mean reward: 0.601 [0.512, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.126, 10.236], loss: 0.001609, mae: 0.042614, mean_q: 1.226821
 85181/100000: episode: 1728, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.016, mean reward: 0.600 [0.511, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.756, 10.271], loss: 0.001561, mae: 0.042133, mean_q: 1.230122
 85281/100000: episode: 1729, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.198, mean reward: 0.602 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.537, 10.356], loss: 0.001713, mae: 0.044178, mean_q: 1.233007
 85381/100000: episode: 1730, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.869, mean reward: 0.579 [0.506, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.813, 10.122], loss: 0.001613, mae: 0.042419, mean_q: 1.227525
 85481/100000: episode: 1731, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 57.610, mean reward: 0.576 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.712, 10.195], loss: 0.001645, mae: 0.043105, mean_q: 1.225103
 85581/100000: episode: 1732, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.815, mean reward: 0.578 [0.504, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.965, 10.129], loss: 0.001389, mae: 0.039792, mean_q: 1.227815
 85681/100000: episode: 1733, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.555, mean reward: 0.586 [0.504, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.421, 10.127], loss: 0.001537, mae: 0.041573, mean_q: 1.228374
 85781/100000: episode: 1734, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 57.936, mean reward: 0.579 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.202, 10.099], loss: 0.001609, mae: 0.042431, mean_q: 1.228687
 85881/100000: episode: 1735, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.461, mean reward: 0.575 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.061, 10.133], loss: 0.001707, mae: 0.043669, mean_q: 1.224455
 85981/100000: episode: 1736, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.802, mean reward: 0.578 [0.502, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.098], loss: 0.001478, mae: 0.041473, mean_q: 1.225647
 86081/100000: episode: 1737, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.859, mean reward: 0.589 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.682, 10.175], loss: 0.001464, mae: 0.040867, mean_q: 1.222653
 86181/100000: episode: 1738, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 57.036, mean reward: 0.570 [0.510, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.248, 10.223], loss: 0.001462, mae: 0.040926, mean_q: 1.222224
 86281/100000: episode: 1739, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.285, mean reward: 0.593 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.865, 10.098], loss: 0.001789, mae: 0.044463, mean_q: 1.218604
 86381/100000: episode: 1740, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.806, mean reward: 0.598 [0.499, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.479, 10.098], loss: 0.001722, mae: 0.044610, mean_q: 1.219992
 86481/100000: episode: 1741, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.027, mean reward: 0.580 [0.503, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.085, 10.098], loss: 0.001749, mae: 0.044398, mean_q: 1.215699
 86581/100000: episode: 1742, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 60.916, mean reward: 0.609 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.014, 10.334], loss: 0.001610, mae: 0.042954, mean_q: 1.214552
 86681/100000: episode: 1743, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 60.575, mean reward: 0.606 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.894, 10.098], loss: 0.001502, mae: 0.042089, mean_q: 1.213551
 86781/100000: episode: 1744, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.151, mean reward: 0.572 [0.500, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.563, 10.107], loss: 0.001711, mae: 0.043553, mean_q: 1.219481
 86881/100000: episode: 1745, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.809, mean reward: 0.578 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.797, 10.098], loss: 0.001706, mae: 0.044028, mean_q: 1.215100
 86981/100000: episode: 1746, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.344, mean reward: 0.583 [0.511, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.747, 10.098], loss: 0.001614, mae: 0.042954, mean_q: 1.215482
 87081/100000: episode: 1747, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 61.795, mean reward: 0.618 [0.503, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.454, 10.275], loss: 0.001653, mae: 0.043787, mean_q: 1.207767
 87181/100000: episode: 1748, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.454, mean reward: 0.605 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.918, 10.098], loss: 0.001595, mae: 0.042634, mean_q: 1.211018
 87281/100000: episode: 1749, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 62.616, mean reward: 0.626 [0.521, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.781, 10.161], loss: 0.001575, mae: 0.043144, mean_q: 1.210255
 87381/100000: episode: 1750, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 60.848, mean reward: 0.608 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.835, 10.112], loss: 0.001518, mae: 0.041495, mean_q: 1.209312
 87481/100000: episode: 1751, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.241, mean reward: 0.592 [0.516, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.834, 10.195], loss: 0.001538, mae: 0.042195, mean_q: 1.211861
 87581/100000: episode: 1752, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.498, mean reward: 0.595 [0.509, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.513, 10.098], loss: 0.001780, mae: 0.045129, mean_q: 1.213022
 87681/100000: episode: 1753, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 60.209, mean reward: 0.602 [0.515, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.823, 10.117], loss: 0.001640, mae: 0.043491, mean_q: 1.207944
 87781/100000: episode: 1754, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.172, mean reward: 0.582 [0.505, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.696, 10.195], loss: 0.001765, mae: 0.045039, mean_q: 1.201066
 87881/100000: episode: 1755, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.379, mean reward: 0.574 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.755, 10.098], loss: 0.001590, mae: 0.042983, mean_q: 1.198913
 87981/100000: episode: 1756, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.061, mean reward: 0.581 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.616, 10.098], loss: 0.001604, mae: 0.042963, mean_q: 1.203728
 88081/100000: episode: 1757, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 60.737, mean reward: 0.607 [0.518, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.737, 10.098], loss: 0.001642, mae: 0.043731, mean_q: 1.197228
 88181/100000: episode: 1758, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.601, mean reward: 0.576 [0.500, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.898, 10.170], loss: 0.001542, mae: 0.042128, mean_q: 1.198217
 88281/100000: episode: 1759, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.761, mean reward: 0.598 [0.506, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.841, 10.445], loss: 0.001631, mae: 0.043191, mean_q: 1.196234
 88381/100000: episode: 1760, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.639, mean reward: 0.576 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.809, 10.098], loss: 0.001669, mae: 0.043735, mean_q: 1.187937
 88481/100000: episode: 1761, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.777, mean reward: 0.598 [0.512, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.159, 10.098], loss: 0.001805, mae: 0.045875, mean_q: 1.185892
 88581/100000: episode: 1762, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 57.706, mean reward: 0.577 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.454, 10.110], loss: 0.001630, mae: 0.042628, mean_q: 1.182450
 88681/100000: episode: 1763, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 56.763, mean reward: 0.568 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.874, 10.167], loss: 0.001542, mae: 0.042624, mean_q: 1.186688
 88781/100000: episode: 1764, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.744, mean reward: 0.587 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.662, 10.098], loss: 0.001579, mae: 0.042414, mean_q: 1.179764
 88881/100000: episode: 1765, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 59.410, mean reward: 0.594 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.418, 10.098], loss: 0.001752, mae: 0.044563, mean_q: 1.176524
 88981/100000: episode: 1766, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 60.829, mean reward: 0.608 [0.511, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.358, 10.098], loss: 0.001578, mae: 0.042893, mean_q: 1.176613
 89081/100000: episode: 1767, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.657, mean reward: 0.597 [0.505, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.146, 10.182], loss: 0.001612, mae: 0.042871, mean_q: 1.173568
 89181/100000: episode: 1768, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.745, mean reward: 0.587 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.818, 10.180], loss: 0.001558, mae: 0.042540, mean_q: 1.171026
 89281/100000: episode: 1769, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.987, mean reward: 0.610 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.379, 10.135], loss: 0.001440, mae: 0.041121, mean_q: 1.171013
 89381/100000: episode: 1770, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.873, mean reward: 0.609 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.467, 10.098], loss: 0.001489, mae: 0.041862, mean_q: 1.171691
 89481/100000: episode: 1771, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.495, mean reward: 0.585 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.157, 10.208], loss: 0.001396, mae: 0.040604, mean_q: 1.165524
 89581/100000: episode: 1772, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.630, mean reward: 0.586 [0.503, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.526, 10.098], loss: 0.001486, mae: 0.041574, mean_q: 1.168084
 89681/100000: episode: 1773, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 57.748, mean reward: 0.577 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.618, 10.098], loss: 0.001382, mae: 0.040299, mean_q: 1.165071
 89781/100000: episode: 1774, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.556, mean reward: 0.596 [0.499, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.706, 10.098], loss: 0.001571, mae: 0.042765, mean_q: 1.168612
 89881/100000: episode: 1775, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.233, mean reward: 0.592 [0.506, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.830, 10.098], loss: 0.001490, mae: 0.041982, mean_q: 1.168574
 89981/100000: episode: 1776, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.461, mean reward: 0.575 [0.514, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.336, 10.098], loss: 0.001496, mae: 0.041622, mean_q: 1.168694
 90081/100000: episode: 1777, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.351, mean reward: 0.594 [0.509, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.142, 10.118], loss: 0.001517, mae: 0.042822, mean_q: 1.166729
 90181/100000: episode: 1778, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.936, mean reward: 0.589 [0.516, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.094, 10.098], loss: 0.001515, mae: 0.042134, mean_q: 1.164887
 90281/100000: episode: 1779, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.882, mean reward: 0.579 [0.507, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.377, 10.098], loss: 0.001392, mae: 0.040744, mean_q: 1.164936
 90381/100000: episode: 1780, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 60.597, mean reward: 0.606 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.600, 10.105], loss: 0.001446, mae: 0.041376, mean_q: 1.166875
 90481/100000: episode: 1781, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 61.813, mean reward: 0.618 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.986, 10.379], loss: 0.001448, mae: 0.041527, mean_q: 1.168409
 90581/100000: episode: 1782, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.485, mean reward: 0.585 [0.510, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.045, 10.329], loss: 0.001532, mae: 0.042826, mean_q: 1.168906
 90681/100000: episode: 1783, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.584, mean reward: 0.586 [0.505, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.544, 10.098], loss: 0.001434, mae: 0.041281, mean_q: 1.168742
 90781/100000: episode: 1784, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.044, mean reward: 0.590 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.639, 10.098], loss: 0.001539, mae: 0.042617, mean_q: 1.169166
 90881/100000: episode: 1785, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.885, mean reward: 0.579 [0.504, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.188, 10.246], loss: 0.001489, mae: 0.042048, mean_q: 1.169343
 90981/100000: episode: 1786, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.959, mean reward: 0.580 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.632, 10.346], loss: 0.001465, mae: 0.041473, mean_q: 1.166447
 91081/100000: episode: 1787, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.524, mean reward: 0.575 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.202, 10.098], loss: 0.001509, mae: 0.042216, mean_q: 1.168199
 91181/100000: episode: 1788, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.946, mean reward: 0.579 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.255, 10.098], loss: 0.001537, mae: 0.042624, mean_q: 1.168149
 91281/100000: episode: 1789, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.796, mean reward: 0.578 [0.509, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.493, 10.098], loss: 0.001383, mae: 0.040952, mean_q: 1.166334
 91381/100000: episode: 1790, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.766, mean reward: 0.578 [0.507, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.425, 10.098], loss: 0.001412, mae: 0.041187, mean_q: 1.167392
 91481/100000: episode: 1791, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.198, mean reward: 0.572 [0.499, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.811, 10.098], loss: 0.001532, mae: 0.042725, mean_q: 1.168562
 91581/100000: episode: 1792, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.792, mean reward: 0.588 [0.501, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.021, 10.261], loss: 0.001665, mae: 0.044467, mean_q: 1.165935
 91681/100000: episode: 1793, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.876, mean reward: 0.589 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.478, 10.206], loss: 0.001584, mae: 0.042867, mean_q: 1.165802
 91781/100000: episode: 1794, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.171, mean reward: 0.582 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.495, 10.098], loss: 0.001499, mae: 0.041995, mean_q: 1.165779
 91881/100000: episode: 1795, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 60.266, mean reward: 0.603 [0.512, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.281, 10.144], loss: 0.001549, mae: 0.042981, mean_q: 1.167204
 91981/100000: episode: 1796, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 63.844, mean reward: 0.638 [0.502, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.061, 10.489], loss: 0.001511, mae: 0.042151, mean_q: 1.168045
 92081/100000: episode: 1797, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.828, mean reward: 0.598 [0.515, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.660, 10.258], loss: 0.001599, mae: 0.042812, mean_q: 1.167770
 92181/100000: episode: 1798, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.658, mean reward: 0.587 [0.514, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.794, 10.190], loss: 0.001667, mae: 0.044252, mean_q: 1.169117
 92281/100000: episode: 1799, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.048, mean reward: 0.600 [0.517, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.198, 10.098], loss: 0.001708, mae: 0.044546, mean_q: 1.167760
 92381/100000: episode: 1800, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.302, mean reward: 0.593 [0.519, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.265, 10.104], loss: 0.001510, mae: 0.042261, mean_q: 1.165696
 92481/100000: episode: 1801, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.655, mean reward: 0.597 [0.512, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.104, 10.098], loss: 0.001638, mae: 0.043993, mean_q: 1.165666
 92581/100000: episode: 1802, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 59.829, mean reward: 0.598 [0.501, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.582, 10.429], loss: 0.001512, mae: 0.041606, mean_q: 1.166826
 92681/100000: episode: 1803, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.011, mean reward: 0.600 [0.502, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.016, 10.163], loss: 0.001528, mae: 0.042556, mean_q: 1.165384
 92781/100000: episode: 1804, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.560, mean reward: 0.596 [0.512, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.729, 10.098], loss: 0.001420, mae: 0.041421, mean_q: 1.166033
 92881/100000: episode: 1805, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.014, mean reward: 0.580 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.454, 10.098], loss: 0.001509, mae: 0.041673, mean_q: 1.168713
 92981/100000: episode: 1806, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.738, mean reward: 0.597 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.609, 10.098], loss: 0.001466, mae: 0.041244, mean_q: 1.165966
 93081/100000: episode: 1807, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.992, mean reward: 0.590 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.396, 10.098], loss: 0.001539, mae: 0.043342, mean_q: 1.167001
 93181/100000: episode: 1808, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.815, mean reward: 0.578 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.309, 10.287], loss: 0.001547, mae: 0.042694, mean_q: 1.166352
 93281/100000: episode: 1809, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 60.206, mean reward: 0.602 [0.499, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.787, 10.518], loss: 0.001469, mae: 0.041892, mean_q: 1.166020
 93381/100000: episode: 1810, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.828, mean reward: 0.588 [0.505, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.425, 10.098], loss: 0.001617, mae: 0.043802, mean_q: 1.167987
 93481/100000: episode: 1811, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.456, mean reward: 0.585 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.874, 10.346], loss: 0.001593, mae: 0.043298, mean_q: 1.167606
 93581/100000: episode: 1812, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.878, mean reward: 0.589 [0.516, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.862, 10.098], loss: 0.001479, mae: 0.042059, mean_q: 1.169328
 93681/100000: episode: 1813, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 56.434, mean reward: 0.564 [0.498, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.588, 10.129], loss: 0.001489, mae: 0.042356, mean_q: 1.167448
 93781/100000: episode: 1814, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.080, mean reward: 0.601 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.271, 10.238], loss: 0.001629, mae: 0.043581, mean_q: 1.167053
 93881/100000: episode: 1815, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.644, mean reward: 0.586 [0.503, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.685, 10.098], loss: 0.001557, mae: 0.043340, mean_q: 1.170963
 93981/100000: episode: 1816, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.977, mean reward: 0.590 [0.502, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.127, 10.098], loss: 0.001557, mae: 0.042443, mean_q: 1.163382
 94081/100000: episode: 1817, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 59.617, mean reward: 0.596 [0.513, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.555, 10.133], loss: 0.001557, mae: 0.042995, mean_q: 1.168178
 94181/100000: episode: 1818, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.786, mean reward: 0.578 [0.507, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.899, 10.244], loss: 0.001476, mae: 0.041955, mean_q: 1.166754
 94281/100000: episode: 1819, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.793, mean reward: 0.578 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.933, 10.099], loss: 0.001526, mae: 0.042690, mean_q: 1.165180
[Info] 1-TH LEVEL FOUND: 1.3496090173721313, Considering 10/90 traces
 94381/100000: episode: 1820, duration: 4.830s, episode steps: 100, steps per second: 21, episode reward: 58.362, mean reward: 0.584 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.549, 10.355], loss: 0.001595, mae: 0.043199, mean_q: 1.165942
 94418/100000: episode: 1821, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 26.267, mean reward: 0.710 [0.630, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.368, 10.100], loss: 0.001479, mae: 0.042069, mean_q: 1.164608
 94517/100000: episode: 1822, duration: 0.522s, episode steps: 99, steps per second: 190, episode reward: 59.961, mean reward: 0.606 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-0.530, 10.194], loss: 0.001566, mae: 0.043072, mean_q: 1.166686
 94555/100000: episode: 1823, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 23.377, mean reward: 0.615 [0.551, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-1.173, 10.208], loss: 0.001604, mae: 0.042532, mean_q: 1.163621
 94598/100000: episode: 1824, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 25.863, mean reward: 0.601 [0.521, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.530, 10.341], loss: 0.001549, mae: 0.042804, mean_q: 1.162506
 94635/100000: episode: 1825, duration: 0.198s, episode steps: 37, steps per second: 186, episode reward: 21.845, mean reward: 0.590 [0.518, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.095, 10.100], loss: 0.001738, mae: 0.045199, mean_q: 1.170806
 94734/100000: episode: 1826, duration: 0.510s, episode steps: 99, steps per second: 194, episode reward: 56.258, mean reward: 0.568 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.268, 10.176], loss: 0.001656, mae: 0.044148, mean_q: 1.165973
 94777/100000: episode: 1827, duration: 0.229s, episode steps: 43, steps per second: 187, episode reward: 25.120, mean reward: 0.584 [0.529, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.415, 10.100], loss: 0.001621, mae: 0.043558, mean_q: 1.165206
 94814/100000: episode: 1828, duration: 0.203s, episode steps: 37, steps per second: 183, episode reward: 23.469, mean reward: 0.634 [0.525, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.068, 10.100], loss: 0.001489, mae: 0.042423, mean_q: 1.163844
 94832/100000: episode: 1829, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 12.562, mean reward: 0.698 [0.654, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.468], loss: 0.001330, mae: 0.040638, mean_q: 1.168160
 94876/100000: episode: 1830, duration: 0.235s, episode steps: 44, steps per second: 188, episode reward: 29.452, mean reward: 0.669 [0.594, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-1.083, 10.295], loss: 0.001501, mae: 0.042758, mean_q: 1.168523
 94975/100000: episode: 1831, duration: 0.550s, episode steps: 99, steps per second: 180, episode reward: 59.924, mean reward: 0.605 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-1.011, 10.100], loss: 0.001680, mae: 0.044689, mean_q: 1.167952
 95013/100000: episode: 1832, duration: 0.214s, episode steps: 38, steps per second: 178, episode reward: 26.965, mean reward: 0.710 [0.621, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.791, 10.296], loss: 0.001533, mae: 0.041875, mean_q: 1.166028
 95112/100000: episode: 1833, duration: 0.510s, episode steps: 99, steps per second: 194, episode reward: 56.301, mean reward: 0.569 [0.499, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.784, 10.100], loss: 0.001753, mae: 0.044821, mean_q: 1.165963
 95141/100000: episode: 1834, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 18.786, mean reward: 0.648 [0.590, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.461, 10.387], loss: 0.001856, mae: 0.046162, mean_q: 1.166311
 95159/100000: episode: 1835, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 14.056, mean reward: 0.781 [0.718, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.596], loss: 0.001608, mae: 0.044237, mean_q: 1.172433
 95258/100000: episode: 1836, duration: 0.539s, episode steps: 99, steps per second: 184, episode reward: 57.805, mean reward: 0.584 [0.499, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.756, 10.185], loss: 0.001691, mae: 0.044499, mean_q: 1.171782
 95296/100000: episode: 1837, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 22.893, mean reward: 0.602 [0.503, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.625, 10.121], loss: 0.001549, mae: 0.042485, mean_q: 1.169375
 95333/100000: episode: 1838, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 25.774, mean reward: 0.697 [0.603, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.218, 10.100], loss: 0.001758, mae: 0.044562, mean_q: 1.169212
 95377/100000: episode: 1839, duration: 0.249s, episode steps: 44, steps per second: 177, episode reward: 26.742, mean reward: 0.608 [0.523, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.470, 10.144], loss: 0.001696, mae: 0.043800, mean_q: 1.173335
 95395/100000: episode: 1840, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 13.060, mean reward: 0.726 [0.653, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.071, 10.385], loss: 0.001374, mae: 0.041104, mean_q: 1.166465
 95424/100000: episode: 1841, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 19.141, mean reward: 0.660 [0.588, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.273, 10.377], loss: 0.001504, mae: 0.041339, mean_q: 1.169085
 95523/100000: episode: 1842, duration: 0.540s, episode steps: 99, steps per second: 183, episode reward: 58.490, mean reward: 0.591 [0.511, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-1.641, 10.100], loss: 0.001632, mae: 0.043098, mean_q: 1.171224
 95561/100000: episode: 1843, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 23.434, mean reward: 0.617 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.035, 10.214], loss: 0.001428, mae: 0.040414, mean_q: 1.174002
 95568/100000: episode: 1844, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 4.320, mean reward: 0.617 [0.581, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.139, 10.100], loss: 0.001078, mae: 0.037938, mean_q: 1.170364
 95611/100000: episode: 1845, duration: 0.233s, episode steps: 43, steps per second: 185, episode reward: 26.226, mean reward: 0.610 [0.516, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.498, 10.100], loss: 0.001483, mae: 0.042149, mean_q: 1.171013
 95710/100000: episode: 1846, duration: 0.553s, episode steps: 99, steps per second: 179, episode reward: 58.879, mean reward: 0.595 [0.498, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.429, 10.100], loss: 0.001695, mae: 0.043660, mean_q: 1.171599
 95728/100000: episode: 1847, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 13.766, mean reward: 0.765 [0.695, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-1.250, 10.466], loss: 0.001814, mae: 0.046480, mean_q: 1.186265
 95772/100000: episode: 1848, duration: 0.214s, episode steps: 44, steps per second: 206, episode reward: 30.589, mean reward: 0.695 [0.544, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.160, 10.160], loss: 0.001711, mae: 0.043558, mean_q: 1.172323
 95783/100000: episode: 1849, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 6.932, mean reward: 0.630 [0.584, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.700, 10.100], loss: 0.001928, mae: 0.048027, mean_q: 1.178693
 95820/100000: episode: 1850, duration: 0.181s, episode steps: 37, steps per second: 204, episode reward: 23.138, mean reward: 0.625 [0.530, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.103, 10.100], loss: 0.001569, mae: 0.042748, mean_q: 1.178792
 95849/100000: episode: 1851, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 18.812, mean reward: 0.649 [0.558, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.516, 10.308], loss: 0.001686, mae: 0.043164, mean_q: 1.171672
 95869/100000: episode: 1852, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 14.871, mean reward: 0.744 [0.640, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.354, 10.100], loss: 0.001859, mae: 0.045858, mean_q: 1.183627
 95913/100000: episode: 1853, duration: 0.231s, episode steps: 44, steps per second: 190, episode reward: 27.720, mean reward: 0.630 [0.529, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-1.273, 10.100], loss: 0.001847, mae: 0.045628, mean_q: 1.181245
 95950/100000: episode: 1854, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 24.544, mean reward: 0.663 [0.595, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.763, 10.100], loss: 0.001767, mae: 0.044166, mean_q: 1.181993
 95994/100000: episode: 1855, duration: 0.232s, episode steps: 44, steps per second: 190, episode reward: 27.580, mean reward: 0.627 [0.521, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.650, 10.356], loss: 0.001999, mae: 0.046720, mean_q: 1.189585
 96023/100000: episode: 1856, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 18.166, mean reward: 0.626 [0.551, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.621, 10.306], loss: 0.001702, mae: 0.044841, mean_q: 1.189831
 96067/100000: episode: 1857, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 27.223, mean reward: 0.619 [0.503, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.469, 10.100], loss: 0.001710, mae: 0.044618, mean_q: 1.186214
 96110/100000: episode: 1858, duration: 0.228s, episode steps: 43, steps per second: 189, episode reward: 25.417, mean reward: 0.591 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.148, 10.123], loss: 0.001678, mae: 0.043854, mean_q: 1.179647
 96121/100000: episode: 1859, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 7.691, mean reward: 0.699 [0.637, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.335, 10.100], loss: 0.001870, mae: 0.044616, mean_q: 1.181600
 96159/100000: episode: 1860, duration: 0.218s, episode steps: 38, steps per second: 174, episode reward: 23.565, mean reward: 0.620 [0.527, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.104, 10.221], loss: 0.001604, mae: 0.042780, mean_q: 1.187206
 96202/100000: episode: 1861, duration: 0.235s, episode steps: 43, steps per second: 183, episode reward: 26.339, mean reward: 0.613 [0.521, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.822, 10.100], loss: 0.001588, mae: 0.043029, mean_q: 1.191456
 96231/100000: episode: 1862, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 20.101, mean reward: 0.693 [0.606, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.315, 10.306], loss: 0.001682, mae: 0.042267, mean_q: 1.185062
 96268/100000: episode: 1863, duration: 0.203s, episode steps: 37, steps per second: 183, episode reward: 23.273, mean reward: 0.629 [0.544, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.269, 10.181], loss: 0.001696, mae: 0.043989, mean_q: 1.186345
 96286/100000: episode: 1864, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 11.546, mean reward: 0.641 [0.584, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.350, 10.298], loss: 0.001549, mae: 0.041403, mean_q: 1.185851
 96306/100000: episode: 1865, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 13.573, mean reward: 0.679 [0.582, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.447, 10.100], loss: 0.001681, mae: 0.044783, mean_q: 1.185056
 96313/100000: episode: 1866, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 4.616, mean reward: 0.659 [0.579, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.129, 10.100], loss: 0.001556, mae: 0.042767, mean_q: 1.199349
 96331/100000: episode: 1867, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 13.487, mean reward: 0.749 [0.708, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.588, 10.487], loss: 0.001480, mae: 0.041039, mean_q: 1.183077
 96342/100000: episode: 1868, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 7.287, mean reward: 0.662 [0.638, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.358, 10.100], loss: 0.001554, mae: 0.041289, mean_q: 1.190692
 96379/100000: episode: 1869, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 24.231, mean reward: 0.655 [0.572, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.764, 10.100], loss: 0.001527, mae: 0.042088, mean_q: 1.189830
 96422/100000: episode: 1870, duration: 0.239s, episode steps: 43, steps per second: 180, episode reward: 31.443, mean reward: 0.731 [0.621, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.965, 10.367], loss: 0.001858, mae: 0.045655, mean_q: 1.188827
 96442/100000: episode: 1871, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 13.502, mean reward: 0.675 [0.620, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.600, 10.100], loss: 0.001776, mae: 0.045447, mean_q: 1.187693
 96453/100000: episode: 1872, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 7.068, mean reward: 0.643 [0.578, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.125, 10.100], loss: 0.001774, mae: 0.044175, mean_q: 1.186609
 96464/100000: episode: 1873, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 8.462, mean reward: 0.769 [0.637, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.425, 10.100], loss: 0.001419, mae: 0.040331, mean_q: 1.187980
 96563/100000: episode: 1874, duration: 0.547s, episode steps: 99, steps per second: 181, episode reward: 60.130, mean reward: 0.607 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.963, 10.100], loss: 0.001680, mae: 0.044149, mean_q: 1.190655
 96606/100000: episode: 1875, duration: 0.222s, episode steps: 43, steps per second: 194, episode reward: 26.225, mean reward: 0.610 [0.511, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.745, 10.100], loss: 0.001862, mae: 0.045026, mean_q: 1.190254
 96705/100000: episode: 1876, duration: 0.510s, episode steps: 99, steps per second: 194, episode reward: 57.502, mean reward: 0.581 [0.510, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-0.704, 10.172], loss: 0.001703, mae: 0.043915, mean_q: 1.191070
 96725/100000: episode: 1877, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 13.232, mean reward: 0.662 [0.602, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.281, 10.100], loss: 0.001587, mae: 0.042658, mean_q: 1.188705
 96754/100000: episode: 1878, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 19.834, mean reward: 0.684 [0.573, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.507, 10.291], loss: 0.001675, mae: 0.042935, mean_q: 1.189277
 96783/100000: episode: 1879, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 19.559, mean reward: 0.674 [0.592, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.544, 10.292], loss: 0.001726, mae: 0.044241, mean_q: 1.197500
 96826/100000: episode: 1880, duration: 0.248s, episode steps: 43, steps per second: 173, episode reward: 30.029, mean reward: 0.698 [0.620, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.218, 10.359], loss: 0.001605, mae: 0.042873, mean_q: 1.192739
 96855/100000: episode: 1881, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 19.216, mean reward: 0.663 [0.607, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.161, 10.313], loss: 0.001598, mae: 0.042045, mean_q: 1.191959
 96954/100000: episode: 1882, duration: 0.527s, episode steps: 99, steps per second: 188, episode reward: 63.965, mean reward: 0.646 [0.515, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.448, 10.100], loss: 0.001710, mae: 0.043890, mean_q: 1.199636
 96992/100000: episode: 1883, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 23.815, mean reward: 0.627 [0.562, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.230, 10.332], loss: 0.001675, mae: 0.043420, mean_q: 1.200068
 97012/100000: episode: 1884, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 13.309, mean reward: 0.665 [0.600, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.508, 10.100], loss: 0.001497, mae: 0.042003, mean_q: 1.194113
 97023/100000: episode: 1885, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 7.311, mean reward: 0.665 [0.628, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.389, 10.100], loss: 0.001873, mae: 0.045101, mean_q: 1.201746
 97043/100000: episode: 1886, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 14.175, mean reward: 0.709 [0.613, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.401, 10.100], loss: 0.001692, mae: 0.043332, mean_q: 1.203892
 97050/100000: episode: 1887, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 4.239, mean reward: 0.606 [0.566, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.376, 10.100], loss: 0.001662, mae: 0.041475, mean_q: 1.203790
 97057/100000: episode: 1888, duration: 0.050s, episode steps: 7, steps per second: 139, episode reward: 4.195, mean reward: 0.599 [0.567, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.384, 10.100], loss: 0.001966, mae: 0.045303, mean_q: 1.211616
 97064/100000: episode: 1889, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 4.660, mean reward: 0.666 [0.610, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.424, 10.100], loss: 0.001296, mae: 0.037795, mean_q: 1.177955
[Info] FALSIFICATION!
 97094/100000: episode: 1890, duration: 0.426s, episode steps: 30, steps per second: 70, episode reward: 22.809, mean reward: 0.760 [0.645, 1.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.161, 9.988], loss: 0.001526, mae: 0.041632, mean_q: 1.199973
 97123/100000: episode: 1891, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 20.983, mean reward: 0.724 [0.636, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.122, 10.366], loss: 0.001601, mae: 0.042720, mean_q: 1.204355
 97134/100000: episode: 1892, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 7.333, mean reward: 0.667 [0.632, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.269, 10.100], loss: 0.001719, mae: 0.043602, mean_q: 1.208207
 97178/100000: episode: 1893, duration: 0.223s, episode steps: 44, steps per second: 198, episode reward: 28.863, mean reward: 0.656 [0.520, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.259, 10.149], loss: 0.001709, mae: 0.043438, mean_q: 1.200192
 97222/100000: episode: 1894, duration: 0.245s, episode steps: 44, steps per second: 180, episode reward: 27.136, mean reward: 0.617 [0.498, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.243, 10.102], loss: 0.001701, mae: 0.044222, mean_q: 1.201248
 97233/100000: episode: 1895, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 7.541, mean reward: 0.686 [0.655, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.286, 10.100], loss: 0.001602, mae: 0.044550, mean_q: 1.206154
 97253/100000: episode: 1896, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 13.844, mean reward: 0.692 [0.630, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.298, 10.100], loss: 0.001509, mae: 0.041358, mean_q: 1.194973
 97352/100000: episode: 1897, duration: 0.543s, episode steps: 99, steps per second: 182, episode reward: 56.466, mean reward: 0.570 [0.506, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.754, 10.296], loss: 0.001503, mae: 0.041919, mean_q: 1.206493
 97370/100000: episode: 1898, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 12.040, mean reward: 0.669 [0.563, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.223], loss: 0.001739, mae: 0.044555, mean_q: 1.193855
 97469/100000: episode: 1899, duration: 0.527s, episode steps: 99, steps per second: 188, episode reward: 58.240, mean reward: 0.588 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-1.123, 10.223], loss: 0.001661, mae: 0.043794, mean_q: 1.200794
 97568/100000: episode: 1900, duration: 0.494s, episode steps: 99, steps per second: 200, episode reward: 59.220, mean reward: 0.598 [0.501, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-1.056, 10.204], loss: 0.001580, mae: 0.041650, mean_q: 1.202517
 97606/100000: episode: 1901, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 21.963, mean reward: 0.578 [0.522, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.790, 10.100], loss: 0.001545, mae: 0.042242, mean_q: 1.194627
 97643/100000: episode: 1902, duration: 0.180s, episode steps: 37, steps per second: 206, episode reward: 24.721, mean reward: 0.668 [0.569, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.751, 10.100], loss: 0.001435, mae: 0.040431, mean_q: 1.200784
 97663/100000: episode: 1903, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 14.277, mean reward: 0.714 [0.652, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.306, 10.100], loss: 0.001623, mae: 0.043772, mean_q: 1.202949
 97674/100000: episode: 1904, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 8.063, mean reward: 0.733 [0.686, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.374, 10.100], loss: 0.001324, mae: 0.039850, mean_q: 1.208803
 97712/100000: episode: 1905, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 26.544, mean reward: 0.699 [0.598, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.383, 10.457], loss: 0.001641, mae: 0.042949, mean_q: 1.202621
 97723/100000: episode: 1906, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 8.254, mean reward: 0.750 [0.654, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.389, 10.100], loss: 0.001524, mae: 0.041573, mean_q: 1.208476
 97741/100000: episode: 1907, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 12.830, mean reward: 0.713 [0.664, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.579], loss: 0.001673, mae: 0.041475, mean_q: 1.195973
 97779/100000: episode: 1908, duration: 0.208s, episode steps: 38, steps per second: 182, episode reward: 26.838, mean reward: 0.706 [0.626, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.806, 10.493], loss: 0.001746, mae: 0.044405, mean_q: 1.208576
 97817/100000: episode: 1909, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 24.041, mean reward: 0.633 [0.548, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.491, 10.273], loss: 0.001566, mae: 0.043754, mean_q: 1.209917
[Info] Complete ISplit Iteration
[Info] Levels: [1.349609, 1.6071721]
[Info] Cond. Prob: [0.1, 0.02]
[Info] Error Prob: 0.002

 97855/100000: episode: 1910, duration: 4.644s, episode steps: 38, steps per second: 8, episode reward: 22.996, mean reward: 0.605 [0.523, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.471, 10.100], loss: 0.001712, mae: 0.042989, mean_q: 1.210189
 97955/100000: episode: 1911, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.436, mean reward: 0.584 [0.510, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.376, 10.260], loss: 0.001660, mae: 0.044428, mean_q: 1.210263
 98055/100000: episode: 1912, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 57.787, mean reward: 0.578 [0.499, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.752, 10.239], loss: 0.001490, mae: 0.041054, mean_q: 1.208156
 98155/100000: episode: 1913, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.274, mean reward: 0.583 [0.504, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.575, 10.098], loss: 0.001483, mae: 0.041637, mean_q: 1.210812
 98255/100000: episode: 1914, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.951, mean reward: 0.580 [0.502, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.029, 10.098], loss: 0.001500, mae: 0.042047, mean_q: 1.205449
 98355/100000: episode: 1915, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.836, mean reward: 0.598 [0.506, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.196, 10.372], loss: 0.001605, mae: 0.042736, mean_q: 1.208635
 98455/100000: episode: 1916, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.556, mean reward: 0.596 [0.522, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.433, 10.202], loss: 0.001646, mae: 0.043353, mean_q: 1.209881
 98555/100000: episode: 1917, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.876, mean reward: 0.599 [0.507, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.852, 10.098], loss: 0.001556, mae: 0.042223, mean_q: 1.208940
 98655/100000: episode: 1918, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.417, mean reward: 0.594 [0.508, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.757, 10.098], loss: 0.001703, mae: 0.043861, mean_q: 1.208350
 98755/100000: episode: 1919, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.524, mean reward: 0.585 [0.503, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.247, 10.271], loss: 0.001614, mae: 0.042799, mean_q: 1.207334
 98855/100000: episode: 1920, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.749, mean reward: 0.577 [0.515, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.616, 10.310], loss: 0.001813, mae: 0.044980, mean_q: 1.212253
 98955/100000: episode: 1921, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 60.072, mean reward: 0.601 [0.511, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.286, 10.198], loss: 0.001488, mae: 0.041176, mean_q: 1.209816
 99055/100000: episode: 1922, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 57.345, mean reward: 0.573 [0.498, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.117, 10.124], loss: 0.001419, mae: 0.040266, mean_q: 1.205836
 99155/100000: episode: 1923, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.383, mean reward: 0.574 [0.505, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.867, 10.114], loss: 0.001662, mae: 0.042433, mean_q: 1.215516
 99255/100000: episode: 1924, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.414, mean reward: 0.574 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.685, 10.143], loss: 0.001659, mae: 0.042420, mean_q: 1.204094
 99355/100000: episode: 1925, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.625, mean reward: 0.576 [0.507, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.390, 10.278], loss: 0.001619, mae: 0.042653, mean_q: 1.206924
 99455/100000: episode: 1926, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.872, mean reward: 0.579 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.541, 10.165], loss: 0.001516, mae: 0.041017, mean_q: 1.207261
 99555/100000: episode: 1927, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.818, mean reward: 0.588 [0.511, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.808, 10.338], loss: 0.001571, mae: 0.041562, mean_q: 1.204652
 99655/100000: episode: 1928, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.158, mean reward: 0.582 [0.499, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.263, 10.098], loss: 0.001404, mae: 0.039792, mean_q: 1.206636
 99755/100000: episode: 1929, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.689, mean reward: 0.577 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.570, 10.151], loss: 0.001785, mae: 0.044807, mean_q: 1.206284
 99855/100000: episode: 1930, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.659, mean reward: 0.577 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.202, 10.257], loss: 0.001694, mae: 0.043011, mean_q: 1.200792
 99955/100000: episode: 1931, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.625, mean reward: 0.576 [0.503, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.324, 10.141], loss: 0.001618, mae: 0.041894, mean_q: 1.204857
done, took 1422.784 seconds
[Info] End Importance Splitting. Falsification occurred 16 times.
