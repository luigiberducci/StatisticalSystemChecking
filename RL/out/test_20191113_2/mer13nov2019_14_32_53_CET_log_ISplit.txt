Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.181s, episode steps: 100, steps per second: 553, episode reward: 59.322, mean reward: 0.593 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.478, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.086s, episode steps: 100, steps per second: 1162, episode reward: 59.433, mean reward: 0.594 [0.514, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.007, 10.229], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.066s, episode steps: 100, steps per second: 1519, episode reward: 58.976, mean reward: 0.590 [0.505, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.405, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.069s, episode steps: 100, steps per second: 1452, episode reward: 60.579, mean reward: 0.606 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.830, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.077s, episode steps: 100, steps per second: 1294, episode reward: 59.502, mean reward: 0.595 [0.514, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.787, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.099s, episode steps: 100, steps per second: 1007, episode reward: 61.570, mean reward: 0.616 [0.507, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.180, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.101s, episode steps: 100, steps per second: 995, episode reward: 60.560, mean reward: 0.606 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.075, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.068s, episode steps: 100, steps per second: 1481, episode reward: 60.841, mean reward: 0.608 [0.504, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.205, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.068s, episode steps: 100, steps per second: 1466, episode reward: 60.100, mean reward: 0.601 [0.499, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.875, 10.116], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.074s, episode steps: 100, steps per second: 1350, episode reward: 57.303, mean reward: 0.573 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.346, 10.233], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.067s, episode steps: 100, steps per second: 1498, episode reward: 57.652, mean reward: 0.577 [0.498, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.599, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.111s, episode steps: 100, steps per second: 904, episode reward: 58.913, mean reward: 0.589 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.440, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.069s, episode steps: 100, steps per second: 1444, episode reward: 61.592, mean reward: 0.616 [0.527, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.191, 10.425], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.067s, episode steps: 100, steps per second: 1490, episode reward: 58.469, mean reward: 0.585 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.710, 10.273], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.088s, episode steps: 100, steps per second: 1139, episode reward: 62.988, mean reward: 0.630 [0.509, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.218, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.070s, episode steps: 100, steps per second: 1431, episode reward: 61.249, mean reward: 0.612 [0.514, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.447, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.067s, episode steps: 100, steps per second: 1500, episode reward: 57.257, mean reward: 0.573 [0.504, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.693, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.072s, episode steps: 100, steps per second: 1382, episode reward: 58.087, mean reward: 0.581 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.807, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.064s, episode steps: 100, steps per second: 1554, episode reward: 58.671, mean reward: 0.587 [0.500, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.409, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.070s, episode steps: 100, steps per second: 1435, episode reward: 57.786, mean reward: 0.578 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.730, 10.141], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.068s, episode steps: 100, steps per second: 1467, episode reward: 58.012, mean reward: 0.580 [0.500, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.473, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.082s, episode steps: 100, steps per second: 1222, episode reward: 58.397, mean reward: 0.584 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.977, 10.149], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 58.719, mean reward: 0.587 [0.499, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.664, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.062s, episode steps: 100, steps per second: 1607, episode reward: 57.699, mean reward: 0.577 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.399, 10.278], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.065s, episode steps: 100, steps per second: 1529, episode reward: 57.854, mean reward: 0.579 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.111, 10.431], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.093s, episode steps: 100, steps per second: 1078, episode reward: 58.019, mean reward: 0.580 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.845, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.063s, episode steps: 100, steps per second: 1576, episode reward: 58.472, mean reward: 0.585 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.427, 10.147], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 57.181, mean reward: 0.572 [0.507, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.857, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.065s, episode steps: 100, steps per second: 1532, episode reward: 57.563, mean reward: 0.576 [0.503, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.141, 10.125], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.062s, episode steps: 100, steps per second: 1601, episode reward: 59.460, mean reward: 0.595 [0.504, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.612, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.062s, episode steps: 100, steps per second: 1601, episode reward: 59.454, mean reward: 0.595 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.258, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.068s, episode steps: 100, steps per second: 1474, episode reward: 60.582, mean reward: 0.606 [0.505, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.675, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 59.209, mean reward: 0.592 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.458, 10.268], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.072s, episode steps: 100, steps per second: 1383, episode reward: 57.421, mean reward: 0.574 [0.500, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.315, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.084s, episode steps: 100, steps per second: 1189, episode reward: 59.790, mean reward: 0.598 [0.525, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.214, 10.188], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.108s, episode steps: 100, steps per second: 925, episode reward: 58.379, mean reward: 0.584 [0.506, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.699, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.092s, episode steps: 100, steps per second: 1086, episode reward: 58.572, mean reward: 0.586 [0.519, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.933, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.072s, episode steps: 100, steps per second: 1397, episode reward: 59.650, mean reward: 0.597 [0.522, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.671, 10.195], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.074s, episode steps: 100, steps per second: 1346, episode reward: 62.401, mean reward: 0.624 [0.513, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.918, 10.328], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.087s, episode steps: 100, steps per second: 1150, episode reward: 57.520, mean reward: 0.575 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.944, 10.155], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.062s, episode steps: 100, steps per second: 1603, episode reward: 61.608, mean reward: 0.616 [0.504, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.465, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 58.149, mean reward: 0.581 [0.499, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.165, 10.149], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.062s, episode steps: 100, steps per second: 1602, episode reward: 58.988, mean reward: 0.590 [0.505, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.676, 10.253], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.062s, episode steps: 100, steps per second: 1600, episode reward: 57.484, mean reward: 0.575 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.169, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.067s, episode steps: 100, steps per second: 1502, episode reward: 60.312, mean reward: 0.603 [0.511, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.384, 10.244], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.063s, episode steps: 100, steps per second: 1597, episode reward: 59.690, mean reward: 0.597 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.674, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.074s, episode steps: 100, steps per second: 1350, episode reward: 56.735, mean reward: 0.567 [0.504, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.391, 10.163], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.076s, episode steps: 100, steps per second: 1318, episode reward: 58.601, mean reward: 0.586 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.147, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.074s, episode steps: 100, steps per second: 1343, episode reward: 62.930, mean reward: 0.629 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.326, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.140s, episode steps: 100, steps per second: 713, episode reward: 57.454, mean reward: 0.575 [0.503, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.758, 10.131], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.399s, episode steps: 100, steps per second: 71, episode reward: 58.549, mean reward: 0.585 [0.511, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.690, 10.347], loss: 0.085270, mae: 0.227236, mean_q: -0.213808
  5200/100000: episode: 52, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 57.816, mean reward: 0.578 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.169, 10.287], loss: 0.004414, mae: 0.065769, mean_q: 0.397668
  5300/100000: episode: 53, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.841, mean reward: 0.588 [0.498, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.632, 10.295], loss: 0.002392, mae: 0.053045, mean_q: 0.694326
  5400/100000: episode: 54, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.732, mean reward: 0.587 [0.507, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.850, 10.098], loss: 0.002277, mae: 0.052217, mean_q: 0.882484
  5500/100000: episode: 55, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 57.258, mean reward: 0.573 [0.497, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.644, 10.179], loss: 0.002444, mae: 0.051491, mean_q: 0.990765
  5600/100000: episode: 56, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.958, mean reward: 0.590 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.662, 10.156], loss: 0.002480, mae: 0.051818, mean_q: 1.060850
  5700/100000: episode: 57, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: 61.500, mean reward: 0.615 [0.509, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.599, 10.098], loss: 0.002630, mae: 0.051740, mean_q: 1.101830
  5800/100000: episode: 58, duration: 0.635s, episode steps: 100, steps per second: 158, episode reward: 58.799, mean reward: 0.588 [0.511, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.672, 10.154], loss: 0.002580, mae: 0.050961, mean_q: 1.129971
  5900/100000: episode: 59, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: 57.708, mean reward: 0.577 [0.505, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.298, 10.254], loss: 0.002393, mae: 0.049908, mean_q: 1.144296
  6000/100000: episode: 60, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.043, mean reward: 0.580 [0.499, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.906, 10.098], loss: 0.002865, mae: 0.053243, mean_q: 1.149331
  6100/100000: episode: 61, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.509, mean reward: 0.585 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.053, 10.098], loss: 0.002311, mae: 0.047762, mean_q: 1.159252
  6200/100000: episode: 62, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 65.853, mean reward: 0.659 [0.501, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.892, 10.098], loss: 0.002611, mae: 0.050931, mean_q: 1.162648
  6300/100000: episode: 63, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 63.030, mean reward: 0.630 [0.501, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.154, 10.392], loss: 0.002462, mae: 0.048680, mean_q: 1.162482
  6400/100000: episode: 64, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 61.059, mean reward: 0.611 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.457, 10.098], loss: 0.002592, mae: 0.051914, mean_q: 1.169497
  6500/100000: episode: 65, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.941, mean reward: 0.599 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.744, 10.308], loss: 0.002432, mae: 0.049963, mean_q: 1.170166
  6600/100000: episode: 66, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.112, mean reward: 0.581 [0.503, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.966, 10.245], loss: 0.002474, mae: 0.049941, mean_q: 1.170858
  6700/100000: episode: 67, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 61.469, mean reward: 0.615 [0.521, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.149, 10.098], loss: 0.002481, mae: 0.050923, mean_q: 1.172662
  6800/100000: episode: 68, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.477, mean reward: 0.595 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.517, 10.098], loss: 0.002630, mae: 0.051688, mean_q: 1.171006
  6900/100000: episode: 69, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.527, mean reward: 0.595 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.376, 10.098], loss: 0.002514, mae: 0.049846, mean_q: 1.171514
  7000/100000: episode: 70, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 60.571, mean reward: 0.606 [0.499, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.563, 10.105], loss: 0.002675, mae: 0.052801, mean_q: 1.170709
  7100/100000: episode: 71, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 60.301, mean reward: 0.603 [0.510, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.758, 10.098], loss: 0.002582, mae: 0.052057, mean_q: 1.175789
  7200/100000: episode: 72, duration: 0.601s, episode steps: 100, steps per second: 167, episode reward: 57.335, mean reward: 0.573 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.183, 10.173], loss: 0.002464, mae: 0.050149, mean_q: 1.173158
  7300/100000: episode: 73, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.751, mean reward: 0.588 [0.503, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.872, 10.099], loss: 0.002634, mae: 0.052166, mean_q: 1.171922
  7400/100000: episode: 74, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 58.061, mean reward: 0.581 [0.509, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.232, 10.221], loss: 0.002436, mae: 0.049936, mean_q: 1.172789
  7500/100000: episode: 75, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.548, mean reward: 0.585 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.227, 10.184], loss: 0.002368, mae: 0.049555, mean_q: 1.174677
  7600/100000: episode: 76, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 56.762, mean reward: 0.568 [0.502, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.132, 10.184], loss: 0.002328, mae: 0.049709, mean_q: 1.176053
  7700/100000: episode: 77, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.817, mean reward: 0.598 [0.520, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.530, 10.102], loss: 0.002454, mae: 0.051231, mean_q: 1.175580
  7800/100000: episode: 78, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.295, mean reward: 0.573 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.338, 10.178], loss: 0.002590, mae: 0.052870, mean_q: 1.173704
  7900/100000: episode: 79, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.582, mean reward: 0.586 [0.509, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.065, 10.098], loss: 0.002377, mae: 0.049519, mean_q: 1.173805
  8000/100000: episode: 80, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.783, mean reward: 0.588 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.363, 10.299], loss: 0.002792, mae: 0.054226, mean_q: 1.172929
  8100/100000: episode: 81, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 61.270, mean reward: 0.613 [0.499, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.725, 10.138], loss: 0.002478, mae: 0.051296, mean_q: 1.171831
  8200/100000: episode: 82, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 56.949, mean reward: 0.569 [0.505, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.523, 10.204], loss: 0.002677, mae: 0.052044, mean_q: 1.171431
  8300/100000: episode: 83, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 57.857, mean reward: 0.579 [0.503, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.753, 10.149], loss: 0.002460, mae: 0.051128, mean_q: 1.175560
  8400/100000: episode: 84, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.101, mean reward: 0.581 [0.500, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.898, 10.098], loss: 0.002389, mae: 0.050156, mean_q: 1.173772
  8500/100000: episode: 85, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 59.252, mean reward: 0.593 [0.504, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.012, 10.105], loss: 0.002496, mae: 0.051621, mean_q: 1.168043
  8600/100000: episode: 86, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 60.657, mean reward: 0.607 [0.508, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.575, 10.098], loss: 0.002367, mae: 0.051035, mean_q: 1.170695
  8700/100000: episode: 87, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.023, mean reward: 0.580 [0.498, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.773, 10.098], loss: 0.002341, mae: 0.050381, mean_q: 1.171657
  8800/100000: episode: 88, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.763, mean reward: 0.588 [0.507, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.813, 10.297], loss: 0.002280, mae: 0.050253, mean_q: 1.174776
  8900/100000: episode: 89, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 64.608, mean reward: 0.646 [0.520, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.968, 10.253], loss: 0.002400, mae: 0.050476, mean_q: 1.171105
  9000/100000: episode: 90, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.217, mean reward: 0.602 [0.509, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.281, 10.364], loss: 0.003022, mae: 0.057353, mean_q: 1.171040
  9100/100000: episode: 91, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 57.593, mean reward: 0.576 [0.506, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.234, 10.098], loss: 0.002327, mae: 0.050651, mean_q: 1.173305
  9200/100000: episode: 92, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 60.719, mean reward: 0.607 [0.501, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.810, 10.132], loss: 0.002426, mae: 0.050624, mean_q: 1.170446
  9300/100000: episode: 93, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 57.980, mean reward: 0.580 [0.514, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.162, 10.171], loss: 0.002522, mae: 0.053012, mean_q: 1.169753
  9400/100000: episode: 94, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 60.204, mean reward: 0.602 [0.510, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.056, 10.098], loss: 0.002566, mae: 0.053152, mean_q: 1.173411
  9500/100000: episode: 95, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 61.947, mean reward: 0.619 [0.516, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.636, 10.215], loss: 0.002247, mae: 0.050179, mean_q: 1.173277
  9600/100000: episode: 96, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.430, mean reward: 0.574 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.180, 10.160], loss: 0.002557, mae: 0.053581, mean_q: 1.172979
  9700/100000: episode: 97, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 58.206, mean reward: 0.582 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.869, 10.098], loss: 0.002419, mae: 0.051291, mean_q: 1.173595
  9800/100000: episode: 98, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.404, mean reward: 0.594 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.244, 10.170], loss: 0.002529, mae: 0.053624, mean_q: 1.172284
  9900/100000: episode: 99, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.717, mean reward: 0.587 [0.502, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.702, 10.098], loss: 0.002192, mae: 0.050395, mean_q: 1.172153
 10000/100000: episode: 100, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 60.846, mean reward: 0.608 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.250, 10.363], loss: 0.002209, mae: 0.050036, mean_q: 1.171324
 10100/100000: episode: 101, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.054, mean reward: 0.601 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.625, 10.098], loss: 0.002339, mae: 0.051284, mean_q: 1.172139
 10200/100000: episode: 102, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.646, mean reward: 0.576 [0.507, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.157, 10.098], loss: 0.002343, mae: 0.052317, mean_q: 1.176485
 10300/100000: episode: 103, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 57.913, mean reward: 0.579 [0.499, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.731, 10.122], loss: 0.002112, mae: 0.048737, mean_q: 1.172198
 10400/100000: episode: 104, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.332, mean reward: 0.603 [0.507, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.754, 10.098], loss: 0.002012, mae: 0.048539, mean_q: 1.174925
 10500/100000: episode: 105, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 58.066, mean reward: 0.581 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.495, 10.098], loss: 0.002288, mae: 0.051449, mean_q: 1.173020
 10600/100000: episode: 106, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.181, mean reward: 0.572 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.272, 10.279], loss: 0.002215, mae: 0.050347, mean_q: 1.175862
 10700/100000: episode: 107, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.473, mean reward: 0.585 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.384, 10.098], loss: 0.002111, mae: 0.049957, mean_q: 1.178745
 10800/100000: episode: 108, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.945, mean reward: 0.589 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.775, 10.098], loss: 0.002126, mae: 0.049810, mean_q: 1.175919
 10900/100000: episode: 109, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.413, mean reward: 0.574 [0.501, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.980, 10.098], loss: 0.002046, mae: 0.049393, mean_q: 1.174198
 11000/100000: episode: 110, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.558, mean reward: 0.586 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.064, 10.098], loss: 0.002097, mae: 0.049465, mean_q: 1.175892
 11100/100000: episode: 111, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.751, mean reward: 0.578 [0.510, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.162, 10.098], loss: 0.002021, mae: 0.049685, mean_q: 1.177108
 11200/100000: episode: 112, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.802, mean reward: 0.568 [0.501, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.286, 10.349], loss: 0.002067, mae: 0.049304, mean_q: 1.173837
 11300/100000: episode: 113, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 63.749, mean reward: 0.637 [0.503, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.408 [-0.906, 10.098], loss: 0.001971, mae: 0.048918, mean_q: 1.168695
 11400/100000: episode: 114, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.262, mean reward: 0.603 [0.515, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.762, 10.218], loss: 0.001933, mae: 0.048328, mean_q: 1.166556
 11500/100000: episode: 115, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.482, mean reward: 0.585 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.541, 10.175], loss: 0.001961, mae: 0.048245, mean_q: 1.171893
 11600/100000: episode: 116, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.517, mean reward: 0.585 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.831, 10.098], loss: 0.002295, mae: 0.052171, mean_q: 1.172481
 11700/100000: episode: 117, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.188, mean reward: 0.592 [0.501, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.557, 10.098], loss: 0.001857, mae: 0.046939, mean_q: 1.170358
 11800/100000: episode: 118, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 56.049, mean reward: 0.560 [0.501, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.854, 10.098], loss: 0.001998, mae: 0.048600, mean_q: 1.164538
 11900/100000: episode: 119, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 58.280, mean reward: 0.583 [0.507, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.505, 10.120], loss: 0.002001, mae: 0.048853, mean_q: 1.168249
 12000/100000: episode: 120, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 62.056, mean reward: 0.621 [0.507, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.524, 10.098], loss: 0.002266, mae: 0.051843, mean_q: 1.163649
 12100/100000: episode: 121, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 58.789, mean reward: 0.588 [0.515, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.696, 10.098], loss: 0.002509, mae: 0.053892, mean_q: 1.168291
 12200/100000: episode: 122, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: 58.942, mean reward: 0.589 [0.500, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.215, 10.320], loss: 0.001898, mae: 0.047952, mean_q: 1.165145
 12300/100000: episode: 123, duration: 0.882s, episode steps: 100, steps per second: 113, episode reward: 58.219, mean reward: 0.582 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.307, 10.123], loss: 0.002025, mae: 0.049505, mean_q: 1.168384
 12400/100000: episode: 124, duration: 0.986s, episode steps: 100, steps per second: 101, episode reward: 57.671, mean reward: 0.577 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.865, 10.226], loss: 0.002013, mae: 0.049181, mean_q: 1.169532
 12500/100000: episode: 125, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: 57.675, mean reward: 0.577 [0.502, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.821, 10.164], loss: 0.001842, mae: 0.047073, mean_q: 1.167620
 12600/100000: episode: 126, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 56.644, mean reward: 0.566 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.616, 10.127], loss: 0.001806, mae: 0.046728, mean_q: 1.168045
 12700/100000: episode: 127, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.727, mean reward: 0.587 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.983, 10.327], loss: 0.001940, mae: 0.048225, mean_q: 1.166234
 12800/100000: episode: 128, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 61.590, mean reward: 0.616 [0.499, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.830, 10.221], loss: 0.001903, mae: 0.048104, mean_q: 1.165609
 12900/100000: episode: 129, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.272, mean reward: 0.573 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.839, 10.098], loss: 0.001771, mae: 0.046521, mean_q: 1.162852
 13000/100000: episode: 130, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.921, mean reward: 0.599 [0.513, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.380, 10.210], loss: 0.002030, mae: 0.049260, mean_q: 1.165102
 13100/100000: episode: 131, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.633, mean reward: 0.576 [0.502, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.382, 10.207], loss: 0.001993, mae: 0.048794, mean_q: 1.164811
 13200/100000: episode: 132, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 67.366, mean reward: 0.674 [0.509, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.772, 10.306], loss: 0.002047, mae: 0.049460, mean_q: 1.169014
 13300/100000: episode: 133, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.282, mean reward: 0.573 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.439, 10.126], loss: 0.001822, mae: 0.046993, mean_q: 1.169664
 13400/100000: episode: 134, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.051, mean reward: 0.581 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.801, 10.098], loss: 0.001841, mae: 0.047341, mean_q: 1.169876
 13500/100000: episode: 135, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 61.448, mean reward: 0.614 [0.523, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.855, 10.098], loss: 0.001839, mae: 0.047213, mean_q: 1.170515
 13600/100000: episode: 136, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 60.645, mean reward: 0.606 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.924, 10.105], loss: 0.001835, mae: 0.047579, mean_q: 1.171041
 13700/100000: episode: 137, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.268, mean reward: 0.573 [0.499, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.943, 10.192], loss: 0.001875, mae: 0.047981, mean_q: 1.170354
 13800/100000: episode: 138, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.678, mean reward: 0.577 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.385, 10.098], loss: 0.001733, mae: 0.046373, mean_q: 1.170953
 13900/100000: episode: 139, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 61.040, mean reward: 0.610 [0.513, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.548, 10.125], loss: 0.001994, mae: 0.049096, mean_q: 1.166099
 14000/100000: episode: 140, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 61.122, mean reward: 0.611 [0.514, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.793, 10.386], loss: 0.002026, mae: 0.049546, mean_q: 1.167648
 14100/100000: episode: 141, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.893, mean reward: 0.589 [0.504, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.372, 10.335], loss: 0.001886, mae: 0.047603, mean_q: 1.168288
 14200/100000: episode: 142, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.035, mean reward: 0.590 [0.500, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.689, 10.216], loss: 0.002022, mae: 0.049576, mean_q: 1.168909
 14300/100000: episode: 143, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 56.886, mean reward: 0.569 [0.500, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.788, 10.098], loss: 0.001795, mae: 0.046821, mean_q: 1.171185
 14400/100000: episode: 144, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.140, mean reward: 0.581 [0.506, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.152, 10.179], loss: 0.001761, mae: 0.046799, mean_q: 1.168894
 14500/100000: episode: 145, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 57.880, mean reward: 0.579 [0.515, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.890, 10.098], loss: 0.001757, mae: 0.045776, mean_q: 1.166093
 14600/100000: episode: 146, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.534, mean reward: 0.595 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.788, 10.098], loss: 0.001776, mae: 0.046482, mean_q: 1.167863
 14700/100000: episode: 147, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.240, mean reward: 0.572 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.117, 10.284], loss: 0.001840, mae: 0.046775, mean_q: 1.168637
 14800/100000: episode: 148, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.908, mean reward: 0.579 [0.511, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.580, 10.098], loss: 0.001814, mae: 0.046538, mean_q: 1.166276
 14900/100000: episode: 149, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.822, mean reward: 0.578 [0.507, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.466, 10.098], loss: 0.001581, mae: 0.043749, mean_q: 1.166091
[Info] 1-TH LEVEL FOUND: 1.3717433214187622, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.116s, episode steps: 100, steps per second: 20, episode reward: 58.347, mean reward: 0.583 [0.509, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.700, 10.098], loss: 0.001770, mae: 0.046122, mean_q: 1.166415
 15041/100000: episode: 151, duration: 0.346s, episode steps: 41, steps per second: 119, episode reward: 26.734, mean reward: 0.652 [0.572, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.338, 10.316], loss: 0.001861, mae: 0.047545, mean_q: 1.164680
 15071/100000: episode: 152, duration: 0.211s, episode steps: 30, steps per second: 142, episode reward: 19.195, mean reward: 0.640 [0.574, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.199, 10.345], loss: 0.001935, mae: 0.047942, mean_q: 1.162964
 15101/100000: episode: 153, duration: 0.149s, episode steps: 30, steps per second: 202, episode reward: 19.673, mean reward: 0.656 [0.543, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.249, 10.274], loss: 0.001754, mae: 0.046511, mean_q: 1.168107
 15139/100000: episode: 154, duration: 0.230s, episode steps: 38, steps per second: 165, episode reward: 23.721, mean reward: 0.624 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.702, 10.100], loss: 0.001877, mae: 0.046483, mean_q: 1.168532
 15176/100000: episode: 155, duration: 0.314s, episode steps: 37, steps per second: 118, episode reward: 23.734, mean reward: 0.641 [0.537, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.292, 10.258], loss: 0.001750, mae: 0.045312, mean_q: 1.169333
 15188/100000: episode: 156, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 9.453, mean reward: 0.788 [0.742, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.598, 10.100], loss: 0.001560, mae: 0.044280, mean_q: 1.172433
 15226/100000: episode: 157, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 23.437, mean reward: 0.617 [0.515, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.481, 10.231], loss: 0.001737, mae: 0.045826, mean_q: 1.166284
 15271/100000: episode: 158, duration: 0.227s, episode steps: 45, steps per second: 198, episode reward: 33.453, mean reward: 0.743 [0.525, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.719, 10.212], loss: 0.002103, mae: 0.050878, mean_q: 1.171991
 15309/100000: episode: 159, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 22.091, mean reward: 0.581 [0.511, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.035, 10.101], loss: 0.001949, mae: 0.048602, mean_q: 1.174033
 15347/100000: episode: 160, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 22.998, mean reward: 0.605 [0.544, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.570, 10.268], loss: 0.001957, mae: 0.049031, mean_q: 1.173310
 15381/100000: episode: 161, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 21.359, mean reward: 0.628 [0.548, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.666, 10.194], loss: 0.002024, mae: 0.048613, mean_q: 1.169672
 15423/100000: episode: 162, duration: 0.234s, episode steps: 42, steps per second: 179, episode reward: 27.111, mean reward: 0.645 [0.543, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-1.210, 10.269], loss: 0.002002, mae: 0.048833, mean_q: 1.172873
 15440/100000: episode: 163, duration: 0.084s, episode steps: 17, steps per second: 204, episode reward: 11.970, mean reward: 0.704 [0.630, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.240, 10.100], loss: 0.001899, mae: 0.048346, mean_q: 1.168419
 15461/100000: episode: 164, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 15.612, mean reward: 0.743 [0.642, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.228, 10.100], loss: 0.002141, mae: 0.050761, mean_q: 1.170971
 15482/100000: episode: 165, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 16.587, mean reward: 0.790 [0.705, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.834, 10.100], loss: 0.001976, mae: 0.048800, mean_q: 1.170043
 15523/100000: episode: 166, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 25.759, mean reward: 0.628 [0.563, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.170, 10.240], loss: 0.002203, mae: 0.050486, mean_q: 1.176438
 15557/100000: episode: 167, duration: 0.204s, episode steps: 34, steps per second: 166, episode reward: 20.502, mean reward: 0.603 [0.508, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.480, 10.100], loss: 0.002064, mae: 0.048890, mean_q: 1.173419
 15587/100000: episode: 168, duration: 0.179s, episode steps: 30, steps per second: 168, episode reward: 20.201, mean reward: 0.673 [0.593, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.203, 10.426], loss: 0.002128, mae: 0.049172, mean_q: 1.170688
 15632/100000: episode: 169, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 28.127, mean reward: 0.625 [0.541, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-1.606, 10.139], loss: 0.002026, mae: 0.048876, mean_q: 1.175919
 15669/100000: episode: 170, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 23.768, mean reward: 0.642 [0.540, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.151, 10.296], loss: 0.001994, mae: 0.047787, mean_q: 1.168637
 15681/100000: episode: 171, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 9.929, mean reward: 0.827 [0.743, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.294, 10.100], loss: 0.002119, mae: 0.049967, mean_q: 1.179399
 15711/100000: episode: 172, duration: 0.178s, episode steps: 30, steps per second: 169, episode reward: 21.761, mean reward: 0.725 [0.673, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.624, 10.538], loss: 0.001910, mae: 0.047001, mean_q: 1.178443
 15728/100000: episode: 173, duration: 0.115s, episode steps: 17, steps per second: 147, episode reward: 13.013, mean reward: 0.765 [0.693, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.247, 10.100], loss: 0.002720, mae: 0.054962, mean_q: 1.183254
 15758/100000: episode: 174, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 20.414, mean reward: 0.680 [0.562, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.035, 10.234], loss: 0.002239, mae: 0.051089, mean_q: 1.181601
 15796/100000: episode: 175, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 23.619, mean reward: 0.622 [0.503, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.597, 10.100], loss: 0.001992, mae: 0.048727, mean_q: 1.178896
 15838/100000: episode: 176, duration: 0.246s, episode steps: 42, steps per second: 171, episode reward: 25.943, mean reward: 0.618 [0.501, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.119, 10.104], loss: 0.002047, mae: 0.050409, mean_q: 1.190695
 15876/100000: episode: 177, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 25.064, mean reward: 0.660 [0.592, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.924, 10.469], loss: 0.002196, mae: 0.050223, mean_q: 1.183348
 15917/100000: episode: 178, duration: 0.213s, episode steps: 41, steps per second: 192, episode reward: 25.725, mean reward: 0.627 [0.517, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.286, 10.105], loss: 0.002252, mae: 0.051099, mean_q: 1.180547
 15951/100000: episode: 179, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 23.274, mean reward: 0.685 [0.557, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.468, 10.283], loss: 0.002379, mae: 0.053486, mean_q: 1.185248
 15993/100000: episode: 180, duration: 0.223s, episode steps: 42, steps per second: 189, episode reward: 26.385, mean reward: 0.628 [0.561, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.285, 10.253], loss: 0.002027, mae: 0.048483, mean_q: 1.188424
 16038/100000: episode: 181, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 32.647, mean reward: 0.725 [0.638, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-1.050, 10.485], loss: 0.002186, mae: 0.050745, mean_q: 1.187839
 16059/100000: episode: 182, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 14.987, mean reward: 0.714 [0.603, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.270, 10.100], loss: 0.002141, mae: 0.050626, mean_q: 1.185205
 16104/100000: episode: 183, duration: 0.250s, episode steps: 45, steps per second: 180, episode reward: 29.168, mean reward: 0.648 [0.555, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.299, 10.367], loss: 0.001822, mae: 0.045754, mean_q: 1.190161
 16142/100000: episode: 184, duration: 0.195s, episode steps: 38, steps per second: 195, episode reward: 25.827, mean reward: 0.680 [0.600, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.564, 10.344], loss: 0.001964, mae: 0.048617, mean_q: 1.194747
 16184/100000: episode: 185, duration: 0.235s, episode steps: 42, steps per second: 178, episode reward: 26.666, mean reward: 0.635 [0.540, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.217, 10.169], loss: 0.002101, mae: 0.049943, mean_q: 1.191210
 16221/100000: episode: 186, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 22.433, mean reward: 0.606 [0.505, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-1.147, 10.202], loss: 0.002530, mae: 0.054662, mean_q: 1.194151
 16266/100000: episode: 187, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 30.976, mean reward: 0.688 [0.623, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.391, 10.405], loss: 0.002457, mae: 0.053680, mean_q: 1.193472
 16311/100000: episode: 188, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 29.899, mean reward: 0.664 [0.574, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.298, 10.340], loss: 0.002051, mae: 0.048518, mean_q: 1.189095
 16352/100000: episode: 189, duration: 0.209s, episode steps: 41, steps per second: 196, episode reward: 25.559, mean reward: 0.623 [0.521, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.604, 10.254], loss: 0.002189, mae: 0.051032, mean_q: 1.190136
 16393/100000: episode: 190, duration: 0.220s, episode steps: 41, steps per second: 187, episode reward: 27.200, mean reward: 0.663 [0.566, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.011, 10.204], loss: 0.002171, mae: 0.049480, mean_q: 1.188539
 16414/100000: episode: 191, duration: 0.128s, episode steps: 21, steps per second: 163, episode reward: 16.255, mean reward: 0.774 [0.731, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.393, 10.100], loss: 0.002282, mae: 0.052142, mean_q: 1.201386
 16451/100000: episode: 192, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 24.351, mean reward: 0.658 [0.586, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.371, 10.310], loss: 0.002307, mae: 0.051839, mean_q: 1.196961
 16485/100000: episode: 193, duration: 0.184s, episode steps: 34, steps per second: 184, episode reward: 23.382, mean reward: 0.688 [0.536, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.138, 10.274], loss: 0.002079, mae: 0.048826, mean_q: 1.196512
 16506/100000: episode: 194, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 16.849, mean reward: 0.802 [0.704, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.308, 10.100], loss: 0.002244, mae: 0.051692, mean_q: 1.206086
 16536/100000: episode: 195, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 19.569, mean reward: 0.652 [0.526, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.226, 10.130], loss: 0.002133, mae: 0.049207, mean_q: 1.196445
 16573/100000: episode: 196, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 23.822, mean reward: 0.644 [0.587, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.468, 10.374], loss: 0.001911, mae: 0.047314, mean_q: 1.199983
 16585/100000: episode: 197, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 9.529, mean reward: 0.794 [0.753, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.747, 10.100], loss: 0.002036, mae: 0.049417, mean_q: 1.215474
 16597/100000: episode: 198, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 9.101, mean reward: 0.758 [0.717, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.210, 10.100], loss: 0.002218, mae: 0.049634, mean_q: 1.190389
 16638/100000: episode: 199, duration: 0.215s, episode steps: 41, steps per second: 190, episode reward: 25.651, mean reward: 0.626 [0.546, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.469, 10.255], loss: 0.002005, mae: 0.048710, mean_q: 1.205708
 16679/100000: episode: 200, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 24.863, mean reward: 0.606 [0.505, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.085, 10.100], loss: 0.001978, mae: 0.048428, mean_q: 1.195648
 16716/100000: episode: 201, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 23.901, mean reward: 0.646 [0.563, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.867, 10.225], loss: 0.001962, mae: 0.046588, mean_q: 1.204952
 16753/100000: episode: 202, duration: 0.194s, episode steps: 37, steps per second: 190, episode reward: 24.709, mean reward: 0.668 [0.577, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.997, 10.389], loss: 0.001947, mae: 0.047695, mean_q: 1.202489
 16770/100000: episode: 203, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 14.031, mean reward: 0.825 [0.771, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.918, 10.100], loss: 0.001999, mae: 0.049746, mean_q: 1.216029
 16791/100000: episode: 204, duration: 0.118s, episode steps: 21, steps per second: 177, episode reward: 16.463, mean reward: 0.784 [0.739, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.495, 10.100], loss: 0.002158, mae: 0.049984, mean_q: 1.211780
 16828/100000: episode: 205, duration: 0.187s, episode steps: 37, steps per second: 197, episode reward: 25.063, mean reward: 0.677 [0.602, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.035, 10.405], loss: 0.002106, mae: 0.049541, mean_q: 1.206066
 16840/100000: episode: 206, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 9.748, mean reward: 0.812 [0.751, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.491, 10.100], loss: 0.001921, mae: 0.048083, mean_q: 1.213489
 16861/100000: episode: 207, duration: 0.131s, episode steps: 21, steps per second: 160, episode reward: 15.729, mean reward: 0.749 [0.650, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.165, 10.100], loss: 0.002034, mae: 0.049782, mean_q: 1.210443
 16878/100000: episode: 208, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 13.092, mean reward: 0.770 [0.664, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.543, 10.100], loss: 0.002488, mae: 0.052378, mean_q: 1.198296
 16916/100000: episode: 209, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 24.447, mean reward: 0.643 [0.549, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.612, 10.346], loss: 0.002107, mae: 0.050718, mean_q: 1.206793
 16937/100000: episode: 210, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 16.605, mean reward: 0.791 [0.698, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.341, 10.100], loss: 0.002256, mae: 0.050134, mean_q: 1.208841
 16975/100000: episode: 211, duration: 0.214s, episode steps: 38, steps per second: 178, episode reward: 23.717, mean reward: 0.624 [0.538, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.429, 10.167], loss: 0.002182, mae: 0.050624, mean_q: 1.214235
 16992/100000: episode: 212, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 13.395, mean reward: 0.788 [0.662, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.426, 10.100], loss: 0.002189, mae: 0.050521, mean_q: 1.216304
 17009/100000: episode: 213, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 13.988, mean reward: 0.823 [0.773, 0.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.417, 10.100], loss: 0.002273, mae: 0.051264, mean_q: 1.207934
 17051/100000: episode: 214, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 26.524, mean reward: 0.632 [0.546, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.337, 10.226], loss: 0.002727, mae: 0.055435, mean_q: 1.215416
 17072/100000: episode: 215, duration: 0.104s, episode steps: 21, steps per second: 201, episode reward: 16.628, mean reward: 0.792 [0.667, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.637, 10.100], loss: 0.002039, mae: 0.047991, mean_q: 1.226313
 17113/100000: episode: 216, duration: 0.222s, episode steps: 41, steps per second: 184, episode reward: 27.791, mean reward: 0.678 [0.594, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.333, 10.481], loss: 0.002158, mae: 0.049081, mean_q: 1.214951
 17147/100000: episode: 217, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 20.618, mean reward: 0.606 [0.515, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.239, 10.159], loss: 0.002111, mae: 0.049560, mean_q: 1.215576
 17192/100000: episode: 218, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 31.652, mean reward: 0.703 [0.553, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.315, 10.300], loss: 0.002467, mae: 0.053839, mean_q: 1.219839
 17204/100000: episode: 219, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 8.728, mean reward: 0.727 [0.614, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.204, 10.100], loss: 0.002421, mae: 0.051584, mean_q: 1.225327
 17221/100000: episode: 220, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 11.867, mean reward: 0.698 [0.642, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.306, 10.100], loss: 0.002285, mae: 0.051936, mean_q: 1.223395
 17263/100000: episode: 221, duration: 0.242s, episode steps: 42, steps per second: 174, episode reward: 27.229, mean reward: 0.648 [0.528, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.923, 10.214], loss: 0.002159, mae: 0.049676, mean_q: 1.218651
 17284/100000: episode: 222, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 16.864, mean reward: 0.803 [0.711, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.259, 10.100], loss: 0.002328, mae: 0.053296, mean_q: 1.235934
 17321/100000: episode: 223, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 22.194, mean reward: 0.600 [0.514, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.261, 10.151], loss: 0.002139, mae: 0.050019, mean_q: 1.227549
 17359/100000: episode: 224, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 23.165, mean reward: 0.610 [0.549, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.035, 10.250], loss: 0.002015, mae: 0.048029, mean_q: 1.231281
 17380/100000: episode: 225, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 15.840, mean reward: 0.754 [0.665, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.262, 10.100], loss: 0.002257, mae: 0.050271, mean_q: 1.215928
 17414/100000: episode: 226, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 22.107, mean reward: 0.650 [0.548, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.043, 10.295], loss: 0.001994, mae: 0.047956, mean_q: 1.228200
 17452/100000: episode: 227, duration: 0.206s, episode steps: 38, steps per second: 184, episode reward: 24.160, mean reward: 0.636 [0.551, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.660, 10.276], loss: 0.002170, mae: 0.049041, mean_q: 1.227814
 17469/100000: episode: 228, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 13.431, mean reward: 0.790 [0.744, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.318, 10.100], loss: 0.001936, mae: 0.047655, mean_q: 1.238442
 17514/100000: episode: 229, duration: 0.215s, episode steps: 45, steps per second: 209, episode reward: 37.091, mean reward: 0.824 [0.769, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.593, 10.689], loss: 0.002173, mae: 0.050376, mean_q: 1.239946
 17531/100000: episode: 230, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 13.221, mean reward: 0.778 [0.676, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.290, 10.100], loss: 0.002596, mae: 0.055368, mean_q: 1.233786
 17576/100000: episode: 231, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 34.151, mean reward: 0.759 [0.606, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.296, 10.492], loss: 0.002140, mae: 0.049570, mean_q: 1.241391
 17597/100000: episode: 232, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 17.350, mean reward: 0.826 [0.754, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.337, 10.100], loss: 0.002070, mae: 0.047188, mean_q: 1.237851
 17638/100000: episode: 233, duration: 0.238s, episode steps: 41, steps per second: 173, episode reward: 26.616, mean reward: 0.649 [0.519, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.657, 10.199], loss: 0.001985, mae: 0.047048, mean_q: 1.240907
 17675/100000: episode: 234, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 22.798, mean reward: 0.616 [0.519, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.471, 10.144], loss: 0.002273, mae: 0.050804, mean_q: 1.237879
 17696/100000: episode: 235, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 14.971, mean reward: 0.713 [0.660, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.703, 10.100], loss: 0.002105, mae: 0.049314, mean_q: 1.247974
 17737/100000: episode: 236, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 28.136, mean reward: 0.686 [0.615, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.197, 10.383], loss: 0.002121, mae: 0.048838, mean_q: 1.239277
 17774/100000: episode: 237, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 22.112, mean reward: 0.598 [0.528, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.704, 10.266], loss: 0.002234, mae: 0.049708, mean_q: 1.235887
 17811/100000: episode: 238, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 21.696, mean reward: 0.586 [0.511, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.773, 10.100], loss: 0.002237, mae: 0.050230, mean_q: 1.245458
 17848/100000: episode: 239, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 25.988, mean reward: 0.702 [0.624, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.241, 10.524], loss: 0.002635, mae: 0.055757, mean_q: 1.253832
[Info] 2-TH LEVEL FOUND: 1.64200758934021, Considering 10/90 traces
 17882/100000: episode: 240, duration: 4.271s, episode steps: 34, steps per second: 8, episode reward: 20.873, mean reward: 0.614 [0.528, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.508, 10.105], loss: 0.002266, mae: 0.051123, mean_q: 1.247030
 17888/100000: episode: 241, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 4.792, mean reward: 0.799 [0.745, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.255, 10.100], loss: 0.002522, mae: 0.052529, mean_q: 1.252479
 17900/100000: episode: 242, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 9.546, mean reward: 0.795 [0.739, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.472, 10.100], loss: 0.002375, mae: 0.051664, mean_q: 1.232604
 17915/100000: episode: 243, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 11.259, mean reward: 0.751 [0.618, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.174, 10.100], loss: 0.001946, mae: 0.047872, mean_q: 1.239513
 17922/100000: episode: 244, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 5.275, mean reward: 0.754 [0.717, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.400, 10.100], loss: 0.002434, mae: 0.054341, mean_q: 1.245168
 17931/100000: episode: 245, duration: 0.068s, episode steps: 9, steps per second: 132, episode reward: 7.646, mean reward: 0.850 [0.771, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.441, 10.100], loss: 0.002362, mae: 0.049443, mean_q: 1.220824
 17946/100000: episode: 246, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 11.857, mean reward: 0.790 [0.687, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.227, 10.100], loss: 0.002175, mae: 0.051039, mean_q: 1.250182
 17961/100000: episode: 247, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 10.858, mean reward: 0.724 [0.684, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.468, 10.100], loss: 0.002026, mae: 0.048276, mean_q: 1.247191
 17970/100000: episode: 248, duration: 0.050s, episode steps: 9, steps per second: 182, episode reward: 7.436, mean reward: 0.826 [0.793, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.917, 10.100], loss: 0.002287, mae: 0.049391, mean_q: 1.255347
 17977/100000: episode: 249, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 5.155, mean reward: 0.736 [0.693, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.375, 10.100], loss: 0.001639, mae: 0.043838, mean_q: 1.276053
 17983/100000: episode: 250, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 4.931, mean reward: 0.822 [0.798, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.408, 10.100], loss: 0.001860, mae: 0.046272, mean_q: 1.264561
 17998/100000: episode: 251, duration: 0.080s, episode steps: 15, steps per second: 186, episode reward: 11.897, mean reward: 0.793 [0.709, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.194, 10.100], loss: 0.002088, mae: 0.047935, mean_q: 1.261526
 18007/100000: episode: 252, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 7.278, mean reward: 0.809 [0.778, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.326, 10.100], loss: 0.002336, mae: 0.051864, mean_q: 1.245033
 18016/100000: episode: 253, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 7.522, mean reward: 0.836 [0.797, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.402, 10.100], loss: 0.001771, mae: 0.047317, mean_q: 1.267001
 18023/100000: episode: 254, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 5.443, mean reward: 0.778 [0.757, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.291, 10.100], loss: 0.001930, mae: 0.046587, mean_q: 1.220922
 18035/100000: episode: 255, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 9.988, mean reward: 0.832 [0.794, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.450, 10.100], loss: 0.002151, mae: 0.049201, mean_q: 1.264011
 18050/100000: episode: 256, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 11.871, mean reward: 0.791 [0.686, 0.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.365, 10.100], loss: 0.002058, mae: 0.048713, mean_q: 1.249723
 18065/100000: episode: 257, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 11.952, mean reward: 0.797 [0.760, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.420, 10.100], loss: 0.002310, mae: 0.050086, mean_q: 1.242584
 18071/100000: episode: 258, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 4.954, mean reward: 0.826 [0.791, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.377, 10.100], loss: 0.002210, mae: 0.052761, mean_q: 1.263053
 18087/100000: episode: 259, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 11.575, mean reward: 0.723 [0.645, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.261, 10.100], loss: 0.002631, mae: 0.055404, mean_q: 1.251433
 18102/100000: episode: 260, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 11.636, mean reward: 0.776 [0.648, 0.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.220, 10.100], loss: 0.001762, mae: 0.044172, mean_q: 1.246532
 18111/100000: episode: 261, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 7.642, mean reward: 0.849 [0.815, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.032, 10.100], loss: 0.002399, mae: 0.052714, mean_q: 1.253225
 18126/100000: episode: 262, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 11.280, mean reward: 0.752 [0.655, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.416, 10.100], loss: 0.002545, mae: 0.054473, mean_q: 1.235020
 18132/100000: episode: 263, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 4.758, mean reward: 0.793 [0.739, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.389, 10.100], loss: 0.002049, mae: 0.047838, mean_q: 1.240212
 18144/100000: episode: 264, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 9.388, mean reward: 0.782 [0.713, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.307, 10.100], loss: 0.002465, mae: 0.052904, mean_q: 1.267184
 18153/100000: episode: 265, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 7.747, mean reward: 0.861 [0.830, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.496, 10.100], loss: 0.002207, mae: 0.049579, mean_q: 1.259201
 18162/100000: episode: 266, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 7.837, mean reward: 0.871 [0.796, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.350, 10.100], loss: 0.002621, mae: 0.053695, mean_q: 1.262668
 18178/100000: episode: 267, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 12.467, mean reward: 0.779 [0.663, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.132, 10.100], loss: 0.002326, mae: 0.049703, mean_q: 1.262427
 18193/100000: episode: 268, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 12.055, mean reward: 0.804 [0.651, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.279, 10.100], loss: 0.002305, mae: 0.052756, mean_q: 1.257064
 18202/100000: episode: 269, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 7.050, mean reward: 0.783 [0.721, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.376, 10.100], loss: 0.003051, mae: 0.057448, mean_q: 1.242625
 18214/100000: episode: 270, duration: 0.073s, episode steps: 12, steps per second: 163, episode reward: 10.092, mean reward: 0.841 [0.789, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.507, 10.100], loss: 0.003066, mae: 0.057144, mean_q: 1.262790
 18223/100000: episode: 271, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 7.655, mean reward: 0.851 [0.791, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.583, 10.100], loss: 0.003301, mae: 0.063340, mean_q: 1.267915
 18238/100000: episode: 272, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 11.404, mean reward: 0.760 [0.706, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.549, 10.100], loss: 0.002965, mae: 0.060854, mean_q: 1.258883
 18254/100000: episode: 273, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 13.466, mean reward: 0.842 [0.782, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.973, 10.100], loss: 0.002835, mae: 0.057445, mean_q: 1.259378
 18269/100000: episode: 274, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 12.062, mean reward: 0.804 [0.719, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.469, 10.100], loss: 0.001840, mae: 0.045894, mean_q: 1.274146
 18281/100000: episode: 275, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 9.820, mean reward: 0.818 [0.705, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.253, 10.100], loss: 0.002008, mae: 0.049568, mean_q: 1.249821
 18296/100000: episode: 276, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 12.053, mean reward: 0.804 [0.727, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.817, 10.100], loss: 0.002571, mae: 0.053774, mean_q: 1.263524
[Info] FALSIFICATION!
 18301/100000: episode: 277, duration: 0.369s, episode steps: 5, steps per second: 14, episode reward: 4.519, mean reward: 0.904 [0.827, 1.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.071, 9.864], loss: 0.002246, mae: 0.050507, mean_q: 1.279984
 18317/100000: episode: 278, duration: 0.109s, episode steps: 16, steps per second: 147, episode reward: 12.417, mean reward: 0.776 [0.716, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.404, 10.100], loss: 0.003307, mae: 0.058600, mean_q: 1.282600
 18332/100000: episode: 279, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 11.172, mean reward: 0.745 [0.630, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.220, 10.100], loss: 0.002731, mae: 0.054023, mean_q: 1.262230
 18348/100000: episode: 280, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 13.154, mean reward: 0.822 [0.732, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.403, 10.100], loss: 0.001978, mae: 0.047347, mean_q: 1.272095
 18354/100000: episode: 281, duration: 0.034s, episode steps: 6, steps per second: 176, episode reward: 4.768, mean reward: 0.795 [0.713, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.238, 10.100], loss: 0.002151, mae: 0.050938, mean_q: 1.250047
 18361/100000: episode: 282, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 5.874, mean reward: 0.839 [0.795, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.454, 10.100], loss: 0.002239, mae: 0.048228, mean_q: 1.255210
 18370/100000: episode: 283, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 7.412, mean reward: 0.824 [0.773, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.341, 10.100], loss: 0.002207, mae: 0.049924, mean_q: 1.253996
 18385/100000: episode: 284, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 11.370, mean reward: 0.758 [0.683, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.385, 10.100], loss: 0.002281, mae: 0.052858, mean_q: 1.273900
 18401/100000: episode: 285, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 13.079, mean reward: 0.817 [0.710, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.316, 10.100], loss: 0.002454, mae: 0.052592, mean_q: 1.275183
 18413/100000: episode: 286, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 9.650, mean reward: 0.804 [0.758, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.929, 10.100], loss: 0.001949, mae: 0.046647, mean_q: 1.274972
 18422/100000: episode: 287, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 7.097, mean reward: 0.789 [0.750, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.558, 10.100], loss: 0.002124, mae: 0.048140, mean_q: 1.282555
 18437/100000: episode: 288, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 11.679, mean reward: 0.779 [0.724, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.377, 10.100], loss: 0.002411, mae: 0.052732, mean_q: 1.273161
 18446/100000: episode: 289, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 7.121, mean reward: 0.791 [0.752, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.349, 10.100], loss: 0.001905, mae: 0.047226, mean_q: 1.278534
 18455/100000: episode: 290, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 7.398, mean reward: 0.822 [0.727, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.207, 10.100], loss: 0.002099, mae: 0.048805, mean_q: 1.277038
 18461/100000: episode: 291, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 4.641, mean reward: 0.774 [0.749, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.285, 10.100], loss: 0.002116, mae: 0.049595, mean_q: 1.281976
 18476/100000: episode: 292, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 10.917, mean reward: 0.728 [0.615, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.253, 10.100], loss: 0.002207, mae: 0.050575, mean_q: 1.275208
 18482/100000: episode: 293, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 4.625, mean reward: 0.771 [0.713, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.275, 10.100], loss: 0.002516, mae: 0.053945, mean_q: 1.277569
 18489/100000: episode: 294, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 5.666, mean reward: 0.809 [0.773, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.439, 10.100], loss: 0.003020, mae: 0.057300, mean_q: 1.266951
 18501/100000: episode: 295, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 9.774, mean reward: 0.814 [0.741, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.268, 10.100], loss: 0.002749, mae: 0.056417, mean_q: 1.275437
 18517/100000: episode: 296, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 12.630, mean reward: 0.789 [0.734, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.439, 10.100], loss: 0.002841, mae: 0.052644, mean_q: 1.281474
 18529/100000: episode: 297, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 9.644, mean reward: 0.804 [0.709, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.458, 10.100], loss: 0.002535, mae: 0.055259, mean_q: 1.288833
 18544/100000: episode: 298, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 11.488, mean reward: 0.766 [0.686, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.918, 10.100], loss: 0.002456, mae: 0.051951, mean_q: 1.299738
 18550/100000: episode: 299, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 4.901, mean reward: 0.817 [0.789, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.348, 10.100], loss: 0.002393, mae: 0.052196, mean_q: 1.272602
 18565/100000: episode: 300, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 12.067, mean reward: 0.804 [0.698, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.257, 10.100], loss: 0.002181, mae: 0.051859, mean_q: 1.270767
 18574/100000: episode: 301, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 6.876, mean reward: 0.764 [0.686, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.402, 10.100], loss: 0.002855, mae: 0.055831, mean_q: 1.300779
 18589/100000: episode: 302, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 11.994, mean reward: 0.800 [0.658, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.557, 10.100], loss: 0.002549, mae: 0.050182, mean_q: 1.272120
 18596/100000: episode: 303, duration: 0.052s, episode steps: 7, steps per second: 135, episode reward: 5.427, mean reward: 0.775 [0.764, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.307, 10.100], loss: 0.003153, mae: 0.060691, mean_q: 1.305762
 18611/100000: episode: 304, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 11.433, mean reward: 0.762 [0.693, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.271, 10.100], loss: 0.002537, mae: 0.054558, mean_q: 1.278519
 18617/100000: episode: 305, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 4.762, mean reward: 0.794 [0.716, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.268, 10.100], loss: 0.002246, mae: 0.050967, mean_q: 1.277052
 18626/100000: episode: 306, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 7.747, mean reward: 0.861 [0.810, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.529, 10.100], loss: 0.002970, mae: 0.057772, mean_q: 1.280339
[Info] FALSIFICATION!
 18630/100000: episode: 307, duration: 0.279s, episode steps: 4, steps per second: 14, episode reward: 3.634, mean reward: 0.909 [0.838, 1.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.282, 10.046], loss: 0.002860, mae: 0.055274, mean_q: 1.249396
 18642/100000: episode: 308, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 9.273, mean reward: 0.773 [0.747, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.338, 10.100], loss: 0.002922, mae: 0.058736, mean_q: 1.282122
 18657/100000: episode: 309, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 12.279, mean reward: 0.819 [0.733, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.493, 10.100], loss: 0.003376, mae: 0.059934, mean_q: 1.319303
 18669/100000: episode: 310, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 10.164, mean reward: 0.847 [0.798, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.941, 10.100], loss: 0.002893, mae: 0.058420, mean_q: 1.285167
 18685/100000: episode: 311, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 14.101, mean reward: 0.881 [0.825, 0.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.473, 10.100], loss: 0.003006, mae: 0.058665, mean_q: 1.284895
 18700/100000: episode: 312, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 11.929, mean reward: 0.795 [0.750, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.297, 10.100], loss: 0.002439, mae: 0.051748, mean_q: 1.298493
 18712/100000: episode: 313, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 9.927, mean reward: 0.827 [0.771, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.012, 10.100], loss: 0.002629, mae: 0.054593, mean_q: 1.286572
 18727/100000: episode: 314, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 10.860, mean reward: 0.724 [0.664, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.362, 10.100], loss: 0.002514, mae: 0.053104, mean_q: 1.280794
 18742/100000: episode: 315, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 11.698, mean reward: 0.780 [0.736, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.450, 10.100], loss: 0.002116, mae: 0.048865, mean_q: 1.301433
 18757/100000: episode: 316, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 11.068, mean reward: 0.738 [0.667, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.457, 10.100], loss: 0.002371, mae: 0.052515, mean_q: 1.301868
 18766/100000: episode: 317, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 7.155, mean reward: 0.795 [0.743, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.409, 10.100], loss: 0.003629, mae: 0.064898, mean_q: 1.281985
 18781/100000: episode: 318, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 11.199, mean reward: 0.747 [0.682, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.187, 10.100], loss: 0.002873, mae: 0.055112, mean_q: 1.292996
 18793/100000: episode: 319, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 10.612, mean reward: 0.884 [0.821, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.591, 10.100], loss: 0.002267, mae: 0.050177, mean_q: 1.303239
 18802/100000: episode: 320, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 7.468, mean reward: 0.830 [0.767, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.316, 10.100], loss: 0.002925, mae: 0.054840, mean_q: 1.304963
 18811/100000: episode: 321, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 7.331, mean reward: 0.815 [0.778, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.415, 10.100], loss: 0.001960, mae: 0.048988, mean_q: 1.297184
 18817/100000: episode: 322, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 4.886, mean reward: 0.814 [0.780, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.366, 10.100], loss: 0.003006, mae: 0.057429, mean_q: 1.273891
 18832/100000: episode: 323, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 11.784, mean reward: 0.786 [0.689, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.580, 10.100], loss: 0.002180, mae: 0.051612, mean_q: 1.329442
 18847/100000: episode: 324, duration: 0.092s, episode steps: 15, steps per second: 164, episode reward: 11.975, mean reward: 0.798 [0.616, 0.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.267, 10.100], loss: 0.003129, mae: 0.058868, mean_q: 1.299108
 18856/100000: episode: 325, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 7.729, mean reward: 0.859 [0.820, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.322, 10.100], loss: 0.002301, mae: 0.049794, mean_q: 1.301720
 18871/100000: episode: 326, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 12.667, mean reward: 0.844 [0.765, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.937, 10.100], loss: 0.002438, mae: 0.054092, mean_q: 1.299718
 18877/100000: episode: 327, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 5.191, mean reward: 0.865 [0.819, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.411, 10.100], loss: 0.002109, mae: 0.048080, mean_q: 1.303564
 18884/100000: episode: 328, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 5.362, mean reward: 0.766 [0.704, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.354, 10.100], loss: 0.001708, mae: 0.044067, mean_q: 1.298331
 18900/100000: episode: 329, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 12.097, mean reward: 0.756 [0.658, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.369, 10.100], loss: 0.002878, mae: 0.053912, mean_q: 1.294652
[Info] Complete ISplit Iteration
[Info] Levels: [1.3717433, 1.6420076, 1.6330903]
[Info] Cond. Prob: [0.1, 0.1, 0.63]
[Info] Error Prob: 0.006300000000000001

 18909/100000: episode: 330, duration: 4.336s, episode steps: 9, steps per second: 2, episode reward: 7.362, mean reward: 0.818 [0.788, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.423, 10.100], loss: 0.001880, mae: 0.047241, mean_q: 1.302454
 19009/100000: episode: 331, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.001, mean reward: 0.580 [0.509, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.506, 10.287], loss: 0.002340, mae: 0.051366, mean_q: 1.305315
 19109/100000: episode: 332, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.674, mean reward: 0.597 [0.498, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.745, 10.098], loss: 0.002547, mae: 0.053037, mean_q: 1.303722
 19209/100000: episode: 333, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.685, mean reward: 0.597 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.192, 10.098], loss: 0.002711, mae: 0.054004, mean_q: 1.306030
 19309/100000: episode: 334, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.547, mean reward: 0.575 [0.503, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.838, 10.098], loss: 0.002455, mae: 0.052358, mean_q: 1.306822
 19409/100000: episode: 335, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.544, mean reward: 0.585 [0.512, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.628, 10.257], loss: 0.002267, mae: 0.051292, mean_q: 1.299161
 19509/100000: episode: 336, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.603, mean reward: 0.586 [0.502, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.571, 10.098], loss: 0.002414, mae: 0.051190, mean_q: 1.301046
 19609/100000: episode: 337, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.741, mean reward: 0.587 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.667, 10.118], loss: 0.002239, mae: 0.050463, mean_q: 1.301162
 19709/100000: episode: 338, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 62.735, mean reward: 0.627 [0.521, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.351, 10.098], loss: 0.002187, mae: 0.050002, mean_q: 1.303956
 19809/100000: episode: 339, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.972, mean reward: 0.590 [0.507, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.129, 10.098], loss: 0.002215, mae: 0.050259, mean_q: 1.311962
 19909/100000: episode: 340, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 63.790, mean reward: 0.638 [0.538, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.563, 10.360], loss: 0.002247, mae: 0.049742, mean_q: 1.309356
 20009/100000: episode: 341, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.968, mean reward: 0.610 [0.499, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.349, 10.098], loss: 0.002169, mae: 0.049881, mean_q: 1.314854
 20109/100000: episode: 342, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 56.079, mean reward: 0.561 [0.501, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.199, 10.165], loss: 0.002130, mae: 0.048943, mean_q: 1.300522
 20209/100000: episode: 343, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.598, mean reward: 0.606 [0.514, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.060, 10.254], loss: 0.002196, mae: 0.050500, mean_q: 1.306393
 20309/100000: episode: 344, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.643, mean reward: 0.576 [0.504, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.737, 10.098], loss: 0.002315, mae: 0.051146, mean_q: 1.305678
 20409/100000: episode: 345, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.308, mean reward: 0.603 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.719, 10.098], loss: 0.002125, mae: 0.048636, mean_q: 1.302750
 20509/100000: episode: 346, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.500, mean reward: 0.585 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.846, 10.384], loss: 0.001957, mae: 0.047296, mean_q: 1.302706
 20609/100000: episode: 347, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.478, mean reward: 0.575 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.621, 10.098], loss: 0.002023, mae: 0.048351, mean_q: 1.291250
 20709/100000: episode: 348, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.362, mean reward: 0.584 [0.508, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.113, 10.098], loss: 0.002196, mae: 0.049379, mean_q: 1.294647
 20809/100000: episode: 349, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 60.194, mean reward: 0.602 [0.522, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.660, 10.178], loss: 0.002314, mae: 0.051917, mean_q: 1.293589
 20909/100000: episode: 350, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 59.676, mean reward: 0.597 [0.498, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.343, 10.098], loss: 0.002051, mae: 0.048858, mean_q: 1.289262
 21009/100000: episode: 351, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.949, mean reward: 0.589 [0.507, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.693, 10.098], loss: 0.002318, mae: 0.051355, mean_q: 1.284914
 21109/100000: episode: 352, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 60.976, mean reward: 0.610 [0.505, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.727, 10.109], loss: 0.002266, mae: 0.050622, mean_q: 1.281040
 21209/100000: episode: 353, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 56.713, mean reward: 0.567 [0.500, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.167, 10.140], loss: 0.002043, mae: 0.048813, mean_q: 1.273020
 21309/100000: episode: 354, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.093, mean reward: 0.591 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.876, 10.112], loss: 0.002279, mae: 0.051607, mean_q: 1.273000
 21409/100000: episode: 355, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 55.993, mean reward: 0.560 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.208, 10.098], loss: 0.002100, mae: 0.048285, mean_q: 1.271952
 21509/100000: episode: 356, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 62.028, mean reward: 0.620 [0.504, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.008, 10.126], loss: 0.002105, mae: 0.049022, mean_q: 1.276088
 21609/100000: episode: 357, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.468, mean reward: 0.595 [0.511, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.412, 10.098], loss: 0.002238, mae: 0.049316, mean_q: 1.266036
 21709/100000: episode: 358, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 60.333, mean reward: 0.603 [0.513, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.488, 10.309], loss: 0.002284, mae: 0.051471, mean_q: 1.263602
 21809/100000: episode: 359, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 61.761, mean reward: 0.618 [0.518, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.307, 10.562], loss: 0.002165, mae: 0.049103, mean_q: 1.259024
 21909/100000: episode: 360, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 60.984, mean reward: 0.610 [0.503, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.751, 10.098], loss: 0.002484, mae: 0.051495, mean_q: 1.255028
 22009/100000: episode: 361, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.006, mean reward: 0.600 [0.511, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.730, 10.098], loss: 0.002342, mae: 0.051051, mean_q: 1.251927
 22109/100000: episode: 362, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.710, mean reward: 0.577 [0.505, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.151, 10.098], loss: 0.002202, mae: 0.049666, mean_q: 1.248398
 22209/100000: episode: 363, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.899, mean reward: 0.579 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.736, 10.323], loss: 0.002127, mae: 0.048978, mean_q: 1.244735
 22309/100000: episode: 364, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.373, mean reward: 0.584 [0.506, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.130, 10.098], loss: 0.001982, mae: 0.047912, mean_q: 1.251513
 22409/100000: episode: 365, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.861, mean reward: 0.589 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.593, 10.098], loss: 0.001989, mae: 0.047919, mean_q: 1.245639
 22509/100000: episode: 366, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 63.286, mean reward: 0.633 [0.502, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.734, 10.098], loss: 0.002338, mae: 0.051326, mean_q: 1.238793
 22609/100000: episode: 367, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.425, mean reward: 0.594 [0.506, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.182, 10.399], loss: 0.001978, mae: 0.047545, mean_q: 1.233403
 22709/100000: episode: 368, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.146, mean reward: 0.591 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.052, 10.098], loss: 0.002204, mae: 0.050918, mean_q: 1.242414
 22809/100000: episode: 369, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 57.642, mean reward: 0.576 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.635, 10.098], loss: 0.002073, mae: 0.048589, mean_q: 1.228961
 22909/100000: episode: 370, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.648, mean reward: 0.596 [0.510, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.768, 10.193], loss: 0.002003, mae: 0.047356, mean_q: 1.227724
 23009/100000: episode: 371, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.095, mean reward: 0.581 [0.498, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.687, 10.162], loss: 0.001916, mae: 0.047249, mean_q: 1.221801
 23109/100000: episode: 372, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.501, mean reward: 0.585 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.130, 10.098], loss: 0.001683, mae: 0.044002, mean_q: 1.220404
 23209/100000: episode: 373, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.689, mean reward: 0.577 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.100, 10.183], loss: 0.001830, mae: 0.046739, mean_q: 1.210827
 23309/100000: episode: 374, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 59.326, mean reward: 0.593 [0.501, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.806, 10.374], loss: 0.001790, mae: 0.045177, mean_q: 1.200319
 23409/100000: episode: 375, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.713, mean reward: 0.587 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.535, 10.248], loss: 0.001827, mae: 0.045989, mean_q: 1.199380
 23509/100000: episode: 376, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.906, mean reward: 0.599 [0.509, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.499, 10.124], loss: 0.001962, mae: 0.046973, mean_q: 1.194077
 23609/100000: episode: 377, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.032, mean reward: 0.590 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.799, 10.249], loss: 0.001683, mae: 0.044494, mean_q: 1.190503
 23709/100000: episode: 378, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.319, mean reward: 0.603 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.898, 10.372], loss: 0.001663, mae: 0.044146, mean_q: 1.183663
 23809/100000: episode: 379, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.002, mean reward: 0.590 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.474, 10.098], loss: 0.001734, mae: 0.045143, mean_q: 1.180093
 23909/100000: episode: 380, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.584, mean reward: 0.576 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.174, 10.241], loss: 0.001685, mae: 0.045484, mean_q: 1.172524
 24009/100000: episode: 381, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.318, mean reward: 0.593 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.672, 10.098], loss: 0.001635, mae: 0.044090, mean_q: 1.170782
 24109/100000: episode: 382, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 56.701, mean reward: 0.567 [0.499, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.895, 10.098], loss: 0.001667, mae: 0.044915, mean_q: 1.172999
 24209/100000: episode: 383, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.325, mean reward: 0.603 [0.521, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.860, 10.436], loss: 0.001576, mae: 0.043548, mean_q: 1.171680
 24309/100000: episode: 384, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.731, mean reward: 0.597 [0.502, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.792, 10.098], loss: 0.001630, mae: 0.044105, mean_q: 1.171912
 24409/100000: episode: 385, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.267, mean reward: 0.573 [0.501, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.782, 10.220], loss: 0.001638, mae: 0.043636, mean_q: 1.172030
 24509/100000: episode: 386, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.706, mean reward: 0.577 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.587, 10.098], loss: 0.001735, mae: 0.045481, mean_q: 1.171885
 24609/100000: episode: 387, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 57.534, mean reward: 0.575 [0.502, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.785, 10.253], loss: 0.001672, mae: 0.044588, mean_q: 1.172991
 24709/100000: episode: 388, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.346, mean reward: 0.573 [0.499, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.340, 10.106], loss: 0.001690, mae: 0.044160, mean_q: 1.167769
 24809/100000: episode: 389, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.483, mean reward: 0.585 [0.507, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.641, 10.098], loss: 0.001558, mae: 0.043001, mean_q: 1.168405
 24909/100000: episode: 390, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 60.395, mean reward: 0.604 [0.510, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.838, 10.165], loss: 0.001744, mae: 0.045242, mean_q: 1.166346
 25009/100000: episode: 391, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.078, mean reward: 0.581 [0.505, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.548, 10.098], loss: 0.001577, mae: 0.043384, mean_q: 1.166182
 25109/100000: episode: 392, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 61.070, mean reward: 0.611 [0.507, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.152, 10.098], loss: 0.001622, mae: 0.043817, mean_q: 1.164730
 25209/100000: episode: 393, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.583, mean reward: 0.576 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.289, 10.098], loss: 0.001547, mae: 0.042803, mean_q: 1.165712
 25309/100000: episode: 394, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.620, mean reward: 0.566 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.107, 10.098], loss: 0.001683, mae: 0.044748, mean_q: 1.164427
 25409/100000: episode: 395, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 56.252, mean reward: 0.563 [0.500, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.423, 10.098], loss: 0.001490, mae: 0.041854, mean_q: 1.164443
 25509/100000: episode: 396, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.192, mean reward: 0.592 [0.505, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.760, 10.098], loss: 0.001737, mae: 0.045870, mean_q: 1.162337
 25609/100000: episode: 397, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.189, mean reward: 0.592 [0.500, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.165, 10.377], loss: 0.001570, mae: 0.043534, mean_q: 1.164273
 25709/100000: episode: 398, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 56.212, mean reward: 0.562 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.136], loss: 0.001692, mae: 0.045170, mean_q: 1.163402
 25809/100000: episode: 399, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.583, mean reward: 0.576 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.841, 10.173], loss: 0.001707, mae: 0.045162, mean_q: 1.168074
 25909/100000: episode: 400, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.439, mean reward: 0.574 [0.499, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.857, 10.220], loss: 0.001419, mae: 0.041269, mean_q: 1.163578
 26009/100000: episode: 401, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 57.626, mean reward: 0.576 [0.514, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.181, 10.158], loss: 0.001543, mae: 0.042661, mean_q: 1.160915
 26109/100000: episode: 402, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 59.925, mean reward: 0.599 [0.503, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.946, 10.233], loss: 0.001510, mae: 0.042201, mean_q: 1.160186
 26209/100000: episode: 403, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.892, mean reward: 0.589 [0.499, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.411, 10.098], loss: 0.001455, mae: 0.041862, mean_q: 1.163129
 26309/100000: episode: 404, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.404, mean reward: 0.584 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.802, 10.098], loss: 0.001518, mae: 0.043006, mean_q: 1.158590
 26409/100000: episode: 405, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.036, mean reward: 0.570 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.198, 10.150], loss: 0.001620, mae: 0.043675, mean_q: 1.164711
 26509/100000: episode: 406, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.368, mean reward: 0.564 [0.503, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.757, 10.185], loss: 0.001559, mae: 0.043495, mean_q: 1.163103
 26609/100000: episode: 407, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.769, mean reward: 0.588 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.336, 10.098], loss: 0.001676, mae: 0.044365, mean_q: 1.163611
 26709/100000: episode: 408, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.710, mean reward: 0.577 [0.506, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.503, 10.122], loss: 0.001533, mae: 0.043008, mean_q: 1.158231
 26809/100000: episode: 409, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 56.814, mean reward: 0.568 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.470, 10.162], loss: 0.001696, mae: 0.045001, mean_q: 1.158419
 26909/100000: episode: 410, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.403, mean reward: 0.584 [0.514, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.143, 10.098], loss: 0.001598, mae: 0.043308, mean_q: 1.158713
 27009/100000: episode: 411, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.799, mean reward: 0.598 [0.513, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.928, 10.297], loss: 0.001618, mae: 0.044134, mean_q: 1.158003
 27109/100000: episode: 412, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.072, mean reward: 0.581 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.011, 10.174], loss: 0.001558, mae: 0.042727, mean_q: 1.156376
 27209/100000: episode: 413, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 57.170, mean reward: 0.572 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.563, 10.098], loss: 0.001628, mae: 0.043983, mean_q: 1.157550
 27309/100000: episode: 414, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.826, mean reward: 0.588 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.915, 10.098], loss: 0.001569, mae: 0.043477, mean_q: 1.156743
 27409/100000: episode: 415, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 57.612, mean reward: 0.576 [0.511, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.580, 10.098], loss: 0.001586, mae: 0.043819, mean_q: 1.153575
 27509/100000: episode: 416, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.248, mean reward: 0.582 [0.507, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.013, 10.098], loss: 0.001574, mae: 0.043026, mean_q: 1.154295
 27609/100000: episode: 417, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.769, mean reward: 0.598 [0.509, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.768, 10.352], loss: 0.001550, mae: 0.042642, mean_q: 1.150797
 27709/100000: episode: 418, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.729, mean reward: 0.587 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.892, 10.174], loss: 0.001560, mae: 0.043407, mean_q: 1.152723
 27809/100000: episode: 419, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.199, mean reward: 0.592 [0.501, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.511, 10.098], loss: 0.001598, mae: 0.043773, mean_q: 1.155892
 27909/100000: episode: 420, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 63.009, mean reward: 0.630 [0.518, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.116, 10.448], loss: 0.001542, mae: 0.042908, mean_q: 1.154707
 28009/100000: episode: 421, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.215, mean reward: 0.582 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.175, 10.098], loss: 0.001571, mae: 0.043193, mean_q: 1.154407
 28109/100000: episode: 422, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.206, mean reward: 0.582 [0.497, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.544, 10.100], loss: 0.001629, mae: 0.044087, mean_q: 1.157357
 28209/100000: episode: 423, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.461, mean reward: 0.585 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.261, 10.155], loss: 0.001455, mae: 0.041866, mean_q: 1.155558
 28309/100000: episode: 424, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 60.134, mean reward: 0.601 [0.511, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.715, 10.098], loss: 0.001538, mae: 0.042640, mean_q: 1.156935
 28409/100000: episode: 425, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.776, mean reward: 0.598 [0.500, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.654, 10.098], loss: 0.001446, mae: 0.041938, mean_q: 1.155797
 28509/100000: episode: 426, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.394, mean reward: 0.594 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.218, 10.349], loss: 0.001486, mae: 0.042436, mean_q: 1.151208
 28609/100000: episode: 427, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.027, mean reward: 0.590 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.261, 10.254], loss: 0.001584, mae: 0.044029, mean_q: 1.152509
 28709/100000: episode: 428, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 57.698, mean reward: 0.577 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.271, 10.247], loss: 0.001526, mae: 0.042509, mean_q: 1.154824
 28809/100000: episode: 429, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.552, mean reward: 0.586 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.773, 10.364], loss: 0.001743, mae: 0.045419, mean_q: 1.157911
[Info] 1-TH LEVEL FOUND: 1.3141525983810425, Considering 10/90 traces
 28909/100000: episode: 430, duration: 4.640s, episode steps: 100, steps per second: 22, episode reward: 56.729, mean reward: 0.567 [0.509, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.944, 10.145], loss: 0.001486, mae: 0.042466, mean_q: 1.152842
 28998/100000: episode: 431, duration: 0.478s, episode steps: 89, steps per second: 186, episode reward: 54.301, mean reward: 0.610 [0.510, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.573 [-0.464, 10.358], loss: 0.001493, mae: 0.042100, mean_q: 1.154245
 29087/100000: episode: 432, duration: 0.483s, episode steps: 89, steps per second: 184, episode reward: 53.247, mean reward: 0.598 [0.509, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-1.446, 10.100], loss: 0.001723, mae: 0.045069, mean_q: 1.154544
 29109/100000: episode: 433, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 15.727, mean reward: 0.715 [0.647, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-1.571, 10.372], loss: 0.001513, mae: 0.041836, mean_q: 1.154492
 29131/100000: episode: 434, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 14.820, mean reward: 0.674 [0.623, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.302], loss: 0.001513, mae: 0.042790, mean_q: 1.158232
 29153/100000: episode: 435, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 14.262, mean reward: 0.648 [0.588, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.605, 10.308], loss: 0.001372, mae: 0.040696, mean_q: 1.158880
 29187/100000: episode: 436, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 22.668, mean reward: 0.667 [0.583, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.036, 10.314], loss: 0.001536, mae: 0.042439, mean_q: 1.161762
 29206/100000: episode: 437, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 13.702, mean reward: 0.721 [0.666, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-1.048, 10.483], loss: 0.001820, mae: 0.045552, mean_q: 1.157913
 29225/100000: episode: 438, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 11.925, mean reward: 0.628 [0.576, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.485, 10.262], loss: 0.001667, mae: 0.044685, mean_q: 1.158720
 29247/100000: episode: 439, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 13.539, mean reward: 0.615 [0.516, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.700, 10.129], loss: 0.001596, mae: 0.043506, mean_q: 1.161900
 29302/100000: episode: 440, duration: 0.294s, episode steps: 55, steps per second: 187, episode reward: 34.504, mean reward: 0.627 [0.562, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.427, 10.414], loss: 0.001558, mae: 0.043336, mean_q: 1.156919
 29357/100000: episode: 441, duration: 0.302s, episode steps: 55, steps per second: 182, episode reward: 33.905, mean reward: 0.616 [0.535, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.557, 10.200], loss: 0.001650, mae: 0.043057, mean_q: 1.158088
 29457/100000: episode: 442, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 60.348, mean reward: 0.603 [0.514, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.120, 10.240], loss: 0.001591, mae: 0.043002, mean_q: 1.162657
 29557/100000: episode: 443, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.718, mean reward: 0.577 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.700, 10.100], loss: 0.001725, mae: 0.044661, mean_q: 1.160453
 29576/100000: episode: 444, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 13.546, mean reward: 0.713 [0.632, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.604], loss: 0.001640, mae: 0.042754, mean_q: 1.166266
 29655/100000: episode: 445, duration: 0.406s, episode steps: 79, steps per second: 195, episode reward: 49.222, mean reward: 0.623 [0.522, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.638 [-0.407, 10.100], loss: 0.001708, mae: 0.044571, mean_q: 1.162959
 29677/100000: episode: 446, duration: 0.114s, episode steps: 22, steps per second: 192, episode reward: 15.408, mean reward: 0.700 [0.657, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.476], loss: 0.001708, mae: 0.045298, mean_q: 1.155883
 29727/100000: episode: 447, duration: 0.260s, episode steps: 50, steps per second: 192, episode reward: 34.773, mean reward: 0.695 [0.611, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.326, 10.413], loss: 0.001688, mae: 0.043535, mean_q: 1.163533
 29816/100000: episode: 448, duration: 0.459s, episode steps: 89, steps per second: 194, episode reward: 54.194, mean reward: 0.609 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.567 [-1.314, 10.362], loss: 0.001776, mae: 0.045573, mean_q: 1.165397
 29915/100000: episode: 449, duration: 0.515s, episode steps: 99, steps per second: 192, episode reward: 57.867, mean reward: 0.585 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-1.055, 10.248], loss: 0.001576, mae: 0.042944, mean_q: 1.168038
 30004/100000: episode: 450, duration: 0.493s, episode steps: 89, steps per second: 181, episode reward: 53.476, mean reward: 0.601 [0.515, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.562 [-1.018, 10.246], loss: 0.001698, mae: 0.044855, mean_q: 1.165821
 30026/100000: episode: 451, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 16.352, mean reward: 0.743 [0.620, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.475], loss: 0.001627, mae: 0.043548, mean_q: 1.168515
 30081/100000: episode: 452, duration: 0.310s, episode steps: 55, steps per second: 177, episode reward: 35.132, mean reward: 0.639 [0.517, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.657, 10.177], loss: 0.001722, mae: 0.044590, mean_q: 1.164869
 30103/100000: episode: 453, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 14.168, mean reward: 0.644 [0.596, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.405], loss: 0.001735, mae: 0.045884, mean_q: 1.174626
 30153/100000: episode: 454, duration: 0.288s, episode steps: 50, steps per second: 174, episode reward: 29.319, mean reward: 0.586 [0.507, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.319, 10.100], loss: 0.001766, mae: 0.044155, mean_q: 1.171626
 30175/100000: episode: 455, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 14.575, mean reward: 0.663 [0.586, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.468, 10.316], loss: 0.001575, mae: 0.041812, mean_q: 1.172681
 30264/100000: episode: 456, duration: 0.460s, episode steps: 89, steps per second: 194, episode reward: 53.409, mean reward: 0.600 [0.512, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-0.523, 10.100], loss: 0.001612, mae: 0.042860, mean_q: 1.169349
 30286/100000: episode: 457, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 15.261, mean reward: 0.694 [0.631, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.840, 10.421], loss: 0.001801, mae: 0.045410, mean_q: 1.167820
 30375/100000: episode: 458, duration: 0.495s, episode steps: 89, steps per second: 180, episode reward: 57.661, mean reward: 0.648 [0.523, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-1.432, 10.100], loss: 0.001552, mae: 0.042135, mean_q: 1.173964
 30475/100000: episode: 459, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.433, mean reward: 0.594 [0.514, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.692, 10.305], loss: 0.001708, mae: 0.043897, mean_q: 1.177493
 30574/100000: episode: 460, duration: 0.538s, episode steps: 99, steps per second: 184, episode reward: 58.281, mean reward: 0.589 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-1.015, 10.196], loss: 0.001719, mae: 0.044666, mean_q: 1.177347
 30596/100000: episode: 461, duration: 0.105s, episode steps: 22, steps per second: 209, episode reward: 14.133, mean reward: 0.642 [0.528, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.193], loss: 0.001646, mae: 0.043638, mean_q: 1.171452
 30696/100000: episode: 462, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 61.199, mean reward: 0.612 [0.502, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.631, 10.277], loss: 0.001794, mae: 0.045470, mean_q: 1.175443
 30785/100000: episode: 463, duration: 0.486s, episode steps: 89, steps per second: 183, episode reward: 55.566, mean reward: 0.624 [0.520, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-0.937, 10.131], loss: 0.001581, mae: 0.042256, mean_q: 1.177019
 30864/100000: episode: 464, duration: 0.405s, episode steps: 79, steps per second: 195, episode reward: 46.968, mean reward: 0.595 [0.508, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.653 [-0.569, 10.158], loss: 0.001743, mae: 0.044667, mean_q: 1.178973
 30953/100000: episode: 465, duration: 0.479s, episode steps: 89, steps per second: 186, episode reward: 52.885, mean reward: 0.594 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.561 [-0.818, 10.181], loss: 0.001773, mae: 0.045195, mean_q: 1.178009
 31052/100000: episode: 466, duration: 0.510s, episode steps: 99, steps per second: 194, episode reward: 57.270, mean reward: 0.578 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.354, 10.100], loss: 0.001670, mae: 0.044426, mean_q: 1.181161
 31071/100000: episode: 467, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 12.930, mean reward: 0.681 [0.618, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.193, 10.483], loss: 0.001625, mae: 0.042534, mean_q: 1.183539
 31121/100000: episode: 468, duration: 0.269s, episode steps: 50, steps per second: 186, episode reward: 33.057, mean reward: 0.661 [0.555, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-1.534, 10.253], loss: 0.001574, mae: 0.041414, mean_q: 1.179968
 31155/100000: episode: 469, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 23.836, mean reward: 0.701 [0.632, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.105, 10.397], loss: 0.001647, mae: 0.043010, mean_q: 1.173206
 31255/100000: episode: 470, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 63.131, mean reward: 0.631 [0.502, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.889, 10.100], loss: 0.001597, mae: 0.042930, mean_q: 1.185803
 31310/100000: episode: 471, duration: 0.307s, episode steps: 55, steps per second: 179, episode reward: 39.284, mean reward: 0.714 [0.513, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.839 [-0.938, 10.100], loss: 0.001865, mae: 0.046570, mean_q: 1.190453
 31332/100000: episode: 472, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 13.705, mean reward: 0.623 [0.542, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.215], loss: 0.001700, mae: 0.044135, mean_q: 1.186767
 31421/100000: episode: 473, duration: 0.489s, episode steps: 89, steps per second: 182, episode reward: 52.918, mean reward: 0.595 [0.511, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.552 [-1.425, 10.153], loss: 0.001698, mae: 0.043516, mean_q: 1.189664
 31521/100000: episode: 474, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.494, mean reward: 0.595 [0.500, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-1.808, 10.332], loss: 0.001730, mae: 0.044608, mean_q: 1.184460
 31600/100000: episode: 475, duration: 0.412s, episode steps: 79, steps per second: 192, episode reward: 45.652, mean reward: 0.578 [0.512, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.654 [-0.878, 10.305], loss: 0.001641, mae: 0.043280, mean_q: 1.187016
 31622/100000: episode: 476, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 15.553, mean reward: 0.707 [0.630, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.444, 10.399], loss: 0.001631, mae: 0.042951, mean_q: 1.185733
 31656/100000: episode: 477, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 23.376, mean reward: 0.688 [0.629, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.035, 10.472], loss: 0.001592, mae: 0.042724, mean_q: 1.196558
 31745/100000: episode: 478, duration: 0.451s, episode steps: 89, steps per second: 197, episode reward: 53.954, mean reward: 0.606 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.565 [-1.235, 10.315], loss: 0.001771, mae: 0.045933, mean_q: 1.193957
 31800/100000: episode: 479, duration: 0.296s, episode steps: 55, steps per second: 186, episode reward: 36.523, mean reward: 0.664 [0.598, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-3.031, 10.351], loss: 0.001747, mae: 0.043757, mean_q: 1.193807
 31889/100000: episode: 480, duration: 0.462s, episode steps: 89, steps per second: 193, episode reward: 52.484, mean reward: 0.590 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.564 [-0.485, 10.100], loss: 0.001663, mae: 0.043265, mean_q: 1.196380
 31978/100000: episode: 481, duration: 0.480s, episode steps: 89, steps per second: 185, episode reward: 58.389, mean reward: 0.656 [0.567, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.386, 10.333], loss: 0.001730, mae: 0.044247, mean_q: 1.193532
 32000/100000: episode: 482, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 14.289, mean reward: 0.650 [0.588, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.091, 10.428], loss: 0.001468, mae: 0.041786, mean_q: 1.186763
 32100/100000: episode: 483, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.765, mean reward: 0.578 [0.504, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.727, 10.100], loss: 0.001604, mae: 0.043239, mean_q: 1.200547
 32199/100000: episode: 484, duration: 0.526s, episode steps: 99, steps per second: 188, episode reward: 61.272, mean reward: 0.619 [0.510, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-1.091, 10.205], loss: 0.001663, mae: 0.043632, mean_q: 1.200135
 32278/100000: episode: 485, duration: 0.442s, episode steps: 79, steps per second: 179, episode reward: 45.131, mean reward: 0.571 [0.499, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.653 [-0.583, 10.100], loss: 0.001696, mae: 0.045095, mean_q: 1.198606
 32357/100000: episode: 486, duration: 0.417s, episode steps: 79, steps per second: 189, episode reward: 48.051, mean reward: 0.608 [0.506, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.647 [-1.378, 10.100], loss: 0.001650, mae: 0.043718, mean_q: 1.199721
 32379/100000: episode: 487, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 15.975, mean reward: 0.726 [0.646, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.082, 10.362], loss: 0.001450, mae: 0.041628, mean_q: 1.198298
 32429/100000: episode: 488, duration: 0.258s, episode steps: 50, steps per second: 194, episode reward: 29.973, mean reward: 0.599 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.193, 10.113], loss: 0.001509, mae: 0.042527, mean_q: 1.196560
 32484/100000: episode: 489, duration: 0.278s, episode steps: 55, steps per second: 198, episode reward: 35.562, mean reward: 0.647 [0.507, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.802, 10.123], loss: 0.001597, mae: 0.042256, mean_q: 1.207150
 32534/100000: episode: 490, duration: 0.277s, episode steps: 50, steps per second: 181, episode reward: 32.869, mean reward: 0.657 [0.564, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.323, 10.323], loss: 0.001577, mae: 0.042844, mean_q: 1.200163
 32556/100000: episode: 491, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 14.316, mean reward: 0.651 [0.564, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.255], loss: 0.001832, mae: 0.046210, mean_q: 1.200450
 32635/100000: episode: 492, duration: 0.429s, episode steps: 79, steps per second: 184, episode reward: 47.746, mean reward: 0.604 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.655 [-0.519, 10.113], loss: 0.001784, mae: 0.045341, mean_q: 1.201617
 32735/100000: episode: 493, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.089, mean reward: 0.591 [0.512, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.146, 10.134], loss: 0.001521, mae: 0.042115, mean_q: 1.200018
 32835/100000: episode: 494, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 58.615, mean reward: 0.586 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.806, 10.100], loss: 0.001489, mae: 0.041688, mean_q: 1.202780
 32935/100000: episode: 495, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 60.343, mean reward: 0.603 [0.499, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.853, 10.100], loss: 0.001436, mae: 0.040983, mean_q: 1.200703
 32957/100000: episode: 496, duration: 0.137s, episode steps: 22, steps per second: 161, episode reward: 14.398, mean reward: 0.654 [0.599, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.208, 10.308], loss: 0.001744, mae: 0.043659, mean_q: 1.207093
 33007/100000: episode: 497, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 29.898, mean reward: 0.598 [0.523, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.522, 10.100], loss: 0.001498, mae: 0.042848, mean_q: 1.204834
 33026/100000: episode: 498, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 12.383, mean reward: 0.652 [0.590, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-1.072, 10.311], loss: 0.001371, mae: 0.039830, mean_q: 1.198982
 33076/100000: episode: 499, duration: 0.289s, episode steps: 50, steps per second: 173, episode reward: 30.606, mean reward: 0.612 [0.522, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.321, 10.316], loss: 0.001506, mae: 0.041942, mean_q: 1.202205
 33176/100000: episode: 500, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.714, mean reward: 0.587 [0.509, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.293, 10.310], loss: 0.001583, mae: 0.042991, mean_q: 1.204066
 33255/100000: episode: 501, duration: 0.410s, episode steps: 79, steps per second: 192, episode reward: 45.986, mean reward: 0.582 [0.503, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.659 [-0.522, 10.100], loss: 0.001476, mae: 0.042503, mean_q: 1.203419
 33354/100000: episode: 502, duration: 0.514s, episode steps: 99, steps per second: 193, episode reward: 59.114, mean reward: 0.597 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.559, 10.100], loss: 0.001563, mae: 0.043150, mean_q: 1.199596
 33373/100000: episode: 503, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 12.409, mean reward: 0.653 [0.580, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.298, 10.295], loss: 0.001624, mae: 0.043618, mean_q: 1.202348
 33407/100000: episode: 504, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 23.611, mean reward: 0.694 [0.599, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.295, 10.340], loss: 0.001474, mae: 0.041622, mean_q: 1.195825
 33496/100000: episode: 505, duration: 0.466s, episode steps: 89, steps per second: 191, episode reward: 52.421, mean reward: 0.589 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.561 [-0.097, 10.100], loss: 0.001471, mae: 0.041787, mean_q: 1.200732
 33575/100000: episode: 506, duration: 0.418s, episode steps: 79, steps per second: 189, episode reward: 48.141, mean reward: 0.609 [0.514, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.649 [-1.370, 10.121], loss: 0.001541, mae: 0.043574, mean_q: 1.200458
 33675/100000: episode: 507, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.495, mean reward: 0.595 [0.503, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.758, 10.251], loss: 0.001569, mae: 0.042691, mean_q: 1.200332
 33694/100000: episode: 508, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 13.388, mean reward: 0.705 [0.646, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.401], loss: 0.001683, mae: 0.044864, mean_q: 1.208492
 33749/100000: episode: 509, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 39.429, mean reward: 0.717 [0.642, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.897, 10.390], loss: 0.001363, mae: 0.041172, mean_q: 1.199633
 33771/100000: episode: 510, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 14.207, mean reward: 0.646 [0.581, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.443], loss: 0.001319, mae: 0.039936, mean_q: 1.208316
 33805/100000: episode: 511, duration: 0.172s, episode steps: 34, steps per second: 197, episode reward: 26.620, mean reward: 0.783 [0.670, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.876, 10.572], loss: 0.001463, mae: 0.041563, mean_q: 1.212164
 33884/100000: episode: 512, duration: 0.429s, episode steps: 79, steps per second: 184, episode reward: 48.620, mean reward: 0.615 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.637 [-1.487, 10.100], loss: 0.001475, mae: 0.041354, mean_q: 1.213439
 33918/100000: episode: 513, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 24.301, mean reward: 0.715 [0.615, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.462, 10.387], loss: 0.001505, mae: 0.042366, mean_q: 1.214617
 33952/100000: episode: 514, duration: 0.175s, episode steps: 34, steps per second: 195, episode reward: 24.766, mean reward: 0.728 [0.647, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.931, 10.456], loss: 0.001438, mae: 0.041643, mean_q: 1.216452
 34041/100000: episode: 515, duration: 0.483s, episode steps: 89, steps per second: 184, episode reward: 50.894, mean reward: 0.572 [0.505, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.557 [-1.329, 10.100], loss: 0.001457, mae: 0.041431, mean_q: 1.210784
 34141/100000: episode: 516, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 61.637, mean reward: 0.616 [0.517, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.338, 10.100], loss: 0.001392, mae: 0.041186, mean_q: 1.213356
 34160/100000: episode: 517, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 12.805, mean reward: 0.674 [0.593, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.083, 10.295], loss: 0.001455, mae: 0.042071, mean_q: 1.210168
 34249/100000: episode: 518, duration: 0.497s, episode steps: 89, steps per second: 179, episode reward: 51.939, mean reward: 0.584 [0.505, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.560 [-1.520, 10.102], loss: 0.001536, mae: 0.043473, mean_q: 1.215837
 34299/100000: episode: 519, duration: 0.294s, episode steps: 50, steps per second: 170, episode reward: 30.635, mean reward: 0.613 [0.533, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.397, 10.245], loss: 0.001492, mae: 0.042502, mean_q: 1.213277
[Info] 2-TH LEVEL FOUND: 1.5107897520065308, Considering 10/90 traces
 34388/100000: episode: 520, duration: 4.550s, episode steps: 89, steps per second: 20, episode reward: 51.600, mean reward: 0.580 [0.514, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.555 [-0.455, 10.100], loss: 0.001415, mae: 0.041012, mean_q: 1.210211
 34415/100000: episode: 521, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 21.017, mean reward: 0.778 [0.655, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.176, 10.419], loss: 0.001253, mae: 0.038761, mean_q: 1.211449
 34442/100000: episode: 522, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 19.790, mean reward: 0.733 [0.623, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.390], loss: 0.001579, mae: 0.044983, mean_q: 1.200681
 34469/100000: episode: 523, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 19.849, mean reward: 0.735 [0.616, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.780, 10.363], loss: 0.001457, mae: 0.041446, mean_q: 1.212737
 34518/100000: episode: 524, duration: 0.267s, episode steps: 49, steps per second: 184, episode reward: 37.899, mean reward: 0.773 [0.555, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-2.507, 10.356], loss: 0.001752, mae: 0.045652, mean_q: 1.212140
 34562/100000: episode: 525, duration: 0.227s, episode steps: 44, steps per second: 193, episode reward: 31.962, mean reward: 0.726 [0.606, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.684, 10.376], loss: 0.001684, mae: 0.043946, mean_q: 1.215875
 34605/100000: episode: 526, duration: 0.223s, episode steps: 43, steps per second: 193, episode reward: 28.053, mean reward: 0.652 [0.554, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.628, 10.301], loss: 0.001511, mae: 0.042749, mean_q: 1.223405
 34620/100000: episode: 527, duration: 0.101s, episode steps: 15, steps per second: 148, episode reward: 11.549, mean reward: 0.770 [0.743, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.311, 10.100], loss: 0.001669, mae: 0.044804, mean_q: 1.220968
 34635/100000: episode: 528, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 10.956, mean reward: 0.730 [0.688, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.319, 10.100], loss: 0.001759, mae: 0.046266, mean_q: 1.228142
 34650/100000: episode: 529, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 11.852, mean reward: 0.790 [0.734, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.727, 10.100], loss: 0.001650, mae: 0.045797, mean_q: 1.233618
 34699/100000: episode: 530, duration: 0.267s, episode steps: 49, steps per second: 183, episode reward: 30.096, mean reward: 0.614 [0.526, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.400, 10.133], loss: 0.001596, mae: 0.043680, mean_q: 1.220021
 34742/100000: episode: 531, duration: 0.241s, episode steps: 43, steps per second: 178, episode reward: 29.480, mean reward: 0.686 [0.521, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.216, 10.224], loss: 0.001446, mae: 0.042559, mean_q: 1.224963
 34769/100000: episode: 532, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 19.649, mean reward: 0.728 [0.659, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.606, 10.484], loss: 0.001850, mae: 0.045536, mean_q: 1.222103
 34812/100000: episode: 533, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 27.062, mean reward: 0.629 [0.542, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.417, 10.245], loss: 0.001603, mae: 0.043176, mean_q: 1.221144
 34855/100000: episode: 534, duration: 0.244s, episode steps: 43, steps per second: 176, episode reward: 31.224, mean reward: 0.726 [0.612, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.752, 10.354], loss: 0.001438, mae: 0.041440, mean_q: 1.213651
 34882/100000: episode: 535, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 19.395, mean reward: 0.718 [0.573, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.292], loss: 0.001504, mae: 0.042759, mean_q: 1.230404
 34903/100000: episode: 536, duration: 0.138s, episode steps: 21, steps per second: 152, episode reward: 15.170, mean reward: 0.722 [0.639, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.398], loss: 0.001406, mae: 0.041810, mean_q: 1.223937
 34952/100000: episode: 537, duration: 0.267s, episode steps: 49, steps per second: 184, episode reward: 36.268, mean reward: 0.740 [0.663, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.654, 10.462], loss: 0.001467, mae: 0.042514, mean_q: 1.228182
 34996/100000: episode: 538, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 30.397, mean reward: 0.691 [0.521, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-1.069, 10.100], loss: 0.001497, mae: 0.043089, mean_q: 1.229438
 35040/100000: episode: 539, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 31.573, mean reward: 0.718 [0.540, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.258, 10.277], loss: 0.001567, mae: 0.043892, mean_q: 1.225641
 35083/100000: episode: 540, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 32.719, mean reward: 0.761 [0.643, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-1.720, 10.388], loss: 0.001487, mae: 0.043261, mean_q: 1.237339
 35129/100000: episode: 541, duration: 0.245s, episode steps: 46, steps per second: 188, episode reward: 28.441, mean reward: 0.618 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.278, 10.100], loss: 0.001692, mae: 0.045137, mean_q: 1.226581
 35178/100000: episode: 542, duration: 0.256s, episode steps: 49, steps per second: 192, episode reward: 29.936, mean reward: 0.611 [0.505, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.391, 10.183], loss: 0.001453, mae: 0.041925, mean_q: 1.238417
 35193/100000: episode: 543, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 11.488, mean reward: 0.766 [0.710, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.219, 10.100], loss: 0.001321, mae: 0.040338, mean_q: 1.242823
 35237/100000: episode: 544, duration: 0.233s, episode steps: 44, steps per second: 189, episode reward: 28.678, mean reward: 0.652 [0.544, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.316, 10.234], loss: 0.001611, mae: 0.044014, mean_q: 1.239360
[Info] FALSIFICATION!
 35248/100000: episode: 545, duration: 0.347s, episode steps: 11, steps per second: 32, episode reward: 9.429, mean reward: 0.857 [0.794, 1.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.475, 9.989], loss: 0.001651, mae: 0.045309, mean_q: 1.240222
 35275/100000: episode: 546, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 20.603, mean reward: 0.763 [0.632, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.193, 10.270], loss: 0.001595, mae: 0.043496, mean_q: 1.234518
 35304/100000: episode: 547, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 22.182, mean reward: 0.765 [0.698, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.035, 10.429], loss: 0.001823, mae: 0.044748, mean_q: 1.246159
 35319/100000: episode: 548, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 12.004, mean reward: 0.800 [0.769, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.364, 10.100], loss: 0.002152, mae: 0.050408, mean_q: 1.220287
 35346/100000: episode: 549, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 20.714, mean reward: 0.767 [0.690, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.755, 10.514], loss: 0.001764, mae: 0.045397, mean_q: 1.239956
 35373/100000: episode: 550, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 19.911, mean reward: 0.737 [0.615, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.353], loss: 0.001533, mae: 0.043558, mean_q: 1.245756
 35388/100000: episode: 551, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 11.964, mean reward: 0.798 [0.701, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.369, 10.100], loss: 0.001503, mae: 0.042620, mean_q: 1.251869
 35415/100000: episode: 552, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 18.538, mean reward: 0.687 [0.623, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.443], loss: 0.001504, mae: 0.042965, mean_q: 1.238871
 35464/100000: episode: 553, duration: 0.243s, episode steps: 49, steps per second: 202, episode reward: 32.490, mean reward: 0.663 [0.530, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.377, 10.282], loss: 0.001362, mae: 0.041204, mean_q: 1.245860
 35513/100000: episode: 554, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 30.711, mean reward: 0.627 [0.540, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.936, 10.205], loss: 0.001534, mae: 0.041650, mean_q: 1.246510
 35556/100000: episode: 555, duration: 0.235s, episode steps: 43, steps per second: 183, episode reward: 29.773, mean reward: 0.692 [0.608, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.432, 10.418], loss: 0.001491, mae: 0.042776, mean_q: 1.233033
 35600/100000: episode: 556, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 29.587, mean reward: 0.672 [0.546, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.622, 10.228], loss: 0.001443, mae: 0.041720, mean_q: 1.235512
 35644/100000: episode: 557, duration: 0.233s, episode steps: 44, steps per second: 189, episode reward: 31.631, mean reward: 0.719 [0.585, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-1.074, 10.396], loss: 0.001374, mae: 0.040615, mean_q: 1.250908
 35671/100000: episode: 558, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 18.782, mean reward: 0.696 [0.598, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.708, 10.321], loss: 0.001800, mae: 0.043969, mean_q: 1.254142
 35698/100000: episode: 559, duration: 0.179s, episode steps: 27, steps per second: 151, episode reward: 19.200, mean reward: 0.711 [0.633, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.387], loss: 0.001692, mae: 0.042394, mean_q: 1.253778
 35744/100000: episode: 560, duration: 0.242s, episode steps: 46, steps per second: 190, episode reward: 29.565, mean reward: 0.643 [0.510, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.700, 10.151], loss: 0.001493, mae: 0.041806, mean_q: 1.255733
 35788/100000: episode: 561, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 30.285, mean reward: 0.688 [0.636, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-1.210, 10.344], loss: 0.001651, mae: 0.042062, mean_q: 1.251162
 35837/100000: episode: 562, duration: 0.270s, episode steps: 49, steps per second: 181, episode reward: 34.887, mean reward: 0.712 [0.589, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.408, 10.337], loss: 0.001615, mae: 0.044866, mean_q: 1.251396
 35864/100000: episode: 563, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 19.123, mean reward: 0.708 [0.543, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.290], loss: 0.001599, mae: 0.044584, mean_q: 1.262842
 35910/100000: episode: 564, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 31.209, mean reward: 0.678 [0.543, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.279, 10.187], loss: 0.001404, mae: 0.041814, mean_q: 1.261441
 35939/100000: episode: 565, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 22.734, mean reward: 0.784 [0.710, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.632, 10.423], loss: 0.001701, mae: 0.046137, mean_q: 1.257864
 35960/100000: episode: 566, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 14.514, mean reward: 0.691 [0.631, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.289, 10.348], loss: 0.001744, mae: 0.046516, mean_q: 1.260185
 35981/100000: episode: 567, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 14.476, mean reward: 0.689 [0.549, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.009, 10.299], loss: 0.001484, mae: 0.041873, mean_q: 1.270678
 36008/100000: episode: 568, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 18.059, mean reward: 0.669 [0.591, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.340], loss: 0.001262, mae: 0.040110, mean_q: 1.266494
 36035/100000: episode: 569, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 21.180, mean reward: 0.784 [0.731, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.276, 10.382], loss: 0.001292, mae: 0.039711, mean_q: 1.262510
 36078/100000: episode: 570, duration: 0.237s, episode steps: 43, steps per second: 181, episode reward: 30.252, mean reward: 0.704 [0.616, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.721, 10.380], loss: 0.001626, mae: 0.044628, mean_q: 1.267382
 36105/100000: episode: 571, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 18.228, mean reward: 0.675 [0.590, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.604, 10.339], loss: 0.001380, mae: 0.040875, mean_q: 1.268982
 36154/100000: episode: 572, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 30.136, mean reward: 0.615 [0.530, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.386, 10.218], loss: 0.001545, mae: 0.043414, mean_q: 1.268071
 36203/100000: episode: 573, duration: 0.249s, episode steps: 49, steps per second: 197, episode reward: 34.552, mean reward: 0.705 [0.520, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.409, 10.156], loss: 0.001604, mae: 0.043854, mean_q: 1.262312
 36252/100000: episode: 574, duration: 0.261s, episode steps: 49, steps per second: 188, episode reward: 33.592, mean reward: 0.686 [0.587, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.419, 10.359], loss: 0.001495, mae: 0.042469, mean_q: 1.257908
 36298/100000: episode: 575, duration: 0.238s, episode steps: 46, steps per second: 194, episode reward: 32.709, mean reward: 0.711 [0.641, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-1.226, 10.330], loss: 0.001600, mae: 0.043577, mean_q: 1.256900
 36342/100000: episode: 576, duration: 0.242s, episode steps: 44, steps per second: 182, episode reward: 32.368, mean reward: 0.736 [0.676, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-1.366, 10.544], loss: 0.001504, mae: 0.042707, mean_q: 1.264078
 36369/100000: episode: 577, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 19.132, mean reward: 0.709 [0.627, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.378, 10.376], loss: 0.001645, mae: 0.041996, mean_q: 1.260688
 36390/100000: episode: 578, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 16.470, mean reward: 0.784 [0.713, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.831, 10.586], loss: 0.001319, mae: 0.039385, mean_q: 1.270331
 36417/100000: episode: 579, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 19.012, mean reward: 0.704 [0.617, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.418, 10.386], loss: 0.001214, mae: 0.039650, mean_q: 1.263369
 36463/100000: episode: 580, duration: 0.258s, episode steps: 46, steps per second: 178, episode reward: 34.113, mean reward: 0.742 [0.552, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-1.211, 10.422], loss: 0.001470, mae: 0.042336, mean_q: 1.269531
 36509/100000: episode: 581, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 33.135, mean reward: 0.720 [0.604, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.284, 10.339], loss: 0.001542, mae: 0.043639, mean_q: 1.260853
 36538/100000: episode: 582, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 19.645, mean reward: 0.677 [0.623, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.635, 10.401], loss: 0.001469, mae: 0.042909, mean_q: 1.267931
 36582/100000: episode: 583, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 31.844, mean reward: 0.724 [0.655, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.696, 10.473], loss: 0.001456, mae: 0.040107, mean_q: 1.273952
 36628/100000: episode: 584, duration: 0.258s, episode steps: 46, steps per second: 178, episode reward: 36.998, mean reward: 0.804 [0.664, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.781, 10.419], loss: 0.001917, mae: 0.045973, mean_q: 1.277840
 36649/100000: episode: 585, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 17.244, mean reward: 0.821 [0.775, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.448, 10.561], loss: 0.001798, mae: 0.039826, mean_q: 1.287393
 36698/100000: episode: 586, duration: 0.273s, episode steps: 49, steps per second: 180, episode reward: 29.547, mean reward: 0.603 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.513, 10.100], loss: 0.001488, mae: 0.042640, mean_q: 1.278842
 36725/100000: episode: 587, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 18.104, mean reward: 0.671 [0.555, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.441, 10.260], loss: 0.001391, mae: 0.042146, mean_q: 1.279831
 36752/100000: episode: 588, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 18.494, mean reward: 0.685 [0.580, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-1.008, 10.363], loss: 0.001315, mae: 0.040517, mean_q: 1.293515
 36798/100000: episode: 589, duration: 0.265s, episode steps: 46, steps per second: 173, episode reward: 32.477, mean reward: 0.706 [0.588, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.401, 10.362], loss: 0.001376, mae: 0.041440, mean_q: 1.286496
 36827/100000: episode: 590, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 21.181, mean reward: 0.730 [0.687, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.086, 10.464], loss: 0.001383, mae: 0.041095, mean_q: 1.290175
 36856/100000: episode: 591, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 22.422, mean reward: 0.773 [0.696, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.058, 10.461], loss: 0.001338, mae: 0.040878, mean_q: 1.288302
[Info] FALSIFICATION!
 36857/100000: episode: 592, duration: 0.258s, episode steps: 1, steps per second: 4, episode reward: 1.020, mean reward: 1.020 [1.020, 1.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-0.015, 8.502], loss: 0.001348, mae: 0.045139, mean_q: 1.316663
 36900/100000: episode: 593, duration: 0.248s, episode steps: 43, steps per second: 173, episode reward: 27.525, mean reward: 0.640 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.726, 10.133], loss: 0.001582, mae: 0.044740, mean_q: 1.276510
 36921/100000: episode: 594, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 16.762, mean reward: 0.798 [0.719, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.528], loss: 0.001374, mae: 0.041308, mean_q: 1.271469
 36965/100000: episode: 595, duration: 0.270s, episode steps: 44, steps per second: 163, episode reward: 30.287, mean reward: 0.688 [0.614, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.792, 10.464], loss: 0.001719, mae: 0.044725, mean_q: 1.287888
 36992/100000: episode: 596, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 19.054, mean reward: 0.706 [0.648, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.313, 10.343], loss: 0.001507, mae: 0.043856, mean_q: 1.286465
 37007/100000: episode: 597, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 11.226, mean reward: 0.748 [0.707, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.362, 10.100], loss: 0.002277, mae: 0.044866, mean_q: 1.280840
 37034/100000: episode: 598, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 20.694, mean reward: 0.766 [0.696, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.445, 10.660], loss: 0.002002, mae: 0.048165, mean_q: 1.305997
 37061/100000: episode: 599, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 19.233, mean reward: 0.712 [0.626, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.103, 10.429], loss: 0.001399, mae: 0.041366, mean_q: 1.304742
[Info] FALSIFICATION!
 37098/100000: episode: 600, duration: 0.463s, episode steps: 37, steps per second: 80, episode reward: 30.667, mean reward: 0.829 [0.661, 1.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.783 [-0.416, 9.996], loss: 0.001463, mae: 0.042291, mean_q: 1.295751
 37125/100000: episode: 601, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 19.302, mean reward: 0.715 [0.553, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.259], loss: 0.001862, mae: 0.043686, mean_q: 1.293830
 37174/100000: episode: 602, duration: 0.245s, episode steps: 49, steps per second: 200, episode reward: 32.252, mean reward: 0.658 [0.504, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.897, 10.111], loss: 0.001648, mae: 0.043627, mean_q: 1.297414
 37223/100000: episode: 603, duration: 0.270s, episode steps: 49, steps per second: 182, episode reward: 37.520, mean reward: 0.766 [0.649, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.719, 10.563], loss: 0.001790, mae: 0.044216, mean_q: 1.302083
 37252/100000: episode: 604, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 19.853, mean reward: 0.685 [0.602, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.082, 10.438], loss: 0.001634, mae: 0.042704, mean_q: 1.308629
 37301/100000: episode: 605, duration: 0.260s, episode steps: 49, steps per second: 188, episode reward: 33.998, mean reward: 0.694 [0.622, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.407, 10.273], loss: 0.001575, mae: 0.042522, mean_q: 1.300667
 37322/100000: episode: 606, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 15.200, mean reward: 0.724 [0.669, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.440, 10.478], loss: 0.001535, mae: 0.043484, mean_q: 1.305132
 37349/100000: episode: 607, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 20.397, mean reward: 0.755 [0.643, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.484, 10.322], loss: 0.001540, mae: 0.043211, mean_q: 1.318890
 37376/100000: episode: 608, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 17.717, mean reward: 0.656 [0.523, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.235], loss: 0.001294, mae: 0.039572, mean_q: 1.315653
 37425/100000: episode: 609, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 31.957, mean reward: 0.652 [0.549, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.397, 10.294], loss: 0.001707, mae: 0.044256, mean_q: 1.303999
[Info] Complete ISplit Iteration
[Info] Levels: [1.3141526, 1.5107898, 1.5858127]
[Info] Cond. Prob: [0.1, 0.1, 0.42]
[Info] Error Prob: 0.004200000000000001

 37440/100000: episode: 610, duration: 4.405s, episode steps: 15, steps per second: 3, episode reward: 11.702, mean reward: 0.780 [0.718, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.581, 10.100], loss: 0.001540, mae: 0.043654, mean_q: 1.301755
 37540/100000: episode: 611, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.889, mean reward: 0.599 [0.517, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.928, 10.149], loss: 0.001787, mae: 0.044307, mean_q: 1.305151
 37640/100000: episode: 612, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.841, mean reward: 0.598 [0.498, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.960, 10.174], loss: 0.001819, mae: 0.044138, mean_q: 1.311069
 37740/100000: episode: 613, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.548, mean reward: 0.595 [0.503, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.314, 10.332], loss: 0.001666, mae: 0.043079, mean_q: 1.307560
 37840/100000: episode: 614, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 57.135, mean reward: 0.571 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.666, 10.098], loss: 0.001749, mae: 0.044999, mean_q: 1.308967
 37940/100000: episode: 615, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.302, mean reward: 0.583 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.635, 10.098], loss: 0.001641, mae: 0.043984, mean_q: 1.307311
 38040/100000: episode: 616, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.856, mean reward: 0.589 [0.501, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.656, 10.192], loss: 0.001828, mae: 0.044227, mean_q: 1.305754
 38140/100000: episode: 617, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.379, mean reward: 0.594 [0.499, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.535, 10.098], loss: 0.001557, mae: 0.042283, mean_q: 1.309407
 38240/100000: episode: 618, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 61.861, mean reward: 0.619 [0.521, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.512, 10.215], loss: 0.001816, mae: 0.043707, mean_q: 1.310241
 38340/100000: episode: 619, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.889, mean reward: 0.599 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.754, 10.098], loss: 0.001803, mae: 0.044213, mean_q: 1.304495
 38440/100000: episode: 620, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.393, mean reward: 0.584 [0.501, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.879, 10.098], loss: 0.001735, mae: 0.043466, mean_q: 1.305335
 38540/100000: episode: 621, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.733, mean reward: 0.587 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.360, 10.230], loss: 0.001846, mae: 0.043970, mean_q: 1.300757
 38640/100000: episode: 622, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 62.357, mean reward: 0.624 [0.515, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.957, 10.334], loss: 0.001758, mae: 0.045000, mean_q: 1.303373
 38740/100000: episode: 623, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.892, mean reward: 0.599 [0.505, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.289, 10.136], loss: 0.001540, mae: 0.042463, mean_q: 1.302241
 38840/100000: episode: 624, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.136, mean reward: 0.591 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.019, 10.176], loss: 0.002061, mae: 0.046694, mean_q: 1.297346
 38940/100000: episode: 625, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.556, mean reward: 0.576 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.545, 10.101], loss: 0.001902, mae: 0.045641, mean_q: 1.292786
 39040/100000: episode: 626, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 59.711, mean reward: 0.597 [0.510, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.009, 10.098], loss: 0.001761, mae: 0.044670, mean_q: 1.293313
 39140/100000: episode: 627, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.336, mean reward: 0.573 [0.500, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.214, 10.098], loss: 0.001713, mae: 0.044143, mean_q: 1.293914
 39240/100000: episode: 628, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.270, mean reward: 0.603 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.726, 10.104], loss: 0.001622, mae: 0.043475, mean_q: 1.292947
 39340/100000: episode: 629, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.489, mean reward: 0.575 [0.498, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.372, 10.137], loss: 0.001657, mae: 0.044045, mean_q: 1.297963
 39440/100000: episode: 630, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.777, mean reward: 0.588 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.896, 10.099], loss: 0.001778, mae: 0.044473, mean_q: 1.291625
 39540/100000: episode: 631, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.940, mean reward: 0.599 [0.498, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.887, 10.098], loss: 0.001780, mae: 0.044167, mean_q: 1.285334
 39640/100000: episode: 632, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.762, mean reward: 0.568 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.857, 10.098], loss: 0.001910, mae: 0.045539, mean_q: 1.284178
 39740/100000: episode: 633, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 58.831, mean reward: 0.588 [0.500, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.497, 10.100], loss: 0.001884, mae: 0.044726, mean_q: 1.276999
 39840/100000: episode: 634, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.886, mean reward: 0.599 [0.509, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.258, 10.289], loss: 0.001621, mae: 0.043122, mean_q: 1.273766
 39940/100000: episode: 635, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.692, mean reward: 0.587 [0.521, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.552, 10.169], loss: 0.002079, mae: 0.048157, mean_q: 1.268498
 40040/100000: episode: 636, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 56.987, mean reward: 0.570 [0.498, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.059, 10.098], loss: 0.001797, mae: 0.044656, mean_q: 1.261749
 40140/100000: episode: 637, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.400, mean reward: 0.584 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.522, 10.251], loss: 0.001500, mae: 0.042010, mean_q: 1.263525
 40240/100000: episode: 638, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.647, mean reward: 0.586 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.068, 10.098], loss: 0.001779, mae: 0.044111, mean_q: 1.255756
 40340/100000: episode: 639, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.722, mean reward: 0.587 [0.505, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.068, 10.418], loss: 0.001839, mae: 0.044633, mean_q: 1.247042
 40440/100000: episode: 640, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.434, mean reward: 0.584 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.001, 10.098], loss: 0.001855, mae: 0.045303, mean_q: 1.238573
 40540/100000: episode: 641, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.400, mean reward: 0.584 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.263, 10.173], loss: 0.001747, mae: 0.044578, mean_q: 1.240843
 40640/100000: episode: 642, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 61.590, mean reward: 0.616 [0.511, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.750, 10.098], loss: 0.001662, mae: 0.043184, mean_q: 1.239154
 40740/100000: episode: 643, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.126, mean reward: 0.581 [0.499, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.716, 10.233], loss: 0.001575, mae: 0.042792, mean_q: 1.239104
 40840/100000: episode: 644, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.316, mean reward: 0.573 [0.502, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.601, 10.148], loss: 0.001701, mae: 0.043206, mean_q: 1.232176
 40940/100000: episode: 645, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.072, mean reward: 0.581 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.768, 10.098], loss: 0.001612, mae: 0.043040, mean_q: 1.230554
 41040/100000: episode: 646, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.356, mean reward: 0.594 [0.499, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.460, 10.371], loss: 0.001931, mae: 0.045251, mean_q: 1.225072
 41140/100000: episode: 647, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.322, mean reward: 0.593 [0.516, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.541, 10.208], loss: 0.001740, mae: 0.043753, mean_q: 1.224461
 41240/100000: episode: 648, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 59.565, mean reward: 0.596 [0.507, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.617, 10.098], loss: 0.001732, mae: 0.045077, mean_q: 1.218667
 41340/100000: episode: 649, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.637, mean reward: 0.576 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.873, 10.098], loss: 0.001715, mae: 0.043624, mean_q: 1.215252
 41440/100000: episode: 650, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.593, mean reward: 0.586 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.563, 10.098], loss: 0.001769, mae: 0.044748, mean_q: 1.209402
 41540/100000: episode: 651, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.182, mean reward: 0.592 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.338, 10.182], loss: 0.001731, mae: 0.043756, mean_q: 1.202034
 41640/100000: episode: 652, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.470, mean reward: 0.595 [0.514, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.727, 10.098], loss: 0.001675, mae: 0.043676, mean_q: 1.197049
 41740/100000: episode: 653, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.208, mean reward: 0.582 [0.498, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.430, 10.319], loss: 0.001642, mae: 0.042685, mean_q: 1.193141
 41840/100000: episode: 654, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.800, mean reward: 0.588 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.742, 10.098], loss: 0.001743, mae: 0.043809, mean_q: 1.186132
 41940/100000: episode: 655, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 61.486, mean reward: 0.615 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.267, 10.098], loss: 0.001608, mae: 0.042374, mean_q: 1.187261
 42040/100000: episode: 656, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.126, mean reward: 0.581 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.001, 10.098], loss: 0.001722, mae: 0.043435, mean_q: 1.185439
 42140/100000: episode: 657, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.770, mean reward: 0.578 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.285, 10.098], loss: 0.001613, mae: 0.043333, mean_q: 1.177590
 42240/100000: episode: 658, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 60.985, mean reward: 0.610 [0.508, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.411, 10.098], loss: 0.001486, mae: 0.041704, mean_q: 1.176093
 42340/100000: episode: 659, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.513, mean reward: 0.585 [0.500, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.450, 10.098], loss: 0.001472, mae: 0.041815, mean_q: 1.174844
 42440/100000: episode: 660, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.298, mean reward: 0.573 [0.499, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.949, 10.098], loss: 0.001479, mae: 0.041721, mean_q: 1.167841
 42540/100000: episode: 661, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 60.097, mean reward: 0.601 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.693, 10.098], loss: 0.001483, mae: 0.042445, mean_q: 1.169435
 42640/100000: episode: 662, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 61.202, mean reward: 0.612 [0.513, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.275, 10.529], loss: 0.001468, mae: 0.042710, mean_q: 1.170895
 42740/100000: episode: 663, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.260, mean reward: 0.583 [0.504, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.515, 10.312], loss: 0.001368, mae: 0.040560, mean_q: 1.165044
 42840/100000: episode: 664, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.860, mean reward: 0.579 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.529, 10.101], loss: 0.001479, mae: 0.042448, mean_q: 1.166392
 42940/100000: episode: 665, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.836, mean reward: 0.578 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.426, 10.322], loss: 0.001306, mae: 0.040458, mean_q: 1.166675
 43040/100000: episode: 666, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.313, mean reward: 0.573 [0.511, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.915, 10.098], loss: 0.001349, mae: 0.040489, mean_q: 1.163959
 43140/100000: episode: 667, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 60.212, mean reward: 0.602 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.677, 10.194], loss: 0.001417, mae: 0.041165, mean_q: 1.164557
 43240/100000: episode: 668, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.534, mean reward: 0.575 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.613, 10.098], loss: 0.001442, mae: 0.041378, mean_q: 1.164105
 43340/100000: episode: 669, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 64.258, mean reward: 0.643 [0.512, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.269, 10.512], loss: 0.001425, mae: 0.041404, mean_q: 1.164213
 43440/100000: episode: 670, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 61.114, mean reward: 0.611 [0.506, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.708, 10.325], loss: 0.001420, mae: 0.041858, mean_q: 1.162872
 43540/100000: episode: 671, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.843, mean reward: 0.598 [0.519, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.704, 10.098], loss: 0.001372, mae: 0.040874, mean_q: 1.168005
 43640/100000: episode: 672, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 62.269, mean reward: 0.623 [0.506, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.470, 10.320], loss: 0.001546, mae: 0.043585, mean_q: 1.167696
 43740/100000: episode: 673, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.656, mean reward: 0.587 [0.512, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.911, 10.098], loss: 0.001381, mae: 0.040884, mean_q: 1.165656
 43840/100000: episode: 674, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 59.758, mean reward: 0.598 [0.515, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.402, 10.098], loss: 0.001332, mae: 0.040867, mean_q: 1.165488
 43940/100000: episode: 675, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 59.422, mean reward: 0.594 [0.498, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.887, 10.098], loss: 0.001390, mae: 0.041136, mean_q: 1.164516
 44040/100000: episode: 676, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.020, mean reward: 0.600 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.440, 10.172], loss: 0.001378, mae: 0.041072, mean_q: 1.168329
 44140/100000: episode: 677, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.594, mean reward: 0.576 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.054, 10.098], loss: 0.001356, mae: 0.040616, mean_q: 1.166273
 44240/100000: episode: 678, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 57.243, mean reward: 0.572 [0.507, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.799, 10.098], loss: 0.001466, mae: 0.042034, mean_q: 1.165726
 44340/100000: episode: 679, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.220, mean reward: 0.582 [0.503, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.761, 10.142], loss: 0.001349, mae: 0.040827, mean_q: 1.165688
 44440/100000: episode: 680, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 62.724, mean reward: 0.627 [0.506, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.021, 10.447], loss: 0.001411, mae: 0.041193, mean_q: 1.166745
 44540/100000: episode: 681, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.529, mean reward: 0.575 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.498, 10.218], loss: 0.001456, mae: 0.042026, mean_q: 1.165743
 44640/100000: episode: 682, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.111, mean reward: 0.591 [0.512, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.129, 10.277], loss: 0.001384, mae: 0.040914, mean_q: 1.166770
 44740/100000: episode: 683, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 60.344, mean reward: 0.603 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.950, 10.345], loss: 0.001361, mae: 0.041077, mean_q: 1.167311
 44840/100000: episode: 684, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 61.771, mean reward: 0.618 [0.523, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.023, 10.098], loss: 0.001371, mae: 0.041168, mean_q: 1.167753
 44940/100000: episode: 685, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 61.043, mean reward: 0.610 [0.507, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.750, 10.098], loss: 0.001363, mae: 0.040604, mean_q: 1.169059
 45040/100000: episode: 686, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.531, mean reward: 0.585 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.943, 10.098], loss: 0.001330, mae: 0.040020, mean_q: 1.169444
 45140/100000: episode: 687, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 57.668, mean reward: 0.577 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.500, 10.163], loss: 0.001385, mae: 0.040970, mean_q: 1.169762
 45240/100000: episode: 688, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 60.041, mean reward: 0.600 [0.508, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.326, 10.466], loss: 0.001521, mae: 0.042691, mean_q: 1.170690
 45340/100000: episode: 689, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 56.631, mean reward: 0.566 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.567, 10.098], loss: 0.001403, mae: 0.041317, mean_q: 1.167905
 45440/100000: episode: 690, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 61.100, mean reward: 0.611 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.731, 10.098], loss: 0.001374, mae: 0.041309, mean_q: 1.168624
 45540/100000: episode: 691, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 59.792, mean reward: 0.598 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.705, 10.310], loss: 0.001448, mae: 0.042212, mean_q: 1.172133
 45640/100000: episode: 692, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 59.185, mean reward: 0.592 [0.508, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.751, 10.302], loss: 0.001241, mae: 0.039286, mean_q: 1.169335
 45740/100000: episode: 693, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.391, mean reward: 0.594 [0.512, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.734, 10.348], loss: 0.001395, mae: 0.041763, mean_q: 1.173237
 45840/100000: episode: 694, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.909, mean reward: 0.599 [0.507, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.169, 10.318], loss: 0.001365, mae: 0.040417, mean_q: 1.171183
 45940/100000: episode: 695, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 57.820, mean reward: 0.578 [0.503, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.149, 10.159], loss: 0.001528, mae: 0.043128, mean_q: 1.173024
 46040/100000: episode: 696, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.221, mean reward: 0.592 [0.509, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.725, 10.212], loss: 0.001440, mae: 0.041791, mean_q: 1.173380
 46140/100000: episode: 697, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.412, mean reward: 0.594 [0.511, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.956, 10.098], loss: 0.001313, mae: 0.040616, mean_q: 1.173651
 46240/100000: episode: 698, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.661, mean reward: 0.587 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.714, 10.098], loss: 0.001386, mae: 0.041329, mean_q: 1.173355
 46340/100000: episode: 699, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.958, mean reward: 0.590 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.810, 10.098], loss: 0.001341, mae: 0.040551, mean_q: 1.170469
 46440/100000: episode: 700, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.724, mean reward: 0.597 [0.514, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.792, 10.098], loss: 0.001415, mae: 0.041590, mean_q: 1.173237
 46540/100000: episode: 701, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.448, mean reward: 0.584 [0.502, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.311, 10.134], loss: 0.001488, mae: 0.042672, mean_q: 1.172184
 46640/100000: episode: 702, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.357, mean reward: 0.574 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.177, 10.249], loss: 0.001436, mae: 0.042395, mean_q: 1.174832
 46740/100000: episode: 703, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.290, mean reward: 0.603 [0.505, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.315, 10.098], loss: 0.001448, mae: 0.041423, mean_q: 1.174623
 46840/100000: episode: 704, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 56.764, mean reward: 0.568 [0.508, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.438, 10.162], loss: 0.001308, mae: 0.040034, mean_q: 1.174302
 46940/100000: episode: 705, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 59.186, mean reward: 0.592 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.640, 10.098], loss: 0.001395, mae: 0.041367, mean_q: 1.173395
 47040/100000: episode: 706, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 57.819, mean reward: 0.578 [0.510, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.304, 10.305], loss: 0.001377, mae: 0.041398, mean_q: 1.170357
 47140/100000: episode: 707, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 61.064, mean reward: 0.611 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.273, 10.269], loss: 0.001428, mae: 0.041604, mean_q: 1.170131
 47240/100000: episode: 708, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 61.541, mean reward: 0.615 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.786, 10.112], loss: 0.001360, mae: 0.040924, mean_q: 1.173205
 47340/100000: episode: 709, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.139, mean reward: 0.591 [0.512, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.864, 10.122], loss: 0.001313, mae: 0.039552, mean_q: 1.168904
[Info] 1-TH LEVEL FOUND: 1.4132715463638306, Considering 10/90 traces
 47440/100000: episode: 710, duration: 4.673s, episode steps: 100, steps per second: 21, episode reward: 57.627, mean reward: 0.576 [0.503, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.106, 10.121], loss: 0.001438, mae: 0.041305, mean_q: 1.174167
 47464/100000: episode: 711, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 15.061, mean reward: 0.628 [0.575, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.324], loss: 0.001475, mae: 0.042791, mean_q: 1.174388
 47471/100000: episode: 712, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 4.719, mean reward: 0.674 [0.639, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.397], loss: 0.001593, mae: 0.044952, mean_q: 1.176362
 47480/100000: episode: 713, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 6.125, mean reward: 0.681 [0.616, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.523], loss: 0.001269, mae: 0.039296, mean_q: 1.166465
 47577/100000: episode: 714, duration: 0.544s, episode steps: 97, steps per second: 178, episode reward: 58.504, mean reward: 0.603 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.215, 10.241], loss: 0.001384, mae: 0.040924, mean_q: 1.175111
 47601/100000: episode: 715, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 16.447, mean reward: 0.685 [0.606, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.456, 10.434], loss: 0.001327, mae: 0.039355, mean_q: 1.173806
 47623/100000: episode: 716, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 16.087, mean reward: 0.731 [0.682, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-1.182, 10.471], loss: 0.001300, mae: 0.040137, mean_q: 1.175751
 47632/100000: episode: 717, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 6.133, mean reward: 0.681 [0.636, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.343], loss: 0.001376, mae: 0.039499, mean_q: 1.163690
 47656/100000: episode: 718, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 17.470, mean reward: 0.728 [0.626, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.871, 10.437], loss: 0.001502, mae: 0.044095, mean_q: 1.166600
 47679/100000: episode: 719, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 16.493, mean reward: 0.717 [0.653, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.470], loss: 0.001718, mae: 0.044398, mean_q: 1.179403
 47702/100000: episode: 720, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 15.786, mean reward: 0.686 [0.630, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.407, 10.372], loss: 0.001459, mae: 0.042178, mean_q: 1.178275
 47799/100000: episode: 721, duration: 0.519s, episode steps: 97, steps per second: 187, episode reward: 55.662, mean reward: 0.574 [0.498, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.632, 10.319], loss: 0.001482, mae: 0.041950, mean_q: 1.175854
 47808/100000: episode: 722, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 6.614, mean reward: 0.735 [0.638, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.639, 10.539], loss: 0.001607, mae: 0.044400, mean_q: 1.185947
 47905/100000: episode: 723, duration: 0.542s, episode steps: 97, steps per second: 179, episode reward: 55.807, mean reward: 0.575 [0.502, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.485 [-0.621, 10.137], loss: 0.001813, mae: 0.045319, mean_q: 1.175026
 47964/100000: episode: 724, duration: 0.307s, episode steps: 59, steps per second: 192, episode reward: 38.275, mean reward: 0.649 [0.531, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.828 [-1.252, 10.100], loss: 0.001695, mae: 0.044081, mean_q: 1.171549
 47988/100000: episode: 725, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 16.610, mean reward: 0.692 [0.639, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.364, 10.445], loss: 0.001616, mae: 0.042839, mean_q: 1.169550
 48047/100000: episode: 726, duration: 0.318s, episode steps: 59, steps per second: 186, episode reward: 37.056, mean reward: 0.628 [0.504, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.824 [-1.542, 10.100], loss: 0.001393, mae: 0.040797, mean_q: 1.180889
 48106/100000: episode: 727, duration: 0.330s, episode steps: 59, steps per second: 179, episode reward: 38.542, mean reward: 0.653 [0.536, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.832 [-0.586, 10.100], loss: 0.001445, mae: 0.041217, mean_q: 1.180326
 48128/100000: episode: 728, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 15.438, mean reward: 0.702 [0.615, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.445, 10.368], loss: 0.001609, mae: 0.043823, mean_q: 1.180288
 48152/100000: episode: 729, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 16.109, mean reward: 0.671 [0.604, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.435, 10.414], loss: 0.001509, mae: 0.041148, mean_q: 1.184429
 48175/100000: episode: 730, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 16.038, mean reward: 0.697 [0.637, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.398], loss: 0.001663, mae: 0.043928, mean_q: 1.179325
 48272/100000: episode: 731, duration: 0.535s, episode steps: 97, steps per second: 181, episode reward: 58.199, mean reward: 0.600 [0.512, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-0.570, 10.151], loss: 0.001518, mae: 0.042338, mean_q: 1.182032
 48296/100000: episode: 732, duration: 0.141s, episode steps: 24, steps per second: 171, episode reward: 17.099, mean reward: 0.712 [0.605, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.127, 10.452], loss: 0.001582, mae: 0.042533, mean_q: 1.177657
 48393/100000: episode: 733, duration: 0.506s, episode steps: 97, steps per second: 192, episode reward: 58.591, mean reward: 0.604 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.442, 10.100], loss: 0.001604, mae: 0.042882, mean_q: 1.183412
 48401/100000: episode: 734, duration: 0.057s, episode steps: 8, steps per second: 142, episode reward: 5.150, mean reward: 0.644 [0.613, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.351], loss: 0.001491, mae: 0.041633, mean_q: 1.182109
 48411/100000: episode: 735, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 6.874, mean reward: 0.687 [0.650, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.407, 10.401], loss: 0.001342, mae: 0.041392, mean_q: 1.188527
 48508/100000: episode: 736, duration: 0.554s, episode steps: 97, steps per second: 175, episode reward: 58.586, mean reward: 0.604 [0.512, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-0.442, 10.254], loss: 0.001563, mae: 0.042357, mean_q: 1.180778
 48518/100000: episode: 737, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 7.174, mean reward: 0.717 [0.672, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.420], loss: 0.001397, mae: 0.040954, mean_q: 1.193850
 48542/100000: episode: 738, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 16.038, mean reward: 0.668 [0.613, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.431], loss: 0.001809, mae: 0.045661, mean_q: 1.185216
 48564/100000: episode: 739, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 17.019, mean reward: 0.774 [0.657, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.078, 10.491], loss: 0.001547, mae: 0.041963, mean_q: 1.172640
 48571/100000: episode: 740, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.670, mean reward: 0.667 [0.635, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.367], loss: 0.001621, mae: 0.044162, mean_q: 1.195163
 48668/100000: episode: 741, duration: 0.520s, episode steps: 97, steps per second: 187, episode reward: 57.271, mean reward: 0.590 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-1.434, 10.100], loss: 0.001633, mae: 0.043132, mean_q: 1.181173
 48765/100000: episode: 742, duration: 0.527s, episode steps: 97, steps per second: 184, episode reward: 56.387, mean reward: 0.581 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.485 [-0.758, 10.100], loss: 0.001683, mae: 0.043160, mean_q: 1.181291
 48824/100000: episode: 743, duration: 0.318s, episode steps: 59, steps per second: 185, episode reward: 35.738, mean reward: 0.606 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.845 [-0.298, 10.100], loss: 0.001612, mae: 0.043480, mean_q: 1.182465
 48831/100000: episode: 744, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 5.144, mean reward: 0.735 [0.673, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.452], loss: 0.002121, mae: 0.049468, mean_q: 1.191459
 48838/100000: episode: 745, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 4.858, mean reward: 0.694 [0.676, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.458], loss: 0.001321, mae: 0.038951, mean_q: 1.177529
 48847/100000: episode: 746, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 6.201, mean reward: 0.689 [0.641, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.379], loss: 0.001263, mae: 0.037057, mean_q: 1.165029
 48854/100000: episode: 747, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 4.677, mean reward: 0.668 [0.624, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.252], loss: 0.001941, mae: 0.044355, mean_q: 1.187873
 48876/100000: episode: 748, duration: 0.110s, episode steps: 22, steps per second: 199, episode reward: 16.101, mean reward: 0.732 [0.660, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.752, 10.417], loss: 0.001536, mae: 0.041077, mean_q: 1.188274
 48885/100000: episode: 749, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 5.618, mean reward: 0.624 [0.603, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.331], loss: 0.001622, mae: 0.042594, mean_q: 1.183174
 48982/100000: episode: 750, duration: 0.543s, episode steps: 97, steps per second: 179, episode reward: 57.704, mean reward: 0.595 [0.504, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-0.666, 10.401], loss: 0.001484, mae: 0.041201, mean_q: 1.185443
 48989/100000: episode: 751, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 4.668, mean reward: 0.667 [0.611, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.365], loss: 0.001508, mae: 0.040779, mean_q: 1.181703
 49013/100000: episode: 752, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 16.303, mean reward: 0.679 [0.611, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.128, 10.389], loss: 0.001519, mae: 0.042275, mean_q: 1.180174
 49035/100000: episode: 753, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 16.327, mean reward: 0.742 [0.673, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.063, 10.435], loss: 0.001352, mae: 0.040415, mean_q: 1.185785
 49057/100000: episode: 754, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 16.618, mean reward: 0.755 [0.677, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.484], loss: 0.001736, mae: 0.043279, mean_q: 1.184857
 49154/100000: episode: 755, duration: 0.505s, episode steps: 97, steps per second: 192, episode reward: 55.020, mean reward: 0.567 [0.506, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-0.991, 10.100], loss: 0.001930, mae: 0.046606, mean_q: 1.184387
 49164/100000: episode: 756, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 7.272, mean reward: 0.727 [0.651, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.435], loss: 0.001560, mae: 0.042607, mean_q: 1.189463
 49171/100000: episode: 757, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 4.818, mean reward: 0.688 [0.667, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.347], loss: 0.001472, mae: 0.041413, mean_q: 1.197802
 49193/100000: episode: 758, duration: 0.140s, episode steps: 22, steps per second: 157, episode reward: 16.089, mean reward: 0.731 [0.583, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.445], loss: 0.001542, mae: 0.042643, mean_q: 1.183544
 49215/100000: episode: 759, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 18.206, mean reward: 0.828 [0.719, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.676, 10.588], loss: 0.001690, mae: 0.045630, mean_q: 1.185912
 49237/100000: episode: 760, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 17.800, mean reward: 0.809 [0.735, 0.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.546], loss: 0.001465, mae: 0.040402, mean_q: 1.195022
 49296/100000: episode: 761, duration: 0.322s, episode steps: 59, steps per second: 183, episode reward: 36.620, mean reward: 0.621 [0.532, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-0.487, 10.100], loss: 0.001562, mae: 0.043463, mean_q: 1.191208
 49305/100000: episode: 762, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 5.999, mean reward: 0.667 [0.641, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.399], loss: 0.002028, mae: 0.046738, mean_q: 1.196349
 49312/100000: episode: 763, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 5.061, mean reward: 0.723 [0.671, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.531], loss: 0.002004, mae: 0.048777, mean_q: 1.193675
 49336/100000: episode: 764, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 15.830, mean reward: 0.660 [0.604, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.334], loss: 0.001771, mae: 0.045296, mean_q: 1.189634
 49343/100000: episode: 765, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 4.520, mean reward: 0.646 [0.615, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.380], loss: 0.001779, mae: 0.043004, mean_q: 1.179553
 49366/100000: episode: 766, duration: 0.141s, episode steps: 23, steps per second: 163, episode reward: 16.168, mean reward: 0.703 [0.648, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.459], loss: 0.001481, mae: 0.040921, mean_q: 1.191838
 49425/100000: episode: 767, duration: 0.323s, episode steps: 59, steps per second: 182, episode reward: 38.566, mean reward: 0.654 [0.577, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.825 [-0.478, 10.100], loss: 0.001701, mae: 0.043582, mean_q: 1.191975
 49434/100000: episode: 768, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 5.913, mean reward: 0.657 [0.606, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.078, 10.374], loss: 0.001360, mae: 0.040337, mean_q: 1.209361
 49531/100000: episode: 769, duration: 0.522s, episode steps: 97, steps per second: 186, episode reward: 56.591, mean reward: 0.583 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.500, 10.145], loss: 0.001504, mae: 0.042027, mean_q: 1.194067
 49555/100000: episode: 770, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 14.340, mean reward: 0.598 [0.536, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.364, 10.244], loss: 0.001592, mae: 0.042579, mean_q: 1.187759
 49652/100000: episode: 771, duration: 0.510s, episode steps: 97, steps per second: 190, episode reward: 56.612, mean reward: 0.584 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.478 [-0.602, 10.230], loss: 0.001428, mae: 0.040953, mean_q: 1.194731
 49749/100000: episode: 772, duration: 0.529s, episode steps: 97, steps per second: 183, episode reward: 56.763, mean reward: 0.585 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-0.530, 10.100], loss: 0.001503, mae: 0.040966, mean_q: 1.191694
 49771/100000: episode: 773, duration: 0.109s, episode steps: 22, steps per second: 201, episode reward: 16.272, mean reward: 0.740 [0.674, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.984, 10.549], loss: 0.001515, mae: 0.042127, mean_q: 1.192767
 49779/100000: episode: 774, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 5.521, mean reward: 0.690 [0.677, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.416], loss: 0.001334, mae: 0.041397, mean_q: 1.198008
 49802/100000: episode: 775, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 17.064, mean reward: 0.742 [0.642, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.475, 10.633], loss: 0.001499, mae: 0.042182, mean_q: 1.189012
 49899/100000: episode: 776, duration: 0.520s, episode steps: 97, steps per second: 186, episode reward: 57.820, mean reward: 0.596 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-0.385, 10.208], loss: 0.001443, mae: 0.041428, mean_q: 1.194878
 49921/100000: episode: 777, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 15.447, mean reward: 0.702 [0.616, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.216, 10.362], loss: 0.001431, mae: 0.041059, mean_q: 1.192545
 49931/100000: episode: 778, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 6.736, mean reward: 0.674 [0.641, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.323], loss: 0.001352, mae: 0.038763, mean_q: 1.186584
 49940/100000: episode: 779, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 5.974, mean reward: 0.664 [0.637, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.331], loss: 0.001595, mae: 0.042577, mean_q: 1.187423
 49999/100000: episode: 780, duration: 0.315s, episode steps: 59, steps per second: 187, episode reward: 38.214, mean reward: 0.648 [0.566, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.828 [-0.249, 10.100], loss: 0.001482, mae: 0.041592, mean_q: 1.195410
 50021/100000: episode: 781, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 16.770, mean reward: 0.762 [0.687, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.108, 10.515], loss: 0.001489, mae: 0.041061, mean_q: 1.188862
 50080/100000: episode: 782, duration: 0.334s, episode steps: 59, steps per second: 176, episode reward: 37.664, mean reward: 0.638 [0.516, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-0.622, 10.100], loss: 0.001506, mae: 0.040868, mean_q: 1.198575
 50090/100000: episode: 783, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 6.783, mean reward: 0.678 [0.628, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.367], loss: 0.001746, mae: 0.044434, mean_q: 1.188300
 50098/100000: episode: 784, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 6.102, mean reward: 0.763 [0.685, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.629], loss: 0.001327, mae: 0.038529, mean_q: 1.203426
 50107/100000: episode: 785, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 5.964, mean reward: 0.663 [0.632, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.098, 10.341], loss: 0.001446, mae: 0.040302, mean_q: 1.215284
 50129/100000: episode: 786, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 15.800, mean reward: 0.718 [0.653, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.702, 10.395], loss: 0.001403, mae: 0.040309, mean_q: 1.205095
 50226/100000: episode: 787, duration: 0.500s, episode steps: 97, steps per second: 194, episode reward: 58.674, mean reward: 0.605 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.562, 10.351], loss: 0.001561, mae: 0.041947, mean_q: 1.200999
 50248/100000: episode: 788, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 16.309, mean reward: 0.741 [0.699, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.427, 10.482], loss: 0.001382, mae: 0.039127, mean_q: 1.198379
 50272/100000: episode: 789, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 15.900, mean reward: 0.663 [0.575, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.294], loss: 0.001474, mae: 0.040997, mean_q: 1.199701
 50281/100000: episode: 790, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 5.992, mean reward: 0.666 [0.623, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.410], loss: 0.001632, mae: 0.041511, mean_q: 1.193166
 50378/100000: episode: 791, duration: 0.550s, episode steps: 97, steps per second: 176, episode reward: 57.625, mean reward: 0.594 [0.506, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-0.783, 10.100], loss: 0.001596, mae: 0.042177, mean_q: 1.201981
 50385/100000: episode: 792, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.141, mean reward: 0.734 [0.655, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.562], loss: 0.001540, mae: 0.043085, mean_q: 1.209067
 50444/100000: episode: 793, duration: 0.316s, episode steps: 59, steps per second: 186, episode reward: 40.786, mean reward: 0.691 [0.566, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.830 [-0.804, 10.100], loss: 0.001698, mae: 0.043661, mean_q: 1.203453
 50451/100000: episode: 794, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 4.790, mean reward: 0.684 [0.638, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.510, 10.403], loss: 0.001761, mae: 0.043799, mean_q: 1.182341
 50475/100000: episode: 795, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 15.491, mean reward: 0.645 [0.573, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.228, 10.326], loss: 0.001692, mae: 0.045154, mean_q: 1.211167
 50483/100000: episode: 796, duration: 0.061s, episode steps: 8, steps per second: 132, episode reward: 5.410, mean reward: 0.676 [0.641, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.396], loss: 0.001397, mae: 0.039023, mean_q: 1.219452
 50491/100000: episode: 797, duration: 0.063s, episode steps: 8, steps per second: 127, episode reward: 5.282, mean reward: 0.660 [0.600, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.410], loss: 0.001793, mae: 0.045468, mean_q: 1.214642
 50501/100000: episode: 798, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 7.096, mean reward: 0.710 [0.649, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.737, 10.338], loss: 0.001401, mae: 0.040165, mean_q: 1.200165
 50509/100000: episode: 799, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 5.172, mean reward: 0.646 [0.569, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.231], loss: 0.001599, mae: 0.043113, mean_q: 1.214605
[Info] 2-TH LEVEL FOUND: 1.5509663820266724, Considering 10/90 traces
 50519/100000: episode: 800, duration: 4.187s, episode steps: 10, steps per second: 2, episode reward: 6.673, mean reward: 0.667 [0.619, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.105, 10.363], loss: 0.001538, mae: 0.043098, mean_q: 1.208553
 50535/100000: episode: 801, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 12.412, mean reward: 0.776 [0.654, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.315, 10.502], loss: 0.001359, mae: 0.039198, mean_q: 1.207670
 50591/100000: episode: 802, duration: 0.301s, episode steps: 56, steps per second: 186, episode reward: 36.603, mean reward: 0.654 [0.592, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.509, 10.100], loss: 0.001525, mae: 0.042047, mean_q: 1.205067
 50607/100000: episode: 803, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 11.531, mean reward: 0.721 [0.634, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.358], loss: 0.001768, mae: 0.043834, mean_q: 1.213962
 50623/100000: episode: 804, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 11.765, mean reward: 0.735 [0.683, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.473, 10.447], loss: 0.001857, mae: 0.045276, mean_q: 1.210638
 50637/100000: episode: 805, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 10.313, mean reward: 0.737 [0.700, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.051, 10.489], loss: 0.001529, mae: 0.041116, mean_q: 1.221525
 50654/100000: episode: 806, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 13.310, mean reward: 0.783 [0.671, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.382, 10.416], loss: 0.001548, mae: 0.043286, mean_q: 1.210463
 50670/100000: episode: 807, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 14.203, mean reward: 0.888 [0.808, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.646], loss: 0.001405, mae: 0.038656, mean_q: 1.220962
 50726/100000: episode: 808, duration: 0.304s, episode steps: 56, steps per second: 185, episode reward: 37.324, mean reward: 0.667 [0.510, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.090, 10.195], loss: 0.001307, mae: 0.038903, mean_q: 1.218394
 50782/100000: episode: 809, duration: 0.294s, episode steps: 56, steps per second: 191, episode reward: 34.326, mean reward: 0.613 [0.522, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-1.034, 10.145], loss: 0.001454, mae: 0.041274, mean_q: 1.214934
 50798/100000: episode: 810, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 12.068, mean reward: 0.754 [0.717, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.231, 10.488], loss: 0.001446, mae: 0.041346, mean_q: 1.222848
 50814/100000: episode: 811, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 13.149, mean reward: 0.822 [0.774, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.541], loss: 0.001789, mae: 0.046115, mean_q: 1.221569
 50830/100000: episode: 812, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 14.001, mean reward: 0.875 [0.757, 0.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.630], loss: 0.001519, mae: 0.041330, mean_q: 1.224687
 50846/100000: episode: 813, duration: 0.084s, episode steps: 16, steps per second: 189, episode reward: 12.242, mean reward: 0.765 [0.719, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.297, 10.474], loss: 0.001698, mae: 0.043499, mean_q: 1.215255
 50862/100000: episode: 814, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 11.318, mean reward: 0.707 [0.593, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.039, 10.313], loss: 0.001568, mae: 0.043067, mean_q: 1.203245
 50878/100000: episode: 815, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 11.587, mean reward: 0.724 [0.678, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.411], loss: 0.001416, mae: 0.039843, mean_q: 1.224448
 50934/100000: episode: 816, duration: 0.303s, episode steps: 56, steps per second: 185, episode reward: 35.516, mean reward: 0.634 [0.526, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-1.248, 10.126], loss: 0.001252, mae: 0.038377, mean_q: 1.219440
 50990/100000: episode: 817, duration: 0.294s, episode steps: 56, steps per second: 190, episode reward: 39.629, mean reward: 0.708 [0.606, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.376, 10.100], loss: 0.001657, mae: 0.044036, mean_q: 1.219615
 51006/100000: episode: 818, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 12.773, mean reward: 0.798 [0.736, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.132, 10.506], loss: 0.001776, mae: 0.044417, mean_q: 1.220299
 51062/100000: episode: 819, duration: 0.299s, episode steps: 56, steps per second: 188, episode reward: 39.454, mean reward: 0.705 [0.578, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.332, 10.100], loss: 0.001582, mae: 0.041999, mean_q: 1.226394
 51118/100000: episode: 820, duration: 0.310s, episode steps: 56, steps per second: 181, episode reward: 37.863, mean reward: 0.676 [0.592, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-0.748, 10.100], loss: 0.001543, mae: 0.041825, mean_q: 1.219677
 51128/100000: episode: 821, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 7.453, mean reward: 0.745 [0.692, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.765, 10.485], loss: 0.001389, mae: 0.041111, mean_q: 1.239566
 51140/100000: episode: 822, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 10.154, mean reward: 0.846 [0.719, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.628], loss: 0.001309, mae: 0.039890, mean_q: 1.236662
 51156/100000: episode: 823, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 12.555, mean reward: 0.785 [0.728, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.228, 10.440], loss: 0.001270, mae: 0.038542, mean_q: 1.229941
 51172/100000: episode: 824, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 11.615, mean reward: 0.726 [0.698, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.484], loss: 0.001230, mae: 0.038082, mean_q: 1.230301
 51186/100000: episode: 825, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 12.073, mean reward: 0.862 [0.816, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.081, 10.730], loss: 0.001120, mae: 0.035887, mean_q: 1.244956
 51242/100000: episode: 826, duration: 0.303s, episode steps: 56, steps per second: 185, episode reward: 36.766, mean reward: 0.657 [0.520, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-1.576, 10.190], loss: 0.001450, mae: 0.041509, mean_q: 1.240270
 51258/100000: episode: 827, duration: 0.105s, episode steps: 16, steps per second: 153, episode reward: 12.890, mean reward: 0.806 [0.758, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.039, 10.511], loss: 0.001487, mae: 0.040660, mean_q: 1.228968
 51270/100000: episode: 828, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 10.161, mean reward: 0.847 [0.730, 0.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.730], loss: 0.001537, mae: 0.043114, mean_q: 1.238112
 51284/100000: episode: 829, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 11.837, mean reward: 0.846 [0.780, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.383, 10.646], loss: 0.001642, mae: 0.043380, mean_q: 1.241456
 51300/100000: episode: 830, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 13.421, mean reward: 0.839 [0.721, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.436, 10.626], loss: 0.001336, mae: 0.040679, mean_q: 1.244802
 51316/100000: episode: 831, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 12.678, mean reward: 0.792 [0.721, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.716, 10.476], loss: 0.001763, mae: 0.044628, mean_q: 1.242573
 51326/100000: episode: 832, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 8.145, mean reward: 0.815 [0.733, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.565], loss: 0.001666, mae: 0.043837, mean_q: 1.255752
 51342/100000: episode: 833, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 11.310, mean reward: 0.707 [0.631, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.452], loss: 0.001319, mae: 0.040247, mean_q: 1.236501
 51358/100000: episode: 834, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 12.825, mean reward: 0.802 [0.710, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.636, 10.441], loss: 0.001377, mae: 0.040360, mean_q: 1.234615
 51374/100000: episode: 835, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 12.447, mean reward: 0.778 [0.716, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.129, 10.531], loss: 0.001605, mae: 0.042317, mean_q: 1.240945
 51384/100000: episode: 836, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 7.200, mean reward: 0.720 [0.675, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.480], loss: 0.001250, mae: 0.038639, mean_q: 1.237516
 51400/100000: episode: 837, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 13.012, mean reward: 0.813 [0.760, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-1.102, 10.619], loss: 0.001124, mae: 0.037405, mean_q: 1.249418
 51416/100000: episode: 838, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 11.332, mean reward: 0.708 [0.590, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.709, 10.312], loss: 0.001551, mae: 0.042129, mean_q: 1.247413
 51430/100000: episode: 839, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 10.335, mean reward: 0.738 [0.665, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.441], loss: 0.001573, mae: 0.042882, mean_q: 1.244813
 51486/100000: episode: 840, duration: 0.311s, episode steps: 56, steps per second: 180, episode reward: 39.127, mean reward: 0.699 [0.626, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-0.553, 10.100], loss: 0.001317, mae: 0.039569, mean_q: 1.250263
 51502/100000: episode: 841, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 12.981, mean reward: 0.811 [0.688, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.104, 10.568], loss: 0.001636, mae: 0.041513, mean_q: 1.248212
 51516/100000: episode: 842, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 11.219, mean reward: 0.801 [0.738, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.318, 10.484], loss: 0.001506, mae: 0.043160, mean_q: 1.242280
 51528/100000: episode: 843, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 9.575, mean reward: 0.798 [0.712, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.505], loss: 0.001335, mae: 0.038854, mean_q: 1.257083
 51540/100000: episode: 844, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 9.472, mean reward: 0.789 [0.748, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.129, 10.518], loss: 0.001213, mae: 0.038630, mean_q: 1.235521
 51550/100000: episode: 845, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 7.678, mean reward: 0.768 [0.707, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.502], loss: 0.001363, mae: 0.040055, mean_q: 1.251845
 51606/100000: episode: 846, duration: 0.319s, episode steps: 56, steps per second: 176, episode reward: 36.347, mean reward: 0.649 [0.523, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-1.585, 10.100], loss: 0.001456, mae: 0.041616, mean_q: 1.257912
 51662/100000: episode: 847, duration: 0.339s, episode steps: 56, steps per second: 165, episode reward: 35.977, mean reward: 0.642 [0.502, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.648, 10.100], loss: 0.001333, mae: 0.040363, mean_q: 1.253508
 51679/100000: episode: 848, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 13.253, mean reward: 0.780 [0.745, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.069, 10.540], loss: 0.001472, mae: 0.040831, mean_q: 1.258520
 51691/100000: episode: 849, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 9.834, mean reward: 0.820 [0.737, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.567], loss: 0.001711, mae: 0.043776, mean_q: 1.257858
 51703/100000: episode: 850, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 9.077, mean reward: 0.756 [0.727, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.705, 10.480], loss: 0.001309, mae: 0.038947, mean_q: 1.256489
 51719/100000: episode: 851, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 12.243, mean reward: 0.765 [0.692, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.075, 10.441], loss: 0.001505, mae: 0.042690, mean_q: 1.265124
 51733/100000: episode: 852, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 11.601, mean reward: 0.829 [0.723, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.912, 10.509], loss: 0.001776, mae: 0.046505, mean_q: 1.276502
 51745/100000: episode: 853, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 9.129, mean reward: 0.761 [0.713, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.685, 10.533], loss: 0.001430, mae: 0.043318, mean_q: 1.252775
 51801/100000: episode: 854, duration: 0.306s, episode steps: 56, steps per second: 183, episode reward: 42.079, mean reward: 0.751 [0.666, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.836 [-1.082, 10.100], loss: 0.001542, mae: 0.042168, mean_q: 1.253764
 51817/100000: episode: 855, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 12.706, mean reward: 0.794 [0.746, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.537], loss: 0.001419, mae: 0.041687, mean_q: 1.251999
 51833/100000: episode: 856, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 12.389, mean reward: 0.774 [0.629, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.154, 10.398], loss: 0.001608, mae: 0.043678, mean_q: 1.269948
 51847/100000: episode: 857, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 11.216, mean reward: 0.801 [0.739, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.548], loss: 0.001452, mae: 0.041302, mean_q: 1.278623
 51863/100000: episode: 858, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 13.014, mean reward: 0.813 [0.758, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.514], loss: 0.001525, mae: 0.040891, mean_q: 1.278958
 51873/100000: episode: 859, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 7.529, mean reward: 0.753 [0.709, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.542], loss: 0.001608, mae: 0.043210, mean_q: 1.271984
 51885/100000: episode: 860, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 8.973, mean reward: 0.748 [0.680, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.494], loss: 0.001466, mae: 0.041888, mean_q: 1.270271
 51901/100000: episode: 861, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 13.412, mean reward: 0.838 [0.794, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.272, 10.610], loss: 0.001668, mae: 0.045230, mean_q: 1.279864
 51915/100000: episode: 862, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 10.736, mean reward: 0.767 [0.732, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.512], loss: 0.001359, mae: 0.041280, mean_q: 1.279618
 51971/100000: episode: 863, duration: 0.288s, episode steps: 56, steps per second: 194, episode reward: 35.952, mean reward: 0.642 [0.508, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.276, 10.100], loss: 0.001370, mae: 0.041151, mean_q: 1.279454
 51987/100000: episode: 864, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 13.572, mean reward: 0.848 [0.807, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.610], loss: 0.001491, mae: 0.043144, mean_q: 1.282183
 52003/100000: episode: 865, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 12.931, mean reward: 0.808 [0.717, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.551], loss: 0.001674, mae: 0.046405, mean_q: 1.284707
 52059/100000: episode: 866, duration: 0.308s, episode steps: 56, steps per second: 182, episode reward: 41.454, mean reward: 0.740 [0.634, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-0.993, 10.100], loss: 0.001232, mae: 0.038765, mean_q: 1.278054
 52075/100000: episode: 867, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 11.899, mean reward: 0.744 [0.692, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.414], loss: 0.001342, mae: 0.040316, mean_q: 1.275503
 52091/100000: episode: 868, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 12.038, mean reward: 0.752 [0.656, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.538, 10.400], loss: 0.001214, mae: 0.039506, mean_q: 1.277447
 52147/100000: episode: 869, duration: 0.310s, episode steps: 56, steps per second: 181, episode reward: 38.220, mean reward: 0.682 [0.524, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-1.275, 10.100], loss: 0.001442, mae: 0.041670, mean_q: 1.272763
 52159/100000: episode: 870, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 9.205, mean reward: 0.767 [0.688, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.154, 10.426], loss: 0.001424, mae: 0.042604, mean_q: 1.294415
 52175/100000: episode: 871, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 12.689, mean reward: 0.793 [0.739, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.150, 10.527], loss: 0.001444, mae: 0.041915, mean_q: 1.275297
 52187/100000: episode: 872, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 8.937, mean reward: 0.745 [0.720, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.493], loss: 0.001511, mae: 0.043612, mean_q: 1.277436
 52199/100000: episode: 873, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 9.571, mean reward: 0.798 [0.735, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.047, 10.504], loss: 0.001590, mae: 0.043910, mean_q: 1.275437
 52215/100000: episode: 874, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 11.791, mean reward: 0.737 [0.675, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.185, 10.584], loss: 0.001368, mae: 0.042169, mean_q: 1.284482
 52271/100000: episode: 875, duration: 0.308s, episode steps: 56, steps per second: 182, episode reward: 39.784, mean reward: 0.710 [0.621, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.409, 10.100], loss: 0.001248, mae: 0.038494, mean_q: 1.293492
 52288/100000: episode: 876, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 11.419, mean reward: 0.672 [0.540, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.293, 10.197], loss: 0.001364, mae: 0.041315, mean_q: 1.279408
 52304/100000: episode: 877, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 13.676, mean reward: 0.855 [0.773, 0.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.531], loss: 0.001472, mae: 0.042844, mean_q: 1.276932
 52314/100000: episode: 878, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 7.291, mean reward: 0.729 [0.655, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.404, 10.423], loss: 0.001341, mae: 0.040147, mean_q: 1.312967
 52326/100000: episode: 879, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 9.908, mean reward: 0.826 [0.766, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.438, 10.620], loss: 0.001431, mae: 0.041570, mean_q: 1.301826
 52382/100000: episode: 880, duration: 0.317s, episode steps: 56, steps per second: 177, episode reward: 37.616, mean reward: 0.672 [0.552, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.716, 10.100], loss: 0.001274, mae: 0.039865, mean_q: 1.289099
 52438/100000: episode: 881, duration: 0.297s, episode steps: 56, steps per second: 189, episode reward: 38.068, mean reward: 0.680 [0.539, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-0.789, 10.100], loss: 0.001271, mae: 0.039263, mean_q: 1.297793
 52450/100000: episode: 882, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 10.339, mean reward: 0.862 [0.763, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.680], loss: 0.001181, mae: 0.039540, mean_q: 1.300668
 52467/100000: episode: 883, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 14.269, mean reward: 0.839 [0.759, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-1.039, 10.587], loss: 0.001395, mae: 0.040582, mean_q: 1.310437
 52523/100000: episode: 884, duration: 0.289s, episode steps: 56, steps per second: 194, episode reward: 37.750, mean reward: 0.674 [0.534, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.843 [-1.297, 10.100], loss: 0.001240, mae: 0.038929, mean_q: 1.296293
 52535/100000: episode: 885, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 9.321, mean reward: 0.777 [0.727, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.616], loss: 0.001354, mae: 0.040562, mean_q: 1.301844
 52552/100000: episode: 886, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 12.707, mean reward: 0.747 [0.626, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.426, 10.475], loss: 0.001413, mae: 0.041348, mean_q: 1.307007
 52568/100000: episode: 887, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 11.640, mean reward: 0.727 [0.667, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.052, 10.396], loss: 0.001248, mae: 0.039780, mean_q: 1.289451
 52578/100000: episode: 888, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 8.374, mean reward: 0.837 [0.747, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.547], loss: 0.001572, mae: 0.044236, mean_q: 1.304617
 52594/100000: episode: 889, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 12.674, mean reward: 0.792 [0.741, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.469], loss: 0.001312, mae: 0.040289, mean_q: 1.312729
[Info] 3-TH LEVEL FOUND: 1.6473087072372437, Considering 10/90 traces
 52611/100000: episode: 890, duration: 4.324s, episode steps: 17, steps per second: 4, episode reward: 13.429, mean reward: 0.790 [0.733, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.563], loss: 0.001281, mae: 0.039718, mean_q: 1.297507
 52666/100000: episode: 891, duration: 0.295s, episode steps: 55, steps per second: 186, episode reward: 34.830, mean reward: 0.633 [0.500, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.557, 10.100], loss: 0.001479, mae: 0.042491, mean_q: 1.299891
 52675/100000: episode: 892, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 7.929, mean reward: 0.881 [0.810, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.316, 10.640], loss: 0.001527, mae: 0.043969, mean_q: 1.316668
 52689/100000: episode: 893, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 11.375, mean reward: 0.812 [0.737, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.531], loss: 0.001294, mae: 0.040070, mean_q: 1.297249
 52696/100000: episode: 894, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 6.335, mean reward: 0.905 [0.843, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.686], loss: 0.001271, mae: 0.039269, mean_q: 1.295133
 52710/100000: episode: 895, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 11.774, mean reward: 0.841 [0.767, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.636], loss: 0.001428, mae: 0.041697, mean_q: 1.301432
 52719/100000: episode: 896, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 8.254, mean reward: 0.917 [0.868, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.674], loss: 0.001427, mae: 0.040660, mean_q: 1.255451
 52733/100000: episode: 897, duration: 0.088s, episode steps: 14, steps per second: 158, episode reward: 12.080, mean reward: 0.863 [0.804, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.410, 10.558], loss: 0.001397, mae: 0.042761, mean_q: 1.300648
 52740/100000: episode: 898, duration: 0.053s, episode steps: 7, steps per second: 132, episode reward: 5.951, mean reward: 0.850 [0.830, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.597], loss: 0.001105, mae: 0.036347, mean_q: 1.322227
 52749/100000: episode: 899, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 8.124, mean reward: 0.903 [0.847, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.555], loss: 0.001113, mae: 0.037550, mean_q: 1.305741
 52804/100000: episode: 900, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 34.598, mean reward: 0.629 [0.504, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.375, 10.100], loss: 0.001262, mae: 0.039630, mean_q: 1.298394
 52815/100000: episode: 901, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 9.750, mean reward: 0.886 [0.842, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.584, 10.583], loss: 0.001200, mae: 0.037242, mean_q: 1.311781
 52870/100000: episode: 902, duration: 0.285s, episode steps: 55, steps per second: 193, episode reward: 37.165, mean reward: 0.676 [0.540, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.227, 10.100], loss: 0.001558, mae: 0.043697, mean_q: 1.313986
 52925/100000: episode: 903, duration: 0.292s, episode steps: 55, steps per second: 188, episode reward: 38.775, mean reward: 0.705 [0.559, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-1.469, 10.100], loss: 0.001654, mae: 0.044976, mean_q: 1.304962
 52933/100000: episode: 904, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 6.649, mean reward: 0.831 [0.806, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.574], loss: 0.001470, mae: 0.043864, mean_q: 1.329437
 52947/100000: episode: 905, duration: 0.089s, episode steps: 14, steps per second: 158, episode reward: 11.482, mean reward: 0.820 [0.752, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.479], loss: 0.001637, mae: 0.046320, mean_q: 1.304859
 53002/100000: episode: 906, duration: 0.299s, episode steps: 55, steps per second: 184, episode reward: 41.389, mean reward: 0.753 [0.672, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.841 [-0.296, 10.100], loss: 0.001583, mae: 0.043372, mean_q: 1.321821
[Info] FALSIFICATION!
 53008/100000: episode: 907, duration: 0.299s, episode steps: 6, steps per second: 20, episode reward: 5.615, mean reward: 0.936 [0.880, 1.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.211, 10.561], loss: 0.000938, mae: 0.034676, mean_q: 1.313465
 53016/100000: episode: 908, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 6.607, mean reward: 0.826 [0.762, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.647, 10.549], loss: 0.001237, mae: 0.038201, mean_q: 1.318041
 53071/100000: episode: 909, duration: 0.292s, episode steps: 55, steps per second: 189, episode reward: 34.285, mean reward: 0.623 [0.505, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.129, 10.100], loss: 0.001222, mae: 0.039055, mean_q: 1.311735
 53080/100000: episode: 910, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 7.793, mean reward: 0.866 [0.791, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.539, 10.623], loss: 0.001581, mae: 0.042995, mean_q: 1.324383
[Info] FALSIFICATION!
 53087/100000: episode: 911, duration: 0.313s, episode steps: 7, steps per second: 22, episode reward: 6.488, mean reward: 0.927 [0.863, 1.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.015, 10.730], loss: 0.001236, mae: 0.038306, mean_q: 1.312149
 53096/100000: episode: 912, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 7.319, mean reward: 0.813 [0.760, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.599], loss: 0.001238, mae: 0.038921, mean_q: 1.319443
 53105/100000: episode: 913, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 8.063, mean reward: 0.896 [0.855, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.637], loss: 0.001423, mae: 0.041206, mean_q: 1.315939
 53116/100000: episode: 914, duration: 0.079s, episode steps: 11, steps per second: 138, episode reward: 10.084, mean reward: 0.917 [0.860, 0.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.714], loss: 0.001351, mae: 0.039873, mean_q: 1.326280
[Info] FALSIFICATION!
 53117/100000: episode: 915, duration: 0.269s, episode steps: 1, steps per second: 4, episode reward: 1.033, mean reward: 1.033 [1.033, 1.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.014, 10.669], loss: 0.001154, mae: 0.037293, mean_q: 1.291113
 53128/100000: episode: 916, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 9.261, mean reward: 0.842 [0.814, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.561], loss: 0.001283, mae: 0.040558, mean_q: 1.318199
 53142/100000: episode: 917, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 12.272, mean reward: 0.877 [0.783, 0.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.599], loss: 0.001462, mae: 0.041707, mean_q: 1.314376
 53156/100000: episode: 918, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 11.226, mean reward: 0.802 [0.741, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.539], loss: 0.001656, mae: 0.045018, mean_q: 1.326649
 53170/100000: episode: 919, duration: 0.093s, episode steps: 14, steps per second: 150, episode reward: 11.458, mean reward: 0.818 [0.752, 0.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.310, 10.549], loss: 0.002421, mae: 0.048068, mean_q: 1.320038
 53184/100000: episode: 920, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 10.718, mean reward: 0.766 [0.713, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.463], loss: 0.001501, mae: 0.043277, mean_q: 1.312699
 53198/100000: episode: 921, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 12.605, mean reward: 0.900 [0.842, 0.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.646], loss: 0.001490, mae: 0.043006, mean_q: 1.308278
 53253/100000: episode: 922, duration: 0.308s, episode steps: 55, steps per second: 178, episode reward: 39.204, mean reward: 0.713 [0.512, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.642, 10.100], loss: 0.001424, mae: 0.040172, mean_q: 1.318604
 53308/100000: episode: 923, duration: 0.316s, episode steps: 55, steps per second: 174, episode reward: 34.606, mean reward: 0.629 [0.517, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.285, 10.190], loss: 0.001759, mae: 0.044824, mean_q: 1.325682
 53322/100000: episode: 924, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 11.776, mean reward: 0.841 [0.807, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.619], loss: 0.002142, mae: 0.044854, mean_q: 1.318172
 53329/100000: episode: 925, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 6.231, mean reward: 0.890 [0.874, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.488, 10.650], loss: 0.001618, mae: 0.045068, mean_q: 1.335558
 53337/100000: episode: 926, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 6.519, mean reward: 0.815 [0.753, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.567], loss: 0.001137, mae: 0.038573, mean_q: 1.336023
[Info] FALSIFICATION!
 53344/100000: episode: 927, duration: 0.329s, episode steps: 7, steps per second: 21, episode reward: 6.186, mean reward: 0.884 [0.785, 1.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.839, 10.637], loss: 0.001373, mae: 0.040198, mean_q: 1.322558
 53399/100000: episode: 928, duration: 0.303s, episode steps: 55, steps per second: 181, episode reward: 36.627, mean reward: 0.666 [0.554, 0.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.537, 10.100], loss: 0.001507, mae: 0.040339, mean_q: 1.335051
 53408/100000: episode: 929, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 7.889, mean reward: 0.877 [0.830, 0.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.685], loss: 0.001008, mae: 0.034770, mean_q: 1.313490
 53417/100000: episode: 930, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 7.546, mean reward: 0.838 [0.787, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.649], loss: 0.001203, mae: 0.039056, mean_q: 1.310426
 53431/100000: episode: 931, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 11.554, mean reward: 0.825 [0.750, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.691, 10.541], loss: 0.001779, mae: 0.041026, mean_q: 1.328728
 53445/100000: episode: 932, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 11.777, mean reward: 0.841 [0.783, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.549], loss: 0.001389, mae: 0.042016, mean_q: 1.320979
 53454/100000: episode: 933, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 8.318, mean reward: 0.924 [0.898, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.578, 10.622], loss: 0.001448, mae: 0.039793, mean_q: 1.334151
 53465/100000: episode: 934, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 9.717, mean reward: 0.883 [0.797, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.544], loss: 0.001606, mae: 0.044260, mean_q: 1.348175
 53476/100000: episode: 935, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 9.297, mean reward: 0.845 [0.790, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.402, 10.600], loss: 0.001320, mae: 0.040959, mean_q: 1.335731
 53490/100000: episode: 936, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 12.836, mean reward: 0.917 [0.861, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.792, 10.629], loss: 0.001466, mae: 0.043587, mean_q: 1.339661
 53499/100000: episode: 937, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 7.635, mean reward: 0.848 [0.783, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.670], loss: 0.001131, mae: 0.038396, mean_q: 1.355380
 53513/100000: episode: 938, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 11.743, mean reward: 0.839 [0.745, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.552], loss: 0.001518, mae: 0.043168, mean_q: 1.326989
 53568/100000: episode: 939, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 35.841, mean reward: 0.652 [0.510, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.260, 10.263], loss: 0.001261, mae: 0.038878, mean_q: 1.341482
 53582/100000: episode: 940, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 12.244, mean reward: 0.875 [0.813, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.663], loss: 0.001154, mae: 0.037882, mean_q: 1.329981
 53589/100000: episode: 941, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 6.198, mean reward: 0.885 [0.838, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.656], loss: 0.002357, mae: 0.043016, mean_q: 1.344422
 53596/100000: episode: 942, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 5.961, mean reward: 0.852 [0.796, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.255, 10.594], loss: 0.001596, mae: 0.042797, mean_q: 1.371017
 53607/100000: episode: 943, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 8.943, mean reward: 0.813 [0.762, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.455], loss: 0.001319, mae: 0.039099, mean_q: 1.353339
 53616/100000: episode: 944, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 7.576, mean reward: 0.842 [0.810, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.616], loss: 0.002339, mae: 0.042291, mean_q: 1.342173
 53671/100000: episode: 945, duration: 0.284s, episode steps: 55, steps per second: 193, episode reward: 42.771, mean reward: 0.778 [0.683, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.375, 10.100], loss: 0.001704, mae: 0.042550, mean_q: 1.349430
 53685/100000: episode: 946, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 10.339, mean reward: 0.739 [0.669, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.498], loss: 0.001302, mae: 0.039700, mean_q: 1.352015
 53692/100000: episode: 947, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 6.080, mean reward: 0.869 [0.829, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.206, 10.597], loss: 0.001099, mae: 0.037533, mean_q: 1.357074
 53706/100000: episode: 948, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 11.418, mean reward: 0.816 [0.742, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.219, 10.593], loss: 0.001366, mae: 0.040578, mean_q: 1.343548
 53761/100000: episode: 949, duration: 0.307s, episode steps: 55, steps per second: 179, episode reward: 38.750, mean reward: 0.705 [0.611, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.270, 10.100], loss: 0.001303, mae: 0.039705, mean_q: 1.342080
[Info] FALSIFICATION!
 53763/100000: episode: 950, duration: 0.188s, episode steps: 2, steps per second: 11, episode reward: 1.898, mean reward: 0.949 [0.875, 1.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.016, 10.437], loss: 0.001459, mae: 0.042899, mean_q: 1.371827
 53818/100000: episode: 951, duration: 0.303s, episode steps: 55, steps per second: 182, episode reward: 37.036, mean reward: 0.673 [0.547, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.781, 10.100], loss: 0.001660, mae: 0.040843, mean_q: 1.343940
 53873/100000: episode: 952, duration: 0.290s, episode steps: 55, steps per second: 190, episode reward: 37.580, mean reward: 0.683 [0.600, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-1.121, 10.100], loss: 0.001768, mae: 0.044462, mean_q: 1.348707
 53882/100000: episode: 953, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 7.503, mean reward: 0.834 [0.805, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.595], loss: 0.001861, mae: 0.049706, mean_q: 1.315828
 53891/100000: episode: 954, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 7.981, mean reward: 0.887 [0.861, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-1.244, 10.602], loss: 0.002628, mae: 0.047167, mean_q: 1.352223
 53900/100000: episode: 955, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 7.787, mean reward: 0.865 [0.813, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.224, 10.622], loss: 0.001099, mae: 0.036451, mean_q: 1.332715
[Info] FALSIFICATION!
 53906/100000: episode: 956, duration: 0.323s, episode steps: 6, steps per second: 19, episode reward: 5.750, mean reward: 0.958 [0.866, 1.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-1.052, 10.588], loss: 0.001177, mae: 0.039264, mean_q: 1.365337
 53915/100000: episode: 957, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 8.000, mean reward: 0.889 [0.842, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.662, 10.497], loss: 0.002072, mae: 0.040559, mean_q: 1.358398
 53929/100000: episode: 958, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 11.414, mean reward: 0.815 [0.765, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.486], loss: 0.001187, mae: 0.038732, mean_q: 1.356328
 53940/100000: episode: 959, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 8.826, mean reward: 0.802 [0.718, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.928, 10.505], loss: 0.001219, mae: 0.039059, mean_q: 1.337138
 53954/100000: episode: 960, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 12.731, mean reward: 0.909 [0.811, 0.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.596], loss: 0.001531, mae: 0.042703, mean_q: 1.349686
 53961/100000: episode: 961, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 6.353, mean reward: 0.908 [0.858, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.695], loss: 0.001457, mae: 0.043422, mean_q: 1.335650
 53975/100000: episode: 962, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 12.066, mean reward: 0.862 [0.783, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.469], loss: 0.001565, mae: 0.043534, mean_q: 1.330650
 53984/100000: episode: 963, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 7.535, mean reward: 0.837 [0.807, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.602], loss: 0.001329, mae: 0.039621, mean_q: 1.362866
 53993/100000: episode: 964, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 7.809, mean reward: 0.868 [0.814, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.760, 10.617], loss: 0.001579, mae: 0.042453, mean_q: 1.337582
 54000/100000: episode: 965, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 6.298, mean reward: 0.900 [0.853, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.623], loss: 0.001420, mae: 0.042699, mean_q: 1.357321
 54008/100000: episode: 966, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 7.364, mean reward: 0.921 [0.859, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.825, 10.679], loss: 0.004072, mae: 0.057173, mean_q: 1.369119
 54022/100000: episode: 967, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 12.195, mean reward: 0.871 [0.743, 0.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.632], loss: 0.002772, mae: 0.047256, mean_q: 1.349054
 54077/100000: episode: 968, duration: 0.283s, episode steps: 55, steps per second: 194, episode reward: 38.282, mean reward: 0.696 [0.587, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.656, 10.100], loss: 0.001802, mae: 0.041233, mean_q: 1.358740
 54091/100000: episode: 969, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 11.171, mean reward: 0.798 [0.713, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.571], loss: 0.001312, mae: 0.039563, mean_q: 1.377462
 54098/100000: episode: 970, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 5.672, mean reward: 0.810 [0.756, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.531], loss: 0.001141, mae: 0.038032, mean_q: 1.393369
 54107/100000: episode: 971, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 7.915, mean reward: 0.879 [0.849, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.707], loss: 0.001638, mae: 0.045203, mean_q: 1.376886
[Info] FALSIFICATION!
 54116/100000: episode: 972, duration: 0.222s, episode steps: 9, steps per second: 41, episode reward: 8.339, mean reward: 0.927 [0.861, 1.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.014, 10.791], loss: 0.001979, mae: 0.040339, mean_q: 1.349667
 54125/100000: episode: 973, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 8.171, mean reward: 0.908 [0.888, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.677], loss: 0.000960, mae: 0.034530, mean_q: 1.332208
[Info] FALSIFICATION!
 54135/100000: episode: 974, duration: 0.243s, episode steps: 10, steps per second: 41, episode reward: 9.087, mean reward: 0.909 [0.834, 1.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.015, 10.758], loss: 0.002087, mae: 0.042945, mean_q: 1.358094
[Info] FALSIFICATION!
 54138/100000: episode: 975, duration: 0.193s, episode steps: 3, steps per second: 16, episode reward: 2.777, mean reward: 0.926 [0.871, 1.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.642, 10.434], loss: 0.001620, mae: 0.046699, mean_q: 1.394264
 54152/100000: episode: 976, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 11.877, mean reward: 0.848 [0.735, 0.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.142, 10.446], loss: 0.002154, mae: 0.045401, mean_q: 1.360623
 54161/100000: episode: 977, duration: 0.065s, episode steps: 9, steps per second: 139, episode reward: 7.267, mean reward: 0.807 [0.743, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.099, 10.547], loss: 0.001463, mae: 0.041251, mean_q: 1.358786
 54172/100000: episode: 978, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 9.070, mean reward: 0.825 [0.782, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.346, 10.611], loss: 0.001398, mae: 0.041300, mean_q: 1.345363
 54181/100000: episode: 979, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 8.038, mean reward: 0.893 [0.855, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.772, 10.581], loss: 0.001494, mae: 0.042767, mean_q: 1.383016
[Info] Complete ISplit Iteration
[Info] Levels: [1.4132715, 1.5509664, 1.6473087, 1.7151153]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.83]
[Info] Error Prob: 0.0008300000000000001

 54188/100000: episode: 980, duration: 4.440s, episode steps: 7, steps per second: 2, episode reward: 6.089, mean reward: 0.870 [0.828, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.614], loss: 0.001319, mae: 0.039211, mean_q: 1.368766
 54288/100000: episode: 981, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.750, mean reward: 0.578 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.820, 10.146], loss: 0.001782, mae: 0.042038, mean_q: 1.363122
 54388/100000: episode: 982, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 60.750, mean reward: 0.608 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.764, 10.098], loss: 0.001962, mae: 0.045263, mean_q: 1.356614
 54488/100000: episode: 983, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 61.967, mean reward: 0.620 [0.511, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.218, 10.508], loss: 0.001662, mae: 0.042319, mean_q: 1.360641
 54588/100000: episode: 984, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.692, mean reward: 0.577 [0.510, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.090, 10.154], loss: 0.001774, mae: 0.041734, mean_q: 1.364502
 54688/100000: episode: 985, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.032, mean reward: 0.580 [0.498, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.681, 10.400], loss: 0.002100, mae: 0.045210, mean_q: 1.367759
 54788/100000: episode: 986, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.694, mean reward: 0.607 [0.510, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.169, 10.296], loss: 0.001632, mae: 0.040595, mean_q: 1.367573
 54888/100000: episode: 987, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 62.571, mean reward: 0.626 [0.504, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.651, 10.098], loss: 0.001967, mae: 0.042961, mean_q: 1.366075
 54988/100000: episode: 988, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.897, mean reward: 0.579 [0.506, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.339, 10.203], loss: 0.001855, mae: 0.043644, mean_q: 1.368927
 55088/100000: episode: 989, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 63.716, mean reward: 0.637 [0.514, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.981, 10.098], loss: 0.001985, mae: 0.043961, mean_q: 1.356028
 55188/100000: episode: 990, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.268, mean reward: 0.593 [0.515, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.209, 10.293], loss: 0.001869, mae: 0.042394, mean_q: 1.364171
 55288/100000: episode: 991, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.212, mean reward: 0.592 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.024, 10.204], loss: 0.001984, mae: 0.042834, mean_q: 1.355314
 55388/100000: episode: 992, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.124, mean reward: 0.581 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.103, 10.098], loss: 0.001854, mae: 0.042553, mean_q: 1.353913
 55488/100000: episode: 993, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 56.861, mean reward: 0.569 [0.497, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.591, 10.275], loss: 0.001967, mae: 0.042961, mean_q: 1.344367
 55588/100000: episode: 994, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 60.587, mean reward: 0.606 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.610, 10.114], loss: 0.002318, mae: 0.047347, mean_q: 1.342979
 55688/100000: episode: 995, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.697, mean reward: 0.597 [0.514, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.567, 10.219], loss: 0.002319, mae: 0.046866, mean_q: 1.341197
 55788/100000: episode: 996, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 60.485, mean reward: 0.605 [0.517, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.345, 10.098], loss: 0.001811, mae: 0.042902, mean_q: 1.339771
 55888/100000: episode: 997, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 62.627, mean reward: 0.626 [0.502, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.759, 10.399], loss: 0.002031, mae: 0.043679, mean_q: 1.329832
 55988/100000: episode: 998, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.223, mean reward: 0.592 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.544, 10.098], loss: 0.001882, mae: 0.043033, mean_q: 1.335516
 56088/100000: episode: 999, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.243, mean reward: 0.582 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.636, 10.098], loss: 0.002007, mae: 0.044082, mean_q: 1.328528
 56188/100000: episode: 1000, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.865, mean reward: 0.589 [0.505, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.238, 10.098], loss: 0.001779, mae: 0.043556, mean_q: 1.323478
 56288/100000: episode: 1001, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 61.810, mean reward: 0.618 [0.520, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.080, 10.284], loss: 0.002176, mae: 0.045543, mean_q: 1.320295
 56388/100000: episode: 1002, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.994, mean reward: 0.580 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.533, 10.248], loss: 0.001851, mae: 0.043877, mean_q: 1.315449
 56488/100000: episode: 1003, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.515, mean reward: 0.585 [0.506, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.395, 10.098], loss: 0.001857, mae: 0.042642, mean_q: 1.301429
 56588/100000: episode: 1004, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.578, mean reward: 0.576 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.496, 10.098], loss: 0.002292, mae: 0.046490, mean_q: 1.307395
 56688/100000: episode: 1005, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.280, mean reward: 0.573 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.346, 10.195], loss: 0.002056, mae: 0.045333, mean_q: 1.298176
 56788/100000: episode: 1006, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.648, mean reward: 0.586 [0.509, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.268, 10.098], loss: 0.001586, mae: 0.041590, mean_q: 1.290154
 56888/100000: episode: 1007, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.341, mean reward: 0.573 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.915, 10.098], loss: 0.002064, mae: 0.044926, mean_q: 1.290368
 56988/100000: episode: 1008, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.242, mean reward: 0.602 [0.510, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.897, 10.098], loss: 0.002030, mae: 0.043330, mean_q: 1.281789
 57088/100000: episode: 1009, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.014, mean reward: 0.590 [0.502, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.458, 10.320], loss: 0.002339, mae: 0.045784, mean_q: 1.279258
 57188/100000: episode: 1010, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.401, mean reward: 0.594 [0.499, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.070, 10.165], loss: 0.001978, mae: 0.043387, mean_q: 1.271573
 57288/100000: episode: 1011, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.073, mean reward: 0.591 [0.503, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.631, 10.098], loss: 0.002044, mae: 0.043412, mean_q: 1.261759
 57388/100000: episode: 1012, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.027, mean reward: 0.570 [0.506, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.075, 10.104], loss: 0.002060, mae: 0.044259, mean_q: 1.260522
 57488/100000: episode: 1013, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.655, mean reward: 0.597 [0.497, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.469, 10.098], loss: 0.002228, mae: 0.046084, mean_q: 1.261523
 57588/100000: episode: 1014, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.749, mean reward: 0.577 [0.499, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.680, 10.118], loss: 0.001637, mae: 0.041881, mean_q: 1.255633
 57688/100000: episode: 1015, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.132, mean reward: 0.601 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.766, 10.394], loss: 0.001714, mae: 0.042573, mean_q: 1.252191
 57788/100000: episode: 1016, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.865, mean reward: 0.589 [0.503, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.245, 10.098], loss: 0.002184, mae: 0.045231, mean_q: 1.244572
 57888/100000: episode: 1017, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.825, mean reward: 0.588 [0.503, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.777, 10.109], loss: 0.001707, mae: 0.041050, mean_q: 1.241778
 57988/100000: episode: 1018, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.256, mean reward: 0.593 [0.509, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.324, 10.251], loss: 0.001623, mae: 0.041896, mean_q: 1.242088
 58088/100000: episode: 1019, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.602, mean reward: 0.596 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.922, 10.098], loss: 0.001867, mae: 0.044184, mean_q: 1.236007
 58188/100000: episode: 1020, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 57.413, mean reward: 0.574 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.579, 10.098], loss: 0.001831, mae: 0.043601, mean_q: 1.228828
 58288/100000: episode: 1021, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.726, mean reward: 0.587 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.773, 10.427], loss: 0.001586, mae: 0.041369, mean_q: 1.222838
 58388/100000: episode: 1022, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.571, mean reward: 0.566 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.248, 10.118], loss: 0.001634, mae: 0.041681, mean_q: 1.214318
 58488/100000: episode: 1023, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.440, mean reward: 0.604 [0.514, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.867, 10.327], loss: 0.001487, mae: 0.040639, mean_q: 1.209339
 58588/100000: episode: 1024, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 61.473, mean reward: 0.615 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.563, 10.524], loss: 0.001504, mae: 0.041154, mean_q: 1.203481
 58688/100000: episode: 1025, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.620, mean reward: 0.606 [0.511, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.434, 10.235], loss: 0.001416, mae: 0.040719, mean_q: 1.196924
 58788/100000: episode: 1026, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 60.543, mean reward: 0.605 [0.516, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.428, 10.277], loss: 0.001439, mae: 0.040636, mean_q: 1.194827
 58888/100000: episode: 1027, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.664, mean reward: 0.587 [0.510, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.809, 10.103], loss: 0.001352, mae: 0.040230, mean_q: 1.192546
 58988/100000: episode: 1028, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.212, mean reward: 0.582 [0.509, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.618, 10.125], loss: 0.001666, mae: 0.041886, mean_q: 1.184050
 59088/100000: episode: 1029, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.025, mean reward: 0.570 [0.507, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.900, 10.118], loss: 0.001440, mae: 0.040688, mean_q: 1.175354
 59188/100000: episode: 1030, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.961, mean reward: 0.590 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.428, 10.311], loss: 0.001334, mae: 0.040347, mean_q: 1.172653
 59288/100000: episode: 1031, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.323, mean reward: 0.583 [0.508, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.138, 10.286], loss: 0.001318, mae: 0.040601, mean_q: 1.169249
 59388/100000: episode: 1032, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 60.901, mean reward: 0.609 [0.519, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.374, 10.098], loss: 0.001354, mae: 0.040454, mean_q: 1.173630
 59488/100000: episode: 1033, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.088, mean reward: 0.581 [0.510, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.174, 10.104], loss: 0.001300, mae: 0.039688, mean_q: 1.169895
 59588/100000: episode: 1034, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 60.026, mean reward: 0.600 [0.500, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.811, 10.239], loss: 0.001241, mae: 0.038772, mean_q: 1.167174
 59688/100000: episode: 1035, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.293, mean reward: 0.583 [0.501, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.015, 10.098], loss: 0.001289, mae: 0.039769, mean_q: 1.171214
 59788/100000: episode: 1036, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.712, mean reward: 0.587 [0.511, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.525, 10.164], loss: 0.001303, mae: 0.040383, mean_q: 1.171288
 59888/100000: episode: 1037, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 61.318, mean reward: 0.613 [0.506, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.430, 10.140], loss: 0.001325, mae: 0.040336, mean_q: 1.171894
 59988/100000: episode: 1038, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.575, mean reward: 0.596 [0.498, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.372, 10.241], loss: 0.001295, mae: 0.039908, mean_q: 1.169733
 60088/100000: episode: 1039, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.877, mean reward: 0.609 [0.503, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.492, 10.098], loss: 0.001359, mae: 0.040897, mean_q: 1.169348
 60188/100000: episode: 1040, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 61.318, mean reward: 0.613 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.408, 10.098], loss: 0.001339, mae: 0.040503, mean_q: 1.167667
 60288/100000: episode: 1041, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 56.860, mean reward: 0.569 [0.500, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.448, 10.165], loss: 0.001312, mae: 0.040157, mean_q: 1.172603
 60388/100000: episode: 1042, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.202, mean reward: 0.582 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.796, 10.098], loss: 0.001318, mae: 0.040238, mean_q: 1.168009
 60488/100000: episode: 1043, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.283, mean reward: 0.573 [0.513, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.154, 10.196], loss: 0.001262, mae: 0.039399, mean_q: 1.170996
 60588/100000: episode: 1044, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.063, mean reward: 0.591 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.742, 10.098], loss: 0.001167, mae: 0.038102, mean_q: 1.166399
 60688/100000: episode: 1045, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.699, mean reward: 0.587 [0.506, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.805, 10.098], loss: 0.001351, mae: 0.040422, mean_q: 1.166162
 60788/100000: episode: 1046, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 61.668, mean reward: 0.617 [0.505, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.473, 10.445], loss: 0.001297, mae: 0.040089, mean_q: 1.167488
 60888/100000: episode: 1047, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.932, mean reward: 0.599 [0.508, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.130, 10.098], loss: 0.001377, mae: 0.041024, mean_q: 1.165306
 60988/100000: episode: 1048, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.369, mean reward: 0.574 [0.499, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.961, 10.222], loss: 0.001435, mae: 0.042192, mean_q: 1.169145
 61088/100000: episode: 1049, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.798, mean reward: 0.598 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.576, 10.185], loss: 0.001338, mae: 0.040307, mean_q: 1.167698
 61188/100000: episode: 1050, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.451, mean reward: 0.575 [0.502, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.676, 10.143], loss: 0.001309, mae: 0.040453, mean_q: 1.164862
 61288/100000: episode: 1051, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.584, mean reward: 0.576 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.239], loss: 0.001193, mae: 0.038413, mean_q: 1.166702
 61388/100000: episode: 1052, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.262, mean reward: 0.583 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.632, 10.124], loss: 0.001324, mae: 0.040014, mean_q: 1.168812
 61488/100000: episode: 1053, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 61.641, mean reward: 0.616 [0.514, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.753, 10.098], loss: 0.001436, mae: 0.042156, mean_q: 1.168725
 61588/100000: episode: 1054, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.689, mean reward: 0.597 [0.505, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.271, 10.172], loss: 0.001310, mae: 0.039931, mean_q: 1.166875
 61688/100000: episode: 1055, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.467, mean reward: 0.575 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.691, 10.098], loss: 0.001498, mae: 0.042467, mean_q: 1.168505
 61788/100000: episode: 1056, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 61.727, mean reward: 0.617 [0.500, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.383, 10.329], loss: 0.001502, mae: 0.042644, mean_q: 1.166290
 61888/100000: episode: 1057, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.784, mean reward: 0.588 [0.500, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.155, 10.098], loss: 0.001427, mae: 0.041476, mean_q: 1.170430
 61988/100000: episode: 1058, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.017, mean reward: 0.580 [0.507, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.202, 10.114], loss: 0.001406, mae: 0.041067, mean_q: 1.167912
 62088/100000: episode: 1059, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.943, mean reward: 0.589 [0.518, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.482, 10.140], loss: 0.001386, mae: 0.040742, mean_q: 1.164655
 62188/100000: episode: 1060, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.365, mean reward: 0.574 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.611, 10.098], loss: 0.001264, mae: 0.038661, mean_q: 1.165268
 62288/100000: episode: 1061, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.351, mean reward: 0.584 [0.510, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.570, 10.189], loss: 0.001485, mae: 0.041936, mean_q: 1.167792
 62388/100000: episode: 1062, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.241, mean reward: 0.592 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.084, 10.098], loss: 0.001335, mae: 0.040069, mean_q: 1.166706
 62488/100000: episode: 1063, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 63.704, mean reward: 0.637 [0.513, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.856, 10.098], loss: 0.001301, mae: 0.040085, mean_q: 1.165318
 62588/100000: episode: 1064, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 57.196, mean reward: 0.572 [0.500, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.773, 10.173], loss: 0.001450, mae: 0.042040, mean_q: 1.169163
 62688/100000: episode: 1065, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 61.642, mean reward: 0.616 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.702, 10.098], loss: 0.001569, mae: 0.043298, mean_q: 1.174087
 62788/100000: episode: 1066, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 62.233, mean reward: 0.622 [0.502, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.462, 10.098], loss: 0.001475, mae: 0.041939, mean_q: 1.174268
 62888/100000: episode: 1067, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.086, mean reward: 0.591 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.200, 10.098], loss: 0.001402, mae: 0.040654, mean_q: 1.172590
 62988/100000: episode: 1068, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.442, mean reward: 0.574 [0.503, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.680, 10.102], loss: 0.001415, mae: 0.041226, mean_q: 1.173219
 63088/100000: episode: 1069, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.350, mean reward: 0.573 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.103, 10.171], loss: 0.001629, mae: 0.043572, mean_q: 1.172669
 63188/100000: episode: 1070, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 58.456, mean reward: 0.585 [0.506, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.897, 10.223], loss: 0.001433, mae: 0.041162, mean_q: 1.168125
 63288/100000: episode: 1071, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.619, mean reward: 0.596 [0.507, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.574, 10.243], loss: 0.001408, mae: 0.040774, mean_q: 1.170304
 63388/100000: episode: 1072, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.919, mean reward: 0.589 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.834, 10.335], loss: 0.001403, mae: 0.041206, mean_q: 1.170613
 63488/100000: episode: 1073, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 61.986, mean reward: 0.620 [0.518, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.009, 10.098], loss: 0.001453, mae: 0.042182, mean_q: 1.170744
 63588/100000: episode: 1074, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.412, mean reward: 0.574 [0.504, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.614, 10.120], loss: 0.001325, mae: 0.039915, mean_q: 1.171375
 63688/100000: episode: 1075, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 61.488, mean reward: 0.615 [0.502, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.970, 10.313], loss: 0.001467, mae: 0.041861, mean_q: 1.168396
 63788/100000: episode: 1076, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 62.704, mean reward: 0.627 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.429, 10.447], loss: 0.001510, mae: 0.042466, mean_q: 1.174495
 63888/100000: episode: 1077, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.639, mean reward: 0.576 [0.507, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.656, 10.181], loss: 0.001420, mae: 0.040990, mean_q: 1.170030
 63988/100000: episode: 1078, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.971, mean reward: 0.580 [0.504, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.666, 10.151], loss: 0.001459, mae: 0.041060, mean_q: 1.172532
 64088/100000: episode: 1079, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.831, mean reward: 0.578 [0.498, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.948, 10.297], loss: 0.001498, mae: 0.042408, mean_q: 1.170670
[Info] 1-TH LEVEL FOUND: 1.3782848119735718, Considering 10/90 traces
 64188/100000: episode: 1080, duration: 4.735s, episode steps: 100, steps per second: 21, episode reward: 58.749, mean reward: 0.587 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.813, 10.156], loss: 0.001357, mae: 0.040504, mean_q: 1.173593
 64208/100000: episode: 1081, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 12.829, mean reward: 0.641 [0.582, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.169, 10.100], loss: 0.001612, mae: 0.042360, mean_q: 1.175215
 64245/100000: episode: 1082, duration: 0.236s, episode steps: 37, steps per second: 157, episode reward: 24.145, mean reward: 0.653 [0.566, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.837, 10.100], loss: 0.001375, mae: 0.039963, mean_q: 1.172685
 64282/100000: episode: 1083, duration: 0.198s, episode steps: 37, steps per second: 186, episode reward: 24.144, mean reward: 0.653 [0.516, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.318, 10.100], loss: 0.001366, mae: 0.040294, mean_q: 1.170504
 64300/100000: episode: 1084, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 12.514, mean reward: 0.695 [0.584, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.246, 10.100], loss: 0.001523, mae: 0.042234, mean_q: 1.177963
 64337/100000: episode: 1085, duration: 0.211s, episode steps: 37, steps per second: 175, episode reward: 24.521, mean reward: 0.663 [0.536, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.150, 10.100], loss: 0.001503, mae: 0.041923, mean_q: 1.180932
 64347/100000: episode: 1086, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 7.812, mean reward: 0.781 [0.723, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.472], loss: 0.001601, mae: 0.043883, mean_q: 1.160109
 64367/100000: episode: 1087, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 14.881, mean reward: 0.744 [0.629, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.540], loss: 0.001331, mae: 0.040764, mean_q: 1.177003
 64391/100000: episode: 1088, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 15.560, mean reward: 0.648 [0.550, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.182, 10.100], loss: 0.001485, mae: 0.041134, mean_q: 1.174356
 64414/100000: episode: 1089, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 15.035, mean reward: 0.654 [0.584, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.103, 10.100], loss: 0.001503, mae: 0.042312, mean_q: 1.180086
 64424/100000: episode: 1090, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 7.064, mean reward: 0.706 [0.679, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.740, 10.457], loss: 0.001635, mae: 0.042086, mean_q: 1.181275
 64461/100000: episode: 1091, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 25.281, mean reward: 0.683 [0.628, 0.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.709, 10.100], loss: 0.001425, mae: 0.041640, mean_q: 1.174685
 64471/100000: episode: 1092, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 7.712, mean reward: 0.771 [0.699, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.616], loss: 0.001388, mae: 0.040833, mean_q: 1.180076
 64508/100000: episode: 1093, duration: 0.211s, episode steps: 37, steps per second: 175, episode reward: 25.934, mean reward: 0.701 [0.609, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.986, 10.100], loss: 0.001509, mae: 0.042389, mean_q: 1.179979
 64531/100000: episode: 1094, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 15.924, mean reward: 0.692 [0.646, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.495, 10.100], loss: 0.001488, mae: 0.041909, mean_q: 1.180620
 64549/100000: episode: 1095, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 13.097, mean reward: 0.728 [0.653, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.489, 10.100], loss: 0.001743, mae: 0.043335, mean_q: 1.174883
 64586/100000: episode: 1096, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 23.785, mean reward: 0.643 [0.499, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.762, 10.149], loss: 0.001990, mae: 0.047069, mean_q: 1.179582
 64623/100000: episode: 1097, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 22.971, mean reward: 0.621 [0.532, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.499, 10.100], loss: 0.001574, mae: 0.042390, mean_q: 1.180484
 64671/100000: episode: 1098, duration: 0.269s, episode steps: 48, steps per second: 179, episode reward: 33.669, mean reward: 0.701 [0.612, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.688, 10.350], loss: 0.001566, mae: 0.043132, mean_q: 1.184344
 64695/100000: episode: 1099, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 17.875, mean reward: 0.745 [0.663, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.358, 10.100], loss: 0.001596, mae: 0.041907, mean_q: 1.186289
 64715/100000: episode: 1100, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 12.746, mean reward: 0.637 [0.550, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.100], loss: 0.001878, mae: 0.045042, mean_q: 1.187129
 64725/100000: episode: 1101, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 7.531, mean reward: 0.753 [0.719, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.576], loss: 0.001508, mae: 0.042473, mean_q: 1.186533
 64749/100000: episode: 1102, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 17.033, mean reward: 0.710 [0.664, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.603, 10.100], loss: 0.001327, mae: 0.039317, mean_q: 1.183774
 64772/100000: episode: 1103, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 16.640, mean reward: 0.723 [0.659, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.648, 10.100], loss: 0.001571, mae: 0.042491, mean_q: 1.179996
 64790/100000: episode: 1104, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 12.825, mean reward: 0.712 [0.664, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.466, 10.100], loss: 0.001785, mae: 0.045161, mean_q: 1.191155
 64810/100000: episode: 1105, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 14.248, mean reward: 0.712 [0.627, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.247, 10.100], loss: 0.001791, mae: 0.045164, mean_q: 1.182844
 64820/100000: episode: 1106, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 7.602, mean reward: 0.760 [0.732, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.143, 10.580], loss: 0.001597, mae: 0.042023, mean_q: 1.190343
 64840/100000: episode: 1107, duration: 0.127s, episode steps: 20, steps per second: 157, episode reward: 12.910, mean reward: 0.645 [0.589, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.503, 10.100], loss: 0.001552, mae: 0.041229, mean_q: 1.182404
 64877/100000: episode: 1108, duration: 0.193s, episode steps: 37, steps per second: 191, episode reward: 23.091, mean reward: 0.624 [0.536, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.614, 10.100], loss: 0.001698, mae: 0.044205, mean_q: 1.189058
 64914/100000: episode: 1109, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 23.600, mean reward: 0.638 [0.535, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.000, 10.100], loss: 0.001776, mae: 0.043676, mean_q: 1.189031
 64934/100000: episode: 1110, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 14.671, mean reward: 0.734 [0.681, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.311, 10.100], loss: 0.001491, mae: 0.040908, mean_q: 1.183092
 64982/100000: episode: 1111, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 30.079, mean reward: 0.627 [0.525, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.616, 10.100], loss: 0.001737, mae: 0.044339, mean_q: 1.189033
 65011/100000: episode: 1112, duration: 0.140s, episode steps: 29, steps per second: 207, episode reward: 17.564, mean reward: 0.606 [0.525, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.035, 10.175], loss: 0.001691, mae: 0.043293, mean_q: 1.191302
 65034/100000: episode: 1113, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 16.357, mean reward: 0.711 [0.603, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.219, 10.100], loss: 0.001567, mae: 0.042178, mean_q: 1.186906
 65063/100000: episode: 1114, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 19.222, mean reward: 0.663 [0.553, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.349, 10.100], loss: 0.001667, mae: 0.043458, mean_q: 1.187454
 65100/100000: episode: 1115, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 23.976, mean reward: 0.648 [0.522, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.270, 10.100], loss: 0.001871, mae: 0.045100, mean_q: 1.185336
 65129/100000: episode: 1116, duration: 0.163s, episode steps: 29, steps per second: 177, episode reward: 18.762, mean reward: 0.647 [0.516, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.018, 10.176], loss: 0.001702, mae: 0.042527, mean_q: 1.189272
 65139/100000: episode: 1117, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 6.701, mean reward: 0.670 [0.646, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.356], loss: 0.001894, mae: 0.046048, mean_q: 1.187530
 65162/100000: episode: 1118, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 16.396, mean reward: 0.713 [0.641, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.257, 10.100], loss: 0.001730, mae: 0.042933, mean_q: 1.189905
 65199/100000: episode: 1119, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 27.094, mean reward: 0.732 [0.598, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.134, 10.100], loss: 0.001990, mae: 0.046809, mean_q: 1.194946
 65217/100000: episode: 1120, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 12.800, mean reward: 0.711 [0.630, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.436, 10.100], loss: 0.001707, mae: 0.042589, mean_q: 1.198706
 65254/100000: episode: 1121, duration: 0.195s, episode steps: 37, steps per second: 189, episode reward: 25.334, mean reward: 0.685 [0.581, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.166, 10.100], loss: 0.002302, mae: 0.048912, mean_q: 1.194156
 65264/100000: episode: 1122, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 6.962, mean reward: 0.696 [0.674, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.468], loss: 0.001795, mae: 0.044238, mean_q: 1.208610
 65284/100000: episode: 1123, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 14.577, mean reward: 0.729 [0.668, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.476], loss: 0.001901, mae: 0.044945, mean_q: 1.190314
 65294/100000: episode: 1124, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 6.916, mean reward: 0.692 [0.663, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.205, 10.432], loss: 0.001266, mae: 0.039156, mean_q: 1.217046
 65318/100000: episode: 1125, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 16.604, mean reward: 0.692 [0.612, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.265, 10.100], loss: 0.001910, mae: 0.045894, mean_q: 1.198983
 65342/100000: episode: 1126, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 17.626, mean reward: 0.734 [0.655, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.410, 10.100], loss: 0.002152, mae: 0.048855, mean_q: 1.198010
 65365/100000: episode: 1127, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 16.228, mean reward: 0.706 [0.603, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.775, 10.100], loss: 0.001807, mae: 0.044428, mean_q: 1.193828
 65389/100000: episode: 1128, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 15.653, mean reward: 0.652 [0.562, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.339, 10.100], loss: 0.001625, mae: 0.043194, mean_q: 1.195557
 65399/100000: episode: 1129, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 7.532, mean reward: 0.753 [0.736, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.815, 10.507], loss: 0.001919, mae: 0.043193, mean_q: 1.178269
 65447/100000: episode: 1130, duration: 0.270s, episode steps: 48, steps per second: 178, episode reward: 30.967, mean reward: 0.645 [0.529, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.476, 10.135], loss: 0.001629, mae: 0.043344, mean_q: 1.202191
 65457/100000: episode: 1131, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 6.661, mean reward: 0.666 [0.628, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.360], loss: 0.001958, mae: 0.044589, mean_q: 1.213144
 65477/100000: episode: 1132, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 15.021, mean reward: 0.751 [0.665, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.508, 10.473], loss: 0.001787, mae: 0.045054, mean_q: 1.208108
 65497/100000: episode: 1133, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 13.982, mean reward: 0.699 [0.654, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.446, 10.100], loss: 0.001502, mae: 0.042120, mean_q: 1.206688
 65545/100000: episode: 1134, duration: 0.266s, episode steps: 48, steps per second: 180, episode reward: 30.721, mean reward: 0.640 [0.513, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.887, 10.184], loss: 0.001493, mae: 0.041427, mean_q: 1.206539
 65565/100000: episode: 1135, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 13.658, mean reward: 0.683 [0.603, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.152, 10.421], loss: 0.001672, mae: 0.041879, mean_q: 1.196517
 65589/100000: episode: 1136, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 16.103, mean reward: 0.671 [0.621, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.175, 10.100], loss: 0.001980, mae: 0.045697, mean_q: 1.208007
 65609/100000: episode: 1137, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 13.470, mean reward: 0.673 [0.616, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.734, 10.100], loss: 0.001699, mae: 0.043072, mean_q: 1.211182
 65632/100000: episode: 1138, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 16.180, mean reward: 0.703 [0.623, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.244, 10.100], loss: 0.001609, mae: 0.042658, mean_q: 1.204212
 65680/100000: episode: 1139, duration: 0.235s, episode steps: 48, steps per second: 204, episode reward: 31.699, mean reward: 0.660 [0.519, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-1.488, 10.263], loss: 0.001820, mae: 0.045503, mean_q: 1.210835
 65700/100000: episode: 1140, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 13.802, mean reward: 0.690 [0.585, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.605, 10.100], loss: 0.002201, mae: 0.048817, mean_q: 1.214720
 65748/100000: episode: 1141, duration: 0.266s, episode steps: 48, steps per second: 181, episode reward: 32.260, mean reward: 0.672 [0.519, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.341, 10.231], loss: 0.001745, mae: 0.043595, mean_q: 1.212790
 65796/100000: episode: 1142, duration: 0.256s, episode steps: 48, steps per second: 187, episode reward: 33.725, mean reward: 0.703 [0.641, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.309, 10.427], loss: 0.001727, mae: 0.043099, mean_q: 1.214356
 65844/100000: episode: 1143, duration: 0.263s, episode steps: 48, steps per second: 182, episode reward: 28.438, mean reward: 0.592 [0.509, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.823, 10.172], loss: 0.001887, mae: 0.046644, mean_q: 1.216184
 65868/100000: episode: 1144, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 15.229, mean reward: 0.635 [0.563, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.100], loss: 0.002199, mae: 0.049286, mean_q: 1.217996
 65888/100000: episode: 1145, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 14.511, mean reward: 0.726 [0.676, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.222, 10.530], loss: 0.001834, mae: 0.046505, mean_q: 1.210059
 65925/100000: episode: 1146, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 23.781, mean reward: 0.643 [0.517, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.388, 10.224], loss: 0.002189, mae: 0.048920, mean_q: 1.210812
 65945/100000: episode: 1147, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 13.807, mean reward: 0.690 [0.591, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.070, 10.100], loss: 0.001509, mae: 0.042177, mean_q: 1.208999
 65982/100000: episode: 1148, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 23.705, mean reward: 0.641 [0.538, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.848, 10.100], loss: 0.002012, mae: 0.046109, mean_q: 1.215881
 66019/100000: episode: 1149, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 23.770, mean reward: 0.642 [0.530, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.493, 10.140], loss: 0.001847, mae: 0.045001, mean_q: 1.219976
 66067/100000: episode: 1150, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 30.728, mean reward: 0.640 [0.534, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.465, 10.339], loss: 0.002001, mae: 0.045149, mean_q: 1.220325
 66091/100000: episode: 1151, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 17.664, mean reward: 0.736 [0.657, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.262, 10.100], loss: 0.001613, mae: 0.043680, mean_q: 1.223125
 66128/100000: episode: 1152, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 23.267, mean reward: 0.629 [0.529, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.548, 10.146], loss: 0.001639, mae: 0.042501, mean_q: 1.219886
 66157/100000: episode: 1153, duration: 0.141s, episode steps: 29, steps per second: 205, episode reward: 19.417, mean reward: 0.670 [0.606, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.143, 10.100], loss: 0.001580, mae: 0.042334, mean_q: 1.219363
 66180/100000: episode: 1154, duration: 0.141s, episode steps: 23, steps per second: 163, episode reward: 14.883, mean reward: 0.647 [0.598, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.547, 10.100], loss: 0.001465, mae: 0.040799, mean_q: 1.231800
 66200/100000: episode: 1155, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 15.494, mean reward: 0.775 [0.710, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.496], loss: 0.001952, mae: 0.045550, mean_q: 1.224013
 66237/100000: episode: 1156, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 24.144, mean reward: 0.653 [0.534, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.974, 10.100], loss: 0.001595, mae: 0.041973, mean_q: 1.225633
 66257/100000: episode: 1157, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 13.633, mean reward: 0.682 [0.627, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.625, 10.392], loss: 0.001893, mae: 0.044413, mean_q: 1.230724
 66267/100000: episode: 1158, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 7.286, mean reward: 0.729 [0.680, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.467], loss: 0.001395, mae: 0.038758, mean_q: 1.225138
 66277/100000: episode: 1159, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 6.857, mean reward: 0.686 [0.663, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.345, 10.471], loss: 0.001335, mae: 0.040471, mean_q: 1.233018
 66295/100000: episode: 1160, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 11.891, mean reward: 0.661 [0.586, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.797, 10.100], loss: 0.001745, mae: 0.043729, mean_q: 1.237083
 66315/100000: episode: 1161, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 13.927, mean reward: 0.696 [0.614, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.812, 10.342], loss: 0.002018, mae: 0.045581, mean_q: 1.232933
 66352/100000: episode: 1162, duration: 0.224s, episode steps: 37, steps per second: 166, episode reward: 27.175, mean reward: 0.734 [0.634, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.563, 10.100], loss: 0.001668, mae: 0.044077, mean_q: 1.228315
 66400/100000: episode: 1163, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 30.637, mean reward: 0.638 [0.539, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.513, 10.315], loss: 0.001497, mae: 0.042122, mean_q: 1.233732
 66420/100000: episode: 1164, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 13.170, mean reward: 0.658 [0.612, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.462, 10.100], loss: 0.001392, mae: 0.039822, mean_q: 1.227132
 66440/100000: episode: 1165, duration: 0.125s, episode steps: 20, steps per second: 160, episode reward: 13.560, mean reward: 0.678 [0.592, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.022, 10.324], loss: 0.001847, mae: 0.045215, mean_q: 1.231369
 66477/100000: episode: 1166, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 25.373, mean reward: 0.686 [0.603, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.472, 10.100], loss: 0.001808, mae: 0.045457, mean_q: 1.234831
 66500/100000: episode: 1167, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 18.061, mean reward: 0.785 [0.601, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.620, 10.100], loss: 0.001552, mae: 0.042164, mean_q: 1.227646
 66524/100000: episode: 1168, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 16.803, mean reward: 0.700 [0.631, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.341, 10.100], loss: 0.002092, mae: 0.046042, mean_q: 1.236088
 66544/100000: episode: 1169, duration: 0.115s, episode steps: 20, steps per second: 175, episode reward: 12.659, mean reward: 0.633 [0.536, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.945, 10.100], loss: 0.001961, mae: 0.046976, mean_q: 1.241766
[Info] 2-TH LEVEL FOUND: 1.5121285915374756, Considering 10/90 traces
 66562/100000: episode: 1170, duration: 4.292s, episode steps: 18, steps per second: 4, episode reward: 12.643, mean reward: 0.702 [0.666, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.362, 10.100], loss: 0.001819, mae: 0.043615, mean_q: 1.236071
 66597/100000: episode: 1171, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 23.661, mean reward: 0.676 [0.567, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.761, 10.100], loss: 0.001513, mae: 0.041578, mean_q: 1.233230
 66612/100000: episode: 1172, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 12.306, mean reward: 0.820 [0.757, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.966, 10.100], loss: 0.001286, mae: 0.038665, mean_q: 1.233706
 66648/100000: episode: 1173, duration: 0.212s, episode steps: 36, steps per second: 170, episode reward: 24.465, mean reward: 0.680 [0.564, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.165, 10.100], loss: 0.001567, mae: 0.041902, mean_q: 1.245100
 66683/100000: episode: 1174, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 23.221, mean reward: 0.663 [0.539, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.475, 10.100], loss: 0.001545, mae: 0.041128, mean_q: 1.230404
 66718/100000: episode: 1175, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 24.735, mean reward: 0.707 [0.594, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.695, 10.100], loss: 0.001530, mae: 0.041330, mean_q: 1.233118
 66753/100000: episode: 1176, duration: 0.193s, episode steps: 35, steps per second: 182, episode reward: 24.950, mean reward: 0.713 [0.554, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.689, 10.100], loss: 0.001613, mae: 0.042545, mean_q: 1.245371
 66783/100000: episode: 1177, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 22.442, mean reward: 0.748 [0.678, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.869, 10.100], loss: 0.001501, mae: 0.041929, mean_q: 1.235494
 66799/100000: episode: 1178, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 11.296, mean reward: 0.706 [0.567, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.145, 10.100], loss: 0.001391, mae: 0.041280, mean_q: 1.251228
 66834/100000: episode: 1179, duration: 0.180s, episode steps: 35, steps per second: 195, episode reward: 21.953, mean reward: 0.627 [0.507, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.762, 10.153], loss: 0.001723, mae: 0.042989, mean_q: 1.249307
 66869/100000: episode: 1180, duration: 0.178s, episode steps: 35, steps per second: 196, episode reward: 26.192, mean reward: 0.748 [0.683, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.390, 10.100], loss: 0.001669, mae: 0.041582, mean_q: 1.232573
 66899/100000: episode: 1181, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 19.604, mean reward: 0.653 [0.530, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.983, 10.100], loss: 0.001510, mae: 0.042774, mean_q: 1.248324
 66914/100000: episode: 1182, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 11.782, mean reward: 0.785 [0.729, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.482, 10.100], loss: 0.001338, mae: 0.039230, mean_q: 1.247678
 66949/100000: episode: 1183, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 24.884, mean reward: 0.711 [0.619, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.304, 10.100], loss: 0.001502, mae: 0.041414, mean_q: 1.248447
 66957/100000: episode: 1184, duration: 0.055s, episode steps: 8, steps per second: 146, episode reward: 6.828, mean reward: 0.853 [0.764, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-2.289, 10.100], loss: 0.001095, mae: 0.036263, mean_q: 1.229655
 66992/100000: episode: 1185, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 23.361, mean reward: 0.667 [0.554, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.408, 10.100], loss: 0.001548, mae: 0.039524, mean_q: 1.247684
 67022/100000: episode: 1186, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 22.460, mean reward: 0.749 [0.643, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-1.125, 10.100], loss: 0.001772, mae: 0.042899, mean_q: 1.253144
 67038/100000: episode: 1187, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 14.047, mean reward: 0.878 [0.817, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.604, 10.100], loss: 0.001951, mae: 0.045817, mean_q: 1.252618
 67050/100000: episode: 1188, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 9.702, mean reward: 0.809 [0.717, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.408, 10.100], loss: 0.001439, mae: 0.038600, mean_q: 1.239780
 67086/100000: episode: 1189, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 28.769, mean reward: 0.799 [0.671, 0.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.252, 10.100], loss: 0.001534, mae: 0.041859, mean_q: 1.250089
 67122/100000: episode: 1190, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 25.518, mean reward: 0.709 [0.659, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.896, 10.100], loss: 0.001695, mae: 0.044501, mean_q: 1.265050
 67134/100000: episode: 1191, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 9.965, mean reward: 0.830 [0.798, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.493, 10.100], loss: 0.002283, mae: 0.049499, mean_q: 1.248754
 67146/100000: episode: 1192, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 9.867, mean reward: 0.822 [0.773, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.720, 10.100], loss: 0.001685, mae: 0.044814, mean_q: 1.259258
 67182/100000: episode: 1193, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 28.547, mean reward: 0.793 [0.710, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.824, 10.100], loss: 0.001731, mae: 0.043771, mean_q: 1.259546
 67194/100000: episode: 1194, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 9.957, mean reward: 0.830 [0.787, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.460, 10.100], loss: 0.001453, mae: 0.041882, mean_q: 1.273438
 67229/100000: episode: 1195, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 25.801, mean reward: 0.737 [0.582, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.946, 10.100], loss: 0.001594, mae: 0.042413, mean_q: 1.255627
 67265/100000: episode: 1196, duration: 0.188s, episode steps: 36, steps per second: 192, episode reward: 24.993, mean reward: 0.694 [0.542, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.355, 10.100], loss: 0.001733, mae: 0.043949, mean_q: 1.265924
 67281/100000: episode: 1197, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 13.321, mean reward: 0.833 [0.773, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.722, 10.100], loss: 0.001489, mae: 0.041985, mean_q: 1.271326
 67316/100000: episode: 1198, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 24.394, mean reward: 0.697 [0.637, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.868, 10.100], loss: 0.001471, mae: 0.042130, mean_q: 1.271513
 67346/100000: episode: 1199, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 20.607, mean reward: 0.687 [0.593, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.307, 10.100], loss: 0.001622, mae: 0.041239, mean_q: 1.257123
 67381/100000: episode: 1200, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 23.060, mean reward: 0.659 [0.578, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.085, 10.100], loss: 0.001851, mae: 0.044623, mean_q: 1.273452
 67393/100000: episode: 1201, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 10.379, mean reward: 0.865 [0.792, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.443, 10.100], loss: 0.001675, mae: 0.041558, mean_q: 1.271081
 67428/100000: episode: 1202, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 26.697, mean reward: 0.763 [0.704, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.314, 10.100], loss: 0.001645, mae: 0.043359, mean_q: 1.277687
 67444/100000: episode: 1203, duration: 0.079s, episode steps: 16, steps per second: 201, episode reward: 13.570, mean reward: 0.848 [0.746, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.406, 10.100], loss: 0.001301, mae: 0.039793, mean_q: 1.275559
 67459/100000: episode: 1204, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 11.667, mean reward: 0.778 [0.727, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.455, 10.100], loss: 0.001465, mae: 0.040972, mean_q: 1.273525
 67493/100000: episode: 1205, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 24.723, mean reward: 0.727 [0.647, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.361, 10.100], loss: 0.001678, mae: 0.044817, mean_q: 1.273031
 67528/100000: episode: 1206, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 24.577, mean reward: 0.702 [0.606, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.342, 10.100], loss: 0.001470, mae: 0.040912, mean_q: 1.276795
 67544/100000: episode: 1207, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 13.317, mean reward: 0.832 [0.780, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-2.029, 10.100], loss: 0.001416, mae: 0.041287, mean_q: 1.275767
 67574/100000: episode: 1208, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 20.469, mean reward: 0.682 [0.587, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.398, 10.100], loss: 0.001356, mae: 0.040117, mean_q: 1.275698
 67609/100000: episode: 1209, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 24.891, mean reward: 0.711 [0.605, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.276, 10.100], loss: 0.001621, mae: 0.042333, mean_q: 1.274268
 67625/100000: episode: 1210, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 13.625, mean reward: 0.852 [0.803, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.389, 10.100], loss: 0.001375, mae: 0.040095, mean_q: 1.295063
 67660/100000: episode: 1211, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 24.049, mean reward: 0.687 [0.577, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.540, 10.100], loss: 0.001589, mae: 0.041406, mean_q: 1.276992
 67676/100000: episode: 1212, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 12.981, mean reward: 0.811 [0.748, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.325, 10.100], loss: 0.001565, mae: 0.042544, mean_q: 1.282042
 67710/100000: episode: 1213, duration: 0.167s, episode steps: 34, steps per second: 203, episode reward: 22.552, mean reward: 0.663 [0.528, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.170, 10.100], loss: 0.001337, mae: 0.039207, mean_q: 1.273597
 67745/100000: episode: 1214, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 23.705, mean reward: 0.677 [0.589, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.023, 10.100], loss: 0.001481, mae: 0.040508, mean_q: 1.286469
 67761/100000: episode: 1215, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 12.821, mean reward: 0.801 [0.663, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.189, 10.100], loss: 0.001514, mae: 0.040322, mean_q: 1.294378
 67777/100000: episode: 1216, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 12.447, mean reward: 0.778 [0.690, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.517, 10.100], loss: 0.001776, mae: 0.044779, mean_q: 1.283061
 67812/100000: episode: 1217, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 23.798, mean reward: 0.680 [0.538, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.268, 10.100], loss: 0.001585, mae: 0.042363, mean_q: 1.283124
 67846/100000: episode: 1218, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 24.245, mean reward: 0.713 [0.633, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.116, 10.100], loss: 0.001653, mae: 0.042880, mean_q: 1.291209
 67881/100000: episode: 1219, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 27.889, mean reward: 0.797 [0.712, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-1.213, 10.100], loss: 0.001330, mae: 0.039414, mean_q: 1.294531
 67916/100000: episode: 1220, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 24.283, mean reward: 0.694 [0.533, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.983, 10.100], loss: 0.001977, mae: 0.046822, mean_q: 1.293885
 67950/100000: episode: 1221, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 22.792, mean reward: 0.670 [0.616, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.741, 10.100], loss: 0.001584, mae: 0.042014, mean_q: 1.289887
 67966/100000: episode: 1222, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 13.688, mean reward: 0.855 [0.787, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.428, 10.100], loss: 0.001420, mae: 0.041439, mean_q: 1.308975
 68000/100000: episode: 1223, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 25.695, mean reward: 0.756 [0.545, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.056, 10.100], loss: 0.001422, mae: 0.040310, mean_q: 1.300123
 68036/100000: episode: 1224, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 26.404, mean reward: 0.733 [0.601, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-1.197, 10.100], loss: 0.001365, mae: 0.041333, mean_q: 1.306693
 68066/100000: episode: 1225, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 21.931, mean reward: 0.731 [0.605, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.296, 10.100], loss: 0.001914, mae: 0.047948, mean_q: 1.308973
 68096/100000: episode: 1226, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 21.252, mean reward: 0.708 [0.612, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.381, 10.100], loss: 0.001265, mae: 0.039153, mean_q: 1.310125
 68126/100000: episode: 1227, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 21.266, mean reward: 0.709 [0.624, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.374, 10.100], loss: 0.001540, mae: 0.040478, mean_q: 1.306724
 68138/100000: episode: 1228, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 10.003, mean reward: 0.834 [0.767, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.516, 10.100], loss: 0.001407, mae: 0.039796, mean_q: 1.297281
 68173/100000: episode: 1229, duration: 0.178s, episode steps: 35, steps per second: 196, episode reward: 27.179, mean reward: 0.777 [0.622, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.320, 10.100], loss: 0.001412, mae: 0.041524, mean_q: 1.321538
 68208/100000: episode: 1230, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 21.757, mean reward: 0.622 [0.502, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.041, 10.190], loss: 0.001486, mae: 0.040803, mean_q: 1.304836
 68220/100000: episode: 1231, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 9.921, mean reward: 0.827 [0.782, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.380, 10.100], loss: 0.001441, mae: 0.039976, mean_q: 1.319459
 68254/100000: episode: 1232, duration: 0.194s, episode steps: 34, steps per second: 176, episode reward: 25.301, mean reward: 0.744 [0.609, 0.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.138, 10.100], loss: 0.001301, mae: 0.040508, mean_q: 1.321899
 68289/100000: episode: 1233, duration: 0.197s, episode steps: 35, steps per second: 178, episode reward: 23.369, mean reward: 0.668 [0.574, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.299, 10.100], loss: 0.001293, mae: 0.040610, mean_q: 1.310302
 68325/100000: episode: 1234, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 26.060, mean reward: 0.724 [0.655, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.676, 10.100], loss: 0.001436, mae: 0.040267, mean_q: 1.296177
 68333/100000: episode: 1235, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 6.735, mean reward: 0.842 [0.791, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.556, 10.100], loss: 0.001062, mae: 0.037536, mean_q: 1.315272
 68348/100000: episode: 1236, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 12.260, mean reward: 0.817 [0.787, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.420, 10.100], loss: 0.001155, mae: 0.037274, mean_q: 1.331061
 68363/100000: episode: 1237, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 11.606, mean reward: 0.774 [0.718, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.802, 10.100], loss: 0.001407, mae: 0.039720, mean_q: 1.324485
 68399/100000: episode: 1238, duration: 0.185s, episode steps: 36, steps per second: 194, episode reward: 26.342, mean reward: 0.732 [0.608, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.882, 10.100], loss: 0.001207, mae: 0.038613, mean_q: 1.318140
 68429/100000: episode: 1239, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 21.799, mean reward: 0.727 [0.619, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.112, 10.100], loss: 0.001250, mae: 0.038949, mean_q: 1.327406
 68459/100000: episode: 1240, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 21.021, mean reward: 0.701 [0.565, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.326, 10.100], loss: 0.001352, mae: 0.040784, mean_q: 1.316906
 68474/100000: episode: 1241, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 11.951, mean reward: 0.797 [0.750, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.290, 10.100], loss: 0.001626, mae: 0.044888, mean_q: 1.318033
 68486/100000: episode: 1242, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 9.612, mean reward: 0.801 [0.757, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.445, 10.100], loss: 0.001130, mae: 0.037220, mean_q: 1.346288
 68498/100000: episode: 1243, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 10.572, mean reward: 0.881 [0.788, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.534, 10.100], loss: 0.001118, mae: 0.036616, mean_q: 1.334508
 68533/100000: episode: 1244, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 25.995, mean reward: 0.743 [0.671, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.655, 10.100], loss: 0.001688, mae: 0.044989, mean_q: 1.324952
 68545/100000: episode: 1245, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 9.879, mean reward: 0.823 [0.791, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.528, 10.100], loss: 0.001449, mae: 0.040836, mean_q: 1.322826
 68575/100000: episode: 1246, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 24.017, mean reward: 0.801 [0.714, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.332, 10.100], loss: 0.001441, mae: 0.041086, mean_q: 1.321132
 68610/100000: episode: 1247, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 25.087, mean reward: 0.717 [0.600, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.315, 10.100], loss: 0.001384, mae: 0.040386, mean_q: 1.337434
 68625/100000: episode: 1248, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 11.901, mean reward: 0.793 [0.681, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.338, 10.100], loss: 0.001195, mae: 0.037979, mean_q: 1.324159
 68659/100000: episode: 1249, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 22.097, mean reward: 0.650 [0.528, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.376, 10.104], loss: 0.001295, mae: 0.040191, mean_q: 1.328327
 68694/100000: episode: 1250, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 23.327, mean reward: 0.666 [0.588, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.510, 10.100], loss: 0.001369, mae: 0.041210, mean_q: 1.336185
 68706/100000: episode: 1251, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 9.179, mean reward: 0.765 [0.659, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.062, 10.100], loss: 0.001229, mae: 0.039508, mean_q: 1.287337
 68741/100000: episode: 1252, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 25.658, mean reward: 0.733 [0.650, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.731, 10.100], loss: 0.001319, mae: 0.039034, mean_q: 1.345230
 68756/100000: episode: 1253, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 9.778, mean reward: 0.652 [0.577, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.309, 10.100], loss: 0.001090, mae: 0.036543, mean_q: 1.355028
 68786/100000: episode: 1254, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 20.048, mean reward: 0.668 [0.557, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.252, 10.100], loss: 0.001257, mae: 0.037666, mean_q: 1.346001
 68816/100000: episode: 1255, duration: 0.170s, episode steps: 30, steps per second: 177, episode reward: 19.433, mean reward: 0.648 [0.571, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.162, 10.100], loss: 0.001168, mae: 0.037524, mean_q: 1.340365
 68851/100000: episode: 1256, duration: 0.175s, episode steps: 35, steps per second: 201, episode reward: 22.933, mean reward: 0.655 [0.554, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.098, 10.100], loss: 0.001320, mae: 0.040175, mean_q: 1.338784
 68886/100000: episode: 1257, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 24.180, mean reward: 0.691 [0.527, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.383, 10.222], loss: 0.001156, mae: 0.037614, mean_q: 1.337471
 68920/100000: episode: 1258, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 26.121, mean reward: 0.768 [0.717, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.887, 10.100], loss: 0.001434, mae: 0.041026, mean_q: 1.344424
 68928/100000: episode: 1259, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 6.316, mean reward: 0.790 [0.750, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.481, 10.100], loss: 0.001247, mae: 0.039307, mean_q: 1.349163
[Info] 3-TH LEVEL FOUND: 1.685706615447998, Considering 10/90 traces
 68944/100000: episode: 1260, duration: 4.228s, episode steps: 16, steps per second: 4, episode reward: 12.451, mean reward: 0.778 [0.724, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.505, 10.100], loss: 0.001303, mae: 0.039482, mean_q: 1.331925
 68954/100000: episode: 1261, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 8.311, mean reward: 0.831 [0.744, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.480, 10.100], loss: 0.001074, mae: 0.036361, mean_q: 1.348461
 68961/100000: episode: 1262, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 6.172, mean reward: 0.882 [0.808, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.413, 10.100], loss: 0.001243, mae: 0.040807, mean_q: 1.336912
 68971/100000: episode: 1263, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 8.010, mean reward: 0.801 [0.759, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.507, 10.100], loss: 0.001323, mae: 0.040891, mean_q: 1.342465
 68977/100000: episode: 1264, duration: 0.044s, episode steps: 6, steps per second: 136, episode reward: 5.061, mean reward: 0.844 [0.794, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.466, 10.100], loss: 0.001181, mae: 0.037591, mean_q: 1.376152
 68982/100000: episode: 1265, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 4.226, mean reward: 0.845 [0.829, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.468, 10.100], loss: 0.001468, mae: 0.039045, mean_q: 1.295030
 68989/100000: episode: 1266, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 5.732, mean reward: 0.819 [0.780, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.434, 10.100], loss: 0.001048, mae: 0.035689, mean_q: 1.326466
 68996/100000: episode: 1267, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 5.885, mean reward: 0.841 [0.763, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.388, 10.100], loss: 0.001261, mae: 0.041032, mean_q: 1.339349
 69004/100000: episode: 1268, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 7.389, mean reward: 0.924 [0.875, 0.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.444, 10.100], loss: 0.001320, mae: 0.038909, mean_q: 1.339869
 69014/100000: episode: 1269, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 8.639, mean reward: 0.864 [0.816, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.456, 10.100], loss: 0.001115, mae: 0.037763, mean_q: 1.363149
 69021/100000: episode: 1270, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.649, mean reward: 0.807 [0.779, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.884, 10.100], loss: 0.001231, mae: 0.037880, mean_q: 1.365568
 69031/100000: episode: 1271, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 7.899, mean reward: 0.790 [0.715, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.332, 10.100], loss: 0.001175, mae: 0.038192, mean_q: 1.358869
 69039/100000: episode: 1272, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 7.461, mean reward: 0.933 [0.854, 0.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.613, 10.100], loss: 0.001182, mae: 0.037700, mean_q: 1.348202
 69046/100000: episode: 1273, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 6.027, mean reward: 0.861 [0.821, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.411, 10.100], loss: 0.001323, mae: 0.041503, mean_q: 1.365976
 69073/100000: episode: 1274, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 21.867, mean reward: 0.810 [0.690, 0.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.005, 10.100], loss: 0.001290, mae: 0.040184, mean_q: 1.348979
 69078/100000: episode: 1275, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 4.119, mean reward: 0.824 [0.738, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.393, 10.100], loss: 0.001324, mae: 0.040248, mean_q: 1.385987
 69084/100000: episode: 1276, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 5.233, mean reward: 0.872 [0.838, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.549, 10.100], loss: 0.001249, mae: 0.039231, mean_q: 1.332188
 69089/100000: episode: 1277, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 4.344, mean reward: 0.869 [0.811, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.501, 10.100], loss: 0.001532, mae: 0.043745, mean_q: 1.335755
 69099/100000: episode: 1278, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 8.209, mean reward: 0.821 [0.716, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.315, 10.100], loss: 0.001167, mae: 0.037367, mean_q: 1.359051
 69105/100000: episode: 1279, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 5.288, mean reward: 0.881 [0.830, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.009, 10.100], loss: 0.001133, mae: 0.036435, mean_q: 1.353409
 69112/100000: episode: 1280, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 6.014, mean reward: 0.859 [0.835, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.544, 10.100], loss: 0.001261, mae: 0.039581, mean_q: 1.351577
 69139/100000: episode: 1281, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 20.630, mean reward: 0.764 [0.653, 0.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.290, 10.100], loss: 0.001159, mae: 0.037415, mean_q: 1.345728
 69152/100000: episode: 1282, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 11.167, mean reward: 0.859 [0.741, 0.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.425, 10.100], loss: 0.001282, mae: 0.039193, mean_q: 1.367193
 69158/100000: episode: 1283, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: 4.704, mean reward: 0.784 [0.732, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.557, 10.100], loss: 0.001078, mae: 0.035860, mean_q: 1.363954
 69168/100000: episode: 1284, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 8.092, mean reward: 0.809 [0.778, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.499, 10.100], loss: 0.001057, mae: 0.035072, mean_q: 1.323588
 69175/100000: episode: 1285, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 5.987, mean reward: 0.855 [0.802, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.870, 10.100], loss: 0.001199, mae: 0.038099, mean_q: 1.346056
 69202/100000: episode: 1286, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 22.277, mean reward: 0.825 [0.651, 0.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.282, 10.100], loss: 0.001461, mae: 0.042199, mean_q: 1.375746
 69210/100000: episode: 1287, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 6.597, mean reward: 0.825 [0.800, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-1.163, 10.100], loss: 0.001568, mae: 0.043696, mean_q: 1.352771
 69217/100000: episode: 1288, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 6.214, mean reward: 0.888 [0.807, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.455, 10.100], loss: 0.001289, mae: 0.039505, mean_q: 1.332843
 69224/100000: episode: 1289, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 5.949, mean reward: 0.850 [0.820, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.210, 10.100], loss: 0.001150, mae: 0.036246, mean_q: 1.377923
 69231/100000: episode: 1290, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 5.473, mean reward: 0.782 [0.732, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.452, 10.100], loss: 0.001570, mae: 0.045032, mean_q: 1.348956
 69258/100000: episode: 1291, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 21.101, mean reward: 0.782 [0.724, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.385, 10.100], loss: 0.001322, mae: 0.039905, mean_q: 1.358266
 69265/100000: episode: 1292, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 6.574, mean reward: 0.939 [0.872, 0.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.381, 10.100], loss: 0.001584, mae: 0.043412, mean_q: 1.380262
 69275/100000: episode: 1293, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 8.322, mean reward: 0.832 [0.764, 0.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.502, 10.100], loss: 0.001870, mae: 0.047788, mean_q: 1.356123
 69285/100000: episode: 1294, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 8.587, mean reward: 0.859 [0.794, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.003, 10.100], loss: 0.001289, mae: 0.040468, mean_q: 1.347690
 69292/100000: episode: 1295, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 5.816, mean reward: 0.831 [0.796, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.721, 10.100], loss: 0.001140, mae: 0.037906, mean_q: 1.365479
 69302/100000: episode: 1296, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 8.669, mean reward: 0.867 [0.826, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.902, 10.100], loss: 0.001296, mae: 0.039592, mean_q: 1.352514
 69309/100000: episode: 1297, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 5.680, mean reward: 0.811 [0.739, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.641, 10.100], loss: 0.001563, mae: 0.045004, mean_q: 1.364446
 69319/100000: episode: 1298, duration: 0.075s, episode steps: 10, steps per second: 134, episode reward: 8.584, mean reward: 0.858 [0.803, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.209, 10.100], loss: 0.001440, mae: 0.043588, mean_q: 1.358390
 69346/100000: episode: 1299, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 22.665, mean reward: 0.839 [0.710, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.733, 10.100], loss: 0.001455, mae: 0.042066, mean_q: 1.352130
 69359/100000: episode: 1300, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 11.777, mean reward: 0.906 [0.849, 0.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.452, 10.100], loss: 0.001413, mae: 0.041742, mean_q: 1.357196
 69386/100000: episode: 1301, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 20.623, mean reward: 0.764 [0.688, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.312, 10.100], loss: 0.001556, mae: 0.044256, mean_q: 1.371812
 69396/100000: episode: 1302, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 8.382, mean reward: 0.838 [0.796, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.491, 10.100], loss: 0.001499, mae: 0.043579, mean_q: 1.379897
 69403/100000: episode: 1303, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 5.780, mean reward: 0.826 [0.787, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.507, 10.100], loss: 0.001472, mae: 0.042821, mean_q: 1.367059
 69410/100000: episode: 1304, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 5.900, mean reward: 0.843 [0.817, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.552, 10.100], loss: 0.001194, mae: 0.038754, mean_q: 1.318185
 69417/100000: episode: 1305, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 5.893, mean reward: 0.842 [0.792, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.812, 10.100], loss: 0.001414, mae: 0.041688, mean_q: 1.391397
 69422/100000: episode: 1306, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 4.387, mean reward: 0.877 [0.842, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.500, 10.100], loss: 0.001315, mae: 0.041542, mean_q: 1.351784
 69429/100000: episode: 1307, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 5.775, mean reward: 0.825 [0.721, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.874, 10.100], loss: 0.001307, mae: 0.037993, mean_q: 1.356277
 69442/100000: episode: 1308, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 11.776, mean reward: 0.906 [0.841, 0.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.454, 10.100], loss: 0.001157, mae: 0.038118, mean_q: 1.356468
[Info] FALSIFICATION!
 69452/100000: episode: 1309, duration: 0.249s, episode steps: 10, steps per second: 40, episode reward: 9.365, mean reward: 0.937 [0.882, 1.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.592, 9.404], loss: 0.001495, mae: 0.042229, mean_q: 1.364685
 69459/100000: episode: 1310, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 5.826, mean reward: 0.832 [0.794, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.718, 10.100], loss: 0.002134, mae: 0.041469, mean_q: 1.375340
 69466/100000: episode: 1311, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.716, mean reward: 0.817 [0.755, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.449, 10.100], loss: 0.001470, mae: 0.042207, mean_q: 1.362607
 69473/100000: episode: 1312, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 5.678, mean reward: 0.811 [0.704, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.432, 10.100], loss: 0.001284, mae: 0.039785, mean_q: 1.380756
[Info] FALSIFICATION!
 69474/100000: episode: 1313, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 1.030, mean reward: 1.030 [1.030, 1.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.337, 10.020], loss: 0.001044, mae: 0.034397, mean_q: 1.357925
 69481/100000: episode: 1314, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 5.900, mean reward: 0.843 [0.804, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.477, 10.100], loss: 0.001186, mae: 0.037843, mean_q: 1.372171
[Info] FALSIFICATION!
 69487/100000: episode: 1315, duration: 0.306s, episode steps: 6, steps per second: 20, episode reward: 5.501, mean reward: 0.917 [0.861, 1.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.456, 10.100], loss: 0.001346, mae: 0.039603, mean_q: 1.372054
 69493/100000: episode: 1316, duration: 0.035s, episode steps: 6, steps per second: 171, episode reward: 4.763, mean reward: 0.794 [0.779, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.618, 10.100], loss: 0.002621, mae: 0.045903, mean_q: 1.353564
 69501/100000: episode: 1317, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 6.038, mean reward: 0.755 [0.714, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.410, 10.100], loss: 0.001294, mae: 0.041124, mean_q: 1.387242
 69508/100000: episode: 1318, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 5.742, mean reward: 0.820 [0.787, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.788, 10.100], loss: 0.001387, mae: 0.041251, mean_q: 1.355892
 69515/100000: episode: 1319, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 5.885, mean reward: 0.841 [0.772, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.368, 10.100], loss: 0.001489, mae: 0.042876, mean_q: 1.372996
 69521/100000: episode: 1320, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 4.865, mean reward: 0.811 [0.763, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.504, 10.100], loss: 0.001438, mae: 0.042026, mean_q: 1.350008
 69534/100000: episode: 1321, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 10.890, mean reward: 0.838 [0.767, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.320, 10.100], loss: 0.001459, mae: 0.042375, mean_q: 1.385925
[Info] FALSIFICATION!
 69541/100000: episode: 1322, duration: 0.301s, episode steps: 7, steps per second: 23, episode reward: 6.608, mean reward: 0.944 [0.919, 1.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.292, 9.139], loss: 0.001163, mae: 0.038344, mean_q: 1.365380
 69568/100000: episode: 1323, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 22.811, mean reward: 0.845 [0.745, 0.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.598, 10.100], loss: 0.001479, mae: 0.043396, mean_q: 1.377774
 69595/100000: episode: 1324, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 21.121, mean reward: 0.782 [0.690, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.371, 10.100], loss: 0.001478, mae: 0.041780, mean_q: 1.372544
 69600/100000: episode: 1325, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 4.204, mean reward: 0.841 [0.818, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.558, 10.100], loss: 0.001400, mae: 0.042962, mean_q: 1.362697
 69610/100000: episode: 1326, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 8.315, mean reward: 0.831 [0.794, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.528, 10.100], loss: 0.001197, mae: 0.039040, mean_q: 1.374427
 69623/100000: episode: 1327, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 11.041, mean reward: 0.849 [0.758, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.612, 10.100], loss: 0.001338, mae: 0.040444, mean_q: 1.361309
 69633/100000: episode: 1328, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 8.597, mean reward: 0.860 [0.793, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.577, 10.100], loss: 0.001390, mae: 0.040182, mean_q: 1.381742
 69640/100000: episode: 1329, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 6.227, mean reward: 0.890 [0.836, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.492, 10.100], loss: 0.002520, mae: 0.044154, mean_q: 1.376644
 69645/100000: episode: 1330, duration: 0.039s, episode steps: 5, steps per second: 127, episode reward: 4.297, mean reward: 0.859 [0.837, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.522, 10.100], loss: 0.001510, mae: 0.043985, mean_q: 1.360787
 69650/100000: episode: 1331, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 4.324, mean reward: 0.865 [0.850, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.418, 10.100], loss: 0.002642, mae: 0.043662, mean_q: 1.368006
 69658/100000: episode: 1332, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 7.508, mean reward: 0.939 [0.895, 0.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.481, 10.100], loss: 0.001475, mae: 0.042516, mean_q: 1.384950
 69665/100000: episode: 1333, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 5.787, mean reward: 0.827 [0.752, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.472, 10.100], loss: 0.001393, mae: 0.041446, mean_q: 1.370909
 69678/100000: episode: 1334, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 11.452, mean reward: 0.881 [0.833, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.894, 10.100], loss: 0.001420, mae: 0.041341, mean_q: 1.393911
 69688/100000: episode: 1335, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 7.816, mean reward: 0.782 [0.714, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.347, 10.100], loss: 0.001389, mae: 0.041819, mean_q: 1.387224
 69695/100000: episode: 1336, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 5.999, mean reward: 0.857 [0.813, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.764, 10.100], loss: 0.001525, mae: 0.043274, mean_q: 1.371401
 69705/100000: episode: 1337, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 8.043, mean reward: 0.804 [0.784, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.492, 10.100], loss: 0.001280, mae: 0.040559, mean_q: 1.391215
 69713/100000: episode: 1338, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 6.976, mean reward: 0.872 [0.806, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.656, 10.100], loss: 0.002085, mae: 0.039852, mean_q: 1.381544
 69718/100000: episode: 1339, duration: 0.040s, episode steps: 5, steps per second: 125, episode reward: 4.263, mean reward: 0.853 [0.814, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.451, 10.100], loss: 0.000981, mae: 0.035042, mean_q: 1.393433
 69728/100000: episode: 1340, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 7.530, mean reward: 0.753 [0.712, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.321, 10.100], loss: 0.001247, mae: 0.039201, mean_q: 1.397553
 69734/100000: episode: 1341, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 4.866, mean reward: 0.811 [0.772, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.638, 10.100], loss: 0.001471, mae: 0.042552, mean_q: 1.366394
[Info] FALSIFICATION!
 69743/100000: episode: 1342, duration: 0.323s, episode steps: 9, steps per second: 28, episode reward: 8.031, mean reward: 0.892 [0.802, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.154, 10.098], loss: 0.001325, mae: 0.040235, mean_q: 1.404034
 69749/100000: episode: 1343, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 5.130, mean reward: 0.855 [0.820, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.515, 10.100], loss: 0.001464, mae: 0.044766, mean_q: 1.366922
 69756/100000: episode: 1344, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 5.469, mean reward: 0.781 [0.742, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.474, 10.100], loss: 0.001284, mae: 0.039368, mean_q: 1.373197
 69763/100000: episode: 1345, duration: 0.042s, episode steps: 7, steps per second: 169, episode reward: 5.930, mean reward: 0.847 [0.815, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.623, 10.100], loss: 0.001751, mae: 0.045880, mean_q: 1.400309
 69771/100000: episode: 1346, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 7.377, mean reward: 0.922 [0.868, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.530, 10.100], loss: 0.001325, mae: 0.041613, mean_q: 1.384484
 69781/100000: episode: 1347, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 8.979, mean reward: 0.898 [0.823, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.429, 10.100], loss: 0.001356, mae: 0.040394, mean_q: 1.377975
[Info] FALSIFICATION!
 69793/100000: episode: 1348, duration: 0.335s, episode steps: 12, steps per second: 36, episode reward: 11.084, mean reward: 0.924 [0.858, 1.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.218, 9.559], loss: 0.001284, mae: 0.039706, mean_q: 1.393984
 69800/100000: episode: 1349, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 5.386, mean reward: 0.769 [0.720, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.474, 10.100], loss: 0.001409, mae: 0.042181, mean_q: 1.385673
[Info] Complete ISplit Iteration
[Info] Levels: [1.3782848, 1.5121286, 1.6857066, 1.721413]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.64]
[Info] Error Prob: 0.0006400000000000002

 69810/100000: episode: 1350, duration: 4.394s, episode steps: 10, steps per second: 2, episode reward: 7.982, mean reward: 0.798 [0.669, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.551, 10.100], loss: 0.001406, mae: 0.041460, mean_q: 1.368760
 69910/100000: episode: 1351, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 60.641, mean reward: 0.606 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.600, 10.098], loss: 0.001494, mae: 0.040939, mean_q: 1.381249
 70010/100000: episode: 1352, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 63.391, mean reward: 0.634 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.415, 10.376], loss: 0.001465, mae: 0.042569, mean_q: 1.384644
 70110/100000: episode: 1353, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.658, mean reward: 0.607 [0.512, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.702, 10.249], loss: 0.001742, mae: 0.043179, mean_q: 1.373138
 70210/100000: episode: 1354, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 62.264, mean reward: 0.623 [0.503, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.097, 10.246], loss: 0.001763, mae: 0.043777, mean_q: 1.378778
 70310/100000: episode: 1355, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.523, mean reward: 0.605 [0.504, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.265, 10.218], loss: 0.001421, mae: 0.040362, mean_q: 1.377612
 70410/100000: episode: 1356, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 64.992, mean reward: 0.650 [0.520, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.042, 10.098], loss: 0.001397, mae: 0.040975, mean_q: 1.367104
 70510/100000: episode: 1357, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.552, mean reward: 0.586 [0.505, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.544, 10.204], loss: 0.002057, mae: 0.043647, mean_q: 1.377818
 70610/100000: episode: 1358, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.790, mean reward: 0.588 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.419, 10.098], loss: 0.001595, mae: 0.041905, mean_q: 1.373961
 70710/100000: episode: 1359, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 60.233, mean reward: 0.602 [0.515, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.761, 10.456], loss: 0.001613, mae: 0.042921, mean_q: 1.356534
 70810/100000: episode: 1360, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.121, mean reward: 0.581 [0.498, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.046, 10.120], loss: 0.001857, mae: 0.042243, mean_q: 1.362961
 70910/100000: episode: 1361, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.446, mean reward: 0.594 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.553, 10.312], loss: 0.001753, mae: 0.041589, mean_q: 1.354119
 71010/100000: episode: 1362, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.731, mean reward: 0.587 [0.504, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.714, 10.434], loss: 0.001598, mae: 0.040786, mean_q: 1.364347
 71110/100000: episode: 1363, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 56.550, mean reward: 0.566 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.320, 10.125], loss: 0.001944, mae: 0.044203, mean_q: 1.351099
 71210/100000: episode: 1364, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.133, mean reward: 0.571 [0.501, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.431, 10.125], loss: 0.001724, mae: 0.043665, mean_q: 1.350306
 71310/100000: episode: 1365, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.583, mean reward: 0.576 [0.509, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.668, 10.273], loss: 0.001574, mae: 0.041654, mean_q: 1.353063
 71410/100000: episode: 1366, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 60.148, mean reward: 0.601 [0.510, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.943, 10.098], loss: 0.001862, mae: 0.043431, mean_q: 1.347697
 71510/100000: episode: 1367, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.162, mean reward: 0.582 [0.510, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.123, 10.137], loss: 0.001616, mae: 0.042898, mean_q: 1.334766
 71610/100000: episode: 1368, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 62.669, mean reward: 0.627 [0.521, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.496, 10.098], loss: 0.001979, mae: 0.044569, mean_q: 1.338901
 71710/100000: episode: 1369, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.973, mean reward: 0.590 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.848, 10.342], loss: 0.002098, mae: 0.046958, mean_q: 1.334984
 71810/100000: episode: 1370, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.996, mean reward: 0.600 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.483, 10.343], loss: 0.001824, mae: 0.044560, mean_q: 1.334246
 71910/100000: episode: 1371, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.315, mean reward: 0.603 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.379, 10.379], loss: 0.001601, mae: 0.042088, mean_q: 1.321172
 72010/100000: episode: 1372, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 65.351, mean reward: 0.654 [0.511, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.094, 10.098], loss: 0.001571, mae: 0.042559, mean_q: 1.326827
 72110/100000: episode: 1373, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.931, mean reward: 0.579 [0.510, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.729, 10.178], loss: 0.001429, mae: 0.040926, mean_q: 1.315739
 72210/100000: episode: 1374, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.207, mean reward: 0.582 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.936, 10.402], loss: 0.001765, mae: 0.044302, mean_q: 1.310649
 72310/100000: episode: 1375, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.317, mean reward: 0.583 [0.505, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.835, 10.187], loss: 0.001774, mae: 0.043699, mean_q: 1.303427
 72410/100000: episode: 1376, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.631, mean reward: 0.586 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.080, 10.153], loss: 0.001914, mae: 0.044835, mean_q: 1.300015
 72510/100000: episode: 1377, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.187, mean reward: 0.572 [0.503, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.425, 10.165], loss: 0.002024, mae: 0.046680, mean_q: 1.291571
 72610/100000: episode: 1378, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.512, mean reward: 0.575 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.141, 10.098], loss: 0.001684, mae: 0.042796, mean_q: 1.288474
 72710/100000: episode: 1379, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.717, mean reward: 0.587 [0.510, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.063, 10.098], loss: 0.001660, mae: 0.043567, mean_q: 1.283698
 72810/100000: episode: 1380, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.463, mean reward: 0.595 [0.500, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.534, 10.098], loss: 0.001852, mae: 0.044387, mean_q: 1.280916
 72910/100000: episode: 1381, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.401, mean reward: 0.594 [0.512, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.428, 10.237], loss: 0.001748, mae: 0.043908, mean_q: 1.274163
 73010/100000: episode: 1382, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 61.984, mean reward: 0.620 [0.514, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.558, 10.104], loss: 0.001675, mae: 0.042811, mean_q: 1.265464
 73110/100000: episode: 1383, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.199, mean reward: 0.592 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.401, 10.352], loss: 0.001909, mae: 0.045056, mean_q: 1.258845
 73210/100000: episode: 1384, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 60.041, mean reward: 0.600 [0.509, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.355, 10.224], loss: 0.001578, mae: 0.042717, mean_q: 1.266349
 73310/100000: episode: 1385, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.404, mean reward: 0.584 [0.514, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.206, 10.277], loss: 0.001643, mae: 0.043553, mean_q: 1.263025
 73410/100000: episode: 1386, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.793, mean reward: 0.578 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.513, 10.098], loss: 0.002044, mae: 0.045490, mean_q: 1.252582
 73510/100000: episode: 1387, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 61.288, mean reward: 0.613 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.030, 10.098], loss: 0.001775, mae: 0.043812, mean_q: 1.247841
 73610/100000: episode: 1388, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.317, mean reward: 0.603 [0.499, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.887, 10.472], loss: 0.001925, mae: 0.044572, mean_q: 1.244327
 73710/100000: episode: 1389, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 56.446, mean reward: 0.564 [0.502, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.995, 10.224], loss: 0.001967, mae: 0.044915, mean_q: 1.241470
 73810/100000: episode: 1390, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 59.229, mean reward: 0.592 [0.507, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.642, 10.098], loss: 0.002022, mae: 0.044994, mean_q: 1.235581
 73910/100000: episode: 1391, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.281, mean reward: 0.573 [0.501, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.021, 10.098], loss: 0.001574, mae: 0.041940, mean_q: 1.229647
 74010/100000: episode: 1392, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.628, mean reward: 0.586 [0.515, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.448, 10.210], loss: 0.001717, mae: 0.044320, mean_q: 1.225175
 74110/100000: episode: 1393, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.805, mean reward: 0.588 [0.511, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.619, 10.227], loss: 0.002082, mae: 0.045412, mean_q: 1.224278
 74210/100000: episode: 1394, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.841, mean reward: 0.588 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.919, 10.223], loss: 0.001697, mae: 0.042535, mean_q: 1.209672
 74310/100000: episode: 1395, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.031, mean reward: 0.590 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.869, 10.263], loss: 0.001768, mae: 0.044321, mean_q: 1.207376
 74410/100000: episode: 1396, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.255, mean reward: 0.583 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.055, 10.098], loss: 0.001735, mae: 0.042711, mean_q: 1.198070
 74510/100000: episode: 1397, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.473, mean reward: 0.595 [0.510, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.868, 10.278], loss: 0.001589, mae: 0.042785, mean_q: 1.193049
 74610/100000: episode: 1398, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 56.290, mean reward: 0.563 [0.499, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.659, 10.200], loss: 0.001571, mae: 0.043132, mean_q: 1.188963
 74710/100000: episode: 1399, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.212, mean reward: 0.602 [0.533, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.565, 10.098], loss: 0.001557, mae: 0.042078, mean_q: 1.183636
 74810/100000: episode: 1400, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 62.302, mean reward: 0.623 [0.499, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.491, 10.254], loss: 0.001356, mae: 0.040525, mean_q: 1.176068
 74910/100000: episode: 1401, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 61.071, mean reward: 0.611 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.602, 10.226], loss: 0.001354, mae: 0.040763, mean_q: 1.172959
 75010/100000: episode: 1402, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.624, mean reward: 0.576 [0.497, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.537, 10.229], loss: 0.001395, mae: 0.041035, mean_q: 1.174834
 75110/100000: episode: 1403, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.422, mean reward: 0.574 [0.507, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.888, 10.126], loss: 0.001435, mae: 0.041805, mean_q: 1.173107
 75210/100000: episode: 1404, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.447, mean reward: 0.604 [0.505, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.253, 10.098], loss: 0.001424, mae: 0.041306, mean_q: 1.171847
 75310/100000: episode: 1405, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.637, mean reward: 0.576 [0.513, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.098], loss: 0.001387, mae: 0.040752, mean_q: 1.174631
 75410/100000: episode: 1406, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.292, mean reward: 0.593 [0.510, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.429, 10.339], loss: 0.001387, mae: 0.040378, mean_q: 1.172023
 75510/100000: episode: 1407, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 61.837, mean reward: 0.618 [0.507, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.236, 10.275], loss: 0.001396, mae: 0.041239, mean_q: 1.167750
 75610/100000: episode: 1408, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.936, mean reward: 0.569 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.441, 10.098], loss: 0.001437, mae: 0.041097, mean_q: 1.171921
 75710/100000: episode: 1409, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.269, mean reward: 0.583 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.673, 10.098], loss: 0.001411, mae: 0.041383, mean_q: 1.167762
 75810/100000: episode: 1410, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.231, mean reward: 0.592 [0.511, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.920, 10.098], loss: 0.001357, mae: 0.040310, mean_q: 1.168682
 75910/100000: episode: 1411, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 56.432, mean reward: 0.564 [0.503, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.569, 10.098], loss: 0.001383, mae: 0.041012, mean_q: 1.169438
 76010/100000: episode: 1412, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.450, mean reward: 0.584 [0.500, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.163, 10.128], loss: 0.001376, mae: 0.041416, mean_q: 1.168547
 76110/100000: episode: 1413, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.513, mean reward: 0.585 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.456, 10.098], loss: 0.001445, mae: 0.041745, mean_q: 1.170532
 76210/100000: episode: 1414, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.552, mean reward: 0.586 [0.509, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.664, 10.199], loss: 0.001409, mae: 0.041056, mean_q: 1.167124
 76310/100000: episode: 1415, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.376, mean reward: 0.584 [0.509, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.718, 10.098], loss: 0.001388, mae: 0.040968, mean_q: 1.166197
 76410/100000: episode: 1416, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.604, mean reward: 0.586 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.037, 10.098], loss: 0.001391, mae: 0.040854, mean_q: 1.169645
 76510/100000: episode: 1417, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.678, mean reward: 0.577 [0.504, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.964, 10.098], loss: 0.001478, mae: 0.042076, mean_q: 1.166026
 76610/100000: episode: 1418, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.564, mean reward: 0.606 [0.501, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.169, 10.155], loss: 0.001374, mae: 0.040993, mean_q: 1.167735
 76710/100000: episode: 1419, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 60.003, mean reward: 0.600 [0.501, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.693, 10.116], loss: 0.001494, mae: 0.042200, mean_q: 1.168509
 76810/100000: episode: 1420, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 63.425, mean reward: 0.634 [0.501, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.852, 10.098], loss: 0.001298, mae: 0.039340, mean_q: 1.166694
 76910/100000: episode: 1421, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.481, mean reward: 0.575 [0.499, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.112, 10.289], loss: 0.001439, mae: 0.041831, mean_q: 1.165620
 77010/100000: episode: 1422, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.149, mean reward: 0.581 [0.499, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.731, 10.098], loss: 0.001453, mae: 0.042100, mean_q: 1.165841
 77110/100000: episode: 1423, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.333, mean reward: 0.583 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.325, 10.221], loss: 0.001477, mae: 0.042413, mean_q: 1.165131
 77210/100000: episode: 1424, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 61.303, mean reward: 0.613 [0.504, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.408, 10.181], loss: 0.001375, mae: 0.041023, mean_q: 1.163928
 77310/100000: episode: 1425, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.402, mean reward: 0.594 [0.522, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.646, 10.214], loss: 0.001466, mae: 0.042163, mean_q: 1.172007
 77410/100000: episode: 1426, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.997, mean reward: 0.590 [0.512, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.612, 10.196], loss: 0.001455, mae: 0.042086, mean_q: 1.168813
 77510/100000: episode: 1427, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.202, mean reward: 0.572 [0.506, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.060, 10.098], loss: 0.001418, mae: 0.040680, mean_q: 1.164381
 77610/100000: episode: 1428, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 62.317, mean reward: 0.623 [0.511, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.454, 10.274], loss: 0.001533, mae: 0.042755, mean_q: 1.167383
 77710/100000: episode: 1429, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 60.905, mean reward: 0.609 [0.512, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.331, 10.295], loss: 0.001490, mae: 0.041953, mean_q: 1.163581
 77810/100000: episode: 1430, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.261, mean reward: 0.573 [0.500, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.187], loss: 0.001288, mae: 0.039346, mean_q: 1.166089
 77910/100000: episode: 1431, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.651, mean reward: 0.577 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.375, 10.098], loss: 0.001295, mae: 0.039585, mean_q: 1.168790
 78010/100000: episode: 1432, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.611, mean reward: 0.596 [0.503, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.347, 10.098], loss: 0.001325, mae: 0.040084, mean_q: 1.170794
 78110/100000: episode: 1433, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 61.613, mean reward: 0.616 [0.507, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.384, 10.098], loss: 0.001357, mae: 0.041556, mean_q: 1.169646
 78210/100000: episode: 1434, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 57.721, mean reward: 0.577 [0.507, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.222, 10.098], loss: 0.001305, mae: 0.039822, mean_q: 1.168833
 78310/100000: episode: 1435, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.185, mean reward: 0.592 [0.508, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.051, 10.284], loss: 0.001356, mae: 0.040347, mean_q: 1.166149
 78410/100000: episode: 1436, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.732, mean reward: 0.587 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.621, 10.302], loss: 0.001324, mae: 0.039962, mean_q: 1.165950
 78510/100000: episode: 1437, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.818, mean reward: 0.598 [0.511, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.525, 10.389], loss: 0.001316, mae: 0.039601, mean_q: 1.168094
 78610/100000: episode: 1438, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.715, mean reward: 0.587 [0.499, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.563, 10.202], loss: 0.001446, mae: 0.041784, mean_q: 1.168540
 78710/100000: episode: 1439, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.627, mean reward: 0.596 [0.513, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.185, 10.115], loss: 0.001497, mae: 0.041411, mean_q: 1.166139
 78810/100000: episode: 1440, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.617, mean reward: 0.586 [0.499, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.747, 10.098], loss: 0.001442, mae: 0.041414, mean_q: 1.165529
 78910/100000: episode: 1441, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.073, mean reward: 0.601 [0.517, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.933, 10.098], loss: 0.001279, mae: 0.038870, mean_q: 1.166377
 79010/100000: episode: 1442, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 62.484, mean reward: 0.625 [0.513, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.614, 10.115], loss: 0.001389, mae: 0.039952, mean_q: 1.168136
 79110/100000: episode: 1443, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.628, mean reward: 0.586 [0.519, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.731, 10.204], loss: 0.001371, mae: 0.040355, mean_q: 1.171454
 79210/100000: episode: 1444, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.476, mean reward: 0.585 [0.512, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.775, 10.124], loss: 0.001410, mae: 0.040690, mean_q: 1.170734
 79310/100000: episode: 1445, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.611, mean reward: 0.586 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.801, 10.342], loss: 0.001298, mae: 0.038903, mean_q: 1.169475
 79410/100000: episode: 1446, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.097, mean reward: 0.581 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.733, 10.249], loss: 0.001349, mae: 0.040600, mean_q: 1.170662
 79510/100000: episode: 1447, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.793, mean reward: 0.578 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.858, 10.360], loss: 0.001405, mae: 0.040638, mean_q: 1.168651
 79610/100000: episode: 1448, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 64.211, mean reward: 0.642 [0.500, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.830, 10.098], loss: 0.001455, mae: 0.041057, mean_q: 1.168607
 79710/100000: episode: 1449, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 61.274, mean reward: 0.613 [0.505, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.153, 10.098], loss: 0.001416, mae: 0.040735, mean_q: 1.165882
[Info] 1-TH LEVEL FOUND: 1.4195889234542847, Considering 10/90 traces
 79810/100000: episode: 1450, duration: 4.636s, episode steps: 100, steps per second: 22, episode reward: 59.241, mean reward: 0.592 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.628, 10.098], loss: 0.001462, mae: 0.041135, mean_q: 1.169367
 79852/100000: episode: 1451, duration: 0.234s, episode steps: 42, steps per second: 179, episode reward: 32.641, mean reward: 0.777 [0.662, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.383, 10.100], loss: 0.001517, mae: 0.042627, mean_q: 1.172197
 79901/100000: episode: 1452, duration: 0.255s, episode steps: 49, steps per second: 192, episode reward: 31.994, mean reward: 0.653 [0.513, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.215, 10.232], loss: 0.001361, mae: 0.040718, mean_q: 1.171830
 79958/100000: episode: 1453, duration: 0.297s, episode steps: 57, steps per second: 192, episode reward: 36.860, mean reward: 0.647 [0.506, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.854 [-0.920, 10.189], loss: 0.001395, mae: 0.041124, mean_q: 1.184201
 79995/100000: episode: 1454, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 26.364, mean reward: 0.713 [0.642, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.035, 10.465], loss: 0.001643, mae: 0.043876, mean_q: 1.180624
 80044/100000: episode: 1455, duration: 0.254s, episode steps: 49, steps per second: 193, episode reward: 32.555, mean reward: 0.664 [0.561, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.362, 10.202], loss: 0.001392, mae: 0.040663, mean_q: 1.175545
 80101/100000: episode: 1456, duration: 0.299s, episode steps: 57, steps per second: 190, episode reward: 40.714, mean reward: 0.714 [0.564, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.435, 10.100], loss: 0.001436, mae: 0.041220, mean_q: 1.179306
 80138/100000: episode: 1457, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 24.367, mean reward: 0.659 [0.574, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.041, 10.315], loss: 0.001522, mae: 0.041817, mean_q: 1.181335
 80179/100000: episode: 1458, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 26.186, mean reward: 0.639 [0.548, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.432, 10.100], loss: 0.001645, mae: 0.042444, mean_q: 1.172177
 80228/100000: episode: 1459, duration: 0.259s, episode steps: 49, steps per second: 189, episode reward: 36.763, mean reward: 0.750 [0.658, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.736, 10.433], loss: 0.001457, mae: 0.042065, mean_q: 1.186490
 80266/100000: episode: 1460, duration: 0.209s, episode steps: 38, steps per second: 181, episode reward: 22.978, mean reward: 0.605 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.779, 10.149], loss: 0.001560, mae: 0.043045, mean_q: 1.187375
 80323/100000: episode: 1461, duration: 0.299s, episode steps: 57, steps per second: 191, episode reward: 38.181, mean reward: 0.670 [0.524, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-0.692, 10.100], loss: 0.001825, mae: 0.046208, mean_q: 1.185888
 80360/100000: episode: 1462, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 23.296, mean reward: 0.630 [0.509, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.778, 10.168], loss: 0.001499, mae: 0.041822, mean_q: 1.184243
 80406/100000: episode: 1463, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 31.291, mean reward: 0.680 [0.600, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.250, 10.100], loss: 0.001640, mae: 0.042774, mean_q: 1.191242
 80463/100000: episode: 1464, duration: 0.310s, episode steps: 57, steps per second: 184, episode reward: 33.807, mean reward: 0.593 [0.515, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.591, 10.157], loss: 0.001671, mae: 0.043688, mean_q: 1.185330
 80470/100000: episode: 1465, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.825, mean reward: 0.689 [0.646, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.337, 10.100], loss: 0.001789, mae: 0.045851, mean_q: 1.179600
 80507/100000: episode: 1466, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 25.353, mean reward: 0.685 [0.601, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.641, 10.468], loss: 0.001483, mae: 0.041388, mean_q: 1.187166
 80564/100000: episode: 1467, duration: 0.320s, episode steps: 57, steps per second: 178, episode reward: 40.106, mean reward: 0.704 [0.594, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.834 [-0.617, 10.100], loss: 0.001554, mae: 0.042078, mean_q: 1.189052
 80577/100000: episode: 1468, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 9.044, mean reward: 0.696 [0.637, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.219, 10.100], loss: 0.001499, mae: 0.042314, mean_q: 1.197155
 80618/100000: episode: 1469, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 26.280, mean reward: 0.641 [0.536, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.207, 10.100], loss: 0.001607, mae: 0.042941, mean_q: 1.192048
 80631/100000: episode: 1470, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 9.374, mean reward: 0.721 [0.678, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.283, 10.100], loss: 0.001596, mae: 0.041365, mean_q: 1.192997
 80688/100000: episode: 1471, duration: 0.303s, episode steps: 57, steps per second: 188, episode reward: 35.533, mean reward: 0.623 [0.516, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-0.686, 10.100], loss: 0.001760, mae: 0.045170, mean_q: 1.195515
 80730/100000: episode: 1472, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 26.870, mean reward: 0.640 [0.560, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.606, 10.100], loss: 0.001681, mae: 0.043226, mean_q: 1.197057
 80787/100000: episode: 1473, duration: 0.330s, episode steps: 57, steps per second: 173, episode reward: 41.681, mean reward: 0.731 [0.638, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-1.002, 10.100], loss: 0.001804, mae: 0.044925, mean_q: 1.199128
 80824/100000: episode: 1474, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 27.796, mean reward: 0.751 [0.655, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.158, 10.416], loss: 0.001595, mae: 0.043082, mean_q: 1.202683
 80837/100000: episode: 1475, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 8.978, mean reward: 0.691 [0.634, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.465, 10.100], loss: 0.001653, mae: 0.044973, mean_q: 1.197044
 80844/100000: episode: 1476, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 5.135, mean reward: 0.734 [0.671, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.281, 10.100], loss: 0.001627, mae: 0.044848, mean_q: 1.207223
 80887/100000: episode: 1477, duration: 0.239s, episode steps: 43, steps per second: 180, episode reward: 27.181, mean reward: 0.632 [0.547, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.221, 10.100], loss: 0.001502, mae: 0.041767, mean_q: 1.197180
 80929/100000: episode: 1478, duration: 0.243s, episode steps: 42, steps per second: 173, episode reward: 28.011, mean reward: 0.667 [0.596, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.860, 10.100], loss: 0.001777, mae: 0.044123, mean_q: 1.204554
 80972/100000: episode: 1479, duration: 0.221s, episode steps: 43, steps per second: 195, episode reward: 31.640, mean reward: 0.736 [0.666, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.317, 10.100], loss: 0.001813, mae: 0.045079, mean_q: 1.203953
 81029/100000: episode: 1480, duration: 0.335s, episode steps: 57, steps per second: 170, episode reward: 35.450, mean reward: 0.622 [0.500, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.412, 10.334], loss: 0.001641, mae: 0.042363, mean_q: 1.211228
 81072/100000: episode: 1481, duration: 0.240s, episode steps: 43, steps per second: 179, episode reward: 30.477, mean reward: 0.709 [0.584, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.441, 10.100], loss: 0.001897, mae: 0.046666, mean_q: 1.210400
 81118/100000: episode: 1482, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 31.673, mean reward: 0.689 [0.591, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.388, 10.100], loss: 0.002159, mae: 0.046988, mean_q: 1.205370
 81155/100000: episode: 1483, duration: 0.221s, episode steps: 37, steps per second: 168, episode reward: 24.220, mean reward: 0.655 [0.562, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.035, 10.295], loss: 0.001759, mae: 0.043964, mean_q: 1.212301
 81212/100000: episode: 1484, duration: 0.329s, episode steps: 57, steps per second: 173, episode reward: 36.760, mean reward: 0.645 [0.531, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.838 [-0.734, 10.100], loss: 0.001915, mae: 0.044690, mean_q: 1.212946
 81253/100000: episode: 1485, duration: 0.213s, episode steps: 41, steps per second: 192, episode reward: 26.993, mean reward: 0.658 [0.555, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-1.051, 10.100], loss: 0.001656, mae: 0.043469, mean_q: 1.205782
 81302/100000: episode: 1486, duration: 0.261s, episode steps: 49, steps per second: 188, episode reward: 30.784, mean reward: 0.628 [0.502, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.330, 10.205], loss: 0.001735, mae: 0.043538, mean_q: 1.217236
 81345/100000: episode: 1487, duration: 0.235s, episode steps: 43, steps per second: 183, episode reward: 29.638, mean reward: 0.689 [0.547, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.501, 10.100], loss: 0.001840, mae: 0.044483, mean_q: 1.216205
 81386/100000: episode: 1488, duration: 0.227s, episode steps: 41, steps per second: 181, episode reward: 26.323, mean reward: 0.642 [0.511, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.493, 10.100], loss: 0.001977, mae: 0.046422, mean_q: 1.217938
 81393/100000: episode: 1489, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 5.394, mean reward: 0.771 [0.736, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.613, 10.100], loss: 0.001459, mae: 0.041434, mean_q: 1.220227
 81400/100000: episode: 1490, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 5.215, mean reward: 0.745 [0.715, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.204, 10.100], loss: 0.001748, mae: 0.044703, mean_q: 1.207482
 81437/100000: episode: 1491, duration: 0.204s, episode steps: 37, steps per second: 182, episode reward: 27.069, mean reward: 0.732 [0.644, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.402, 10.496], loss: 0.001705, mae: 0.043583, mean_q: 1.216694
 81478/100000: episode: 1492, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 29.793, mean reward: 0.727 [0.661, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.819, 10.100], loss: 0.001664, mae: 0.043781, mean_q: 1.219890
 81519/100000: episode: 1493, duration: 0.221s, episode steps: 41, steps per second: 186, episode reward: 25.699, mean reward: 0.627 [0.498, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.561, 10.140], loss: 0.001667, mae: 0.044032, mean_q: 1.226594
 81562/100000: episode: 1494, duration: 0.236s, episode steps: 43, steps per second: 182, episode reward: 28.089, mean reward: 0.653 [0.532, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.503, 10.100], loss: 0.001642, mae: 0.043197, mean_q: 1.221201
 81619/100000: episode: 1495, duration: 0.320s, episode steps: 57, steps per second: 178, episode reward: 36.926, mean reward: 0.648 [0.499, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-0.158, 10.100], loss: 0.001592, mae: 0.042678, mean_q: 1.222334
 81661/100000: episode: 1496, duration: 0.238s, episode steps: 42, steps per second: 177, episode reward: 27.447, mean reward: 0.653 [0.575, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.154, 10.100], loss: 0.001723, mae: 0.043791, mean_q: 1.218168
 81707/100000: episode: 1497, duration: 0.244s, episode steps: 46, steps per second: 189, episode reward: 28.691, mean reward: 0.624 [0.509, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.410, 10.120], loss: 0.001675, mae: 0.043827, mean_q: 1.232771
 81749/100000: episode: 1498, duration: 0.214s, episode steps: 42, steps per second: 196, episode reward: 26.311, mean reward: 0.626 [0.520, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.292, 10.252], loss: 0.001445, mae: 0.040866, mean_q: 1.218744
 81787/100000: episode: 1499, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 23.828, mean reward: 0.627 [0.532, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.944, 10.100], loss: 0.001668, mae: 0.043218, mean_q: 1.221654
 81825/100000: episode: 1500, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 29.456, mean reward: 0.775 [0.638, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.306, 10.100], loss: 0.001640, mae: 0.043446, mean_q: 1.225643
 81868/100000: episode: 1501, duration: 0.228s, episode steps: 43, steps per second: 189, episode reward: 27.043, mean reward: 0.629 [0.517, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.630, 10.150], loss: 0.001881, mae: 0.045799, mean_q: 1.227735
 81917/100000: episode: 1502, duration: 0.264s, episode steps: 49, steps per second: 185, episode reward: 32.451, mean reward: 0.662 [0.562, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.570, 10.386], loss: 0.002020, mae: 0.047129, mean_q: 1.220821
 81963/100000: episode: 1503, duration: 0.249s, episode steps: 46, steps per second: 185, episode reward: 27.681, mean reward: 0.602 [0.528, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.368, 10.100], loss: 0.001567, mae: 0.043029, mean_q: 1.232841
 81970/100000: episode: 1504, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 5.034, mean reward: 0.719 [0.679, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.305, 10.100], loss: 0.001782, mae: 0.045428, mean_q: 1.229282
 82013/100000: episode: 1505, duration: 0.244s, episode steps: 43, steps per second: 176, episode reward: 29.180, mean reward: 0.679 [0.567, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.368, 10.100], loss: 0.001691, mae: 0.042440, mean_q: 1.235368
 82050/100000: episode: 1506, duration: 0.214s, episode steps: 37, steps per second: 173, episode reward: 27.549, mean reward: 0.745 [0.694, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.092, 10.436], loss: 0.001686, mae: 0.043902, mean_q: 1.234267
 82088/100000: episode: 1507, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 25.703, mean reward: 0.676 [0.578, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.789, 10.100], loss: 0.001613, mae: 0.043255, mean_q: 1.237018
 82125/100000: episode: 1508, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 24.588, mean reward: 0.665 [0.551, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.332, 10.299], loss: 0.001594, mae: 0.043820, mean_q: 1.240648
 82174/100000: episode: 1509, duration: 0.268s, episode steps: 49, steps per second: 183, episode reward: 37.047, mean reward: 0.756 [0.669, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.601, 10.650], loss: 0.001393, mae: 0.040615, mean_q: 1.242042
 82216/100000: episode: 1510, duration: 0.223s, episode steps: 42, steps per second: 189, episode reward: 29.830, mean reward: 0.710 [0.634, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.378, 10.100], loss: 0.001650, mae: 0.042043, mean_q: 1.236374
 82254/100000: episode: 1511, duration: 0.220s, episode steps: 38, steps per second: 173, episode reward: 25.988, mean reward: 0.684 [0.565, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.805, 10.100], loss: 0.001815, mae: 0.045643, mean_q: 1.232771
 82303/100000: episode: 1512, duration: 0.258s, episode steps: 49, steps per second: 190, episode reward: 31.432, mean reward: 0.641 [0.574, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.347, 10.395], loss: 0.001692, mae: 0.043816, mean_q: 1.244120
 82341/100000: episode: 1513, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 25.521, mean reward: 0.672 [0.601, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.284, 10.100], loss: 0.001651, mae: 0.043162, mean_q: 1.240259
 82398/100000: episode: 1514, duration: 0.320s, episode steps: 57, steps per second: 178, episode reward: 39.651, mean reward: 0.696 [0.588, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.835 [-1.268, 10.100], loss: 0.001779, mae: 0.044370, mean_q: 1.250361
 82405/100000: episode: 1515, duration: 0.052s, episode steps: 7, steps per second: 134, episode reward: 5.407, mean reward: 0.772 [0.696, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.354, 10.100], loss: 0.001807, mae: 0.044349, mean_q: 1.220083
 82448/100000: episode: 1516, duration: 0.219s, episode steps: 43, steps per second: 196, episode reward: 34.968, mean reward: 0.813 [0.740, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.842, 10.100], loss: 0.001703, mae: 0.043134, mean_q: 1.251905
 82505/100000: episode: 1517, duration: 0.301s, episode steps: 57, steps per second: 189, episode reward: 33.912, mean reward: 0.595 [0.507, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-1.487, 10.249], loss: 0.001503, mae: 0.041271, mean_q: 1.250078
 82548/100000: episode: 1518, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 27.845, mean reward: 0.648 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.991, 10.105], loss: 0.001612, mae: 0.042343, mean_q: 1.251639
 82605/100000: episode: 1519, duration: 0.298s, episode steps: 57, steps per second: 191, episode reward: 35.445, mean reward: 0.622 [0.533, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-1.496, 10.100], loss: 0.001583, mae: 0.043153, mean_q: 1.252127
 82662/100000: episode: 1520, duration: 0.288s, episode steps: 57, steps per second: 198, episode reward: 42.255, mean reward: 0.741 [0.623, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-0.252, 10.100], loss: 0.001609, mae: 0.042684, mean_q: 1.258320
 82669/100000: episode: 1521, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 4.888, mean reward: 0.698 [0.667, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.237, 10.100], loss: 0.001527, mae: 0.043474, mean_q: 1.263294
 82707/100000: episode: 1522, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 23.722, mean reward: 0.624 [0.514, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.879, 10.100], loss: 0.001341, mae: 0.039505, mean_q: 1.249454
 82764/100000: episode: 1523, duration: 0.318s, episode steps: 57, steps per second: 179, episode reward: 34.552, mean reward: 0.606 [0.500, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.160, 10.267], loss: 0.001676, mae: 0.042928, mean_q: 1.256291
 82806/100000: episode: 1524, duration: 0.245s, episode steps: 42, steps per second: 171, episode reward: 32.647, mean reward: 0.777 [0.653, 0.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.931, 10.100], loss: 0.001491, mae: 0.042257, mean_q: 1.256790
 82847/100000: episode: 1525, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 27.820, mean reward: 0.679 [0.556, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.310, 10.100], loss: 0.001555, mae: 0.041460, mean_q: 1.249009
 82888/100000: episode: 1526, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 26.359, mean reward: 0.643 [0.553, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.287, 10.100], loss: 0.001598, mae: 0.042419, mean_q: 1.256284
 82929/100000: episode: 1527, duration: 0.243s, episode steps: 41, steps per second: 169, episode reward: 28.368, mean reward: 0.692 [0.626, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.412, 10.100], loss: 0.001402, mae: 0.040444, mean_q: 1.252664
 82967/100000: episode: 1528, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 24.457, mean reward: 0.644 [0.570, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.860, 10.100], loss: 0.001492, mae: 0.042206, mean_q: 1.269045
 83008/100000: episode: 1529, duration: 0.222s, episode steps: 41, steps per second: 185, episode reward: 28.475, mean reward: 0.695 [0.592, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.289, 10.100], loss: 0.001344, mae: 0.039945, mean_q: 1.261843
 83049/100000: episode: 1530, duration: 0.222s, episode steps: 41, steps per second: 185, episode reward: 27.193, mean reward: 0.663 [0.549, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.079, 10.100], loss: 0.001594, mae: 0.042976, mean_q: 1.258862
 83091/100000: episode: 1531, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 28.103, mean reward: 0.669 [0.523, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.207, 10.100], loss: 0.001936, mae: 0.046957, mean_q: 1.271964
 83133/100000: episode: 1532, duration: 0.221s, episode steps: 42, steps per second: 190, episode reward: 26.783, mean reward: 0.638 [0.523, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.164, 10.421], loss: 0.001455, mae: 0.040925, mean_q: 1.269668
 83174/100000: episode: 1533, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 25.363, mean reward: 0.619 [0.518, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.557, 10.100], loss: 0.001706, mae: 0.043982, mean_q: 1.275275
 83216/100000: episode: 1534, duration: 0.224s, episode steps: 42, steps per second: 188, episode reward: 29.649, mean reward: 0.706 [0.626, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.389, 10.100], loss: 0.001573, mae: 0.042065, mean_q: 1.261079
 83258/100000: episode: 1535, duration: 0.239s, episode steps: 42, steps per second: 176, episode reward: 27.707, mean reward: 0.660 [0.594, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.589, 10.100], loss: 0.001325, mae: 0.038739, mean_q: 1.266795
 83265/100000: episode: 1536, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 4.581, mean reward: 0.654 [0.642, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.437, 10.100], loss: 0.001773, mae: 0.043540, mean_q: 1.295012
 83278/100000: episode: 1537, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 9.195, mean reward: 0.707 [0.658, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.639, 10.100], loss: 0.002044, mae: 0.044537, mean_q: 1.251360
 83335/100000: episode: 1538, duration: 0.314s, episode steps: 57, steps per second: 181, episode reward: 36.546, mean reward: 0.641 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.183, 10.100], loss: 0.001580, mae: 0.042555, mean_q: 1.268890
 83342/100000: episode: 1539, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 4.764, mean reward: 0.681 [0.628, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.266, 10.100], loss: 0.001310, mae: 0.039971, mean_q: 1.272486
[Info] 2-TH LEVEL FOUND: 1.5653165578842163, Considering 10/90 traces
 83391/100000: episode: 1540, duration: 4.554s, episode steps: 49, steps per second: 11, episode reward: 35.001, mean reward: 0.714 [0.636, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.420, 10.358], loss: 0.001314, mae: 0.039319, mean_q: 1.276612
 83433/100000: episode: 1541, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 28.971, mean reward: 0.690 [0.507, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.683, 10.123], loss: 0.001281, mae: 0.038787, mean_q: 1.274607
 83465/100000: episode: 1542, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 25.643, mean reward: 0.801 [0.722, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.370, 10.100], loss: 0.001449, mae: 0.040966, mean_q: 1.276772
 83507/100000: episode: 1543, duration: 0.221s, episode steps: 42, steps per second: 190, episode reward: 28.488, mean reward: 0.678 [0.540, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-1.052, 10.100], loss: 0.001347, mae: 0.039530, mean_q: 1.280098
 83537/100000: episode: 1544, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 22.080, mean reward: 0.736 [0.591, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.099, 10.100], loss: 0.001720, mae: 0.041830, mean_q: 1.283917
 83567/100000: episode: 1545, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 20.372, mean reward: 0.679 [0.597, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.373, 10.100], loss: 0.001491, mae: 0.041520, mean_q: 1.278358
 83597/100000: episode: 1546, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 21.966, mean reward: 0.732 [0.597, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.292, 10.100], loss: 0.001572, mae: 0.041576, mean_q: 1.275014
 83629/100000: episode: 1547, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 25.190, mean reward: 0.787 [0.703, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.302, 10.100], loss: 0.001429, mae: 0.040688, mean_q: 1.271799
 83655/100000: episode: 1548, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 19.333, mean reward: 0.744 [0.672, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.317, 10.100], loss: 0.001493, mae: 0.040600, mean_q: 1.270629
 83681/100000: episode: 1549, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 18.605, mean reward: 0.716 [0.626, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.788, 10.100], loss: 0.001356, mae: 0.039059, mean_q: 1.289818
[Info] FALSIFICATION!
 83703/100000: episode: 1550, duration: 0.290s, episode steps: 22, steps per second: 76, episode reward: 18.840, mean reward: 0.856 [0.755, 1.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.748, 10.067], loss: 0.001567, mae: 0.041629, mean_q: 1.286892
 83729/100000: episode: 1551, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 19.760, mean reward: 0.760 [0.715, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.706, 10.513], loss: 0.001322, mae: 0.039496, mean_q: 1.291619
 83771/100000: episode: 1552, duration: 0.254s, episode steps: 42, steps per second: 165, episode reward: 27.817, mean reward: 0.662 [0.524, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-1.594, 10.210], loss: 0.001361, mae: 0.040681, mean_q: 1.287369
 83795/100000: episode: 1553, duration: 0.152s, episode steps: 24, steps per second: 158, episode reward: 18.579, mean reward: 0.774 [0.693, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.351, 10.100], loss: 0.001362, mae: 0.040496, mean_q: 1.290420
 83819/100000: episode: 1554, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 17.430, mean reward: 0.726 [0.609, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.345, 10.100], loss: 0.001999, mae: 0.046644, mean_q: 1.295413
 83845/100000: episode: 1555, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 18.851, mean reward: 0.725 [0.633, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.429, 10.100], loss: 0.001767, mae: 0.044350, mean_q: 1.293742
 83869/100000: episode: 1556, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 15.876, mean reward: 0.661 [0.571, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.238, 10.100], loss: 0.001776, mae: 0.042158, mean_q: 1.289954
 83895/100000: episode: 1557, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 17.779, mean reward: 0.684 [0.550, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.401, 10.284], loss: 0.001474, mae: 0.042066, mean_q: 1.294710
 83921/100000: episode: 1558, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 17.534, mean reward: 0.674 [0.536, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.357, 10.100], loss: 0.001452, mae: 0.039642, mean_q: 1.282927
 83945/100000: episode: 1559, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 18.655, mean reward: 0.777 [0.622, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.732, 10.100], loss: 0.001457, mae: 0.040637, mean_q: 1.296665
 83969/100000: episode: 1560, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 18.815, mean reward: 0.784 [0.722, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.532, 10.100], loss: 0.001269, mae: 0.039292, mean_q: 1.277787
 83993/100000: episode: 1561, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 17.049, mean reward: 0.710 [0.639, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.459], loss: 0.001912, mae: 0.043983, mean_q: 1.304215
 84018/100000: episode: 1562, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 18.712, mean reward: 0.748 [0.601, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.295], loss: 0.001790, mae: 0.046502, mean_q: 1.297604
 84060/100000: episode: 1563, duration: 0.237s, episode steps: 42, steps per second: 177, episode reward: 30.861, mean reward: 0.735 [0.634, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.598, 10.100], loss: 0.001305, mae: 0.040194, mean_q: 1.295725
 84086/100000: episode: 1564, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 18.343, mean reward: 0.705 [0.619, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.908, 10.407], loss: 0.001450, mae: 0.041044, mean_q: 1.299080
 84112/100000: episode: 1565, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 18.074, mean reward: 0.695 [0.587, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.292], loss: 0.001576, mae: 0.041414, mean_q: 1.304415
 84154/100000: episode: 1566, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 31.131, mean reward: 0.741 [0.620, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.601, 10.100], loss: 0.001859, mae: 0.044848, mean_q: 1.301046
 84180/100000: episode: 1567, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 20.677, mean reward: 0.795 [0.742, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.192, 10.534], loss: 0.001482, mae: 0.042021, mean_q: 1.305240
 84222/100000: episode: 1568, duration: 0.221s, episode steps: 42, steps per second: 190, episode reward: 29.077, mean reward: 0.692 [0.554, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.667, 10.100], loss: 0.001367, mae: 0.038290, mean_q: 1.317561
 84254/100000: episode: 1569, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 24.668, mean reward: 0.771 [0.698, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.540, 10.100], loss: 0.001319, mae: 0.039434, mean_q: 1.304737
 84278/100000: episode: 1570, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 18.122, mean reward: 0.755 [0.658, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.635, 10.100], loss: 0.001992, mae: 0.044569, mean_q: 1.310252
 84320/100000: episode: 1571, duration: 0.233s, episode steps: 42, steps per second: 180, episode reward: 31.281, mean reward: 0.745 [0.590, 0.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.345, 10.100], loss: 0.001467, mae: 0.039698, mean_q: 1.318814
 84344/100000: episode: 1572, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 17.811, mean reward: 0.742 [0.641, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.319, 10.100], loss: 0.001332, mae: 0.039693, mean_q: 1.322022
 84370/100000: episode: 1573, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 18.691, mean reward: 0.719 [0.566, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.326], loss: 0.001408, mae: 0.040955, mean_q: 1.319576
 84394/100000: episode: 1574, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 16.448, mean reward: 0.685 [0.570, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.104, 10.100], loss: 0.001274, mae: 0.038639, mean_q: 1.327932
 84418/100000: episode: 1575, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 19.246, mean reward: 0.802 [0.714, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.656, 10.100], loss: 0.001823, mae: 0.043004, mean_q: 1.323592
 84442/100000: episode: 1576, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 17.162, mean reward: 0.715 [0.626, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.281, 10.100], loss: 0.001440, mae: 0.040946, mean_q: 1.330727
 84466/100000: episode: 1577, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 19.144, mean reward: 0.798 [0.703, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.305, 10.100], loss: 0.001520, mae: 0.038602, mean_q: 1.325971
 84490/100000: episode: 1578, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 18.018, mean reward: 0.751 [0.672, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.807, 10.100], loss: 0.001144, mae: 0.036414, mean_q: 1.324124
 84515/100000: episode: 1579, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 21.258, mean reward: 0.850 [0.796, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.223, 10.694], loss: 0.001406, mae: 0.040669, mean_q: 1.326062
 84545/100000: episode: 1580, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 19.023, mean reward: 0.634 [0.521, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.744, 10.210], loss: 0.001394, mae: 0.040331, mean_q: 1.333954
 84569/100000: episode: 1581, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 15.857, mean reward: 0.661 [0.520, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.610, 10.242], loss: 0.001213, mae: 0.038746, mean_q: 1.322024
 84599/100000: episode: 1582, duration: 0.183s, episode steps: 30, steps per second: 164, episode reward: 20.118, mean reward: 0.671 [0.528, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.424, 10.100], loss: 0.001253, mae: 0.038534, mean_q: 1.318339
 84625/100000: episode: 1583, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 18.227, mean reward: 0.701 [0.600, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.553, 10.100], loss: 0.001237, mae: 0.038688, mean_q: 1.324472
 84657/100000: episode: 1584, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 23.413, mean reward: 0.732 [0.626, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.122, 10.100], loss: 0.001244, mae: 0.038843, mean_q: 1.323835
 84681/100000: episode: 1585, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 17.641, mean reward: 0.735 [0.651, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.031, 10.334], loss: 0.001397, mae: 0.041460, mean_q: 1.340809
 84713/100000: episode: 1586, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 21.928, mean reward: 0.685 [0.532, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.641, 10.100], loss: 0.001266, mae: 0.039261, mean_q: 1.319930
 84737/100000: episode: 1587, duration: 0.142s, episode steps: 24, steps per second: 168, episode reward: 18.111, mean reward: 0.755 [0.624, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.123, 10.100], loss: 0.001407, mae: 0.040987, mean_q: 1.336760
 84769/100000: episode: 1588, duration: 0.175s, episode steps: 32, steps per second: 182, episode reward: 25.458, mean reward: 0.796 [0.704, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.430, 10.100], loss: 0.001232, mae: 0.038965, mean_q: 1.326760
 84793/100000: episode: 1589, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 18.161, mean reward: 0.757 [0.683, 0.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.433, 10.100], loss: 0.001204, mae: 0.038989, mean_q: 1.332467
 84819/100000: episode: 1590, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 20.269, mean reward: 0.780 [0.690, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.432, 10.100], loss: 0.001234, mae: 0.039040, mean_q: 1.334053
 84861/100000: episode: 1591, duration: 0.205s, episode steps: 42, steps per second: 204, episode reward: 31.400, mean reward: 0.748 [0.617, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.418, 10.100], loss: 0.001181, mae: 0.037707, mean_q: 1.342935
 84885/100000: episode: 1592, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 17.306, mean reward: 0.721 [0.622, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.273, 10.100], loss: 0.001325, mae: 0.040453, mean_q: 1.337509
 84909/100000: episode: 1593, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 17.868, mean reward: 0.744 [0.685, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.338, 10.100], loss: 0.001198, mae: 0.037906, mean_q: 1.338471
 84934/100000: episode: 1594, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 19.245, mean reward: 0.770 [0.668, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.157, 10.463], loss: 0.001316, mae: 0.040354, mean_q: 1.339523
 84976/100000: episode: 1595, duration: 0.236s, episode steps: 42, steps per second: 178, episode reward: 29.124, mean reward: 0.693 [0.562, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-1.269, 10.145], loss: 0.001377, mae: 0.039591, mean_q: 1.339846
 85002/100000: episode: 1596, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 17.026, mean reward: 0.655 [0.538, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.251], loss: 0.001212, mae: 0.038368, mean_q: 1.347690
 85032/100000: episode: 1597, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 21.082, mean reward: 0.703 [0.527, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.794, 10.100], loss: 0.001402, mae: 0.038171, mean_q: 1.331137
 85056/100000: episode: 1598, duration: 0.157s, episode steps: 24, steps per second: 153, episode reward: 18.457, mean reward: 0.769 [0.694, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.287, 10.100], loss: 0.001324, mae: 0.041098, mean_q: 1.347008
 85081/100000: episode: 1599, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 18.928, mean reward: 0.757 [0.618, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.746, 10.432], loss: 0.001288, mae: 0.039636, mean_q: 1.330130
 85107/100000: episode: 1600, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 19.313, mean reward: 0.743 [0.613, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.634, 10.100], loss: 0.001311, mae: 0.039308, mean_q: 1.343299
 85131/100000: episode: 1601, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 18.892, mean reward: 0.787 [0.713, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.633, 10.505], loss: 0.001291, mae: 0.039609, mean_q: 1.344751
 85155/100000: episode: 1602, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 16.369, mean reward: 0.682 [0.589, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.720, 10.100], loss: 0.001666, mae: 0.041792, mean_q: 1.336827
 85197/100000: episode: 1603, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 27.784, mean reward: 0.662 [0.563, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.844, 10.100], loss: 0.001271, mae: 0.039046, mean_q: 1.343944
 85222/100000: episode: 1604, duration: 0.161s, episode steps: 25, steps per second: 155, episode reward: 18.887, mean reward: 0.755 [0.705, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.666, 10.451], loss: 0.001163, mae: 0.038346, mean_q: 1.341027
 85246/100000: episode: 1605, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 19.246, mean reward: 0.802 [0.697, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.179, 10.514], loss: 0.001202, mae: 0.038317, mean_q: 1.355180
 85271/100000: episode: 1606, duration: 0.149s, episode steps: 25, steps per second: 167, episode reward: 20.051, mean reward: 0.802 [0.730, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.835, 10.583], loss: 0.001230, mae: 0.038579, mean_q: 1.343769
 85295/100000: episode: 1607, duration: 0.127s, episode steps: 24, steps per second: 188, episode reward: 17.271, mean reward: 0.720 [0.622, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.776, 10.400], loss: 0.001496, mae: 0.039083, mean_q: 1.341973
 85319/100000: episode: 1608, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 17.811, mean reward: 0.742 [0.638, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.901, 10.100], loss: 0.001320, mae: 0.039535, mean_q: 1.354627
 85349/100000: episode: 1609, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 20.571, mean reward: 0.686 [0.612, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.300, 10.100], loss: 0.001457, mae: 0.042898, mean_q: 1.345786
 85373/100000: episode: 1610, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 16.925, mean reward: 0.705 [0.582, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.493, 10.263], loss: 0.001357, mae: 0.040359, mean_q: 1.348920
 85397/100000: episode: 1611, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 19.343, mean reward: 0.806 [0.727, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.599, 10.612], loss: 0.001593, mae: 0.043679, mean_q: 1.357491
 85439/100000: episode: 1612, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 28.280, mean reward: 0.673 [0.558, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.126, 10.100], loss: 0.001342, mae: 0.040464, mean_q: 1.356474
 85463/100000: episode: 1613, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 16.691, mean reward: 0.695 [0.576, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.242, 10.100], loss: 0.001081, mae: 0.036692, mean_q: 1.346497
 85487/100000: episode: 1614, duration: 0.145s, episode steps: 24, steps per second: 165, episode reward: 17.891, mean reward: 0.745 [0.664, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.455, 10.100], loss: 0.001216, mae: 0.038420, mean_q: 1.339849
 85511/100000: episode: 1615, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 17.481, mean reward: 0.728 [0.642, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.049, 10.454], loss: 0.001275, mae: 0.038839, mean_q: 1.344516
 85537/100000: episode: 1616, duration: 0.126s, episode steps: 26, steps per second: 207, episode reward: 20.005, mean reward: 0.769 [0.615, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.094, 10.367], loss: 0.001359, mae: 0.041009, mean_q: 1.350367
 85579/100000: episode: 1617, duration: 0.216s, episode steps: 42, steps per second: 195, episode reward: 35.232, mean reward: 0.839 [0.752, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.964, 10.100], loss: 0.001268, mae: 0.039427, mean_q: 1.351933
 85603/100000: episode: 1618, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 17.583, mean reward: 0.733 [0.611, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.982, 10.100], loss: 0.001506, mae: 0.043557, mean_q: 1.362826
 85627/100000: episode: 1619, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 17.914, mean reward: 0.746 [0.656, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.664, 10.100], loss: 0.001200, mae: 0.037923, mean_q: 1.359919
 85669/100000: episode: 1620, duration: 0.233s, episode steps: 42, steps per second: 180, episode reward: 32.094, mean reward: 0.764 [0.616, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.122, 10.100], loss: 0.001349, mae: 0.040901, mean_q: 1.363599
 85693/100000: episode: 1621, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 17.479, mean reward: 0.728 [0.658, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.405, 10.100], loss: 0.001677, mae: 0.044096, mean_q: 1.349162
 85719/100000: episode: 1622, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 19.643, mean reward: 0.756 [0.723, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.284, 10.514], loss: 0.001242, mae: 0.038013, mean_q: 1.360739
 85743/100000: episode: 1623, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 18.164, mean reward: 0.757 [0.703, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.759, 10.100], loss: 0.001382, mae: 0.041421, mean_q: 1.347401
 85769/100000: episode: 1624, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 18.465, mean reward: 0.710 [0.575, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.505, 10.281], loss: 0.001231, mae: 0.038488, mean_q: 1.357416
 85793/100000: episode: 1625, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 17.391, mean reward: 0.725 [0.650, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.394, 10.100], loss: 0.001225, mae: 0.038254, mean_q: 1.365756
 85819/100000: episode: 1626, duration: 0.151s, episode steps: 26, steps per second: 173, episode reward: 20.495, mean reward: 0.788 [0.734, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.871, 10.563], loss: 0.001237, mae: 0.038030, mean_q: 1.344973
 85844/100000: episode: 1627, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 20.363, mean reward: 0.815 [0.728, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.765, 10.512], loss: 0.001703, mae: 0.041498, mean_q: 1.371377
 85868/100000: episode: 1628, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 17.943, mean reward: 0.748 [0.677, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.498, 10.100], loss: 0.001097, mae: 0.036025, mean_q: 1.362051
 85892/100000: episode: 1629, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 15.956, mean reward: 0.665 [0.567, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.407, 10.243], loss: 0.001233, mae: 0.039424, mean_q: 1.353150
[Info] Complete ISplit Iteration
[Info] Levels: [1.4195889, 1.5653166, 1.7933736]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 85918/100000: episode: 1630, duration: 4.574s, episode steps: 26, steps per second: 6, episode reward: 17.412, mean reward: 0.670 [0.538, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.560, 10.297], loss: 0.001298, mae: 0.039078, mean_q: 1.350847
 86018/100000: episode: 1631, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 56.911, mean reward: 0.569 [0.498, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.648, 10.098], loss: 0.001477, mae: 0.040007, mean_q: 1.360615
 86118/100000: episode: 1632, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 61.140, mean reward: 0.611 [0.510, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.823, 10.270], loss: 0.001405, mae: 0.039393, mean_q: 1.344358
 86218/100000: episode: 1633, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 61.149, mean reward: 0.611 [0.500, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.394, 10.098], loss: 0.001337, mae: 0.038806, mean_q: 1.355155
 86318/100000: episode: 1634, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 63.040, mean reward: 0.630 [0.501, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.388, 10.098], loss: 0.001474, mae: 0.041704, mean_q: 1.359106
 86418/100000: episode: 1635, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.895, mean reward: 0.599 [0.506, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.615, 10.283], loss: 0.001394, mae: 0.040679, mean_q: 1.341687
 86518/100000: episode: 1636, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.059, mean reward: 0.591 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.239, 10.098], loss: 0.001592, mae: 0.040643, mean_q: 1.343680
 86618/100000: episode: 1637, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.406, mean reward: 0.584 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.210, 10.173], loss: 0.001493, mae: 0.041861, mean_q: 1.350607
 86718/100000: episode: 1638, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.753, mean reward: 0.598 [0.509, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.782, 10.098], loss: 0.001690, mae: 0.045060, mean_q: 1.344004
 86818/100000: episode: 1639, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.900, mean reward: 0.579 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.187, 10.150], loss: 0.001427, mae: 0.040659, mean_q: 1.342756
 86918/100000: episode: 1640, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 61.066, mean reward: 0.611 [0.507, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.436, 10.312], loss: 0.001437, mae: 0.041278, mean_q: 1.337565
 87018/100000: episode: 1641, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 59.218, mean reward: 0.592 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.510, 10.141], loss: 0.001415, mae: 0.040712, mean_q: 1.338681
 87118/100000: episode: 1642, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.500, mean reward: 0.585 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.073, 10.098], loss: 0.001678, mae: 0.043397, mean_q: 1.322384
 87218/100000: episode: 1643, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.536, mean reward: 0.595 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.885, 10.098], loss: 0.001761, mae: 0.043942, mean_q: 1.327294
 87318/100000: episode: 1644, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 58.865, mean reward: 0.589 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.688, 10.315], loss: 0.001561, mae: 0.042040, mean_q: 1.324321
 87418/100000: episode: 1645, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.935, mean reward: 0.589 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.893, 10.216], loss: 0.001469, mae: 0.040974, mean_q: 1.325031
 87518/100000: episode: 1646, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.199, mean reward: 0.582 [0.512, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.915, 10.232], loss: 0.001779, mae: 0.044953, mean_q: 1.311113
 87618/100000: episode: 1647, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.339, mean reward: 0.583 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.406, 10.098], loss: 0.001508, mae: 0.041967, mean_q: 1.318419
 87718/100000: episode: 1648, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 58.950, mean reward: 0.589 [0.513, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.845, 10.325], loss: 0.001591, mae: 0.041976, mean_q: 1.303850
 87818/100000: episode: 1649, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.148, mean reward: 0.601 [0.506, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.349, 10.292], loss: 0.001719, mae: 0.044483, mean_q: 1.306088
 87918/100000: episode: 1650, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.425, mean reward: 0.584 [0.509, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.901, 10.128], loss: 0.001644, mae: 0.043678, mean_q: 1.306998
 88018/100000: episode: 1651, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 60.554, mean reward: 0.606 [0.503, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.974, 10.389], loss: 0.001723, mae: 0.044250, mean_q: 1.299333
 88118/100000: episode: 1652, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.004, mean reward: 0.580 [0.498, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.381, 10.098], loss: 0.001646, mae: 0.043328, mean_q: 1.300924
 88218/100000: episode: 1653, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.059, mean reward: 0.571 [0.506, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.254, 10.208], loss: 0.001680, mae: 0.043359, mean_q: 1.301571
 88318/100000: episode: 1654, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 56.468, mean reward: 0.565 [0.499, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.629, 10.098], loss: 0.001551, mae: 0.041808, mean_q: 1.292896
 88418/100000: episode: 1655, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.053, mean reward: 0.581 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.896, 10.103], loss: 0.001754, mae: 0.044303, mean_q: 1.287625
 88518/100000: episode: 1656, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 60.633, mean reward: 0.606 [0.517, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.657, 10.098], loss: 0.001762, mae: 0.044470, mean_q: 1.283417
 88618/100000: episode: 1657, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.234, mean reward: 0.592 [0.511, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.787, 10.219], loss: 0.001812, mae: 0.045244, mean_q: 1.279287
 88718/100000: episode: 1658, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 56.484, mean reward: 0.565 [0.498, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.387, 10.103], loss: 0.001769, mae: 0.044431, mean_q: 1.272320
 88818/100000: episode: 1659, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.630, mean reward: 0.586 [0.505, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.626, 10.286], loss: 0.001594, mae: 0.042755, mean_q: 1.267551
 88918/100000: episode: 1660, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 59.167, mean reward: 0.592 [0.516, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.807, 10.098], loss: 0.001715, mae: 0.044226, mean_q: 1.271161
 89018/100000: episode: 1661, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 57.409, mean reward: 0.574 [0.504, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.077, 10.098], loss: 0.001719, mae: 0.044780, mean_q: 1.264107
 89118/100000: episode: 1662, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 59.564, mean reward: 0.596 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.972, 10.098], loss: 0.001621, mae: 0.043770, mean_q: 1.255537
 89218/100000: episode: 1663, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.444, mean reward: 0.584 [0.506, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.968, 10.098], loss: 0.001837, mae: 0.045660, mean_q: 1.246563
 89318/100000: episode: 1664, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.401, mean reward: 0.584 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.066, 10.098], loss: 0.001818, mae: 0.045090, mean_q: 1.241766
 89418/100000: episode: 1665, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.402, mean reward: 0.574 [0.504, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.196, 10.123], loss: 0.001808, mae: 0.045743, mean_q: 1.239946
 89518/100000: episode: 1666, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.477, mean reward: 0.575 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.599, 10.098], loss: 0.001999, mae: 0.047561, mean_q: 1.235343
 89618/100000: episode: 1667, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.701, mean reward: 0.577 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.808, 10.157], loss: 0.001758, mae: 0.044812, mean_q: 1.234946
 89718/100000: episode: 1668, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 59.015, mean reward: 0.590 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.818, 10.098], loss: 0.001829, mae: 0.046264, mean_q: 1.229186
 89818/100000: episode: 1669, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.552, mean reward: 0.566 [0.500, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.336, 10.145], loss: 0.001701, mae: 0.044414, mean_q: 1.221549
 89918/100000: episode: 1670, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.038, mean reward: 0.580 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.242, 10.098], loss: 0.001670, mae: 0.044287, mean_q: 1.217526
 90018/100000: episode: 1671, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.613, mean reward: 0.586 [0.499, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.529, 10.301], loss: 0.001828, mae: 0.046237, mean_q: 1.211880
 90118/100000: episode: 1672, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.577, mean reward: 0.596 [0.517, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.320, 10.098], loss: 0.001673, mae: 0.044072, mean_q: 1.202598
 90218/100000: episode: 1673, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.176, mean reward: 0.582 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.662, 10.098], loss: 0.001765, mae: 0.044961, mean_q: 1.198493
 90318/100000: episode: 1674, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.863, mean reward: 0.579 [0.510, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.891, 10.124], loss: 0.001748, mae: 0.045679, mean_q: 1.195623
 90418/100000: episode: 1675, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.476, mean reward: 0.585 [0.500, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.027, 10.208], loss: 0.001504, mae: 0.041967, mean_q: 1.187593
 90518/100000: episode: 1676, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.366, mean reward: 0.574 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.211, 10.098], loss: 0.001614, mae: 0.043821, mean_q: 1.186853
 90618/100000: episode: 1677, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.736, mean reward: 0.587 [0.511, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.258, 10.120], loss: 0.001570, mae: 0.042955, mean_q: 1.180364
 90718/100000: episode: 1678, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 61.054, mean reward: 0.611 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.100, 10.098], loss: 0.001552, mae: 0.042851, mean_q: 1.170212
 90818/100000: episode: 1679, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.482, mean reward: 0.575 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.808, 10.098], loss: 0.001515, mae: 0.042390, mean_q: 1.168311
 90918/100000: episode: 1680, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.168, mean reward: 0.592 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.474, 10.255], loss: 0.001473, mae: 0.042318, mean_q: 1.162073
 91018/100000: episode: 1681, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.962, mean reward: 0.600 [0.500, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.930, 10.098], loss: 0.001454, mae: 0.042077, mean_q: 1.159856
 91118/100000: episode: 1682, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.328, mean reward: 0.583 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.674, 10.366], loss: 0.001494, mae: 0.042184, mean_q: 1.159999
 91218/100000: episode: 1683, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.781, mean reward: 0.578 [0.503, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.292, 10.278], loss: 0.001433, mae: 0.041831, mean_q: 1.161912
 91318/100000: episode: 1684, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.291, mean reward: 0.583 [0.508, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.401, 10.156], loss: 0.001415, mae: 0.041332, mean_q: 1.158693
 91418/100000: episode: 1685, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.071, mean reward: 0.591 [0.505, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.824, 10.346], loss: 0.001354, mae: 0.040504, mean_q: 1.158295
 91518/100000: episode: 1686, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.892, mean reward: 0.579 [0.498, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.271, 10.247], loss: 0.001453, mae: 0.041975, mean_q: 1.158518
 91618/100000: episode: 1687, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.965, mean reward: 0.580 [0.504, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.482, 10.177], loss: 0.001429, mae: 0.041274, mean_q: 1.159949
 91718/100000: episode: 1688, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.562, mean reward: 0.576 [0.502, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.010, 10.098], loss: 0.001382, mae: 0.041091, mean_q: 1.159920
 91818/100000: episode: 1689, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.807, mean reward: 0.578 [0.508, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.281, 10.155], loss: 0.001599, mae: 0.043282, mean_q: 1.160805
 91918/100000: episode: 1690, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 57.888, mean reward: 0.579 [0.500, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.319, 10.349], loss: 0.001392, mae: 0.040809, mean_q: 1.157711
 92018/100000: episode: 1691, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.563, mean reward: 0.576 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.898, 10.098], loss: 0.001320, mae: 0.040132, mean_q: 1.153929
 92118/100000: episode: 1692, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.817, mean reward: 0.578 [0.502, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.715, 10.098], loss: 0.001445, mae: 0.041976, mean_q: 1.156131
 92218/100000: episode: 1693, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.217, mean reward: 0.592 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.956, 10.098], loss: 0.001434, mae: 0.041367, mean_q: 1.156976
 92318/100000: episode: 1694, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 60.112, mean reward: 0.601 [0.507, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.980, 10.098], loss: 0.001399, mae: 0.040703, mean_q: 1.157638
 92418/100000: episode: 1695, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.835, mean reward: 0.598 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.279, 10.098], loss: 0.001562, mae: 0.043949, mean_q: 1.158508
 92518/100000: episode: 1696, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.245, mean reward: 0.592 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.387, 10.098], loss: 0.001426, mae: 0.041510, mean_q: 1.152984
 92618/100000: episode: 1697, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.645, mean reward: 0.586 [0.505, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.027, 10.098], loss: 0.001418, mae: 0.041223, mean_q: 1.155367
 92718/100000: episode: 1698, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 58.727, mean reward: 0.587 [0.506, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.394, 10.307], loss: 0.001379, mae: 0.040311, mean_q: 1.154408
 92818/100000: episode: 1699, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.158, mean reward: 0.602 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.924, 10.098], loss: 0.001347, mae: 0.040176, mean_q: 1.154576
 92918/100000: episode: 1700, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.906, mean reward: 0.599 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.445, 10.098], loss: 0.001358, mae: 0.040648, mean_q: 1.153489
 93018/100000: episode: 1701, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.951, mean reward: 0.590 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.856, 10.098], loss: 0.001341, mae: 0.039830, mean_q: 1.155865
 93118/100000: episode: 1702, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.047, mean reward: 0.590 [0.506, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.029, 10.098], loss: 0.001538, mae: 0.043517, mean_q: 1.155580
 93218/100000: episode: 1703, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.074, mean reward: 0.571 [0.502, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.680, 10.204], loss: 0.001363, mae: 0.040363, mean_q: 1.154347
 93318/100000: episode: 1704, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 61.706, mean reward: 0.617 [0.525, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.711, 10.277], loss: 0.001356, mae: 0.040340, mean_q: 1.156288
 93418/100000: episode: 1705, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.964, mean reward: 0.580 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.927, 10.105], loss: 0.001402, mae: 0.040717, mean_q: 1.155690
 93518/100000: episode: 1706, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.330, mean reward: 0.593 [0.510, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.525, 10.098], loss: 0.001328, mae: 0.040141, mean_q: 1.156239
 93618/100000: episode: 1707, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.417, mean reward: 0.584 [0.508, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.777, 10.098], loss: 0.001540, mae: 0.042432, mean_q: 1.158213
 93718/100000: episode: 1708, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.959, mean reward: 0.590 [0.500, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.512, 10.098], loss: 0.001440, mae: 0.041760, mean_q: 1.159601
 93818/100000: episode: 1709, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 59.570, mean reward: 0.596 [0.505, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.115, 10.249], loss: 0.001348, mae: 0.040166, mean_q: 1.158417
 93918/100000: episode: 1710, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.794, mean reward: 0.588 [0.517, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.882, 10.098], loss: 0.001531, mae: 0.042610, mean_q: 1.156943
 94018/100000: episode: 1711, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.830, mean reward: 0.598 [0.510, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.476, 10.283], loss: 0.001388, mae: 0.040358, mean_q: 1.157708
 94118/100000: episode: 1712, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 61.086, mean reward: 0.611 [0.509, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.209, 10.153], loss: 0.001415, mae: 0.040529, mean_q: 1.157226
 94218/100000: episode: 1713, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 61.163, mean reward: 0.612 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.641, 10.145], loss: 0.001364, mae: 0.040664, mean_q: 1.160269
 94318/100000: episode: 1714, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.781, mean reward: 0.598 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.743, 10.098], loss: 0.001366, mae: 0.040387, mean_q: 1.160812
 94418/100000: episode: 1715, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.244, mean reward: 0.602 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.940, 10.098], loss: 0.001436, mae: 0.041071, mean_q: 1.160701
 94518/100000: episode: 1716, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 59.291, mean reward: 0.593 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.848, 10.098], loss: 0.001389, mae: 0.040375, mean_q: 1.159395
 94618/100000: episode: 1717, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.738, mean reward: 0.577 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.902, 10.193], loss: 0.001369, mae: 0.040296, mean_q: 1.159089
 94718/100000: episode: 1718, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.978, mean reward: 0.590 [0.501, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.861, 10.296], loss: 0.001384, mae: 0.040537, mean_q: 1.163401
 94818/100000: episode: 1719, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.904, mean reward: 0.589 [0.503, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.429, 10.098], loss: 0.001425, mae: 0.041550, mean_q: 1.166779
 94918/100000: episode: 1720, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.139, mean reward: 0.581 [0.509, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.015, 10.098], loss: 0.001455, mae: 0.041524, mean_q: 1.164287
 95018/100000: episode: 1721, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.662, mean reward: 0.577 [0.500, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.873, 10.167], loss: 0.001431, mae: 0.041432, mean_q: 1.163630
 95118/100000: episode: 1722, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.432, mean reward: 0.584 [0.507, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.580, 10.281], loss: 0.001360, mae: 0.040247, mean_q: 1.163416
 95218/100000: episode: 1723, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 62.523, mean reward: 0.625 [0.514, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.419, 10.358], loss: 0.001351, mae: 0.040060, mean_q: 1.161250
 95318/100000: episode: 1724, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.432, mean reward: 0.584 [0.506, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.261, 10.098], loss: 0.001433, mae: 0.040892, mean_q: 1.166915
 95418/100000: episode: 1725, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.606, mean reward: 0.566 [0.509, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.919, 10.144], loss: 0.001417, mae: 0.040969, mean_q: 1.166803
 95518/100000: episode: 1726, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.607, mean reward: 0.576 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.416, 10.098], loss: 0.001423, mae: 0.041091, mean_q: 1.169388
 95618/100000: episode: 1727, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.893, mean reward: 0.599 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.529, 10.271], loss: 0.001452, mae: 0.041660, mean_q: 1.165769
 95718/100000: episode: 1728, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.492, mean reward: 0.585 [0.507, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.530, 10.232], loss: 0.001500, mae: 0.041875, mean_q: 1.167392
 95818/100000: episode: 1729, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.588, mean reward: 0.586 [0.509, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.512, 10.098], loss: 0.001434, mae: 0.040960, mean_q: 1.163457
[Info] 1-TH LEVEL FOUND: 1.3689650297164917, Considering 10/90 traces
 95918/100000: episode: 1730, duration: 4.756s, episode steps: 100, steps per second: 21, episode reward: 58.397, mean reward: 0.584 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.639, 10.098], loss: 0.001479, mae: 0.041670, mean_q: 1.164743
 95933/100000: episode: 1731, duration: 0.080s, episode steps: 15, steps per second: 189, episode reward: 11.253, mean reward: 0.750 [0.715, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.335, 10.100], loss: 0.001421, mae: 0.039497, mean_q: 1.159443
 95963/100000: episode: 1732, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 19.217, mean reward: 0.641 [0.562, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.867, 10.300], loss: 0.001548, mae: 0.043013, mean_q: 1.164316
 95994/100000: episode: 1733, duration: 0.171s, episode steps: 31, steps per second: 182, episode reward: 19.973, mean reward: 0.644 [0.543, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.969, 10.245], loss: 0.001638, mae: 0.045102, mean_q: 1.168508
 96014/100000: episode: 1734, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 13.065, mean reward: 0.653 [0.583, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.396, 10.100], loss: 0.001580, mae: 0.043375, mean_q: 1.160495
 96029/100000: episode: 1735, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 10.938, mean reward: 0.729 [0.629, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.252, 10.100], loss: 0.001348, mae: 0.040283, mean_q: 1.171503
 96044/100000: episode: 1736, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 10.132, mean reward: 0.675 [0.644, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.307, 10.100], loss: 0.001352, mae: 0.040119, mean_q: 1.172025
 96057/100000: episode: 1737, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 7.949, mean reward: 0.611 [0.531, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.202, 10.100], loss: 0.001602, mae: 0.043841, mean_q: 1.170150
 96067/100000: episode: 1738, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 6.866, mean reward: 0.687 [0.663, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.302, 10.100], loss: 0.001808, mae: 0.045491, mean_q: 1.170274
 96074/100000: episode: 1739, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 4.876, mean reward: 0.697 [0.627, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.270, 10.100], loss: 0.001289, mae: 0.039381, mean_q: 1.152743
 96081/100000: episode: 1740, duration: 0.051s, episode steps: 7, steps per second: 137, episode reward: 4.851, mean reward: 0.693 [0.634, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.090, 10.100], loss: 0.001438, mae: 0.042892, mean_q: 1.162201
 96094/100000: episode: 1741, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 8.260, mean reward: 0.635 [0.582, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.273, 10.100], loss: 0.001186, mae: 0.038542, mean_q: 1.175657
 96128/100000: episode: 1742, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 21.691, mean reward: 0.638 [0.521, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.832, 10.170], loss: 0.001517, mae: 0.042415, mean_q: 1.164965
 96152/100000: episode: 1743, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 16.040, mean reward: 0.668 [0.587, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.298, 10.243], loss: 0.001656, mae: 0.043425, mean_q: 1.175221
 96167/100000: episode: 1744, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 10.683, mean reward: 0.712 [0.632, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.290, 10.100], loss: 0.001380, mae: 0.040094, mean_q: 1.163851
 96191/100000: episode: 1745, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 17.492, mean reward: 0.729 [0.646, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.698, 10.332], loss: 0.001819, mae: 0.045984, mean_q: 1.171068
 96221/100000: episode: 1746, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 19.644, mean reward: 0.655 [0.581, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.350], loss: 0.001722, mae: 0.043985, mean_q: 1.167069
 96236/100000: episode: 1747, duration: 0.092s, episode steps: 15, steps per second: 164, episode reward: 10.853, mean reward: 0.724 [0.650, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.369, 10.100], loss: 0.001535, mae: 0.041131, mean_q: 1.159306
 96246/100000: episode: 1748, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 7.009, mean reward: 0.701 [0.671, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.264, 10.100], loss: 0.001290, mae: 0.040690, mean_q: 1.170600
 96256/100000: episode: 1749, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 7.529, mean reward: 0.753 [0.681, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.540, 10.100], loss: 0.001296, mae: 0.041345, mean_q: 1.176039
 96280/100000: episode: 1750, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 14.576, mean reward: 0.607 [0.541, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.194], loss: 0.001736, mae: 0.043998, mean_q: 1.168399
 96287/100000: episode: 1751, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 4.844, mean reward: 0.692 [0.636, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.144, 10.100], loss: 0.001710, mae: 0.046169, mean_q: 1.173078
 96300/100000: episode: 1752, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 8.077, mean reward: 0.621 [0.541, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.365, 10.100], loss: 0.001645, mae: 0.043958, mean_q: 1.167896
 96324/100000: episode: 1753, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 15.826, mean reward: 0.659 [0.569, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.071, 10.318], loss: 0.001665, mae: 0.042961, mean_q: 1.163631
 96348/100000: episode: 1754, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 15.827, mean reward: 0.659 [0.559, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.380], loss: 0.001626, mae: 0.045145, mean_q: 1.175869
 96368/100000: episode: 1755, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 14.128, mean reward: 0.706 [0.651, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.271, 10.100], loss: 0.001615, mae: 0.042702, mean_q: 1.160473
 96375/100000: episode: 1756, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 4.907, mean reward: 0.701 [0.638, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.219, 10.100], loss: 0.001393, mae: 0.041346, mean_q: 1.179529
 96405/100000: episode: 1757, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 19.716, mean reward: 0.657 [0.591, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.364], loss: 0.001775, mae: 0.044876, mean_q: 1.181977
 96420/100000: episode: 1758, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 10.346, mean reward: 0.690 [0.593, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.302, 10.100], loss: 0.001548, mae: 0.041845, mean_q: 1.165632
 96451/100000: episode: 1759, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 19.943, mean reward: 0.643 [0.560, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.154, 10.214], loss: 0.001738, mae: 0.044015, mean_q: 1.167186
 96475/100000: episode: 1760, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 15.015, mean reward: 0.626 [0.542, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.872, 10.189], loss: 0.001613, mae: 0.042935, mean_q: 1.179215
 96488/100000: episode: 1761, duration: 0.091s, episode steps: 13, steps per second: 142, episode reward: 8.156, mean reward: 0.627 [0.584, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.141, 10.100], loss: 0.001727, mae: 0.042633, mean_q: 1.166408
 96503/100000: episode: 1762, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 11.104, mean reward: 0.740 [0.655, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.912, 10.100], loss: 0.001741, mae: 0.044966, mean_q: 1.170937
 96534/100000: episode: 1763, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 22.430, mean reward: 0.724 [0.655, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.836, 10.543], loss: 0.001702, mae: 0.044505, mean_q: 1.175855
 96565/100000: episode: 1764, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 20.502, mean reward: 0.661 [0.595, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.398, 10.339], loss: 0.001794, mae: 0.046492, mean_q: 1.182495
 96596/100000: episode: 1765, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 21.142, mean reward: 0.682 [0.612, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.643, 10.418], loss: 0.001784, mae: 0.045000, mean_q: 1.169542
 96609/100000: episode: 1766, duration: 0.072s, episode steps: 13, steps per second: 182, episode reward: 8.730, mean reward: 0.672 [0.627, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.237, 10.100], loss: 0.001505, mae: 0.042476, mean_q: 1.172690
 96643/100000: episode: 1767, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 22.164, mean reward: 0.652 [0.522, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.732, 10.164], loss: 0.001633, mae: 0.043389, mean_q: 1.183480
 96650/100000: episode: 1768, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 4.748, mean reward: 0.678 [0.629, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.121, 10.100], loss: 0.001789, mae: 0.044279, mean_q: 1.179950
 96684/100000: episode: 1769, duration: 0.196s, episode steps: 34, steps per second: 174, episode reward: 23.264, mean reward: 0.684 [0.625, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.646, 10.364], loss: 0.001697, mae: 0.044633, mean_q: 1.178512
 96704/100000: episode: 1770, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 14.699, mean reward: 0.735 [0.667, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.180, 10.100], loss: 0.002027, mae: 0.047360, mean_q: 1.176767
 96724/100000: episode: 1771, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 14.061, mean reward: 0.703 [0.654, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.744, 10.100], loss: 0.001744, mae: 0.045833, mean_q: 1.179703
 96748/100000: episode: 1772, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 16.537, mean reward: 0.689 [0.617, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.548, 10.354], loss: 0.001729, mae: 0.044108, mean_q: 1.185462
 96772/100000: episode: 1773, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 15.068, mean reward: 0.628 [0.531, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.204], loss: 0.001530, mae: 0.042878, mean_q: 1.181171
 96792/100000: episode: 1774, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 13.194, mean reward: 0.660 [0.594, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.394, 10.100], loss: 0.001735, mae: 0.045373, mean_q: 1.185756
 96822/100000: episode: 1775, duration: 0.158s, episode steps: 30, steps per second: 189, episode reward: 18.988, mean reward: 0.633 [0.537, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.472], loss: 0.001669, mae: 0.043331, mean_q: 1.191007
 96837/100000: episode: 1776, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 10.609, mean reward: 0.707 [0.640, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.091, 10.100], loss: 0.001574, mae: 0.042763, mean_q: 1.184803
 96868/100000: episode: 1777, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 18.577, mean reward: 0.599 [0.522, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.640, 10.166], loss: 0.001843, mae: 0.045437, mean_q: 1.185160
 96888/100000: episode: 1778, duration: 0.119s, episode steps: 20, steps per second: 169, episode reward: 15.937, mean reward: 0.797 [0.702, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.396, 10.100], loss: 0.001567, mae: 0.042709, mean_q: 1.185550
 96908/100000: episode: 1779, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 14.877, mean reward: 0.744 [0.689, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.986, 10.100], loss: 0.001563, mae: 0.042801, mean_q: 1.194032
 96918/100000: episode: 1780, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 6.959, mean reward: 0.696 [0.653, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.987, 10.100], loss: 0.002159, mae: 0.049345, mean_q: 1.190502
 96933/100000: episode: 1781, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 10.337, mean reward: 0.689 [0.608, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.864, 10.100], loss: 0.001934, mae: 0.046309, mean_q: 1.189091
 96953/100000: episode: 1782, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 15.525, mean reward: 0.776 [0.693, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.384, 10.100], loss: 0.001873, mae: 0.046839, mean_q: 1.196548
 96968/100000: episode: 1783, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 10.716, mean reward: 0.714 [0.647, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.212, 10.100], loss: 0.001861, mae: 0.047490, mean_q: 1.188270
 97058/100000: episode: 1784, duration: 0.475s, episode steps: 90, steps per second: 189, episode reward: 51.015, mean reward: 0.567 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.433, 10.206], loss: 0.001756, mae: 0.044851, mean_q: 1.195318
 97065/100000: episode: 1785, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 4.718, mean reward: 0.674 [0.589, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.096, 10.100], loss: 0.001318, mae: 0.039974, mean_q: 1.185099
 97155/100000: episode: 1786, duration: 0.495s, episode steps: 90, steps per second: 182, episode reward: 54.916, mean reward: 0.610 [0.505, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.481, 10.218], loss: 0.001833, mae: 0.045916, mean_q: 1.190963
 97170/100000: episode: 1787, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 10.877, mean reward: 0.725 [0.664, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.182, 10.100], loss: 0.001949, mae: 0.048059, mean_q: 1.196938
 97260/100000: episode: 1788, duration: 0.488s, episode steps: 90, steps per second: 184, episode reward: 52.875, mean reward: 0.587 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.555 [-1.070, 10.252], loss: 0.001878, mae: 0.046724, mean_q: 1.195006
 97294/100000: episode: 1789, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 20.284, mean reward: 0.597 [0.539, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.035, 10.227], loss: 0.001717, mae: 0.044158, mean_q: 1.191167
 97304/100000: episode: 1790, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 7.508, mean reward: 0.751 [0.664, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.422, 10.100], loss: 0.001546, mae: 0.041069, mean_q: 1.180088
 97324/100000: episode: 1791, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 13.563, mean reward: 0.678 [0.617, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.308, 10.100], loss: 0.001527, mae: 0.042771, mean_q: 1.193861
 97358/100000: episode: 1792, duration: 0.194s, episode steps: 34, steps per second: 176, episode reward: 24.019, mean reward: 0.706 [0.636, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.762, 10.373], loss: 0.001638, mae: 0.043534, mean_q: 1.186993
 97448/100000: episode: 1793, duration: 0.464s, episode steps: 90, steps per second: 194, episode reward: 53.045, mean reward: 0.589 [0.512, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.550 [-1.117, 10.100], loss: 0.001635, mae: 0.042713, mean_q: 1.203742
 97463/100000: episode: 1794, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 12.045, mean reward: 0.803 [0.743, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.730, 10.100], loss: 0.001621, mae: 0.044213, mean_q: 1.198316
 97497/100000: episode: 1795, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 20.821, mean reward: 0.612 [0.553, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.111, 10.227], loss: 0.001887, mae: 0.045706, mean_q: 1.191935
 97531/100000: episode: 1796, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 22.080, mean reward: 0.649 [0.534, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.812, 10.255], loss: 0.001742, mae: 0.044043, mean_q: 1.195830
 97562/100000: episode: 1797, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 20.151, mean reward: 0.650 [0.558, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.646, 10.220], loss: 0.002049, mae: 0.048008, mean_q: 1.191375
 97582/100000: episode: 1798, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 14.730, mean reward: 0.736 [0.680, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.217, 10.100], loss: 0.001509, mae: 0.041426, mean_q: 1.186765
 97589/100000: episode: 1799, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 4.841, mean reward: 0.692 [0.649, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.063, 10.100], loss: 0.001834, mae: 0.043576, mean_q: 1.200552
 97620/100000: episode: 1800, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 22.452, mean reward: 0.724 [0.571, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.647, 10.497], loss: 0.001798, mae: 0.045753, mean_q: 1.195379
 97650/100000: episode: 1801, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 20.139, mean reward: 0.671 [0.580, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.659, 10.294], loss: 0.001707, mae: 0.044620, mean_q: 1.197262
 97663/100000: episode: 1802, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 8.656, mean reward: 0.666 [0.622, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.287, 10.100], loss: 0.001356, mae: 0.039850, mean_q: 1.198565
 97673/100000: episode: 1803, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 7.303, mean reward: 0.730 [0.655, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-1.033, 10.100], loss: 0.001622, mae: 0.044537, mean_q: 1.205283
 97688/100000: episode: 1804, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 10.422, mean reward: 0.695 [0.636, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.213, 10.100], loss: 0.001827, mae: 0.044930, mean_q: 1.198267
 97722/100000: episode: 1805, duration: 0.184s, episode steps: 34, steps per second: 184, episode reward: 23.933, mean reward: 0.704 [0.600, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.397], loss: 0.001793, mae: 0.044847, mean_q: 1.197089
 97737/100000: episode: 1806, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 10.458, mean reward: 0.697 [0.663, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.597, 10.100], loss: 0.002030, mae: 0.048605, mean_q: 1.203626
 97768/100000: episode: 1807, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 20.499, mean reward: 0.661 [0.582, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.244, 10.216], loss: 0.001542, mae: 0.042109, mean_q: 1.198018
 97802/100000: episode: 1808, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 25.921, mean reward: 0.762 [0.674, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.113, 10.461], loss: 0.001820, mae: 0.046061, mean_q: 1.207731
 97836/100000: episode: 1809, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 24.594, mean reward: 0.723 [0.645, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.366, 10.268], loss: 0.001621, mae: 0.043449, mean_q: 1.204790
 97843/100000: episode: 1810, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 4.642, mean reward: 0.663 [0.605, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.150, 10.100], loss: 0.002083, mae: 0.049042, mean_q: 1.221490
 97856/100000: episode: 1811, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 8.494, mean reward: 0.653 [0.545, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.252, 10.100], loss: 0.001571, mae: 0.043174, mean_q: 1.207606
 97890/100000: episode: 1812, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 24.438, mean reward: 0.719 [0.640, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.147, 10.443], loss: 0.001647, mae: 0.043456, mean_q: 1.209477
 97924/100000: episode: 1813, duration: 0.167s, episode steps: 34, steps per second: 203, episode reward: 22.234, mean reward: 0.654 [0.540, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.065, 10.277], loss: 0.001662, mae: 0.043576, mean_q: 1.205875
 97954/100000: episode: 1814, duration: 0.150s, episode steps: 30, steps per second: 201, episode reward: 18.703, mean reward: 0.623 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.820, 10.104], loss: 0.001757, mae: 0.045349, mean_q: 1.213213
 97988/100000: episode: 1815, duration: 0.185s, episode steps: 34, steps per second: 183, episode reward: 22.076, mean reward: 0.649 [0.576, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.035, 10.284], loss: 0.001851, mae: 0.045597, mean_q: 1.209641
 98019/100000: episode: 1816, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 21.767, mean reward: 0.702 [0.626, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.198, 10.494], loss: 0.001628, mae: 0.042667, mean_q: 1.215830
 98050/100000: episode: 1817, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 20.105, mean reward: 0.649 [0.587, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.586, 10.298], loss: 0.001628, mae: 0.042715, mean_q: 1.208034
 98081/100000: episode: 1818, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 21.121, mean reward: 0.681 [0.609, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.677, 10.414], loss: 0.001767, mae: 0.044517, mean_q: 1.209545
 98111/100000: episode: 1819, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 19.338, mean reward: 0.645 [0.571, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.596, 10.334], loss: 0.001606, mae: 0.043032, mean_q: 1.216728
[Info] 2-TH LEVEL FOUND: 1.5020191669464111, Considering 10/90 traces
 98142/100000: episode: 1820, duration: 4.335s, episode steps: 31, steps per second: 7, episode reward: 19.208, mean reward: 0.620 [0.523, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.105], loss: 0.001672, mae: 0.043593, mean_q: 1.215103
 98148/100000: episode: 1821, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 4.810, mean reward: 0.802 [0.768, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.514, 10.100], loss: 0.001411, mae: 0.040871, mean_q: 1.211783
 98154/100000: episode: 1822, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 4.394, mean reward: 0.732 [0.712, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.346, 10.100], loss: 0.001484, mae: 0.041397, mean_q: 1.218891
 98167/100000: episode: 1823, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 9.936, mean reward: 0.764 [0.695, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.337, 10.100], loss: 0.001620, mae: 0.043337, mean_q: 1.227953
 98183/100000: episode: 1824, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 12.470, mean reward: 0.779 [0.745, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.355, 10.100], loss: 0.001669, mae: 0.042724, mean_q: 1.207619
 98208/100000: episode: 1825, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 18.094, mean reward: 0.724 [0.649, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.449], loss: 0.001759, mae: 0.043236, mean_q: 1.219891
 98225/100000: episode: 1826, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 12.438, mean reward: 0.732 [0.679, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.383, 10.100], loss: 0.001552, mae: 0.041812, mean_q: 1.219231
 98238/100000: episode: 1827, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 9.158, mean reward: 0.704 [0.604, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.192, 10.100], loss: 0.001442, mae: 0.039246, mean_q: 1.216818
 98251/100000: episode: 1828, duration: 0.086s, episode steps: 13, steps per second: 152, episode reward: 10.250, mean reward: 0.788 [0.738, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.394, 10.100], loss: 0.001723, mae: 0.043385, mean_q: 1.212654
 98263/100000: episode: 1829, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 9.649, mean reward: 0.804 [0.766, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.343, 10.100], loss: 0.001845, mae: 0.044884, mean_q: 1.219555
 98274/100000: episode: 1830, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 8.360, mean reward: 0.760 [0.701, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.524], loss: 0.001672, mae: 0.044091, mean_q: 1.228638
 98290/100000: episode: 1831, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 12.268, mean reward: 0.767 [0.700, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.281, 10.100], loss: 0.001450, mae: 0.041456, mean_q: 1.222499
 98308/100000: episode: 1832, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 13.040, mean reward: 0.724 [0.668, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.432, 10.100], loss: 0.001823, mae: 0.044876, mean_q: 1.218196
 98319/100000: episode: 1833, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 8.455, mean reward: 0.769 [0.713, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.470], loss: 0.002106, mae: 0.050083, mean_q: 1.213047
 98332/100000: episode: 1834, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 9.535, mean reward: 0.733 [0.687, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.869, 10.100], loss: 0.002038, mae: 0.047153, mean_q: 1.219309
 98349/100000: episode: 1835, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 14.308, mean reward: 0.842 [0.724, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.738, 10.100], loss: 0.001691, mae: 0.043650, mean_q: 1.226083
 98360/100000: episode: 1836, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 9.622, mean reward: 0.875 [0.783, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.658], loss: 0.001846, mae: 0.045121, mean_q: 1.231520
 98371/100000: episode: 1837, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 8.432, mean reward: 0.767 [0.722, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-1.182, 10.611], loss: 0.001897, mae: 0.044968, mean_q: 1.222369
 98377/100000: episode: 1838, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 4.703, mean reward: 0.784 [0.737, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.462, 10.100], loss: 0.001872, mae: 0.045327, mean_q: 1.215995
 98383/100000: episode: 1839, duration: 0.042s, episode steps: 6, steps per second: 141, episode reward: 4.819, mean reward: 0.803 [0.779, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.476, 10.100], loss: 0.001555, mae: 0.044057, mean_q: 1.229437
 98395/100000: episode: 1840, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 9.236, mean reward: 0.770 [0.714, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.281, 10.100], loss: 0.001447, mae: 0.040399, mean_q: 1.209412
 98406/100000: episode: 1841, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 8.451, mean reward: 0.768 [0.724, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.408], loss: 0.001396, mae: 0.040679, mean_q: 1.231130
 98418/100000: episode: 1842, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 8.978, mean reward: 0.748 [0.718, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.309, 10.100], loss: 0.001474, mae: 0.040180, mean_q: 1.234769
 98431/100000: episode: 1843, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 8.958, mean reward: 0.689 [0.612, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.167, 10.100], loss: 0.001614, mae: 0.043412, mean_q: 1.230307
 98437/100000: episode: 1844, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 4.749, mean reward: 0.791 [0.754, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.538, 10.100], loss: 0.001794, mae: 0.047066, mean_q: 1.248417
 98455/100000: episode: 1845, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 12.545, mean reward: 0.697 [0.637, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.261, 10.100], loss: 0.001721, mae: 0.042635, mean_q: 1.242859
 98466/100000: episode: 1846, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 8.981, mean reward: 0.816 [0.744, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.642], loss: 0.002083, mae: 0.048677, mean_q: 1.216665
 98491/100000: episode: 1847, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 15.580, mean reward: 0.623 [0.545, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.872, 10.272], loss: 0.001480, mae: 0.041973, mean_q: 1.234520
 98508/100000: episode: 1848, duration: 0.083s, episode steps: 17, steps per second: 206, episode reward: 13.380, mean reward: 0.787 [0.741, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.477, 10.100], loss: 0.001912, mae: 0.045004, mean_q: 1.233757
 98524/100000: episode: 1849, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 12.069, mean reward: 0.754 [0.692, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.071, 10.100], loss: 0.001532, mae: 0.042578, mean_q: 1.229942
 98535/100000: episode: 1850, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 8.201, mean reward: 0.746 [0.692, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.100, 10.476], loss: 0.001726, mae: 0.045340, mean_q: 1.225661
 98552/100000: episode: 1851, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 12.964, mean reward: 0.763 [0.716, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.436, 10.100], loss: 0.001914, mae: 0.047434, mean_q: 1.239099
 98569/100000: episode: 1852, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 12.407, mean reward: 0.730 [0.655, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.286, 10.100], loss: 0.001485, mae: 0.042846, mean_q: 1.229704
 98585/100000: episode: 1853, duration: 0.110s, episode steps: 16, steps per second: 146, episode reward: 11.896, mean reward: 0.744 [0.696, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.283, 10.100], loss: 0.001572, mae: 0.041806, mean_q: 1.237534
 98602/100000: episode: 1854, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 12.884, mean reward: 0.758 [0.702, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.404, 10.100], loss: 0.001583, mae: 0.043637, mean_q: 1.234919
 98614/100000: episode: 1855, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 9.325, mean reward: 0.777 [0.698, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.364, 10.100], loss: 0.001630, mae: 0.043871, mean_q: 1.227377
 98631/100000: episode: 1856, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 12.369, mean reward: 0.728 [0.681, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.182, 10.100], loss: 0.001698, mae: 0.044964, mean_q: 1.241212
 98642/100000: episode: 1857, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 8.659, mean reward: 0.787 [0.703, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.243, 10.452], loss: 0.001796, mae: 0.046911, mean_q: 1.242040
 98667/100000: episode: 1858, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 19.839, mean reward: 0.794 [0.698, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.600, 10.409], loss: 0.001584, mae: 0.042432, mean_q: 1.240752
 98684/100000: episode: 1859, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 13.721, mean reward: 0.807 [0.710, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.954, 10.100], loss: 0.001494, mae: 0.041450, mean_q: 1.246288
 98697/100000: episode: 1860, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 9.837, mean reward: 0.757 [0.617, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.155, 10.100], loss: 0.001574, mae: 0.042831, mean_q: 1.240606
 98710/100000: episode: 1861, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 9.984, mean reward: 0.768 [0.696, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.220, 10.100], loss: 0.001424, mae: 0.040319, mean_q: 1.247656
 98735/100000: episode: 1862, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 21.778, mean reward: 0.871 [0.797, 0.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.614, 10.656], loss: 0.001556, mae: 0.042957, mean_q: 1.247559
 98760/100000: episode: 1863, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 17.052, mean reward: 0.682 [0.564, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.257], loss: 0.001485, mae: 0.041314, mean_q: 1.244057
 98766/100000: episode: 1864, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 4.439, mean reward: 0.740 [0.711, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.465, 10.100], loss: 0.001347, mae: 0.038163, mean_q: 1.247553
 98779/100000: episode: 1865, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 10.474, mean reward: 0.806 [0.749, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.309, 10.100], loss: 0.001540, mae: 0.042006, mean_q: 1.241500
 98785/100000: episode: 1866, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 4.573, mean reward: 0.762 [0.745, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.360, 10.100], loss: 0.001371, mae: 0.041700, mean_q: 1.239859
 98802/100000: episode: 1867, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 12.217, mean reward: 0.719 [0.673, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.302, 10.100], loss: 0.001930, mae: 0.046269, mean_q: 1.254761
 98820/100000: episode: 1868, duration: 0.115s, episode steps: 18, steps per second: 156, episode reward: 13.353, mean reward: 0.742 [0.681, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.114, 10.100], loss: 0.001384, mae: 0.040414, mean_q: 1.224783
 98833/100000: episode: 1869, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 9.368, mean reward: 0.721 [0.646, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.394, 10.100], loss: 0.001265, mae: 0.038959, mean_q: 1.252598
 98845/100000: episode: 1870, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 8.616, mean reward: 0.718 [0.635, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-2.046, 10.100], loss: 0.002033, mae: 0.046026, mean_q: 1.251678
 98863/100000: episode: 1871, duration: 0.109s, episode steps: 18, steps per second: 166, episode reward: 12.251, mean reward: 0.681 [0.524, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.562, 10.100], loss: 0.001713, mae: 0.044642, mean_q: 1.253772
 98881/100000: episode: 1872, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 13.601, mean reward: 0.756 [0.703, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.660, 10.100], loss: 0.002128, mae: 0.051119, mean_q: 1.250635
 98898/100000: episode: 1873, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 11.919, mean reward: 0.701 [0.603, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.312, 10.100], loss: 0.002275, mae: 0.050193, mean_q: 1.252595
 98916/100000: episode: 1874, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 13.093, mean reward: 0.727 [0.586, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.372, 10.100], loss: 0.001785, mae: 0.044851, mean_q: 1.248861
 98922/100000: episode: 1875, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 4.729, mean reward: 0.788 [0.773, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.297, 10.100], loss: 0.001491, mae: 0.041383, mean_q: 1.240560
 98928/100000: episode: 1876, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 4.517, mean reward: 0.753 [0.734, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.379, 10.100], loss: 0.001768, mae: 0.045381, mean_q: 1.258808
 98953/100000: episode: 1877, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 18.585, mean reward: 0.743 [0.652, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.335, 10.531], loss: 0.001743, mae: 0.044289, mean_q: 1.248888
 98970/100000: episode: 1878, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 12.584, mean reward: 0.740 [0.681, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.372, 10.100], loss: 0.001348, mae: 0.039926, mean_q: 1.252482
 98986/100000: episode: 1879, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 11.973, mean reward: 0.748 [0.641, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.191, 10.100], loss: 0.001605, mae: 0.042953, mean_q: 1.254087
 99003/100000: episode: 1880, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 12.046, mean reward: 0.709 [0.667, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.211, 10.100], loss: 0.001505, mae: 0.041883, mean_q: 1.255631
 99015/100000: episode: 1881, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 8.594, mean reward: 0.716 [0.596, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.084, 10.100], loss: 0.001760, mae: 0.044367, mean_q: 1.253284
 99031/100000: episode: 1882, duration: 0.102s, episode steps: 16, steps per second: 158, episode reward: 12.385, mean reward: 0.774 [0.700, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.241, 10.100], loss: 0.001511, mae: 0.041864, mean_q: 1.256548
 99048/100000: episode: 1883, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 14.925, mean reward: 0.878 [0.799, 0.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.482, 10.100], loss: 0.001590, mae: 0.042985, mean_q: 1.261502
 99065/100000: episode: 1884, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 13.820, mean reward: 0.813 [0.718, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.525, 10.100], loss: 0.001669, mae: 0.043466, mean_q: 1.259187
 99081/100000: episode: 1885, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 12.576, mean reward: 0.786 [0.728, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.435, 10.100], loss: 0.001959, mae: 0.046811, mean_q: 1.256725
 99097/100000: episode: 1886, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 11.747, mean reward: 0.734 [0.674, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.413, 10.100], loss: 0.001642, mae: 0.044145, mean_q: 1.263787
 99110/100000: episode: 1887, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 10.078, mean reward: 0.775 [0.729, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.371, 10.100], loss: 0.001541, mae: 0.043310, mean_q: 1.264311
 99123/100000: episode: 1888, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 10.080, mean reward: 0.775 [0.707, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.066, 10.100], loss: 0.001455, mae: 0.039906, mean_q: 1.253833
 99141/100000: episode: 1889, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 14.816, mean reward: 0.823 [0.723, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.534, 10.100], loss: 0.001548, mae: 0.040358, mean_q: 1.256926
 99153/100000: episode: 1890, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 9.209, mean reward: 0.767 [0.699, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.246, 10.100], loss: 0.001370, mae: 0.039039, mean_q: 1.249361
 99164/100000: episode: 1891, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 8.421, mean reward: 0.766 [0.716, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-1.401, 10.476], loss: 0.001360, mae: 0.041787, mean_q: 1.263012
 99181/100000: episode: 1892, duration: 0.107s, episode steps: 17, steps per second: 158, episode reward: 12.671, mean reward: 0.745 [0.724, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.233, 10.100], loss: 0.001902, mae: 0.046042, mean_q: 1.267591
 99193/100000: episode: 1893, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 9.104, mean reward: 0.759 [0.655, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.295, 10.100], loss: 0.001738, mae: 0.045498, mean_q: 1.267655
 99205/100000: episode: 1894, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 9.227, mean reward: 0.769 [0.717, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.262, 10.100], loss: 0.001453, mae: 0.041725, mean_q: 1.238116
 99211/100000: episode: 1895, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 4.939, mean reward: 0.823 [0.773, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.959, 10.100], loss: 0.001931, mae: 0.044543, mean_q: 1.257296
 99222/100000: episode: 1896, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 8.947, mean reward: 0.813 [0.786, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.598], loss: 0.001784, mae: 0.043194, mean_q: 1.267083
 99247/100000: episode: 1897, duration: 0.153s, episode steps: 25, steps per second: 163, episode reward: 17.094, mean reward: 0.684 [0.587, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.067, 10.295], loss: 0.001682, mae: 0.043695, mean_q: 1.261713
[Info] FALSIFICATION!
 99253/100000: episode: 1898, duration: 0.205s, episode steps: 6, steps per second: 29, episode reward: 5.310, mean reward: 0.885 [0.803, 1.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-1.742, 9.231], loss: 0.001794, mae: 0.045774, mean_q: 1.265911
 99259/100000: episode: 1899, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 4.626, mean reward: 0.771 [0.747, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.410, 10.100], loss: 0.001641, mae: 0.042621, mean_q: 1.291885
 99276/100000: episode: 1900, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 12.029, mean reward: 0.708 [0.607, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.495, 10.100], loss: 0.001316, mae: 0.039403, mean_q: 1.251748
 99293/100000: episode: 1901, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 12.354, mean reward: 0.727 [0.655, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.422, 10.100], loss: 0.001621, mae: 0.042727, mean_q: 1.276489
 99309/100000: episode: 1902, duration: 0.100s, episode steps: 16, steps per second: 160, episode reward: 11.546, mean reward: 0.722 [0.624, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.877, 10.100], loss: 0.001663, mae: 0.043089, mean_q: 1.280509
 99326/100000: episode: 1903, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 12.533, mean reward: 0.737 [0.678, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.346, 10.100], loss: 0.001602, mae: 0.041631, mean_q: 1.264257
 99351/100000: episode: 1904, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 18.653, mean reward: 0.746 [0.645, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.647, 10.333], loss: 0.001526, mae: 0.042433, mean_q: 1.263846
 99363/100000: episode: 1905, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 8.685, mean reward: 0.724 [0.645, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.294, 10.100], loss: 0.002062, mae: 0.043578, mean_q: 1.267388
 99380/100000: episode: 1906, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 11.911, mean reward: 0.701 [0.648, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.983, 10.100], loss: 0.001775, mae: 0.044274, mean_q: 1.282157
[Info] FALSIFICATION!
 99389/100000: episode: 1907, duration: 0.227s, episode steps: 9, steps per second: 40, episode reward: 8.138, mean reward: 0.904 [0.787, 1.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.581, 10.067], loss: 0.001485, mae: 0.041278, mean_q: 1.272446
 99406/100000: episode: 1908, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 14.052, mean reward: 0.827 [0.757, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.480, 10.100], loss: 0.001273, mae: 0.039414, mean_q: 1.258401
 99412/100000: episode: 1909, duration: 0.049s, episode steps: 6, steps per second: 122, episode reward: 4.518, mean reward: 0.753 [0.733, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.478, 10.100], loss: 0.001567, mae: 0.044193, mean_q: 1.277629
[Info] Complete ISplit Iteration
[Info] Levels: [1.368965, 1.5020192, 1.5705411]
[Info] Cond. Prob: [0.1, 0.1, 0.31]
[Info] Error Prob: 0.0031000000000000008

 99423/100000: episode: 1910, duration: 4.489s, episode steps: 11, steps per second: 2, episode reward: 8.372, mean reward: 0.761 [0.667, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.541], loss: 0.001835, mae: 0.041068, mean_q: 1.275342
 99523/100000: episode: 1911, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.533, mean reward: 0.575 [0.505, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.556, 10.148], loss: 0.001655, mae: 0.041996, mean_q: 1.277393
 99623/100000: episode: 1912, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.100, mean reward: 0.571 [0.506, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.924, 10.150], loss: 0.001546, mae: 0.041862, mean_q: 1.268037
 99723/100000: episode: 1913, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.506, mean reward: 0.595 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.812, 10.122], loss: 0.001558, mae: 0.043074, mean_q: 1.275831
 99823/100000: episode: 1914, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 57.343, mean reward: 0.573 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.906, 10.237], loss: 0.001596, mae: 0.042486, mean_q: 1.270526
 99923/100000: episode: 1915, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 61.116, mean reward: 0.611 [0.509, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.274, 10.320], loss: 0.001642, mae: 0.043153, mean_q: 1.274879
done, took 610.191 seconds
[Info] End Importance Splitting. Falsification occurred 23 times.
