Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.177s, episode steps: 100, steps per second: 565, episode reward: 60.629, mean reward: 0.606 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.779, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.077s, episode steps: 100, steps per second: 1302, episode reward: 56.937, mean reward: 0.569 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.430, 10.212], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.074s, episode steps: 100, steps per second: 1348, episode reward: 59.050, mean reward: 0.591 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.944, 10.157], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.066s, episode steps: 100, steps per second: 1513, episode reward: 57.186, mean reward: 0.572 [0.499, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.809, 10.241], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.070s, episode steps: 100, steps per second: 1421, episode reward: 58.103, mean reward: 0.581 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.496, 10.198], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.064s, episode steps: 100, steps per second: 1563, episode reward: 62.850, mean reward: 0.629 [0.504, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.760, 10.555], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 57.638, mean reward: 0.576 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.983, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.067s, episode steps: 100, steps per second: 1503, episode reward: 60.577, mean reward: 0.606 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.486, 10.122], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.067s, episode steps: 100, steps per second: 1503, episode reward: 58.056, mean reward: 0.581 [0.509, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.873, 10.216], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 58.723, mean reward: 0.587 [0.508, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.781, 10.148], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.075s, episode steps: 100, steps per second: 1340, episode reward: 60.852, mean reward: 0.609 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.391, 10.130], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.074s, episode steps: 100, steps per second: 1351, episode reward: 58.301, mean reward: 0.583 [0.510, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.581, 10.127], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 58.670, mean reward: 0.587 [0.512, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.683, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 58.102, mean reward: 0.581 [0.505, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.458, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 58.307, mean reward: 0.583 [0.508, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.273, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.079s, episode steps: 100, steps per second: 1273, episode reward: 59.885, mean reward: 0.599 [0.506, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.795, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.077s, episode steps: 100, steps per second: 1298, episode reward: 58.137, mean reward: 0.581 [0.507, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.014, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.092s, episode steps: 100, steps per second: 1086, episode reward: 58.491, mean reward: 0.585 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.494, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 59.043, mean reward: 0.590 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.993, 10.160], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.070s, episode steps: 100, steps per second: 1421, episode reward: 56.526, mean reward: 0.565 [0.503, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.621, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.068s, episode steps: 100, steps per second: 1465, episode reward: 59.211, mean reward: 0.592 [0.510, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.572, 10.222], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.065s, episode steps: 100, steps per second: 1538, episode reward: 63.842, mean reward: 0.638 [0.509, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.021, 10.327], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 57.604, mean reward: 0.576 [0.497, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.912, 10.133], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: 58.595, mean reward: 0.586 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.250, 10.390], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 57.554, mean reward: 0.576 [0.501, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.162, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.064s, episode steps: 100, steps per second: 1552, episode reward: 58.695, mean reward: 0.587 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.287, 10.311], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 57.826, mean reward: 0.578 [0.498, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.623, 10.117], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 59.167, mean reward: 0.592 [0.498, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.811, 10.165], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.063s, episode steps: 100, steps per second: 1593, episode reward: 60.161, mean reward: 0.602 [0.499, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.462, 10.268], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.069s, episode steps: 100, steps per second: 1456, episode reward: 58.087, mean reward: 0.581 [0.505, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.243, 10.302], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.065s, episode steps: 100, steps per second: 1536, episode reward: 57.793, mean reward: 0.578 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.429, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 62.675, mean reward: 0.627 [0.507, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.742, 10.174], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 57.692, mean reward: 0.577 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.770, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 58.862, mean reward: 0.589 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.647, 10.189], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.063s, episode steps: 100, steps per second: 1592, episode reward: 59.115, mean reward: 0.591 [0.504, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.374, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.063s, episode steps: 100, steps per second: 1582, episode reward: 60.341, mean reward: 0.603 [0.511, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.160, 10.341], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 59.481, mean reward: 0.595 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.618, 10.254], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.064s, episode steps: 100, steps per second: 1575, episode reward: 58.087, mean reward: 0.581 [0.504, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.131, 10.154], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.070s, episode steps: 100, steps per second: 1429, episode reward: 58.373, mean reward: 0.584 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.639, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.068s, episode steps: 100, steps per second: 1463, episode reward: 61.372, mean reward: 0.614 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.423, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.065s, episode steps: 100, steps per second: 1549, episode reward: 57.735, mean reward: 0.577 [0.516, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.666, 10.159], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.064s, episode steps: 100, steps per second: 1563, episode reward: 59.688, mean reward: 0.597 [0.508, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.331, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 59.743, mean reward: 0.597 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.731, 10.182], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 58.789, mean reward: 0.588 [0.501, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.174, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 64.114, mean reward: 0.641 [0.501, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.250, 10.231], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.079s, episode steps: 100, steps per second: 1265, episode reward: 58.299, mean reward: 0.583 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.077, 10.266], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.075s, episode steps: 100, steps per second: 1342, episode reward: 58.977, mean reward: 0.590 [0.504, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.232, 10.227], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.070s, episode steps: 100, steps per second: 1425, episode reward: 58.386, mean reward: 0.584 [0.508, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.114, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.073s, episode steps: 100, steps per second: 1369, episode reward: 58.623, mean reward: 0.586 [0.502, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.452, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.067s, episode steps: 100, steps per second: 1495, episode reward: 57.705, mean reward: 0.577 [0.500, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.603, 10.211], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.314s, episode steps: 100, steps per second: 76, episode reward: 57.722, mean reward: 0.577 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.961, 10.098], loss: 0.095385, mae: 0.221315, mean_q: 1.991095
  5200/100000: episode: 52, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: 58.989, mean reward: 0.590 [0.512, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.743, 10.278], loss: 0.019820, mae: 0.092887, mean_q: 1.658221
  5300/100000: episode: 53, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 60.062, mean reward: 0.601 [0.508, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.318, 10.098], loss: 0.010262, mae: 0.076095, mean_q: 1.457685
  5400/100000: episode: 54, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.309, mean reward: 0.583 [0.498, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.494, 10.098], loss: 0.005189, mae: 0.061536, mean_q: 1.342727
  5500/100000: episode: 55, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 57.250, mean reward: 0.573 [0.501, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.976, 10.345], loss: 0.004349, mae: 0.059111, mean_q: 1.273494
  5600/100000: episode: 56, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 57.938, mean reward: 0.579 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.528, 10.277], loss: 0.003217, mae: 0.054953, mean_q: 1.232355
  5700/100000: episode: 57, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 61.511, mean reward: 0.615 [0.519, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.721, 10.098], loss: 0.003381, mae: 0.055738, mean_q: 1.202199
  5800/100000: episode: 58, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.142, mean reward: 0.601 [0.504, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.758, 10.255], loss: 0.003087, mae: 0.055287, mean_q: 1.188733
  5900/100000: episode: 59, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.107, mean reward: 0.591 [0.507, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.948, 10.147], loss: 0.002743, mae: 0.053563, mean_q: 1.184499
  6000/100000: episode: 60, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.866, mean reward: 0.579 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.092, 10.125], loss: 0.002911, mae: 0.054356, mean_q: 1.174622
  6100/100000: episode: 61, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 61.063, mean reward: 0.611 [0.518, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.596, 10.098], loss: 0.002582, mae: 0.052609, mean_q: 1.175423
  6200/100000: episode: 62, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.124, mean reward: 0.581 [0.498, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.612, 10.098], loss: 0.002609, mae: 0.052039, mean_q: 1.169154
  6300/100000: episode: 63, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 59.467, mean reward: 0.595 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.806, 10.098], loss: 0.002902, mae: 0.054553, mean_q: 1.170720
  6400/100000: episode: 64, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.654, mean reward: 0.577 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.502, 10.098], loss: 0.003047, mae: 0.054649, mean_q: 1.166446
  6500/100000: episode: 65, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.800, mean reward: 0.578 [0.508, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.815, 10.098], loss: 0.002788, mae: 0.053024, mean_q: 1.169099
  6600/100000: episode: 66, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 60.693, mean reward: 0.607 [0.502, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.644, 10.241], loss: 0.002653, mae: 0.052060, mean_q: 1.168640
  6700/100000: episode: 67, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.964, mean reward: 0.600 [0.507, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.538, 10.146], loss: 0.002537, mae: 0.051380, mean_q: 1.170030
  6800/100000: episode: 68, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 58.789, mean reward: 0.588 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.158, 10.354], loss: 0.002624, mae: 0.051966, mean_q: 1.167890
  6900/100000: episode: 69, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 61.062, mean reward: 0.611 [0.507, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.591, 10.098], loss: 0.002437, mae: 0.049725, mean_q: 1.167058
  7000/100000: episode: 70, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 60.329, mean reward: 0.603 [0.515, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.989, 10.098], loss: 0.002577, mae: 0.051874, mean_q: 1.170919
  7100/100000: episode: 71, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 60.309, mean reward: 0.603 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.629, 10.098], loss: 0.002785, mae: 0.052878, mean_q: 1.171146
  7200/100000: episode: 72, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 57.672, mean reward: 0.577 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.338, 10.183], loss: 0.002721, mae: 0.053298, mean_q: 1.170503
  7300/100000: episode: 73, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.053, mean reward: 0.601 [0.522, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.212, 10.112], loss: 0.002917, mae: 0.056353, mean_q: 1.172740
  7400/100000: episode: 74, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.433, mean reward: 0.584 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.699, 10.098], loss: 0.002710, mae: 0.053022, mean_q: 1.170348
  7500/100000: episode: 75, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.662, mean reward: 0.577 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.733, 10.118], loss: 0.002403, mae: 0.049619, mean_q: 1.172884
  7600/100000: episode: 76, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.895, mean reward: 0.589 [0.508, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.400, 10.098], loss: 0.002533, mae: 0.050887, mean_q: 1.174981
  7700/100000: episode: 77, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 60.125, mean reward: 0.601 [0.509, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.955, 10.151], loss: 0.002852, mae: 0.055436, mean_q: 1.174028
  7800/100000: episode: 78, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.212, mean reward: 0.592 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.352, 10.098], loss: 0.002412, mae: 0.050916, mean_q: 1.175248
  7900/100000: episode: 79, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 59.763, mean reward: 0.598 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.098, 10.098], loss: 0.002771, mae: 0.053302, mean_q: 1.171585
  8000/100000: episode: 80, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 57.961, mean reward: 0.580 [0.505, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.911, 10.226], loss: 0.002666, mae: 0.053225, mean_q: 1.173657
  8100/100000: episode: 81, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.521, mean reward: 0.605 [0.511, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.814, 10.098], loss: 0.002287, mae: 0.049800, mean_q: 1.172465
  8200/100000: episode: 82, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 57.251, mean reward: 0.573 [0.504, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.766, 10.133], loss: 0.002671, mae: 0.053757, mean_q: 1.170430
  8300/100000: episode: 83, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.603, mean reward: 0.576 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.096, 10.098], loss: 0.002269, mae: 0.048908, mean_q: 1.171598
  8400/100000: episode: 84, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 57.048, mean reward: 0.570 [0.500, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.707, 10.098], loss: 0.002567, mae: 0.050708, mean_q: 1.167220
  8500/100000: episode: 85, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.779, mean reward: 0.598 [0.502, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.679, 10.289], loss: 0.002163, mae: 0.048673, mean_q: 1.171768
  8600/100000: episode: 86, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 58.873, mean reward: 0.589 [0.500, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.822, 10.241], loss: 0.002853, mae: 0.055815, mean_q: 1.171233
  8700/100000: episode: 87, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.191, mean reward: 0.572 [0.502, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.289, 10.098], loss: 0.002568, mae: 0.052322, mean_q: 1.169658
  8800/100000: episode: 88, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 59.417, mean reward: 0.594 [0.502, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.593, 10.098], loss: 0.002248, mae: 0.048111, mean_q: 1.167794
  8900/100000: episode: 89, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 59.902, mean reward: 0.599 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.915, 10.098], loss: 0.002538, mae: 0.052411, mean_q: 1.168059
  9000/100000: episode: 90, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 57.491, mean reward: 0.575 [0.498, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.981, 10.249], loss: 0.002358, mae: 0.049831, mean_q: 1.166542
  9100/100000: episode: 91, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 59.323, mean reward: 0.593 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.971, 10.098], loss: 0.002377, mae: 0.049987, mean_q: 1.169653
  9200/100000: episode: 92, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 60.466, mean reward: 0.605 [0.509, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.006, 10.141], loss: 0.002121, mae: 0.047559, mean_q: 1.169547
  9300/100000: episode: 93, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.153, mean reward: 0.562 [0.501, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.308, 10.098], loss: 0.002254, mae: 0.049062, mean_q: 1.167816
  9400/100000: episode: 94, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 60.044, mean reward: 0.600 [0.502, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.996, 10.414], loss: 0.002401, mae: 0.050732, mean_q: 1.168045
  9500/100000: episode: 95, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 57.506, mean reward: 0.575 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.777, 10.304], loss: 0.002332, mae: 0.049899, mean_q: 1.166922
  9600/100000: episode: 96, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.895, mean reward: 0.599 [0.506, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.543, 10.098], loss: 0.002378, mae: 0.050692, mean_q: 1.166042
  9700/100000: episode: 97, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 59.236, mean reward: 0.592 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.665, 10.202], loss: 0.002351, mae: 0.050253, mean_q: 1.165216
  9800/100000: episode: 98, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 56.124, mean reward: 0.561 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.083, 10.153], loss: 0.002155, mae: 0.049542, mean_q: 1.169093
  9900/100000: episode: 99, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 56.499, mean reward: 0.565 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.234, 10.098], loss: 0.002272, mae: 0.050181, mean_q: 1.164282
 10000/100000: episode: 100, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 60.683, mean reward: 0.607 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.105, 10.098], loss: 0.002180, mae: 0.049387, mean_q: 1.165346
 10100/100000: episode: 101, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 57.893, mean reward: 0.579 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.084, 10.098], loss: 0.002161, mae: 0.048198, mean_q: 1.166194
 10200/100000: episode: 102, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 58.985, mean reward: 0.590 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.376, 10.098], loss: 0.002061, mae: 0.048133, mean_q: 1.165179
 10300/100000: episode: 103, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.950, mean reward: 0.589 [0.507, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.243, 10.098], loss: 0.002001, mae: 0.047053, mean_q: 1.166413
 10400/100000: episode: 104, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 59.357, mean reward: 0.594 [0.503, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.770, 10.098], loss: 0.002245, mae: 0.049705, mean_q: 1.165386
 10500/100000: episode: 105, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 56.725, mean reward: 0.567 [0.501, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.908, 10.115], loss: 0.002311, mae: 0.051053, mean_q: 1.163881
 10600/100000: episode: 106, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 62.431, mean reward: 0.624 [0.502, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.013, 10.375], loss: 0.002198, mae: 0.049069, mean_q: 1.165590
 10700/100000: episode: 107, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 63.078, mean reward: 0.631 [0.508, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.691, 10.098], loss: 0.002332, mae: 0.050314, mean_q: 1.165732
 10800/100000: episode: 108, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 59.567, mean reward: 0.596 [0.511, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.900, 10.098], loss: 0.002396, mae: 0.052037, mean_q: 1.169360
 10900/100000: episode: 109, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 57.246, mean reward: 0.572 [0.508, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.917, 10.098], loss: 0.002375, mae: 0.051470, mean_q: 1.169022
 11000/100000: episode: 110, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 63.597, mean reward: 0.636 [0.511, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.555, 10.098], loss: 0.002087, mae: 0.048588, mean_q: 1.170813
 11100/100000: episode: 111, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.501, mean reward: 0.575 [0.503, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.823, 10.098], loss: 0.002195, mae: 0.049200, mean_q: 1.166010
 11200/100000: episode: 112, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 57.139, mean reward: 0.571 [0.509, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.643, 10.137], loss: 0.002105, mae: 0.049249, mean_q: 1.166900
 11300/100000: episode: 113, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 57.828, mean reward: 0.578 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.151, 10.222], loss: 0.001968, mae: 0.047570, mean_q: 1.168151
 11400/100000: episode: 114, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 59.278, mean reward: 0.593 [0.504, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.263, 10.473], loss: 0.002123, mae: 0.049639, mean_q: 1.166162
 11500/100000: episode: 115, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 58.642, mean reward: 0.586 [0.500, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.638, 10.223], loss: 0.002253, mae: 0.050517, mean_q: 1.166384
 11600/100000: episode: 116, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 57.758, mean reward: 0.578 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.567, 10.098], loss: 0.002171, mae: 0.049760, mean_q: 1.171062
 11700/100000: episode: 117, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.746, mean reward: 0.577 [0.508, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.410, 10.229], loss: 0.002348, mae: 0.051449, mean_q: 1.163237
 11800/100000: episode: 118, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.655, mean reward: 0.587 [0.516, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.170, 10.122], loss: 0.001976, mae: 0.047979, mean_q: 1.164971
 11900/100000: episode: 119, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 57.746, mean reward: 0.577 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.321, 10.106], loss: 0.002080, mae: 0.048867, mean_q: 1.166122
 12000/100000: episode: 120, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 58.862, mean reward: 0.589 [0.508, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.926, 10.155], loss: 0.001930, mae: 0.048174, mean_q: 1.165731
 12100/100000: episode: 121, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.481, mean reward: 0.595 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.486, 10.098], loss: 0.002146, mae: 0.049636, mean_q: 1.160062
 12200/100000: episode: 122, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 58.564, mean reward: 0.586 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.369, 10.098], loss: 0.001859, mae: 0.046810, mean_q: 1.163178
 12300/100000: episode: 123, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 58.161, mean reward: 0.582 [0.509, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.254, 10.185], loss: 0.001994, mae: 0.048521, mean_q: 1.161873
 12400/100000: episode: 124, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.723, mean reward: 0.587 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.013, 10.098], loss: 0.001944, mae: 0.047325, mean_q: 1.165183
 12500/100000: episode: 125, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.273, mean reward: 0.583 [0.508, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.996, 10.101], loss: 0.002054, mae: 0.048889, mean_q: 1.163608
 12600/100000: episode: 126, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.028, mean reward: 0.590 [0.508, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.734, 10.278], loss: 0.002089, mae: 0.049157, mean_q: 1.163611
 12700/100000: episode: 127, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 64.649, mean reward: 0.646 [0.505, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.478, 10.377], loss: 0.001967, mae: 0.047697, mean_q: 1.162872
 12800/100000: episode: 128, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 58.789, mean reward: 0.588 [0.499, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.257, 10.352], loss: 0.001976, mae: 0.047967, mean_q: 1.164150
 12900/100000: episode: 129, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.299, mean reward: 0.573 [0.507, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.443, 10.419], loss: 0.001826, mae: 0.045892, mean_q: 1.163794
 13000/100000: episode: 130, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 57.232, mean reward: 0.572 [0.502, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.846, 10.098], loss: 0.001862, mae: 0.046716, mean_q: 1.162993
 13100/100000: episode: 131, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.934, mean reward: 0.599 [0.509, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.488, 10.105], loss: 0.001994, mae: 0.048455, mean_q: 1.164059
 13200/100000: episode: 132, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.552, mean reward: 0.586 [0.509, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.386, 10.098], loss: 0.001943, mae: 0.047475, mean_q: 1.162426
 13300/100000: episode: 133, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 59.346, mean reward: 0.593 [0.514, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.285, 10.098], loss: 0.001965, mae: 0.047849, mean_q: 1.161740
 13400/100000: episode: 134, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 66.840, mean reward: 0.668 [0.528, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.975, 10.611], loss: 0.001890, mae: 0.047130, mean_q: 1.160927
 13500/100000: episode: 135, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 57.807, mean reward: 0.578 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.219, 10.098], loss: 0.002128, mae: 0.050507, mean_q: 1.164395
 13600/100000: episode: 136, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 56.666, mean reward: 0.567 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.407, 10.112], loss: 0.001927, mae: 0.047192, mean_q: 1.167762
 13700/100000: episode: 137, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 63.083, mean reward: 0.631 [0.509, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.237, 10.425], loss: 0.002010, mae: 0.048948, mean_q: 1.170240
 13800/100000: episode: 138, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 57.784, mean reward: 0.578 [0.503, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.666, 10.296], loss: 0.001943, mae: 0.047287, mean_q: 1.167948
 13900/100000: episode: 139, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.706, mean reward: 0.597 [0.510, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.865, 10.098], loss: 0.002000, mae: 0.048015, mean_q: 1.168863
 14000/100000: episode: 140, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.358, mean reward: 0.574 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.116, 10.116], loss: 0.002157, mae: 0.049955, mean_q: 1.164580
 14100/100000: episode: 141, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 57.755, mean reward: 0.578 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.504, 10.098], loss: 0.002033, mae: 0.048067, mean_q: 1.163174
 14200/100000: episode: 142, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 57.059, mean reward: 0.571 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.310, 10.098], loss: 0.002015, mae: 0.048454, mean_q: 1.163033
 14300/100000: episode: 143, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 59.467, mean reward: 0.595 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.147, 10.126], loss: 0.001995, mae: 0.048041, mean_q: 1.164544
 14400/100000: episode: 144, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 58.432, mean reward: 0.584 [0.511, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.923, 10.165], loss: 0.001762, mae: 0.045623, mean_q: 1.166423
 14500/100000: episode: 145, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.131, mean reward: 0.581 [0.499, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.269, 10.247], loss: 0.001774, mae: 0.045737, mean_q: 1.167349
 14600/100000: episode: 146, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 58.729, mean reward: 0.587 [0.508, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.784, 10.136], loss: 0.001841, mae: 0.046240, mean_q: 1.166141
 14700/100000: episode: 147, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 58.112, mean reward: 0.581 [0.513, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.253, 10.098], loss: 0.001804, mae: 0.046309, mean_q: 1.167115
 14800/100000: episode: 148, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.382, mean reward: 0.574 [0.500, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.243, 10.098], loss: 0.001729, mae: 0.045312, mean_q: 1.165842
 14900/100000: episode: 149, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.927, mean reward: 0.589 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.638, 10.098], loss: 0.001888, mae: 0.046912, mean_q: 1.166679
[Info] 1-TH LEVEL FOUND: 1.2282792329788208, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.271s, episode steps: 100, steps per second: 19, episode reward: 58.458, mean reward: 0.585 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.666, 10.186], loss: 0.001802, mae: 0.046424, mean_q: 1.168217
 15100/100000: episode: 151, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 62.599, mean reward: 0.626 [0.506, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.696, 10.100], loss: 0.001953, mae: 0.047554, mean_q: 1.167300
 15153/100000: episode: 152, duration: 0.276s, episode steps: 53, steps per second: 192, episode reward: 34.101, mean reward: 0.643 [0.542, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.380, 10.295], loss: 0.001904, mae: 0.046851, mean_q: 1.170299
 15253/100000: episode: 153, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.845, mean reward: 0.588 [0.508, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.075, 10.100], loss: 0.001758, mae: 0.045235, mean_q: 1.165720
 15306/100000: episode: 154, duration: 0.286s, episode steps: 53, steps per second: 185, episode reward: 34.210, mean reward: 0.645 [0.578, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.567, 10.334], loss: 0.001877, mae: 0.045884, mean_q: 1.169961
 15406/100000: episode: 155, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 58.099, mean reward: 0.581 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.075, 10.100], loss: 0.001920, mae: 0.046823, mean_q: 1.166820
 15458/100000: episode: 156, duration: 0.294s, episode steps: 52, steps per second: 177, episode reward: 33.372, mean reward: 0.642 [0.542, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.515, 10.210], loss: 0.001899, mae: 0.046317, mean_q: 1.169378
 15511/100000: episode: 157, duration: 0.271s, episode steps: 53, steps per second: 196, episode reward: 38.002, mean reward: 0.717 [0.606, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.815, 10.382], loss: 0.002039, mae: 0.048030, mean_q: 1.177309
 15562/100000: episode: 158, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 30.524, mean reward: 0.599 [0.501, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-1.162, 10.100], loss: 0.002021, mae: 0.047597, mean_q: 1.174399
 15614/100000: episode: 159, duration: 0.257s, episode steps: 52, steps per second: 202, episode reward: 35.943, mean reward: 0.691 [0.613, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.333, 10.444], loss: 0.002155, mae: 0.048969, mean_q: 1.174430
 15709/100000: episode: 160, duration: 0.597s, episode steps: 95, steps per second: 159, episode reward: 58.048, mean reward: 0.611 [0.520, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-1.351, 10.482], loss: 0.002432, mae: 0.052109, mean_q: 1.170881
 15803/100000: episode: 161, duration: 0.474s, episode steps: 94, steps per second: 198, episode reward: 56.837, mean reward: 0.605 [0.500, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.511 [-1.362, 10.173], loss: 0.002095, mae: 0.048405, mean_q: 1.172156
 15903/100000: episode: 162, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 60.350, mean reward: 0.604 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.718, 10.232], loss: 0.001848, mae: 0.045892, mean_q: 1.176802
 15955/100000: episode: 163, duration: 0.257s, episode steps: 52, steps per second: 202, episode reward: 34.522, mean reward: 0.664 [0.538, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.118, 10.225], loss: 0.001739, mae: 0.045205, mean_q: 1.179005
 16007/100000: episode: 164, duration: 0.275s, episode steps: 52, steps per second: 189, episode reward: 30.895, mean reward: 0.594 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.500, 10.100], loss: 0.002080, mae: 0.049209, mean_q: 1.175536
 16058/100000: episode: 165, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 30.898, mean reward: 0.606 [0.516, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-2.161, 10.328], loss: 0.001962, mae: 0.048414, mean_q: 1.177518
 16109/100000: episode: 166, duration: 0.254s, episode steps: 51, steps per second: 201, episode reward: 31.334, mean reward: 0.614 [0.511, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.848, 10.100], loss: 0.002260, mae: 0.051480, mean_q: 1.170179
 16160/100000: episode: 167, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 30.497, mean reward: 0.598 [0.511, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.281, 10.172], loss: 0.002124, mae: 0.048763, mean_q: 1.176628
 16213/100000: episode: 168, duration: 0.270s, episode steps: 53, steps per second: 197, episode reward: 34.521, mean reward: 0.651 [0.570, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.396, 10.387], loss: 0.002026, mae: 0.048318, mean_q: 1.172207
 16264/100000: episode: 169, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 32.474, mean reward: 0.637 [0.547, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.300, 10.195], loss: 0.001721, mae: 0.045271, mean_q: 1.176696
 16359/100000: episode: 170, duration: 0.485s, episode steps: 95, steps per second: 196, episode reward: 58.039, mean reward: 0.611 [0.511, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.913, 10.286], loss: 0.001903, mae: 0.047313, mean_q: 1.180436
 16411/100000: episode: 171, duration: 0.258s, episode steps: 52, steps per second: 201, episode reward: 29.953, mean reward: 0.576 [0.527, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.666, 10.100], loss: 0.001774, mae: 0.045604, mean_q: 1.180818
 16462/100000: episode: 172, duration: 0.250s, episode steps: 51, steps per second: 204, episode reward: 30.125, mean reward: 0.591 [0.506, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.289, 10.100], loss: 0.001721, mae: 0.044816, mean_q: 1.178228
 16513/100000: episode: 173, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 31.470, mean reward: 0.617 [0.537, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.289, 10.274], loss: 0.002055, mae: 0.048849, mean_q: 1.175948
 16565/100000: episode: 174, duration: 0.279s, episode steps: 52, steps per second: 186, episode reward: 31.604, mean reward: 0.608 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-1.031, 10.352], loss: 0.002116, mae: 0.049256, mean_q: 1.181246
 16618/100000: episode: 175, duration: 0.256s, episode steps: 53, steps per second: 207, episode reward: 39.106, mean reward: 0.738 [0.614, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-1.902, 10.560], loss: 0.001874, mae: 0.046404, mean_q: 1.180487
 16671/100000: episode: 176, duration: 0.266s, episode steps: 53, steps per second: 200, episode reward: 33.239, mean reward: 0.627 [0.517, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.151, 10.266], loss: 0.002048, mae: 0.049099, mean_q: 1.178533
 16766/100000: episode: 177, duration: 0.489s, episode steps: 95, steps per second: 194, episode reward: 55.465, mean reward: 0.584 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.483, 10.161], loss: 0.001912, mae: 0.046069, mean_q: 1.182321
 16818/100000: episode: 178, duration: 0.257s, episode steps: 52, steps per second: 202, episode reward: 32.677, mean reward: 0.628 [0.523, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-1.593, 10.273], loss: 0.001841, mae: 0.045519, mean_q: 1.189128
 16912/100000: episode: 179, duration: 0.505s, episode steps: 94, steps per second: 186, episode reward: 55.257, mean reward: 0.588 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.811, 10.161], loss: 0.001776, mae: 0.045842, mean_q: 1.188712
 17012/100000: episode: 180, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.211, mean reward: 0.602 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-1.212, 10.334], loss: 0.001801, mae: 0.046105, mean_q: 1.189742
 17065/100000: episode: 181, duration: 0.268s, episode steps: 53, steps per second: 198, episode reward: 30.465, mean reward: 0.575 [0.510, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.357, 10.259], loss: 0.001776, mae: 0.044785, mean_q: 1.184656
 17117/100000: episode: 182, duration: 0.270s, episode steps: 52, steps per second: 192, episode reward: 34.691, mean reward: 0.667 [0.585, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.324, 10.351], loss: 0.001912, mae: 0.046414, mean_q: 1.188878
 17169/100000: episode: 183, duration: 0.263s, episode steps: 52, steps per second: 197, episode reward: 34.864, mean reward: 0.670 [0.562, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.416, 10.404], loss: 0.001761, mae: 0.045093, mean_q: 1.190436
 17220/100000: episode: 184, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 29.039, mean reward: 0.569 [0.501, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.266, 10.120], loss: 0.001668, mae: 0.043984, mean_q: 1.192747
 17320/100000: episode: 185, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 61.684, mean reward: 0.617 [0.513, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-2.674, 10.100], loss: 0.001859, mae: 0.046907, mean_q: 1.192180
 17371/100000: episode: 186, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 31.996, mean reward: 0.627 [0.576, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.471, 10.306], loss: 0.001914, mae: 0.046170, mean_q: 1.193608
 17466/100000: episode: 187, duration: 0.499s, episode steps: 95, steps per second: 190, episode reward: 57.763, mean reward: 0.608 [0.518, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.394, 10.100], loss: 0.002007, mae: 0.048128, mean_q: 1.194050
 17519/100000: episode: 188, duration: 0.302s, episode steps: 53, steps per second: 176, episode reward: 33.181, mean reward: 0.626 [0.516, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-0.547, 10.100], loss: 0.002018, mae: 0.048364, mean_q: 1.197702
 17571/100000: episode: 189, duration: 0.265s, episode steps: 52, steps per second: 196, episode reward: 31.836, mean reward: 0.612 [0.557, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.307, 10.241], loss: 0.001850, mae: 0.045548, mean_q: 1.195960
 17622/100000: episode: 190, duration: 0.251s, episode steps: 51, steps per second: 203, episode reward: 30.383, mean reward: 0.596 [0.508, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.651, 10.221], loss: 0.001828, mae: 0.045609, mean_q: 1.199221
 17674/100000: episode: 191, duration: 0.259s, episode steps: 52, steps per second: 201, episode reward: 30.639, mean reward: 0.589 [0.500, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.753, 10.100], loss: 0.001990, mae: 0.047928, mean_q: 1.189837
 17727/100000: episode: 192, duration: 0.278s, episode steps: 53, steps per second: 191, episode reward: 33.913, mean reward: 0.640 [0.514, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.348, 10.170], loss: 0.001714, mae: 0.045467, mean_q: 1.195511
 17827/100000: episode: 193, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 58.554, mean reward: 0.586 [0.512, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.866, 10.159], loss: 0.001943, mae: 0.046721, mean_q: 1.190938
 17927/100000: episode: 194, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.234, mean reward: 0.582 [0.512, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.291, 10.100], loss: 0.001971, mae: 0.047229, mean_q: 1.193092
 18027/100000: episode: 195, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.327, mean reward: 0.603 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.838, 10.288], loss: 0.001888, mae: 0.046611, mean_q: 1.192176
 18080/100000: episode: 196, duration: 0.271s, episode steps: 53, steps per second: 196, episode reward: 36.207, mean reward: 0.683 [0.600, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.946, 10.499], loss: 0.001814, mae: 0.046093, mean_q: 1.194535
 18132/100000: episode: 197, duration: 0.268s, episode steps: 52, steps per second: 194, episode reward: 31.766, mean reward: 0.611 [0.516, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-1.584, 10.107], loss: 0.002062, mae: 0.047667, mean_q: 1.199182
 18184/100000: episode: 198, duration: 0.258s, episode steps: 52, steps per second: 201, episode reward: 31.679, mean reward: 0.609 [0.525, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.604, 10.100], loss: 0.002036, mae: 0.048013, mean_q: 1.197657
 18235/100000: episode: 199, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 30.690, mean reward: 0.602 [0.505, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.863, 10.166], loss: 0.002091, mae: 0.047563, mean_q: 1.196981
 18288/100000: episode: 200, duration: 0.270s, episode steps: 53, steps per second: 196, episode reward: 33.280, mean reward: 0.628 [0.521, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.573, 10.159], loss: 0.001913, mae: 0.046328, mean_q: 1.194660
 18341/100000: episode: 201, duration: 0.285s, episode steps: 53, steps per second: 186, episode reward: 35.458, mean reward: 0.669 [0.544, 0.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.766, 10.355], loss: 0.001817, mae: 0.047096, mean_q: 1.195205
 18394/100000: episode: 202, duration: 0.290s, episode steps: 53, steps per second: 183, episode reward: 34.053, mean reward: 0.643 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.534, 10.100], loss: 0.002297, mae: 0.051152, mean_q: 1.192487
 18494/100000: episode: 203, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.906, mean reward: 0.579 [0.507, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-1.029, 10.207], loss: 0.001973, mae: 0.047337, mean_q: 1.195994
 18547/100000: episode: 204, duration: 0.269s, episode steps: 53, steps per second: 197, episode reward: 33.839, mean reward: 0.638 [0.528, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-1.283, 10.146], loss: 0.002157, mae: 0.048495, mean_q: 1.189664
 18600/100000: episode: 205, duration: 0.292s, episode steps: 53, steps per second: 181, episode reward: 32.210, mean reward: 0.608 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.393, 10.239], loss: 0.001727, mae: 0.045534, mean_q: 1.198472
 18651/100000: episode: 206, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 30.992, mean reward: 0.608 [0.524, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.702, 10.100], loss: 0.001821, mae: 0.044588, mean_q: 1.198141
 18702/100000: episode: 207, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 30.847, mean reward: 0.605 [0.515, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.017, 10.100], loss: 0.001920, mae: 0.046357, mean_q: 1.196461
 18755/100000: episode: 208, duration: 0.292s, episode steps: 53, steps per second: 182, episode reward: 34.131, mean reward: 0.644 [0.517, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.380, 10.162], loss: 0.001598, mae: 0.044043, mean_q: 1.198724
 18808/100000: episode: 209, duration: 0.277s, episode steps: 53, steps per second: 191, episode reward: 36.340, mean reward: 0.686 [0.620, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.594, 10.307], loss: 0.001878, mae: 0.045589, mean_q: 1.200245
 18859/100000: episode: 210, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 31.893, mean reward: 0.625 [0.547, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.893, 10.257], loss: 0.001859, mae: 0.045955, mean_q: 1.204059
 18910/100000: episode: 211, duration: 0.305s, episode steps: 51, steps per second: 167, episode reward: 30.625, mean reward: 0.600 [0.534, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.284, 10.426], loss: 0.001786, mae: 0.045416, mean_q: 1.203497
 18961/100000: episode: 212, duration: 0.299s, episode steps: 51, steps per second: 171, episode reward: 32.371, mean reward: 0.635 [0.512, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.347, 10.220], loss: 0.002019, mae: 0.048081, mean_q: 1.199724
 19061/100000: episode: 213, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 60.832, mean reward: 0.608 [0.510, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.546, 10.100], loss: 0.001749, mae: 0.045337, mean_q: 1.198942
 19161/100000: episode: 214, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.274, mean reward: 0.583 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-2.042, 10.250], loss: 0.001968, mae: 0.047817, mean_q: 1.200407
 19214/100000: episode: 215, duration: 0.270s, episode steps: 53, steps per second: 196, episode reward: 33.499, mean reward: 0.632 [0.503, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.731, 10.100], loss: 0.001776, mae: 0.045257, mean_q: 1.205084
 19266/100000: episode: 216, duration: 0.261s, episode steps: 52, steps per second: 200, episode reward: 31.611, mean reward: 0.608 [0.530, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.826, 10.100], loss: 0.001761, mae: 0.044631, mean_q: 1.206316
 19317/100000: episode: 217, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 30.181, mean reward: 0.592 [0.507, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.638, 10.100], loss: 0.001668, mae: 0.043761, mean_q: 1.199234
 19369/100000: episode: 218, duration: 0.253s, episode steps: 52, steps per second: 205, episode reward: 29.861, mean reward: 0.574 [0.506, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.306, 10.109], loss: 0.001552, mae: 0.042836, mean_q: 1.205409
 19421/100000: episode: 219, duration: 0.282s, episode steps: 52, steps per second: 184, episode reward: 32.198, mean reward: 0.619 [0.530, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.616, 10.338], loss: 0.001632, mae: 0.043009, mean_q: 1.206003
 19474/100000: episode: 220, duration: 0.273s, episode steps: 53, steps per second: 194, episode reward: 31.536, mean reward: 0.595 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.360, 10.100], loss: 0.001672, mae: 0.044762, mean_q: 1.208666
 19574/100000: episode: 221, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.269, mean reward: 0.583 [0.505, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.738, 10.100], loss: 0.001749, mae: 0.044959, mean_q: 1.206994
 19625/100000: episode: 222, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 31.117, mean reward: 0.610 [0.517, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.651, 10.247], loss: 0.001613, mae: 0.043296, mean_q: 1.204739
 19677/100000: episode: 223, duration: 0.279s, episode steps: 52, steps per second: 186, episode reward: 32.140, mean reward: 0.618 [0.535, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.326, 10.226], loss: 0.001776, mae: 0.046010, mean_q: 1.211364
 19728/100000: episode: 224, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 31.945, mean reward: 0.626 [0.559, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-2.345, 10.100], loss: 0.001754, mae: 0.044738, mean_q: 1.215425
 19828/100000: episode: 225, duration: 0.473s, episode steps: 100, steps per second: 212, episode reward: 57.198, mean reward: 0.572 [0.502, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.272, 10.200], loss: 0.001518, mae: 0.042563, mean_q: 1.204771
 19922/100000: episode: 226, duration: 0.490s, episode steps: 94, steps per second: 192, episode reward: 55.174, mean reward: 0.587 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.813, 10.307], loss: 0.001816, mae: 0.045962, mean_q: 1.205668
 19974/100000: episode: 227, duration: 0.260s, episode steps: 52, steps per second: 200, episode reward: 30.528, mean reward: 0.587 [0.503, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.296, 10.128], loss: 0.001527, mae: 0.042949, mean_q: 1.209562
 20026/100000: episode: 228, duration: 0.274s, episode steps: 52, steps per second: 190, episode reward: 41.333, mean reward: 0.795 [0.580, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.318, 10.505], loss: 0.001524, mae: 0.042202, mean_q: 1.203980
 20078/100000: episode: 229, duration: 0.270s, episode steps: 52, steps per second: 193, episode reward: 32.632, mean reward: 0.628 [0.541, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.992, 10.237], loss: 0.001789, mae: 0.045702, mean_q: 1.206369
 20178/100000: episode: 230, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 59.851, mean reward: 0.599 [0.514, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-0.293, 10.287], loss: 0.001591, mae: 0.043406, mean_q: 1.211353
 20229/100000: episode: 231, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 29.465, mean reward: 0.578 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.366, 10.160], loss: 0.001611, mae: 0.043066, mean_q: 1.205422
 20281/100000: episode: 232, duration: 0.288s, episode steps: 52, steps per second: 181, episode reward: 35.107, mean reward: 0.675 [0.568, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.480, 10.432], loss: 0.001485, mae: 0.042478, mean_q: 1.208089
 20375/100000: episode: 233, duration: 0.484s, episode steps: 94, steps per second: 194, episode reward: 56.323, mean reward: 0.599 [0.502, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.536, 10.100], loss: 0.001593, mae: 0.043105, mean_q: 1.207991
 20426/100000: episode: 234, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 33.683, mean reward: 0.660 [0.571, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-1.483, 10.358], loss: 0.001719, mae: 0.043611, mean_q: 1.211714
 20477/100000: episode: 235, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 34.434, mean reward: 0.675 [0.599, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.795, 10.388], loss: 0.001647, mae: 0.043834, mean_q: 1.209587
 20529/100000: episode: 236, duration: 0.264s, episode steps: 52, steps per second: 197, episode reward: 37.268, mean reward: 0.717 [0.610, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.327, 10.422], loss: 0.001980, mae: 0.047174, mean_q: 1.207036
 20581/100000: episode: 237, duration: 0.259s, episode steps: 52, steps per second: 201, episode reward: 34.629, mean reward: 0.666 [0.573, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-1.274, 10.271], loss: 0.002326, mae: 0.049743, mean_q: 1.207457
 20676/100000: episode: 238, duration: 0.484s, episode steps: 95, steps per second: 196, episode reward: 55.844, mean reward: 0.588 [0.505, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.760, 10.100], loss: 0.002206, mae: 0.048471, mean_q: 1.207547
 20728/100000: episode: 239, duration: 0.252s, episode steps: 52, steps per second: 206, episode reward: 39.785, mean reward: 0.765 [0.636, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.729, 10.716], loss: 0.002055, mae: 0.047254, mean_q: 1.203565
[Info] 2-TH LEVEL FOUND: 1.4282333850860596, Considering 10/90 traces
 20823/100000: episode: 240, duration: 4.678s, episode steps: 95, steps per second: 20, episode reward: 57.113, mean reward: 0.601 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-1.376, 10.260], loss: 0.001960, mae: 0.046860, mean_q: 1.209581
 20843/100000: episode: 241, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 14.234, mean reward: 0.712 [0.654, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.428, 10.100], loss: 0.001980, mae: 0.046735, mean_q: 1.212556
 20886/100000: episode: 242, duration: 0.217s, episode steps: 43, steps per second: 198, episode reward: 28.638, mean reward: 0.666 [0.554, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.218, 10.256], loss: 0.001818, mae: 0.045776, mean_q: 1.214218
 20922/100000: episode: 243, duration: 0.173s, episode steps: 36, steps per second: 208, episode reward: 22.605, mean reward: 0.628 [0.521, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.157, 10.119], loss: 0.001929, mae: 0.046128, mean_q: 1.211467
 20950/100000: episode: 244, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 19.869, mean reward: 0.710 [0.623, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.631, 10.264], loss: 0.002220, mae: 0.048298, mean_q: 1.215734
 20986/100000: episode: 245, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 27.090, mean reward: 0.753 [0.691, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.311, 10.545], loss: 0.001955, mae: 0.045758, mean_q: 1.211682
 21027/100000: episode: 246, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 30.594, mean reward: 0.746 [0.652, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.194, 10.363], loss: 0.001832, mae: 0.045298, mean_q: 1.224965
 21063/100000: episode: 247, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 30.649, mean reward: 0.851 [0.775, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.531, 10.543], loss: 0.002135, mae: 0.049521, mean_q: 1.221527
 21103/100000: episode: 248, duration: 0.212s, episode steps: 40, steps per second: 188, episode reward: 28.111, mean reward: 0.703 [0.594, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-1.211, 10.342], loss: 0.001886, mae: 0.046095, mean_q: 1.219965
 21141/100000: episode: 249, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 26.273, mean reward: 0.691 [0.565, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.861, 10.358], loss: 0.002012, mae: 0.046824, mean_q: 1.221560
 21177/100000: episode: 250, duration: 0.186s, episode steps: 36, steps per second: 193, episode reward: 23.259, mean reward: 0.646 [0.553, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.288, 10.257], loss: 0.001887, mae: 0.045413, mean_q: 1.223751
 21197/100000: episode: 251, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 15.679, mean reward: 0.784 [0.724, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.798, 10.100], loss: 0.002090, mae: 0.048228, mean_q: 1.226052
 21235/100000: episode: 252, duration: 0.183s, episode steps: 38, steps per second: 208, episode reward: 26.575, mean reward: 0.699 [0.613, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.113, 10.345], loss: 0.001967, mae: 0.047344, mean_q: 1.225416
 21278/100000: episode: 253, duration: 0.212s, episode steps: 43, steps per second: 203, episode reward: 30.673, mean reward: 0.713 [0.627, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.221, 10.326], loss: 0.001924, mae: 0.046757, mean_q: 1.227995
 21298/100000: episode: 254, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 13.426, mean reward: 0.671 [0.641, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.319, 10.100], loss: 0.001833, mae: 0.046394, mean_q: 1.228553
 21338/100000: episode: 255, duration: 0.191s, episode steps: 40, steps per second: 209, episode reward: 27.608, mean reward: 0.690 [0.526, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.148, 10.206], loss: 0.001702, mae: 0.045153, mean_q: 1.227319
 21376/100000: episode: 256, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 26.633, mean reward: 0.701 [0.521, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-1.138, 10.311], loss: 0.001618, mae: 0.044040, mean_q: 1.237339
 21416/100000: episode: 257, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 31.684, mean reward: 0.792 [0.718, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-1.173, 10.589], loss: 0.001895, mae: 0.046940, mean_q: 1.227355
 21454/100000: episode: 258, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 31.145, mean reward: 0.820 [0.685, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.928, 10.628], loss: 0.001701, mae: 0.044829, mean_q: 1.235141
 21492/100000: episode: 259, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 24.589, mean reward: 0.647 [0.527, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.522, 10.143], loss: 0.001690, mae: 0.044609, mean_q: 1.241901
 21512/100000: episode: 260, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 12.905, mean reward: 0.645 [0.570, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.539, 10.100], loss: 0.001742, mae: 0.045439, mean_q: 1.245344
 21548/100000: episode: 261, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 24.889, mean reward: 0.691 [0.619, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.035, 10.450], loss: 0.001689, mae: 0.044490, mean_q: 1.238359
 21589/100000: episode: 262, duration: 0.232s, episode steps: 41, steps per second: 177, episode reward: 28.039, mean reward: 0.684 [0.547, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.177, 10.269], loss: 0.001678, mae: 0.045166, mean_q: 1.239115
 21627/100000: episode: 263, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 29.143, mean reward: 0.767 [0.658, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.157, 10.511], loss: 0.001734, mae: 0.045392, mean_q: 1.241459
 21665/100000: episode: 264, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 28.497, mean reward: 0.750 [0.633, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.345, 10.647], loss: 0.001770, mae: 0.045025, mean_q: 1.239490
 21706/100000: episode: 265, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 28.766, mean reward: 0.702 [0.590, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.657, 10.319], loss: 0.001827, mae: 0.046263, mean_q: 1.240210
 21726/100000: episode: 266, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 13.648, mean reward: 0.682 [0.624, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.242, 10.100], loss: 0.001924, mae: 0.046291, mean_q: 1.245705
 21766/100000: episode: 267, duration: 0.212s, episode steps: 40, steps per second: 189, episode reward: 27.839, mean reward: 0.696 [0.586, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.171, 10.444], loss: 0.001901, mae: 0.044588, mean_q: 1.250497
 21794/100000: episode: 268, duration: 0.145s, episode steps: 28, steps per second: 194, episode reward: 22.646, mean reward: 0.809 [0.721, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.922, 10.583], loss: 0.001722, mae: 0.042907, mean_q: 1.238301
 21814/100000: episode: 269, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 14.744, mean reward: 0.737 [0.660, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.608, 10.100], loss: 0.002206, mae: 0.048435, mean_q: 1.238302
 21842/100000: episode: 270, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 18.564, mean reward: 0.663 [0.593, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.722, 10.222], loss: 0.002644, mae: 0.052916, mean_q: 1.254212
 21883/100000: episode: 271, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 26.406, mean reward: 0.644 [0.529, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.181, 10.294], loss: 0.002245, mae: 0.051074, mean_q: 1.253995
 21926/100000: episode: 272, duration: 0.225s, episode steps: 43, steps per second: 191, episode reward: 25.614, mean reward: 0.596 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.514, 10.148], loss: 0.001570, mae: 0.043968, mean_q: 1.251007
 21964/100000: episode: 273, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 26.388, mean reward: 0.694 [0.544, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.049, 10.245], loss: 0.001609, mae: 0.043470, mean_q: 1.239841
 22005/100000: episode: 274, duration: 0.204s, episode steps: 41, steps per second: 201, episode reward: 26.026, mean reward: 0.635 [0.498, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.173, 10.100], loss: 0.001562, mae: 0.042977, mean_q: 1.254323
 22041/100000: episode: 275, duration: 0.181s, episode steps: 36, steps per second: 198, episode reward: 23.334, mean reward: 0.648 [0.529, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.077, 10.100], loss: 0.001660, mae: 0.044376, mean_q: 1.249779
 22069/100000: episode: 276, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 20.306, mean reward: 0.725 [0.674, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.487], loss: 0.001812, mae: 0.045200, mean_q: 1.250141
 22112/100000: episode: 277, duration: 0.228s, episode steps: 43, steps per second: 188, episode reward: 31.941, mean reward: 0.743 [0.605, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.012, 10.334], loss: 0.001496, mae: 0.042329, mean_q: 1.256724
 22150/100000: episode: 278, duration: 0.207s, episode steps: 38, steps per second: 183, episode reward: 24.423, mean reward: 0.643 [0.512, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.256, 10.100], loss: 0.001509, mae: 0.042086, mean_q: 1.257455
 22188/100000: episode: 279, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 25.184, mean reward: 0.663 [0.578, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.101, 10.423], loss: 0.001469, mae: 0.042246, mean_q: 1.245886
 22226/100000: episode: 280, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 26.664, mean reward: 0.702 [0.521, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.224, 10.273], loss: 0.001380, mae: 0.041518, mean_q: 1.263191
 22246/100000: episode: 281, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 14.514, mean reward: 0.726 [0.660, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.853, 10.100], loss: 0.001494, mae: 0.042624, mean_q: 1.261402
 22274/100000: episode: 282, duration: 0.137s, episode steps: 28, steps per second: 205, episode reward: 19.560, mean reward: 0.699 [0.606, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.430], loss: 0.001480, mae: 0.041902, mean_q: 1.251779
 22302/100000: episode: 283, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 22.441, mean reward: 0.801 [0.654, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.618, 10.559], loss: 0.001652, mae: 0.045128, mean_q: 1.247545
 22338/100000: episode: 284, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 25.685, mean reward: 0.713 [0.628, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.180, 10.403], loss: 0.001696, mae: 0.045170, mean_q: 1.266502
 22358/100000: episode: 285, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 14.525, mean reward: 0.726 [0.664, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-2.521, 10.100], loss: 0.001600, mae: 0.043252, mean_q: 1.264484
 22398/100000: episode: 286, duration: 0.192s, episode steps: 40, steps per second: 208, episode reward: 30.240, mean reward: 0.756 [0.633, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.472, 10.540], loss: 0.001465, mae: 0.042404, mean_q: 1.259788
 22436/100000: episode: 287, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 29.243, mean reward: 0.770 [0.678, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.666, 10.505], loss: 0.001475, mae: 0.042612, mean_q: 1.264067
 22471/100000: episode: 288, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 23.971, mean reward: 0.685 [0.584, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.315, 10.320], loss: 0.001578, mae: 0.043481, mean_q: 1.258511
 22507/100000: episode: 289, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 27.000, mean reward: 0.750 [0.640, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.046, 10.658], loss: 0.001452, mae: 0.041315, mean_q: 1.267327
 22535/100000: episode: 290, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 22.149, mean reward: 0.791 [0.721, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.343, 10.444], loss: 0.001457, mae: 0.043145, mean_q: 1.271425
 22573/100000: episode: 291, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 30.010, mean reward: 0.790 [0.696, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.783, 10.631], loss: 0.001734, mae: 0.045082, mean_q: 1.269484
 22601/100000: episode: 292, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 21.157, mean reward: 0.756 [0.631, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.783, 10.389], loss: 0.001765, mae: 0.045336, mean_q: 1.268505
 22636/100000: episode: 293, duration: 0.169s, episode steps: 35, steps per second: 207, episode reward: 23.862, mean reward: 0.682 [0.584, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.053, 10.275], loss: 0.002096, mae: 0.047811, mean_q: 1.274909
 22676/100000: episode: 294, duration: 0.207s, episode steps: 40, steps per second: 193, episode reward: 28.968, mean reward: 0.724 [0.637, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-1.512, 10.366], loss: 0.001577, mae: 0.042647, mean_q: 1.276061
 22711/100000: episode: 295, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 24.741, mean reward: 0.707 [0.610, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.348, 10.360], loss: 0.001516, mae: 0.043157, mean_q: 1.278590
 22752/100000: episode: 296, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 33.251, mean reward: 0.811 [0.563, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.232, 10.239], loss: 0.001466, mae: 0.042111, mean_q: 1.277880
 22788/100000: episode: 297, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 26.640, mean reward: 0.740 [0.649, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.035, 10.399], loss: 0.001700, mae: 0.045195, mean_q: 1.282049
 22828/100000: episode: 298, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 26.209, mean reward: 0.655 [0.561, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-1.068, 10.264], loss: 0.001417, mae: 0.041413, mean_q: 1.290228
 22856/100000: episode: 299, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 22.078, mean reward: 0.788 [0.652, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.130, 10.417], loss: 0.001702, mae: 0.045003, mean_q: 1.284679
 22891/100000: episode: 300, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 24.979, mean reward: 0.714 [0.629, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.035, 10.405], loss: 0.001519, mae: 0.042665, mean_q: 1.281728
 22927/100000: episode: 301, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 27.337, mean reward: 0.759 [0.658, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.675, 10.428], loss: 0.001585, mae: 0.043226, mean_q: 1.279148
 22963/100000: episode: 302, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 24.859, mean reward: 0.691 [0.588, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.365, 10.374], loss: 0.001726, mae: 0.044479, mean_q: 1.297563
 23006/100000: episode: 303, duration: 0.222s, episode steps: 43, steps per second: 193, episode reward: 28.408, mean reward: 0.661 [0.542, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-1.395, 10.236], loss: 0.001600, mae: 0.043323, mean_q: 1.283091
 23026/100000: episode: 304, duration: 0.132s, episode steps: 20, steps per second: 152, episode reward: 14.919, mean reward: 0.746 [0.674, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.388, 10.100], loss: 0.001534, mae: 0.042095, mean_q: 1.302140
 23064/100000: episode: 305, duration: 0.215s, episode steps: 38, steps per second: 176, episode reward: 27.462, mean reward: 0.723 [0.618, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.609, 10.337], loss: 0.001517, mae: 0.042937, mean_q: 1.290476
 23104/100000: episode: 306, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 29.775, mean reward: 0.744 [0.638, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-1.200, 10.436], loss: 0.001561, mae: 0.043791, mean_q: 1.291271
 23144/100000: episode: 307, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 26.989, mean reward: 0.675 [0.519, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.866, 10.100], loss: 0.001569, mae: 0.042612, mean_q: 1.299319
 23172/100000: episode: 308, duration: 0.139s, episode steps: 28, steps per second: 201, episode reward: 20.408, mean reward: 0.729 [0.592, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.750, 10.322], loss: 0.001826, mae: 0.046673, mean_q: 1.292090
 23208/100000: episode: 309, duration: 0.180s, episode steps: 36, steps per second: 199, episode reward: 27.462, mean reward: 0.763 [0.694, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.939, 10.523], loss: 0.001676, mae: 0.046159, mean_q: 1.308747
 23246/100000: episode: 310, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 25.310, mean reward: 0.666 [0.563, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.917, 10.257], loss: 0.001818, mae: 0.046147, mean_q: 1.292947
 23286/100000: episode: 311, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 28.890, mean reward: 0.722 [0.656, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-1.126, 10.481], loss: 0.001500, mae: 0.041891, mean_q: 1.303792
 23327/100000: episode: 312, duration: 0.213s, episode steps: 41, steps per second: 192, episode reward: 27.636, mean reward: 0.674 [0.571, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.186, 10.237], loss: 0.001685, mae: 0.044297, mean_q: 1.306888
 23347/100000: episode: 313, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 14.019, mean reward: 0.701 [0.669, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.272, 10.100], loss: 0.001394, mae: 0.041393, mean_q: 1.311686
 23385/100000: episode: 314, duration: 0.187s, episode steps: 38, steps per second: 204, episode reward: 30.814, mean reward: 0.811 [0.691, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.387, 10.610], loss: 0.001395, mae: 0.040500, mean_q: 1.304291
 23423/100000: episode: 315, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 25.621, mean reward: 0.674 [0.581, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.528, 10.423], loss: 0.001810, mae: 0.044096, mean_q: 1.302274
 23461/100000: episode: 316, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 23.844, mean reward: 0.627 [0.529, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.169, 10.173], loss: 0.002021, mae: 0.046193, mean_q: 1.308010
 23502/100000: episode: 317, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 25.785, mean reward: 0.629 [0.502, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.916, 10.100], loss: 0.001917, mae: 0.046767, mean_q: 1.311301
 23530/100000: episode: 318, duration: 0.137s, episode steps: 28, steps per second: 205, episode reward: 20.541, mean reward: 0.734 [0.652, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.330, 10.578], loss: 0.002438, mae: 0.052041, mean_q: 1.312310
 23573/100000: episode: 319, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 30.898, mean reward: 0.719 [0.621, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.911, 10.509], loss: 0.002393, mae: 0.052165, mean_q: 1.309041
 23609/100000: episode: 320, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 24.221, mean reward: 0.673 [0.549, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.083, 10.207], loss: 0.001943, mae: 0.044705, mean_q: 1.306409
 23637/100000: episode: 321, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 20.107, mean reward: 0.718 [0.655, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.903, 10.438], loss: 0.001965, mae: 0.046163, mean_q: 1.313392
 23672/100000: episode: 322, duration: 0.215s, episode steps: 35, steps per second: 163, episode reward: 23.750, mean reward: 0.679 [0.552, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.159, 10.310], loss: 0.001871, mae: 0.045908, mean_q: 1.311564
 23710/100000: episode: 323, duration: 0.234s, episode steps: 38, steps per second: 162, episode reward: 26.672, mean reward: 0.702 [0.581, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.181, 10.342], loss: 0.002168, mae: 0.049308, mean_q: 1.306412
 23753/100000: episode: 324, duration: 0.245s, episode steps: 43, steps per second: 175, episode reward: 29.966, mean reward: 0.697 [0.575, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.803, 10.275], loss: 0.001939, mae: 0.048058, mean_q: 1.318969
 23791/100000: episode: 325, duration: 0.249s, episode steps: 38, steps per second: 152, episode reward: 23.989, mean reward: 0.631 [0.559, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.369, 10.303], loss: 0.001980, mae: 0.047329, mean_q: 1.313585
 23834/100000: episode: 326, duration: 0.253s, episode steps: 43, steps per second: 170, episode reward: 27.935, mean reward: 0.650 [0.530, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.720, 10.342], loss: 0.002453, mae: 0.053393, mean_q: 1.319612
 23862/100000: episode: 327, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 19.509, mean reward: 0.697 [0.634, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.296, 10.374], loss: 0.001645, mae: 0.044991, mean_q: 1.318003
 23900/100000: episode: 328, duration: 0.195s, episode steps: 38, steps per second: 195, episode reward: 23.725, mean reward: 0.624 [0.536, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.108, 10.318], loss: 0.002009, mae: 0.046729, mean_q: 1.314341
 23943/100000: episode: 329, duration: 0.238s, episode steps: 43, steps per second: 180, episode reward: 29.560, mean reward: 0.687 [0.606, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.714, 10.373], loss: 0.002160, mae: 0.049001, mean_q: 1.312097
[Info] 3-TH LEVEL FOUND: 1.6094318628311157, Considering 10/90 traces
 23963/100000: episode: 330, duration: 5.714s, episode steps: 20, steps per second: 4, episode reward: 13.902, mean reward: 0.695 [0.611, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.159, 10.100], loss: 0.001813, mae: 0.045495, mean_q: 1.305476
 23989/100000: episode: 331, duration: 0.276s, episode steps: 26, steps per second: 94, episode reward: 19.922, mean reward: 0.766 [0.644, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.388], loss: 0.001979, mae: 0.048112, mean_q: 1.324420
 24007/100000: episode: 332, duration: 0.206s, episode steps: 18, steps per second: 87, episode reward: 14.296, mean reward: 0.794 [0.739, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.475], loss: 0.001889, mae: 0.046503, mean_q: 1.318552
 24037/100000: episode: 333, duration: 0.365s, episode steps: 30, steps per second: 82, episode reward: 22.302, mean reward: 0.743 [0.631, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.472, 10.335], loss: 0.001764, mae: 0.045370, mean_q: 1.323769
 24051/100000: episode: 334, duration: 0.170s, episode steps: 14, steps per second: 82, episode reward: 10.495, mean reward: 0.750 [0.662, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.538], loss: 0.002106, mae: 0.048506, mean_q: 1.319673
 24079/100000: episode: 335, duration: 0.441s, episode steps: 28, steps per second: 64, episode reward: 22.679, mean reward: 0.810 [0.666, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.592, 10.440], loss: 0.001800, mae: 0.045421, mean_q: 1.320030
[Info] FALSIFICATION!
 24095/100000: episode: 336, duration: 0.755s, episode steps: 16, steps per second: 21, episode reward: 14.709, mean reward: 0.919 [0.855, 1.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.051, 9.986], loss: 0.002040, mae: 0.047196, mean_q: 1.316297
 24117/100000: episode: 337, duration: 0.254s, episode steps: 22, steps per second: 87, episode reward: 17.292, mean reward: 0.786 [0.619, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.522], loss: 0.002209, mae: 0.050366, mean_q: 1.317824
 24139/100000: episode: 338, duration: 0.188s, episode steps: 22, steps per second: 117, episode reward: 18.506, mean reward: 0.841 [0.788, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.597], loss: 0.002300, mae: 0.048655, mean_q: 1.336374
 24170/100000: episode: 339, duration: 0.235s, episode steps: 31, steps per second: 132, episode reward: 24.966, mean reward: 0.805 [0.738, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.639, 10.523], loss: 0.002110, mae: 0.047808, mean_q: 1.331246
 24192/100000: episode: 340, duration: 0.250s, episode steps: 22, steps per second: 88, episode reward: 18.030, mean reward: 0.820 [0.761, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-1.100, 10.631], loss: 0.002262, mae: 0.049899, mean_q: 1.322182
[Info] FALSIFICATION!
 24206/100000: episode: 341, duration: 0.576s, episode steps: 14, steps per second: 24, episode reward: 11.868, mean reward: 0.848 [0.786, 1.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.661, 9.973], loss: 0.001668, mae: 0.045283, mean_q: 1.360098
 24233/100000: episode: 342, duration: 0.360s, episode steps: 27, steps per second: 75, episode reward: 19.731, mean reward: 0.731 [0.658, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.774, 10.346], loss: 0.002107, mae: 0.047257, mean_q: 1.332961
 24247/100000: episode: 343, duration: 0.161s, episode steps: 14, steps per second: 87, episode reward: 11.297, mean reward: 0.807 [0.778, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.509], loss: 0.001996, mae: 0.047054, mean_q: 1.337499
 24277/100000: episode: 344, duration: 0.316s, episode steps: 30, steps per second: 95, episode reward: 23.114, mean reward: 0.770 [0.629, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.566, 10.461], loss: 0.002120, mae: 0.048977, mean_q: 1.335822
 24291/100000: episode: 345, duration: 0.152s, episode steps: 14, steps per second: 92, episode reward: 11.016, mean reward: 0.787 [0.691, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.559], loss: 0.002003, mae: 0.046457, mean_q: 1.339083
[Info] FALSIFICATION!
 24303/100000: episode: 346, duration: 0.350s, episode steps: 12, steps per second: 34, episode reward: 10.598, mean reward: 0.883 [0.816, 1.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.021, 9.797], loss: 0.002105, mae: 0.049997, mean_q: 1.341965
 24333/100000: episode: 347, duration: 0.174s, episode steps: 30, steps per second: 173, episode reward: 25.123, mean reward: 0.837 [0.738, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.052, 10.492], loss: 0.002178, mae: 0.047823, mean_q: 1.350893
 24355/100000: episode: 348, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 18.615, mean reward: 0.846 [0.748, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.228, 10.696], loss: 0.002411, mae: 0.048978, mean_q: 1.355989
 24385/100000: episode: 349, duration: 0.187s, episode steps: 30, steps per second: 160, episode reward: 23.304, mean reward: 0.777 [0.656, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.101, 10.397], loss: 0.001940, mae: 0.047201, mean_q: 1.346771
 24415/100000: episode: 350, duration: 0.195s, episode steps: 30, steps per second: 154, episode reward: 22.919, mean reward: 0.764 [0.615, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.189, 10.391], loss: 0.002550, mae: 0.049093, mean_q: 1.339500
 24437/100000: episode: 351, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 17.645, mean reward: 0.802 [0.713, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.517], loss: 0.002135, mae: 0.045865, mean_q: 1.340955
 24463/100000: episode: 352, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 20.559, mean reward: 0.791 [0.698, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.148, 10.453], loss: 0.002350, mae: 0.049358, mean_q: 1.361540
 24481/100000: episode: 353, duration: 0.117s, episode steps: 18, steps per second: 154, episode reward: 14.503, mean reward: 0.806 [0.727, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-1.443, 10.512], loss: 0.001831, mae: 0.046697, mean_q: 1.338915
 24503/100000: episode: 354, duration: 0.138s, episode steps: 22, steps per second: 160, episode reward: 16.953, mean reward: 0.771 [0.700, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.671], loss: 0.002060, mae: 0.049666, mean_q: 1.352856
 24517/100000: episode: 355, duration: 0.107s, episode steps: 14, steps per second: 131, episode reward: 12.450, mean reward: 0.889 [0.818, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.740], loss: 0.002360, mae: 0.048314, mean_q: 1.347406
 24531/100000: episode: 356, duration: 0.116s, episode steps: 14, steps per second: 120, episode reward: 11.348, mean reward: 0.811 [0.757, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.653], loss: 0.002006, mae: 0.049856, mean_q: 1.367277
 24557/100000: episode: 357, duration: 0.214s, episode steps: 26, steps per second: 122, episode reward: 20.936, mean reward: 0.805 [0.703, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.179, 10.538], loss: 0.002025, mae: 0.048342, mean_q: 1.368937
 24571/100000: episode: 358, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 11.110, mean reward: 0.794 [0.742, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.397, 10.458], loss: 0.002072, mae: 0.049280, mean_q: 1.351285
 24603/100000: episode: 359, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 23.277, mean reward: 0.727 [0.665, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.418, 10.496], loss: 0.001999, mae: 0.047940, mean_q: 1.361114
 24634/100000: episode: 360, duration: 0.174s, episode steps: 31, steps per second: 179, episode reward: 23.312, mean reward: 0.752 [0.576, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.323], loss: 0.002529, mae: 0.050328, mean_q: 1.347273
 24656/100000: episode: 361, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 17.589, mean reward: 0.799 [0.712, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.196, 10.535], loss: 0.002045, mae: 0.045465, mean_q: 1.372721
 24678/100000: episode: 362, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 17.106, mean reward: 0.778 [0.692, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.519], loss: 0.002032, mae: 0.048132, mean_q: 1.369792
 24704/100000: episode: 363, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 20.519, mean reward: 0.789 [0.716, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.350, 10.505], loss: 0.002336, mae: 0.053902, mean_q: 1.363264
 24722/100000: episode: 364, duration: 0.110s, episode steps: 18, steps per second: 163, episode reward: 13.647, mean reward: 0.758 [0.694, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.672, 10.458], loss: 0.002043, mae: 0.048410, mean_q: 1.370277
 24754/100000: episode: 365, duration: 0.239s, episode steps: 32, steps per second: 134, episode reward: 23.262, mean reward: 0.727 [0.550, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.590, 10.391], loss: 0.002811, mae: 0.050971, mean_q: 1.367444
 24785/100000: episode: 366, duration: 0.193s, episode steps: 31, steps per second: 161, episode reward: 25.854, mean reward: 0.834 [0.654, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.498, 10.388], loss: 0.002376, mae: 0.049176, mean_q: 1.377103
 24817/100000: episode: 367, duration: 0.190s, episode steps: 32, steps per second: 169, episode reward: 23.327, mean reward: 0.729 [0.554, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.447, 10.291], loss: 0.002258, mae: 0.048513, mean_q: 1.389839
 24839/100000: episode: 368, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 17.382, mean reward: 0.790 [0.718, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.963, 10.484], loss: 0.002205, mae: 0.047842, mean_q: 1.368625
 24853/100000: episode: 369, duration: 0.087s, episode steps: 14, steps per second: 162, episode reward: 12.403, mean reward: 0.886 [0.808, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.591], loss: 0.002083, mae: 0.047629, mean_q: 1.371390
 24884/100000: episode: 370, duration: 0.197s, episode steps: 31, steps per second: 158, episode reward: 23.222, mean reward: 0.749 [0.637, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.850, 10.458], loss: 0.002831, mae: 0.055819, mean_q: 1.374828
 24902/100000: episode: 371, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 15.728, mean reward: 0.874 [0.808, 0.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-1.103, 10.584], loss: 0.001892, mae: 0.048078, mean_q: 1.383939
 24920/100000: episode: 372, duration: 0.117s, episode steps: 18, steps per second: 153, episode reward: 14.118, mean reward: 0.784 [0.744, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.049, 10.536], loss: 0.001798, mae: 0.046877, mean_q: 1.379196
 24951/100000: episode: 373, duration: 0.202s, episode steps: 31, steps per second: 153, episode reward: 25.476, mean reward: 0.822 [0.744, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.591, 10.485], loss: 0.002391, mae: 0.048374, mean_q: 1.375349
 24965/100000: episode: 374, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 11.930, mean reward: 0.852 [0.759, 0.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.359, 10.618], loss: 0.001695, mae: 0.044789, mean_q: 1.379060
 24987/100000: episode: 375, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 18.886, mean reward: 0.858 [0.784, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.447, 10.658], loss: 0.002307, mae: 0.049161, mean_q: 1.377830
 25019/100000: episode: 376, duration: 0.192s, episode steps: 32, steps per second: 166, episode reward: 26.186, mean reward: 0.818 [0.733, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.609, 10.500], loss: 0.002188, mae: 0.048744, mean_q: 1.383401
[Info] FALSIFICATION!
 25026/100000: episode: 377, duration: 0.314s, episode steps: 7, steps per second: 22, episode reward: 6.380, mean reward: 0.911 [0.847, 1.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.427, 10.704], loss: 0.004058, mae: 0.049882, mean_q: 1.396347
 25058/100000: episode: 378, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 24.732, mean reward: 0.773 [0.614, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.035, 10.454], loss: 0.002041, mae: 0.048760, mean_q: 1.387748
 25090/100000: episode: 379, duration: 0.227s, episode steps: 32, steps per second: 141, episode reward: 22.906, mean reward: 0.716 [0.605, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.613, 10.382], loss: 0.001685, mae: 0.044491, mean_q: 1.389901
 25104/100000: episode: 380, duration: 0.110s, episode steps: 14, steps per second: 127, episode reward: 11.357, mean reward: 0.811 [0.663, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.669, 10.463], loss: 0.001747, mae: 0.044132, mean_q: 1.376211
[Info] FALSIFICATION!
 25118/100000: episode: 381, duration: 0.397s, episode steps: 14, steps per second: 35, episode reward: 12.686, mean reward: 0.906 [0.786, 1.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.288, 9.825], loss: 0.001840, mae: 0.045930, mean_q: 1.401950
 25140/100000: episode: 382, duration: 0.209s, episode steps: 22, steps per second: 105, episode reward: 17.953, mean reward: 0.816 [0.756, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.822, 10.603], loss: 0.001663, mae: 0.043691, mean_q: 1.401520
 25170/100000: episode: 383, duration: 0.209s, episode steps: 30, steps per second: 143, episode reward: 22.144, mean reward: 0.738 [0.621, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.720, 10.344], loss: 0.002128, mae: 0.050076, mean_q: 1.393367
 25184/100000: episode: 384, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 12.249, mean reward: 0.875 [0.786, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.664], loss: 0.001670, mae: 0.042816, mean_q: 1.399200
 25210/100000: episode: 385, duration: 0.162s, episode steps: 26, steps per second: 160, episode reward: 22.789, mean reward: 0.877 [0.774, 0.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.797, 10.729], loss: 0.002652, mae: 0.051068, mean_q: 1.397950
 25228/100000: episode: 386, duration: 0.115s, episode steps: 18, steps per second: 156, episode reward: 15.216, mean reward: 0.845 [0.743, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-1.097, 10.496], loss: 0.001928, mae: 0.048571, mean_q: 1.410085
 25256/100000: episode: 387, duration: 0.205s, episode steps: 28, steps per second: 137, episode reward: 23.936, mean reward: 0.855 [0.778, 0.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.499, 10.631], loss: 0.002390, mae: 0.048468, mean_q: 1.400716
 25278/100000: episode: 388, duration: 0.153s, episode steps: 22, steps per second: 144, episode reward: 16.590, mean reward: 0.754 [0.649, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.353], loss: 0.002116, mae: 0.045632, mean_q: 1.412049
 25296/100000: episode: 389, duration: 0.124s, episode steps: 18, steps per second: 145, episode reward: 14.941, mean reward: 0.830 [0.754, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.468, 10.611], loss: 0.002929, mae: 0.047916, mean_q: 1.398783
 25324/100000: episode: 390, duration: 0.170s, episode steps: 28, steps per second: 165, episode reward: 22.564, mean reward: 0.806 [0.707, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.560], loss: 0.002462, mae: 0.050846, mean_q: 1.411159
 25355/100000: episode: 391, duration: 0.206s, episode steps: 31, steps per second: 151, episode reward: 25.872, mean reward: 0.835 [0.657, 0.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.221, 10.403], loss: 0.002381, mae: 0.053884, mean_q: 1.405771
 25373/100000: episode: 392, duration: 0.137s, episode steps: 18, steps per second: 132, episode reward: 14.673, mean reward: 0.815 [0.766, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.917, 10.604], loss: 0.001988, mae: 0.048406, mean_q: 1.407667
 25391/100000: episode: 393, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 14.872, mean reward: 0.826 [0.764, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-1.556, 10.631], loss: 0.002524, mae: 0.051407, mean_q: 1.415788
 25418/100000: episode: 394, duration: 0.282s, episode steps: 27, steps per second: 96, episode reward: 21.686, mean reward: 0.803 [0.748, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.186, 10.482], loss: 0.002140, mae: 0.050660, mean_q: 1.398996
 25446/100000: episode: 395, duration: 0.306s, episode steps: 28, steps per second: 92, episode reward: 21.735, mean reward: 0.776 [0.718, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.087, 10.519], loss: 0.002777, mae: 0.053984, mean_q: 1.409550
 25460/100000: episode: 396, duration: 0.177s, episode steps: 14, steps per second: 79, episode reward: 12.777, mean reward: 0.913 [0.836, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.704], loss: 0.002785, mae: 0.051380, mean_q: 1.408631
 25482/100000: episode: 397, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 17.910, mean reward: 0.814 [0.755, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.583], loss: 0.001939, mae: 0.046747, mean_q: 1.425040
 25509/100000: episode: 398, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 21.406, mean reward: 0.793 [0.685, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.625, 10.456], loss: 0.002053, mae: 0.049762, mean_q: 1.412812
 25536/100000: episode: 399, duration: 0.166s, episode steps: 27, steps per second: 162, episode reward: 22.499, mean reward: 0.833 [0.775, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.240, 10.463], loss: 0.002587, mae: 0.046204, mean_q: 1.436344
 25568/100000: episode: 400, duration: 0.188s, episode steps: 32, steps per second: 170, episode reward: 25.217, mean reward: 0.788 [0.700, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.385, 10.572], loss: 0.002002, mae: 0.047876, mean_q: 1.418853
 25599/100000: episode: 401, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 24.754, mean reward: 0.799 [0.713, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.115, 10.571], loss: 0.002498, mae: 0.050320, mean_q: 1.425733
 25621/100000: episode: 402, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 18.644, mean reward: 0.847 [0.744, 0.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.081, 10.487], loss: 0.003857, mae: 0.056253, mean_q: 1.419425
 25653/100000: episode: 403, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 25.616, mean reward: 0.801 [0.729, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.236, 10.509], loss: 0.002434, mae: 0.049389, mean_q: 1.427259
 25679/100000: episode: 404, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 21.590, mean reward: 0.830 [0.722, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.736, 10.455], loss: 0.002612, mae: 0.050772, mean_q: 1.420174
 25707/100000: episode: 405, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 21.676, mean reward: 0.774 [0.639, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.582, 10.398], loss: 0.001878, mae: 0.045029, mean_q: 1.433345
 25725/100000: episode: 406, duration: 0.132s, episode steps: 18, steps per second: 136, episode reward: 15.486, mean reward: 0.860 [0.777, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.332, 10.558], loss: 0.002305, mae: 0.047759, mean_q: 1.420137
 25739/100000: episode: 407, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 12.174, mean reward: 0.870 [0.779, 0.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.686, 10.684], loss: 0.002172, mae: 0.045416, mean_q: 1.433685
 25765/100000: episode: 408, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 19.483, mean reward: 0.749 [0.608, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.410, 10.385], loss: 0.001955, mae: 0.048332, mean_q: 1.438675
 25783/100000: episode: 409, duration: 0.124s, episode steps: 18, steps per second: 146, episode reward: 14.625, mean reward: 0.813 [0.719, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.275, 10.532], loss: 0.002072, mae: 0.049149, mean_q: 1.414147
[Info] FALSIFICATION!
 25794/100000: episode: 410, duration: 0.316s, episode steps: 11, steps per second: 35, episode reward: 9.765, mean reward: 0.888 [0.799, 1.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.018, 10.229], loss: 0.003287, mae: 0.049246, mean_q: 1.443394
 25820/100000: episode: 411, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 21.178, mean reward: 0.815 [0.751, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.551], loss: 0.002616, mae: 0.048797, mean_q: 1.433132
 25838/100000: episode: 412, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 14.611, mean reward: 0.812 [0.762, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.258, 10.609], loss: 0.002385, mae: 0.049778, mean_q: 1.431466
 25870/100000: episode: 413, duration: 0.188s, episode steps: 32, steps per second: 171, episode reward: 24.672, mean reward: 0.771 [0.688, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.614, 10.647], loss: 0.002260, mae: 0.047767, mean_q: 1.437120
 25892/100000: episode: 414, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 18.161, mean reward: 0.825 [0.721, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.471], loss: 0.002150, mae: 0.045225, mean_q: 1.420736
[Info] FALSIFICATION!
 25903/100000: episode: 415, duration: 0.245s, episode steps: 11, steps per second: 45, episode reward: 9.809, mean reward: 0.892 [0.829, 1.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.265, 9.555], loss: 0.002879, mae: 0.052782, mean_q: 1.433570
 25934/100000: episode: 416, duration: 0.224s, episode steps: 31, steps per second: 138, episode reward: 27.121, mean reward: 0.875 [0.793, 0.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.451, 10.697], loss: 0.001869, mae: 0.046063, mean_q: 1.433488
 25966/100000: episode: 417, duration: 0.217s, episode steps: 32, steps per second: 148, episode reward: 26.402, mean reward: 0.825 [0.701, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.747, 10.569], loss: 0.002618, mae: 0.049197, mean_q: 1.431698
 25993/100000: episode: 418, duration: 0.204s, episode steps: 27, steps per second: 133, episode reward: 21.314, mean reward: 0.789 [0.737, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.519], loss: 0.002605, mae: 0.050621, mean_q: 1.433140
 26021/100000: episode: 419, duration: 0.213s, episode steps: 28, steps per second: 131, episode reward: 23.379, mean reward: 0.835 [0.766, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.138, 10.577], loss: 0.001916, mae: 0.047359, mean_q: 1.435992
[Info] Complete ISplit Iteration
[Info] Levels: [1.2282792, 1.4282334, 1.6094319, 1.7017713]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.31]
[Info] Error Prob: 0.00031000000000000005

 26053/100000: episode: 420, duration: 5.260s, episode steps: 32, steps per second: 6, episode reward: 26.131, mean reward: 0.817 [0.719, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.067, 10.488], loss: 0.001888, mae: 0.046522, mean_q: 1.427108
 26153/100000: episode: 421, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.481, mean reward: 0.585 [0.510, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.464, 10.324], loss: 0.002266, mae: 0.048517, mean_q: 1.443671
 26253/100000: episode: 422, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 58.517, mean reward: 0.585 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.751, 10.199], loss: 0.002114, mae: 0.048023, mean_q: 1.442382
 26353/100000: episode: 423, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.499, mean reward: 0.585 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.753, 10.284], loss: 0.002589, mae: 0.050831, mean_q: 1.425971
 26453/100000: episode: 424, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 60.204, mean reward: 0.602 [0.509, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.811, 10.116], loss: 0.002360, mae: 0.049432, mean_q: 1.431025
 26553/100000: episode: 425, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 57.339, mean reward: 0.573 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.416, 10.133], loss: 0.002552, mae: 0.049466, mean_q: 1.422414
 26653/100000: episode: 426, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 58.226, mean reward: 0.582 [0.515, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.739, 10.311], loss: 0.002300, mae: 0.049164, mean_q: 1.413893
 26753/100000: episode: 427, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 58.193, mean reward: 0.582 [0.501, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.230, 10.098], loss: 0.002625, mae: 0.050066, mean_q: 1.399280
 26853/100000: episode: 428, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 59.244, mean reward: 0.592 [0.512, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.331, 10.232], loss: 0.001978, mae: 0.045519, mean_q: 1.409726
 26953/100000: episode: 429, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 59.056, mean reward: 0.591 [0.503, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.869, 10.098], loss: 0.002336, mae: 0.048189, mean_q: 1.406649
 27053/100000: episode: 430, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 58.940, mean reward: 0.589 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.638, 10.098], loss: 0.001948, mae: 0.045629, mean_q: 1.403014
 27153/100000: episode: 431, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 58.271, mean reward: 0.583 [0.507, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.553, 10.098], loss: 0.002641, mae: 0.051056, mean_q: 1.395566
 27253/100000: episode: 432, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 59.726, mean reward: 0.597 [0.512, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.298, 10.098], loss: 0.002551, mae: 0.048497, mean_q: 1.389183
 27353/100000: episode: 433, duration: 0.615s, episode steps: 100, steps per second: 162, episode reward: 57.810, mean reward: 0.578 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.973, 10.253], loss: 0.002257, mae: 0.048339, mean_q: 1.393244
 27453/100000: episode: 434, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 57.512, mean reward: 0.575 [0.499, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.098, 10.115], loss: 0.002680, mae: 0.049609, mean_q: 1.378496
 27553/100000: episode: 435, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 58.773, mean reward: 0.588 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.605, 10.129], loss: 0.002228, mae: 0.048159, mean_q: 1.372818
 27653/100000: episode: 436, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 60.656, mean reward: 0.607 [0.502, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.293, 10.098], loss: 0.001915, mae: 0.046744, mean_q: 1.375863
 27753/100000: episode: 437, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 59.614, mean reward: 0.596 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.629, 10.371], loss: 0.002171, mae: 0.047192, mean_q: 1.368541
 27853/100000: episode: 438, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 58.967, mean reward: 0.590 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.125, 10.098], loss: 0.002591, mae: 0.048257, mean_q: 1.365179
 27953/100000: episode: 439, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 56.770, mean reward: 0.568 [0.504, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.336, 10.182], loss: 0.001961, mae: 0.045802, mean_q: 1.365280
 28053/100000: episode: 440, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 59.499, mean reward: 0.595 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.284, 10.189], loss: 0.002087, mae: 0.047632, mean_q: 1.356518
 28153/100000: episode: 441, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 60.147, mean reward: 0.601 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.142, 10.098], loss: 0.002439, mae: 0.049221, mean_q: 1.351194
 28253/100000: episode: 442, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 57.824, mean reward: 0.578 [0.500, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.726, 10.207], loss: 0.002018, mae: 0.045205, mean_q: 1.343498
 28353/100000: episode: 443, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 60.560, mean reward: 0.606 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.716, 10.098], loss: 0.002642, mae: 0.051919, mean_q: 1.337620
 28453/100000: episode: 444, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.239, mean reward: 0.602 [0.512, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.059, 10.362], loss: 0.002598, mae: 0.047542, mean_q: 1.342736
 28553/100000: episode: 445, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.758, mean reward: 0.588 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.718, 10.098], loss: 0.001973, mae: 0.045966, mean_q: 1.336820
 28653/100000: episode: 446, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 57.831, mean reward: 0.578 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.830, 10.098], loss: 0.002568, mae: 0.048173, mean_q: 1.328758
 28753/100000: episode: 447, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.748, mean reward: 0.577 [0.502, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.426, 10.192], loss: 0.001766, mae: 0.044357, mean_q: 1.328950
 28853/100000: episode: 448, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 59.623, mean reward: 0.596 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.211, 10.368], loss: 0.001818, mae: 0.044613, mean_q: 1.324711
 28953/100000: episode: 449, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 62.089, mean reward: 0.621 [0.500, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.097, 10.098], loss: 0.002317, mae: 0.047747, mean_q: 1.322820
 29053/100000: episode: 450, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.103, mean reward: 0.581 [0.499, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.633, 10.098], loss: 0.001973, mae: 0.046031, mean_q: 1.318800
 29153/100000: episode: 451, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.702, mean reward: 0.587 [0.510, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.806, 10.248], loss: 0.001713, mae: 0.045461, mean_q: 1.302685
 29253/100000: episode: 452, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.575, mean reward: 0.586 [0.501, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.261, 10.098], loss: 0.002045, mae: 0.045836, mean_q: 1.292585
 29353/100000: episode: 453, duration: 0.658s, episode steps: 100, steps per second: 152, episode reward: 60.673, mean reward: 0.607 [0.505, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.548, 10.098], loss: 0.002155, mae: 0.047152, mean_q: 1.284299
 29453/100000: episode: 454, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 58.412, mean reward: 0.584 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.424, 10.098], loss: 0.002201, mae: 0.047147, mean_q: 1.286009
 29553/100000: episode: 455, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 58.152, mean reward: 0.582 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.955, 10.098], loss: 0.001923, mae: 0.045079, mean_q: 1.275601
 29653/100000: episode: 456, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 56.804, mean reward: 0.568 [0.499, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.899, 10.098], loss: 0.001919, mae: 0.045241, mean_q: 1.265095
 29753/100000: episode: 457, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 57.710, mean reward: 0.577 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.010, 10.123], loss: 0.001669, mae: 0.044088, mean_q: 1.268902
 29853/100000: episode: 458, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.369, mean reward: 0.594 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.316, 10.357], loss: 0.001747, mae: 0.045523, mean_q: 1.261812
 29953/100000: episode: 459, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 59.594, mean reward: 0.596 [0.507, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.651, 10.098], loss: 0.001878, mae: 0.045069, mean_q: 1.254398
 30053/100000: episode: 460, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 58.918, mean reward: 0.589 [0.516, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.840, 10.098], loss: 0.001972, mae: 0.045282, mean_q: 1.243529
 30153/100000: episode: 461, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 57.725, mean reward: 0.577 [0.502, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.654, 10.098], loss: 0.001617, mae: 0.043972, mean_q: 1.233418
 30253/100000: episode: 462, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.875, mean reward: 0.599 [0.512, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.634, 10.098], loss: 0.001832, mae: 0.045730, mean_q: 1.226551
 30353/100000: episode: 463, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 59.333, mean reward: 0.593 [0.504, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.257, 10.370], loss: 0.001682, mae: 0.042723, mean_q: 1.214810
 30453/100000: episode: 464, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 59.569, mean reward: 0.596 [0.510, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.813, 10.098], loss: 0.001690, mae: 0.042843, mean_q: 1.208377
 30553/100000: episode: 465, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 57.470, mean reward: 0.575 [0.507, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.098], loss: 0.001996, mae: 0.044247, mean_q: 1.199888
 30653/100000: episode: 466, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 57.823, mean reward: 0.578 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.410, 10.105], loss: 0.001578, mae: 0.043140, mean_q: 1.191973
 30753/100000: episode: 467, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 59.198, mean reward: 0.592 [0.501, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.021, 10.108], loss: 0.001665, mae: 0.043883, mean_q: 1.185680
 30853/100000: episode: 468, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.708, mean reward: 0.587 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.754, 10.098], loss: 0.001563, mae: 0.043047, mean_q: 1.182015
 30953/100000: episode: 469, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 58.593, mean reward: 0.586 [0.508, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.188, 10.098], loss: 0.001460, mae: 0.041999, mean_q: 1.172328
 31053/100000: episode: 470, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 58.006, mean reward: 0.580 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.909, 10.367], loss: 0.001504, mae: 0.041978, mean_q: 1.167611
 31153/100000: episode: 471, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 58.357, mean reward: 0.584 [0.504, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.198, 10.170], loss: 0.001450, mae: 0.041641, mean_q: 1.164646
 31253/100000: episode: 472, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.513, mean reward: 0.575 [0.503, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.537, 10.118], loss: 0.001536, mae: 0.042706, mean_q: 1.162093
 31353/100000: episode: 473, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 59.329, mean reward: 0.593 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.704, 10.098], loss: 0.001629, mae: 0.043667, mean_q: 1.164438
 31453/100000: episode: 474, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 60.397, mean reward: 0.604 [0.501, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.881, 10.539], loss: 0.001497, mae: 0.042077, mean_q: 1.159604
 31553/100000: episode: 475, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 62.034, mean reward: 0.620 [0.525, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.400, 10.366], loss: 0.001464, mae: 0.041780, mean_q: 1.161435
 31653/100000: episode: 476, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 59.621, mean reward: 0.596 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.349, 10.301], loss: 0.001409, mae: 0.041278, mean_q: 1.165382
 31753/100000: episode: 477, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 61.644, mean reward: 0.616 [0.510, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.729, 10.174], loss: 0.001586, mae: 0.043043, mean_q: 1.165792
 31853/100000: episode: 478, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 61.255, mean reward: 0.613 [0.518, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.985, 10.292], loss: 0.001546, mae: 0.043202, mean_q: 1.166198
 31953/100000: episode: 479, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 57.302, mean reward: 0.573 [0.507, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.647, 10.115], loss: 0.001479, mae: 0.042732, mean_q: 1.165981
 32053/100000: episode: 480, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 58.516, mean reward: 0.585 [0.508, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.820, 10.216], loss: 0.001574, mae: 0.043145, mean_q: 1.163787
 32153/100000: episode: 481, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 58.418, mean reward: 0.584 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.783, 10.153], loss: 0.001415, mae: 0.041242, mean_q: 1.166099
 32253/100000: episode: 482, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 60.882, mean reward: 0.609 [0.518, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.963, 10.098], loss: 0.001476, mae: 0.042191, mean_q: 1.168684
 32353/100000: episode: 483, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 59.617, mean reward: 0.596 [0.505, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.286, 10.098], loss: 0.001458, mae: 0.042239, mean_q: 1.167166
 32453/100000: episode: 484, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 56.121, mean reward: 0.561 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.587, 10.190], loss: 0.001451, mae: 0.041630, mean_q: 1.168185
 32553/100000: episode: 485, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 58.616, mean reward: 0.586 [0.510, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.600, 10.211], loss: 0.001459, mae: 0.041279, mean_q: 1.165690
 32653/100000: episode: 486, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.280, mean reward: 0.583 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.580, 10.098], loss: 0.001421, mae: 0.041562, mean_q: 1.167634
 32753/100000: episode: 487, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.804, mean reward: 0.588 [0.511, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.237, 10.320], loss: 0.001544, mae: 0.043205, mean_q: 1.165002
 32853/100000: episode: 488, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 60.685, mean reward: 0.607 [0.502, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.117, 10.098], loss: 0.001462, mae: 0.042482, mean_q: 1.166678
 32953/100000: episode: 489, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 59.636, mean reward: 0.596 [0.509, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.436, 10.340], loss: 0.001548, mae: 0.042806, mean_q: 1.165790
 33053/100000: episode: 490, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 59.974, mean reward: 0.600 [0.517, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.926, 10.098], loss: 0.001550, mae: 0.043335, mean_q: 1.166721
 33153/100000: episode: 491, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 60.186, mean reward: 0.602 [0.517, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.287, 10.359], loss: 0.001478, mae: 0.042199, mean_q: 1.168646
 33253/100000: episode: 492, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.358, mean reward: 0.584 [0.506, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.802, 10.216], loss: 0.001524, mae: 0.042461, mean_q: 1.168470
 33353/100000: episode: 493, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 57.822, mean reward: 0.578 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.906, 10.098], loss: 0.001500, mae: 0.042367, mean_q: 1.168318
 33453/100000: episode: 494, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.191, mean reward: 0.582 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.747, 10.098], loss: 0.001576, mae: 0.042757, mean_q: 1.167863
 33553/100000: episode: 495, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 58.825, mean reward: 0.588 [0.509, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.977, 10.098], loss: 0.001449, mae: 0.041557, mean_q: 1.166938
 33653/100000: episode: 496, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.434, mean reward: 0.584 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.849, 10.135], loss: 0.001715, mae: 0.045063, mean_q: 1.165460
 33753/100000: episode: 497, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 59.916, mean reward: 0.599 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.221, 10.165], loss: 0.001563, mae: 0.043344, mean_q: 1.166539
 33853/100000: episode: 498, duration: 0.678s, episode steps: 100, steps per second: 147, episode reward: 57.185, mean reward: 0.572 [0.505, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.773, 10.098], loss: 0.001327, mae: 0.039963, mean_q: 1.167245
 33953/100000: episode: 499, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 57.779, mean reward: 0.578 [0.503, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.139, 10.157], loss: 0.001583, mae: 0.043031, mean_q: 1.166893
 34053/100000: episode: 500, duration: 0.685s, episode steps: 100, steps per second: 146, episode reward: 58.506, mean reward: 0.585 [0.514, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.766, 10.299], loss: 0.001432, mae: 0.041575, mean_q: 1.164435
 34153/100000: episode: 501, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 58.407, mean reward: 0.584 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.250], loss: 0.001549, mae: 0.042501, mean_q: 1.163118
 34253/100000: episode: 502, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.020, mean reward: 0.590 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.801, 10.280], loss: 0.001533, mae: 0.042842, mean_q: 1.169037
 34353/100000: episode: 503, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.367, mean reward: 0.574 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.778, 10.098], loss: 0.001580, mae: 0.043779, mean_q: 1.162865
 34453/100000: episode: 504, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 58.461, mean reward: 0.585 [0.514, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.656, 10.225], loss: 0.001491, mae: 0.042096, mean_q: 1.164426
 34553/100000: episode: 505, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.054, mean reward: 0.591 [0.507, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.706, 10.219], loss: 0.001393, mae: 0.040746, mean_q: 1.160997
 34653/100000: episode: 506, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 57.708, mean reward: 0.577 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.707, 10.098], loss: 0.001355, mae: 0.040365, mean_q: 1.165374
 34753/100000: episode: 507, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 62.236, mean reward: 0.622 [0.509, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.321, 10.098], loss: 0.001409, mae: 0.040961, mean_q: 1.163666
 34853/100000: episode: 508, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.763, mean reward: 0.598 [0.506, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.008, 10.248], loss: 0.001372, mae: 0.040441, mean_q: 1.163398
 34953/100000: episode: 509, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 57.322, mean reward: 0.573 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.347, 10.098], loss: 0.001541, mae: 0.042556, mean_q: 1.165805
 35053/100000: episode: 510, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 61.604, mean reward: 0.616 [0.497, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.572, 10.380], loss: 0.001399, mae: 0.040547, mean_q: 1.165169
 35153/100000: episode: 511, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 60.828, mean reward: 0.608 [0.501, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.904, 10.189], loss: 0.001424, mae: 0.040664, mean_q: 1.166815
 35253/100000: episode: 512, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 57.629, mean reward: 0.576 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.954, 10.261], loss: 0.001491, mae: 0.042171, mean_q: 1.166427
 35353/100000: episode: 513, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 57.994, mean reward: 0.580 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.640, 10.399], loss: 0.001659, mae: 0.044202, mean_q: 1.167078
 35453/100000: episode: 514, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 61.933, mean reward: 0.619 [0.501, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.038, 10.098], loss: 0.001766, mae: 0.045407, mean_q: 1.168019
 35553/100000: episode: 515, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 62.223, mean reward: 0.622 [0.510, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.700, 10.098], loss: 0.001448, mae: 0.041320, mean_q: 1.166835
 35653/100000: episode: 516, duration: 0.619s, episode steps: 100, steps per second: 161, episode reward: 57.681, mean reward: 0.577 [0.502, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.960, 10.256], loss: 0.001598, mae: 0.043513, mean_q: 1.170253
 35753/100000: episode: 517, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 59.294, mean reward: 0.593 [0.515, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.630, 10.178], loss: 0.001738, mae: 0.045217, mean_q: 1.166373
 35853/100000: episode: 518, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 58.675, mean reward: 0.587 [0.514, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.392, 10.228], loss: 0.001488, mae: 0.042146, mean_q: 1.167725
 35953/100000: episode: 519, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 58.275, mean reward: 0.583 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.438, 10.098], loss: 0.001518, mae: 0.042080, mean_q: 1.170445
[Info] 1-TH LEVEL FOUND: 1.3455902338027954, Considering 10/90 traces
 36053/100000: episode: 520, duration: 5.150s, episode steps: 100, steps per second: 19, episode reward: 57.542, mean reward: 0.575 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.543, 10.098], loss: 0.001571, mae: 0.043128, mean_q: 1.170750
 36065/100000: episode: 521, duration: 0.082s, episode steps: 12, steps per second: 146, episode reward: 8.097, mean reward: 0.675 [0.627, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.189, 10.353], loss: 0.001689, mae: 0.043996, mean_q: 1.175146
 36071/100000: episode: 522, duration: 0.050s, episode steps: 6, steps per second: 119, episode reward: 3.911, mean reward: 0.652 [0.581, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.246, 10.100], loss: 0.001649, mae: 0.043493, mean_q: 1.166802
 36084/100000: episode: 523, duration: 0.094s, episode steps: 13, steps per second: 138, episode reward: 8.808, mean reward: 0.678 [0.598, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-1.027, 10.496], loss: 0.001246, mae: 0.038837, mean_q: 1.166082
 36097/100000: episode: 524, duration: 0.095s, episode steps: 13, steps per second: 137, episode reward: 8.973, mean reward: 0.690 [0.609, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.464], loss: 0.002196, mae: 0.049543, mean_q: 1.167127
 36125/100000: episode: 525, duration: 0.164s, episode steps: 28, steps per second: 170, episode reward: 19.857, mean reward: 0.709 [0.606, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.417, 10.100], loss: 0.001710, mae: 0.045383, mean_q: 1.171684
 36151/100000: episode: 526, duration: 0.160s, episode steps: 26, steps per second: 162, episode reward: 18.245, mean reward: 0.702 [0.617, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.720, 10.100], loss: 0.001856, mae: 0.046746, mean_q: 1.164757
 36175/100000: episode: 527, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 15.283, mean reward: 0.637 [0.544, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.352, 10.100], loss: 0.001937, mae: 0.048099, mean_q: 1.169066
 36201/100000: episode: 528, duration: 0.168s, episode steps: 26, steps per second: 155, episode reward: 16.595, mean reward: 0.638 [0.592, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.227, 10.100], loss: 0.001536, mae: 0.043511, mean_q: 1.168592
 36227/100000: episode: 529, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 17.829, mean reward: 0.686 [0.573, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.619, 10.100], loss: 0.001709, mae: 0.044514, mean_q: 1.173808
 36253/100000: episode: 530, duration: 0.151s, episode steps: 26, steps per second: 173, episode reward: 18.057, mean reward: 0.694 [0.660, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.415, 10.100], loss: 0.001365, mae: 0.040733, mean_q: 1.174351
 36281/100000: episode: 531, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 21.095, mean reward: 0.753 [0.665, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.280, 10.100], loss: 0.001640, mae: 0.043382, mean_q: 1.174019
 36305/100000: episode: 532, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 15.712, mean reward: 0.655 [0.594, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.350, 10.100], loss: 0.001523, mae: 0.042983, mean_q: 1.174442
 36329/100000: episode: 533, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 14.840, mean reward: 0.618 [0.535, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.434, 10.100], loss: 0.001799, mae: 0.044785, mean_q: 1.173746
 36353/100000: episode: 534, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 16.541, mean reward: 0.689 [0.598, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.954, 10.100], loss: 0.001998, mae: 0.047376, mean_q: 1.174018
 36381/100000: episode: 535, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 19.215, mean reward: 0.686 [0.619, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.069, 10.100], loss: 0.001651, mae: 0.043629, mean_q: 1.176601
 36405/100000: episode: 536, duration: 0.175s, episode steps: 24, steps per second: 137, episode reward: 16.254, mean reward: 0.677 [0.598, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.322, 10.100], loss: 0.001584, mae: 0.044231, mean_q: 1.177914
 36429/100000: episode: 537, duration: 0.163s, episode steps: 24, steps per second: 147, episode reward: 16.728, mean reward: 0.697 [0.648, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.120, 10.100], loss: 0.001661, mae: 0.045966, mean_q: 1.182364
 36442/100000: episode: 538, duration: 0.094s, episode steps: 13, steps per second: 139, episode reward: 8.316, mean reward: 0.640 [0.592, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.433], loss: 0.001827, mae: 0.044828, mean_q: 1.184314
 36459/100000: episode: 539, duration: 0.116s, episode steps: 17, steps per second: 146, episode reward: 11.851, mean reward: 0.697 [0.595, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.639, 10.100], loss: 0.002258, mae: 0.050207, mean_q: 1.169926
 36480/100000: episode: 540, duration: 0.158s, episode steps: 21, steps per second: 133, episode reward: 13.938, mean reward: 0.664 [0.556, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.120, 10.100], loss: 0.001897, mae: 0.047382, mean_q: 1.182644
 36508/100000: episode: 541, duration: 0.188s, episode steps: 28, steps per second: 149, episode reward: 20.702, mean reward: 0.739 [0.649, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.173, 10.100], loss: 0.001738, mae: 0.044358, mean_q: 1.177125
 36532/100000: episode: 542, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 15.516, mean reward: 0.647 [0.589, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.703, 10.100], loss: 0.001682, mae: 0.043483, mean_q: 1.177603
 36556/100000: episode: 543, duration: 0.193s, episode steps: 24, steps per second: 124, episode reward: 16.174, mean reward: 0.674 [0.551, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.100, 10.100], loss: 0.001683, mae: 0.044685, mean_q: 1.182232
 36577/100000: episode: 544, duration: 0.135s, episode steps: 21, steps per second: 156, episode reward: 13.667, mean reward: 0.651 [0.570, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.122, 10.100], loss: 0.001727, mae: 0.043857, mean_q: 1.184937
 36601/100000: episode: 545, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 14.768, mean reward: 0.615 [0.524, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.100], loss: 0.001634, mae: 0.043242, mean_q: 1.185951
 36613/100000: episode: 546, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 8.209, mean reward: 0.684 [0.626, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.501], loss: 0.001781, mae: 0.042382, mean_q: 1.178089
 36619/100000: episode: 547, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 3.969, mean reward: 0.661 [0.606, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.355, 10.100], loss: 0.002175, mae: 0.047109, mean_q: 1.185980
 36643/100000: episode: 548, duration: 0.177s, episode steps: 24, steps per second: 136, episode reward: 15.543, mean reward: 0.648 [0.539, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.784, 10.100], loss: 0.001915, mae: 0.047297, mean_q: 1.177507
 36671/100000: episode: 549, duration: 0.212s, episode steps: 28, steps per second: 132, episode reward: 20.209, mean reward: 0.722 [0.629, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.564, 10.100], loss: 0.001777, mae: 0.045491, mean_q: 1.179417
 36695/100000: episode: 550, duration: 0.196s, episode steps: 24, steps per second: 122, episode reward: 16.473, mean reward: 0.686 [0.603, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.317, 10.100], loss: 0.001968, mae: 0.047174, mean_q: 1.185735
 36721/100000: episode: 551, duration: 0.193s, episode steps: 26, steps per second: 135, episode reward: 17.417, mean reward: 0.670 [0.609, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.253, 10.100], loss: 0.002148, mae: 0.048930, mean_q: 1.182530
 36745/100000: episode: 552, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 16.540, mean reward: 0.689 [0.620, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.302, 10.100], loss: 0.002120, mae: 0.048629, mean_q: 1.186838
 36762/100000: episode: 553, duration: 0.113s, episode steps: 17, steps per second: 150, episode reward: 11.050, mean reward: 0.650 [0.607, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.110, 10.100], loss: 0.002137, mae: 0.048467, mean_q: 1.180873
 36768/100000: episode: 554, duration: 0.042s, episode steps: 6, steps per second: 143, episode reward: 3.802, mean reward: 0.634 [0.584, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.670, 10.100], loss: 0.001724, mae: 0.042207, mean_q: 1.163837
 36789/100000: episode: 555, duration: 0.139s, episode steps: 21, steps per second: 151, episode reward: 13.837, mean reward: 0.659 [0.571, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.259, 10.100], loss: 0.001905, mae: 0.046040, mean_q: 1.186194
 36801/100000: episode: 556, duration: 0.085s, episode steps: 12, steps per second: 141, episode reward: 8.198, mean reward: 0.683 [0.617, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.326], loss: 0.001556, mae: 0.042755, mean_q: 1.180663
 36818/100000: episode: 557, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 11.605, mean reward: 0.683 [0.620, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.308, 10.100], loss: 0.001878, mae: 0.044691, mean_q: 1.187866
 36842/100000: episode: 558, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 16.394, mean reward: 0.683 [0.622, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.935, 10.100], loss: 0.001920, mae: 0.047060, mean_q: 1.185086
 36866/100000: episode: 559, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 14.628, mean reward: 0.610 [0.543, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.676, 10.100], loss: 0.001627, mae: 0.042917, mean_q: 1.184084
 36887/100000: episode: 560, duration: 0.142s, episode steps: 21, steps per second: 147, episode reward: 13.707, mean reward: 0.653 [0.598, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.798, 10.100], loss: 0.001972, mae: 0.047157, mean_q: 1.187188
 36911/100000: episode: 561, duration: 0.163s, episode steps: 24, steps per second: 148, episode reward: 15.047, mean reward: 0.627 [0.519, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.419, 10.106], loss: 0.001906, mae: 0.045895, mean_q: 1.180357
 36939/100000: episode: 562, duration: 0.272s, episode steps: 28, steps per second: 103, episode reward: 17.757, mean reward: 0.634 [0.540, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.205, 10.100], loss: 0.001783, mae: 0.044282, mean_q: 1.186337
 36963/100000: episode: 563, duration: 0.164s, episode steps: 24, steps per second: 146, episode reward: 16.771, mean reward: 0.699 [0.656, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.954, 10.100], loss: 0.001951, mae: 0.047898, mean_q: 1.187757
 36991/100000: episode: 564, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 19.255, mean reward: 0.688 [0.633, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.504, 10.100], loss: 0.002165, mae: 0.047948, mean_q: 1.189469
 37015/100000: episode: 565, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 16.031, mean reward: 0.668 [0.604, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.239, 10.100], loss: 0.001760, mae: 0.045031, mean_q: 1.184381
 37028/100000: episode: 566, duration: 0.121s, episode steps: 13, steps per second: 107, episode reward: 8.620, mean reward: 0.663 [0.622, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.439, 10.377], loss: 0.001579, mae: 0.043841, mean_q: 1.188570
 37052/100000: episode: 567, duration: 0.244s, episode steps: 24, steps per second: 99, episode reward: 14.876, mean reward: 0.620 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.784, 10.133], loss: 0.001501, mae: 0.041275, mean_q: 1.184870
 37076/100000: episode: 568, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 15.763, mean reward: 0.657 [0.552, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.075, 10.100], loss: 0.001792, mae: 0.044445, mean_q: 1.190800
 37102/100000: episode: 569, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 18.269, mean reward: 0.703 [0.642, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.302, 10.100], loss: 0.002266, mae: 0.050193, mean_q: 1.188433
 37123/100000: episode: 570, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 14.316, mean reward: 0.682 [0.615, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.423, 10.100], loss: 0.001951, mae: 0.047588, mean_q: 1.188154
 37135/100000: episode: 571, duration: 0.098s, episode steps: 12, steps per second: 122, episode reward: 7.742, mean reward: 0.645 [0.591, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.728, 10.351], loss: 0.001809, mae: 0.044238, mean_q: 1.190227
 37152/100000: episode: 572, duration: 0.132s, episode steps: 17, steps per second: 129, episode reward: 10.838, mean reward: 0.638 [0.583, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.231, 10.100], loss: 0.001628, mae: 0.043576, mean_q: 1.199302
 37165/100000: episode: 573, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.330, mean reward: 0.718 [0.634, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.443], loss: 0.001658, mae: 0.044535, mean_q: 1.200222
 37178/100000: episode: 574, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 7.901, mean reward: 0.608 [0.549, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.211], loss: 0.001787, mae: 0.044277, mean_q: 1.189375
 37184/100000: episode: 575, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 4.124, mean reward: 0.687 [0.660, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.305, 10.100], loss: 0.001774, mae: 0.047857, mean_q: 1.187999
 37208/100000: episode: 576, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 16.057, mean reward: 0.669 [0.599, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.497, 10.100], loss: 0.002272, mae: 0.051063, mean_q: 1.188032
 37214/100000: episode: 577, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 4.414, mean reward: 0.736 [0.610, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-2.536, 10.100], loss: 0.001397, mae: 0.041540, mean_q: 1.218627
 37226/100000: episode: 578, duration: 0.085s, episode steps: 12, steps per second: 142, episode reward: 8.766, mean reward: 0.731 [0.683, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.116, 10.422], loss: 0.001943, mae: 0.046804, mean_q: 1.176055
 37250/100000: episode: 579, duration: 0.164s, episode steps: 24, steps per second: 147, episode reward: 15.057, mean reward: 0.627 [0.525, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.749, 10.133], loss: 0.001617, mae: 0.042618, mean_q: 1.197420
 37274/100000: episode: 580, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 15.859, mean reward: 0.661 [0.586, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.977, 10.100], loss: 0.002003, mae: 0.046480, mean_q: 1.194100
 37286/100000: episode: 581, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 8.714, mean reward: 0.726 [0.665, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.468], loss: 0.001767, mae: 0.044972, mean_q: 1.193981
 37298/100000: episode: 582, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 8.154, mean reward: 0.680 [0.635, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.563, 10.380], loss: 0.001963, mae: 0.046885, mean_q: 1.199436
 37322/100000: episode: 583, duration: 0.197s, episode steps: 24, steps per second: 122, episode reward: 15.028, mean reward: 0.626 [0.559, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.154, 10.100], loss: 0.002187, mae: 0.050668, mean_q: 1.188181
 37335/100000: episode: 584, duration: 0.104s, episode steps: 13, steps per second: 125, episode reward: 8.563, mean reward: 0.659 [0.604, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.462], loss: 0.001999, mae: 0.047443, mean_q: 1.197785
 37359/100000: episode: 585, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 15.437, mean reward: 0.643 [0.574, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.377, 10.100], loss: 0.001821, mae: 0.045583, mean_q: 1.189173
 37365/100000: episode: 586, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 4.215, mean reward: 0.703 [0.653, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.315, 10.100], loss: 0.001912, mae: 0.046475, mean_q: 1.169824
 37382/100000: episode: 587, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 11.029, mean reward: 0.649 [0.628, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.157, 10.100], loss: 0.002154, mae: 0.049608, mean_q: 1.194513
 37394/100000: episode: 588, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 8.218, mean reward: 0.685 [0.574, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.571, 10.451], loss: 0.001456, mae: 0.041293, mean_q: 1.199795
 37400/100000: episode: 589, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 4.345, mean reward: 0.724 [0.670, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.233, 10.100], loss: 0.001827, mae: 0.046350, mean_q: 1.183646
 37406/100000: episode: 590, duration: 0.044s, episode steps: 6, steps per second: 136, episode reward: 3.708, mean reward: 0.618 [0.598, 0.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.064, 10.100], loss: 0.002057, mae: 0.047538, mean_q: 1.189060
 37423/100000: episode: 591, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 12.567, mean reward: 0.739 [0.670, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.378, 10.100], loss: 0.002203, mae: 0.051231, mean_q: 1.189313
 37447/100000: episode: 592, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 14.633, mean reward: 0.610 [0.575, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.600, 10.100], loss: 0.002060, mae: 0.047634, mean_q: 1.195865
 37471/100000: episode: 593, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 13.976, mean reward: 0.582 [0.506, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.061, 10.100], loss: 0.001494, mae: 0.043065, mean_q: 1.200127
 37495/100000: episode: 594, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 16.587, mean reward: 0.691 [0.629, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.299, 10.100], loss: 0.001965, mae: 0.046093, mean_q: 1.194596
 37512/100000: episode: 595, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 10.745, mean reward: 0.632 [0.605, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.353, 10.100], loss: 0.001969, mae: 0.046710, mean_q: 1.199893
 37529/100000: episode: 596, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 12.462, mean reward: 0.733 [0.654, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.299, 10.100], loss: 0.002337, mae: 0.051927, mean_q: 1.195729
 37553/100000: episode: 597, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 15.478, mean reward: 0.645 [0.567, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.613, 10.100], loss: 0.001815, mae: 0.045519, mean_q: 1.198011
 37579/100000: episode: 598, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 19.966, mean reward: 0.768 [0.667, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.444, 10.100], loss: 0.002542, mae: 0.053607, mean_q: 1.186928
 37585/100000: episode: 599, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 4.102, mean reward: 0.684 [0.654, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.219, 10.100], loss: 0.001580, mae: 0.041311, mean_q: 1.182475
 37611/100000: episode: 600, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 17.833, mean reward: 0.686 [0.616, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.759, 10.100], loss: 0.001974, mae: 0.048087, mean_q: 1.200405
 37623/100000: episode: 601, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 8.142, mean reward: 0.679 [0.634, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.407], loss: 0.001970, mae: 0.046145, mean_q: 1.184467
 37640/100000: episode: 602, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 12.133, mean reward: 0.714 [0.661, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.545, 10.100], loss: 0.001709, mae: 0.044852, mean_q: 1.200263
 37664/100000: episode: 603, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 14.275, mean reward: 0.595 [0.507, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.183, 10.100], loss: 0.001805, mae: 0.044689, mean_q: 1.190225
 37670/100000: episode: 604, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 3.905, mean reward: 0.651 [0.624, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.179, 10.100], loss: 0.001861, mae: 0.048899, mean_q: 1.195750
 37698/100000: episode: 605, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 17.891, mean reward: 0.639 [0.520, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.079, 10.100], loss: 0.001890, mae: 0.046211, mean_q: 1.195916
 37711/100000: episode: 606, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 7.966, mean reward: 0.613 [0.582, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.299], loss: 0.001599, mae: 0.042488, mean_q: 1.195027
 37739/100000: episode: 607, duration: 0.157s, episode steps: 28, steps per second: 179, episode reward: 17.786, mean reward: 0.635 [0.516, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.450, 10.164], loss: 0.001601, mae: 0.042191, mean_q: 1.209546
 37763/100000: episode: 608, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 16.548, mean reward: 0.690 [0.637, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.240, 10.100], loss: 0.001709, mae: 0.045288, mean_q: 1.208048
 37775/100000: episode: 609, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 8.596, mean reward: 0.716 [0.676, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.552], loss: 0.002495, mae: 0.050533, mean_q: 1.193195
[Info] 2-TH LEVEL FOUND: 1.4945356845855713, Considering 10/90 traces
 37803/100000: episode: 610, duration: 5.336s, episode steps: 28, steps per second: 5, episode reward: 18.346, mean reward: 0.655 [0.583, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.659, 10.100], loss: 0.001886, mae: 0.046595, mean_q: 1.195017
 37814/100000: episode: 611, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 9.274, mean reward: 0.843 [0.806, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.472, 10.100], loss: 0.001927, mae: 0.045172, mean_q: 1.190767
 37834/100000: episode: 612, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 15.520, mean reward: 0.776 [0.699, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.348, 10.100], loss: 0.001932, mae: 0.046907, mean_q: 1.203299
 37856/100000: episode: 613, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 17.004, mean reward: 0.773 [0.697, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.374, 10.100], loss: 0.002138, mae: 0.049843, mean_q: 1.205906
 37875/100000: episode: 614, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 14.188, mean reward: 0.747 [0.696, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.166, 10.100], loss: 0.002020, mae: 0.047862, mean_q: 1.209554
 37894/100000: episode: 615, duration: 0.123s, episode steps: 19, steps per second: 154, episode reward: 14.047, mean reward: 0.739 [0.629, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.467, 10.100], loss: 0.001707, mae: 0.044945, mean_q: 1.209940
 37914/100000: episode: 616, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 14.625, mean reward: 0.731 [0.701, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.938, 10.100], loss: 0.001986, mae: 0.046670, mean_q: 1.201858
 37933/100000: episode: 617, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 12.690, mean reward: 0.668 [0.587, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.193, 10.100], loss: 0.001833, mae: 0.046708, mean_q: 1.209500
 37949/100000: episode: 618, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 12.001, mean reward: 0.750 [0.719, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.712, 10.100], loss: 0.002089, mae: 0.048911, mean_q: 1.190786
 37960/100000: episode: 619, duration: 0.088s, episode steps: 11, steps per second: 126, episode reward: 7.229, mean reward: 0.657 [0.574, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.177, 10.100], loss: 0.002141, mae: 0.050077, mean_q: 1.207234
 37983/100000: episode: 620, duration: 0.165s, episode steps: 23, steps per second: 140, episode reward: 16.249, mean reward: 0.706 [0.616, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.347, 10.100], loss: 0.001979, mae: 0.046752, mean_q: 1.200138
 37994/100000: episode: 621, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 8.232, mean reward: 0.748 [0.676, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.610, 10.100], loss: 0.001793, mae: 0.046685, mean_q: 1.218028
 38010/100000: episode: 622, duration: 0.109s, episode steps: 16, steps per second: 147, episode reward: 10.342, mean reward: 0.646 [0.583, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.299, 10.100], loss: 0.001830, mae: 0.045142, mean_q: 1.206654
 38021/100000: episode: 623, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 8.151, mean reward: 0.741 [0.709, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.517, 10.100], loss: 0.001408, mae: 0.041479, mean_q: 1.227505
 38041/100000: episode: 624, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 15.005, mean reward: 0.750 [0.696, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.328, 10.100], loss: 0.002042, mae: 0.048027, mean_q: 1.213935
 38052/100000: episode: 625, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 8.146, mean reward: 0.741 [0.701, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.319, 10.100], loss: 0.001711, mae: 0.043566, mean_q: 1.216871
 38072/100000: episode: 626, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 14.731, mean reward: 0.737 [0.698, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.861, 10.100], loss: 0.001908, mae: 0.046481, mean_q: 1.213155
 38083/100000: episode: 627, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 9.282, mean reward: 0.844 [0.793, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.678, 10.100], loss: 0.002005, mae: 0.049478, mean_q: 1.211850
 38101/100000: episode: 628, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 12.225, mean reward: 0.679 [0.536, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.500, 10.106], loss: 0.001760, mae: 0.046185, mean_q: 1.215777
 38120/100000: episode: 629, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 13.401, mean reward: 0.705 [0.623, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.391, 10.100], loss: 0.001925, mae: 0.046912, mean_q: 1.218398
 38139/100000: episode: 630, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 13.199, mean reward: 0.695 [0.574, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.548, 10.100], loss: 0.001933, mae: 0.045772, mean_q: 1.217623
 38157/100000: episode: 631, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 13.366, mean reward: 0.743 [0.678, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.210, 10.100], loss: 0.001690, mae: 0.044331, mean_q: 1.222287
 38176/100000: episode: 632, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 13.736, mean reward: 0.723 [0.655, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.027, 10.100], loss: 0.001720, mae: 0.043927, mean_q: 1.220970
 38192/100000: episode: 633, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 10.987, mean reward: 0.687 [0.655, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.600, 10.100], loss: 0.002061, mae: 0.050061, mean_q: 1.209798
 38203/100000: episode: 634, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 8.504, mean reward: 0.773 [0.735, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.296, 10.100], loss: 0.002765, mae: 0.056646, mean_q: 1.220373
 38222/100000: episode: 635, duration: 0.113s, episode steps: 19, steps per second: 169, episode reward: 13.850, mean reward: 0.729 [0.634, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.901, 10.100], loss: 0.002193, mae: 0.049683, mean_q: 1.214868
 38238/100000: episode: 636, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 11.183, mean reward: 0.699 [0.644, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.415, 10.100], loss: 0.002066, mae: 0.048406, mean_q: 1.218462
 38249/100000: episode: 637, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 7.938, mean reward: 0.722 [0.674, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.295, 10.100], loss: 0.002054, mae: 0.050126, mean_q: 1.224152
 38271/100000: episode: 638, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 16.159, mean reward: 0.734 [0.675, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.426, 10.100], loss: 0.001723, mae: 0.044657, mean_q: 1.225594
 38294/100000: episode: 639, duration: 0.159s, episode steps: 23, steps per second: 145, episode reward: 18.477, mean reward: 0.803 [0.756, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.296, 10.100], loss: 0.002033, mae: 0.047706, mean_q: 1.215980
 38305/100000: episode: 640, duration: 0.082s, episode steps: 11, steps per second: 134, episode reward: 8.095, mean reward: 0.736 [0.664, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.294, 10.100], loss: 0.001950, mae: 0.049097, mean_q: 1.240818
 38323/100000: episode: 641, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 12.573, mean reward: 0.699 [0.621, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.313, 10.100], loss: 0.001786, mae: 0.044198, mean_q: 1.214256
 38345/100000: episode: 642, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 16.896, mean reward: 0.768 [0.673, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.412, 10.100], loss: 0.001862, mae: 0.045812, mean_q: 1.233969
 38364/100000: episode: 643, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 12.931, mean reward: 0.681 [0.641, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.387, 10.100], loss: 0.001803, mae: 0.045186, mean_q: 1.222470
 38386/100000: episode: 644, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 16.900, mean reward: 0.768 [0.684, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.318, 10.100], loss: 0.002211, mae: 0.050656, mean_q: 1.213207
 38409/100000: episode: 645, duration: 0.134s, episode steps: 23, steps per second: 171, episode reward: 17.128, mean reward: 0.745 [0.667, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.778, 10.100], loss: 0.001897, mae: 0.045753, mean_q: 1.226561
 38420/100000: episode: 646, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 8.699, mean reward: 0.791 [0.759, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.209, 10.100], loss: 0.002256, mae: 0.049584, mean_q: 1.219628
 38442/100000: episode: 647, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 17.788, mean reward: 0.809 [0.657, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.617, 10.100], loss: 0.002136, mae: 0.051172, mean_q: 1.237105
 38453/100000: episode: 648, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 8.059, mean reward: 0.733 [0.659, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.255, 10.100], loss: 0.002411, mae: 0.051166, mean_q: 1.232325
 38473/100000: episode: 649, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 15.847, mean reward: 0.792 [0.744, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.247, 10.100], loss: 0.001933, mae: 0.047758, mean_q: 1.228250
 38484/100000: episode: 650, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 9.035, mean reward: 0.821 [0.756, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.401, 10.100], loss: 0.002137, mae: 0.048459, mean_q: 1.234719
 38495/100000: episode: 651, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 8.535, mean reward: 0.776 [0.712, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.382, 10.100], loss: 0.002350, mae: 0.049559, mean_q: 1.213153
 38506/100000: episode: 652, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 8.630, mean reward: 0.785 [0.751, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.364, 10.100], loss: 0.001876, mae: 0.046689, mean_q: 1.245861
 38525/100000: episode: 653, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 12.749, mean reward: 0.671 [0.606, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.276, 10.100], loss: 0.002007, mae: 0.046748, mean_q: 1.234464
 38544/100000: episode: 654, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 12.744, mean reward: 0.671 [0.619, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.333, 10.100], loss: 0.002230, mae: 0.051291, mean_q: 1.234844
 38562/100000: episode: 655, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 14.606, mean reward: 0.811 [0.702, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.673, 10.100], loss: 0.001891, mae: 0.047694, mean_q: 1.244635
 38582/100000: episode: 656, duration: 0.121s, episode steps: 20, steps per second: 166, episode reward: 15.701, mean reward: 0.785 [0.736, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.217, 10.100], loss: 0.001610, mae: 0.043349, mean_q: 1.240001
 38593/100000: episode: 657, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 8.412, mean reward: 0.765 [0.727, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.375, 10.100], loss: 0.002236, mae: 0.048834, mean_q: 1.240922
 38609/100000: episode: 658, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 11.457, mean reward: 0.716 [0.653, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.239, 10.100], loss: 0.001937, mae: 0.046727, mean_q: 1.247410
 38628/100000: episode: 659, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 14.197, mean reward: 0.747 [0.667, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.380, 10.100], loss: 0.002017, mae: 0.047870, mean_q: 1.241863
 38650/100000: episode: 660, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 14.954, mean reward: 0.680 [0.588, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.496, 10.100], loss: 0.001918, mae: 0.047910, mean_q: 1.244177
 38669/100000: episode: 661, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 11.928, mean reward: 0.628 [0.561, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.138, 10.100], loss: 0.002001, mae: 0.048021, mean_q: 1.229355
 38680/100000: episode: 662, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 8.065, mean reward: 0.733 [0.688, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.510, 10.100], loss: 0.002033, mae: 0.049426, mean_q: 1.238562
 38691/100000: episode: 663, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 7.750, mean reward: 0.705 [0.654, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.280, 10.100], loss: 0.001845, mae: 0.047451, mean_q: 1.240258
 38702/100000: episode: 664, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 7.511, mean reward: 0.683 [0.612, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.075, 10.100], loss: 0.002017, mae: 0.048518, mean_q: 1.255825
 38721/100000: episode: 665, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 13.750, mean reward: 0.724 [0.654, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.374, 10.100], loss: 0.002586, mae: 0.054657, mean_q: 1.224979
 38740/100000: episode: 666, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 13.212, mean reward: 0.695 [0.638, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.170, 10.100], loss: 0.002031, mae: 0.048221, mean_q: 1.242740
 38760/100000: episode: 667, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 13.842, mean reward: 0.692 [0.623, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.391, 10.100], loss: 0.001995, mae: 0.048008, mean_q: 1.258336
 38779/100000: episode: 668, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 13.875, mean reward: 0.730 [0.637, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.317, 10.100], loss: 0.002210, mae: 0.050454, mean_q: 1.254741
 38801/100000: episode: 669, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 15.727, mean reward: 0.715 [0.647, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.445, 10.100], loss: 0.001894, mae: 0.046688, mean_q: 1.254585
 38817/100000: episode: 670, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 11.789, mean reward: 0.737 [0.705, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.738, 10.100], loss: 0.002325, mae: 0.051165, mean_q: 1.260986
 38835/100000: episode: 671, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 13.761, mean reward: 0.764 [0.695, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.422, 10.100], loss: 0.002337, mae: 0.051687, mean_q: 1.239541
 38854/100000: episode: 672, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 13.144, mean reward: 0.692 [0.656, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.259, 10.100], loss: 0.001731, mae: 0.046290, mean_q: 1.233321
 38865/100000: episode: 673, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 7.750, mean reward: 0.705 [0.659, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.468, 10.100], loss: 0.002299, mae: 0.051141, mean_q: 1.244012
 38887/100000: episode: 674, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 16.359, mean reward: 0.744 [0.694, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.343, 10.100], loss: 0.002242, mae: 0.050689, mean_q: 1.237719
 38898/100000: episode: 675, duration: 0.077s, episode steps: 11, steps per second: 144, episode reward: 8.835, mean reward: 0.803 [0.764, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.488, 10.100], loss: 0.002218, mae: 0.053551, mean_q: 1.246112
 38909/100000: episode: 676, duration: 0.079s, episode steps: 11, steps per second: 139, episode reward: 7.625, mean reward: 0.693 [0.632, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.264, 10.100], loss: 0.002777, mae: 0.058360, mean_q: 1.229943
 38932/100000: episode: 677, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 18.735, mean reward: 0.815 [0.673, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.371, 10.100], loss: 0.002063, mae: 0.048264, mean_q: 1.244977
 38943/100000: episode: 678, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 8.478, mean reward: 0.771 [0.736, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.415, 10.100], loss: 0.001938, mae: 0.048271, mean_q: 1.265049
 38965/100000: episode: 679, duration: 0.138s, episode steps: 22, steps per second: 160, episode reward: 16.974, mean reward: 0.772 [0.733, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.541, 10.100], loss: 0.002047, mae: 0.047657, mean_q: 1.253906
 38976/100000: episode: 680, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 8.688, mean reward: 0.790 [0.734, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.478, 10.100], loss: 0.002590, mae: 0.054006, mean_q: 1.260238
 38987/100000: episode: 681, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 7.729, mean reward: 0.703 [0.660, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.705, 10.100], loss: 0.002257, mae: 0.051725, mean_q: 1.243428
 38998/100000: episode: 682, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 8.317, mean reward: 0.756 [0.708, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.538, 10.100], loss: 0.001999, mae: 0.047951, mean_q: 1.259394
 39009/100000: episode: 683, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 7.922, mean reward: 0.720 [0.665, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.421, 10.100], loss: 0.001881, mae: 0.046839, mean_q: 1.261945
 39020/100000: episode: 684, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 8.215, mean reward: 0.747 [0.718, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.630, 10.100], loss: 0.002373, mae: 0.056739, mean_q: 1.261429
 39039/100000: episode: 685, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 13.708, mean reward: 0.721 [0.663, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.371, 10.100], loss: 0.001750, mae: 0.046645, mean_q: 1.257105
 39062/100000: episode: 686, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 17.138, mean reward: 0.745 [0.617, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.118, 10.100], loss: 0.001610, mae: 0.041814, mean_q: 1.246737
 39073/100000: episode: 687, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 7.872, mean reward: 0.716 [0.677, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.288, 10.100], loss: 0.002029, mae: 0.047643, mean_q: 1.266944
 39091/100000: episode: 688, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 13.348, mean reward: 0.742 [0.586, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.760, 10.100], loss: 0.001989, mae: 0.048445, mean_q: 1.272501
 39110/100000: episode: 689, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 11.285, mean reward: 0.594 [0.529, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.499, 10.100], loss: 0.001744, mae: 0.045301, mean_q: 1.269887
 39128/100000: episode: 690, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 14.669, mean reward: 0.815 [0.706, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.197, 10.100], loss: 0.001815, mae: 0.045025, mean_q: 1.256389
 39139/100000: episode: 691, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 8.229, mean reward: 0.748 [0.695, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.889, 10.100], loss: 0.002127, mae: 0.049924, mean_q: 1.270664
 39158/100000: episode: 692, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 14.402, mean reward: 0.758 [0.644, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.543, 10.100], loss: 0.002302, mae: 0.050733, mean_q: 1.263531
 39177/100000: episode: 693, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 13.153, mean reward: 0.692 [0.620, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.297, 10.100], loss: 0.001870, mae: 0.047009, mean_q: 1.273713
 39188/100000: episode: 694, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 8.098, mean reward: 0.736 [0.691, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.573, 10.100], loss: 0.001457, mae: 0.040150, mean_q: 1.250303
 39207/100000: episode: 695, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 12.474, mean reward: 0.657 [0.610, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.488, 10.100], loss: 0.001590, mae: 0.043685, mean_q: 1.267547
 39226/100000: episode: 696, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 13.233, mean reward: 0.696 [0.631, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.240, 10.100], loss: 0.001767, mae: 0.044146, mean_q: 1.251373
 39242/100000: episode: 697, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 11.455, mean reward: 0.716 [0.676, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.391, 10.100], loss: 0.001830, mae: 0.046512, mean_q: 1.267585
 39264/100000: episode: 698, duration: 0.157s, episode steps: 22, steps per second: 140, episode reward: 15.083, mean reward: 0.686 [0.629, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.446, 10.100], loss: 0.001725, mae: 0.045912, mean_q: 1.276281
 39282/100000: episode: 699, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 13.355, mean reward: 0.742 [0.647, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.380, 10.100], loss: 0.001770, mae: 0.046240, mean_q: 1.255646
[Info] 3-TH LEVEL FOUND: 1.531646966934204, Considering 10/90 traces
 39301/100000: episode: 700, duration: 4.551s, episode steps: 19, steps per second: 4, episode reward: 12.539, mean reward: 0.660 [0.594, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.271, 10.100], loss: 0.001876, mae: 0.046889, mean_q: 1.264748
 39317/100000: episode: 701, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 11.370, mean reward: 0.711 [0.612, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.388, 10.100], loss: 0.001938, mae: 0.046943, mean_q: 1.273688
 39328/100000: episode: 702, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 9.634, mean reward: 0.876 [0.833, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.523, 10.100], loss: 0.001620, mae: 0.044071, mean_q: 1.264850
 39343/100000: episode: 703, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 10.774, mean reward: 0.718 [0.687, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.292, 10.100], loss: 0.001742, mae: 0.045443, mean_q: 1.270148
 39354/100000: episode: 704, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 9.257, mean reward: 0.842 [0.782, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.367, 10.100], loss: 0.002201, mae: 0.051457, mean_q: 1.286856
 39369/100000: episode: 705, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 11.168, mean reward: 0.745 [0.698, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.285, 10.100], loss: 0.001937, mae: 0.046616, mean_q: 1.262123
 39384/100000: episode: 706, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 11.087, mean reward: 0.739 [0.612, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.264, 10.100], loss: 0.002163, mae: 0.048811, mean_q: 1.259910
 39398/100000: episode: 707, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 11.662, mean reward: 0.833 [0.775, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.950, 10.100], loss: 0.002079, mae: 0.048680, mean_q: 1.280272
 39415/100000: episode: 708, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 13.325, mean reward: 0.784 [0.720, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.562, 10.100], loss: 0.001871, mae: 0.046942, mean_q: 1.275470
 39434/100000: episode: 709, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 16.047, mean reward: 0.845 [0.745, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.435, 10.100], loss: 0.001665, mae: 0.044948, mean_q: 1.292213
 39452/100000: episode: 710, duration: 0.113s, episode steps: 18, steps per second: 160, episode reward: 14.071, mean reward: 0.782 [0.712, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.371, 10.100], loss: 0.001757, mae: 0.046446, mean_q: 1.283487
 39468/100000: episode: 711, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 10.996, mean reward: 0.687 [0.580, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.137, 10.100], loss: 0.002065, mae: 0.049186, mean_q: 1.266782
 39479/100000: episode: 712, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 8.810, mean reward: 0.801 [0.749, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.689, 10.100], loss: 0.001734, mae: 0.045078, mean_q: 1.291074
 39494/100000: episode: 713, duration: 0.110s, episode steps: 15, steps per second: 136, episode reward: 11.800, mean reward: 0.787 [0.751, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.369, 10.100], loss: 0.002166, mae: 0.049535, mean_q: 1.269804
[Info] FALSIFICATION!
 39506/100000: episode: 714, duration: 0.247s, episode steps: 12, steps per second: 49, episode reward: 10.434, mean reward: 0.869 [0.789, 1.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.382, 10.020], loss: 0.002379, mae: 0.055783, mean_q: 1.276070
 39525/100000: episode: 715, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 14.936, mean reward: 0.786 [0.687, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.438, 10.100], loss: 0.001827, mae: 0.048127, mean_q: 1.275541
 39541/100000: episode: 716, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 12.105, mean reward: 0.757 [0.646, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.253, 10.100], loss: 0.002261, mae: 0.052394, mean_q: 1.299294
 39556/100000: episode: 717, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 11.520, mean reward: 0.768 [0.681, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.537, 10.100], loss: 0.002486, mae: 0.051255, mean_q: 1.273009
 39574/100000: episode: 718, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 13.952, mean reward: 0.775 [0.699, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.432, 10.100], loss: 0.001606, mae: 0.044903, mean_q: 1.276299
 39591/100000: episode: 719, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 13.016, mean reward: 0.766 [0.730, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.443, 10.100], loss: 0.002281, mae: 0.048766, mean_q: 1.294052
 39605/100000: episode: 720, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 11.381, mean reward: 0.813 [0.741, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.810, 10.100], loss: 0.001873, mae: 0.048151, mean_q: 1.279960
 39623/100000: episode: 721, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 13.948, mean reward: 0.775 [0.725, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.366, 10.100], loss: 0.001849, mae: 0.046790, mean_q: 1.288026
 39641/100000: episode: 722, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 13.271, mean reward: 0.737 [0.675, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.414, 10.100], loss: 0.002024, mae: 0.048829, mean_q: 1.271780
 39660/100000: episode: 723, duration: 0.123s, episode steps: 19, steps per second: 154, episode reward: 16.973, mean reward: 0.893 [0.777, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.441, 10.100], loss: 0.001444, mae: 0.042375, mean_q: 1.297623
 39679/100000: episode: 724, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 16.063, mean reward: 0.845 [0.787, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.008, 10.100], loss: 0.002190, mae: 0.047914, mean_q: 1.293076
 39697/100000: episode: 725, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 14.336, mean reward: 0.796 [0.727, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.485, 10.100], loss: 0.001894, mae: 0.043821, mean_q: 1.287156
 39712/100000: episode: 726, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 11.671, mean reward: 0.778 [0.718, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.417, 10.100], loss: 0.001621, mae: 0.044404, mean_q: 1.305639
 39730/100000: episode: 727, duration: 0.155s, episode steps: 18, steps per second: 116, episode reward: 14.758, mean reward: 0.820 [0.726, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.549, 10.100], loss: 0.001930, mae: 0.047533, mean_q: 1.285304
 39748/100000: episode: 728, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 15.408, mean reward: 0.856 [0.720, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.493, 10.100], loss: 0.001663, mae: 0.044898, mean_q: 1.307596
 39765/100000: episode: 729, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 12.692, mean reward: 0.747 [0.638, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.449, 10.100], loss: 0.001697, mae: 0.045796, mean_q: 1.309060
 39782/100000: episode: 730, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 12.542, mean reward: 0.738 [0.618, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.283, 10.100], loss: 0.001735, mae: 0.045835, mean_q: 1.306059
 39801/100000: episode: 731, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 14.963, mean reward: 0.788 [0.674, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.530, 10.100], loss: 0.001617, mae: 0.044138, mean_q: 1.295707
 39820/100000: episode: 732, duration: 0.120s, episode steps: 19, steps per second: 158, episode reward: 15.613, mean reward: 0.822 [0.785, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.416, 10.100], loss: 0.002057, mae: 0.049701, mean_q: 1.292850
 39831/100000: episode: 733, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 9.344, mean reward: 0.849 [0.775, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.589, 10.100], loss: 0.001485, mae: 0.042005, mean_q: 1.319815
 39846/100000: episode: 734, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 11.645, mean reward: 0.776 [0.680, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.414, 10.100], loss: 0.001845, mae: 0.045806, mean_q: 1.303521
 39863/100000: episode: 735, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 12.297, mean reward: 0.723 [0.672, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.718, 10.100], loss: 0.001726, mae: 0.046148, mean_q: 1.320313
 39881/100000: episode: 736, duration: 0.114s, episode steps: 18, steps per second: 157, episode reward: 15.028, mean reward: 0.835 [0.749, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.055, 10.100], loss: 0.001617, mae: 0.044025, mean_q: 1.295827
 39898/100000: episode: 737, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 13.103, mean reward: 0.771 [0.692, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.362, 10.100], loss: 0.001776, mae: 0.046212, mean_q: 1.315609
 39912/100000: episode: 738, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 9.893, mean reward: 0.707 [0.605, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.343, 10.100], loss: 0.001589, mae: 0.044106, mean_q: 1.309405
 39930/100000: episode: 739, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 13.795, mean reward: 0.766 [0.715, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.337, 10.100], loss: 0.002371, mae: 0.049983, mean_q: 1.300948
 39948/100000: episode: 740, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 14.797, mean reward: 0.822 [0.787, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.462, 10.100], loss: 0.001861, mae: 0.046670, mean_q: 1.301491
 39965/100000: episode: 741, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 13.126, mean reward: 0.772 [0.702, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.427, 10.100], loss: 0.001879, mae: 0.048407, mean_q: 1.310583
 39976/100000: episode: 742, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 8.477, mean reward: 0.771 [0.699, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.311, 10.100], loss: 0.001431, mae: 0.042563, mean_q: 1.311776
 39990/100000: episode: 743, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 11.534, mean reward: 0.824 [0.741, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.589, 10.100], loss: 0.001695, mae: 0.044419, mean_q: 1.309043
 40005/100000: episode: 744, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 12.036, mean reward: 0.802 [0.726, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.479, 10.100], loss: 0.001815, mae: 0.047365, mean_q: 1.312799
 40022/100000: episode: 745, duration: 0.147s, episode steps: 17, steps per second: 115, episode reward: 13.586, mean reward: 0.799 [0.754, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.447, 10.100], loss: 0.002268, mae: 0.049071, mean_q: 1.311150
 40041/100000: episode: 746, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 15.055, mean reward: 0.792 [0.717, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.284, 10.100], loss: 0.001706, mae: 0.044005, mean_q: 1.323938
 40060/100000: episode: 747, duration: 0.106s, episode steps: 19, steps per second: 178, episode reward: 15.468, mean reward: 0.814 [0.759, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.440, 10.100], loss: 0.001799, mae: 0.042487, mean_q: 1.311195
 40077/100000: episode: 748, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 11.733, mean reward: 0.690 [0.636, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.305, 10.100], loss: 0.001614, mae: 0.043846, mean_q: 1.319196
 40094/100000: episode: 749, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 12.945, mean reward: 0.761 [0.698, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.829, 10.100], loss: 0.002112, mae: 0.048127, mean_q: 1.315759
 40109/100000: episode: 750, duration: 0.105s, episode steps: 15, steps per second: 143, episode reward: 11.715, mean reward: 0.781 [0.726, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.366, 10.100], loss: 0.002122, mae: 0.049965, mean_q: 1.305984
 40127/100000: episode: 751, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 13.939, mean reward: 0.774 [0.672, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.232, 10.100], loss: 0.001891, mae: 0.048674, mean_q: 1.327093
 40146/100000: episode: 752, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 16.328, mean reward: 0.859 [0.789, 0.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.441, 10.100], loss: 0.001762, mae: 0.045576, mean_q: 1.328249
 40157/100000: episode: 753, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 10.032, mean reward: 0.912 [0.808, 0.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.706, 10.100], loss: 0.001676, mae: 0.045253, mean_q: 1.332036
 40176/100000: episode: 754, duration: 0.148s, episode steps: 19, steps per second: 128, episode reward: 14.640, mean reward: 0.771 [0.717, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.477, 10.100], loss: 0.001815, mae: 0.045719, mean_q: 1.322139
 40194/100000: episode: 755, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 13.351, mean reward: 0.742 [0.694, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.600, 10.100], loss: 0.001864, mae: 0.045923, mean_q: 1.327091
 40213/100000: episode: 756, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 15.395, mean reward: 0.810 [0.738, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.660, 10.100], loss: 0.001532, mae: 0.043976, mean_q: 1.339179
 40232/100000: episode: 757, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 15.803, mean reward: 0.832 [0.731, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.448, 10.100], loss: 0.001810, mae: 0.046886, mean_q: 1.314767
 40250/100000: episode: 758, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 15.571, mean reward: 0.865 [0.806, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.508, 10.100], loss: 0.001427, mae: 0.041233, mean_q: 1.331547
 40268/100000: episode: 759, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 13.329, mean reward: 0.741 [0.683, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.733, 10.100], loss: 0.001889, mae: 0.046149, mean_q: 1.347570
 40285/100000: episode: 760, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 13.428, mean reward: 0.790 [0.733, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.326, 10.100], loss: 0.001523, mae: 0.043229, mean_q: 1.332765
 40300/100000: episode: 761, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 11.844, mean reward: 0.790 [0.727, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.917, 10.100], loss: 0.001382, mae: 0.041093, mean_q: 1.344833
 40314/100000: episode: 762, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 10.642, mean reward: 0.760 [0.714, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.343, 10.100], loss: 0.001705, mae: 0.045916, mean_q: 1.321708
 40329/100000: episode: 763, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 11.447, mean reward: 0.763 [0.704, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.638, 10.100], loss: 0.001434, mae: 0.043165, mean_q: 1.326716
 40346/100000: episode: 764, duration: 0.117s, episode steps: 17, steps per second: 146, episode reward: 13.386, mean reward: 0.787 [0.721, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.357, 10.100], loss: 0.001372, mae: 0.042178, mean_q: 1.325473
 40362/100000: episode: 765, duration: 0.124s, episode steps: 16, steps per second: 129, episode reward: 12.286, mean reward: 0.768 [0.695, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.220, 10.100], loss: 0.001626, mae: 0.044502, mean_q: 1.334217
 40380/100000: episode: 766, duration: 0.107s, episode steps: 18, steps per second: 167, episode reward: 15.130, mean reward: 0.841 [0.741, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.452, 10.100], loss: 0.001458, mae: 0.042145, mean_q: 1.346860
 40398/100000: episode: 767, duration: 0.114s, episode steps: 18, steps per second: 157, episode reward: 14.468, mean reward: 0.804 [0.714, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.248, 10.100], loss: 0.001549, mae: 0.043918, mean_q: 1.326331
 40414/100000: episode: 768, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 12.621, mean reward: 0.789 [0.741, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.406, 10.100], loss: 0.001474, mae: 0.042188, mean_q: 1.334202
 40431/100000: episode: 769, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 11.884, mean reward: 0.699 [0.619, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.362, 10.100], loss: 0.001528, mae: 0.043761, mean_q: 1.332897
 40450/100000: episode: 770, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 13.814, mean reward: 0.727 [0.613, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.183, 10.100], loss: 0.001747, mae: 0.045618, mean_q: 1.354925
 40461/100000: episode: 771, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 8.843, mean reward: 0.804 [0.728, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.439, 10.100], loss: 0.001592, mae: 0.044401, mean_q: 1.341392
 40479/100000: episode: 772, duration: 0.176s, episode steps: 18, steps per second: 102, episode reward: 16.010, mean reward: 0.889 [0.762, 0.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.880, 10.100], loss: 0.002041, mae: 0.045720, mean_q: 1.341972
 40497/100000: episode: 773, duration: 0.127s, episode steps: 18, steps per second: 142, episode reward: 14.904, mean reward: 0.828 [0.737, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.455, 10.100], loss: 0.001705, mae: 0.046953, mean_q: 1.353137
 40515/100000: episode: 774, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 15.294, mean reward: 0.850 [0.784, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.513, 10.100], loss: 0.001689, mae: 0.045801, mean_q: 1.357795
 40533/100000: episode: 775, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 14.899, mean reward: 0.828 [0.776, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.615, 10.100], loss: 0.002377, mae: 0.053022, mean_q: 1.348742
 40551/100000: episode: 776, duration: 0.141s, episode steps: 18, steps per second: 128, episode reward: 13.953, mean reward: 0.775 [0.709, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.611, 10.100], loss: 0.001898, mae: 0.048444, mean_q: 1.351315
 40566/100000: episode: 777, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 12.257, mean reward: 0.817 [0.717, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.354, 10.100], loss: 0.002170, mae: 0.051278, mean_q: 1.353316
 40583/100000: episode: 778, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 11.730, mean reward: 0.690 [0.574, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.487, 10.100], loss: 0.001600, mae: 0.044645, mean_q: 1.353603
 40600/100000: episode: 779, duration: 0.118s, episode steps: 17, steps per second: 144, episode reward: 13.261, mean reward: 0.780 [0.727, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.691, 10.100], loss: 0.001438, mae: 0.042470, mean_q: 1.343202
 40617/100000: episode: 780, duration: 0.105s, episode steps: 17, steps per second: 161, episode reward: 12.970, mean reward: 0.763 [0.724, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.415, 10.100], loss: 0.001662, mae: 0.044576, mean_q: 1.355898
 40628/100000: episode: 781, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 9.093, mean reward: 0.827 [0.775, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.487, 10.100], loss: 0.001565, mae: 0.044068, mean_q: 1.362696
 40642/100000: episode: 782, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 11.313, mean reward: 0.808 [0.702, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.051, 10.100], loss: 0.001778, mae: 0.046112, mean_q: 1.367809
 40659/100000: episode: 783, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 13.860, mean reward: 0.815 [0.757, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.899, 10.100], loss: 0.001452, mae: 0.042744, mean_q: 1.349553
 40675/100000: episode: 784, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 10.833, mean reward: 0.677 [0.628, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.330, 10.100], loss: 0.001568, mae: 0.044295, mean_q: 1.361532
 40694/100000: episode: 785, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 15.547, mean reward: 0.818 [0.780, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.329, 10.100], loss: 0.002056, mae: 0.045291, mean_q: 1.365366
 40709/100000: episode: 786, duration: 0.203s, episode steps: 15, steps per second: 74, episode reward: 12.598, mean reward: 0.840 [0.736, 0.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.542, 10.100], loss: 0.002132, mae: 0.049865, mean_q: 1.368937
 40720/100000: episode: 787, duration: 0.134s, episode steps: 11, steps per second: 82, episode reward: 9.153, mean reward: 0.832 [0.778, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.385, 10.100], loss: 0.001482, mae: 0.042767, mean_q: 1.369510
 40737/100000: episode: 788, duration: 0.241s, episode steps: 17, steps per second: 70, episode reward: 14.003, mean reward: 0.824 [0.721, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.428, 10.100], loss: 0.002070, mae: 0.045447, mean_q: 1.363583
 40752/100000: episode: 789, duration: 0.153s, episode steps: 15, steps per second: 98, episode reward: 11.384, mean reward: 0.759 [0.721, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.407, 10.100], loss: 0.001335, mae: 0.040867, mean_q: 1.355347
[Info] Complete ISplit Iteration
[Info] Levels: [1.3455902, 1.4945357, 1.531647, 1.6990743]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.03]
[Info] Error Prob: 3.0000000000000008e-05

 40769/100000: episode: 790, duration: 8.244s, episode steps: 17, steps per second: 2, episode reward: 12.556, mean reward: 0.739 [0.658, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.306, 10.100], loss: 0.001613, mae: 0.044317, mean_q: 1.371954
 40869/100000: episode: 791, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 58.585, mean reward: 0.586 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.539, 10.098], loss: 0.001542, mae: 0.043718, mean_q: 1.366131
 40969/100000: episode: 792, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.507, mean reward: 0.585 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.932, 10.098], loss: 0.001641, mae: 0.044237, mean_q: 1.365934
 41069/100000: episode: 793, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.659, mean reward: 0.587 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.246, 10.229], loss: 0.001615, mae: 0.043787, mean_q: 1.359130
 41169/100000: episode: 794, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 57.100, mean reward: 0.571 [0.497, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.243, 10.343], loss: 0.002002, mae: 0.046774, mean_q: 1.359584
 41269/100000: episode: 795, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: 60.751, mean reward: 0.608 [0.499, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.810, 10.098], loss: 0.001764, mae: 0.045815, mean_q: 1.344987
 41369/100000: episode: 796, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 60.885, mean reward: 0.609 [0.511, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.990, 10.209], loss: 0.001452, mae: 0.042291, mean_q: 1.352064
 41469/100000: episode: 797, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 58.644, mean reward: 0.586 [0.499, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.174, 10.182], loss: 0.001390, mae: 0.041060, mean_q: 1.348282
 41569/100000: episode: 798, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.918, mean reward: 0.599 [0.498, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.542, 10.098], loss: 0.001531, mae: 0.042591, mean_q: 1.345102
 41669/100000: episode: 799, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 62.992, mean reward: 0.630 [0.504, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.844, 10.120], loss: 0.001831, mae: 0.045839, mean_q: 1.345338
 41769/100000: episode: 800, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 65.224, mean reward: 0.652 [0.515, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.344, 10.098], loss: 0.001723, mae: 0.045800, mean_q: 1.345188
 41869/100000: episode: 801, duration: 0.623s, episode steps: 100, steps per second: 161, episode reward: 59.419, mean reward: 0.594 [0.512, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.940, 10.195], loss: 0.001510, mae: 0.042995, mean_q: 1.337783
 41969/100000: episode: 802, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 59.308, mean reward: 0.593 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.602, 10.098], loss: 0.001630, mae: 0.044422, mean_q: 1.343312
 42069/100000: episode: 803, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 60.025, mean reward: 0.600 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.251, 10.502], loss: 0.001624, mae: 0.043960, mean_q: 1.341722
 42169/100000: episode: 804, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 61.949, mean reward: 0.619 [0.503, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.762, 10.098], loss: 0.001735, mae: 0.045490, mean_q: 1.349341
 42269/100000: episode: 805, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.549, mean reward: 0.575 [0.504, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.565, 10.211], loss: 0.001661, mae: 0.044393, mean_q: 1.335809
 42369/100000: episode: 806, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 58.130, mean reward: 0.581 [0.504, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.306, 10.185], loss: 0.002005, mae: 0.048967, mean_q: 1.337577
 42469/100000: episode: 807, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: 59.656, mean reward: 0.597 [0.507, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.480, 10.098], loss: 0.001673, mae: 0.043455, mean_q: 1.331420
 42569/100000: episode: 808, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 61.393, mean reward: 0.614 [0.514, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.816, 10.220], loss: 0.001773, mae: 0.046514, mean_q: 1.335855
 42669/100000: episode: 809, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: 59.997, mean reward: 0.600 [0.505, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.878, 10.098], loss: 0.001587, mae: 0.042593, mean_q: 1.333948
 42769/100000: episode: 810, duration: 0.988s, episode steps: 100, steps per second: 101, episode reward: 63.463, mean reward: 0.635 [0.518, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.795, 10.098], loss: 0.001648, mae: 0.045037, mean_q: 1.331988
 42869/100000: episode: 811, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 60.642, mean reward: 0.606 [0.508, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.043, 10.098], loss: 0.001762, mae: 0.045239, mean_q: 1.320659
 42969/100000: episode: 812, duration: 0.649s, episode steps: 100, steps per second: 154, episode reward: 59.332, mean reward: 0.593 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.645, 10.098], loss: 0.001804, mae: 0.046362, mean_q: 1.323912
 43069/100000: episode: 813, duration: 0.707s, episode steps: 100, steps per second: 142, episode reward: 59.122, mean reward: 0.591 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.997, 10.296], loss: 0.001546, mae: 0.042385, mean_q: 1.327371
 43169/100000: episode: 814, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: 62.069, mean reward: 0.621 [0.516, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.594, 10.246], loss: 0.001561, mae: 0.043589, mean_q: 1.311869
 43269/100000: episode: 815, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 60.009, mean reward: 0.600 [0.530, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.448, 10.147], loss: 0.001746, mae: 0.044414, mean_q: 1.309896
 43369/100000: episode: 816, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 57.138, mean reward: 0.571 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.739, 10.098], loss: 0.001685, mae: 0.044913, mean_q: 1.308913
 43469/100000: episode: 817, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 57.204, mean reward: 0.572 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.346, 10.255], loss: 0.001596, mae: 0.043495, mean_q: 1.303343
 43569/100000: episode: 818, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 60.772, mean reward: 0.608 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.913, 10.098], loss: 0.001985, mae: 0.045782, mean_q: 1.297328
 43669/100000: episode: 819, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: 58.004, mean reward: 0.580 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.622, 10.239], loss: 0.001661, mae: 0.044163, mean_q: 1.295944
 43769/100000: episode: 820, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 61.054, mean reward: 0.611 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.742, 10.413], loss: 0.001467, mae: 0.042246, mean_q: 1.286069
 43869/100000: episode: 821, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 64.142, mean reward: 0.641 [0.514, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.870, 10.375], loss: 0.001610, mae: 0.043309, mean_q: 1.285461
 43969/100000: episode: 822, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 57.543, mean reward: 0.575 [0.497, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.548, 10.210], loss: 0.001703, mae: 0.043537, mean_q: 1.282957
 44069/100000: episode: 823, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.907, mean reward: 0.609 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.363, 10.172], loss: 0.001593, mae: 0.043214, mean_q: 1.281537
 44169/100000: episode: 824, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.523, mean reward: 0.575 [0.510, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.906, 10.224], loss: 0.001724, mae: 0.043952, mean_q: 1.274792
 44269/100000: episode: 825, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 57.434, mean reward: 0.574 [0.507, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.388, 10.190], loss: 0.001596, mae: 0.042854, mean_q: 1.271852
 44369/100000: episode: 826, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 63.824, mean reward: 0.638 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.408, 10.098], loss: 0.001523, mae: 0.041796, mean_q: 1.261048
 44469/100000: episode: 827, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 61.038, mean reward: 0.610 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.701, 10.098], loss: 0.001483, mae: 0.042959, mean_q: 1.263599
 44569/100000: episode: 828, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.063, mean reward: 0.591 [0.500, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.424, 10.098], loss: 0.001481, mae: 0.042405, mean_q: 1.255203
 44669/100000: episode: 829, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.755, mean reward: 0.598 [0.516, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.567, 10.098], loss: 0.001504, mae: 0.042497, mean_q: 1.250921
 44769/100000: episode: 830, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 57.996, mean reward: 0.580 [0.499, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.581, 10.368], loss: 0.001556, mae: 0.043007, mean_q: 1.240700
 44869/100000: episode: 831, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.384, mean reward: 0.584 [0.505, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.984, 10.350], loss: 0.001435, mae: 0.041405, mean_q: 1.237331
 44969/100000: episode: 832, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 56.579, mean reward: 0.566 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.748, 10.098], loss: 0.001442, mae: 0.041910, mean_q: 1.228624
 45069/100000: episode: 833, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.415, mean reward: 0.574 [0.503, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.668, 10.178], loss: 0.001453, mae: 0.041977, mean_q: 1.225155
 45169/100000: episode: 834, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.319, mean reward: 0.573 [0.511, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.906, 10.098], loss: 0.001443, mae: 0.041723, mean_q: 1.216261
 45269/100000: episode: 835, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 62.861, mean reward: 0.629 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.667, 10.098], loss: 0.001370, mae: 0.040881, mean_q: 1.208234
 45369/100000: episode: 836, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.415, mean reward: 0.574 [0.509, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.544, 10.206], loss: 0.001504, mae: 0.042892, mean_q: 1.209767
 45469/100000: episode: 837, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 55.677, mean reward: 0.557 [0.503, 0.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.874, 10.098], loss: 0.001435, mae: 0.041270, mean_q: 1.201469
 45569/100000: episode: 838, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.663, mean reward: 0.587 [0.503, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.941, 10.229], loss: 0.001442, mae: 0.042036, mean_q: 1.195571
 45669/100000: episode: 839, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.136, mean reward: 0.581 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.915, 10.228], loss: 0.001494, mae: 0.042756, mean_q: 1.185466
 45769/100000: episode: 840, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.974, mean reward: 0.590 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.571, 10.274], loss: 0.001369, mae: 0.040664, mean_q: 1.180970
 45869/100000: episode: 841, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 59.679, mean reward: 0.597 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.242, 10.153], loss: 0.001297, mae: 0.039303, mean_q: 1.180057
 45969/100000: episode: 842, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.169, mean reward: 0.582 [0.510, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.661, 10.236], loss: 0.001370, mae: 0.040541, mean_q: 1.180205
 46069/100000: episode: 843, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.967, mean reward: 0.580 [0.500, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.631, 10.098], loss: 0.001359, mae: 0.040607, mean_q: 1.179639
 46169/100000: episode: 844, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 59.764, mean reward: 0.598 [0.499, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.263, 10.513], loss: 0.001400, mae: 0.041261, mean_q: 1.181501
 46269/100000: episode: 845, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.711, mean reward: 0.587 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.514, 10.098], loss: 0.001321, mae: 0.040193, mean_q: 1.175166
 46369/100000: episode: 846, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.247, mean reward: 0.592 [0.501, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.596, 10.411], loss: 0.001278, mae: 0.039681, mean_q: 1.178928
 46469/100000: episode: 847, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.418, mean reward: 0.594 [0.510, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.909, 10.268], loss: 0.001413, mae: 0.041349, mean_q: 1.180225
 46569/100000: episode: 848, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.874, mean reward: 0.589 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.750, 10.147], loss: 0.001284, mae: 0.039296, mean_q: 1.176369
 46669/100000: episode: 849, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.133, mean reward: 0.571 [0.513, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.829, 10.098], loss: 0.001389, mae: 0.040791, mean_q: 1.172765
 46769/100000: episode: 850, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.216, mean reward: 0.582 [0.504, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.306], loss: 0.001376, mae: 0.041173, mean_q: 1.175745
 46869/100000: episode: 851, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.280, mean reward: 0.573 [0.504, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.351, 10.177], loss: 0.001323, mae: 0.039783, mean_q: 1.172722
 46969/100000: episode: 852, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.757, mean reward: 0.598 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.276, 10.309], loss: 0.001370, mae: 0.041138, mean_q: 1.174873
 47069/100000: episode: 853, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.986, mean reward: 0.610 [0.507, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.863, 10.379], loss: 0.001304, mae: 0.039954, mean_q: 1.171995
 47169/100000: episode: 854, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.886, mean reward: 0.599 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.788, 10.312], loss: 0.001382, mae: 0.041024, mean_q: 1.172522
 47269/100000: episode: 855, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 60.283, mean reward: 0.603 [0.514, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.965, 10.188], loss: 0.001281, mae: 0.039492, mean_q: 1.171653
 47369/100000: episode: 856, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.653, mean reward: 0.597 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.415, 10.098], loss: 0.001327, mae: 0.040219, mean_q: 1.176437
 47469/100000: episode: 857, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.561, mean reward: 0.606 [0.511, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.337, 10.376], loss: 0.001288, mae: 0.039585, mean_q: 1.173820
 47569/100000: episode: 858, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 56.345, mean reward: 0.563 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.250, 10.162], loss: 0.001287, mae: 0.039341, mean_q: 1.170772
 47669/100000: episode: 859, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.946, mean reward: 0.589 [0.505, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.614, 10.312], loss: 0.001263, mae: 0.039292, mean_q: 1.171183
 47769/100000: episode: 860, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.867, mean reward: 0.579 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.559, 10.098], loss: 0.001385, mae: 0.040987, mean_q: 1.169375
 47869/100000: episode: 861, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.752, mean reward: 0.608 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.059, 10.108], loss: 0.001382, mae: 0.041134, mean_q: 1.171486
 47969/100000: episode: 862, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.053, mean reward: 0.591 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.512, 10.098], loss: 0.001347, mae: 0.040400, mean_q: 1.168942
 48069/100000: episode: 863, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 60.958, mean reward: 0.610 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.138, 10.351], loss: 0.001268, mae: 0.039168, mean_q: 1.169100
 48169/100000: episode: 864, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 59.734, mean reward: 0.597 [0.515, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.351, 10.277], loss: 0.001266, mae: 0.039534, mean_q: 1.168292
 48269/100000: episode: 865, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 63.870, mean reward: 0.639 [0.511, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.612, 10.310], loss: 0.001486, mae: 0.042434, mean_q: 1.170177
 48369/100000: episode: 866, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.222, mean reward: 0.592 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.485, 10.098], loss: 0.001366, mae: 0.040879, mean_q: 1.169361
 48469/100000: episode: 867, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.800, mean reward: 0.588 [0.509, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.893, 10.098], loss: 0.001331, mae: 0.040074, mean_q: 1.169694
 48569/100000: episode: 868, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.661, mean reward: 0.587 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.769, 10.138], loss: 0.001282, mae: 0.039812, mean_q: 1.170536
 48669/100000: episode: 869, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 63.221, mean reward: 0.632 [0.510, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.239, 10.289], loss: 0.001392, mae: 0.040639, mean_q: 1.174068
 48769/100000: episode: 870, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.684, mean reward: 0.597 [0.508, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.952, 10.143], loss: 0.001369, mae: 0.040722, mean_q: 1.172753
 48869/100000: episode: 871, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.690, mean reward: 0.597 [0.513, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.833, 10.223], loss: 0.001444, mae: 0.042152, mean_q: 1.172720
 48969/100000: episode: 872, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 61.634, mean reward: 0.616 [0.503, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.391, 10.401], loss: 0.001402, mae: 0.041341, mean_q: 1.167065
 49069/100000: episode: 873, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 56.493, mean reward: 0.565 [0.510, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.128, 10.203], loss: 0.001479, mae: 0.041988, mean_q: 1.169403
 49169/100000: episode: 874, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.644, mean reward: 0.586 [0.499, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.366, 10.098], loss: 0.001326, mae: 0.040083, mean_q: 1.170213
 49269/100000: episode: 875, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.152, mean reward: 0.592 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.376, 10.098], loss: 0.001487, mae: 0.042820, mean_q: 1.169084
 49369/100000: episode: 876, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 58.105, mean reward: 0.581 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.642, 10.098], loss: 0.001362, mae: 0.040519, mean_q: 1.168177
 49469/100000: episode: 877, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.088, mean reward: 0.571 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.015, 10.098], loss: 0.001420, mae: 0.041768, mean_q: 1.170992
 49569/100000: episode: 878, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 57.881, mean reward: 0.579 [0.505, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.864, 10.200], loss: 0.001427, mae: 0.041512, mean_q: 1.167902
 49669/100000: episode: 879, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 62.299, mean reward: 0.623 [0.512, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.334, 10.512], loss: 0.001374, mae: 0.040779, mean_q: 1.167354
 49769/100000: episode: 880, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.979, mean reward: 0.590 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.507, 10.215], loss: 0.001530, mae: 0.042812, mean_q: 1.167605
 49869/100000: episode: 881, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.523, mean reward: 0.605 [0.505, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.536, 10.098], loss: 0.001299, mae: 0.039770, mean_q: 1.168668
 49969/100000: episode: 882, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.230, mean reward: 0.592 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.490, 10.174], loss: 0.001393, mae: 0.040786, mean_q: 1.169167
 50069/100000: episode: 883, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 60.604, mean reward: 0.606 [0.505, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.931, 10.125], loss: 0.001449, mae: 0.041889, mean_q: 1.173618
 50169/100000: episode: 884, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 62.047, mean reward: 0.620 [0.518, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.028, 10.501], loss: 0.001485, mae: 0.042089, mean_q: 1.167409
 50269/100000: episode: 885, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 56.481, mean reward: 0.565 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.314, 10.098], loss: 0.001422, mae: 0.041200, mean_q: 1.169235
 50369/100000: episode: 886, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.705, mean reward: 0.597 [0.508, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.761, 10.098], loss: 0.001419, mae: 0.041164, mean_q: 1.170287
 50469/100000: episode: 887, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 59.305, mean reward: 0.593 [0.506, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.665, 10.351], loss: 0.001533, mae: 0.042523, mean_q: 1.171381
 50569/100000: episode: 888, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 61.916, mean reward: 0.619 [0.505, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.785, 10.239], loss: 0.001552, mae: 0.042919, mean_q: 1.177068
 50669/100000: episode: 889, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.669, mean reward: 0.597 [0.505, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.539, 10.204], loss: 0.001493, mae: 0.042921, mean_q: 1.176390
[Info] 1-TH LEVEL FOUND: 1.3918492794036865, Considering 10/90 traces
 50769/100000: episode: 890, duration: 4.912s, episode steps: 100, steps per second: 20, episode reward: 58.974, mean reward: 0.590 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.199, 10.303], loss: 0.001501, mae: 0.043090, mean_q: 1.176750
 50814/100000: episode: 891, duration: 0.294s, episode steps: 45, steps per second: 153, episode reward: 28.083, mean reward: 0.624 [0.504, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.294, 10.100], loss: 0.001438, mae: 0.041533, mean_q: 1.177994
 50861/100000: episode: 892, duration: 0.276s, episode steps: 47, steps per second: 170, episode reward: 28.084, mean reward: 0.598 [0.498, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.641, 10.120], loss: 0.001536, mae: 0.043299, mean_q: 1.173848
 50884/100000: episode: 893, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 17.351, mean reward: 0.754 [0.673, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.499], loss: 0.001330, mae: 0.039984, mean_q: 1.172937
 50900/100000: episode: 894, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 11.427, mean reward: 0.714 [0.614, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.619, 10.443], loss: 0.001682, mae: 0.044369, mean_q: 1.179290
 50923/100000: episode: 895, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 15.321, mean reward: 0.666 [0.593, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.269, 10.424], loss: 0.001421, mae: 0.042325, mean_q: 1.181365
 50965/100000: episode: 896, duration: 0.224s, episode steps: 42, steps per second: 187, episode reward: 28.737, mean reward: 0.684 [0.599, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.558, 10.334], loss: 0.001638, mae: 0.044586, mean_q: 1.178314
 50981/100000: episode: 897, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 11.155, mean reward: 0.697 [0.624, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.219, 10.388], loss: 0.001504, mae: 0.043081, mean_q: 1.184520
 51027/100000: episode: 898, duration: 0.277s, episode steps: 46, steps per second: 166, episode reward: 31.657, mean reward: 0.688 [0.618, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.627, 10.404], loss: 0.001483, mae: 0.042659, mean_q: 1.177929
 51072/100000: episode: 899, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 26.961, mean reward: 0.599 [0.526, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.384, 10.100], loss: 0.001581, mae: 0.043809, mean_q: 1.184273
 51117/100000: episode: 900, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 27.906, mean reward: 0.620 [0.536, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.833, 10.371], loss: 0.001477, mae: 0.041944, mean_q: 1.183632
 51134/100000: episode: 901, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 11.380, mean reward: 0.669 [0.617, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.569, 10.408], loss: 0.001653, mae: 0.044625, mean_q: 1.181785
 51155/100000: episode: 902, duration: 0.118s, episode steps: 21, steps per second: 179, episode reward: 14.850, mean reward: 0.707 [0.613, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.544, 10.417], loss: 0.001549, mae: 0.043747, mean_q: 1.185962
 51197/100000: episode: 903, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 27.399, mean reward: 0.652 [0.567, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.651, 10.247], loss: 0.001608, mae: 0.044768, mean_q: 1.192343
 51220/100000: episode: 904, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 15.478, mean reward: 0.673 [0.538, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.287, 10.221], loss: 0.001438, mae: 0.041598, mean_q: 1.184461
 51243/100000: episode: 905, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 16.374, mean reward: 0.712 [0.621, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.408, 10.491], loss: 0.001706, mae: 0.044322, mean_q: 1.185534
 51260/100000: episode: 906, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 11.969, mean reward: 0.704 [0.621, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.703, 10.352], loss: 0.001783, mae: 0.045251, mean_q: 1.186870
 51281/100000: episode: 907, duration: 0.127s, episode steps: 21, steps per second: 166, episode reward: 15.084, mean reward: 0.718 [0.624, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.416], loss: 0.001484, mae: 0.043503, mean_q: 1.186753
 51298/100000: episode: 908, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 11.565, mean reward: 0.680 [0.629, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.429, 10.334], loss: 0.001585, mae: 0.043182, mean_q: 1.190036
 51344/100000: episode: 909, duration: 0.267s, episode steps: 46, steps per second: 172, episode reward: 31.552, mean reward: 0.686 [0.550, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.267, 10.263], loss: 0.001565, mae: 0.043122, mean_q: 1.186084
 51367/100000: episode: 910, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 15.982, mean reward: 0.695 [0.604, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.360], loss: 0.001817, mae: 0.046470, mean_q: 1.187360
 51412/100000: episode: 911, duration: 0.240s, episode steps: 45, steps per second: 188, episode reward: 29.939, mean reward: 0.665 [0.559, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.916, 10.408], loss: 0.001483, mae: 0.042710, mean_q: 1.188083
 51454/100000: episode: 912, duration: 0.248s, episode steps: 42, steps per second: 170, episode reward: 27.414, mean reward: 0.653 [0.548, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.277, 10.284], loss: 0.001737, mae: 0.045413, mean_q: 1.185650
 51500/100000: episode: 913, duration: 0.249s, episode steps: 46, steps per second: 184, episode reward: 28.402, mean reward: 0.617 [0.516, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.525, 10.100], loss: 0.001572, mae: 0.043966, mean_q: 1.191967
 51523/100000: episode: 914, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 14.759, mean reward: 0.642 [0.589, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.329], loss: 0.002066, mae: 0.048542, mean_q: 1.181658
 51537/100000: episode: 915, duration: 0.093s, episode steps: 14, steps per second: 150, episode reward: 9.887, mean reward: 0.706 [0.589, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.333], loss: 0.001935, mae: 0.046813, mean_q: 1.184192
 51581/100000: episode: 916, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 26.752, mean reward: 0.608 [0.531, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.184, 10.100], loss: 0.001651, mae: 0.043173, mean_q: 1.189125
 51626/100000: episode: 917, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 29.477, mean reward: 0.655 [0.541, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.558, 10.342], loss: 0.001527, mae: 0.042936, mean_q: 1.190192
 51640/100000: episode: 918, duration: 0.084s, episode steps: 14, steps per second: 168, episode reward: 10.210, mean reward: 0.729 [0.638, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.429], loss: 0.001691, mae: 0.044712, mean_q: 1.194847
 51684/100000: episode: 919, duration: 0.258s, episode steps: 44, steps per second: 171, episode reward: 28.386, mean reward: 0.645 [0.576, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.855, 10.311], loss: 0.001623, mae: 0.043829, mean_q: 1.191964
 51729/100000: episode: 920, duration: 0.257s, episode steps: 45, steps per second: 175, episode reward: 29.722, mean reward: 0.660 [0.522, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.748, 10.107], loss: 0.001557, mae: 0.043839, mean_q: 1.199574
 51745/100000: episode: 921, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 10.624, mean reward: 0.664 [0.606, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.426], loss: 0.001618, mae: 0.044626, mean_q: 1.195524
 51790/100000: episode: 922, duration: 0.266s, episode steps: 45, steps per second: 169, episode reward: 27.539, mean reward: 0.612 [0.513, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.441, 10.148], loss: 0.001634, mae: 0.043808, mean_q: 1.194851
 51813/100000: episode: 923, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 17.241, mean reward: 0.750 [0.626, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.399, 10.442], loss: 0.001631, mae: 0.045259, mean_q: 1.201686
 51836/100000: episode: 924, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 16.542, mean reward: 0.719 [0.654, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.059, 10.550], loss: 0.001487, mae: 0.043110, mean_q: 1.196077
 51880/100000: episode: 925, duration: 0.252s, episode steps: 44, steps per second: 174, episode reward: 31.207, mean reward: 0.709 [0.582, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.733, 10.418], loss: 0.001782, mae: 0.046056, mean_q: 1.192432
 51894/100000: episode: 926, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 10.748, mean reward: 0.768 [0.710, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.559], loss: 0.001794, mae: 0.044556, mean_q: 1.195203
 51941/100000: episode: 927, duration: 0.231s, episode steps: 47, steps per second: 203, episode reward: 34.163, mean reward: 0.727 [0.617, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.395, 10.479], loss: 0.001639, mae: 0.044976, mean_q: 1.207122
 51955/100000: episode: 928, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 10.985, mean reward: 0.785 [0.749, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.580], loss: 0.002125, mae: 0.048141, mean_q: 1.201176
 51972/100000: episode: 929, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 11.484, mean reward: 0.676 [0.614, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.271, 10.479], loss: 0.001656, mae: 0.041993, mean_q: 1.198903
 51988/100000: episode: 930, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.725, mean reward: 0.733 [0.663, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.099, 10.446], loss: 0.001836, mae: 0.046864, mean_q: 1.200026
 52030/100000: episode: 931, duration: 0.224s, episode steps: 42, steps per second: 187, episode reward: 26.429, mean reward: 0.629 [0.505, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.532, 10.228], loss: 0.001770, mae: 0.046526, mean_q: 1.211760
 52076/100000: episode: 932, duration: 0.248s, episode steps: 46, steps per second: 186, episode reward: 28.946, mean reward: 0.629 [0.573, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.267, 10.302], loss: 0.001772, mae: 0.045859, mean_q: 1.211286
 52120/100000: episode: 933, duration: 0.250s, episode steps: 44, steps per second: 176, episode reward: 29.121, mean reward: 0.662 [0.590, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.069, 10.373], loss: 0.001632, mae: 0.044016, mean_q: 1.203750
 52136/100000: episode: 934, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 11.167, mean reward: 0.698 [0.649, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.150, 10.488], loss: 0.001616, mae: 0.043268, mean_q: 1.202899
 52183/100000: episode: 935, duration: 0.247s, episode steps: 47, steps per second: 190, episode reward: 28.569, mean reward: 0.608 [0.507, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.755, 10.100], loss: 0.001638, mae: 0.043423, mean_q: 1.211414
 52204/100000: episode: 936, duration: 0.138s, episode steps: 21, steps per second: 153, episode reward: 14.901, mean reward: 0.710 [0.630, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.587, 10.479], loss: 0.001678, mae: 0.044975, mean_q: 1.209201
 52218/100000: episode: 937, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.515, mean reward: 0.751 [0.632, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.915, 10.411], loss: 0.001738, mae: 0.045881, mean_q: 1.216343
 52234/100000: episode: 938, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 9.915, mean reward: 0.620 [0.545, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.203], loss: 0.001682, mae: 0.045289, mean_q: 1.213973
 52276/100000: episode: 939, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 29.919, mean reward: 0.712 [0.649, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-1.116, 10.392], loss: 0.001716, mae: 0.044702, mean_q: 1.211090
 52318/100000: episode: 940, duration: 0.225s, episode steps: 42, steps per second: 187, episode reward: 28.393, mean reward: 0.676 [0.588, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.152, 10.581], loss: 0.001768, mae: 0.044960, mean_q: 1.212938
 52339/100000: episode: 941, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 14.599, mean reward: 0.695 [0.609, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.223, 10.273], loss: 0.001988, mae: 0.047428, mean_q: 1.208997
 52355/100000: episode: 942, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 12.015, mean reward: 0.751 [0.669, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.673], loss: 0.001489, mae: 0.041747, mean_q: 1.215967
 52378/100000: episode: 943, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 15.285, mean reward: 0.665 [0.578, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.480, 10.303], loss: 0.001420, mae: 0.041283, mean_q: 1.218139
 52424/100000: episode: 944, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 29.266, mean reward: 0.636 [0.529, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.452, 10.193], loss: 0.001866, mae: 0.046300, mean_q: 1.214799
 52441/100000: episode: 945, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 12.721, mean reward: 0.748 [0.682, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.462, 10.599], loss: 0.001566, mae: 0.042723, mean_q: 1.219779
 52488/100000: episode: 946, duration: 0.255s, episode steps: 47, steps per second: 184, episode reward: 29.255, mean reward: 0.622 [0.545, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-1.328, 10.100], loss: 0.001775, mae: 0.044635, mean_q: 1.214676
 52504/100000: episode: 947, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 11.132, mean reward: 0.696 [0.671, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.468], loss: 0.001698, mae: 0.043828, mean_q: 1.226147
 52546/100000: episode: 948, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 26.679, mean reward: 0.635 [0.506, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.149, 10.100], loss: 0.001542, mae: 0.042636, mean_q: 1.212638
 52593/100000: episode: 949, duration: 0.263s, episode steps: 47, steps per second: 179, episode reward: 29.901, mean reward: 0.636 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-1.117, 10.170], loss: 0.001593, mae: 0.043734, mean_q: 1.220114
 52638/100000: episode: 950, duration: 0.249s, episode steps: 45, steps per second: 181, episode reward: 26.234, mean reward: 0.583 [0.512, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.390, 10.230], loss: 0.001777, mae: 0.045743, mean_q: 1.213177
 52682/100000: episode: 951, duration: 0.239s, episode steps: 44, steps per second: 184, episode reward: 30.996, mean reward: 0.704 [0.622, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.431, 10.440], loss: 0.002032, mae: 0.047982, mean_q: 1.223281
 52696/100000: episode: 952, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 10.124, mean reward: 0.723 [0.655, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.549], loss: 0.001990, mae: 0.048040, mean_q: 1.216628
 52738/100000: episode: 953, duration: 0.256s, episode steps: 42, steps per second: 164, episode reward: 32.335, mean reward: 0.770 [0.645, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.144, 10.514], loss: 0.001656, mae: 0.043888, mean_q: 1.221251
 52783/100000: episode: 954, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 27.373, mean reward: 0.608 [0.547, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.450, 10.266], loss: 0.001749, mae: 0.044431, mean_q: 1.226687
 52800/100000: episode: 955, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 11.502, mean reward: 0.677 [0.603, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.901, 10.335], loss: 0.001710, mae: 0.044950, mean_q: 1.224015
 52817/100000: episode: 956, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 11.464, mean reward: 0.674 [0.622, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.309], loss: 0.001855, mae: 0.046035, mean_q: 1.235663
 52833/100000: episode: 957, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 12.362, mean reward: 0.773 [0.730, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.368, 10.490], loss: 0.001687, mae: 0.044335, mean_q: 1.224294
 52850/100000: episode: 958, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 11.635, mean reward: 0.684 [0.641, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.428], loss: 0.001920, mae: 0.046752, mean_q: 1.236289
 52864/100000: episode: 959, duration: 0.088s, episode steps: 14, steps per second: 158, episode reward: 9.457, mean reward: 0.676 [0.620, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.610, 10.406], loss: 0.001741, mae: 0.045418, mean_q: 1.220930
 52906/100000: episode: 960, duration: 0.230s, episode steps: 42, steps per second: 182, episode reward: 26.470, mean reward: 0.630 [0.534, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.741, 10.263], loss: 0.001691, mae: 0.042896, mean_q: 1.225249
 52948/100000: episode: 961, duration: 0.225s, episode steps: 42, steps per second: 186, episode reward: 29.424, mean reward: 0.701 [0.627, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.916, 10.497], loss: 0.001966, mae: 0.048690, mean_q: 1.224888
 52993/100000: episode: 962, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 26.567, mean reward: 0.590 [0.510, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.593, 10.215], loss: 0.001828, mae: 0.045353, mean_q: 1.225817
 53038/100000: episode: 963, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 29.872, mean reward: 0.664 [0.543, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.533, 10.241], loss: 0.001941, mae: 0.046775, mean_q: 1.232477
 53055/100000: episode: 964, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 10.578, mean reward: 0.622 [0.559, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.083, 10.300], loss: 0.001554, mae: 0.041650, mean_q: 1.231337
 53078/100000: episode: 965, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 15.307, mean reward: 0.666 [0.590, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.420], loss: 0.001817, mae: 0.046099, mean_q: 1.224163
 53101/100000: episode: 966, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 15.947, mean reward: 0.693 [0.598, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.316, 10.535], loss: 0.001749, mae: 0.045969, mean_q: 1.226172
 53118/100000: episode: 967, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 11.765, mean reward: 0.692 [0.626, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.810, 10.366], loss: 0.001489, mae: 0.041786, mean_q: 1.224948
 53164/100000: episode: 968, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 30.715, mean reward: 0.668 [0.583, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.395, 10.405], loss: 0.001623, mae: 0.043777, mean_q: 1.223482
 53185/100000: episode: 969, duration: 0.110s, episode steps: 21, steps per second: 192, episode reward: 15.226, mean reward: 0.725 [0.648, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.484, 10.459], loss: 0.001763, mae: 0.046453, mean_q: 1.229392
 53206/100000: episode: 970, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 14.654, mean reward: 0.698 [0.628, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.365], loss: 0.001939, mae: 0.045614, mean_q: 1.234128
 53223/100000: episode: 971, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 12.263, mean reward: 0.721 [0.632, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.744, 10.458], loss: 0.001407, mae: 0.040492, mean_q: 1.231441
 53269/100000: episode: 972, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 32.697, mean reward: 0.711 [0.564, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.335, 10.272], loss: 0.001694, mae: 0.044555, mean_q: 1.232273
 53314/100000: episode: 973, duration: 0.252s, episode steps: 45, steps per second: 179, episode reward: 27.218, mean reward: 0.605 [0.520, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.433, 10.100], loss: 0.001688, mae: 0.044475, mean_q: 1.235573
 53360/100000: episode: 974, duration: 0.252s, episode steps: 46, steps per second: 183, episode reward: 26.723, mean reward: 0.581 [0.512, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.259, 10.262], loss: 0.001562, mae: 0.042608, mean_q: 1.230873
 53406/100000: episode: 975, duration: 0.252s, episode steps: 46, steps per second: 182, episode reward: 28.308, mean reward: 0.615 [0.508, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.942, 10.100], loss: 0.001800, mae: 0.045162, mean_q: 1.236829
 53422/100000: episode: 976, duration: 0.106s, episode steps: 16, steps per second: 150, episode reward: 10.959, mean reward: 0.685 [0.634, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.375], loss: 0.001786, mae: 0.044981, mean_q: 1.230202
 53445/100000: episode: 977, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 15.786, mean reward: 0.686 [0.645, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.426], loss: 0.001860, mae: 0.045846, mean_q: 1.227303
 53491/100000: episode: 978, duration: 0.259s, episode steps: 46, steps per second: 177, episode reward: 30.487, mean reward: 0.663 [0.532, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.766, 10.298], loss: 0.001612, mae: 0.042908, mean_q: 1.236490
 53508/100000: episode: 979, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 11.965, mean reward: 0.704 [0.642, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.564], loss: 0.001604, mae: 0.043106, mean_q: 1.233904
[Info] 2-TH LEVEL FOUND: 1.4829527139663696, Considering 10/90 traces
 53524/100000: episode: 980, duration: 4.362s, episode steps: 16, steps per second: 4, episode reward: 11.298, mean reward: 0.706 [0.655, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.362, 10.427], loss: 0.001404, mae: 0.040395, mean_q: 1.238157
 53539/100000: episode: 981, duration: 0.099s, episode steps: 15, steps per second: 152, episode reward: 11.446, mean reward: 0.763 [0.667, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.156, 10.501], loss: 0.001441, mae: 0.041562, mean_q: 1.233553
 53571/100000: episode: 982, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 23.702, mean reward: 0.741 [0.557, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.734, 10.245], loss: 0.001541, mae: 0.042687, mean_q: 1.245707
 53602/100000: episode: 983, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 20.280, mean reward: 0.654 [0.565, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.428, 10.440], loss: 0.001535, mae: 0.041753, mean_q: 1.233824
 53644/100000: episode: 984, duration: 0.230s, episode steps: 42, steps per second: 183, episode reward: 28.730, mean reward: 0.684 [0.574, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-1.023, 10.346], loss: 0.001715, mae: 0.045145, mean_q: 1.238852
 53655/100000: episode: 985, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 8.156, mean reward: 0.741 [0.696, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.406], loss: 0.001409, mae: 0.040337, mean_q: 1.231789
 53670/100000: episode: 986, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 12.213, mean reward: 0.814 [0.763, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.545], loss: 0.002075, mae: 0.049932, mean_q: 1.236651
 53702/100000: episode: 987, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 21.945, mean reward: 0.686 [0.591, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.306, 10.278], loss: 0.001759, mae: 0.044882, mean_q: 1.240627
 53733/100000: episode: 988, duration: 0.186s, episode steps: 31, steps per second: 166, episode reward: 21.507, mean reward: 0.694 [0.589, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.484, 10.328], loss: 0.001536, mae: 0.043283, mean_q: 1.245797
 53761/100000: episode: 989, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 21.481, mean reward: 0.767 [0.712, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.331, 10.517], loss: 0.001857, mae: 0.047144, mean_q: 1.248709
 53803/100000: episode: 990, duration: 0.223s, episode steps: 42, steps per second: 188, episode reward: 29.980, mean reward: 0.714 [0.594, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.346, 10.398], loss: 0.001990, mae: 0.047980, mean_q: 1.247640
 53845/100000: episode: 991, duration: 0.217s, episode steps: 42, steps per second: 193, episode reward: 26.159, mean reward: 0.623 [0.525, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.601, 10.100], loss: 0.001703, mae: 0.044910, mean_q: 1.254997
 53877/100000: episode: 992, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 25.156, mean reward: 0.786 [0.680, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.153, 10.406], loss: 0.001782, mae: 0.045622, mean_q: 1.243853
 53894/100000: episode: 993, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 13.903, mean reward: 0.818 [0.680, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-1.317, 10.691], loss: 0.001782, mae: 0.044799, mean_q: 1.250385
 53926/100000: episode: 994, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 22.569, mean reward: 0.705 [0.597, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.470, 10.343], loss: 0.001715, mae: 0.044105, mean_q: 1.241580
 53958/100000: episode: 995, duration: 0.198s, episode steps: 32, steps per second: 162, episode reward: 23.797, mean reward: 0.744 [0.617, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.697, 10.345], loss: 0.001544, mae: 0.041992, mean_q: 1.253634
 54000/100000: episode: 996, duration: 0.242s, episode steps: 42, steps per second: 173, episode reward: 29.085, mean reward: 0.692 [0.602, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.118, 10.419], loss: 0.001656, mae: 0.044083, mean_q: 1.251238
 54032/100000: episode: 997, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 25.235, mean reward: 0.789 [0.688, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.405, 10.471], loss: 0.001742, mae: 0.045249, mean_q: 1.246735
 54064/100000: episode: 998, duration: 0.190s, episode steps: 32, steps per second: 168, episode reward: 25.063, mean reward: 0.783 [0.705, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.784, 10.389], loss: 0.001575, mae: 0.043035, mean_q: 1.256787
 54095/100000: episode: 999, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 23.539, mean reward: 0.759 [0.709, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.401, 10.452], loss: 0.001707, mae: 0.044989, mean_q: 1.256921
 54126/100000: episode: 1000, duration: 0.169s, episode steps: 31, steps per second: 184, episode reward: 19.743, mean reward: 0.637 [0.526, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.419, 10.204], loss: 0.001554, mae: 0.042374, mean_q: 1.265769
 54157/100000: episode: 1001, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 20.452, mean reward: 0.660 [0.573, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.769, 10.148], loss: 0.001722, mae: 0.043982, mean_q: 1.259873
 54189/100000: episode: 1002, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 23.927, mean reward: 0.748 [0.603, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.989, 10.304], loss: 0.002113, mae: 0.049560, mean_q: 1.270362
 54221/100000: episode: 1003, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 28.424, mean reward: 0.888 [0.770, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.154, 10.707], loss: 0.001903, mae: 0.046278, mean_q: 1.268268
 54253/100000: episode: 1004, duration: 0.188s, episode steps: 32, steps per second: 170, episode reward: 23.630, mean reward: 0.738 [0.588, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.278, 10.342], loss: 0.001492, mae: 0.042471, mean_q: 1.272758
 54295/100000: episode: 1005, duration: 0.230s, episode steps: 42, steps per second: 183, episode reward: 25.987, mean reward: 0.619 [0.508, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.562, 10.207], loss: 0.001658, mae: 0.043310, mean_q: 1.265414
 54327/100000: episode: 1006, duration: 0.190s, episode steps: 32, steps per second: 168, episode reward: 24.059, mean reward: 0.752 [0.699, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.883, 10.466], loss: 0.001679, mae: 0.044853, mean_q: 1.271337
 54359/100000: episode: 1007, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 24.003, mean reward: 0.750 [0.680, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.496], loss: 0.001491, mae: 0.042354, mean_q: 1.276416
 54391/100000: episode: 1008, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 23.621, mean reward: 0.738 [0.613, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.035, 10.353], loss: 0.001543, mae: 0.042276, mean_q: 1.276340
 54406/100000: episode: 1009, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 12.589, mean reward: 0.839 [0.741, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.345, 10.686], loss: 0.001594, mae: 0.043414, mean_q: 1.288950
 54421/100000: episode: 1010, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 11.435, mean reward: 0.762 [0.728, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.330, 10.403], loss: 0.001713, mae: 0.046411, mean_q: 1.268711
 54452/100000: episode: 1011, duration: 0.177s, episode steps: 31, steps per second: 176, episode reward: 22.666, mean reward: 0.731 [0.642, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.407, 10.432], loss: 0.001932, mae: 0.048022, mean_q: 1.275792
 54483/100000: episode: 1012, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 21.743, mean reward: 0.701 [0.606, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.169, 10.420], loss: 0.001626, mae: 0.043988, mean_q: 1.271890
 54514/100000: episode: 1013, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 22.645, mean reward: 0.730 [0.676, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.990, 10.557], loss: 0.001729, mae: 0.044788, mean_q: 1.293459
[Info] FALSIFICATION!
 54548/100000: episode: 1014, duration: 0.370s, episode steps: 34, steps per second: 92, episode reward: 25.598, mean reward: 0.753 [0.617, 1.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.050, 10.161], loss: 0.001683, mae: 0.044658, mean_q: 1.283301
 54563/100000: episode: 1015, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 11.549, mean reward: 0.770 [0.724, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-1.218, 10.538], loss: 0.001621, mae: 0.043580, mean_q: 1.295823
 54595/100000: episode: 1016, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 20.594, mean reward: 0.644 [0.549, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.254, 10.176], loss: 0.001625, mae: 0.043693, mean_q: 1.297498
 54626/100000: episode: 1017, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 25.083, mean reward: 0.809 [0.721, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.076, 10.343], loss: 0.001545, mae: 0.042630, mean_q: 1.289278
 54657/100000: episode: 1018, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 22.110, mean reward: 0.713 [0.631, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.920, 10.521], loss: 0.001605, mae: 0.043470, mean_q: 1.289747
 54699/100000: episode: 1019, duration: 0.225s, episode steps: 42, steps per second: 186, episode reward: 27.041, mean reward: 0.644 [0.524, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.306, 10.251], loss: 0.001747, mae: 0.043688, mean_q: 1.283347
 54716/100000: episode: 1020, duration: 0.115s, episode steps: 17, steps per second: 148, episode reward: 12.678, mean reward: 0.746 [0.711, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.517], loss: 0.001480, mae: 0.042583, mean_q: 1.275911
 54758/100000: episode: 1021, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 31.159, mean reward: 0.742 [0.669, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.513, 10.450], loss: 0.001412, mae: 0.041338, mean_q: 1.295323
 54790/100000: episode: 1022, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 28.163, mean reward: 0.880 [0.772, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.325, 10.674], loss: 0.001496, mae: 0.041308, mean_q: 1.299737
 54821/100000: episode: 1023, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 23.145, mean reward: 0.747 [0.669, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.468], loss: 0.001695, mae: 0.043750, mean_q: 1.304197
 54832/100000: episode: 1024, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 8.150, mean reward: 0.741 [0.672, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.184, 10.417], loss: 0.001532, mae: 0.039952, mean_q: 1.303589
 54863/100000: episode: 1025, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 22.598, mean reward: 0.729 [0.604, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.212, 10.305], loss: 0.001693, mae: 0.043933, mean_q: 1.298877
 54894/100000: episode: 1026, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 18.981, mean reward: 0.612 [0.513, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.395, 10.100], loss: 0.001442, mae: 0.041538, mean_q: 1.309720
 54925/100000: episode: 1027, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 24.609, mean reward: 0.794 [0.693, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.035, 10.555], loss: 0.001430, mae: 0.041258, mean_q: 1.304429
 54953/100000: episode: 1028, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 18.127, mean reward: 0.647 [0.573, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.191, 10.303], loss: 0.002322, mae: 0.051733, mean_q: 1.315004
 54984/100000: episode: 1029, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 22.028, mean reward: 0.711 [0.649, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.840, 10.318], loss: 0.001466, mae: 0.042287, mean_q: 1.310895
 54999/100000: episode: 1030, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 11.550, mean reward: 0.770 [0.719, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-1.167, 10.574], loss: 0.001338, mae: 0.039869, mean_q: 1.315065
 55014/100000: episode: 1031, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 12.398, mean reward: 0.827 [0.735, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.339, 10.681], loss: 0.001396, mae: 0.039413, mean_q: 1.301623
 55025/100000: episode: 1032, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 8.337, mean reward: 0.758 [0.733, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.443, 10.444], loss: 0.001576, mae: 0.042887, mean_q: 1.306589
 55042/100000: episode: 1033, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 12.797, mean reward: 0.753 [0.616, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.358], loss: 0.001344, mae: 0.039470, mean_q: 1.312158
 55053/100000: episode: 1034, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 8.548, mean reward: 0.777 [0.737, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.662, 10.453], loss: 0.001198, mae: 0.037747, mean_q: 1.278590
 55070/100000: episode: 1035, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 13.026, mean reward: 0.766 [0.695, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.527], loss: 0.001563, mae: 0.042083, mean_q: 1.314017
 55101/100000: episode: 1036, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 23.257, mean reward: 0.750 [0.679, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.832, 10.517], loss: 0.001514, mae: 0.042083, mean_q: 1.311729
 55133/100000: episode: 1037, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 23.835, mean reward: 0.745 [0.633, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.828, 10.362], loss: 0.001376, mae: 0.040589, mean_q: 1.314664
 55164/100000: episode: 1038, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 23.633, mean reward: 0.762 [0.650, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.084, 10.432], loss: 0.001588, mae: 0.042915, mean_q: 1.315709
 55181/100000: episode: 1039, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 12.211, mean reward: 0.718 [0.672, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.835, 10.457], loss: 0.001432, mae: 0.040618, mean_q: 1.327350
 55209/100000: episode: 1040, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 21.291, mean reward: 0.760 [0.716, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.666, 10.459], loss: 0.001599, mae: 0.043169, mean_q: 1.321777
 55237/100000: episode: 1041, duration: 0.163s, episode steps: 28, steps per second: 172, episode reward: 18.166, mean reward: 0.649 [0.531, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.675, 10.186], loss: 0.001432, mae: 0.041637, mean_q: 1.321777
 55265/100000: episode: 1042, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 20.207, mean reward: 0.722 [0.598, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.445], loss: 0.001463, mae: 0.042120, mean_q: 1.327108
 55282/100000: episode: 1043, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 12.884, mean reward: 0.758 [0.705, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.331, 10.617], loss: 0.001639, mae: 0.042412, mean_q: 1.317778
 55314/100000: episode: 1044, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 25.012, mean reward: 0.782 [0.642, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.972, 10.383], loss: 0.001383, mae: 0.040127, mean_q: 1.326173
 55342/100000: episode: 1045, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 18.669, mean reward: 0.667 [0.524, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.934, 10.134], loss: 0.001422, mae: 0.040228, mean_q: 1.327454
 55353/100000: episode: 1046, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 8.319, mean reward: 0.756 [0.671, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.452], loss: 0.001499, mae: 0.042213, mean_q: 1.338021
 55364/100000: episode: 1047, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 7.720, mean reward: 0.702 [0.597, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.438], loss: 0.001489, mae: 0.043355, mean_q: 1.313107
 55395/100000: episode: 1048, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 23.016, mean reward: 0.742 [0.617, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.657, 10.339], loss: 0.001458, mae: 0.042478, mean_q: 1.329152
 55410/100000: episode: 1049, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 12.152, mean reward: 0.810 [0.746, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.142, 10.685], loss: 0.001330, mae: 0.039470, mean_q: 1.329585
 55427/100000: episode: 1050, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 12.253, mean reward: 0.721 [0.648, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.174, 10.450], loss: 0.001353, mae: 0.039656, mean_q: 1.337947
 55458/100000: episode: 1051, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 21.323, mean reward: 0.688 [0.615, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.926, 10.315], loss: 0.001643, mae: 0.044259, mean_q: 1.331630
 55489/100000: episode: 1052, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 23.192, mean reward: 0.748 [0.663, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.587], loss: 0.001658, mae: 0.044498, mean_q: 1.326581
 55506/100000: episode: 1053, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 13.076, mean reward: 0.769 [0.687, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.355], loss: 0.001708, mae: 0.044636, mean_q: 1.331863
 55537/100000: episode: 1054, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 23.987, mean reward: 0.774 [0.615, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.750, 10.400], loss: 0.001439, mae: 0.041468, mean_q: 1.330645
 55568/100000: episode: 1055, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 22.000, mean reward: 0.710 [0.612, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.211, 10.365], loss: 0.001384, mae: 0.040516, mean_q: 1.338743
 55579/100000: episode: 1056, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 9.179, mean reward: 0.834 [0.772, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.667], loss: 0.001184, mae: 0.038272, mean_q: 1.334745
 55610/100000: episode: 1057, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 23.782, mean reward: 0.767 [0.593, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.035, 10.446], loss: 0.001323, mae: 0.040619, mean_q: 1.340268
 55625/100000: episode: 1058, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 11.883, mean reward: 0.792 [0.745, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.505], loss: 0.001322, mae: 0.039744, mean_q: 1.344408
 55636/100000: episode: 1059, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 7.264, mean reward: 0.660 [0.606, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.273], loss: 0.001292, mae: 0.039191, mean_q: 1.333518
 55667/100000: episode: 1060, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 20.703, mean reward: 0.668 [0.602, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.035, 10.400], loss: 0.001464, mae: 0.040731, mean_q: 1.330708
 55695/100000: episode: 1061, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 20.863, mean reward: 0.745 [0.670, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.396], loss: 0.001359, mae: 0.040524, mean_q: 1.349286
 55706/100000: episode: 1062, duration: 0.066s, episode steps: 11, steps per second: 165, episode reward: 7.537, mean reward: 0.685 [0.617, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.355], loss: 0.001172, mae: 0.037673, mean_q: 1.332267
[Info] FALSIFICATION!
 55730/100000: episode: 1063, duration: 0.289s, episode steps: 24, steps per second: 83, episode reward: 19.553, mean reward: 0.815 [0.720, 1.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.018, 10.651], loss: 0.001361, mae: 0.040442, mean_q: 1.330045
 55741/100000: episode: 1064, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 8.225, mean reward: 0.748 [0.705, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.181, 10.472], loss: 0.001076, mae: 0.036965, mean_q: 1.339650
 55772/100000: episode: 1065, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 20.726, mean reward: 0.669 [0.524, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.834, 10.259], loss: 0.001390, mae: 0.040919, mean_q: 1.341180
 55787/100000: episode: 1066, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 12.026, mean reward: 0.802 [0.733, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.576], loss: 0.001417, mae: 0.040746, mean_q: 1.353487
 55815/100000: episode: 1067, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 17.680, mean reward: 0.631 [0.529, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.296, 10.180], loss: 0.001328, mae: 0.039322, mean_q: 1.345703
 55846/100000: episode: 1068, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 21.086, mean reward: 0.680 [0.547, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.704, 10.285], loss: 0.001682, mae: 0.042660, mean_q: 1.332822
 55863/100000: episode: 1069, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 14.233, mean reward: 0.837 [0.700, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.645], loss: 0.001491, mae: 0.042230, mean_q: 1.348031
[Info] Complete ISplit Iteration
[Info] Levels: [1.3918493, 1.4829527, 1.5857013]
[Info] Cond. Prob: [0.1, 0.1, 0.63]
[Info] Error Prob: 0.006300000000000001

 55894/100000: episode: 1070, duration: 4.541s, episode steps: 31, steps per second: 7, episode reward: 21.463, mean reward: 0.692 [0.550, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.247, 10.268], loss: 0.001845, mae: 0.044483, mean_q: 1.334912
 55994/100000: episode: 1071, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.194, mean reward: 0.592 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.728, 10.098], loss: 0.001456, mae: 0.040719, mean_q: 1.341035
 56094/100000: episode: 1072, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.464, mean reward: 0.605 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.646, 10.183], loss: 0.001546, mae: 0.041241, mean_q: 1.336555
 56194/100000: episode: 1073, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.359, mean reward: 0.594 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.935, 10.366], loss: 0.001613, mae: 0.041776, mean_q: 1.340115
 56294/100000: episode: 1074, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.305, mean reward: 0.573 [0.501, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.592, 10.098], loss: 0.001371, mae: 0.040089, mean_q: 1.335706
 56394/100000: episode: 1075, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.814, mean reward: 0.588 [0.514, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.464, 10.098], loss: 0.001465, mae: 0.041042, mean_q: 1.334861
 56494/100000: episode: 1076, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.401, mean reward: 0.584 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.330, 10.098], loss: 0.001563, mae: 0.040762, mean_q: 1.321695
 56594/100000: episode: 1077, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 57.765, mean reward: 0.578 [0.505, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.279], loss: 0.001352, mae: 0.039557, mean_q: 1.331329
 56694/100000: episode: 1078, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.439, mean reward: 0.574 [0.506, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.865, 10.223], loss: 0.001406, mae: 0.040253, mean_q: 1.318704
 56794/100000: episode: 1079, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 56.959, mean reward: 0.570 [0.500, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.663, 10.141], loss: 0.001398, mae: 0.041182, mean_q: 1.325141
 56894/100000: episode: 1080, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.262, mean reward: 0.603 [0.500, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.550, 10.347], loss: 0.001567, mae: 0.041533, mean_q: 1.308073
 56994/100000: episode: 1081, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.069, mean reward: 0.581 [0.504, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.270, 10.098], loss: 0.001304, mae: 0.039385, mean_q: 1.316353
 57094/100000: episode: 1082, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.929, mean reward: 0.589 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.059, 10.098], loss: 0.001524, mae: 0.040867, mean_q: 1.313372
 57194/100000: episode: 1083, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 60.754, mean reward: 0.608 [0.511, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.942, 10.376], loss: 0.001538, mae: 0.040497, mean_q: 1.313114
 57294/100000: episode: 1084, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 57.068, mean reward: 0.571 [0.507, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.667, 10.180], loss: 0.001541, mae: 0.042092, mean_q: 1.301232
 57394/100000: episode: 1085, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.759, mean reward: 0.588 [0.508, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.152, 10.278], loss: 0.001402, mae: 0.039535, mean_q: 1.298326
 57494/100000: episode: 1086, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.666, mean reward: 0.587 [0.502, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.485, 10.293], loss: 0.001495, mae: 0.040477, mean_q: 1.296554
 57594/100000: episode: 1087, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.008, mean reward: 0.570 [0.501, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.472, 10.245], loss: 0.001397, mae: 0.039644, mean_q: 1.302474
 57694/100000: episode: 1088, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 62.231, mean reward: 0.622 [0.509, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.714, 10.335], loss: 0.001642, mae: 0.042368, mean_q: 1.292131
 57794/100000: episode: 1089, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.725, mean reward: 0.577 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.704, 10.198], loss: 0.001432, mae: 0.041550, mean_q: 1.294721
 57894/100000: episode: 1090, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 60.872, mean reward: 0.609 [0.517, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.196, 10.098], loss: 0.001446, mae: 0.040009, mean_q: 1.288938
 57994/100000: episode: 1091, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.261, mean reward: 0.583 [0.498, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.625, 10.107], loss: 0.001476, mae: 0.041551, mean_q: 1.294687
 58094/100000: episode: 1092, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.528, mean reward: 0.585 [0.501, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.709, 10.225], loss: 0.001899, mae: 0.042572, mean_q: 1.285132
 58194/100000: episode: 1093, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 59.654, mean reward: 0.597 [0.510, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.173, 10.098], loss: 0.001608, mae: 0.042280, mean_q: 1.283276
 58294/100000: episode: 1094, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.904, mean reward: 0.589 [0.509, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.380, 10.124], loss: 0.001561, mae: 0.041743, mean_q: 1.273415
 58394/100000: episode: 1095, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.079, mean reward: 0.571 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.751, 10.098], loss: 0.001978, mae: 0.044289, mean_q: 1.287802
 58494/100000: episode: 1096, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.170, mean reward: 0.582 [0.503, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.230, 10.115], loss: 0.001660, mae: 0.042231, mean_q: 1.281190
 58594/100000: episode: 1097, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.734, mean reward: 0.587 [0.510, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.808, 10.250], loss: 0.001871, mae: 0.044130, mean_q: 1.269936
 58694/100000: episode: 1098, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.287, mean reward: 0.583 [0.504, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.240, 10.256], loss: 0.001773, mae: 0.044384, mean_q: 1.266305
 58794/100000: episode: 1099, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.570, mean reward: 0.586 [0.507, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.375, 10.098], loss: 0.001412, mae: 0.040600, mean_q: 1.265404
 58894/100000: episode: 1100, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.630, mean reward: 0.586 [0.508, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.724, 10.098], loss: 0.001610, mae: 0.042060, mean_q: 1.265283
 58994/100000: episode: 1101, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.014, mean reward: 0.600 [0.509, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.893, 10.102], loss: 0.001447, mae: 0.040393, mean_q: 1.251436
 59094/100000: episode: 1102, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 61.516, mean reward: 0.615 [0.503, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.760, 10.185], loss: 0.001510, mae: 0.041691, mean_q: 1.252623
 59194/100000: episode: 1103, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.354, mean reward: 0.574 [0.507, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.381, 10.098], loss: 0.001661, mae: 0.042722, mean_q: 1.243001
 59294/100000: episode: 1104, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 60.127, mean reward: 0.601 [0.500, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.689, 10.401], loss: 0.001778, mae: 0.044172, mean_q: 1.230712
 59394/100000: episode: 1105, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.669, mean reward: 0.577 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.278, 10.098], loss: 0.001828, mae: 0.045232, mean_q: 1.236549
 59494/100000: episode: 1106, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.070, mean reward: 0.601 [0.504, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.846, 10.098], loss: 0.001571, mae: 0.042894, mean_q: 1.230856
 59594/100000: episode: 1107, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.841, mean reward: 0.578 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.621, 10.098], loss: 0.001520, mae: 0.042407, mean_q: 1.227355
 59694/100000: episode: 1108, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.854, mean reward: 0.589 [0.521, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.472, 10.150], loss: 0.001469, mae: 0.041360, mean_q: 1.217564
 59794/100000: episode: 1109, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.470, mean reward: 0.595 [0.508, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.584, 10.098], loss: 0.001608, mae: 0.042528, mean_q: 1.219614
 59894/100000: episode: 1110, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.598, mean reward: 0.576 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.744, 10.121], loss: 0.001597, mae: 0.043142, mean_q: 1.211024
 59994/100000: episode: 1111, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.133, mean reward: 0.581 [0.505, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.696, 10.098], loss: 0.001478, mae: 0.041545, mean_q: 1.206601
 60094/100000: episode: 1112, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.670, mean reward: 0.587 [0.501, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.188, 10.169], loss: 0.001495, mae: 0.042392, mean_q: 1.195652
 60194/100000: episode: 1113, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.972, mean reward: 0.570 [0.504, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.814, 10.150], loss: 0.001413, mae: 0.040138, mean_q: 1.192680
 60294/100000: episode: 1114, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.001, mean reward: 0.570 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.483, 10.098], loss: 0.001537, mae: 0.042902, mean_q: 1.186966
 60394/100000: episode: 1115, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 60.275, mean reward: 0.603 [0.506, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.318, 10.301], loss: 0.001525, mae: 0.043171, mean_q: 1.185441
 60494/100000: episode: 1116, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 56.652, mean reward: 0.567 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.496, 10.215], loss: 0.001560, mae: 0.042323, mean_q: 1.177260
 60594/100000: episode: 1117, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.207, mean reward: 0.582 [0.500, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.586, 10.224], loss: 0.001501, mae: 0.041401, mean_q: 1.167902
 60694/100000: episode: 1118, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 60.291, mean reward: 0.603 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.874, 10.240], loss: 0.001554, mae: 0.041212, mean_q: 1.165946
 60794/100000: episode: 1119, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.004, mean reward: 0.590 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.254, 10.297], loss: 0.001410, mae: 0.040541, mean_q: 1.161687
 60894/100000: episode: 1120, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.648, mean reward: 0.596 [0.513, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.556, 10.098], loss: 0.001468, mae: 0.041489, mean_q: 1.163227
 60994/100000: episode: 1121, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 62.605, mean reward: 0.626 [0.501, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.094, 10.098], loss: 0.001326, mae: 0.039875, mean_q: 1.158441
 61094/100000: episode: 1122, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.465, mean reward: 0.585 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.133, 10.295], loss: 0.001411, mae: 0.041144, mean_q: 1.161566
 61194/100000: episode: 1123, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 60.247, mean reward: 0.602 [0.513, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.219, 10.098], loss: 0.001370, mae: 0.040687, mean_q: 1.163864
 61294/100000: episode: 1124, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.760, mean reward: 0.588 [0.516, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.946, 10.098], loss: 0.001545, mae: 0.041857, mean_q: 1.161338
 61394/100000: episode: 1125, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.812, mean reward: 0.598 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.901, 10.098], loss: 0.001464, mae: 0.042197, mean_q: 1.162028
 61494/100000: episode: 1126, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 59.335, mean reward: 0.593 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.253, 10.098], loss: 0.001362, mae: 0.040249, mean_q: 1.159614
 61594/100000: episode: 1127, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.446, mean reward: 0.604 [0.505, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.993, 10.098], loss: 0.001441, mae: 0.041022, mean_q: 1.162288
 61694/100000: episode: 1128, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.266, mean reward: 0.573 [0.501, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.233, 10.098], loss: 0.001431, mae: 0.040981, mean_q: 1.163960
 61794/100000: episode: 1129, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.522, mean reward: 0.575 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.837, 10.098], loss: 0.001416, mae: 0.041200, mean_q: 1.168631
 61894/100000: episode: 1130, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 58.177, mean reward: 0.582 [0.508, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.027, 10.098], loss: 0.001462, mae: 0.041649, mean_q: 1.162058
 61994/100000: episode: 1131, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.568, mean reward: 0.606 [0.509, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.841, 10.098], loss: 0.001445, mae: 0.041706, mean_q: 1.166565
 62094/100000: episode: 1132, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.344, mean reward: 0.573 [0.510, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.277, 10.144], loss: 0.001499, mae: 0.041986, mean_q: 1.159495
 62194/100000: episode: 1133, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.523, mean reward: 0.575 [0.508, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.793, 10.098], loss: 0.001618, mae: 0.043903, mean_q: 1.163289
 62294/100000: episode: 1134, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.826, mean reward: 0.608 [0.509, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.170, 10.098], loss: 0.001654, mae: 0.043695, mean_q: 1.162709
 62394/100000: episode: 1135, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.589, mean reward: 0.586 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.187, 10.153], loss: 0.001459, mae: 0.041320, mean_q: 1.162203
 62494/100000: episode: 1136, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.827, mean reward: 0.598 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.794, 10.242], loss: 0.001468, mae: 0.041837, mean_q: 1.163111
 62594/100000: episode: 1137, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 61.150, mean reward: 0.612 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.211, 10.102], loss: 0.001517, mae: 0.042731, mean_q: 1.165970
 62694/100000: episode: 1138, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.912, mean reward: 0.589 [0.502, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.446, 10.241], loss: 0.001567, mae: 0.043089, mean_q: 1.165173
 62794/100000: episode: 1139, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.674, mean reward: 0.587 [0.499, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.971, 10.335], loss: 0.001595, mae: 0.043501, mean_q: 1.162269
 62894/100000: episode: 1140, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.133, mean reward: 0.591 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.710, 10.098], loss: 0.001414, mae: 0.041273, mean_q: 1.164988
 62994/100000: episode: 1141, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 59.158, mean reward: 0.592 [0.516, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.441, 10.221], loss: 0.001538, mae: 0.042672, mean_q: 1.163594
 63094/100000: episode: 1142, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.416, mean reward: 0.584 [0.507, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.985, 10.420], loss: 0.001509, mae: 0.042474, mean_q: 1.169089
 63194/100000: episode: 1143, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.505, mean reward: 0.595 [0.506, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.819, 10.098], loss: 0.001407, mae: 0.040751, mean_q: 1.165312
 63294/100000: episode: 1144, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.707, mean reward: 0.587 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.013, 10.098], loss: 0.001424, mae: 0.041121, mean_q: 1.162215
 63394/100000: episode: 1145, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.590, mean reward: 0.586 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.925, 10.098], loss: 0.001379, mae: 0.040474, mean_q: 1.165397
 63494/100000: episode: 1146, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.151, mean reward: 0.582 [0.499, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.922, 10.178], loss: 0.001407, mae: 0.041023, mean_q: 1.165568
 63594/100000: episode: 1147, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 57.606, mean reward: 0.576 [0.509, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.819, 10.177], loss: 0.001445, mae: 0.042019, mean_q: 1.163795
 63694/100000: episode: 1148, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.455, mean reward: 0.585 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.934, 10.130], loss: 0.001655, mae: 0.043828, mean_q: 1.168521
 63794/100000: episode: 1149, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.444, mean reward: 0.584 [0.500, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.009, 10.116], loss: 0.001559, mae: 0.043175, mean_q: 1.165785
 63894/100000: episode: 1150, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 56.793, mean reward: 0.568 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.378, 10.167], loss: 0.001446, mae: 0.041459, mean_q: 1.163685
 63994/100000: episode: 1151, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 57.431, mean reward: 0.574 [0.510, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.340, 10.287], loss: 0.001492, mae: 0.042820, mean_q: 1.161910
 64094/100000: episode: 1152, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 56.766, mean reward: 0.568 [0.501, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.778, 10.208], loss: 0.001424, mae: 0.041355, mean_q: 1.161741
 64194/100000: episode: 1153, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 58.427, mean reward: 0.584 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.699, 10.130], loss: 0.001521, mae: 0.042409, mean_q: 1.162869
 64294/100000: episode: 1154, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 60.662, mean reward: 0.607 [0.506, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.807, 10.098], loss: 0.001440, mae: 0.041588, mean_q: 1.161499
 64394/100000: episode: 1155, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.524, mean reward: 0.605 [0.508, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.466, 10.566], loss: 0.001511, mae: 0.042765, mean_q: 1.166445
 64494/100000: episode: 1156, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 62.016, mean reward: 0.620 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.782, 10.098], loss: 0.001475, mae: 0.041760, mean_q: 1.159264
 64594/100000: episode: 1157, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.833, mean reward: 0.578 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.837, 10.192], loss: 0.001436, mae: 0.041198, mean_q: 1.162543
 64694/100000: episode: 1158, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.202, mean reward: 0.602 [0.524, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.772, 10.234], loss: 0.001477, mae: 0.041723, mean_q: 1.163905
 64794/100000: episode: 1159, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 63.107, mean reward: 0.631 [0.509, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.417, 10.098], loss: 0.001384, mae: 0.040776, mean_q: 1.164531
 64894/100000: episode: 1160, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 60.014, mean reward: 0.600 [0.515, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.854, 10.277], loss: 0.001419, mae: 0.041356, mean_q: 1.165730
 64994/100000: episode: 1161, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.737, mean reward: 0.587 [0.508, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.574, 10.098], loss: 0.001463, mae: 0.041520, mean_q: 1.167613
 65094/100000: episode: 1162, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.471, mean reward: 0.585 [0.508, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.158, 10.098], loss: 0.001441, mae: 0.041802, mean_q: 1.161900
 65194/100000: episode: 1163, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.679, mean reward: 0.577 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.564, 10.154], loss: 0.001455, mae: 0.041660, mean_q: 1.165479
 65294/100000: episode: 1164, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 59.898, mean reward: 0.599 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.636, 10.367], loss: 0.001341, mae: 0.040217, mean_q: 1.166582
 65394/100000: episode: 1165, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 59.747, mean reward: 0.597 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.355, 10.309], loss: 0.001407, mae: 0.041666, mean_q: 1.168584
 65494/100000: episode: 1166, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.934, mean reward: 0.579 [0.500, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.997, 10.225], loss: 0.001488, mae: 0.042624, mean_q: 1.172155
 65594/100000: episode: 1167, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.223, mean reward: 0.592 [0.516, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.152, 10.098], loss: 0.001421, mae: 0.041873, mean_q: 1.169123
 65694/100000: episode: 1168, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.049, mean reward: 0.590 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.011, 10.139], loss: 0.001366, mae: 0.040535, mean_q: 1.168790
 65794/100000: episode: 1169, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 56.991, mean reward: 0.570 [0.501, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.827, 10.317], loss: 0.001425, mae: 0.041559, mean_q: 1.169592
[Info] 1-TH LEVEL FOUND: 1.4112162590026855, Considering 10/90 traces
 65894/100000: episode: 1170, duration: 4.829s, episode steps: 100, steps per second: 21, episode reward: 57.774, mean reward: 0.578 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.975, 10.167], loss: 0.001419, mae: 0.041572, mean_q: 1.164335
 65913/100000: episode: 1171, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 14.890, mean reward: 0.784 [0.731, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-1.378, 10.488], loss: 0.001499, mae: 0.042758, mean_q: 1.166485
 65968/100000: episode: 1172, duration: 0.287s, episode steps: 55, steps per second: 191, episode reward: 32.589, mean reward: 0.593 [0.514, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-1.385, 10.133], loss: 0.001429, mae: 0.041767, mean_q: 1.164918
 66023/100000: episode: 1173, duration: 0.311s, episode steps: 55, steps per second: 177, episode reward: 40.635, mean reward: 0.739 [0.617, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.438, 10.463], loss: 0.001528, mae: 0.042992, mean_q: 1.168460
 66045/100000: episode: 1174, duration: 0.134s, episode steps: 22, steps per second: 165, episode reward: 14.405, mean reward: 0.655 [0.569, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.488, 10.100], loss: 0.001464, mae: 0.042077, mean_q: 1.172384
 66096/100000: episode: 1175, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 28.832, mean reward: 0.565 [0.515, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.483, 10.100], loss: 0.001460, mae: 0.040776, mean_q: 1.161498
 66118/100000: episode: 1176, duration: 0.148s, episode steps: 22, steps per second: 149, episode reward: 14.429, mean reward: 0.656 [0.589, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.234, 10.100], loss: 0.001683, mae: 0.045882, mean_q: 1.171833
 66169/100000: episode: 1177, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 35.044, mean reward: 0.687 [0.611, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.623, 10.385], loss: 0.001386, mae: 0.040482, mean_q: 1.165839
 66215/100000: episode: 1178, duration: 0.259s, episode steps: 46, steps per second: 177, episode reward: 28.774, mean reward: 0.626 [0.542, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.856, 10.283], loss: 0.001466, mae: 0.042034, mean_q: 1.169796
 66242/100000: episode: 1179, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 19.207, mean reward: 0.711 [0.655, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.411, 10.100], loss: 0.001476, mae: 0.040727, mean_q: 1.172175
 66289/100000: episode: 1180, duration: 0.248s, episode steps: 47, steps per second: 190, episode reward: 30.399, mean reward: 0.647 [0.530, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-1.220, 10.100], loss: 0.001515, mae: 0.042719, mean_q: 1.171710
 66340/100000: episode: 1181, duration: 0.302s, episode steps: 51, steps per second: 169, episode reward: 32.310, mean reward: 0.634 [0.536, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.688, 10.201], loss: 0.001610, mae: 0.044035, mean_q: 1.173640
 66395/100000: episode: 1182, duration: 0.329s, episode steps: 55, steps per second: 167, episode reward: 38.206, mean reward: 0.695 [0.599, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-0.551, 10.354], loss: 0.001643, mae: 0.044133, mean_q: 1.175088
 66446/100000: episode: 1183, duration: 0.290s, episode steps: 51, steps per second: 176, episode reward: 31.756, mean reward: 0.623 [0.523, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.785, 10.196], loss: 0.001358, mae: 0.040224, mean_q: 1.172646
 66493/100000: episode: 1184, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 33.282, mean reward: 0.708 [0.582, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.650, 10.385], loss: 0.001415, mae: 0.041269, mean_q: 1.179682
 66515/100000: episode: 1185, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 15.169, mean reward: 0.689 [0.620, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.134, 10.100], loss: 0.001529, mae: 0.042769, mean_q: 1.180790
 66566/100000: episode: 1186, duration: 0.302s, episode steps: 51, steps per second: 169, episode reward: 30.779, mean reward: 0.604 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.279, 10.108], loss: 0.001555, mae: 0.042631, mean_q: 1.181025
 66621/100000: episode: 1187, duration: 0.289s, episode steps: 55, steps per second: 190, episode reward: 40.189, mean reward: 0.731 [0.625, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-0.433, 10.278], loss: 0.001465, mae: 0.041454, mean_q: 1.181872
 66676/100000: episode: 1188, duration: 0.295s, episode steps: 55, steps per second: 186, episode reward: 34.303, mean reward: 0.624 [0.510, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-2.052, 10.100], loss: 0.001445, mae: 0.041700, mean_q: 1.180291
 66727/100000: episode: 1189, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 32.025, mean reward: 0.628 [0.558, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.288, 10.427], loss: 0.001459, mae: 0.042145, mean_q: 1.183126
 66778/100000: episode: 1190, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 31.879, mean reward: 0.625 [0.535, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.444, 10.251], loss: 0.001604, mae: 0.044097, mean_q: 1.180026
 66825/100000: episode: 1191, duration: 0.268s, episode steps: 47, steps per second: 175, episode reward: 28.421, mean reward: 0.605 [0.524, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.366, 10.100], loss: 0.001500, mae: 0.041693, mean_q: 1.181646
 66883/100000: episode: 1192, duration: 0.319s, episode steps: 58, steps per second: 182, episode reward: 37.643, mean reward: 0.649 [0.506, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.827 [-0.338, 10.165], loss: 0.001544, mae: 0.042431, mean_q: 1.190528
 66910/100000: episode: 1193, duration: 0.164s, episode steps: 27, steps per second: 165, episode reward: 19.174, mean reward: 0.710 [0.588, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.167, 10.100], loss: 0.001781, mae: 0.045059, mean_q: 1.181728
 66968/100000: episode: 1194, duration: 0.331s, episode steps: 58, steps per second: 175, episode reward: 36.163, mean reward: 0.624 [0.519, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.841 [-0.495, 10.217], loss: 0.001596, mae: 0.043417, mean_q: 1.189226
 67026/100000: episode: 1195, duration: 0.331s, episode steps: 58, steps per second: 175, episode reward: 36.297, mean reward: 0.626 [0.499, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.818 [-1.320, 10.100], loss: 0.001609, mae: 0.043874, mean_q: 1.181735
 67077/100000: episode: 1196, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 30.418, mean reward: 0.596 [0.535, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.346, 10.100], loss: 0.001660, mae: 0.043490, mean_q: 1.185364
 67104/100000: episode: 1197, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 18.572, mean reward: 0.688 [0.572, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.126, 10.100], loss: 0.001455, mae: 0.041409, mean_q: 1.201218
 67155/100000: episode: 1198, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 32.432, mean reward: 0.636 [0.533, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.382, 10.321], loss: 0.001588, mae: 0.043376, mean_q: 1.193365
 67210/100000: episode: 1199, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 34.059, mean reward: 0.619 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-0.957, 10.209], loss: 0.001563, mae: 0.042704, mean_q: 1.193081
 67261/100000: episode: 1200, duration: 0.278s, episode steps: 51, steps per second: 183, episode reward: 30.110, mean reward: 0.590 [0.513, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.436, 10.100], loss: 0.001373, mae: 0.040541, mean_q: 1.194805
 67308/100000: episode: 1201, duration: 0.283s, episode steps: 47, steps per second: 166, episode reward: 33.250, mean reward: 0.707 [0.624, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-1.158, 10.413], loss: 0.001488, mae: 0.042007, mean_q: 1.193115
 67330/100000: episode: 1202, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 15.954, mean reward: 0.725 [0.613, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.470, 10.100], loss: 0.001448, mae: 0.039676, mean_q: 1.193418
 67385/100000: episode: 1203, duration: 0.293s, episode steps: 55, steps per second: 188, episode reward: 35.360, mean reward: 0.643 [0.574, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.875, 10.435], loss: 0.001478, mae: 0.041612, mean_q: 1.195847
 67431/100000: episode: 1204, duration: 0.275s, episode steps: 46, steps per second: 167, episode reward: 27.345, mean reward: 0.594 [0.519, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.969, 10.100], loss: 0.001635, mae: 0.043102, mean_q: 1.198961
 67477/100000: episode: 1205, duration: 0.263s, episode steps: 46, steps per second: 175, episode reward: 25.875, mean reward: 0.563 [0.498, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-1.457, 10.180], loss: 0.001555, mae: 0.042044, mean_q: 1.197547
 67496/100000: episode: 1206, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 13.466, mean reward: 0.709 [0.628, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.371], loss: 0.001780, mae: 0.046778, mean_q: 1.189650
 67543/100000: episode: 1207, duration: 0.271s, episode steps: 47, steps per second: 173, episode reward: 28.044, mean reward: 0.597 [0.508, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.592, 10.100], loss: 0.001712, mae: 0.043219, mean_q: 1.189628
 67594/100000: episode: 1208, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 30.645, mean reward: 0.601 [0.502, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.538, 10.100], loss: 0.001396, mae: 0.040497, mean_q: 1.193213
 67644/100000: episode: 1209, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 30.849, mean reward: 0.617 [0.545, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-1.356, 10.100], loss: 0.001698, mae: 0.044255, mean_q: 1.199756
 67699/100000: episode: 1210, duration: 0.287s, episode steps: 55, steps per second: 192, episode reward: 36.149, mean reward: 0.657 [0.518, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-1.451, 10.118], loss: 0.001707, mae: 0.043495, mean_q: 1.192480
 67754/100000: episode: 1211, duration: 0.318s, episode steps: 55, steps per second: 173, episode reward: 36.390, mean reward: 0.662 [0.587, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-1.057, 10.296], loss: 0.001382, mae: 0.040766, mean_q: 1.197731
 67776/100000: episode: 1212, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 15.737, mean reward: 0.715 [0.657, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.302, 10.100], loss: 0.001726, mae: 0.043123, mean_q: 1.199620
 67798/100000: episode: 1213, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 15.977, mean reward: 0.726 [0.617, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.255, 10.100], loss: 0.001561, mae: 0.042959, mean_q: 1.188141
 67849/100000: episode: 1214, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 31.204, mean reward: 0.612 [0.512, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.785, 10.350], loss: 0.001405, mae: 0.040715, mean_q: 1.199029
 67900/100000: episode: 1215, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 32.336, mean reward: 0.634 [0.507, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.852, 10.167], loss: 0.001390, mae: 0.041035, mean_q: 1.197003
 67946/100000: episode: 1216, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 30.491, mean reward: 0.663 [0.579, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.351, 10.263], loss: 0.001428, mae: 0.041030, mean_q: 1.203340
 68004/100000: episode: 1217, duration: 0.339s, episode steps: 58, steps per second: 171, episode reward: 38.615, mean reward: 0.666 [0.517, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.828 [-1.311, 10.217], loss: 0.001614, mae: 0.042297, mean_q: 1.196702
 68055/100000: episode: 1218, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 31.132, mean reward: 0.610 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.558, 10.174], loss: 0.001612, mae: 0.043927, mean_q: 1.202294
 68077/100000: episode: 1219, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 16.353, mean reward: 0.743 [0.678, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.413, 10.100], loss: 0.001791, mae: 0.045366, mean_q: 1.214155
 68096/100000: episode: 1220, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 15.204, mean reward: 0.800 [0.714, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.075, 10.520], loss: 0.001671, mae: 0.044253, mean_q: 1.198712
 68146/100000: episode: 1221, duration: 0.281s, episode steps: 50, steps per second: 178, episode reward: 31.583, mean reward: 0.632 [0.537, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.882, 10.255], loss: 0.001546, mae: 0.042258, mean_q: 1.204968
 68201/100000: episode: 1222, duration: 0.312s, episode steps: 55, steps per second: 176, episode reward: 37.182, mean reward: 0.676 [0.522, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-1.442, 10.100], loss: 0.001669, mae: 0.043646, mean_q: 1.202714
 68256/100000: episode: 1223, duration: 0.312s, episode steps: 55, steps per second: 176, episode reward: 39.729, mean reward: 0.722 [0.569, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-0.807, 10.354], loss: 0.001500, mae: 0.042175, mean_q: 1.203631
 68311/100000: episode: 1224, duration: 0.308s, episode steps: 55, steps per second: 179, episode reward: 35.433, mean reward: 0.644 [0.522, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.631, 10.211], loss: 0.001651, mae: 0.043339, mean_q: 1.207236
 68333/100000: episode: 1225, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 14.778, mean reward: 0.672 [0.606, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.195, 10.100], loss: 0.001687, mae: 0.044914, mean_q: 1.207597
 68384/100000: episode: 1226, duration: 0.296s, episode steps: 51, steps per second: 172, episode reward: 30.923, mean reward: 0.606 [0.516, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.301, 10.325], loss: 0.001595, mae: 0.043144, mean_q: 1.213098
 68406/100000: episode: 1227, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 15.087, mean reward: 0.686 [0.638, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.286, 10.100], loss: 0.001798, mae: 0.044467, mean_q: 1.215328
 68461/100000: episode: 1228, duration: 0.298s, episode steps: 55, steps per second: 184, episode reward: 34.402, mean reward: 0.625 [0.514, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.849 [-1.281, 10.114], loss: 0.001341, mae: 0.039935, mean_q: 1.214540
 68511/100000: episode: 1229, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 30.987, mean reward: 0.620 [0.506, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.811, 10.100], loss: 0.001497, mae: 0.041383, mean_q: 1.217762
 68557/100000: episode: 1230, duration: 0.267s, episode steps: 46, steps per second: 172, episode reward: 29.011, mean reward: 0.631 [0.531, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.528, 10.100], loss: 0.001366, mae: 0.041059, mean_q: 1.217933
 68603/100000: episode: 1231, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 28.601, mean reward: 0.622 [0.539, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.313, 10.318], loss: 0.001487, mae: 0.042202, mean_q: 1.218686
 68650/100000: episode: 1232, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 32.053, mean reward: 0.682 [0.609, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-1.485, 10.416], loss: 0.001652, mae: 0.043790, mean_q: 1.216706
 68701/100000: episode: 1233, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 31.771, mean reward: 0.623 [0.524, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.717, 10.218], loss: 0.001637, mae: 0.043390, mean_q: 1.219462
 68747/100000: episode: 1234, duration: 0.269s, episode steps: 46, steps per second: 171, episode reward: 28.586, mean reward: 0.621 [0.516, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.529, 10.197], loss: 0.001423, mae: 0.040011, mean_q: 1.215493
 68797/100000: episode: 1235, duration: 0.280s, episode steps: 50, steps per second: 179, episode reward: 32.221, mean reward: 0.644 [0.531, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.864, 10.100], loss: 0.001471, mae: 0.041655, mean_q: 1.220966
 68824/100000: episode: 1236, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 19.435, mean reward: 0.720 [0.644, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.264, 10.100], loss: 0.001637, mae: 0.043975, mean_q: 1.217934
 68874/100000: episode: 1237, duration: 0.273s, episode steps: 50, steps per second: 183, episode reward: 32.429, mean reward: 0.649 [0.560, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.816, 10.299], loss: 0.001816, mae: 0.045788, mean_q: 1.222519
 68893/100000: episode: 1238, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 14.459, mean reward: 0.761 [0.677, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.441, 10.271], loss: 0.001283, mae: 0.038417, mean_q: 1.243039
 68943/100000: episode: 1239, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 30.587, mean reward: 0.612 [0.542, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.970, 10.243], loss: 0.001563, mae: 0.042921, mean_q: 1.230066
 68962/100000: episode: 1240, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 13.650, mean reward: 0.718 [0.567, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.078, 10.333], loss: 0.001345, mae: 0.041158, mean_q: 1.229606
 69013/100000: episode: 1241, duration: 0.289s, episode steps: 51, steps per second: 177, episode reward: 31.669, mean reward: 0.621 [0.518, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-1.173, 10.141], loss: 0.001496, mae: 0.041873, mean_q: 1.230500
 69032/100000: episode: 1242, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 14.415, mean reward: 0.759 [0.710, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.505], loss: 0.001334, mae: 0.041098, mean_q: 1.225818
 69059/100000: episode: 1243, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 18.772, mean reward: 0.695 [0.549, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.553, 10.100], loss: 0.001603, mae: 0.043536, mean_q: 1.232964
 69110/100000: episode: 1244, duration: 0.273s, episode steps: 51, steps per second: 187, episode reward: 31.980, mean reward: 0.627 [0.535, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.969, 10.230], loss: 0.001704, mae: 0.045104, mean_q: 1.230335
 69161/100000: episode: 1245, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 31.832, mean reward: 0.624 [0.510, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.469, 10.348], loss: 0.001734, mae: 0.044840, mean_q: 1.229032
 69183/100000: episode: 1246, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 15.513, mean reward: 0.705 [0.625, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.331, 10.100], loss: 0.001482, mae: 0.042281, mean_q: 1.226107
 69230/100000: episode: 1247, duration: 0.271s, episode steps: 47, steps per second: 174, episode reward: 29.463, mean reward: 0.627 [0.522, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.469, 10.345], loss: 0.001435, mae: 0.041035, mean_q: 1.223443
 69280/100000: episode: 1248, duration: 0.274s, episode steps: 50, steps per second: 183, episode reward: 28.704, mean reward: 0.574 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.727, 10.199], loss: 0.001555, mae: 0.041956, mean_q: 1.232693
 69331/100000: episode: 1249, duration: 0.253s, episode steps: 51, steps per second: 201, episode reward: 29.793, mean reward: 0.584 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.328, 10.148], loss: 0.001440, mae: 0.040952, mean_q: 1.233328
 69358/100000: episode: 1250, duration: 0.165s, episode steps: 27, steps per second: 164, episode reward: 19.829, mean reward: 0.734 [0.614, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-1.190, 10.100], loss: 0.001437, mae: 0.041535, mean_q: 1.240386
 69408/100000: episode: 1251, duration: 0.283s, episode steps: 50, steps per second: 177, episode reward: 28.652, mean reward: 0.573 [0.511, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.240, 10.165], loss: 0.001341, mae: 0.039198, mean_q: 1.232777
 69466/100000: episode: 1252, duration: 0.327s, episode steps: 58, steps per second: 177, episode reward: 41.015, mean reward: 0.707 [0.603, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.830 [-0.826, 10.321], loss: 0.001538, mae: 0.042711, mean_q: 1.229170
[Info] FALSIFICATION!
 69470/100000: episode: 1253, duration: 0.182s, episode steps: 4, steps per second: 22, episode reward: 3.609, mean reward: 0.902 [0.822, 1.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.013, 10.198], loss: 0.001240, mae: 0.039683, mean_q: 1.186298
 69528/100000: episode: 1254, duration: 0.308s, episode steps: 58, steps per second: 189, episode reward: 35.015, mean reward: 0.604 [0.512, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-0.721, 10.236], loss: 0.001906, mae: 0.045504, mean_q: 1.233214
 69579/100000: episode: 1255, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 32.500, mean reward: 0.637 [0.524, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.289, 10.283], loss: 0.001801, mae: 0.044931, mean_q: 1.229351
 69601/100000: episode: 1256, duration: 0.131s, episode steps: 22, steps per second: 169, episode reward: 16.179, mean reward: 0.735 [0.620, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.412, 10.100], loss: 0.001543, mae: 0.041909, mean_q: 1.233656
 69648/100000: episode: 1257, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 28.137, mean reward: 0.599 [0.513, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.540, 10.274], loss: 0.001538, mae: 0.042693, mean_q: 1.238740
 69667/100000: episode: 1258, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 15.933, mean reward: 0.839 [0.703, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.787, 10.559], loss: 0.001838, mae: 0.044314, mean_q: 1.240220
 69722/100000: episode: 1259, duration: 0.299s, episode steps: 55, steps per second: 184, episode reward: 36.386, mean reward: 0.662 [0.533, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.865, 10.159], loss: 0.001477, mae: 0.042094, mean_q: 1.236293
[Info] Complete ISplit Iteration
[Info] Levels: [1.4112163, 1.6752653]
[Info] Cond. Prob: [0.1, 0.03]
[Info] Error Prob: 0.003

 69744/100000: episode: 1260, duration: 4.560s, episode steps: 22, steps per second: 5, episode reward: 14.847, mean reward: 0.675 [0.550, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.758, 10.100], loss: 0.001598, mae: 0.043204, mean_q: 1.241993
 69844/100000: episode: 1261, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 63.694, mean reward: 0.637 [0.497, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.912, 10.506], loss: 0.001419, mae: 0.041096, mean_q: 1.237214
 69944/100000: episode: 1262, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.965, mean reward: 0.590 [0.500, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.150, 10.214], loss: 0.001558, mae: 0.041602, mean_q: 1.240594
 70044/100000: episode: 1263, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.935, mean reward: 0.579 [0.512, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.834, 10.098], loss: 0.001561, mae: 0.042070, mean_q: 1.233748
 70144/100000: episode: 1264, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.154, mean reward: 0.592 [0.505, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.237, 10.098], loss: 0.001620, mae: 0.043372, mean_q: 1.235610
 70244/100000: episode: 1265, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.047, mean reward: 0.590 [0.506, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.393, 10.310], loss: 0.001594, mae: 0.043064, mean_q: 1.234443
 70344/100000: episode: 1266, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.239, mean reward: 0.592 [0.506, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.462, 10.098], loss: 0.001557, mae: 0.042761, mean_q: 1.238865
 70444/100000: episode: 1267, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.943, mean reward: 0.589 [0.509, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.547, 10.098], loss: 0.001468, mae: 0.042108, mean_q: 1.238206
 70544/100000: episode: 1268, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 57.124, mean reward: 0.571 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.189, 10.098], loss: 0.001566, mae: 0.042579, mean_q: 1.231848
 70644/100000: episode: 1269, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.526, mean reward: 0.585 [0.507, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.071, 10.266], loss: 0.001563, mae: 0.042064, mean_q: 1.240122
 70744/100000: episode: 1270, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.226, mean reward: 0.582 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.215, 10.099], loss: 0.001726, mae: 0.044821, mean_q: 1.241614
 70844/100000: episode: 1271, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.228, mean reward: 0.582 [0.499, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.911, 10.121], loss: 0.001538, mae: 0.042727, mean_q: 1.240356
 70944/100000: episode: 1272, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 57.251, mean reward: 0.573 [0.509, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.595, 10.098], loss: 0.001656, mae: 0.043761, mean_q: 1.233488
 71044/100000: episode: 1273, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.758, mean reward: 0.598 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.157, 10.098], loss: 0.001595, mae: 0.042627, mean_q: 1.234565
 71144/100000: episode: 1274, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.918, mean reward: 0.589 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.605, 10.099], loss: 0.001653, mae: 0.043531, mean_q: 1.225179
 71244/100000: episode: 1275, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 58.196, mean reward: 0.582 [0.504, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.624, 10.150], loss: 0.001662, mae: 0.043121, mean_q: 1.225144
 71344/100000: episode: 1276, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 61.182, mean reward: 0.612 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.123, 10.136], loss: 0.001690, mae: 0.044105, mean_q: 1.226945
 71444/100000: episode: 1277, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.261, mean reward: 0.583 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.706, 10.098], loss: 0.001651, mae: 0.044499, mean_q: 1.225440
 71544/100000: episode: 1278, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.478, mean reward: 0.595 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.193, 10.332], loss: 0.001723, mae: 0.043693, mean_q: 1.219361
 71644/100000: episode: 1279, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 61.597, mean reward: 0.616 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.775, 10.366], loss: 0.001455, mae: 0.041433, mean_q: 1.218704
 71744/100000: episode: 1280, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.166, mean reward: 0.582 [0.503, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.921, 10.098], loss: 0.001666, mae: 0.044032, mean_q: 1.222879
 71844/100000: episode: 1281, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.813, mean reward: 0.608 [0.508, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.300, 10.209], loss: 0.001699, mae: 0.044043, mean_q: 1.217278
 71944/100000: episode: 1282, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 58.872, mean reward: 0.589 [0.504, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.729, 10.098], loss: 0.001720, mae: 0.043802, mean_q: 1.218765
 72044/100000: episode: 1283, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 61.996, mean reward: 0.620 [0.524, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.013, 10.098], loss: 0.001667, mae: 0.043695, mean_q: 1.216224
 72144/100000: episode: 1284, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.204, mean reward: 0.582 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.932, 10.164], loss: 0.001648, mae: 0.043797, mean_q: 1.220487
 72244/100000: episode: 1285, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 61.685, mean reward: 0.617 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.758, 10.317], loss: 0.001664, mae: 0.043625, mean_q: 1.211031
 72344/100000: episode: 1286, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.585, mean reward: 0.586 [0.500, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.051, 10.204], loss: 0.001766, mae: 0.045152, mean_q: 1.211052
 72444/100000: episode: 1287, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.737, mean reward: 0.587 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.014, 10.098], loss: 0.001781, mae: 0.044478, mean_q: 1.209500
 72544/100000: episode: 1288, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.286, mean reward: 0.583 [0.502, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.041, 10.117], loss: 0.001649, mae: 0.044504, mean_q: 1.212230
 72644/100000: episode: 1289, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 63.478, mean reward: 0.635 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.765, 10.098], loss: 0.001965, mae: 0.046192, mean_q: 1.211579
 72744/100000: episode: 1290, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 60.227, mean reward: 0.602 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.835, 10.098], loss: 0.001663, mae: 0.043394, mean_q: 1.209544
 72844/100000: episode: 1291, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.892, mean reward: 0.589 [0.498, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.428, 10.098], loss: 0.001637, mae: 0.043986, mean_q: 1.205914
 72944/100000: episode: 1292, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.380, mean reward: 0.594 [0.499, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.183, 10.098], loss: 0.001879, mae: 0.046731, mean_q: 1.203734
 73044/100000: episode: 1293, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 56.659, mean reward: 0.567 [0.501, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.539, 10.177], loss: 0.001662, mae: 0.044637, mean_q: 1.201784
 73144/100000: episode: 1294, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.134, mean reward: 0.571 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.229, 10.113], loss: 0.001624, mae: 0.044216, mean_q: 1.198943
 73244/100000: episode: 1295, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.554, mean reward: 0.596 [0.505, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.875, 10.242], loss: 0.001925, mae: 0.046781, mean_q: 1.196928
 73344/100000: episode: 1296, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.844, mean reward: 0.578 [0.505, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.332, 10.191], loss: 0.001803, mae: 0.044928, mean_q: 1.192971
 73444/100000: episode: 1297, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 56.817, mean reward: 0.568 [0.501, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.310, 10.157], loss: 0.001904, mae: 0.046717, mean_q: 1.189485
 73544/100000: episode: 1298, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.709, mean reward: 0.587 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.573, 10.114], loss: 0.002015, mae: 0.046558, mean_q: 1.187833
 73644/100000: episode: 1299, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.484, mean reward: 0.575 [0.506, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.554, 10.098], loss: 0.001517, mae: 0.042549, mean_q: 1.190876
 73744/100000: episode: 1300, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.860, mean reward: 0.599 [0.500, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.265, 10.098], loss: 0.001777, mae: 0.045489, mean_q: 1.187670
 73844/100000: episode: 1301, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.525, mean reward: 0.605 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.360, 10.216], loss: 0.001745, mae: 0.045341, mean_q: 1.184808
 73944/100000: episode: 1302, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.493, mean reward: 0.595 [0.509, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.828, 10.098], loss: 0.001562, mae: 0.043067, mean_q: 1.180399
 74044/100000: episode: 1303, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.552, mean reward: 0.586 [0.511, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.437, 10.277], loss: 0.001696, mae: 0.043899, mean_q: 1.180940
 74144/100000: episode: 1304, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 58.462, mean reward: 0.585 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.587, 10.098], loss: 0.001644, mae: 0.043772, mean_q: 1.175565
 74244/100000: episode: 1305, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 60.975, mean reward: 0.610 [0.504, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.443, 10.184], loss: 0.001802, mae: 0.045473, mean_q: 1.181230
 74344/100000: episode: 1306, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.038, mean reward: 0.580 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.888, 10.098], loss: 0.001741, mae: 0.044931, mean_q: 1.177646
 74444/100000: episode: 1307, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 61.827, mean reward: 0.618 [0.506, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.815, 10.098], loss: 0.001668, mae: 0.044918, mean_q: 1.175898
 74544/100000: episode: 1308, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 56.569, mean reward: 0.566 [0.508, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.653, 10.119], loss: 0.001646, mae: 0.044726, mean_q: 1.175813
 74644/100000: episode: 1309, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 60.289, mean reward: 0.603 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.136, 10.130], loss: 0.001536, mae: 0.043136, mean_q: 1.173210
 74744/100000: episode: 1310, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 58.131, mean reward: 0.581 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.518, 10.196], loss: 0.001539, mae: 0.043054, mean_q: 1.169377
 74844/100000: episode: 1311, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 59.323, mean reward: 0.593 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.987, 10.380], loss: 0.001522, mae: 0.043102, mean_q: 1.171517
 74944/100000: episode: 1312, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.200, mean reward: 0.582 [0.499, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.671, 10.326], loss: 0.001450, mae: 0.041998, mean_q: 1.168418
 75044/100000: episode: 1313, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.792, mean reward: 0.578 [0.506, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.909, 10.098], loss: 0.001536, mae: 0.043429, mean_q: 1.164865
 75144/100000: episode: 1314, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.975, mean reward: 0.590 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.756, 10.187], loss: 0.001568, mae: 0.044156, mean_q: 1.168879
 75244/100000: episode: 1315, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.514, mean reward: 0.565 [0.500, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.447, 10.111], loss: 0.001576, mae: 0.044065, mean_q: 1.166189
 75344/100000: episode: 1316, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 63.812, mean reward: 0.638 [0.522, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.265, 10.098], loss: 0.001567, mae: 0.043579, mean_q: 1.166849
 75444/100000: episode: 1317, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.300, mean reward: 0.583 [0.512, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.834, 10.098], loss: 0.001651, mae: 0.045060, mean_q: 1.168150
 75544/100000: episode: 1318, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.099, mean reward: 0.601 [0.508, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.017, 10.098], loss: 0.001500, mae: 0.042761, mean_q: 1.170904
 75644/100000: episode: 1319, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.107, mean reward: 0.581 [0.509, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.805, 10.179], loss: 0.001521, mae: 0.043021, mean_q: 1.167557
 75744/100000: episode: 1320, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 61.789, mean reward: 0.618 [0.517, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.347, 10.351], loss: 0.001503, mae: 0.043049, mean_q: 1.167478
 75844/100000: episode: 1321, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.306, mean reward: 0.583 [0.502, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.346, 10.215], loss: 0.001522, mae: 0.042915, mean_q: 1.165274
 75944/100000: episode: 1322, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.090, mean reward: 0.571 [0.506, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.230, 10.098], loss: 0.001589, mae: 0.044231, mean_q: 1.171478
 76044/100000: episode: 1323, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 56.584, mean reward: 0.566 [0.505, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.359, 10.260], loss: 0.001495, mae: 0.042348, mean_q: 1.169775
 76144/100000: episode: 1324, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 59.399, mean reward: 0.594 [0.513, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.530, 10.098], loss: 0.001485, mae: 0.042069, mean_q: 1.168873
 76244/100000: episode: 1325, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.052, mean reward: 0.601 [0.511, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.020, 10.360], loss: 0.001530, mae: 0.043564, mean_q: 1.170019
 76344/100000: episode: 1326, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.857, mean reward: 0.589 [0.498, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.855, 10.331], loss: 0.001405, mae: 0.041416, mean_q: 1.169446
 76444/100000: episode: 1327, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.583, mean reward: 0.596 [0.515, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.598, 10.098], loss: 0.001430, mae: 0.041620, mean_q: 1.169216
 76544/100000: episode: 1328, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.693, mean reward: 0.607 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.836, 10.098], loss: 0.001461, mae: 0.042195, mean_q: 1.171297
 76644/100000: episode: 1329, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 60.241, mean reward: 0.602 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.738, 10.186], loss: 0.001520, mae: 0.042761, mean_q: 1.173157
 76744/100000: episode: 1330, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.640, mean reward: 0.606 [0.513, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.544, 10.098], loss: 0.001458, mae: 0.042262, mean_q: 1.168808
 76844/100000: episode: 1331, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 59.549, mean reward: 0.595 [0.507, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.701, 10.098], loss: 0.001502, mae: 0.042787, mean_q: 1.170726
 76944/100000: episode: 1332, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.157, mean reward: 0.572 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.715, 10.185], loss: 0.001467, mae: 0.042437, mean_q: 1.169684
 77044/100000: episode: 1333, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.744, mean reward: 0.577 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.145, 10.152], loss: 0.001492, mae: 0.042407, mean_q: 1.170824
 77144/100000: episode: 1334, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.829, mean reward: 0.578 [0.507, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.597, 10.098], loss: 0.001504, mae: 0.043245, mean_q: 1.170479
 77244/100000: episode: 1335, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.944, mean reward: 0.589 [0.507, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.343, 10.135], loss: 0.001485, mae: 0.042722, mean_q: 1.167072
 77344/100000: episode: 1336, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 62.420, mean reward: 0.624 [0.530, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.835, 10.098], loss: 0.001419, mae: 0.041792, mean_q: 1.165571
 77444/100000: episode: 1337, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.136, mean reward: 0.581 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.895, 10.300], loss: 0.001475, mae: 0.042528, mean_q: 1.167430
 77544/100000: episode: 1338, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.122, mean reward: 0.581 [0.509, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.074, 10.151], loss: 0.001392, mae: 0.041277, mean_q: 1.161332
 77644/100000: episode: 1339, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 63.253, mean reward: 0.633 [0.507, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.579, 10.498], loss: 0.001342, mae: 0.040469, mean_q: 1.163274
 77744/100000: episode: 1340, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 57.979, mean reward: 0.580 [0.500, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.271, 10.098], loss: 0.001322, mae: 0.040086, mean_q: 1.167197
 77844/100000: episode: 1341, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 59.081, mean reward: 0.591 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.206, 10.441], loss: 0.001303, mae: 0.039753, mean_q: 1.163772
 77944/100000: episode: 1342, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.079, mean reward: 0.581 [0.500, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.981, 10.169], loss: 0.001316, mae: 0.040251, mean_q: 1.162547
 78044/100000: episode: 1343, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 62.435, mean reward: 0.624 [0.522, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.561, 10.098], loss: 0.001391, mae: 0.041172, mean_q: 1.165861
 78144/100000: episode: 1344, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.255, mean reward: 0.583 [0.498, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.119], loss: 0.001319, mae: 0.040534, mean_q: 1.168758
 78244/100000: episode: 1345, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.027, mean reward: 0.590 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.206, 10.099], loss: 0.001209, mae: 0.038768, mean_q: 1.168286
 78344/100000: episode: 1346, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 61.982, mean reward: 0.620 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.033, 10.406], loss: 0.001357, mae: 0.040340, mean_q: 1.165731
 78444/100000: episode: 1347, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 62.494, mean reward: 0.625 [0.508, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.472, 10.098], loss: 0.001375, mae: 0.041819, mean_q: 1.169858
 78544/100000: episode: 1348, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.530, mean reward: 0.575 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.308, 10.167], loss: 0.001344, mae: 0.040251, mean_q: 1.169214
 78644/100000: episode: 1349, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 58.988, mean reward: 0.590 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.878, 10.098], loss: 0.001386, mae: 0.040817, mean_q: 1.171516
 78744/100000: episode: 1350, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.088, mean reward: 0.591 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.964, 10.098], loss: 0.001442, mae: 0.042121, mean_q: 1.172878
 78844/100000: episode: 1351, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 60.533, mean reward: 0.605 [0.506, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.375, 10.098], loss: 0.001468, mae: 0.041926, mean_q: 1.173866
 78944/100000: episode: 1352, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 62.988, mean reward: 0.630 [0.532, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.203, 10.098], loss: 0.001366, mae: 0.040705, mean_q: 1.170106
 79044/100000: episode: 1353, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.690, mean reward: 0.577 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.545, 10.098], loss: 0.001408, mae: 0.040619, mean_q: 1.170439
 79144/100000: episode: 1354, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.478, mean reward: 0.585 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.968, 10.194], loss: 0.001421, mae: 0.040976, mean_q: 1.171164
 79244/100000: episode: 1355, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 63.357, mean reward: 0.634 [0.523, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.159, 10.581], loss: 0.001424, mae: 0.041719, mean_q: 1.171883
 79344/100000: episode: 1356, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.365, mean reward: 0.584 [0.514, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.422, 10.120], loss: 0.001396, mae: 0.040646, mean_q: 1.172816
 79444/100000: episode: 1357, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 56.838, mean reward: 0.568 [0.513, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.026, 10.172], loss: 0.001443, mae: 0.041588, mean_q: 1.171284
 79544/100000: episode: 1358, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 58.771, mean reward: 0.588 [0.509, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.889, 10.098], loss: 0.001408, mae: 0.041288, mean_q: 1.173008
 79644/100000: episode: 1359, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 61.302, mean reward: 0.613 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.009, 10.098], loss: 0.001398, mae: 0.041034, mean_q: 1.173488
[Info] 1-TH LEVEL FOUND: 1.3947596549987793, Considering 10/90 traces
 79744/100000: episode: 1360, duration: 4.899s, episode steps: 100, steps per second: 20, episode reward: 58.108, mean reward: 0.581 [0.503, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.998, 10.198], loss: 0.001423, mae: 0.041298, mean_q: 1.176280
 79768/100000: episode: 1361, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 17.104, mean reward: 0.713 [0.639, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.693, 10.423], loss: 0.001476, mae: 0.042574, mean_q: 1.173965
 79785/100000: episode: 1362, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 12.745, mean reward: 0.750 [0.670, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.163, 10.100], loss: 0.001540, mae: 0.042682, mean_q: 1.170209
 79810/100000: episode: 1363, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 17.587, mean reward: 0.703 [0.654, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.624, 10.433], loss: 0.001665, mae: 0.044182, mean_q: 1.176271
 79823/100000: episode: 1364, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 8.852, mean reward: 0.681 [0.609, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.426], loss: 0.001416, mae: 0.040085, mean_q: 1.170327
 79843/100000: episode: 1365, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 14.255, mean reward: 0.713 [0.675, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.704, 10.471], loss: 0.001883, mae: 0.047436, mean_q: 1.177685
 79860/100000: episode: 1366, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 12.953, mean reward: 0.762 [0.679, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.471, 10.100], loss: 0.001633, mae: 0.044553, mean_q: 1.182891
 79884/100000: episode: 1367, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 16.088, mean reward: 0.670 [0.605, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.360], loss: 0.001334, mae: 0.039651, mean_q: 1.174126
 79908/100000: episode: 1368, duration: 0.134s, episode steps: 24, steps per second: 178, episode reward: 14.341, mean reward: 0.598 [0.540, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.289, 10.192], loss: 0.001470, mae: 0.042473, mean_q: 1.182675
 79921/100000: episode: 1369, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 8.995, mean reward: 0.692 [0.623, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.462], loss: 0.001266, mae: 0.039155, mean_q: 1.189784
 79934/100000: episode: 1370, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 8.024, mean reward: 0.617 [0.575, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.348], loss: 0.001282, mae: 0.038410, mean_q: 1.182782
 79956/100000: episode: 1371, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 14.618, mean reward: 0.664 [0.558, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.253], loss: 0.001407, mae: 0.041338, mean_q: 1.180856
 79980/100000: episode: 1372, duration: 0.141s, episode steps: 24, steps per second: 171, episode reward: 15.393, mean reward: 0.641 [0.556, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.521, 10.270], loss: 0.001318, mae: 0.039417, mean_q: 1.174129
 79997/100000: episode: 1373, duration: 0.109s, episode steps: 17, steps per second: 155, episode reward: 12.042, mean reward: 0.708 [0.628, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.286, 10.100], loss: 0.001531, mae: 0.042777, mean_q: 1.182707
 80014/100000: episode: 1374, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 12.504, mean reward: 0.736 [0.683, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.221, 10.100], loss: 0.001443, mae: 0.041560, mean_q: 1.182027
 80038/100000: episode: 1375, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 15.215, mean reward: 0.634 [0.535, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.306, 10.214], loss: 0.001618, mae: 0.044047, mean_q: 1.182571
 80058/100000: episode: 1376, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 12.528, mean reward: 0.626 [0.584, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.467, 10.356], loss: 0.001275, mae: 0.040021, mean_q: 1.168369
 80071/100000: episode: 1377, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 8.543, mean reward: 0.657 [0.543, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.265, 10.214], loss: 0.001283, mae: 0.039878, mean_q: 1.180038
 80088/100000: episode: 1378, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 10.927, mean reward: 0.643 [0.556, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.148, 10.100], loss: 0.001586, mae: 0.043151, mean_q: 1.181245
 80112/100000: episode: 1379, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 18.098, mean reward: 0.754 [0.680, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.634, 10.438], loss: 0.001611, mae: 0.043164, mean_q: 1.191092
 80129/100000: episode: 1380, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 12.377, mean reward: 0.728 [0.679, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.399, 10.100], loss: 0.001498, mae: 0.041221, mean_q: 1.184663
 80154/100000: episode: 1381, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 17.989, mean reward: 0.720 [0.620, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.093, 10.409], loss: 0.001964, mae: 0.047816, mean_q: 1.190634
 80178/100000: episode: 1382, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 15.944, mean reward: 0.664 [0.615, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.295], loss: 0.001757, mae: 0.045590, mean_q: 1.189206
 80202/100000: episode: 1383, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 16.612, mean reward: 0.692 [0.636, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.365], loss: 0.001223, mae: 0.038325, mean_q: 1.180347
 80226/100000: episode: 1384, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 16.065, mean reward: 0.669 [0.599, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.436, 10.329], loss: 0.001506, mae: 0.042717, mean_q: 1.194290
 80250/100000: episode: 1385, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 16.656, mean reward: 0.694 [0.585, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.389], loss: 0.001598, mae: 0.042798, mean_q: 1.188813
 80272/100000: episode: 1386, duration: 0.135s, episode steps: 22, steps per second: 162, episode reward: 14.691, mean reward: 0.668 [0.561, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.052, 10.380], loss: 0.001650, mae: 0.044727, mean_q: 1.191447
 80296/100000: episode: 1387, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 18.065, mean reward: 0.753 [0.671, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.617, 10.458], loss: 0.001450, mae: 0.041580, mean_q: 1.193607
 80318/100000: episode: 1388, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 14.653, mean reward: 0.666 [0.621, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.170, 10.340], loss: 0.001669, mae: 0.045235, mean_q: 1.186174
 80338/100000: episode: 1389, duration: 0.125s, episode steps: 20, steps per second: 160, episode reward: 12.055, mean reward: 0.603 [0.561, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.567, 10.342], loss: 0.001798, mae: 0.046399, mean_q: 1.196444
 80362/100000: episode: 1390, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 15.663, mean reward: 0.653 [0.601, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.298, 10.339], loss: 0.001599, mae: 0.042262, mean_q: 1.184238
 80384/100000: episode: 1391, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 15.258, mean reward: 0.694 [0.640, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.367, 10.346], loss: 0.001464, mae: 0.042121, mean_q: 1.192887
 80406/100000: episode: 1392, duration: 0.142s, episode steps: 22, steps per second: 155, episode reward: 16.227, mean reward: 0.738 [0.681, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.459], loss: 0.001872, mae: 0.047484, mean_q: 1.185300
 80426/100000: episode: 1393, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 13.868, mean reward: 0.693 [0.654, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.318, 10.515], loss: 0.001792, mae: 0.044789, mean_q: 1.196067
 80439/100000: episode: 1394, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 8.700, mean reward: 0.669 [0.632, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.344, 10.332], loss: 0.001437, mae: 0.041090, mean_q: 1.177297
 80456/100000: episode: 1395, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 12.101, mean reward: 0.712 [0.649, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.746, 10.100], loss: 0.001588, mae: 0.041864, mean_q: 1.193085
 80478/100000: episode: 1396, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 14.718, mean reward: 0.669 [0.606, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.355], loss: 0.001591, mae: 0.042677, mean_q: 1.190971
 80491/100000: episode: 1397, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 8.368, mean reward: 0.644 [0.588, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.240], loss: 0.001466, mae: 0.041161, mean_q: 1.194109
 80513/100000: episode: 1398, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 14.421, mean reward: 0.656 [0.560, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.365, 10.293], loss: 0.001497, mae: 0.041444, mean_q: 1.191501
 80535/100000: episode: 1399, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 14.019, mean reward: 0.637 [0.552, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.306, 10.246], loss: 0.001643, mae: 0.043011, mean_q: 1.204357
 80560/100000: episode: 1400, duration: 0.133s, episode steps: 25, steps per second: 187, episode reward: 17.199, mean reward: 0.688 [0.648, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.407], loss: 0.001559, mae: 0.041976, mean_q: 1.195819
 80573/100000: episode: 1401, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 8.309, mean reward: 0.639 [0.607, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.196, 10.356], loss: 0.001350, mae: 0.040165, mean_q: 1.195367
 80597/100000: episode: 1402, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 16.479, mean reward: 0.687 [0.570, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.608, 10.263], loss: 0.001551, mae: 0.041792, mean_q: 1.189080
 80610/100000: episode: 1403, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 8.989, mean reward: 0.691 [0.641, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.459], loss: 0.001446, mae: 0.040369, mean_q: 1.203137
 80630/100000: episode: 1404, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 13.747, mean reward: 0.687 [0.649, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.440], loss: 0.001506, mae: 0.041101, mean_q: 1.200312
 80650/100000: episode: 1405, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 13.522, mean reward: 0.676 [0.580, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.191], loss: 0.001734, mae: 0.044153, mean_q: 1.194218
 80670/100000: episode: 1406, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 13.111, mean reward: 0.656 [0.604, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.333], loss: 0.001779, mae: 0.044081, mean_q: 1.197458
 80692/100000: episode: 1407, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 13.554, mean reward: 0.616 [0.540, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.413, 10.308], loss: 0.001658, mae: 0.043188, mean_q: 1.186840
 80709/100000: episode: 1408, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 11.928, mean reward: 0.702 [0.646, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.292, 10.100], loss: 0.001574, mae: 0.043224, mean_q: 1.195413
 80722/100000: episode: 1409, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 8.868, mean reward: 0.682 [0.645, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.474], loss: 0.001374, mae: 0.041429, mean_q: 1.199453
 80746/100000: episode: 1410, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 15.055, mean reward: 0.627 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.108, 10.354], loss: 0.001366, mae: 0.039507, mean_q: 1.191388
 80763/100000: episode: 1411, duration: 0.110s, episode steps: 17, steps per second: 154, episode reward: 12.221, mean reward: 0.719 [0.681, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.223, 10.100], loss: 0.001783, mae: 0.044870, mean_q: 1.210301
 80783/100000: episode: 1412, duration: 0.108s, episode steps: 20, steps per second: 184, episode reward: 13.628, mean reward: 0.681 [0.630, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.464, 10.341], loss: 0.001738, mae: 0.043926, mean_q: 1.200111
 80807/100000: episode: 1413, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 16.541, mean reward: 0.689 [0.645, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.155, 10.450], loss: 0.001675, mae: 0.043612, mean_q: 1.193184
 80829/100000: episode: 1414, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 15.542, mean reward: 0.706 [0.633, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.349, 10.401], loss: 0.001769, mae: 0.046595, mean_q: 1.195593
 80853/100000: episode: 1415, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 16.926, mean reward: 0.705 [0.615, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.377, 10.376], loss: 0.001642, mae: 0.044211, mean_q: 1.201109
 80875/100000: episode: 1416, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 14.388, mean reward: 0.654 [0.529, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.382, 10.210], loss: 0.001381, mae: 0.041323, mean_q: 1.192840
 80900/100000: episode: 1417, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 17.087, mean reward: 0.683 [0.609, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.047, 10.367], loss: 0.001613, mae: 0.042483, mean_q: 1.203381
 80922/100000: episode: 1418, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 14.981, mean reward: 0.681 [0.625, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.263, 10.349], loss: 0.001788, mae: 0.045197, mean_q: 1.197548
 80947/100000: episode: 1419, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 16.604, mean reward: 0.664 [0.572, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.472, 10.338], loss: 0.001492, mae: 0.041512, mean_q: 1.208553
 80971/100000: episode: 1420, duration: 0.141s, episode steps: 24, steps per second: 171, episode reward: 15.426, mean reward: 0.643 [0.529, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.485, 10.233], loss: 0.001886, mae: 0.044988, mean_q: 1.196777
 80995/100000: episode: 1421, duration: 0.152s, episode steps: 24, steps per second: 158, episode reward: 16.706, mean reward: 0.696 [0.666, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.067, 10.382], loss: 0.001459, mae: 0.041889, mean_q: 1.198745
 81017/100000: episode: 1422, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 14.327, mean reward: 0.651 [0.592, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.511, 10.315], loss: 0.001507, mae: 0.042450, mean_q: 1.197544
 81039/100000: episode: 1423, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 15.779, mean reward: 0.717 [0.653, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.842, 10.394], loss: 0.001825, mae: 0.044281, mean_q: 1.202564
 81061/100000: episode: 1424, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 15.173, mean reward: 0.690 [0.569, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.214, 10.311], loss: 0.001645, mae: 0.044083, mean_q: 1.201453
 81086/100000: episode: 1425, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 17.922, mean reward: 0.717 [0.607, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.584, 10.543], loss: 0.001467, mae: 0.039870, mean_q: 1.203400
 81110/100000: episode: 1426, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 16.464, mean reward: 0.686 [0.600, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.521, 10.327], loss: 0.001507, mae: 0.041290, mean_q: 1.202027
 81134/100000: episode: 1427, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 17.413, mean reward: 0.726 [0.640, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.558, 10.588], loss: 0.001498, mae: 0.042648, mean_q: 1.200378
 81158/100000: episode: 1428, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 15.114, mean reward: 0.630 [0.512, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.917, 10.121], loss: 0.001693, mae: 0.043915, mean_q: 1.203326
 81178/100000: episode: 1429, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 11.911, mean reward: 0.596 [0.502, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.335, 10.150], loss: 0.001486, mae: 0.042100, mean_q: 1.216017
 81195/100000: episode: 1430, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 11.137, mean reward: 0.655 [0.605, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.336, 10.100], loss: 0.001935, mae: 0.045292, mean_q: 1.208506
 81219/100000: episode: 1431, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 17.014, mean reward: 0.709 [0.656, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.147, 10.452], loss: 0.001607, mae: 0.042964, mean_q: 1.211901
 81243/100000: episode: 1432, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 15.851, mean reward: 0.660 [0.613, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.167, 10.408], loss: 0.001575, mae: 0.043501, mean_q: 1.213124
 81263/100000: episode: 1433, duration: 0.132s, episode steps: 20, steps per second: 152, episode reward: 13.681, mean reward: 0.684 [0.628, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.394, 10.338], loss: 0.001523, mae: 0.041875, mean_q: 1.213762
 81285/100000: episode: 1434, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 15.510, mean reward: 0.705 [0.639, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.502], loss: 0.001539, mae: 0.041696, mean_q: 1.208947
 81307/100000: episode: 1435, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 14.212, mean reward: 0.646 [0.599, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.318], loss: 0.001590, mae: 0.041842, mean_q: 1.215561
 81329/100000: episode: 1436, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 14.654, mean reward: 0.666 [0.599, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.223, 10.421], loss: 0.001784, mae: 0.043221, mean_q: 1.212826
 81351/100000: episode: 1437, duration: 0.110s, episode steps: 22, steps per second: 201, episode reward: 13.388, mean reward: 0.609 [0.527, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.579, 10.204], loss: 0.001461, mae: 0.040470, mean_q: 1.223534
 81375/100000: episode: 1438, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 16.398, mean reward: 0.683 [0.575, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.439, 10.374], loss: 0.001320, mae: 0.038966, mean_q: 1.216396
 81399/100000: episode: 1439, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 14.622, mean reward: 0.609 [0.528, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.089, 10.170], loss: 0.001625, mae: 0.043161, mean_q: 1.201476
 81423/100000: episode: 1440, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 15.622, mean reward: 0.651 [0.577, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.616, 10.213], loss: 0.001702, mae: 0.043332, mean_q: 1.213982
 81447/100000: episode: 1441, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 16.169, mean reward: 0.674 [0.634, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.866, 10.434], loss: 0.001789, mae: 0.043797, mean_q: 1.208497
 81460/100000: episode: 1442, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 9.139, mean reward: 0.703 [0.625, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.348], loss: 0.001756, mae: 0.045532, mean_q: 1.206020
 81484/100000: episode: 1443, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 16.476, mean reward: 0.687 [0.606, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.397, 10.373], loss: 0.001703, mae: 0.043190, mean_q: 1.203856
 81508/100000: episode: 1444, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 17.229, mean reward: 0.718 [0.643, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.235, 10.469], loss: 0.001527, mae: 0.040512, mean_q: 1.211609
 81528/100000: episode: 1445, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 12.927, mean reward: 0.646 [0.580, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.399, 10.300], loss: 0.001453, mae: 0.040361, mean_q: 1.214573
 81553/100000: episode: 1446, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 15.637, mean reward: 0.625 [0.520, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.051, 10.184], loss: 0.001417, mae: 0.040619, mean_q: 1.212255
 81577/100000: episode: 1447, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 16.520, mean reward: 0.688 [0.617, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.049, 10.412], loss: 0.001682, mae: 0.042942, mean_q: 1.222298
 81594/100000: episode: 1448, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 12.507, mean reward: 0.736 [0.651, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.388, 10.100], loss: 0.001963, mae: 0.047149, mean_q: 1.220778
 81619/100000: episode: 1449, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 15.887, mean reward: 0.635 [0.560, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.339, 10.225], loss: 0.001987, mae: 0.047097, mean_q: 1.218916
[Info] 2-TH LEVEL FOUND: 1.5422958135604858, Considering 10/90 traces
 81641/100000: episode: 1450, duration: 4.365s, episode steps: 22, steps per second: 5, episode reward: 14.950, mean reward: 0.680 [0.611, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.408, 10.378], loss: 0.001625, mae: 0.043877, mean_q: 1.215379
 81663/100000: episode: 1451, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 16.511, mean reward: 0.751 [0.674, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.085, 10.430], loss: 0.001426, mae: 0.041127, mean_q: 1.224871
 81678/100000: episode: 1452, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 12.011, mean reward: 0.801 [0.746, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.278, 10.570], loss: 0.001443, mae: 0.041812, mean_q: 1.212536
 81698/100000: episode: 1453, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 14.504, mean reward: 0.725 [0.575, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.344], loss: 0.001747, mae: 0.044037, mean_q: 1.221596
 81718/100000: episode: 1454, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 13.798, mean reward: 0.690 [0.573, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.489, 10.367], loss: 0.001375, mae: 0.040639, mean_q: 1.219150
 81733/100000: episode: 1455, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 10.920, mean reward: 0.728 [0.649, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.691, 10.366], loss: 0.001403, mae: 0.040329, mean_q: 1.224016
 81754/100000: episode: 1456, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 14.854, mean reward: 0.707 [0.630, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.053, 10.380], loss: 0.001412, mae: 0.041214, mean_q: 1.231939
 81769/100000: episode: 1457, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 11.601, mean reward: 0.773 [0.736, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.489], loss: 0.001627, mae: 0.045302, mean_q: 1.229844
 81783/100000: episode: 1458, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 10.737, mean reward: 0.767 [0.704, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.437, 10.530], loss: 0.001434, mae: 0.040630, mean_q: 1.229941
 81790/100000: episode: 1459, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 5.687, mean reward: 0.812 [0.772, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-1.627, 10.100], loss: 0.001713, mae: 0.041431, mean_q: 1.207859
 81806/100000: episode: 1460, duration: 0.102s, episode steps: 16, steps per second: 157, episode reward: 12.087, mean reward: 0.755 [0.677, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.481], loss: 0.001695, mae: 0.042169, mean_q: 1.225243
 81828/100000: episode: 1461, duration: 0.144s, episode steps: 22, steps per second: 153, episode reward: 16.765, mean reward: 0.762 [0.711, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.487], loss: 0.001657, mae: 0.042890, mean_q: 1.232251
 81835/100000: episode: 1462, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 5.929, mean reward: 0.847 [0.762, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.414, 10.100], loss: 0.002411, mae: 0.049160, mean_q: 1.217129
 81851/100000: episode: 1463, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 11.441, mean reward: 0.715 [0.669, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.883, 10.578], loss: 0.001701, mae: 0.044546, mean_q: 1.229020
 81866/100000: episode: 1464, duration: 0.098s, episode steps: 15, steps per second: 153, episode reward: 10.872, mean reward: 0.725 [0.682, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.376, 10.547], loss: 0.001724, mae: 0.044170, mean_q: 1.231185
 81880/100000: episode: 1465, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 9.579, mean reward: 0.684 [0.640, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.387], loss: 0.001741, mae: 0.044089, mean_q: 1.227455
 81899/100000: episode: 1466, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 13.142, mean reward: 0.692 [0.645, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.344, 10.398], loss: 0.001295, mae: 0.038085, mean_q: 1.238988
 81918/100000: episode: 1467, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 14.290, mean reward: 0.752 [0.692, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.410, 10.427], loss: 0.001329, mae: 0.039724, mean_q: 1.238536
 81933/100000: episode: 1468, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 10.364, mean reward: 0.691 [0.641, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.809, 10.429], loss: 0.001700, mae: 0.044995, mean_q: 1.213622
 81940/100000: episode: 1469, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 5.570, mean reward: 0.796 [0.755, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.384, 10.100], loss: 0.001692, mae: 0.043626, mean_q: 1.220120
 81960/100000: episode: 1470, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 14.531, mean reward: 0.727 [0.648, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.350, 10.443], loss: 0.001605, mae: 0.041533, mean_q: 1.233078
 81980/100000: episode: 1471, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 14.685, mean reward: 0.734 [0.681, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.272, 10.417], loss: 0.001302, mae: 0.038383, mean_q: 1.232249
 82000/100000: episode: 1472, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 17.190, mean reward: 0.860 [0.762, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.651, 10.509], loss: 0.001510, mae: 0.040445, mean_q: 1.226883
 82014/100000: episode: 1473, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 10.990, mean reward: 0.785 [0.691, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.578], loss: 0.001613, mae: 0.043938, mean_q: 1.232947
 82029/100000: episode: 1474, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 10.902, mean reward: 0.727 [0.677, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.398], loss: 0.001520, mae: 0.039862, mean_q: 1.233201
 82045/100000: episode: 1475, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 11.713, mean reward: 0.732 [0.681, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.470], loss: 0.001460, mae: 0.041781, mean_q: 1.249169
 82066/100000: episode: 1476, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 15.754, mean reward: 0.750 [0.699, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.189, 10.592], loss: 0.001628, mae: 0.042360, mean_q: 1.236549
 82081/100000: episode: 1477, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 10.419, mean reward: 0.695 [0.656, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.487, 10.349], loss: 0.001554, mae: 0.041537, mean_q: 1.236911
 82102/100000: episode: 1478, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 15.297, mean reward: 0.728 [0.643, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.277, 10.550], loss: 0.001612, mae: 0.041729, mean_q: 1.236766
 82121/100000: episode: 1479, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 14.247, mean reward: 0.750 [0.696, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.649, 10.521], loss: 0.001689, mae: 0.044120, mean_q: 1.241282
 82137/100000: episode: 1480, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 12.062, mean reward: 0.754 [0.718, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.526], loss: 0.001609, mae: 0.043617, mean_q: 1.241245
 82156/100000: episode: 1481, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 12.614, mean reward: 0.664 [0.570, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.272], loss: 0.001841, mae: 0.044404, mean_q: 1.247507
 82163/100000: episode: 1482, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 5.533, mean reward: 0.790 [0.722, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.360, 10.100], loss: 0.002383, mae: 0.053786, mean_q: 1.236262
 82182/100000: episode: 1483, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 13.154, mean reward: 0.692 [0.598, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.434, 10.319], loss: 0.002063, mae: 0.045912, mean_q: 1.251920
 82203/100000: episode: 1484, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 16.841, mean reward: 0.802 [0.724, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.322, 10.586], loss: 0.001702, mae: 0.043319, mean_q: 1.248325
 82222/100000: episode: 1485, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 15.001, mean reward: 0.790 [0.739, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.565], loss: 0.001676, mae: 0.042800, mean_q: 1.243441
 82237/100000: episode: 1486, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 11.134, mean reward: 0.742 [0.705, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.040, 10.397], loss: 0.001561, mae: 0.041674, mean_q: 1.247488
 82258/100000: episode: 1487, duration: 0.131s, episode steps: 21, steps per second: 161, episode reward: 15.700, mean reward: 0.748 [0.689, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.675, 10.441], loss: 0.001626, mae: 0.041131, mean_q: 1.237895
 82265/100000: episode: 1488, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 6.139, mean reward: 0.877 [0.852, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.443, 10.100], loss: 0.001792, mae: 0.045988, mean_q: 1.260191
 82279/100000: episode: 1489, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 10.960, mean reward: 0.783 [0.725, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.394], loss: 0.001542, mae: 0.043082, mean_q: 1.251319
 82294/100000: episode: 1490, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 11.068, mean reward: 0.738 [0.698, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.507], loss: 0.002143, mae: 0.046352, mean_q: 1.258005
 82314/100000: episode: 1491, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 14.949, mean reward: 0.747 [0.620, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.467, 10.418], loss: 0.001814, mae: 0.045892, mean_q: 1.241084
 82335/100000: episode: 1492, duration: 0.136s, episode steps: 21, steps per second: 155, episode reward: 15.100, mean reward: 0.719 [0.605, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.244, 10.331], loss: 0.001575, mae: 0.042521, mean_q: 1.249230
 82350/100000: episode: 1493, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 10.880, mean reward: 0.725 [0.679, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.262, 10.371], loss: 0.001232, mae: 0.037965, mean_q: 1.251193
 82371/100000: episode: 1494, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 14.472, mean reward: 0.689 [0.625, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.083, 10.352], loss: 0.001523, mae: 0.041695, mean_q: 1.248247
 82392/100000: episode: 1495, duration: 0.132s, episode steps: 21, steps per second: 159, episode reward: 15.798, mean reward: 0.752 [0.690, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.800, 10.423], loss: 0.001524, mae: 0.041515, mean_q: 1.250394
 82407/100000: episode: 1496, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 12.144, mean reward: 0.810 [0.768, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.549], loss: 0.001337, mae: 0.040064, mean_q: 1.248693
 82428/100000: episode: 1497, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 13.705, mean reward: 0.653 [0.588, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.347, 10.341], loss: 0.002001, mae: 0.048088, mean_q: 1.259564
 82448/100000: episode: 1498, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 14.439, mean reward: 0.722 [0.663, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.096, 10.415], loss: 0.001314, mae: 0.038430, mean_q: 1.259685
 82467/100000: episode: 1499, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 12.535, mean reward: 0.660 [0.592, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.238, 10.360], loss: 0.001650, mae: 0.043713, mean_q: 1.260714
 82481/100000: episode: 1500, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 10.894, mean reward: 0.778 [0.740, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.585], loss: 0.001666, mae: 0.043899, mean_q: 1.257200
 82500/100000: episode: 1501, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 12.943, mean reward: 0.681 [0.589, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.389, 10.387], loss: 0.001361, mae: 0.040603, mean_q: 1.249601
 82522/100000: episode: 1502, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 15.608, mean reward: 0.709 [0.638, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.895, 10.377], loss: 0.001523, mae: 0.041117, mean_q: 1.250934
 82544/100000: episode: 1503, duration: 0.142s, episode steps: 22, steps per second: 154, episode reward: 13.841, mean reward: 0.629 [0.549, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.227], loss: 0.001594, mae: 0.043525, mean_q: 1.269372
 82564/100000: episode: 1504, duration: 0.117s, episode steps: 20, steps per second: 172, episode reward: 13.447, mean reward: 0.672 [0.591, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.340], loss: 0.001890, mae: 0.044898, mean_q: 1.254840
 82580/100000: episode: 1505, duration: 0.102s, episode steps: 16, steps per second: 157, episode reward: 10.972, mean reward: 0.686 [0.630, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.340], loss: 0.001472, mae: 0.041095, mean_q: 1.268805
 82594/100000: episode: 1506, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 11.046, mean reward: 0.789 [0.720, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.396, 10.540], loss: 0.001633, mae: 0.043827, mean_q: 1.265199
 82615/100000: episode: 1507, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 14.598, mean reward: 0.695 [0.592, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.464, 10.345], loss: 0.001706, mae: 0.042598, mean_q: 1.264223
 82635/100000: episode: 1508, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 17.140, mean reward: 0.857 [0.745, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.851, 10.615], loss: 0.001497, mae: 0.041932, mean_q: 1.260775
 82655/100000: episode: 1509, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 15.990, mean reward: 0.800 [0.741, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.594, 10.590], loss: 0.001436, mae: 0.041195, mean_q: 1.266023
 82670/100000: episode: 1510, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 10.422, mean reward: 0.695 [0.651, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.335], loss: 0.001962, mae: 0.047869, mean_q: 1.257381
 82686/100000: episode: 1511, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 10.713, mean reward: 0.670 [0.563, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.239], loss: 0.001677, mae: 0.043990, mean_q: 1.269549
 82706/100000: episode: 1512, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 14.659, mean reward: 0.733 [0.653, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.556, 10.360], loss: 0.001695, mae: 0.044832, mean_q: 1.260969
 82722/100000: episode: 1513, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 11.150, mean reward: 0.697 [0.629, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.223, 10.433], loss: 0.001524, mae: 0.041649, mean_q: 1.259249
 82744/100000: episode: 1514, duration: 0.147s, episode steps: 22, steps per second: 150, episode reward: 15.620, mean reward: 0.710 [0.632, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.264, 10.450], loss: 0.001297, mae: 0.038658, mean_q: 1.269670
 82764/100000: episode: 1515, duration: 0.132s, episode steps: 20, steps per second: 151, episode reward: 13.049, mean reward: 0.652 [0.566, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.267], loss: 0.001295, mae: 0.039486, mean_q: 1.270936
 82784/100000: episode: 1516, duration: 0.133s, episode steps: 20, steps per second: 151, episode reward: 14.849, mean reward: 0.742 [0.679, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.569], loss: 0.001685, mae: 0.044295, mean_q: 1.275841
 82806/100000: episode: 1517, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 16.761, mean reward: 0.762 [0.667, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.311, 10.584], loss: 0.001530, mae: 0.042324, mean_q: 1.262573
 82820/100000: episode: 1518, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 10.504, mean reward: 0.750 [0.720, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.466], loss: 0.001734, mae: 0.043255, mean_q: 1.273781
 82839/100000: episode: 1519, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 13.251, mean reward: 0.697 [0.581, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.347, 10.395], loss: 0.001432, mae: 0.041069, mean_q: 1.266453
 82846/100000: episode: 1520, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 5.883, mean reward: 0.840 [0.796, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.498, 10.100], loss: 0.001609, mae: 0.044706, mean_q: 1.243210
 82862/100000: episode: 1521, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 11.621, mean reward: 0.726 [0.640, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.482, 10.486], loss: 0.001388, mae: 0.040030, mean_q: 1.272494
 82882/100000: episode: 1522, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 14.098, mean reward: 0.705 [0.646, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.479, 10.415], loss: 0.001428, mae: 0.040914, mean_q: 1.256986
 82889/100000: episode: 1523, duration: 0.042s, episode steps: 7, steps per second: 169, episode reward: 5.868, mean reward: 0.838 [0.806, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.441, 10.100], loss: 0.001643, mae: 0.046706, mean_q: 1.287964
 82904/100000: episode: 1524, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 11.559, mean reward: 0.771 [0.753, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.310, 10.501], loss: 0.001221, mae: 0.037314, mean_q: 1.275114
 82925/100000: episode: 1525, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 14.982, mean reward: 0.713 [0.584, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.066, 10.260], loss: 0.001211, mae: 0.038301, mean_q: 1.269883
 82940/100000: episode: 1526, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 12.090, mean reward: 0.806 [0.690, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.648, 10.412], loss: 0.001390, mae: 0.040299, mean_q: 1.264733
 82960/100000: episode: 1527, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 15.432, mean reward: 0.772 [0.636, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.953, 10.366], loss: 0.001344, mae: 0.040546, mean_q: 1.288260
 82967/100000: episode: 1528, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 5.512, mean reward: 0.787 [0.748, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.409, 10.100], loss: 0.001309, mae: 0.038381, mean_q: 1.273780
 82974/100000: episode: 1529, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 5.932, mean reward: 0.847 [0.812, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.746, 10.100], loss: 0.001339, mae: 0.040541, mean_q: 1.281484
 82995/100000: episode: 1530, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 15.258, mean reward: 0.727 [0.688, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.483, 10.447], loss: 0.001697, mae: 0.043943, mean_q: 1.280859
 83015/100000: episode: 1531, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 15.329, mean reward: 0.766 [0.652, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.346, 10.454], loss: 0.001297, mae: 0.039678, mean_q: 1.280412
 83034/100000: episode: 1532, duration: 0.119s, episode steps: 19, steps per second: 160, episode reward: 13.667, mean reward: 0.719 [0.595, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.400], loss: 0.001204, mae: 0.037691, mean_q: 1.264802
 83056/100000: episode: 1533, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 15.668, mean reward: 0.712 [0.619, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.119, 10.516], loss: 0.001494, mae: 0.040996, mean_q: 1.276792
 83077/100000: episode: 1534, duration: 0.122s, episode steps: 21, steps per second: 171, episode reward: 14.258, mean reward: 0.679 [0.602, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.211, 10.354], loss: 0.001290, mae: 0.039605, mean_q: 1.275851
 83099/100000: episode: 1535, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 15.265, mean reward: 0.694 [0.606, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.349], loss: 0.001531, mae: 0.041865, mean_q: 1.300368
 83114/100000: episode: 1536, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 10.022, mean reward: 0.668 [0.605, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.746, 10.289], loss: 0.001751, mae: 0.044266, mean_q: 1.281294
 83133/100000: episode: 1537, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 14.130, mean reward: 0.744 [0.674, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.304, 10.494], loss: 0.001351, mae: 0.040228, mean_q: 1.282294
 83155/100000: episode: 1538, duration: 0.125s, episode steps: 22, steps per second: 177, episode reward: 16.441, mean reward: 0.747 [0.634, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.789, 10.431], loss: 0.001575, mae: 0.042431, mean_q: 1.286795
 83174/100000: episode: 1539, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 14.034, mean reward: 0.739 [0.641, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.237, 10.421], loss: 0.001507, mae: 0.041342, mean_q: 1.279416
[Info] 3-TH LEVEL FOUND: 1.5893034934997559, Considering 10/90 traces
 83188/100000: episode: 1540, duration: 4.371s, episode steps: 14, steps per second: 3, episode reward: 10.792, mean reward: 0.771 [0.652, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-1.241, 10.424], loss: 0.001477, mae: 0.040900, mean_q: 1.278091
 83203/100000: episode: 1541, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 12.915, mean reward: 0.861 [0.791, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.725], loss: 0.001469, mae: 0.042648, mean_q: 1.274729
 83219/100000: episode: 1542, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 12.589, mean reward: 0.787 [0.739, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.532], loss: 0.001480, mae: 0.042080, mean_q: 1.289128
 83232/100000: episode: 1543, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 10.370, mean reward: 0.798 [0.693, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.313, 10.560], loss: 0.001347, mae: 0.040132, mean_q: 1.281546
 83248/100000: episode: 1544, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 13.387, mean reward: 0.837 [0.766, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.679], loss: 0.001450, mae: 0.041438, mean_q: 1.290069
 83260/100000: episode: 1545, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 9.258, mean reward: 0.772 [0.698, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.166, 10.464], loss: 0.001207, mae: 0.037422, mean_q: 1.296196
 83278/100000: episode: 1546, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 13.954, mean reward: 0.775 [0.744, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.394, 10.556], loss: 0.001268, mae: 0.038888, mean_q: 1.297310
 83296/100000: episode: 1547, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 14.247, mean reward: 0.791 [0.735, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.069, 10.576], loss: 0.001136, mae: 0.036946, mean_q: 1.277198
 83311/100000: episode: 1548, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 11.633, mean reward: 0.776 [0.711, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.518], loss: 0.001405, mae: 0.040723, mean_q: 1.292171
 83324/100000: episode: 1549, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 10.542, mean reward: 0.811 [0.709, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.550], loss: 0.001457, mae: 0.041354, mean_q: 1.289645
 83339/100000: episode: 1550, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 11.327, mean reward: 0.755 [0.643, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.363], loss: 0.001751, mae: 0.044275, mean_q: 1.287528
 83351/100000: episode: 1551, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 9.991, mean reward: 0.833 [0.783, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.713], loss: 0.001321, mae: 0.039135, mean_q: 1.305890
 83365/100000: episode: 1552, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 10.893, mean reward: 0.778 [0.738, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.183, 10.544], loss: 0.001289, mae: 0.039419, mean_q: 1.287725
 83379/100000: episode: 1553, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 11.283, mean reward: 0.806 [0.725, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.554], loss: 0.001423, mae: 0.040770, mean_q: 1.305222
 83393/100000: episode: 1554, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 11.055, mean reward: 0.790 [0.709, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.601, 10.549], loss: 0.001221, mae: 0.039058, mean_q: 1.301561
 83406/100000: episode: 1555, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 10.161, mean reward: 0.782 [0.692, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-1.004, 10.499], loss: 0.001285, mae: 0.040019, mean_q: 1.280342
 83417/100000: episode: 1556, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 9.298, mean reward: 0.845 [0.791, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.501, 10.582], loss: 0.001637, mae: 0.043123, mean_q: 1.309546
 83431/100000: episode: 1557, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.995, mean reward: 0.785 [0.730, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.495], loss: 0.001536, mae: 0.042955, mean_q: 1.311577
 83444/100000: episode: 1558, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 10.780, mean reward: 0.829 [0.710, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.121, 10.569], loss: 0.001553, mae: 0.042163, mean_q: 1.281925
 83458/100000: episode: 1559, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 10.817, mean reward: 0.773 [0.673, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.473, 10.551], loss: 0.001413, mae: 0.042711, mean_q: 1.293989
 83469/100000: episode: 1560, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 8.206, mean reward: 0.746 [0.683, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-1.238, 10.307], loss: 0.001552, mae: 0.043149, mean_q: 1.297619
 83485/100000: episode: 1561, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 11.994, mean reward: 0.750 [0.706, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.630, 10.474], loss: 0.001413, mae: 0.040719, mean_q: 1.303522
 83499/100000: episode: 1562, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 10.556, mean reward: 0.754 [0.692, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.985, 10.599], loss: 0.001557, mae: 0.042488, mean_q: 1.299260
 83512/100000: episode: 1563, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 10.150, mean reward: 0.781 [0.724, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.072, 10.593], loss: 0.001687, mae: 0.043472, mean_q: 1.293840
 83523/100000: episode: 1564, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 8.321, mean reward: 0.756 [0.723, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.563, 10.552], loss: 0.001463, mae: 0.043274, mean_q: 1.311161
 83534/100000: episode: 1565, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 8.993, mean reward: 0.818 [0.773, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.393, 10.570], loss: 0.001902, mae: 0.047646, mean_q: 1.313485
 83548/100000: episode: 1566, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.592, mean reward: 0.757 [0.689, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.270, 10.589], loss: 0.001835, mae: 0.047051, mean_q: 1.301884
 83562/100000: episode: 1567, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 11.219, mean reward: 0.801 [0.735, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.529], loss: 0.001244, mae: 0.039052, mean_q: 1.283747
 83577/100000: episode: 1568, duration: 0.095s, episode steps: 15, steps per second: 159, episode reward: 11.362, mean reward: 0.757 [0.680, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.435], loss: 0.001224, mae: 0.038641, mean_q: 1.304555
 83590/100000: episode: 1569, duration: 0.090s, episode steps: 13, steps per second: 145, episode reward: 10.427, mean reward: 0.802 [0.745, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.078, 10.504], loss: 0.001174, mae: 0.038187, mean_q: 1.314388
 83604/100000: episode: 1570, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 10.594, mean reward: 0.757 [0.664, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.744, 10.381], loss: 0.001408, mae: 0.041788, mean_q: 1.300829
 83618/100000: episode: 1571, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 10.275, mean reward: 0.734 [0.694, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.465], loss: 0.001187, mae: 0.037263, mean_q: 1.298638
 83631/100000: episode: 1572, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.429, mean reward: 0.725 [0.686, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.423], loss: 0.001212, mae: 0.038947, mean_q: 1.329042
 83649/100000: episode: 1573, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 14.421, mean reward: 0.801 [0.737, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.223, 10.656], loss: 0.001199, mae: 0.039278, mean_q: 1.326544
 83667/100000: episode: 1574, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 13.511, mean reward: 0.751 [0.615, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.177, 10.420], loss: 0.001678, mae: 0.043625, mean_q: 1.316464
 83682/100000: episode: 1575, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 10.801, mean reward: 0.720 [0.620, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.336], loss: 0.001115, mae: 0.035921, mean_q: 1.301452
 83700/100000: episode: 1576, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 13.840, mean reward: 0.769 [0.661, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.265, 10.614], loss: 0.001439, mae: 0.041508, mean_q: 1.298665
 83714/100000: episode: 1577, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 10.267, mean reward: 0.733 [0.710, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.437], loss: 0.001458, mae: 0.041456, mean_q: 1.306300
 83725/100000: episode: 1578, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 8.685, mean reward: 0.790 [0.729, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.543], loss: 0.001290, mae: 0.039137, mean_q: 1.313193
 83736/100000: episode: 1579, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 9.092, mean reward: 0.827 [0.781, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.438, 10.443], loss: 0.001112, mae: 0.038097, mean_q: 1.313742
 83752/100000: episode: 1580, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 11.584, mean reward: 0.724 [0.644, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.395], loss: 0.001263, mae: 0.038386, mean_q: 1.311277
 83766/100000: episode: 1581, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 10.735, mean reward: 0.767 [0.681, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.353], loss: 0.001315, mae: 0.040328, mean_q: 1.330622
 83780/100000: episode: 1582, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 10.814, mean reward: 0.772 [0.700, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.082, 10.535], loss: 0.001273, mae: 0.039932, mean_q: 1.294494
 83796/100000: episode: 1583, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 11.320, mean reward: 0.707 [0.633, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.698, 10.385], loss: 0.001494, mae: 0.041735, mean_q: 1.318507
 83814/100000: episode: 1584, duration: 0.113s, episode steps: 18, steps per second: 160, episode reward: 12.903, mean reward: 0.717 [0.676, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.046, 10.415], loss: 0.001384, mae: 0.040412, mean_q: 1.315449
 83828/100000: episode: 1585, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 11.451, mean reward: 0.818 [0.737, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.369, 10.475], loss: 0.001666, mae: 0.043885, mean_q: 1.303696
 83846/100000: episode: 1586, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 13.106, mean reward: 0.728 [0.598, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.279], loss: 0.001513, mae: 0.041643, mean_q: 1.314599
 83861/100000: episode: 1587, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 12.363, mean reward: 0.824 [0.737, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.314, 10.435], loss: 0.001116, mae: 0.037823, mean_q: 1.314096
 83872/100000: episode: 1588, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 8.926, mean reward: 0.811 [0.778, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.566], loss: 0.001142, mae: 0.037778, mean_q: 1.303239
 83886/100000: episode: 1589, duration: 0.087s, episode steps: 14, steps per second: 160, episode reward: 10.836, mean reward: 0.774 [0.744, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.509], loss: 0.001504, mae: 0.043559, mean_q: 1.342634
 83897/100000: episode: 1590, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 8.940, mean reward: 0.813 [0.757, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.157, 10.610], loss: 0.001628, mae: 0.042548, mean_q: 1.303816
 83910/100000: episode: 1591, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 11.508, mean reward: 0.885 [0.845, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.629], loss: 0.001345, mae: 0.041614, mean_q: 1.315560
 83924/100000: episode: 1592, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 10.559, mean reward: 0.754 [0.716, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.522], loss: 0.001406, mae: 0.041957, mean_q: 1.329624
 83936/100000: episode: 1593, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 9.559, mean reward: 0.797 [0.729, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.599], loss: 0.001414, mae: 0.041366, mean_q: 1.333016
 83952/100000: episode: 1594, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 12.706, mean reward: 0.794 [0.749, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-1.090, 10.506], loss: 0.001434, mae: 0.039845, mean_q: 1.336871
 83970/100000: episode: 1595, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 14.458, mean reward: 0.803 [0.693, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.503], loss: 0.001259, mae: 0.039429, mean_q: 1.330024
 83985/100000: episode: 1596, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 11.403, mean reward: 0.760 [0.667, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.470], loss: 0.001516, mae: 0.042708, mean_q: 1.326764
 84000/100000: episode: 1597, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 12.088, mean reward: 0.806 [0.724, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.184, 10.382], loss: 0.001691, mae: 0.043970, mean_q: 1.328688
 84015/100000: episode: 1598, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 10.352, mean reward: 0.690 [0.608, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.869, 10.336], loss: 0.001289, mae: 0.040378, mean_q: 1.321157
 84029/100000: episode: 1599, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.710, mean reward: 0.765 [0.644, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.457], loss: 0.001343, mae: 0.040202, mean_q: 1.324388
 84047/100000: episode: 1600, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 13.319, mean reward: 0.740 [0.654, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.258, 10.422], loss: 0.001354, mae: 0.041619, mean_q: 1.350398
 84058/100000: episode: 1601, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 8.907, mean reward: 0.810 [0.718, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.365, 10.424], loss: 0.001484, mae: 0.042589, mean_q: 1.342377
 84074/100000: episode: 1602, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 12.286, mean reward: 0.768 [0.699, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.482], loss: 0.001469, mae: 0.041535, mean_q: 1.319506
 84089/100000: episode: 1603, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 11.799, mean reward: 0.787 [0.726, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.440, 10.490], loss: 0.001373, mae: 0.040436, mean_q: 1.322004
 84104/100000: episode: 1604, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 10.977, mean reward: 0.732 [0.682, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.482], loss: 0.001299, mae: 0.039712, mean_q: 1.334684
 84117/100000: episode: 1605, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 10.166, mean reward: 0.782 [0.736, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.831, 10.480], loss: 0.001404, mae: 0.041119, mean_q: 1.348306
[Info] FALSIFICATION!
 84128/100000: episode: 1606, duration: 0.235s, episode steps: 11, steps per second: 47, episode reward: 9.790, mean reward: 0.890 [0.820, 1.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.060, 10.449], loss: 0.001524, mae: 0.043425, mean_q: 1.349134
 84142/100000: episode: 1607, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 11.766, mean reward: 0.840 [0.779, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.582], loss: 0.001559, mae: 0.042540, mean_q: 1.316325
 84157/100000: episode: 1608, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 11.092, mean reward: 0.739 [0.637, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.396, 10.432], loss: 0.001168, mae: 0.038252, mean_q: 1.345398
 84170/100000: episode: 1609, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 11.089, mean reward: 0.853 [0.776, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.427, 10.612], loss: 0.001427, mae: 0.039116, mean_q: 1.339406
 84185/100000: episode: 1610, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 10.866, mean reward: 0.724 [0.576, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.683, 10.292], loss: 0.002212, mae: 0.048406, mean_q: 1.337643
 84199/100000: episode: 1611, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 10.978, mean reward: 0.784 [0.726, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.157, 10.513], loss: 0.001661, mae: 0.044417, mean_q: 1.324732
 84210/100000: episode: 1612, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 8.452, mean reward: 0.768 [0.696, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.428], loss: 0.001273, mae: 0.038827, mean_q: 1.341010
 84225/100000: episode: 1613, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 10.957, mean reward: 0.730 [0.639, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.422], loss: 0.001429, mae: 0.042123, mean_q: 1.340971
 84241/100000: episode: 1614, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 11.476, mean reward: 0.717 [0.650, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.468], loss: 0.001680, mae: 0.047380, mean_q: 1.355342
 84255/100000: episode: 1615, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 9.834, mean reward: 0.702 [0.625, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.402, 10.453], loss: 0.001287, mae: 0.039598, mean_q: 1.348327
 84267/100000: episode: 1616, duration: 0.075s, episode steps: 12, steps per second: 159, episode reward: 9.468, mean reward: 0.789 [0.718, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.495], loss: 0.001556, mae: 0.042508, mean_q: 1.329321
 84280/100000: episode: 1617, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 9.587, mean reward: 0.737 [0.610, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.483], loss: 0.001457, mae: 0.042881, mean_q: 1.368833
 84298/100000: episode: 1618, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 13.977, mean reward: 0.777 [0.721, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.504], loss: 0.001280, mae: 0.040080, mean_q: 1.356335
 84316/100000: episode: 1619, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 13.211, mean reward: 0.734 [0.671, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.460], loss: 0.001735, mae: 0.046622, mean_q: 1.358736
 84330/100000: episode: 1620, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 10.616, mean reward: 0.758 [0.678, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.488], loss: 0.001554, mae: 0.043405, mean_q: 1.345857
 84344/100000: episode: 1621, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 10.884, mean reward: 0.777 [0.734, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.282, 10.386], loss: 0.001226, mae: 0.039820, mean_q: 1.341695
 84358/100000: episode: 1622, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 10.631, mean reward: 0.759 [0.694, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.397], loss: 0.001251, mae: 0.038777, mean_q: 1.333223
 84371/100000: episode: 1623, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 10.280, mean reward: 0.791 [0.749, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.612], loss: 0.001199, mae: 0.037841, mean_q: 1.362638
 84389/100000: episode: 1624, duration: 0.103s, episode steps: 18, steps per second: 176, episode reward: 14.294, mean reward: 0.794 [0.688, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.694, 10.526], loss: 0.001320, mae: 0.040642, mean_q: 1.364633
 84407/100000: episode: 1625, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 13.800, mean reward: 0.767 [0.720, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.323, 10.466], loss: 0.001443, mae: 0.037382, mean_q: 1.354616
 84421/100000: episode: 1626, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 9.947, mean reward: 0.710 [0.632, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.786, 10.377], loss: 0.001415, mae: 0.041123, mean_q: 1.339689
 84432/100000: episode: 1627, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 8.608, mean reward: 0.783 [0.726, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.199, 10.575], loss: 0.001357, mae: 0.040541, mean_q: 1.367580
 84448/100000: episode: 1628, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 11.772, mean reward: 0.736 [0.692, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.280], loss: 0.001127, mae: 0.036309, mean_q: 1.352052
 84463/100000: episode: 1629, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 11.326, mean reward: 0.755 [0.686, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.527], loss: 0.001469, mae: 0.041759, mean_q: 1.357109
[Info] Complete ISplit Iteration
[Info] Levels: [1.3947597, 1.5422958, 1.5893035, 1.6810311]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.05]
[Info] Error Prob: 5.0000000000000016e-05

 84477/100000: episode: 1630, duration: 4.596s, episode steps: 14, steps per second: 3, episode reward: 10.432, mean reward: 0.745 [0.660, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.512], loss: 0.001409, mae: 0.041899, mean_q: 1.342852
 84577/100000: episode: 1631, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 57.951, mean reward: 0.580 [0.499, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.446, 10.194], loss: 0.001296, mae: 0.038852, mean_q: 1.363190
 84677/100000: episode: 1632, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.215, mean reward: 0.592 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.212, 10.098], loss: 0.001426, mae: 0.041444, mean_q: 1.354293
 84777/100000: episode: 1633, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 56.786, mean reward: 0.568 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.419, 10.098], loss: 0.001444, mae: 0.039555, mean_q: 1.349744
 84877/100000: episode: 1634, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.710, mean reward: 0.607 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.093, 10.098], loss: 0.001453, mae: 0.041429, mean_q: 1.346027
 84977/100000: episode: 1635, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 60.630, mean reward: 0.606 [0.508, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.689, 10.264], loss: 0.001281, mae: 0.038963, mean_q: 1.335006
 85077/100000: episode: 1636, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 57.463, mean reward: 0.575 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.307, 10.225], loss: 0.001459, mae: 0.042399, mean_q: 1.343420
 85177/100000: episode: 1637, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 59.294, mean reward: 0.593 [0.515, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.414, 10.098], loss: 0.001476, mae: 0.041146, mean_q: 1.342382
 85277/100000: episode: 1638, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.972, mean reward: 0.610 [0.509, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.132, 10.266], loss: 0.001474, mae: 0.040354, mean_q: 1.333302
 85377/100000: episode: 1639, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 59.934, mean reward: 0.599 [0.516, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.389, 10.098], loss: 0.001253, mae: 0.038878, mean_q: 1.338561
 85477/100000: episode: 1640, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 62.394, mean reward: 0.624 [0.510, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.720, 10.456], loss: 0.001558, mae: 0.042622, mean_q: 1.332733
 85577/100000: episode: 1641, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.081, mean reward: 0.581 [0.498, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.691, 10.098], loss: 0.001525, mae: 0.041851, mean_q: 1.323505
 85677/100000: episode: 1642, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 59.924, mean reward: 0.599 [0.506, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.688, 10.213], loss: 0.001427, mae: 0.041321, mean_q: 1.330831
 85777/100000: episode: 1643, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.251, mean reward: 0.573 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.534, 10.114], loss: 0.001480, mae: 0.041651, mean_q: 1.327685
 85877/100000: episode: 1644, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 60.310, mean reward: 0.603 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.611, 10.282], loss: 0.001509, mae: 0.042526, mean_q: 1.320574
 85977/100000: episode: 1645, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 58.233, mean reward: 0.582 [0.509, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.478, 10.098], loss: 0.001492, mae: 0.041808, mean_q: 1.323887
 86077/100000: episode: 1646, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 60.170, mean reward: 0.602 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.144, 10.098], loss: 0.001520, mae: 0.040885, mean_q: 1.323540
 86177/100000: episode: 1647, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.759, mean reward: 0.598 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.988, 10.116], loss: 0.001437, mae: 0.041663, mean_q: 1.316749
 86277/100000: episode: 1648, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.025, mean reward: 0.600 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.004, 10.281], loss: 0.001584, mae: 0.041799, mean_q: 1.312210
 86377/100000: episode: 1649, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.666, mean reward: 0.577 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.913, 10.098], loss: 0.001455, mae: 0.041652, mean_q: 1.313086
 86477/100000: episode: 1650, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.802, mean reward: 0.608 [0.516, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.552, 10.184], loss: 0.001674, mae: 0.043851, mean_q: 1.312273
 86577/100000: episode: 1651, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.599, mean reward: 0.586 [0.500, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.317, 10.272], loss: 0.001493, mae: 0.042206, mean_q: 1.308083
 86677/100000: episode: 1652, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.332, mean reward: 0.573 [0.501, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.058, 10.098], loss: 0.001961, mae: 0.045467, mean_q: 1.299593
 86777/100000: episode: 1653, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.432, mean reward: 0.584 [0.508, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.028, 10.257], loss: 0.001399, mae: 0.040426, mean_q: 1.297285
 86877/100000: episode: 1654, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.432, mean reward: 0.594 [0.506, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.315, 10.098], loss: 0.001756, mae: 0.044650, mean_q: 1.288577
 86977/100000: episode: 1655, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.433, mean reward: 0.594 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.618, 10.216], loss: 0.001545, mae: 0.042554, mean_q: 1.292437
 87077/100000: episode: 1656, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 57.792, mean reward: 0.578 [0.505, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.870, 10.098], loss: 0.001535, mae: 0.042041, mean_q: 1.286574
 87177/100000: episode: 1657, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.892, mean reward: 0.589 [0.501, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.260, 10.098], loss: 0.001524, mae: 0.041798, mean_q: 1.270363
 87277/100000: episode: 1658, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.886, mean reward: 0.579 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.632, 10.176], loss: 0.001474, mae: 0.041447, mean_q: 1.272041
 87377/100000: episode: 1659, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 63.665, mean reward: 0.637 [0.507, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.673, 10.377], loss: 0.001747, mae: 0.044765, mean_q: 1.259820
 87477/100000: episode: 1660, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.854, mean reward: 0.589 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.485, 10.098], loss: 0.001589, mae: 0.042651, mean_q: 1.268607
 87577/100000: episode: 1661, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.004, mean reward: 0.590 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.596, 10.165], loss: 0.001556, mae: 0.042811, mean_q: 1.255482
 87677/100000: episode: 1662, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 60.990, mean reward: 0.610 [0.513, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.823, 10.098], loss: 0.001574, mae: 0.042709, mean_q: 1.258590
 87777/100000: episode: 1663, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 59.657, mean reward: 0.597 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.279, 10.098], loss: 0.001658, mae: 0.044920, mean_q: 1.259098
 87877/100000: episode: 1664, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.720, mean reward: 0.577 [0.498, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.634, 10.119], loss: 0.001745, mae: 0.045059, mean_q: 1.249700
 87977/100000: episode: 1665, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.924, mean reward: 0.589 [0.511, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.854, 10.193], loss: 0.001575, mae: 0.043529, mean_q: 1.244561
 88077/100000: episode: 1666, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 63.299, mean reward: 0.633 [0.522, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.238, 10.098], loss: 0.001666, mae: 0.044533, mean_q: 1.245587
 88177/100000: episode: 1667, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.663, mean reward: 0.577 [0.501, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.723, 10.135], loss: 0.001618, mae: 0.044057, mean_q: 1.243223
 88277/100000: episode: 1668, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.553, mean reward: 0.576 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.952, 10.187], loss: 0.001511, mae: 0.042095, mean_q: 1.229953
 88377/100000: episode: 1669, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.678, mean reward: 0.597 [0.509, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.117, 10.270], loss: 0.001570, mae: 0.043380, mean_q: 1.217256
 88477/100000: episode: 1670, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.670, mean reward: 0.587 [0.506, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.069, 10.105], loss: 0.001524, mae: 0.043207, mean_q: 1.225059
 88577/100000: episode: 1671, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.948, mean reward: 0.589 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.644, 10.098], loss: 0.001410, mae: 0.041636, mean_q: 1.219499
 88677/100000: episode: 1672, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.302, mean reward: 0.593 [0.500, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.049, 10.098], loss: 0.001666, mae: 0.043838, mean_q: 1.214351
 88777/100000: episode: 1673, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.302, mean reward: 0.593 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.386, 10.183], loss: 0.001613, mae: 0.043153, mean_q: 1.204587
 88877/100000: episode: 1674, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.659, mean reward: 0.597 [0.511, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.050, 10.098], loss: 0.001561, mae: 0.043138, mean_q: 1.202044
 88977/100000: episode: 1675, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.576, mean reward: 0.596 [0.510, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.558, 10.098], loss: 0.001765, mae: 0.045297, mean_q: 1.197708
 89077/100000: episode: 1676, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 61.200, mean reward: 0.612 [0.503, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.294, 10.098], loss: 0.001655, mae: 0.044763, mean_q: 1.189981
 89177/100000: episode: 1677, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.261, mean reward: 0.573 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.090, 10.098], loss: 0.001647, mae: 0.043734, mean_q: 1.187813
 89277/100000: episode: 1678, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.467, mean reward: 0.605 [0.517, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.298, 10.146], loss: 0.001627, mae: 0.044313, mean_q: 1.183549
 89377/100000: episode: 1679, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.652, mean reward: 0.577 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.533, 10.103], loss: 0.001490, mae: 0.042289, mean_q: 1.174165
 89477/100000: episode: 1680, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 60.293, mean reward: 0.603 [0.510, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.593, 10.308], loss: 0.001646, mae: 0.044744, mean_q: 1.171915
 89577/100000: episode: 1681, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 60.673, mean reward: 0.607 [0.509, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.500, 10.098], loss: 0.001637, mae: 0.044094, mean_q: 1.174979
 89677/100000: episode: 1682, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.156, mean reward: 0.592 [0.498, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.146, 10.318], loss: 0.001714, mae: 0.045155, mean_q: 1.176813
 89777/100000: episode: 1683, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.944, mean reward: 0.599 [0.506, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.019, 10.098], loss: 0.001542, mae: 0.042834, mean_q: 1.172177
 89877/100000: episode: 1684, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.952, mean reward: 0.580 [0.503, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.132, 10.178], loss: 0.001415, mae: 0.041584, mean_q: 1.169961
 89977/100000: episode: 1685, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 60.222, mean reward: 0.602 [0.506, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.080, 10.098], loss: 0.001497, mae: 0.042382, mean_q: 1.173549
 90077/100000: episode: 1686, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 61.327, mean reward: 0.613 [0.516, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.487, 10.214], loss: 0.001469, mae: 0.041618, mean_q: 1.170416
 90177/100000: episode: 1687, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.679, mean reward: 0.587 [0.504, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.501, 10.098], loss: 0.001423, mae: 0.041397, mean_q: 1.174245
 90277/100000: episode: 1688, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.237, mean reward: 0.572 [0.500, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.541, 10.098], loss: 0.001473, mae: 0.042263, mean_q: 1.173171
 90377/100000: episode: 1689, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.763, mean reward: 0.578 [0.501, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.723, 10.098], loss: 0.001438, mae: 0.041595, mean_q: 1.170095
 90477/100000: episode: 1690, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 56.272, mean reward: 0.563 [0.505, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.471, 10.192], loss: 0.001529, mae: 0.043111, mean_q: 1.170806
 90577/100000: episode: 1691, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.951, mean reward: 0.600 [0.507, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.722, 10.320], loss: 0.001534, mae: 0.042348, mean_q: 1.166654
 90677/100000: episode: 1692, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.195, mean reward: 0.582 [0.509, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.287, 10.169], loss: 0.001425, mae: 0.041521, mean_q: 1.167779
 90777/100000: episode: 1693, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.818, mean reward: 0.578 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.545, 10.211], loss: 0.001506, mae: 0.042709, mean_q: 1.170833
 90877/100000: episode: 1694, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 58.826, mean reward: 0.588 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.261, 10.143], loss: 0.001596, mae: 0.043952, mean_q: 1.168562
 90977/100000: episode: 1695, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 57.814, mean reward: 0.578 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.702, 10.224], loss: 0.001619, mae: 0.043918, mean_q: 1.167726
 91077/100000: episode: 1696, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.537, mean reward: 0.575 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.469, 10.253], loss: 0.001605, mae: 0.043415, mean_q: 1.171500
 91177/100000: episode: 1697, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.647, mean reward: 0.586 [0.508, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.065, 10.202], loss: 0.001589, mae: 0.043422, mean_q: 1.167274
 91277/100000: episode: 1698, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.616, mean reward: 0.606 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.369, 10.098], loss: 0.001518, mae: 0.042753, mean_q: 1.164402
 91377/100000: episode: 1699, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 61.636, mean reward: 0.616 [0.512, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.460, 10.098], loss: 0.001543, mae: 0.042859, mean_q: 1.171336
 91477/100000: episode: 1700, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.822, mean reward: 0.588 [0.513, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.071, 10.159], loss: 0.001644, mae: 0.044257, mean_q: 1.171652
 91577/100000: episode: 1701, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 58.284, mean reward: 0.583 [0.504, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.884, 10.098], loss: 0.001686, mae: 0.044643, mean_q: 1.172199
 91677/100000: episode: 1702, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 61.963, mean reward: 0.620 [0.518, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.355, 10.098], loss: 0.001510, mae: 0.042565, mean_q: 1.169578
 91777/100000: episode: 1703, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.057, mean reward: 0.581 [0.513, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.640, 10.098], loss: 0.001724, mae: 0.045483, mean_q: 1.168459
 91877/100000: episode: 1704, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.785, mean reward: 0.588 [0.510, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.233, 10.108], loss: 0.001505, mae: 0.042344, mean_q: 1.169147
 91977/100000: episode: 1705, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.794, mean reward: 0.578 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.243, 10.174], loss: 0.001510, mae: 0.042235, mean_q: 1.170901
 92077/100000: episode: 1706, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.101, mean reward: 0.571 [0.500, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.155, 10.156], loss: 0.001491, mae: 0.041853, mean_q: 1.169805
 92177/100000: episode: 1707, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 57.535, mean reward: 0.575 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.457, 10.122], loss: 0.001507, mae: 0.042424, mean_q: 1.169947
 92277/100000: episode: 1708, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 59.516, mean reward: 0.595 [0.499, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.825, 10.098], loss: 0.001516, mae: 0.042245, mean_q: 1.170115
 92377/100000: episode: 1709, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 59.755, mean reward: 0.598 [0.512, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.274, 10.098], loss: 0.001564, mae: 0.043262, mean_q: 1.166606
 92477/100000: episode: 1710, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.259, mean reward: 0.593 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.017, 10.098], loss: 0.001687, mae: 0.044453, mean_q: 1.168059
 92577/100000: episode: 1711, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 59.151, mean reward: 0.592 [0.514, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.089, 10.186], loss: 0.001541, mae: 0.043105, mean_q: 1.169694
 92677/100000: episode: 1712, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 60.181, mean reward: 0.602 [0.515, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.481, 10.202], loss: 0.001525, mae: 0.042618, mean_q: 1.170279
 92777/100000: episode: 1713, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.533, mean reward: 0.595 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.494, 10.098], loss: 0.001593, mae: 0.043260, mean_q: 1.171353
 92877/100000: episode: 1714, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.589, mean reward: 0.576 [0.508, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.257], loss: 0.001680, mae: 0.044064, mean_q: 1.169817
 92977/100000: episode: 1715, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.855, mean reward: 0.589 [0.498, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.289, 10.098], loss: 0.001604, mae: 0.043742, mean_q: 1.167653
 93077/100000: episode: 1716, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 64.430, mean reward: 0.644 [0.515, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.722, 10.098], loss: 0.001555, mae: 0.043223, mean_q: 1.169294
 93177/100000: episode: 1717, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.109, mean reward: 0.591 [0.507, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.872, 10.098], loss: 0.001525, mae: 0.042417, mean_q: 1.169095
 93277/100000: episode: 1718, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.648, mean reward: 0.586 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.254, 10.243], loss: 0.001530, mae: 0.042651, mean_q: 1.166891
 93377/100000: episode: 1719, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.996, mean reward: 0.600 [0.499, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.566, 10.098], loss: 0.001516, mae: 0.042146, mean_q: 1.168980
 93477/100000: episode: 1720, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.006, mean reward: 0.580 [0.506, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.606, 10.104], loss: 0.001522, mae: 0.042398, mean_q: 1.167855
 93577/100000: episode: 1721, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 63.506, mean reward: 0.635 [0.500, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.803, 10.526], loss: 0.001532, mae: 0.042464, mean_q: 1.171386
 93677/100000: episode: 1722, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.190, mean reward: 0.572 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.151, 10.098], loss: 0.001504, mae: 0.042430, mean_q: 1.173749
 93777/100000: episode: 1723, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.980, mean reward: 0.580 [0.508, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.084, 10.113], loss: 0.001488, mae: 0.041674, mean_q: 1.171671
 93877/100000: episode: 1724, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.505, mean reward: 0.595 [0.515, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.573, 10.213], loss: 0.001668, mae: 0.044049, mean_q: 1.168660
 93977/100000: episode: 1725, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.017, mean reward: 0.560 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.629, 10.098], loss: 0.001527, mae: 0.042838, mean_q: 1.169415
 94077/100000: episode: 1726, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.800, mean reward: 0.588 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.282, 10.161], loss: 0.001571, mae: 0.042634, mean_q: 1.166117
 94177/100000: episode: 1727, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.553, mean reward: 0.596 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.727, 10.098], loss: 0.001489, mae: 0.042452, mean_q: 1.167646
 94277/100000: episode: 1728, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.500, mean reward: 0.595 [0.508, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.509, 10.098], loss: 0.001589, mae: 0.042818, mean_q: 1.164366
 94377/100000: episode: 1729, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 61.986, mean reward: 0.620 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.016, 10.142], loss: 0.001448, mae: 0.041185, mean_q: 1.166518
[Info] 1-TH LEVEL FOUND: 1.341868281364441, Considering 10/90 traces
 94477/100000: episode: 1730, duration: 4.809s, episode steps: 100, steps per second: 21, episode reward: 58.726, mean reward: 0.587 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.441, 10.098], loss: 0.001530, mae: 0.043098, mean_q: 1.168851
 94552/100000: episode: 1731, duration: 0.414s, episode steps: 75, steps per second: 181, episode reward: 45.075, mean reward: 0.601 [0.532, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-1.037, 10.152], loss: 0.001450, mae: 0.041806, mean_q: 1.169568
 94580/100000: episode: 1732, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 19.914, mean reward: 0.711 [0.607, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.794, 10.100], loss: 0.001416, mae: 0.042059, mean_q: 1.169800
 94624/100000: episode: 1733, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 27.140, mean reward: 0.617 [0.515, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.268, 10.251], loss: 0.001585, mae: 0.042690, mean_q: 1.172238
 94662/100000: episode: 1734, duration: 0.192s, episode steps: 38, steps per second: 198, episode reward: 25.477, mean reward: 0.670 [0.526, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.910, 10.172], loss: 0.001510, mae: 0.042450, mean_q: 1.168566
 94690/100000: episode: 1735, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 19.031, mean reward: 0.680 [0.578, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.524, 10.100], loss: 0.001650, mae: 0.044684, mean_q: 1.173809
 94704/100000: episode: 1736, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 9.851, mean reward: 0.704 [0.666, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.340, 10.100], loss: 0.001558, mae: 0.041731, mean_q: 1.171591
 94779/100000: episode: 1737, duration: 0.398s, episode steps: 75, steps per second: 188, episode reward: 45.736, mean reward: 0.610 [0.518, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.694 [-0.171, 10.100], loss: 0.001536, mae: 0.042532, mean_q: 1.170173
 94793/100000: episode: 1738, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 9.563, mean reward: 0.683 [0.642, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.346, 10.100], loss: 0.001570, mae: 0.042051, mean_q: 1.173299
 94817/100000: episode: 1739, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 18.832, mean reward: 0.785 [0.707, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.375, 10.100], loss: 0.001593, mae: 0.043589, mean_q: 1.173546
 94831/100000: episode: 1740, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 8.766, mean reward: 0.626 [0.588, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.207, 10.100], loss: 0.001891, mae: 0.044682, mean_q: 1.175984
 94845/100000: episode: 1741, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 9.855, mean reward: 0.704 [0.619, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.368, 10.100], loss: 0.002228, mae: 0.049242, mean_q: 1.172275
 94873/100000: episode: 1742, duration: 0.163s, episode steps: 28, steps per second: 171, episode reward: 19.897, mean reward: 0.711 [0.615, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.238, 10.100], loss: 0.001920, mae: 0.048236, mean_q: 1.175828
 94901/100000: episode: 1743, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 20.716, mean reward: 0.740 [0.628, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.393, 10.100], loss: 0.001552, mae: 0.042939, mean_q: 1.175460
 94925/100000: episode: 1744, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 18.005, mean reward: 0.750 [0.660, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.945, 10.100], loss: 0.001817, mae: 0.045430, mean_q: 1.173558
 95000/100000: episode: 1745, duration: 0.392s, episode steps: 75, steps per second: 192, episode reward: 46.918, mean reward: 0.626 [0.515, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.681 [-1.418, 10.100], loss: 0.001555, mae: 0.042400, mean_q: 1.179114
 95075/100000: episode: 1746, duration: 0.397s, episode steps: 75, steps per second: 189, episode reward: 42.654, mean reward: 0.569 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.689 [-1.519, 10.100], loss: 0.001674, mae: 0.044479, mean_q: 1.179796
 95089/100000: episode: 1747, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 10.100, mean reward: 0.721 [0.657, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.268, 10.100], loss: 0.001725, mae: 0.044230, mean_q: 1.179721
 95107/100000: episode: 1748, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 12.061, mean reward: 0.670 [0.635, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.280, 10.445], loss: 0.002404, mae: 0.050617, mean_q: 1.168888
 95145/100000: episode: 1749, duration: 0.214s, episode steps: 38, steps per second: 178, episode reward: 26.591, mean reward: 0.700 [0.588, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.771, 10.191], loss: 0.001720, mae: 0.043701, mean_q: 1.182547
 95183/100000: episode: 1750, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 24.865, mean reward: 0.654 [0.551, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.586, 10.234], loss: 0.001513, mae: 0.042304, mean_q: 1.180344
 95227/100000: episode: 1751, duration: 0.236s, episode steps: 44, steps per second: 187, episode reward: 26.642, mean reward: 0.605 [0.524, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.858, 10.199], loss: 0.001617, mae: 0.043192, mean_q: 1.180699
 95271/100000: episode: 1752, duration: 0.245s, episode steps: 44, steps per second: 179, episode reward: 26.037, mean reward: 0.592 [0.499, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.799, 10.100], loss: 0.001518, mae: 0.041945, mean_q: 1.178773
 95346/100000: episode: 1753, duration: 0.407s, episode steps: 75, steps per second: 184, episode reward: 45.471, mean reward: 0.606 [0.519, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-0.561, 10.318], loss: 0.001781, mae: 0.044824, mean_q: 1.177631
 95375/100000: episode: 1754, duration: 0.173s, episode steps: 29, steps per second: 167, episode reward: 17.990, mean reward: 0.620 [0.555, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.440, 10.100], loss: 0.001739, mae: 0.045188, mean_q: 1.178062
 95411/100000: episode: 1755, duration: 0.206s, episode steps: 36, steps per second: 175, episode reward: 25.742, mean reward: 0.715 [0.628, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.521, 10.484], loss: 0.001582, mae: 0.042915, mean_q: 1.181830
 95440/100000: episode: 1756, duration: 0.168s, episode steps: 29, steps per second: 172, episode reward: 17.922, mean reward: 0.618 [0.532, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.365, 10.175], loss: 0.001529, mae: 0.042088, mean_q: 1.182988
 95464/100000: episode: 1757, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 16.630, mean reward: 0.693 [0.618, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.223, 10.100], loss: 0.001474, mae: 0.042620, mean_q: 1.187834
 95503/100000: episode: 1758, duration: 0.215s, episode steps: 39, steps per second: 181, episode reward: 23.321, mean reward: 0.598 [0.514, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.854, 10.168], loss: 0.001639, mae: 0.043931, mean_q: 1.188667
 95517/100000: episode: 1759, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 9.601, mean reward: 0.686 [0.637, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.279, 10.100], loss: 0.002007, mae: 0.049601, mean_q: 1.177495
 95535/100000: episode: 1760, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 13.667, mean reward: 0.759 [0.683, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.427], loss: 0.001612, mae: 0.043716, mean_q: 1.191441
 95563/100000: episode: 1761, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 20.389, mean reward: 0.728 [0.676, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.264, 10.100], loss: 0.001886, mae: 0.046359, mean_q: 1.188308
 95599/100000: episode: 1762, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 26.039, mean reward: 0.723 [0.587, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.649, 10.299], loss: 0.001534, mae: 0.042663, mean_q: 1.187725
 95635/100000: episode: 1763, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 24.081, mean reward: 0.669 [0.560, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.133, 10.285], loss: 0.001828, mae: 0.044767, mean_q: 1.183989
 95674/100000: episode: 1764, duration: 0.229s, episode steps: 39, steps per second: 170, episode reward: 26.721, mean reward: 0.685 [0.608, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-1.183, 10.100], loss: 0.001661, mae: 0.042917, mean_q: 1.193601
 95702/100000: episode: 1765, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 19.985, mean reward: 0.714 [0.639, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.663, 10.100], loss: 0.001891, mae: 0.047074, mean_q: 1.197521
 95777/100000: episode: 1766, duration: 0.411s, episode steps: 75, steps per second: 183, episode reward: 44.667, mean reward: 0.596 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.693 [-0.614, 10.131], loss: 0.001766, mae: 0.044606, mean_q: 1.192080
 95816/100000: episode: 1767, duration: 0.207s, episode steps: 39, steps per second: 189, episode reward: 24.509, mean reward: 0.628 [0.531, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.558, 10.100], loss: 0.001456, mae: 0.040317, mean_q: 1.193885
 95855/100000: episode: 1768, duration: 0.207s, episode steps: 39, steps per second: 189, episode reward: 24.876, mean reward: 0.638 [0.530, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.583, 10.100], loss: 0.001695, mae: 0.044570, mean_q: 1.196081
 95884/100000: episode: 1769, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 18.748, mean reward: 0.646 [0.588, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.145, 10.100], loss: 0.001546, mae: 0.042741, mean_q: 1.194303
 95913/100000: episode: 1770, duration: 0.170s, episode steps: 29, steps per second: 171, episode reward: 16.923, mean reward: 0.584 [0.519, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.395, 10.100], loss: 0.001535, mae: 0.042780, mean_q: 1.189439
 95941/100000: episode: 1771, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 18.970, mean reward: 0.677 [0.522, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.013, 10.100], loss: 0.001670, mae: 0.042510, mean_q: 1.201856
 95980/100000: episode: 1772, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 28.576, mean reward: 0.733 [0.581, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-1.481, 10.100], loss: 0.001538, mae: 0.042453, mean_q: 1.193174
 95998/100000: episode: 1773, duration: 0.116s, episode steps: 18, steps per second: 155, episode reward: 12.994, mean reward: 0.722 [0.676, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.549, 10.495], loss: 0.001534, mae: 0.042631, mean_q: 1.201921
 96026/100000: episode: 1774, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 18.987, mean reward: 0.678 [0.575, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.540, 10.100], loss: 0.001732, mae: 0.044865, mean_q: 1.198167
 96044/100000: episode: 1775, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 11.895, mean reward: 0.661 [0.591, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.483], loss: 0.001681, mae: 0.044144, mean_q: 1.197365
 96072/100000: episode: 1776, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 19.515, mean reward: 0.697 [0.636, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.328, 10.100], loss: 0.001737, mae: 0.043550, mean_q: 1.201373
 96100/100000: episode: 1777, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 19.846, mean reward: 0.709 [0.633, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.250, 10.100], loss: 0.001659, mae: 0.043846, mean_q: 1.202528
 96118/100000: episode: 1778, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 11.742, mean reward: 0.652 [0.571, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.308], loss: 0.001526, mae: 0.041986, mean_q: 1.194146
 96193/100000: episode: 1779, duration: 0.406s, episode steps: 75, steps per second: 185, episode reward: 46.002, mean reward: 0.613 [0.511, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-0.205, 10.100], loss: 0.001697, mae: 0.044276, mean_q: 1.205948
 96217/100000: episode: 1780, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 17.461, mean reward: 0.728 [0.663, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.716, 10.100], loss: 0.001746, mae: 0.046465, mean_q: 1.200808
 96241/100000: episode: 1781, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 18.674, mean reward: 0.778 [0.671, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.211, 10.100], loss: 0.001666, mae: 0.043552, mean_q: 1.207515
 96280/100000: episode: 1782, duration: 0.221s, episode steps: 39, steps per second: 177, episode reward: 25.579, mean reward: 0.656 [0.556, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-1.292, 10.100], loss: 0.001746, mae: 0.044949, mean_q: 1.206219
 96324/100000: episode: 1783, duration: 0.230s, episode steps: 44, steps per second: 191, episode reward: 27.236, mean reward: 0.619 [0.537, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.193, 10.187], loss: 0.001770, mae: 0.044785, mean_q: 1.203832
 96348/100000: episode: 1784, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 18.146, mean reward: 0.756 [0.627, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.406, 10.100], loss: 0.001761, mae: 0.046415, mean_q: 1.204528
 96387/100000: episode: 1785, duration: 0.220s, episode steps: 39, steps per second: 177, episode reward: 24.254, mean reward: 0.622 [0.528, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.177, 10.113], loss: 0.001606, mae: 0.042602, mean_q: 1.207913
 96401/100000: episode: 1786, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 9.416, mean reward: 0.673 [0.637, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.315, 10.100], loss: 0.001332, mae: 0.039192, mean_q: 1.209626
 96425/100000: episode: 1787, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 16.076, mean reward: 0.670 [0.602, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.315, 10.100], loss: 0.001504, mae: 0.041175, mean_q: 1.205955
 96463/100000: episode: 1788, duration: 0.219s, episode steps: 38, steps per second: 174, episode reward: 31.717, mean reward: 0.835 [0.683, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.450, 10.625], loss: 0.001560, mae: 0.042168, mean_q: 1.205612
 96481/100000: episode: 1789, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 12.732, mean reward: 0.707 [0.662, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.670, 10.414], loss: 0.001442, mae: 0.039411, mean_q: 1.215657
 96499/100000: episode: 1790, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 12.647, mean reward: 0.703 [0.609, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.730, 10.325], loss: 0.001557, mae: 0.042635, mean_q: 1.213392
 96517/100000: episode: 1791, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 11.393, mean reward: 0.633 [0.579, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.207, 10.356], loss: 0.001702, mae: 0.044430, mean_q: 1.214480
 96535/100000: episode: 1792, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 12.914, mean reward: 0.717 [0.596, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.371], loss: 0.001740, mae: 0.043729, mean_q: 1.211034
 96610/100000: episode: 1793, duration: 0.421s, episode steps: 75, steps per second: 178, episode reward: 44.918, mean reward: 0.599 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.689 [-0.181, 10.133], loss: 0.001694, mae: 0.044536, mean_q: 1.217224
 96648/100000: episode: 1794, duration: 0.204s, episode steps: 38, steps per second: 187, episode reward: 25.485, mean reward: 0.671 [0.585, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.918, 10.319], loss: 0.001500, mae: 0.041229, mean_q: 1.206744
 96686/100000: episode: 1795, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 27.142, mean reward: 0.714 [0.642, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.378, 10.410], loss: 0.001522, mae: 0.041193, mean_q: 1.208134
 96725/100000: episode: 1796, duration: 0.215s, episode steps: 39, steps per second: 181, episode reward: 26.063, mean reward: 0.668 [0.586, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.226, 10.100], loss: 0.001901, mae: 0.046507, mean_q: 1.222060
 96743/100000: episode: 1797, duration: 0.114s, episode steps: 18, steps per second: 158, episode reward: 11.410, mean reward: 0.634 [0.526, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.813, 10.236], loss: 0.001638, mae: 0.042507, mean_q: 1.214423
 96772/100000: episode: 1798, duration: 0.166s, episode steps: 29, steps per second: 174, episode reward: 20.244, mean reward: 0.698 [0.636, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.108, 10.100], loss: 0.001810, mae: 0.045307, mean_q: 1.216563
 96801/100000: episode: 1799, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 20.569, mean reward: 0.709 [0.618, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.308, 10.100], loss: 0.001568, mae: 0.041957, mean_q: 1.220905
 96815/100000: episode: 1800, duration: 0.083s, episode steps: 14, steps per second: 170, episode reward: 8.780, mean reward: 0.627 [0.590, 0.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.323, 10.100], loss: 0.001701, mae: 0.045024, mean_q: 1.231071
 96833/100000: episode: 1801, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 11.665, mean reward: 0.648 [0.590, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.525, 10.266], loss: 0.001683, mae: 0.042914, mean_q: 1.230962
 96908/100000: episode: 1802, duration: 0.400s, episode steps: 75, steps per second: 188, episode reward: 44.555, mean reward: 0.594 [0.500, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.689 [-0.227, 10.100], loss: 0.001517, mae: 0.042192, mean_q: 1.221455
 96937/100000: episode: 1803, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 19.874, mean reward: 0.685 [0.594, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.081, 10.100], loss: 0.001624, mae: 0.043231, mean_q: 1.227745
 96966/100000: episode: 1804, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 19.449, mean reward: 0.671 [0.621, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.409, 10.100], loss: 0.001634, mae: 0.043325, mean_q: 1.212634
 96994/100000: episode: 1805, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 18.554, mean reward: 0.663 [0.615, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.842, 10.100], loss: 0.002009, mae: 0.047384, mean_q: 1.224419
 97018/100000: episode: 1806, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 15.945, mean reward: 0.664 [0.611, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.401, 10.100], loss: 0.002140, mae: 0.049565, mean_q: 1.232412
 97047/100000: episode: 1807, duration: 0.167s, episode steps: 29, steps per second: 173, episode reward: 19.570, mean reward: 0.675 [0.582, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.161, 10.100], loss: 0.001531, mae: 0.042697, mean_q: 1.224857
 97075/100000: episode: 1808, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 19.889, mean reward: 0.710 [0.635, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.649, 10.100], loss: 0.001674, mae: 0.045484, mean_q: 1.228373
 97114/100000: episode: 1809, duration: 0.231s, episode steps: 39, steps per second: 169, episode reward: 24.249, mean reward: 0.622 [0.507, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.612, 10.171], loss: 0.001661, mae: 0.043048, mean_q: 1.228281
 97189/100000: episode: 1810, duration: 0.383s, episode steps: 75, steps per second: 196, episode reward: 43.659, mean reward: 0.582 [0.512, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.689 [-0.502, 10.100], loss: 0.001676, mae: 0.043594, mean_q: 1.218951
 97264/100000: episode: 1811, duration: 0.410s, episode steps: 75, steps per second: 183, episode reward: 44.523, mean reward: 0.594 [0.510, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.692 [-0.642, 10.100], loss: 0.001556, mae: 0.042148, mean_q: 1.235073
 97292/100000: episode: 1812, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 20.946, mean reward: 0.748 [0.627, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.547, 10.100], loss: 0.001653, mae: 0.043298, mean_q: 1.217218
 97320/100000: episode: 1813, duration: 0.173s, episode steps: 28, steps per second: 161, episode reward: 18.631, mean reward: 0.665 [0.531, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.249, 10.100], loss: 0.001629, mae: 0.043410, mean_q: 1.235480
 97395/100000: episode: 1814, duration: 0.404s, episode steps: 75, steps per second: 185, episode reward: 43.926, mean reward: 0.586 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.694 [-0.103, 10.100], loss: 0.001681, mae: 0.043461, mean_q: 1.229123
 97423/100000: episode: 1815, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 18.511, mean reward: 0.661 [0.522, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.526, 10.100], loss: 0.001788, mae: 0.045735, mean_q: 1.240237
 97461/100000: episode: 1816, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 29.595, mean reward: 0.779 [0.717, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.598, 10.492], loss: 0.001545, mae: 0.042195, mean_q: 1.225079
 97485/100000: episode: 1817, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 17.260, mean reward: 0.719 [0.615, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.581, 10.100], loss: 0.001756, mae: 0.044626, mean_q: 1.229797
 97509/100000: episode: 1818, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 14.810, mean reward: 0.617 [0.521, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.653, 10.118], loss: 0.001329, mae: 0.040248, mean_q: 1.227684
 97547/100000: episode: 1819, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 23.154, mean reward: 0.609 [0.513, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.088, 10.133], loss: 0.001738, mae: 0.043832, mean_q: 1.234232
[Info] 2-TH LEVEL FOUND: 1.5698683261871338, Considering 10/90 traces
 97571/100000: episode: 1820, duration: 4.359s, episode steps: 24, steps per second: 6, episode reward: 15.891, mean reward: 0.662 [0.618, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.243, 10.100], loss: 0.001889, mae: 0.045009, mean_q: 1.239900
 97596/100000: episode: 1821, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 17.933, mean reward: 0.717 [0.605, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.797, 10.270], loss: 0.001719, mae: 0.043665, mean_q: 1.239058
 97607/100000: episode: 1822, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 8.534, mean reward: 0.776 [0.728, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.420, 10.100], loss: 0.001769, mae: 0.044838, mean_q: 1.223720
 97632/100000: episode: 1823, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 20.097, mean reward: 0.804 [0.647, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.470, 10.508], loss: 0.001585, mae: 0.044254, mean_q: 1.240892
 97646/100000: episode: 1824, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 9.740, mean reward: 0.696 [0.602, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.244, 10.100], loss: 0.001544, mae: 0.042034, mean_q: 1.237982
 97679/100000: episode: 1825, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 22.204, mean reward: 0.673 [0.531, 0.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.821, 10.206], loss: 0.001344, mae: 0.040404, mean_q: 1.233431
 97712/100000: episode: 1826, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 21.324, mean reward: 0.646 [0.553, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.063, 10.283], loss: 0.001528, mae: 0.041761, mean_q: 1.239351
 97726/100000: episode: 1827, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 11.041, mean reward: 0.789 [0.749, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.454, 10.100], loss: 0.001628, mae: 0.042582, mean_q: 1.238284
 97757/100000: episode: 1828, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 22.890, mean reward: 0.738 [0.700, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.463], loss: 0.001604, mae: 0.043528, mean_q: 1.237084
[Info] FALSIFICATION!
 97765/100000: episode: 1829, duration: 0.206s, episode steps: 8, steps per second: 39, episode reward: 6.688, mean reward: 0.836 [0.756, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.118, 9.813], loss: 0.002277, mae: 0.046644, mean_q: 1.247435
 97776/100000: episode: 1830, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 9.220, mean reward: 0.838 [0.718, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.556, 10.100], loss: 0.001822, mae: 0.046218, mean_q: 1.234901
 97784/100000: episode: 1831, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 6.243, mean reward: 0.780 [0.758, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.884, 10.100], loss: 0.001438, mae: 0.042302, mean_q: 1.223711
 97803/100000: episode: 1832, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 16.408, mean reward: 0.864 [0.754, 0.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.880, 10.100], loss: 0.002031, mae: 0.045704, mean_q: 1.251554
 97834/100000: episode: 1833, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 22.117, mean reward: 0.713 [0.555, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.202, 10.215], loss: 0.001514, mae: 0.042292, mean_q: 1.244851
 97845/100000: episode: 1834, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 8.707, mean reward: 0.792 [0.744, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.871, 10.100], loss: 0.001316, mae: 0.040615, mean_q: 1.256020
 97878/100000: episode: 1835, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 24.750, mean reward: 0.750 [0.598, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.703, 10.292], loss: 0.001581, mae: 0.042039, mean_q: 1.256487
 97892/100000: episode: 1836, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 10.632, mean reward: 0.759 [0.695, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.418, 10.100], loss: 0.001466, mae: 0.039169, mean_q: 1.254153
 97903/100000: episode: 1837, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 8.526, mean reward: 0.775 [0.745, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.481, 10.100], loss: 0.001560, mae: 0.043057, mean_q: 1.244945
 97915/100000: episode: 1838, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 9.147, mean reward: 0.762 [0.692, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.311, 10.100], loss: 0.001514, mae: 0.041894, mean_q: 1.233717
 97940/100000: episode: 1839, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 18.859, mean reward: 0.754 [0.660, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.385], loss: 0.001471, mae: 0.041892, mean_q: 1.253745
 97971/100000: episode: 1840, duration: 0.169s, episode steps: 31, steps per second: 184, episode reward: 20.989, mean reward: 0.677 [0.562, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.456, 10.280], loss: 0.001611, mae: 0.043393, mean_q: 1.253713
 97982/100000: episode: 1841, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 8.240, mean reward: 0.749 [0.718, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.876, 10.100], loss: 0.001916, mae: 0.045719, mean_q: 1.263798
 97994/100000: episode: 1842, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 9.851, mean reward: 0.821 [0.703, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.439, 10.100], loss: 0.001579, mae: 0.043149, mean_q: 1.242636
 98006/100000: episode: 1843, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 9.003, mean reward: 0.750 [0.660, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.462, 10.100], loss: 0.002046, mae: 0.048758, mean_q: 1.242682
 98025/100000: episode: 1844, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 14.750, mean reward: 0.776 [0.720, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.476, 10.100], loss: 0.001518, mae: 0.043690, mean_q: 1.256549
 98039/100000: episode: 1845, duration: 0.091s, episode steps: 14, steps per second: 153, episode reward: 11.237, mean reward: 0.803 [0.753, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.343, 10.100], loss: 0.001615, mae: 0.043745, mean_q: 1.241916
 98053/100000: episode: 1846, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 11.731, mean reward: 0.838 [0.791, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.436, 10.100], loss: 0.001731, mae: 0.044983, mean_q: 1.274107
 98086/100000: episode: 1847, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 22.833, mean reward: 0.692 [0.558, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.186, 10.330], loss: 0.001568, mae: 0.042018, mean_q: 1.253056
 98094/100000: episode: 1848, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 6.815, mean reward: 0.852 [0.825, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.468, 10.100], loss: 0.001725, mae: 0.045533, mean_q: 1.240772
 98129/100000: episode: 1849, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 28.237, mean reward: 0.807 [0.714, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.260, 10.547], loss: 0.002043, mae: 0.047344, mean_q: 1.253586
 98148/100000: episode: 1850, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 13.978, mean reward: 0.736 [0.639, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.315, 10.100], loss: 0.001684, mae: 0.043999, mean_q: 1.248987
 98173/100000: episode: 1851, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 19.877, mean reward: 0.795 [0.709, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.504, 10.568], loss: 0.001649, mae: 0.044729, mean_q: 1.271958
 98192/100000: episode: 1852, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 14.697, mean reward: 0.774 [0.699, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.384, 10.100], loss: 0.001796, mae: 0.044144, mean_q: 1.264719
 98225/100000: episode: 1853, duration: 0.183s, episode steps: 33, steps per second: 181, episode reward: 24.201, mean reward: 0.733 [0.614, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.035, 10.313], loss: 0.001354, mae: 0.039179, mean_q: 1.257980
 98250/100000: episode: 1854, duration: 0.150s, episode steps: 25, steps per second: 166, episode reward: 17.921, mean reward: 0.717 [0.639, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.396, 10.432], loss: 0.001186, mae: 0.038418, mean_q: 1.256272
 98281/100000: episode: 1855, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 22.916, mean reward: 0.739 [0.568, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.995, 10.350], loss: 0.001399, mae: 0.040526, mean_q: 1.255372
 98293/100000: episode: 1856, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 9.851, mean reward: 0.821 [0.749, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.321, 10.100], loss: 0.002472, mae: 0.050302, mean_q: 1.262329
 98307/100000: episode: 1857, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 10.536, mean reward: 0.753 [0.680, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.368, 10.100], loss: 0.001650, mae: 0.045832, mean_q: 1.285910
 98338/100000: episode: 1858, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 22.914, mean reward: 0.739 [0.652, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.495, 10.386], loss: 0.001502, mae: 0.041699, mean_q: 1.273855
 98350/100000: episode: 1859, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 8.671, mean reward: 0.723 [0.635, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.249, 10.100], loss: 0.001892, mae: 0.040943, mean_q: 1.263691
 98369/100000: episode: 1860, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 15.676, mean reward: 0.825 [0.742, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.932, 10.100], loss: 0.001451, mae: 0.040511, mean_q: 1.269997
 98404/100000: episode: 1861, duration: 0.198s, episode steps: 35, steps per second: 176, episode reward: 29.934, mean reward: 0.855 [0.775, 0.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.277, 10.754], loss: 0.001459, mae: 0.042028, mean_q: 1.272839
 98418/100000: episode: 1862, duration: 0.089s, episode steps: 14, steps per second: 158, episode reward: 10.013, mean reward: 0.715 [0.641, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.254, 10.100], loss: 0.001543, mae: 0.042586, mean_q: 1.270058
 98451/100000: episode: 1863, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 25.860, mean reward: 0.784 [0.697, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.704, 10.485], loss: 0.001424, mae: 0.041889, mean_q: 1.281816
 98470/100000: episode: 1864, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 15.143, mean reward: 0.797 [0.730, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.424, 10.100], loss: 0.001359, mae: 0.039428, mean_q: 1.280567
 98505/100000: episode: 1865, duration: 0.208s, episode steps: 35, steps per second: 168, episode reward: 26.352, mean reward: 0.753 [0.593, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.128, 10.326], loss: 0.001469, mae: 0.041204, mean_q: 1.293286
 98536/100000: episode: 1866, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 21.693, mean reward: 0.700 [0.556, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.313, 10.294], loss: 0.001631, mae: 0.040779, mean_q: 1.278417
 98555/100000: episode: 1867, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 12.965, mean reward: 0.682 [0.554, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.361, 10.100], loss: 0.001414, mae: 0.038854, mean_q: 1.267930
 98569/100000: episode: 1868, duration: 0.093s, episode steps: 14, steps per second: 151, episode reward: 11.768, mean reward: 0.841 [0.743, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.380, 10.100], loss: 0.001643, mae: 0.043039, mean_q: 1.296249
 98580/100000: episode: 1869, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 8.318, mean reward: 0.756 [0.728, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.429, 10.100], loss: 0.001593, mae: 0.042715, mean_q: 1.279289
 98592/100000: episode: 1870, duration: 0.079s, episode steps: 12, steps per second: 153, episode reward: 9.671, mean reward: 0.806 [0.764, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.153, 10.100], loss: 0.001317, mae: 0.039504, mean_q: 1.284103
 98606/100000: episode: 1871, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 10.893, mean reward: 0.778 [0.731, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.223, 10.100], loss: 0.001343, mae: 0.039258, mean_q: 1.285209
 98620/100000: episode: 1872, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 11.267, mean reward: 0.805 [0.740, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.283, 10.100], loss: 0.001759, mae: 0.040953, mean_q: 1.283380
 98631/100000: episode: 1873, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 8.137, mean reward: 0.740 [0.673, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.236, 10.100], loss: 0.001557, mae: 0.043806, mean_q: 1.298020
 98642/100000: episode: 1874, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 8.453, mean reward: 0.768 [0.745, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.463, 10.100], loss: 0.002316, mae: 0.045438, mean_q: 1.285127
 98675/100000: episode: 1875, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 23.980, mean reward: 0.727 [0.655, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.110, 10.420], loss: 0.001798, mae: 0.045239, mean_q: 1.298948
 98710/100000: episode: 1876, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 30.063, mean reward: 0.859 [0.781, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.110, 10.674], loss: 0.001510, mae: 0.040657, mean_q: 1.291888
 98724/100000: episode: 1877, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 10.788, mean reward: 0.771 [0.686, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.587, 10.100], loss: 0.001504, mae: 0.043020, mean_q: 1.309127
 98749/100000: episode: 1878, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 20.457, mean reward: 0.818 [0.723, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.311, 10.457], loss: 0.001386, mae: 0.041223, mean_q: 1.299623
 98760/100000: episode: 1879, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 9.350, mean reward: 0.850 [0.761, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.433, 10.100], loss: 0.001292, mae: 0.040753, mean_q: 1.307023
 98795/100000: episode: 1880, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 26.227, mean reward: 0.749 [0.628, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.322, 10.333], loss: 0.001436, mae: 0.042014, mean_q: 1.289195
 98809/100000: episode: 1881, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 11.115, mean reward: 0.794 [0.744, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.309, 10.100], loss: 0.001251, mae: 0.040706, mean_q: 1.316321
 98842/100000: episode: 1882, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 25.182, mean reward: 0.763 [0.656, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.035, 10.530], loss: 0.001406, mae: 0.040395, mean_q: 1.295661
 98867/100000: episode: 1883, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 19.219, mean reward: 0.769 [0.629, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.384], loss: 0.001615, mae: 0.043542, mean_q: 1.309312
 98881/100000: episode: 1884, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 10.745, mean reward: 0.767 [0.715, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.356, 10.100], loss: 0.001480, mae: 0.041949, mean_q: 1.301955
 98916/100000: episode: 1885, duration: 0.211s, episode steps: 35, steps per second: 166, episode reward: 27.772, mean reward: 0.793 [0.664, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.484, 10.482], loss: 0.001623, mae: 0.041987, mean_q: 1.316095
 98947/100000: episode: 1886, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 21.645, mean reward: 0.698 [0.606, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.399], loss: 0.001532, mae: 0.040078, mean_q: 1.304228
 98961/100000: episode: 1887, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 11.373, mean reward: 0.812 [0.740, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.011, 10.100], loss: 0.001348, mae: 0.040994, mean_q: 1.318308
 98975/100000: episode: 1888, duration: 0.098s, episode steps: 14, steps per second: 143, episode reward: 10.705, mean reward: 0.765 [0.714, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.331, 10.100], loss: 0.001500, mae: 0.042980, mean_q: 1.309631
 99008/100000: episode: 1889, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 23.355, mean reward: 0.708 [0.653, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.895, 10.445], loss: 0.001711, mae: 0.042168, mean_q: 1.327666
 99019/100000: episode: 1890, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 8.755, mean reward: 0.796 [0.765, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.395, 10.100], loss: 0.001610, mae: 0.043563, mean_q: 1.329818
 99030/100000: episode: 1891, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 8.946, mean reward: 0.813 [0.774, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.488, 10.100], loss: 0.001584, mae: 0.044356, mean_q: 1.303753
 99061/100000: episode: 1892, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 20.544, mean reward: 0.663 [0.565, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.364, 10.224], loss: 0.001316, mae: 0.039855, mean_q: 1.315027
 99075/100000: episode: 1893, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 11.541, mean reward: 0.824 [0.792, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.364, 10.100], loss: 0.001475, mae: 0.041748, mean_q: 1.308362
 99108/100000: episode: 1894, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 21.636, mean reward: 0.656 [0.553, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.528, 10.238], loss: 0.002017, mae: 0.043729, mean_q: 1.324877
 99143/100000: episode: 1895, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 26.822, mean reward: 0.766 [0.529, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.121, 10.190], loss: 0.001320, mae: 0.039681, mean_q: 1.321480
 99178/100000: episode: 1896, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 30.194, mean reward: 0.863 [0.736, 0.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.107, 10.641], loss: 0.001485, mae: 0.040845, mean_q: 1.310668
 99211/100000: episode: 1897, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 24.695, mean reward: 0.748 [0.621, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.316], loss: 0.001453, mae: 0.042693, mean_q: 1.321909
 99242/100000: episode: 1898, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 22.347, mean reward: 0.721 [0.597, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.452], loss: 0.001333, mae: 0.039812, mean_q: 1.326337
 99261/100000: episode: 1899, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 14.801, mean reward: 0.779 [0.745, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.410, 10.100], loss: 0.001195, mae: 0.038934, mean_q: 1.298714
 99273/100000: episode: 1900, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 8.974, mean reward: 0.748 [0.682, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.278, 10.100], loss: 0.001365, mae: 0.041589, mean_q: 1.326989
 99306/100000: episode: 1901, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 23.439, mean reward: 0.710 [0.531, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.672, 10.150], loss: 0.001366, mae: 0.040423, mean_q: 1.334333
 99339/100000: episode: 1902, duration: 0.197s, episode steps: 33, steps per second: 168, episode reward: 22.955, mean reward: 0.696 [0.631, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.794, 10.429], loss: 0.001347, mae: 0.040768, mean_q: 1.329289
 99350/100000: episode: 1903, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 8.262, mean reward: 0.751 [0.696, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.247, 10.100], loss: 0.001488, mae: 0.042599, mean_q: 1.316588
 99364/100000: episode: 1904, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 11.556, mean reward: 0.825 [0.760, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.407, 10.100], loss: 0.001686, mae: 0.039216, mean_q: 1.336836
 99395/100000: episode: 1905, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 23.180, mean reward: 0.748 [0.671, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-1.260, 10.394], loss: 0.001738, mae: 0.044198, mean_q: 1.326426
 99430/100000: episode: 1906, duration: 0.208s, episode steps: 35, steps per second: 168, episode reward: 25.918, mean reward: 0.741 [0.586, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.268, 10.351], loss: 0.001496, mae: 0.040575, mean_q: 1.329500
 99463/100000: episode: 1907, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 22.559, mean reward: 0.684 [0.540, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.035, 10.186], loss: 0.001292, mae: 0.040677, mean_q: 1.335326
 99477/100000: episode: 1908, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 11.079, mean reward: 0.791 [0.738, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.355, 10.100], loss: 0.001509, mae: 0.042702, mean_q: 1.338145
 99512/100000: episode: 1909, duration: 0.178s, episode steps: 35, steps per second: 196, episode reward: 27.012, mean reward: 0.772 [0.702, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.836, 10.544], loss: 0.001300, mae: 0.039354, mean_q: 1.339189
[Info] Complete ISplit Iteration
[Info] Levels: [1.3418683, 1.5698683, 1.6652002]
[Info] Cond. Prob: [0.1, 0.1, 0.18]
[Info] Error Prob: 0.0018000000000000004

 99547/100000: episode: 1910, duration: 4.637s, episode steps: 35, steps per second: 8, episode reward: 28.611, mean reward: 0.817 [0.711, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.122, 10.503], loss: 0.001376, mae: 0.041174, mean_q: 1.343758
 99647/100000: episode: 1911, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 60.339, mean reward: 0.603 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.651, 10.229], loss: 0.001367, mae: 0.040830, mean_q: 1.336472
 99747/100000: episode: 1912, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.625, mean reward: 0.576 [0.504, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.293, 10.098], loss: 0.001648, mae: 0.043047, mean_q: 1.339200
 99847/100000: episode: 1913, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 57.502, mean reward: 0.575 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.598, 10.098], loss: 0.001394, mae: 0.041096, mean_q: 1.330042
 99947/100000: episode: 1914, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.965, mean reward: 0.590 [0.511, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.156, 10.098], loss: 0.001506, mae: 0.042875, mean_q: 1.325024
done, took 638.399 seconds
[Info] End Importance Splitting. Falsification occurred 13 times.
