Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 575, episode reward: 58.744, mean reward: 0.587 [0.506, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.855, 10.217], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.066s, episode steps: 100, steps per second: 1527, episode reward: 58.094, mean reward: 0.581 [0.503, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.439, 10.231], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.066s, episode steps: 100, steps per second: 1504, episode reward: 58.248, mean reward: 0.582 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.304, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.067s, episode steps: 100, steps per second: 1503, episode reward: 57.750, mean reward: 0.578 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.274, 10.225], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.079s, episode steps: 100, steps per second: 1270, episode reward: 57.451, mean reward: 0.575 [0.503, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.439, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 58.470, mean reward: 0.585 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.586, 10.278], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.072s, episode steps: 100, steps per second: 1389, episode reward: 57.365, mean reward: 0.574 [0.504, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.891, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.073s, episode steps: 100, steps per second: 1361, episode reward: 57.522, mean reward: 0.575 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.865, 10.215], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.073s, episode steps: 100, steps per second: 1376, episode reward: 57.602, mean reward: 0.576 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.301, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.068s, episode steps: 100, steps per second: 1472, episode reward: 57.951, mean reward: 0.580 [0.505, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.826, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.066s, episode steps: 100, steps per second: 1515, episode reward: 59.129, mean reward: 0.591 [0.503, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.645, 10.325], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.067s, episode steps: 100, steps per second: 1503, episode reward: 57.616, mean reward: 0.576 [0.509, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.350, 10.123], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.075s, episode steps: 100, steps per second: 1336, episode reward: 61.087, mean reward: 0.611 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.859, 10.120], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.066s, episode steps: 100, steps per second: 1513, episode reward: 60.225, mean reward: 0.602 [0.503, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.295, 10.216], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 57.275, mean reward: 0.573 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.930, 10.129], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.070s, episode steps: 100, steps per second: 1421, episode reward: 59.990, mean reward: 0.600 [0.513, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.702, 10.117], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.066s, episode steps: 100, steps per second: 1517, episode reward: 58.376, mean reward: 0.584 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.331, 10.262], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.066s, episode steps: 100, steps per second: 1506, episode reward: 58.465, mean reward: 0.585 [0.497, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.152], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.067s, episode steps: 100, steps per second: 1496, episode reward: 58.154, mean reward: 0.582 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.712, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.072s, episode steps: 100, steps per second: 1383, episode reward: 60.406, mean reward: 0.604 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.931, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.067s, episode steps: 100, steps per second: 1503, episode reward: 62.357, mean reward: 0.624 [0.506, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.376, 10.174], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 57.068, mean reward: 0.571 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.106, 10.211], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.067s, episode steps: 100, steps per second: 1498, episode reward: 59.940, mean reward: 0.599 [0.505, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.472, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.070s, episode steps: 100, steps per second: 1429, episode reward: 59.120, mean reward: 0.591 [0.500, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.559, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.067s, episode steps: 100, steps per second: 1503, episode reward: 58.344, mean reward: 0.583 [0.509, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.256, 10.131], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.066s, episode steps: 100, steps per second: 1509, episode reward: 59.432, mean reward: 0.594 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.942, 10.314], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.073s, episode steps: 100, steps per second: 1373, episode reward: 60.452, mean reward: 0.605 [0.505, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.722, 10.397], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.077s, episode steps: 100, steps per second: 1304, episode reward: 59.264, mean reward: 0.593 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.340, 10.267], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.066s, episode steps: 100, steps per second: 1512, episode reward: 58.895, mean reward: 0.589 [0.511, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.134, 10.098], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 61.353, mean reward: 0.614 [0.509, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.631, 10.327], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.076s, episode steps: 100, steps per second: 1309, episode reward: 58.939, mean reward: 0.589 [0.499, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.765, 10.351], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.066s, episode steps: 100, steps per second: 1511, episode reward: 58.233, mean reward: 0.582 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.179, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.066s, episode steps: 100, steps per second: 1507, episode reward: 57.933, mean reward: 0.579 [0.509, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.666, 10.300], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.067s, episode steps: 100, steps per second: 1494, episode reward: 58.852, mean reward: 0.589 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.024, 10.118], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.066s, episode steps: 100, steps per second: 1508, episode reward: 60.807, mean reward: 0.608 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.841, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.082s, episode steps: 100, steps per second: 1223, episode reward: 57.873, mean reward: 0.579 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.579, 10.178], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.067s, episode steps: 100, steps per second: 1497, episode reward: 58.940, mean reward: 0.589 [0.505, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.570, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.072s, episode steps: 100, steps per second: 1398, episode reward: 61.567, mean reward: 0.616 [0.525, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.760, 10.298], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.067s, episode steps: 100, steps per second: 1493, episode reward: 60.689, mean reward: 0.607 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.606, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.066s, episode steps: 100, steps per second: 1505, episode reward: 57.242, mean reward: 0.572 [0.497, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.878, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.066s, episode steps: 100, steps per second: 1518, episode reward: 62.083, mean reward: 0.621 [0.509, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.233, 10.212], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.083s, episode steps: 100, steps per second: 1200, episode reward: 58.365, mean reward: 0.584 [0.505, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.557, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.072s, episode steps: 100, steps per second: 1392, episode reward: 59.592, mean reward: 0.596 [0.509, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.944, 10.130], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.067s, episode steps: 100, steps per second: 1483, episode reward: 58.678, mean reward: 0.587 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.064, 10.191], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.077s, episode steps: 100, steps per second: 1300, episode reward: 57.402, mean reward: 0.574 [0.505, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.713, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.068s, episode steps: 100, steps per second: 1479, episode reward: 63.114, mean reward: 0.631 [0.507, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.765, 10.282], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.088s, episode steps: 100, steps per second: 1135, episode reward: 60.087, mean reward: 0.601 [0.506, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.504, 10.300], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.076s, episode steps: 100, steps per second: 1319, episode reward: 63.184, mean reward: 0.632 [0.512, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.446, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.065s, episode steps: 100, steps per second: 1532, episode reward: 58.322, mean reward: 0.583 [0.505, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.474, 10.118], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.066s, episode steps: 100, steps per second: 1523, episode reward: 61.803, mean reward: 0.618 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.861, 10.483], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.292s, episode steps: 100, steps per second: 77, episode reward: 59.023, mean reward: 0.590 [0.510, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.877, 10.098], loss: 0.014454, mae: 0.117134, mean_q: 0.719167
  5200/100000: episode: 52, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 57.965, mean reward: 0.580 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.626, 10.227], loss: 0.002942, mae: 0.053945, mean_q: 0.914301
  5300/100000: episode: 53, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 61.294, mean reward: 0.613 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.519, 10.288], loss: 0.002630, mae: 0.051352, mean_q: 1.003419
  5400/100000: episode: 54, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.355, mean reward: 0.594 [0.502, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.657, 10.098], loss: 0.002774, mae: 0.051352, mean_q: 1.065176
  5500/100000: episode: 55, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 57.157, mean reward: 0.572 [0.503, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.650, 10.098], loss: 0.002609, mae: 0.051209, mean_q: 1.107582
  5600/100000: episode: 56, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 58.126, mean reward: 0.581 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.491, 10.358], loss: 0.002627, mae: 0.051218, mean_q: 1.134034
  5700/100000: episode: 57, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 62.499, mean reward: 0.625 [0.509, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.932, 10.175], loss: 0.002880, mae: 0.051022, mean_q: 1.145228
  5800/100000: episode: 58, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: 58.409, mean reward: 0.584 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.771, 10.098], loss: 0.002455, mae: 0.049867, mean_q: 1.160684
  5900/100000: episode: 59, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: 58.323, mean reward: 0.583 [0.499, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.162, 10.249], loss: 0.002927, mae: 0.052551, mean_q: 1.163710
  6000/100000: episode: 60, duration: 0.772s, episode steps: 100, steps per second: 130, episode reward: 57.423, mean reward: 0.574 [0.499, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.578, 10.171], loss: 0.002587, mae: 0.050130, mean_q: 1.170893
  6100/100000: episode: 61, duration: 1.092s, episode steps: 100, steps per second: 92, episode reward: 57.233, mean reward: 0.572 [0.507, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.544, 10.215], loss: 0.003025, mae: 0.054076, mean_q: 1.170931
  6200/100000: episode: 62, duration: 0.960s, episode steps: 100, steps per second: 104, episode reward: 57.700, mean reward: 0.577 [0.507, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.811, 10.098], loss: 0.002289, mae: 0.048859, mean_q: 1.176065
  6300/100000: episode: 63, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 61.953, mean reward: 0.620 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.857, 10.420], loss: 0.003014, mae: 0.052922, mean_q: 1.171338
  6400/100000: episode: 64, duration: 0.621s, episode steps: 100, steps per second: 161, episode reward: 58.055, mean reward: 0.581 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.922, 10.148], loss: 0.002735, mae: 0.052156, mean_q: 1.175498
  6500/100000: episode: 65, duration: 0.859s, episode steps: 100, steps per second: 116, episode reward: 58.594, mean reward: 0.586 [0.497, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.990, 10.173], loss: 0.002496, mae: 0.049884, mean_q: 1.176932
  6600/100000: episode: 66, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 57.786, mean reward: 0.578 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.465, 10.098], loss: 0.002817, mae: 0.051608, mean_q: 1.176062
  6700/100000: episode: 67, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.721, mean reward: 0.597 [0.507, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.305, 10.243], loss: 0.002525, mae: 0.049541, mean_q: 1.176887
  6800/100000: episode: 68, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 58.295, mean reward: 0.583 [0.510, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.236, 10.220], loss: 0.002731, mae: 0.051868, mean_q: 1.176957
  6900/100000: episode: 69, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 58.905, mean reward: 0.589 [0.504, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.843, 10.098], loss: 0.002628, mae: 0.050104, mean_q: 1.176727
  7000/100000: episode: 70, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.686, mean reward: 0.587 [0.503, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.013, 10.240], loss: 0.002314, mae: 0.047453, mean_q: 1.177817
  7100/100000: episode: 71, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.828, mean reward: 0.608 [0.499, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.226, 10.098], loss: 0.002679, mae: 0.051824, mean_q: 1.175665
  7200/100000: episode: 72, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 59.453, mean reward: 0.595 [0.510, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.910, 10.296], loss: 0.002442, mae: 0.049508, mean_q: 1.176638
  7300/100000: episode: 73, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.436, mean reward: 0.584 [0.512, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.149, 10.121], loss: 0.002596, mae: 0.049635, mean_q: 1.174675
  7400/100000: episode: 74, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 61.010, mean reward: 0.610 [0.515, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.127, 10.389], loss: 0.002913, mae: 0.053795, mean_q: 1.171980
  7500/100000: episode: 75, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 59.113, mean reward: 0.591 [0.498, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.684, 10.155], loss: 0.002682, mae: 0.051657, mean_q: 1.173380
  7600/100000: episode: 76, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 62.589, mean reward: 0.626 [0.510, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.848, 10.494], loss: 0.002463, mae: 0.049822, mean_q: 1.175147
  7700/100000: episode: 77, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 60.039, mean reward: 0.600 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.046, 10.267], loss: 0.002645, mae: 0.052428, mean_q: 1.177194
  7800/100000: episode: 78, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 57.250, mean reward: 0.572 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.352, 10.098], loss: 0.002516, mae: 0.050295, mean_q: 1.176759
  7900/100000: episode: 79, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 57.363, mean reward: 0.574 [0.507, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.770, 10.098], loss: 0.002845, mae: 0.052484, mean_q: 1.174167
  8000/100000: episode: 80, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 57.535, mean reward: 0.575 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.044, 10.098], loss: 0.002353, mae: 0.049253, mean_q: 1.174829
  8100/100000: episode: 81, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 61.885, mean reward: 0.619 [0.520, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.767, 10.271], loss: 0.002759, mae: 0.053024, mean_q: 1.174191
  8200/100000: episode: 82, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 63.442, mean reward: 0.634 [0.549, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.738, 10.098], loss: 0.002320, mae: 0.049882, mean_q: 1.178401
  8300/100000: episode: 83, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 57.820, mean reward: 0.578 [0.502, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.366, 10.159], loss: 0.002735, mae: 0.052621, mean_q: 1.179348
  8400/100000: episode: 84, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: 57.198, mean reward: 0.572 [0.501, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.719, 10.098], loss: 0.002562, mae: 0.051305, mean_q: 1.177489
  8500/100000: episode: 85, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.492, mean reward: 0.585 [0.502, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.727, 10.244], loss: 0.002439, mae: 0.050586, mean_q: 1.174791
  8600/100000: episode: 86, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.379, mean reward: 0.584 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.821, 10.098], loss: 0.002300, mae: 0.049592, mean_q: 1.177071
  8700/100000: episode: 87, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 57.937, mean reward: 0.579 [0.504, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.634, 10.259], loss: 0.002269, mae: 0.048571, mean_q: 1.174889
  8800/100000: episode: 88, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 56.106, mean reward: 0.561 [0.501, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.149, 10.125], loss: 0.002295, mae: 0.049663, mean_q: 1.172325
  8900/100000: episode: 89, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.831, mean reward: 0.588 [0.508, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.098], loss: 0.002280, mae: 0.049056, mean_q: 1.175615
  9000/100000: episode: 90, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.615, mean reward: 0.576 [0.508, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.788, 10.098], loss: 0.002137, mae: 0.048098, mean_q: 1.174437
  9100/100000: episode: 91, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 56.924, mean reward: 0.569 [0.500, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.571, 10.098], loss: 0.002552, mae: 0.050304, mean_q: 1.168777
  9200/100000: episode: 92, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 58.639, mean reward: 0.586 [0.509, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.081, 10.106], loss: 0.002169, mae: 0.048305, mean_q: 1.168142
  9300/100000: episode: 93, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 60.403, mean reward: 0.604 [0.509, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.718, 10.250], loss: 0.002120, mae: 0.047728, mean_q: 1.169697
  9400/100000: episode: 94, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 58.810, mean reward: 0.588 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.978, 10.294], loss: 0.002200, mae: 0.048022, mean_q: 1.170065
  9500/100000: episode: 95, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 58.632, mean reward: 0.586 [0.501, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.474, 10.098], loss: 0.002435, mae: 0.050967, mean_q: 1.169834
  9600/100000: episode: 96, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 57.003, mean reward: 0.570 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.016, 10.098], loss: 0.002146, mae: 0.049208, mean_q: 1.173476
  9700/100000: episode: 97, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 59.189, mean reward: 0.592 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.028, 10.291], loss: 0.002376, mae: 0.049909, mean_q: 1.170637
  9800/100000: episode: 98, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 56.606, mean reward: 0.566 [0.500, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.155, 10.098], loss: 0.001924, mae: 0.046441, mean_q: 1.171420
  9900/100000: episode: 99, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: 57.922, mean reward: 0.579 [0.514, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.693, 10.098], loss: 0.002204, mae: 0.048949, mean_q: 1.166951
 10000/100000: episode: 100, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 57.828, mean reward: 0.578 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.872, 10.098], loss: 0.002198, mae: 0.048506, mean_q: 1.166276
 10100/100000: episode: 101, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 57.830, mean reward: 0.578 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.825, 10.240], loss: 0.001965, mae: 0.046199, mean_q: 1.165974
 10200/100000: episode: 102, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 59.266, mean reward: 0.593 [0.501, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.600, 10.098], loss: 0.002068, mae: 0.047326, mean_q: 1.162858
 10300/100000: episode: 103, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward: 60.252, mean reward: 0.603 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.250], loss: 0.002139, mae: 0.048152, mean_q: 1.161415
 10400/100000: episode: 104, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 59.028, mean reward: 0.590 [0.512, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.937, 10.098], loss: 0.001940, mae: 0.045940, mean_q: 1.164212
 10500/100000: episode: 105, duration: 0.576s, episode steps: 100, steps per second: 173, episode reward: 57.253, mean reward: 0.573 [0.497, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.929, 10.120], loss: 0.002116, mae: 0.048182, mean_q: 1.165876
 10600/100000: episode: 106, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: 58.912, mean reward: 0.589 [0.504, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.625, 10.387], loss: 0.002024, mae: 0.046802, mean_q: 1.164037
 10700/100000: episode: 107, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 58.423, mean reward: 0.584 [0.505, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.321, 10.105], loss: 0.001774, mae: 0.045188, mean_q: 1.163563
 10800/100000: episode: 108, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.854, mean reward: 0.589 [0.510, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.896, 10.288], loss: 0.002124, mae: 0.047505, mean_q: 1.161203
 10900/100000: episode: 109, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.163, mean reward: 0.592 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.051, 10.098], loss: 0.002129, mae: 0.047853, mean_q: 1.160688
 11000/100000: episode: 110, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 60.848, mean reward: 0.608 [0.516, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.820, 10.385], loss: 0.002185, mae: 0.048723, mean_q: 1.160600
 11100/100000: episode: 111, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 58.198, mean reward: 0.582 [0.499, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.793, 10.164], loss: 0.002028, mae: 0.046990, mean_q: 1.163540
 11200/100000: episode: 112, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 60.528, mean reward: 0.605 [0.509, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.037, 10.098], loss: 0.002222, mae: 0.049106, mean_q: 1.161298
 11300/100000: episode: 113, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.634, mean reward: 0.586 [0.505, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.787, 10.098], loss: 0.002031, mae: 0.048074, mean_q: 1.164526
 11400/100000: episode: 114, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 58.721, mean reward: 0.587 [0.501, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.239, 10.104], loss: 0.002077, mae: 0.047208, mean_q: 1.163622
 11500/100000: episode: 115, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 58.197, mean reward: 0.582 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.223, 10.098], loss: 0.001960, mae: 0.047131, mean_q: 1.161937
 11600/100000: episode: 116, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 60.558, mean reward: 0.606 [0.515, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.831, 10.098], loss: 0.002005, mae: 0.046839, mean_q: 1.166306
 11700/100000: episode: 117, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 59.184, mean reward: 0.592 [0.515, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.312, 10.304], loss: 0.002107, mae: 0.047441, mean_q: 1.162072
 11800/100000: episode: 118, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 57.812, mean reward: 0.578 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.557, 10.098], loss: 0.001906, mae: 0.045538, mean_q: 1.164494
 11900/100000: episode: 119, duration: 0.601s, episode steps: 100, steps per second: 167, episode reward: 59.751, mean reward: 0.598 [0.515, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.104, 10.098], loss: 0.002129, mae: 0.048922, mean_q: 1.167294
 12000/100000: episode: 120, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.185, mean reward: 0.582 [0.499, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.403, 10.098], loss: 0.001959, mae: 0.047057, mean_q: 1.163862
 12100/100000: episode: 121, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 59.499, mean reward: 0.595 [0.504, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.884, 10.366], loss: 0.002065, mae: 0.047169, mean_q: 1.165051
 12200/100000: episode: 122, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 57.576, mean reward: 0.576 [0.508, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.874, 10.138], loss: 0.001989, mae: 0.047076, mean_q: 1.164396
 12300/100000: episode: 123, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 57.421, mean reward: 0.574 [0.507, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.790, 10.098], loss: 0.002156, mae: 0.048463, mean_q: 1.163552
 12400/100000: episode: 124, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 59.523, mean reward: 0.595 [0.502, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.514, 10.171], loss: 0.001755, mae: 0.045032, mean_q: 1.164880
 12500/100000: episode: 125, duration: 0.816s, episode steps: 100, steps per second: 122, episode reward: 58.420, mean reward: 0.584 [0.511, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.466, 10.116], loss: 0.002068, mae: 0.047600, mean_q: 1.162495
 12600/100000: episode: 126, duration: 1.033s, episode steps: 100, steps per second: 97, episode reward: 58.866, mean reward: 0.589 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.807, 10.140], loss: 0.002056, mae: 0.047392, mean_q: 1.160856
 12700/100000: episode: 127, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: 58.319, mean reward: 0.583 [0.504, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.937, 10.348], loss: 0.001949, mae: 0.046236, mean_q: 1.159701
 12800/100000: episode: 128, duration: 1.017s, episode steps: 100, steps per second: 98, episode reward: 61.783, mean reward: 0.618 [0.506, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.690, 10.369], loss: 0.001980, mae: 0.047500, mean_q: 1.160773
 12900/100000: episode: 129, duration: 0.819s, episode steps: 100, steps per second: 122, episode reward: 57.953, mean reward: 0.580 [0.511, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.266, 10.154], loss: 0.001799, mae: 0.045193, mean_q: 1.162694
 13000/100000: episode: 130, duration: 0.608s, episode steps: 100, steps per second: 165, episode reward: 59.042, mean reward: 0.590 [0.498, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.341, 10.147], loss: 0.002029, mae: 0.047107, mean_q: 1.161496
 13100/100000: episode: 131, duration: 0.632s, episode steps: 100, steps per second: 158, episode reward: 57.189, mean reward: 0.572 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.873, 10.163], loss: 0.001899, mae: 0.045722, mean_q: 1.162160
 13200/100000: episode: 132, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 57.911, mean reward: 0.579 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.307, 10.208], loss: 0.001807, mae: 0.044928, mean_q: 1.160348
 13300/100000: episode: 133, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 57.907, mean reward: 0.579 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.878, 10.272], loss: 0.001862, mae: 0.045645, mean_q: 1.159318
 13400/100000: episode: 134, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.991, mean reward: 0.590 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.905, 10.340], loss: 0.001805, mae: 0.045104, mean_q: 1.160369
 13500/100000: episode: 135, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 58.352, mean reward: 0.584 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.960, 10.098], loss: 0.002016, mae: 0.047430, mean_q: 1.157502
 13600/100000: episode: 136, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 61.216, mean reward: 0.612 [0.510, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.069, 10.098], loss: 0.001911, mae: 0.046717, mean_q: 1.161409
 13700/100000: episode: 137, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 57.131, mean reward: 0.571 [0.512, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.907, 10.257], loss: 0.002103, mae: 0.049107, mean_q: 1.163123
 13800/100000: episode: 138, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 61.215, mean reward: 0.612 [0.512, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.583, 10.120], loss: 0.001864, mae: 0.045630, mean_q: 1.165022
 13900/100000: episode: 139, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 56.841, mean reward: 0.568 [0.507, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.324, 10.396], loss: 0.001971, mae: 0.047014, mean_q: 1.164963
 14000/100000: episode: 140, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 58.640, mean reward: 0.586 [0.507, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.730, 10.186], loss: 0.001863, mae: 0.045624, mean_q: 1.161875
 14100/100000: episode: 141, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 57.436, mean reward: 0.574 [0.508, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.053, 10.098], loss: 0.002130, mae: 0.048904, mean_q: 1.162061
 14200/100000: episode: 142, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 59.339, mean reward: 0.593 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.204, 10.171], loss: 0.001867, mae: 0.045428, mean_q: 1.161557
 14300/100000: episode: 143, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 61.923, mean reward: 0.619 [0.501, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.661, 10.230], loss: 0.001831, mae: 0.046160, mean_q: 1.163540
 14400/100000: episode: 144, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.950, mean reward: 0.590 [0.506, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.519, 10.098], loss: 0.001961, mae: 0.047362, mean_q: 1.163357
 14500/100000: episode: 145, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 57.470, mean reward: 0.575 [0.505, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.805, 10.144], loss: 0.001977, mae: 0.046882, mean_q: 1.164079
 14600/100000: episode: 146, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 57.144, mean reward: 0.571 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.563, 10.148], loss: 0.001903, mae: 0.046890, mean_q: 1.165051
 14700/100000: episode: 147, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 61.621, mean reward: 0.616 [0.501, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.376, 10.224], loss: 0.001850, mae: 0.046260, mean_q: 1.164533
 14800/100000: episode: 148, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 56.748, mean reward: 0.567 [0.508, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.236], loss: 0.001801, mae: 0.045808, mean_q: 1.164425
 14900/100000: episode: 149, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 59.386, mean reward: 0.594 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.289, 10.098], loss: 0.001817, mae: 0.045012, mean_q: 1.163248
[Info] 1-TH LEVEL FOUND: 1.2858550548553467, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.720s, episode steps: 100, steps per second: 17, episode reward: 58.453, mean reward: 0.585 [0.512, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.844, 10.098], loss: 0.001836, mae: 0.046069, mean_q: 1.164732
 15095/100000: episode: 151, duration: 1.095s, episode steps: 95, steps per second: 87, episode reward: 54.886, mean reward: 0.578 [0.498, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.739, 10.100], loss: 0.001880, mae: 0.046224, mean_q: 1.162380
 15195/100000: episode: 152, duration: 0.921s, episode steps: 100, steps per second: 109, episode reward: 59.379, mean reward: 0.594 [0.518, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.667, 10.154], loss: 0.001822, mae: 0.045789, mean_q: 1.161938
 15293/100000: episode: 153, duration: 0.983s, episode steps: 98, steps per second: 100, episode reward: 59.816, mean reward: 0.610 [0.510, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.953, 10.100], loss: 0.001857, mae: 0.046225, mean_q: 1.163567
 15389/100000: episode: 154, duration: 0.909s, episode steps: 96, steps per second: 106, episode reward: 55.442, mean reward: 0.578 [0.500, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.451, 10.243], loss: 0.001726, mae: 0.045266, mean_q: 1.164528
 15484/100000: episode: 155, duration: 0.682s, episode steps: 95, steps per second: 139, episode reward: 57.907, mean reward: 0.610 [0.511, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.607, 10.100], loss: 0.001908, mae: 0.047201, mean_q: 1.163588
 15580/100000: episode: 156, duration: 0.843s, episode steps: 96, steps per second: 114, episode reward: 61.418, mean reward: 0.640 [0.513, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.000, 10.418], loss: 0.001870, mae: 0.046912, mean_q: 1.164262
 15680/100000: episode: 157, duration: 1.043s, episode steps: 100, steps per second: 96, episode reward: 58.189, mean reward: 0.582 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-0.322, 10.123], loss: 0.001834, mae: 0.046316, mean_q: 1.165984
 15776/100000: episode: 158, duration: 0.585s, episode steps: 96, steps per second: 164, episode reward: 58.588, mean reward: 0.610 [0.502, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-0.271, 10.183], loss: 0.001915, mae: 0.046643, mean_q: 1.165053
 15872/100000: episode: 159, duration: 0.580s, episode steps: 96, steps per second: 166, episode reward: 57.853, mean reward: 0.603 [0.504, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.784, 10.248], loss: 0.001994, mae: 0.048114, mean_q: 1.167302
 15972/100000: episode: 160, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 62.666, mean reward: 0.627 [0.509, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.367, 10.100], loss: 0.001923, mae: 0.047468, mean_q: 1.167933
 16070/100000: episode: 161, duration: 0.552s, episode steps: 98, steps per second: 178, episode reward: 56.304, mean reward: 0.575 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.711, 10.261], loss: 0.001937, mae: 0.048233, mean_q: 1.169874
 16165/100000: episode: 162, duration: 0.528s, episode steps: 95, steps per second: 180, episode reward: 54.951, mean reward: 0.578 [0.510, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.098, 10.184], loss: 0.001918, mae: 0.046365, mean_q: 1.167055
 16260/100000: episode: 163, duration: 0.660s, episode steps: 95, steps per second: 144, episode reward: 53.933, mean reward: 0.568 [0.499, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-1.282, 10.100], loss: 0.001883, mae: 0.046599, mean_q: 1.170202
 16356/100000: episode: 164, duration: 0.597s, episode steps: 96, steps per second: 161, episode reward: 56.520, mean reward: 0.589 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.554, 10.278], loss: 0.001792, mae: 0.045622, mean_q: 1.167139
 16452/100000: episode: 165, duration: 0.625s, episode steps: 96, steps per second: 154, episode reward: 55.172, mean reward: 0.575 [0.502, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.530, 10.100], loss: 0.001798, mae: 0.045395, mean_q: 1.168475
 16548/100000: episode: 166, duration: 0.812s, episode steps: 96, steps per second: 118, episode reward: 56.539, mean reward: 0.589 [0.511, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.922, 10.479], loss: 0.001758, mae: 0.045489, mean_q: 1.167647
 16643/100000: episode: 167, duration: 0.817s, episode steps: 95, steps per second: 116, episode reward: 58.002, mean reward: 0.611 [0.504, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.206, 10.158], loss: 0.001983, mae: 0.048060, mean_q: 1.165025
 16739/100000: episode: 168, duration: 0.720s, episode steps: 96, steps per second: 133, episode reward: 57.133, mean reward: 0.595 [0.508, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.727, 10.415], loss: 0.001987, mae: 0.048157, mean_q: 1.166450
 16834/100000: episode: 169, duration: 0.573s, episode steps: 95, steps per second: 166, episode reward: 55.763, mean reward: 0.587 [0.511, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-1.138, 10.100], loss: 0.001778, mae: 0.045790, mean_q: 1.172532
 16929/100000: episode: 170, duration: 0.541s, episode steps: 95, steps per second: 176, episode reward: 55.953, mean reward: 0.589 [0.500, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.864, 10.366], loss: 0.001854, mae: 0.046835, mean_q: 1.170416
 17029/100000: episode: 171, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 58.504, mean reward: 0.585 [0.511, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-2.308, 10.234], loss: 0.001945, mae: 0.046868, mean_q: 1.168972
 17125/100000: episode: 172, duration: 0.633s, episode steps: 96, steps per second: 152, episode reward: 55.525, mean reward: 0.578 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.193, 10.316], loss: 0.001992, mae: 0.048811, mean_q: 1.167584
 17220/100000: episode: 173, duration: 0.725s, episode steps: 95, steps per second: 131, episode reward: 56.206, mean reward: 0.592 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.871, 10.100], loss: 0.001879, mae: 0.047317, mean_q: 1.168459
 17318/100000: episode: 174, duration: 0.558s, episode steps: 98, steps per second: 176, episode reward: 55.104, mean reward: 0.562 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.708, 10.100], loss: 0.001825, mae: 0.046121, mean_q: 1.167408
 17414/100000: episode: 175, duration: 0.553s, episode steps: 96, steps per second: 174, episode reward: 55.122, mean reward: 0.574 [0.504, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.420, 10.163], loss: 0.001827, mae: 0.046497, mean_q: 1.164511
 17514/100000: episode: 176, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 58.555, mean reward: 0.586 [0.511, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-0.483, 10.100], loss: 0.001941, mae: 0.047366, mean_q: 1.167051
 17609/100000: episode: 177, duration: 0.552s, episode steps: 95, steps per second: 172, episode reward: 60.539, mean reward: 0.637 [0.506, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-0.905, 10.100], loss: 0.001912, mae: 0.045923, mean_q: 1.166253
 17704/100000: episode: 178, duration: 0.552s, episode steps: 95, steps per second: 172, episode reward: 55.074, mean reward: 0.580 [0.513, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-0.401, 10.100], loss: 0.001934, mae: 0.046873, mean_q: 1.166560
 17799/100000: episode: 179, duration: 0.554s, episode steps: 95, steps per second: 171, episode reward: 56.450, mean reward: 0.594 [0.508, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.289, 10.349], loss: 0.001850, mae: 0.046328, mean_q: 1.164193
 17894/100000: episode: 180, duration: 0.539s, episode steps: 95, steps per second: 176, episode reward: 56.435, mean reward: 0.594 [0.501, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.680, 10.100], loss: 0.001713, mae: 0.044736, mean_q: 1.166589
 17989/100000: episode: 181, duration: 0.574s, episode steps: 95, steps per second: 165, episode reward: 57.540, mean reward: 0.606 [0.518, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.895, 10.203], loss: 0.001919, mae: 0.046885, mean_q: 1.164695
 18084/100000: episode: 182, duration: 0.580s, episode steps: 95, steps per second: 164, episode reward: 55.418, mean reward: 0.583 [0.510, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.778, 10.105], loss: 0.001902, mae: 0.047526, mean_q: 1.168550
 18180/100000: episode: 183, duration: 0.577s, episode steps: 96, steps per second: 166, episode reward: 57.473, mean reward: 0.599 [0.507, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-1.121, 10.197], loss: 0.001913, mae: 0.047070, mean_q: 1.168286
 18275/100000: episode: 184, duration: 0.569s, episode steps: 95, steps per second: 167, episode reward: 55.041, mean reward: 0.579 [0.499, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.725, 10.127], loss: 0.001833, mae: 0.046190, mean_q: 1.168528
 18370/100000: episode: 185, duration: 0.545s, episode steps: 95, steps per second: 174, episode reward: 55.529, mean reward: 0.585 [0.505, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.978, 10.100], loss: 0.001745, mae: 0.045254, mean_q: 1.167844
 18465/100000: episode: 186, duration: 0.538s, episode steps: 95, steps per second: 177, episode reward: 56.688, mean reward: 0.597 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.053, 10.296], loss: 0.001842, mae: 0.046715, mean_q: 1.164166
 18560/100000: episode: 187, duration: 0.539s, episode steps: 95, steps per second: 176, episode reward: 54.515, mean reward: 0.574 [0.503, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.278, 10.236], loss: 0.001724, mae: 0.044921, mean_q: 1.160423
 18656/100000: episode: 188, duration: 0.545s, episode steps: 96, steps per second: 176, episode reward: 58.099, mean reward: 0.605 [0.508, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-0.788, 10.100], loss: 0.001927, mae: 0.047192, mean_q: 1.164473
 18751/100000: episode: 189, duration: 0.549s, episode steps: 95, steps per second: 173, episode reward: 55.743, mean reward: 0.587 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.507, 10.100], loss: 0.001846, mae: 0.046659, mean_q: 1.165298
 18847/100000: episode: 190, duration: 0.570s, episode steps: 96, steps per second: 168, episode reward: 56.991, mean reward: 0.594 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.774, 10.100], loss: 0.001758, mae: 0.045365, mean_q: 1.167628
 18945/100000: episode: 191, duration: 0.560s, episode steps: 98, steps per second: 175, episode reward: 56.278, mean reward: 0.574 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-0.577, 10.191], loss: 0.001803, mae: 0.045413, mean_q: 1.165012
 19041/100000: episode: 192, duration: 0.567s, episode steps: 96, steps per second: 169, episode reward: 65.957, mean reward: 0.687 [0.534, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-0.813, 10.265], loss: 0.001882, mae: 0.046290, mean_q: 1.165154
 19137/100000: episode: 193, duration: 0.532s, episode steps: 96, steps per second: 181, episode reward: 57.917, mean reward: 0.603 [0.504, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-1.573, 10.100], loss: 0.001808, mae: 0.045529, mean_q: 1.169000
 19232/100000: episode: 194, duration: 0.542s, episode steps: 95, steps per second: 175, episode reward: 54.361, mean reward: 0.572 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-0.963, 10.337], loss: 0.001845, mae: 0.046117, mean_q: 1.171832
 19328/100000: episode: 195, duration: 0.538s, episode steps: 96, steps per second: 179, episode reward: 57.780, mean reward: 0.602 [0.499, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-1.348, 10.146], loss: 0.001849, mae: 0.045981, mean_q: 1.173573
 19428/100000: episode: 196, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 59.081, mean reward: 0.591 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-1.098, 10.209], loss: 0.001808, mae: 0.046243, mean_q: 1.172570
 19528/100000: episode: 197, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 58.440, mean reward: 0.584 [0.498, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-1.726, 10.115], loss: 0.001870, mae: 0.047028, mean_q: 1.171549
 19623/100000: episode: 198, duration: 0.571s, episode steps: 95, steps per second: 166, episode reward: 54.421, mean reward: 0.573 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-1.049, 10.100], loss: 0.001855, mae: 0.046100, mean_q: 1.171161
 19718/100000: episode: 199, duration: 0.554s, episode steps: 95, steps per second: 172, episode reward: 62.788, mean reward: 0.661 [0.512, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.461, 10.282], loss: 0.001836, mae: 0.046752, mean_q: 1.170596
 19814/100000: episode: 200, duration: 0.614s, episode steps: 96, steps per second: 156, episode reward: 55.425, mean reward: 0.577 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-1.176, 10.100], loss: 0.001831, mae: 0.047131, mean_q: 1.171674
 19909/100000: episode: 201, duration: 0.595s, episode steps: 95, steps per second: 160, episode reward: 54.873, mean reward: 0.578 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.245, 10.119], loss: 0.001967, mae: 0.048454, mean_q: 1.174833
 20004/100000: episode: 202, duration: 0.576s, episode steps: 95, steps per second: 165, episode reward: 56.253, mean reward: 0.592 [0.504, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.017, 10.144], loss: 0.001743, mae: 0.045378, mean_q: 1.172421
 20102/100000: episode: 203, duration: 0.568s, episode steps: 98, steps per second: 172, episode reward: 58.619, mean reward: 0.598 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-0.656, 10.100], loss: 0.001768, mae: 0.046228, mean_q: 1.172740
 20198/100000: episode: 204, duration: 0.520s, episode steps: 96, steps per second: 185, episode reward: 55.773, mean reward: 0.581 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-1.462, 10.256], loss: 0.001903, mae: 0.047684, mean_q: 1.173371
 20294/100000: episode: 205, duration: 0.589s, episode steps: 96, steps per second: 163, episode reward: 55.031, mean reward: 0.573 [0.509, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-0.478, 10.350], loss: 0.001705, mae: 0.045649, mean_q: 1.171818
 20394/100000: episode: 206, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 59.548, mean reward: 0.595 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.451 [-1.571, 10.322], loss: 0.001796, mae: 0.046289, mean_q: 1.173155
 20490/100000: episode: 207, duration: 0.579s, episode steps: 96, steps per second: 166, episode reward: 57.405, mean reward: 0.598 [0.501, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.692, 10.191], loss: 0.001786, mae: 0.045619, mean_q: 1.173918
 20590/100000: episode: 208, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 57.309, mean reward: 0.573 [0.501, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.045, 10.100], loss: 0.001789, mae: 0.046197, mean_q: 1.171029
 20686/100000: episode: 209, duration: 0.538s, episode steps: 96, steps per second: 178, episode reward: 54.831, mean reward: 0.571 [0.501, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.874, 10.100], loss: 0.001889, mae: 0.047320, mean_q: 1.171500
 20786/100000: episode: 210, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.372, mean reward: 0.584 [0.502, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-0.471, 10.227], loss: 0.001725, mae: 0.045020, mean_q: 1.167123
 20881/100000: episode: 211, duration: 0.514s, episode steps: 95, steps per second: 185, episode reward: 55.299, mean reward: 0.582 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.468, 10.157], loss: 0.001677, mae: 0.045131, mean_q: 1.171279
 20976/100000: episode: 212, duration: 0.558s, episode steps: 95, steps per second: 170, episode reward: 54.842, mean reward: 0.577 [0.505, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.465, 10.114], loss: 0.001675, mae: 0.044457, mean_q: 1.166872
 21076/100000: episode: 213, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 59.878, mean reward: 0.599 [0.512, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.797, 10.556], loss: 0.001726, mae: 0.044893, mean_q: 1.168298
 21171/100000: episode: 214, duration: 0.535s, episode steps: 95, steps per second: 178, episode reward: 53.705, mean reward: 0.565 [0.510, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.997, 10.197], loss: 0.001780, mae: 0.045655, mean_q: 1.164486
 21266/100000: episode: 215, duration: 0.561s, episode steps: 95, steps per second: 169, episode reward: 56.484, mean reward: 0.595 [0.509, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.496, 10.243], loss: 0.001768, mae: 0.045707, mean_q: 1.169016
 21361/100000: episode: 216, duration: 0.535s, episode steps: 95, steps per second: 178, episode reward: 60.700, mean reward: 0.639 [0.516, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.826, 10.487], loss: 0.001742, mae: 0.045150, mean_q: 1.172051
 21459/100000: episode: 217, duration: 0.579s, episode steps: 98, steps per second: 169, episode reward: 57.898, mean reward: 0.591 [0.498, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.857, 10.143], loss: 0.001860, mae: 0.046541, mean_q: 1.169189
 21554/100000: episode: 218, duration: 0.595s, episode steps: 95, steps per second: 160, episode reward: 56.659, mean reward: 0.596 [0.510, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.826, 10.141], loss: 0.001858, mae: 0.047152, mean_q: 1.170471
 21649/100000: episode: 219, duration: 0.553s, episode steps: 95, steps per second: 172, episode reward: 59.909, mean reward: 0.631 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.942, 10.456], loss: 0.001845, mae: 0.047680, mean_q: 1.168016
 21744/100000: episode: 220, duration: 0.567s, episode steps: 95, steps per second: 167, episode reward: 55.371, mean reward: 0.583 [0.505, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-1.018, 10.100], loss: 0.001863, mae: 0.046936, mean_q: 1.173080
 21840/100000: episode: 221, duration: 0.545s, episode steps: 96, steps per second: 176, episode reward: 58.462, mean reward: 0.609 [0.505, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.581, 10.100], loss: 0.001632, mae: 0.043918, mean_q: 1.171593
 21936/100000: episode: 222, duration: 0.544s, episode steps: 96, steps per second: 176, episode reward: 56.135, mean reward: 0.585 [0.501, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-1.271, 10.317], loss: 0.001833, mae: 0.046896, mean_q: 1.168501
 22031/100000: episode: 223, duration: 0.556s, episode steps: 95, steps per second: 171, episode reward: 57.763, mean reward: 0.608 [0.511, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-0.963, 10.288], loss: 0.001666, mae: 0.044037, mean_q: 1.170835
 22127/100000: episode: 224, duration: 0.562s, episode steps: 96, steps per second: 171, episode reward: 56.948, mean reward: 0.593 [0.508, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-1.288, 10.118], loss: 0.001784, mae: 0.045537, mean_q: 1.171595
 22227/100000: episode: 225, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.471, mean reward: 0.585 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.917, 10.108], loss: 0.001711, mae: 0.045376, mean_q: 1.173529
 22322/100000: episode: 226, duration: 0.519s, episode steps: 95, steps per second: 183, episode reward: 57.820, mean reward: 0.609 [0.501, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.920, 10.100], loss: 0.001697, mae: 0.045013, mean_q: 1.175715
 22417/100000: episode: 227, duration: 0.543s, episode steps: 95, steps per second: 175, episode reward: 55.464, mean reward: 0.584 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.705, 10.100], loss: 0.001848, mae: 0.046493, mean_q: 1.172696
 22512/100000: episode: 228, duration: 0.532s, episode steps: 95, steps per second: 178, episode reward: 55.421, mean reward: 0.583 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.070, 10.197], loss: 0.001703, mae: 0.045428, mean_q: 1.175625
 22612/100000: episode: 229, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 58.362, mean reward: 0.584 [0.502, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.626, 10.291], loss: 0.001747, mae: 0.046147, mean_q: 1.174801
 22712/100000: episode: 230, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 58.588, mean reward: 0.586 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-1.137, 10.320], loss: 0.001788, mae: 0.045870, mean_q: 1.176232
 22807/100000: episode: 231, duration: 0.619s, episode steps: 95, steps per second: 154, episode reward: 55.986, mean reward: 0.589 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.067, 10.191], loss: 0.001796, mae: 0.046072, mean_q: 1.173607
 22903/100000: episode: 232, duration: 0.553s, episode steps: 96, steps per second: 174, episode reward: 56.671, mean reward: 0.590 [0.515, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-0.546, 10.100], loss: 0.001669, mae: 0.044853, mean_q: 1.171636
 22999/100000: episode: 233, duration: 0.532s, episode steps: 96, steps per second: 181, episode reward: 59.617, mean reward: 0.621 [0.505, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.721, 10.398], loss: 0.001694, mae: 0.045132, mean_q: 1.171510
 23095/100000: episode: 234, duration: 0.553s, episode steps: 96, steps per second: 174, episode reward: 57.700, mean reward: 0.601 [0.510, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.522, 10.100], loss: 0.001688, mae: 0.044751, mean_q: 1.173132
 23191/100000: episode: 235, duration: 0.539s, episode steps: 96, steps per second: 178, episode reward: 56.282, mean reward: 0.586 [0.512, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-0.808, 10.100], loss: 0.001701, mae: 0.044992, mean_q: 1.171317
 23286/100000: episode: 236, duration: 0.540s, episode steps: 95, steps per second: 176, episode reward: 57.380, mean reward: 0.604 [0.520, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.342, 10.152], loss: 0.001870, mae: 0.046799, mean_q: 1.170156
 23381/100000: episode: 237, duration: 0.565s, episode steps: 95, steps per second: 168, episode reward: 56.665, mean reward: 0.596 [0.503, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.491, 10.100], loss: 0.001826, mae: 0.046166, mean_q: 1.172015
 23481/100000: episode: 238, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 59.635, mean reward: 0.596 [0.523, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.020, 10.135], loss: 0.001735, mae: 0.045906, mean_q: 1.174516
 23576/100000: episode: 239, duration: 0.552s, episode steps: 95, steps per second: 172, episode reward: 54.656, mean reward: 0.575 [0.504, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.591, 10.100], loss: 0.001781, mae: 0.045818, mean_q: 1.173822
[Info] 2-TH LEVEL FOUND: 1.3429548740386963, Considering 10/90 traces
 23671/100000: episode: 240, duration: 4.956s, episode steps: 95, steps per second: 19, episode reward: 58.023, mean reward: 0.611 [0.504, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.413, 10.223], loss: 0.001654, mae: 0.044773, mean_q: 1.174904
 23708/100000: episode: 241, duration: 0.219s, episode steps: 37, steps per second: 169, episode reward: 26.045, mean reward: 0.704 [0.626, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.826, 10.317], loss: 0.001773, mae: 0.046428, mean_q: 1.171743
 23779/100000: episode: 242, duration: 0.385s, episode steps: 71, steps per second: 185, episode reward: 42.464, mean reward: 0.598 [0.513, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.728 [-0.497, 10.231], loss: 0.001744, mae: 0.045354, mean_q: 1.174769
 23830/100000: episode: 243, duration: 0.289s, episode steps: 51, steps per second: 177, episode reward: 34.104, mean reward: 0.669 [0.572, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.763, 10.262], loss: 0.001778, mae: 0.045738, mean_q: 1.179578
 23923/100000: episode: 244, duration: 0.502s, episode steps: 93, steps per second: 185, episode reward: 56.369, mean reward: 0.606 [0.508, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.520 [-0.607, 10.100], loss: 0.001631, mae: 0.043876, mean_q: 1.175588
 23994/100000: episode: 245, duration: 0.429s, episode steps: 71, steps per second: 166, episode reward: 45.798, mean reward: 0.645 [0.518, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-0.926, 10.100], loss: 0.001620, mae: 0.043775, mean_q: 1.175520
 24040/100000: episode: 246, duration: 0.259s, episode steps: 46, steps per second: 178, episode reward: 33.430, mean reward: 0.727 [0.598, 0.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.571, 10.507], loss: 0.001811, mae: 0.046267, mean_q: 1.171370
 24077/100000: episode: 247, duration: 0.207s, episode steps: 37, steps per second: 178, episode reward: 24.644, mean reward: 0.666 [0.559, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.625, 10.402], loss: 0.001810, mae: 0.046680, mean_q: 1.173764
 24148/100000: episode: 248, duration: 0.401s, episode steps: 71, steps per second: 177, episode reward: 47.206, mean reward: 0.665 [0.527, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-0.440, 10.100], loss: 0.001860, mae: 0.046252, mean_q: 1.174805
 24241/100000: episode: 249, duration: 0.534s, episode steps: 93, steps per second: 174, episode reward: 54.752, mean reward: 0.589 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.780, 10.100], loss: 0.001737, mae: 0.046027, mean_q: 1.179645
 24333/100000: episode: 250, duration: 0.534s, episode steps: 92, steps per second: 172, episode reward: 53.477, mean reward: 0.581 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-0.224, 10.217], loss: 0.001904, mae: 0.047860, mean_q: 1.183161
 24371/100000: episode: 251, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 32.291, mean reward: 0.850 [0.689, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.636, 10.535], loss: 0.001693, mae: 0.045300, mean_q: 1.180463
 24409/100000: episode: 252, duration: 0.227s, episode steps: 38, steps per second: 168, episode reward: 26.578, mean reward: 0.699 [0.586, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.098, 10.449], loss: 0.001831, mae: 0.047006, mean_q: 1.186863
 24502/100000: episode: 253, duration: 0.514s, episode steps: 93, steps per second: 181, episode reward: 53.882, mean reward: 0.579 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.517 [-0.999, 10.227], loss: 0.001956, mae: 0.048177, mean_q: 1.188117
 24595/100000: episode: 254, duration: 0.503s, episode steps: 93, steps per second: 185, episode reward: 55.034, mean reward: 0.592 [0.499, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-1.270, 10.169], loss: 0.001888, mae: 0.047627, mean_q: 1.185831
 24632/100000: episode: 255, duration: 0.206s, episode steps: 37, steps per second: 179, episode reward: 25.898, mean reward: 0.700 [0.611, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.075, 10.358], loss: 0.001914, mae: 0.048526, mean_q: 1.188604
 24662/100000: episode: 256, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 19.222, mean reward: 0.641 [0.567, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.132, 10.281], loss: 0.001729, mae: 0.045916, mean_q: 1.187242
 24755/100000: episode: 257, duration: 0.521s, episode steps: 93, steps per second: 178, episode reward: 53.145, mean reward: 0.571 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-1.185, 10.100], loss: 0.001855, mae: 0.046577, mean_q: 1.191572
 24826/100000: episode: 258, duration: 0.393s, episode steps: 71, steps per second: 181, episode reward: 43.303, mean reward: 0.610 [0.508, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.712 [-0.578, 10.100], loss: 0.001952, mae: 0.048210, mean_q: 1.179450
 24872/100000: episode: 259, duration: 0.263s, episode steps: 46, steps per second: 175, episode reward: 31.436, mean reward: 0.683 [0.603, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.511, 10.403], loss: 0.001908, mae: 0.047577, mean_q: 1.188358
 24965/100000: episode: 260, duration: 0.544s, episode steps: 93, steps per second: 171, episode reward: 55.871, mean reward: 0.601 [0.507, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.846, 10.100], loss: 0.001795, mae: 0.046904, mean_q: 1.189308
 25012/100000: episode: 261, duration: 0.270s, episode steps: 47, steps per second: 174, episode reward: 30.276, mean reward: 0.644 [0.522, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.889, 10.103], loss: 0.001904, mae: 0.048210, mean_q: 1.183939
 25049/100000: episode: 262, duration: 0.211s, episode steps: 37, steps per second: 176, episode reward: 26.696, mean reward: 0.722 [0.613, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.063, 10.492], loss: 0.001656, mae: 0.044760, mean_q: 1.191842
 25141/100000: episode: 263, duration: 0.568s, episode steps: 92, steps per second: 162, episode reward: 55.584, mean reward: 0.604 [0.516, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-0.834, 10.172], loss: 0.001817, mae: 0.046257, mean_q: 1.190431
 25212/100000: episode: 264, duration: 0.437s, episode steps: 71, steps per second: 163, episode reward: 45.047, mean reward: 0.634 [0.525, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-0.547, 10.227], loss: 0.001823, mae: 0.046339, mean_q: 1.197120
 25259/100000: episode: 265, duration: 0.280s, episode steps: 47, steps per second: 168, episode reward: 30.040, mean reward: 0.639 [0.515, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.251, 10.100], loss: 0.001903, mae: 0.046799, mean_q: 1.193589
 25352/100000: episode: 266, duration: 0.559s, episode steps: 93, steps per second: 166, episode reward: 57.779, mean reward: 0.621 [0.514, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.448, 10.100], loss: 0.001848, mae: 0.047455, mean_q: 1.194880
 25382/100000: episode: 267, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 24.333, mean reward: 0.811 [0.663, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.415, 10.526], loss: 0.001874, mae: 0.046669, mean_q: 1.197177
 25453/100000: episode: 268, duration: 0.395s, episode steps: 71, steps per second: 180, episode reward: 41.674, mean reward: 0.587 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-0.796, 10.195], loss: 0.001902, mae: 0.047832, mean_q: 1.199159
 25546/100000: episode: 269, duration: 0.565s, episode steps: 93, steps per second: 165, episode reward: 56.171, mean reward: 0.604 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.527 [-0.744, 10.100], loss: 0.001946, mae: 0.048329, mean_q: 1.196355
 25639/100000: episode: 270, duration: 0.562s, episode steps: 93, steps per second: 165, episode reward: 55.609, mean reward: 0.598 [0.511, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-0.993, 10.134], loss: 0.001812, mae: 0.046516, mean_q: 1.200888
 25669/100000: episode: 271, duration: 0.180s, episode steps: 30, steps per second: 167, episode reward: 20.313, mean reward: 0.677 [0.606, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.035, 10.377], loss: 0.001833, mae: 0.046714, mean_q: 1.196127
 25716/100000: episode: 272, duration: 0.331s, episode steps: 47, steps per second: 142, episode reward: 28.760, mean reward: 0.612 [0.502, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.967, 10.100], loss: 0.001707, mae: 0.045967, mean_q: 1.204883
 25753/100000: episode: 273, duration: 0.273s, episode steps: 37, steps per second: 136, episode reward: 22.749, mean reward: 0.615 [0.517, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.404, 10.220], loss: 0.001867, mae: 0.047914, mean_q: 1.207920
 25790/100000: episode: 274, duration: 0.238s, episode steps: 37, steps per second: 155, episode reward: 22.143, mean reward: 0.598 [0.515, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.248, 10.100], loss: 0.001946, mae: 0.047811, mean_q: 1.198585
 25836/100000: episode: 275, duration: 0.271s, episode steps: 46, steps per second: 170, episode reward: 27.410, mean reward: 0.596 [0.511, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.250, 10.100], loss: 0.001906, mae: 0.048094, mean_q: 1.198096
 25929/100000: episode: 276, duration: 0.577s, episode steps: 93, steps per second: 161, episode reward: 55.848, mean reward: 0.601 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.574, 10.342], loss: 0.001786, mae: 0.046498, mean_q: 1.203767
 26021/100000: episode: 277, duration: 0.553s, episode steps: 92, steps per second: 166, episode reward: 57.387, mean reward: 0.624 [0.507, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.528 [-1.045, 10.100], loss: 0.001983, mae: 0.048722, mean_q: 1.204287
 26092/100000: episode: 278, duration: 0.393s, episode steps: 71, steps per second: 181, episode reward: 45.176, mean reward: 0.636 [0.534, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.722 [-0.265, 10.384], loss: 0.001817, mae: 0.046347, mean_q: 1.206092
 26143/100000: episode: 279, duration: 0.306s, episode steps: 51, steps per second: 167, episode reward: 33.442, mean reward: 0.656 [0.533, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-1.841, 10.433], loss: 0.001767, mae: 0.045447, mean_q: 1.201768
 26194/100000: episode: 280, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 32.382, mean reward: 0.635 [0.515, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.385, 10.100], loss: 0.001915, mae: 0.048034, mean_q: 1.207656
 26287/100000: episode: 281, duration: 0.549s, episode steps: 93, steps per second: 169, episode reward: 54.394, mean reward: 0.585 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-0.373, 10.115], loss: 0.001670, mae: 0.045026, mean_q: 1.205406
 26334/100000: episode: 282, duration: 0.292s, episode steps: 47, steps per second: 161, episode reward: 30.401, mean reward: 0.647 [0.547, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.293, 10.255], loss: 0.001822, mae: 0.047166, mean_q: 1.206079
 26427/100000: episode: 283, duration: 0.505s, episode steps: 93, steps per second: 184, episode reward: 56.741, mean reward: 0.610 [0.513, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-1.019, 10.264], loss: 0.001717, mae: 0.045777, mean_q: 1.209955
 26498/100000: episode: 284, duration: 0.395s, episode steps: 71, steps per second: 180, episode reward: 43.768, mean reward: 0.616 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.719 [-0.547, 10.157], loss: 0.001709, mae: 0.045975, mean_q: 1.204150
 26549/100000: episode: 285, duration: 0.294s, episode steps: 51, steps per second: 173, episode reward: 34.375, mean reward: 0.674 [0.563, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.666, 10.496], loss: 0.001663, mae: 0.044576, mean_q: 1.201604
 26579/100000: episode: 286, duration: 0.184s, episode steps: 30, steps per second: 163, episode reward: 20.753, mean reward: 0.692 [0.596, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.516, 10.352], loss: 0.001793, mae: 0.045410, mean_q: 1.200673
 26672/100000: episode: 287, duration: 0.538s, episode steps: 93, steps per second: 173, episode reward: 56.040, mean reward: 0.603 [0.519, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.996, 10.100], loss: 0.001843, mae: 0.046643, mean_q: 1.211472
 26765/100000: episode: 288, duration: 0.525s, episode steps: 93, steps per second: 177, episode reward: 57.168, mean reward: 0.615 [0.515, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.597, 10.100], loss: 0.001753, mae: 0.045383, mean_q: 1.211441
 26836/100000: episode: 289, duration: 0.432s, episode steps: 71, steps per second: 164, episode reward: 41.533, mean reward: 0.585 [0.500, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.724 [-0.712, 10.200], loss: 0.001673, mae: 0.045258, mean_q: 1.204552
 26929/100000: episode: 290, duration: 0.538s, episode steps: 93, steps per second: 173, episode reward: 56.876, mean reward: 0.612 [0.513, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-0.786, 10.100], loss: 0.001718, mae: 0.045687, mean_q: 1.207712
 27022/100000: episode: 291, duration: 0.539s, episode steps: 93, steps per second: 173, episode reward: 54.856, mean reward: 0.590 [0.499, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-0.515, 10.175], loss: 0.001808, mae: 0.046710, mean_q: 1.208925
 27114/100000: episode: 292, duration: 0.518s, episode steps: 92, steps per second: 178, episode reward: 57.234, mean reward: 0.622 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-0.318, 10.386], loss: 0.001823, mae: 0.046806, mean_q: 1.203570
 27161/100000: episode: 293, duration: 0.259s, episode steps: 47, steps per second: 181, episode reward: 31.155, mean reward: 0.663 [0.597, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.369, 10.446], loss: 0.001853, mae: 0.047032, mean_q: 1.207323
 27198/100000: episode: 294, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 24.797, mean reward: 0.670 [0.591, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.035, 10.486], loss: 0.001758, mae: 0.046462, mean_q: 1.211509
 27291/100000: episode: 295, duration: 0.496s, episode steps: 93, steps per second: 188, episode reward: 54.495, mean reward: 0.586 [0.514, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-1.320, 10.100], loss: 0.001669, mae: 0.044706, mean_q: 1.211286
 27384/100000: episode: 296, duration: 0.521s, episode steps: 93, steps per second: 179, episode reward: 56.920, mean reward: 0.612 [0.507, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.319, 10.100], loss: 0.001881, mae: 0.047559, mean_q: 1.210365
 27421/100000: episode: 297, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 23.838, mean reward: 0.644 [0.595, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.785, 10.343], loss: 0.001699, mae: 0.045324, mean_q: 1.210726
 27459/100000: episode: 298, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 26.401, mean reward: 0.695 [0.608, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.048, 10.436], loss: 0.001657, mae: 0.044636, mean_q: 1.212823
 27489/100000: episode: 299, duration: 0.189s, episode steps: 30, steps per second: 159, episode reward: 19.728, mean reward: 0.658 [0.574, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.109, 10.374], loss: 0.001566, mae: 0.044151, mean_q: 1.220142
 27581/100000: episode: 300, duration: 0.531s, episode steps: 92, steps per second: 173, episode reward: 54.966, mean reward: 0.597 [0.504, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.528 [-1.759, 10.100], loss: 0.001770, mae: 0.045520, mean_q: 1.214658
 27618/100000: episode: 301, duration: 0.217s, episode steps: 37, steps per second: 170, episode reward: 26.705, mean reward: 0.722 [0.589, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.595, 10.564], loss: 0.001684, mae: 0.044802, mean_q: 1.210149
 27689/100000: episode: 302, duration: 0.401s, episode steps: 71, steps per second: 177, episode reward: 44.437, mean reward: 0.626 [0.532, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.726 [-0.783, 10.265], loss: 0.001691, mae: 0.045037, mean_q: 1.220924
 27719/100000: episode: 303, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 18.466, mean reward: 0.616 [0.547, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.135, 10.259], loss: 0.001784, mae: 0.045325, mean_q: 1.217456
 27756/100000: episode: 304, duration: 0.217s, episode steps: 37, steps per second: 170, episode reward: 22.274, mean reward: 0.602 [0.531, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.057, 10.183], loss: 0.001925, mae: 0.047339, mean_q: 1.214747
 27807/100000: episode: 305, duration: 0.294s, episode steps: 51, steps per second: 173, episode reward: 36.039, mean reward: 0.707 [0.519, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.403, 10.238], loss: 0.001698, mae: 0.044952, mean_q: 1.218898
 27858/100000: episode: 306, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 30.374, mean reward: 0.596 [0.518, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-1.190, 10.105], loss: 0.001877, mae: 0.046924, mean_q: 1.220994
 27909/100000: episode: 307, duration: 0.297s, episode steps: 51, steps per second: 172, episode reward: 30.285, mean reward: 0.594 [0.517, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.354, 10.100], loss: 0.001881, mae: 0.047825, mean_q: 1.221730
 27956/100000: episode: 308, duration: 0.291s, episode steps: 47, steps per second: 162, episode reward: 30.111, mean reward: 0.641 [0.544, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.551, 10.342], loss: 0.001717, mae: 0.046577, mean_q: 1.228289
 27986/100000: episode: 309, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 18.469, mean reward: 0.616 [0.530, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.051, 10.339], loss: 0.001825, mae: 0.046591, mean_q: 1.219456
 28037/100000: episode: 310, duration: 0.318s, episode steps: 51, steps per second: 160, episode reward: 32.026, mean reward: 0.628 [0.514, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.670, 10.100], loss: 0.001653, mae: 0.045107, mean_q: 1.222035
 28129/100000: episode: 311, duration: 0.539s, episode steps: 92, steps per second: 171, episode reward: 61.598, mean reward: 0.670 [0.513, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-0.562, 10.100], loss: 0.001879, mae: 0.047372, mean_q: 1.222489
 28221/100000: episode: 312, duration: 0.499s, episode steps: 92, steps per second: 184, episode reward: 57.979, mean reward: 0.630 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-0.561, 10.413], loss: 0.001745, mae: 0.045744, mean_q: 1.223489
 28314/100000: episode: 313, duration: 0.541s, episode steps: 93, steps per second: 172, episode reward: 54.120, mean reward: 0.582 [0.506, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.518 [-1.606, 10.142], loss: 0.001701, mae: 0.045367, mean_q: 1.222509
 28361/100000: episode: 314, duration: 0.281s, episode steps: 47, steps per second: 167, episode reward: 31.724, mean reward: 0.675 [0.557, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-1.524, 10.474], loss: 0.001705, mae: 0.045451, mean_q: 1.224735
 28453/100000: episode: 315, duration: 0.514s, episode steps: 92, steps per second: 179, episode reward: 55.458, mean reward: 0.603 [0.518, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.532 [-1.482, 10.411], loss: 0.001877, mae: 0.047824, mean_q: 1.224645
 28500/100000: episode: 316, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 33.736, mean reward: 0.718 [0.533, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.572, 10.219], loss: 0.001876, mae: 0.046997, mean_q: 1.230619
 28547/100000: episode: 317, duration: 0.270s, episode steps: 47, steps per second: 174, episode reward: 28.794, mean reward: 0.613 [0.523, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.649, 10.100], loss: 0.001669, mae: 0.044430, mean_q: 1.229981
 28640/100000: episode: 318, duration: 0.540s, episode steps: 93, steps per second: 172, episode reward: 54.233, mean reward: 0.583 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-0.695, 10.115], loss: 0.001772, mae: 0.046380, mean_q: 1.229892
 28687/100000: episode: 319, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 31.263, mean reward: 0.665 [0.514, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.546, 10.165], loss: 0.001820, mae: 0.046011, mean_q: 1.229347
 28738/100000: episode: 320, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 32.245, mean reward: 0.632 [0.560, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.866, 10.232], loss: 0.001816, mae: 0.046872, mean_q: 1.226748
 28831/100000: episode: 321, duration: 0.523s, episode steps: 93, steps per second: 178, episode reward: 55.133, mean reward: 0.593 [0.517, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.517 [-0.949, 10.175], loss: 0.001987, mae: 0.048812, mean_q: 1.230615
 28878/100000: episode: 322, duration: 0.262s, episode steps: 47, steps per second: 179, episode reward: 31.676, mean reward: 0.674 [0.568, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.580, 10.381], loss: 0.001781, mae: 0.046408, mean_q: 1.229181
 28970/100000: episode: 323, duration: 0.516s, episode steps: 92, steps per second: 178, episode reward: 52.599, mean reward: 0.572 [0.498, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.535 [-0.931, 10.100], loss: 0.001803, mae: 0.046567, mean_q: 1.224947
 29063/100000: episode: 324, duration: 0.528s, episode steps: 93, steps per second: 176, episode reward: 53.727, mean reward: 0.578 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-1.348, 10.100], loss: 0.001698, mae: 0.045239, mean_q: 1.222358
 29155/100000: episode: 325, duration: 0.527s, episode steps: 92, steps per second: 175, episode reward: 54.917, mean reward: 0.597 [0.511, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-0.948, 10.231], loss: 0.001681, mae: 0.045714, mean_q: 1.222875
 29202/100000: episode: 326, duration: 0.287s, episode steps: 47, steps per second: 164, episode reward: 28.450, mean reward: 0.605 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.759, 10.251], loss: 0.001795, mae: 0.046628, mean_q: 1.215976
 29232/100000: episode: 327, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 19.351, mean reward: 0.645 [0.537, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.268, 10.318], loss: 0.001641, mae: 0.044216, mean_q: 1.224899
 29325/100000: episode: 328, duration: 0.561s, episode steps: 93, steps per second: 166, episode reward: 54.871, mean reward: 0.590 [0.506, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.522 [-1.287, 10.306], loss: 0.001591, mae: 0.044402, mean_q: 1.224490
 29396/100000: episode: 329, duration: 0.410s, episode steps: 71, steps per second: 173, episode reward: 41.529, mean reward: 0.585 [0.513, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.722 [-0.468, 10.100], loss: 0.001666, mae: 0.045039, mean_q: 1.219374
[Info] 3-TH LEVEL FOUND: 1.4806870222091675, Considering 10/90 traces
 29489/100000: episode: 330, duration: 4.660s, episode steps: 93, steps per second: 20, episode reward: 58.035, mean reward: 0.624 [0.515, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.954, 10.100], loss: 0.001524, mae: 0.043414, mean_q: 1.221361
 29537/100000: episode: 331, duration: 0.241s, episode steps: 48, steps per second: 200, episode reward: 31.844, mean reward: 0.663 [0.557, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.972, 10.375], loss: 0.001900, mae: 0.047438, mean_q: 1.216952
 29545/100000: episode: 332, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 5.896, mean reward: 0.737 [0.699, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.459, 10.100], loss: 0.001709, mae: 0.046319, mean_q: 1.220739
 29563/100000: episode: 333, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 14.402, mean reward: 0.800 [0.737, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.561], loss: 0.002000, mae: 0.050198, mean_q: 1.214245
 29611/100000: episode: 334, duration: 0.239s, episode steps: 48, steps per second: 201, episode reward: 33.171, mean reward: 0.691 [0.597, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.345, 10.379], loss: 0.001668, mae: 0.045668, mean_q: 1.217664
 29629/100000: episode: 335, duration: 0.113s, episode steps: 18, steps per second: 160, episode reward: 15.199, mean reward: 0.844 [0.772, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.994, 10.485], loss: 0.001909, mae: 0.047541, mean_q: 1.220878
 29647/100000: episode: 336, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 14.219, mean reward: 0.790 [0.741, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.144, 10.570], loss: 0.001593, mae: 0.043373, mean_q: 1.233028
 29673/100000: episode: 337, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 19.045, mean reward: 0.733 [0.666, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.089, 10.478], loss: 0.001703, mae: 0.045303, mean_q: 1.224124
 29722/100000: episode: 338, duration: 0.241s, episode steps: 49, steps per second: 203, episode reward: 34.358, mean reward: 0.701 [0.533, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.546, 10.207], loss: 0.001739, mae: 0.046219, mean_q: 1.225928
 29754/100000: episode: 339, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 24.933, mean reward: 0.779 [0.730, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.406, 10.526], loss: 0.001880, mae: 0.047552, mean_q: 1.225968
 29786/100000: episode: 340, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 23.730, mean reward: 0.742 [0.650, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.983, 10.516], loss: 0.001639, mae: 0.044438, mean_q: 1.228284
 29829/100000: episode: 341, duration: 0.215s, episode steps: 43, steps per second: 200, episode reward: 27.781, mean reward: 0.646 [0.542, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-1.588, 10.199], loss: 0.001771, mae: 0.046519, mean_q: 1.229496
 29861/100000: episode: 342, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 24.304, mean reward: 0.760 [0.574, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.075, 10.294], loss: 0.001722, mae: 0.045648, mean_q: 1.224535
 29904/100000: episode: 343, duration: 0.221s, episode steps: 43, steps per second: 195, episode reward: 27.728, mean reward: 0.645 [0.541, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.615, 10.124], loss: 0.001790, mae: 0.046966, mean_q: 1.226817
 29936/100000: episode: 344, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 23.778, mean reward: 0.743 [0.623, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.547], loss: 0.001675, mae: 0.045212, mean_q: 1.234737
 29962/100000: episode: 345, duration: 0.159s, episode steps: 26, steps per second: 164, episode reward: 16.707, mean reward: 0.643 [0.513, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.399, 10.228], loss: 0.001854, mae: 0.047601, mean_q: 1.240885
 29991/100000: episode: 346, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 22.159, mean reward: 0.764 [0.696, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.621], loss: 0.001599, mae: 0.044327, mean_q: 1.233289
 30040/100000: episode: 347, duration: 0.243s, episode steps: 49, steps per second: 201, episode reward: 33.346, mean reward: 0.681 [0.516, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.410, 10.100], loss: 0.001900, mae: 0.048556, mean_q: 1.233686
 30088/100000: episode: 348, duration: 0.249s, episode steps: 48, steps per second: 193, episode reward: 31.130, mean reward: 0.649 [0.575, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.563, 10.284], loss: 0.001830, mae: 0.047071, mean_q: 1.231994
 30134/100000: episode: 349, duration: 0.227s, episode steps: 46, steps per second: 203, episode reward: 30.675, mean reward: 0.667 [0.572, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.336, 10.437], loss: 0.001646, mae: 0.044461, mean_q: 1.231417
 30183/100000: episode: 350, duration: 0.260s, episode steps: 49, steps per second: 188, episode reward: 33.207, mean reward: 0.678 [0.597, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.102, 10.350], loss: 0.001673, mae: 0.044272, mean_q: 1.239436
 30231/100000: episode: 351, duration: 0.258s, episode steps: 48, steps per second: 186, episode reward: 31.696, mean reward: 0.660 [0.571, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-1.064, 10.400], loss: 0.001685, mae: 0.045083, mean_q: 1.231075
 30279/100000: episode: 352, duration: 0.259s, episode steps: 48, steps per second: 186, episode reward: 33.323, mean reward: 0.694 [0.627, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.351, 10.305], loss: 0.001852, mae: 0.046877, mean_q: 1.234676
 30311/100000: episode: 353, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 23.483, mean reward: 0.734 [0.654, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.035, 10.437], loss: 0.001778, mae: 0.046125, mean_q: 1.242295
 30319/100000: episode: 354, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 5.912, mean reward: 0.739 [0.696, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.454, 10.100], loss: 0.001649, mae: 0.043975, mean_q: 1.243042
 30367/100000: episode: 355, duration: 0.266s, episode steps: 48, steps per second: 180, episode reward: 31.972, mean reward: 0.666 [0.528, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-1.385, 10.223], loss: 0.001678, mae: 0.044478, mean_q: 1.241121
 30399/100000: episode: 356, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 22.502, mean reward: 0.703 [0.623, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.533, 10.349], loss: 0.001639, mae: 0.044397, mean_q: 1.240762
 30407/100000: episode: 357, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 6.680, mean reward: 0.835 [0.796, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.537, 10.100], loss: 0.001315, mae: 0.039594, mean_q: 1.252998
 30433/100000: episode: 358, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 19.860, mean reward: 0.764 [0.645, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.745, 10.533], loss: 0.001670, mae: 0.045093, mean_q: 1.252152
 30465/100000: episode: 359, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 24.301, mean reward: 0.759 [0.687, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.410, 10.450], loss: 0.001834, mae: 0.046838, mean_q: 1.241805
 30497/100000: episode: 360, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 23.439, mean reward: 0.732 [0.643, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.397, 10.362], loss: 0.001744, mae: 0.045791, mean_q: 1.250136
 30505/100000: episode: 361, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 6.096, mean reward: 0.762 [0.712, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.359, 10.100], loss: 0.001754, mae: 0.046104, mean_q: 1.243152
 30551/100000: episode: 362, duration: 0.251s, episode steps: 46, steps per second: 183, episode reward: 34.064, mean reward: 0.741 [0.621, 0.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.901, 10.459], loss: 0.001719, mae: 0.044883, mean_q: 1.239737
 30577/100000: episode: 363, duration: 0.124s, episode steps: 26, steps per second: 210, episode reward: 18.322, mean reward: 0.705 [0.560, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.128, 10.241], loss: 0.001828, mae: 0.047611, mean_q: 1.240870
 30623/100000: episode: 364, duration: 0.234s, episode steps: 46, steps per second: 197, episode reward: 31.262, mean reward: 0.680 [0.582, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-1.935, 10.325], loss: 0.001715, mae: 0.045455, mean_q: 1.249025
 30672/100000: episode: 365, duration: 0.247s, episode steps: 49, steps per second: 198, episode reward: 33.469, mean reward: 0.683 [0.536, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.671, 10.257], loss: 0.001656, mae: 0.044683, mean_q: 1.258056
 30704/100000: episode: 366, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 24.278, mean reward: 0.759 [0.649, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.507], loss: 0.001909, mae: 0.047610, mean_q: 1.247081
 30712/100000: episode: 367, duration: 0.056s, episode steps: 8, steps per second: 143, episode reward: 6.164, mean reward: 0.770 [0.747, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.576, 10.100], loss: 0.001679, mae: 0.045360, mean_q: 1.262357
 30738/100000: episode: 368, duration: 0.139s, episode steps: 26, steps per second: 186, episode reward: 19.207, mean reward: 0.739 [0.648, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.289, 10.602], loss: 0.001730, mae: 0.045564, mean_q: 1.249012
 30767/100000: episode: 369, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 22.091, mean reward: 0.762 [0.674, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.032, 10.435], loss: 0.001710, mae: 0.045634, mean_q: 1.261427
 30810/100000: episode: 370, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 31.164, mean reward: 0.725 [0.591, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.305, 10.289], loss: 0.001895, mae: 0.047973, mean_q: 1.257783
 30836/100000: episode: 371, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 17.413, mean reward: 0.670 [0.586, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.362], loss: 0.001989, mae: 0.047652, mean_q: 1.259825
 30885/100000: episode: 372, duration: 0.255s, episode steps: 49, steps per second: 192, episode reward: 37.502, mean reward: 0.765 [0.681, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-1.013, 10.426], loss: 0.001980, mae: 0.048365, mean_q: 1.258783
 30934/100000: episode: 373, duration: 0.247s, episode steps: 49, steps per second: 198, episode reward: 35.508, mean reward: 0.725 [0.563, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.993, 10.289], loss: 0.002006, mae: 0.048504, mean_q: 1.259539
 30980/100000: episode: 374, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 30.198, mean reward: 0.656 [0.529, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.491, 10.199], loss: 0.001853, mae: 0.047221, mean_q: 1.264493
 31012/100000: episode: 375, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 23.310, mean reward: 0.728 [0.657, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.035, 10.398], loss: 0.001719, mae: 0.045579, mean_q: 1.268734
 31038/100000: episode: 376, duration: 0.139s, episode steps: 26, steps per second: 188, episode reward: 17.008, mean reward: 0.654 [0.528, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.569, 10.162], loss: 0.001785, mae: 0.046186, mean_q: 1.264845
 31046/100000: episode: 377, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 5.733, mean reward: 0.717 [0.690, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.402, 10.100], loss: 0.001755, mae: 0.046340, mean_q: 1.256388
 31078/100000: episode: 378, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 24.543, mean reward: 0.767 [0.671, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.088, 10.549], loss: 0.001729, mae: 0.045120, mean_q: 1.269796
 31124/100000: episode: 379, duration: 0.239s, episode steps: 46, steps per second: 193, episode reward: 29.967, mean reward: 0.651 [0.563, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.314, 10.367], loss: 0.001830, mae: 0.046664, mean_q: 1.266959
 31170/100000: episode: 380, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 28.975, mean reward: 0.630 [0.510, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.842, 10.293], loss: 0.001789, mae: 0.046683, mean_q: 1.269308
 31216/100000: episode: 381, duration: 0.252s, episode steps: 46, steps per second: 183, episode reward: 34.472, mean reward: 0.749 [0.555, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.821, 10.261], loss: 0.001875, mae: 0.047273, mean_q: 1.279236
 31262/100000: episode: 382, duration: 0.222s, episode steps: 46, steps per second: 208, episode reward: 33.354, mean reward: 0.725 [0.655, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-1.714, 10.396], loss: 0.001878, mae: 0.047448, mean_q: 1.266447
 31305/100000: episode: 383, duration: 0.223s, episode steps: 43, steps per second: 193, episode reward: 29.461, mean reward: 0.685 [0.599, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.804, 10.302], loss: 0.001655, mae: 0.044444, mean_q: 1.277268
 31331/100000: episode: 384, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 19.511, mean reward: 0.750 [0.681, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.766, 10.476], loss: 0.001702, mae: 0.045418, mean_q: 1.286181
 31357/100000: episode: 385, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 19.644, mean reward: 0.756 [0.697, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.492], loss: 0.001776, mae: 0.046102, mean_q: 1.274444
 31389/100000: episode: 386, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 23.081, mean reward: 0.721 [0.637, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.577, 10.407], loss: 0.001763, mae: 0.045755, mean_q: 1.278945
 31432/100000: episode: 387, duration: 0.220s, episode steps: 43, steps per second: 196, episode reward: 29.121, mean reward: 0.677 [0.627, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.234, 10.360], loss: 0.001845, mae: 0.046746, mean_q: 1.281433
 31481/100000: episode: 388, duration: 0.233s, episode steps: 49, steps per second: 210, episode reward: 34.097, mean reward: 0.696 [0.578, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.409, 10.364], loss: 0.001702, mae: 0.045775, mean_q: 1.274682
 31499/100000: episode: 389, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 14.690, mean reward: 0.816 [0.741, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.510], loss: 0.001877, mae: 0.048275, mean_q: 1.286193
 31531/100000: episode: 390, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 24.767, mean reward: 0.774 [0.695, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.424, 10.324], loss: 0.001973, mae: 0.048658, mean_q: 1.281169
 31560/100000: episode: 391, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 22.248, mean reward: 0.767 [0.699, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.197, 10.482], loss: 0.001693, mae: 0.045266, mean_q: 1.283256
 31603/100000: episode: 392, duration: 0.234s, episode steps: 43, steps per second: 183, episode reward: 28.838, mean reward: 0.671 [0.618, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-1.049, 10.274], loss: 0.001676, mae: 0.044872, mean_q: 1.287273
 31649/100000: episode: 393, duration: 0.264s, episode steps: 46, steps per second: 174, episode reward: 30.827, mean reward: 0.670 [0.551, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.994, 10.229], loss: 0.001779, mae: 0.045593, mean_q: 1.273907
 31695/100000: episode: 394, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 34.571, mean reward: 0.752 [0.625, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.497, 10.469], loss: 0.001767, mae: 0.046080, mean_q: 1.285934
 31724/100000: episode: 395, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 22.079, mean reward: 0.761 [0.694, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.615, 10.470], loss: 0.001551, mae: 0.042907, mean_q: 1.278978
 31770/100000: episode: 396, duration: 0.220s, episode steps: 46, steps per second: 209, episode reward: 33.925, mean reward: 0.738 [0.511, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.324, 10.172], loss: 0.001759, mae: 0.045850, mean_q: 1.283087
 31813/100000: episode: 397, duration: 0.212s, episode steps: 43, steps per second: 203, episode reward: 28.530, mean reward: 0.663 [0.536, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.559, 10.159], loss: 0.001751, mae: 0.045830, mean_q: 1.296915
 31839/100000: episode: 398, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 20.015, mean reward: 0.770 [0.717, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.480, 10.472], loss: 0.001712, mae: 0.045730, mean_q: 1.300351
 31857/100000: episode: 399, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 13.311, mean reward: 0.739 [0.645, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.419], loss: 0.001622, mae: 0.044751, mean_q: 1.309029
 31903/100000: episode: 400, duration: 0.244s, episode steps: 46, steps per second: 189, episode reward: 30.319, mean reward: 0.659 [0.519, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.482, 10.285], loss: 0.001842, mae: 0.047430, mean_q: 1.283940
 31932/100000: episode: 401, duration: 0.141s, episode steps: 29, steps per second: 205, episode reward: 21.576, mean reward: 0.744 [0.653, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.117, 10.549], loss: 0.001700, mae: 0.045153, mean_q: 1.305371
 31978/100000: episode: 402, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 33.321, mean reward: 0.724 [0.661, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.687, 10.378], loss: 0.001872, mae: 0.047158, mean_q: 1.298149
 32021/100000: episode: 403, duration: 0.232s, episode steps: 43, steps per second: 185, episode reward: 29.199, mean reward: 0.679 [0.622, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.737, 10.379], loss: 0.001704, mae: 0.045609, mean_q: 1.297546
 32067/100000: episode: 404, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 37.190, mean reward: 0.808 [0.744, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.304, 10.600], loss: 0.001596, mae: 0.044414, mean_q: 1.299806
 32115/100000: episode: 405, duration: 0.272s, episode steps: 48, steps per second: 176, episode reward: 37.821, mean reward: 0.788 [0.622, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.447, 10.591], loss: 0.001778, mae: 0.046457, mean_q: 1.305209
 32163/100000: episode: 406, duration: 0.252s, episode steps: 48, steps per second: 190, episode reward: 28.989, mean reward: 0.604 [0.506, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.331, 10.109], loss: 0.001606, mae: 0.044330, mean_q: 1.302227
 32211/100000: episode: 407, duration: 0.238s, episode steps: 48, steps per second: 202, episode reward: 32.949, mean reward: 0.686 [0.510, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.461, 10.249], loss: 0.002155, mae: 0.050573, mean_q: 1.310969
 32259/100000: episode: 408, duration: 0.247s, episode steps: 48, steps per second: 195, episode reward: 36.224, mean reward: 0.755 [0.646, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.424, 10.449], loss: 0.001950, mae: 0.048348, mean_q: 1.306060
 32288/100000: episode: 409, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 21.463, mean reward: 0.740 [0.626, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.133, 10.368], loss: 0.002070, mae: 0.049997, mean_q: 1.306640
 32314/100000: episode: 410, duration: 0.139s, episode steps: 26, steps per second: 188, episode reward: 17.688, mean reward: 0.680 [0.555, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.587, 10.331], loss: 0.001859, mae: 0.047922, mean_q: 1.301505
 32360/100000: episode: 411, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 36.234, mean reward: 0.788 [0.657, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-1.476, 10.711], loss: 0.001705, mae: 0.045412, mean_q: 1.315133
 32408/100000: episode: 412, duration: 0.247s, episode steps: 48, steps per second: 194, episode reward: 30.370, mean reward: 0.633 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.340, 10.178], loss: 0.001749, mae: 0.046194, mean_q: 1.310346
 32426/100000: episode: 413, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 14.373, mean reward: 0.798 [0.673, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.098, 10.485], loss: 0.001620, mae: 0.044424, mean_q: 1.315432
 32452/100000: episode: 414, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 17.670, mean reward: 0.680 [0.597, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.625, 10.346], loss: 0.001863, mae: 0.046645, mean_q: 1.305567
 32498/100000: episode: 415, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 31.365, mean reward: 0.682 [0.515, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.832, 10.267], loss: 0.001678, mae: 0.045010, mean_q: 1.315259
 32524/100000: episode: 416, duration: 0.151s, episode steps: 26, steps per second: 173, episode reward: 20.816, mean reward: 0.801 [0.730, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.331, 10.456], loss: 0.001565, mae: 0.042762, mean_q: 1.319656
 32570/100000: episode: 417, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 36.158, mean reward: 0.786 [0.606, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.319, 10.331], loss: 0.001667, mae: 0.044503, mean_q: 1.317927
 32616/100000: episode: 418, duration: 0.252s, episode steps: 46, steps per second: 183, episode reward: 29.627, mean reward: 0.644 [0.516, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.593, 10.161], loss: 0.001649, mae: 0.044256, mean_q: 1.314168
 32645/100000: episode: 419, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 20.138, mean reward: 0.694 [0.601, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.388, 10.471], loss: 0.001850, mae: 0.047124, mean_q: 1.316980
[Info] 4-TH LEVEL FOUND: 1.6083794832229614, Considering 10/90 traces
 32671/100000: episode: 420, duration: 4.303s, episode steps: 26, steps per second: 6, episode reward: 17.127, mean reward: 0.659 [0.575, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.491, 10.190], loss: 0.001826, mae: 0.046738, mean_q: 1.325024
 32708/100000: episode: 421, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 25.981, mean reward: 0.702 [0.520, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.162, 10.155], loss: 0.001578, mae: 0.044012, mean_q: 1.315033
 32744/100000: episode: 422, duration: 0.194s, episode steps: 36, steps per second: 185, episode reward: 27.261, mean reward: 0.757 [0.587, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.381, 10.313], loss: 0.001691, mae: 0.045617, mean_q: 1.324407
 32767/100000: episode: 423, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 18.872, mean reward: 0.821 [0.763, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.238, 10.574], loss: 0.001634, mae: 0.044734, mean_q: 1.319443
 32798/100000: episode: 424, duration: 0.148s, episode steps: 31, steps per second: 209, episode reward: 22.905, mean reward: 0.739 [0.582, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.191, 10.328], loss: 0.001720, mae: 0.045904, mean_q: 1.333420
 32822/100000: episode: 425, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 18.119, mean reward: 0.755 [0.664, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.511], loss: 0.001652, mae: 0.045822, mean_q: 1.318121
 32853/100000: episode: 426, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 23.748, mean reward: 0.766 [0.601, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.302], loss: 0.001709, mae: 0.045844, mean_q: 1.335000
 32884/100000: episode: 427, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 25.724, mean reward: 0.830 [0.692, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.350, 10.532], loss: 0.001603, mae: 0.044568, mean_q: 1.336544
 32921/100000: episode: 428, duration: 0.203s, episode steps: 37, steps per second: 182, episode reward: 28.770, mean reward: 0.778 [0.634, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.158, 10.395], loss: 0.001643, mae: 0.044652, mean_q: 1.338418
 32936/100000: episode: 429, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 12.757, mean reward: 0.850 [0.788, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.493], loss: 0.001733, mae: 0.046872, mean_q: 1.327345
 32951/100000: episode: 430, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 12.429, mean reward: 0.829 [0.705, 0.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.284, 10.468], loss: 0.001933, mae: 0.050562, mean_q: 1.359381
 32973/100000: episode: 431, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 16.634, mean reward: 0.756 [0.592, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.228, 10.365], loss: 0.001639, mae: 0.043970, mean_q: 1.335518
 33009/100000: episode: 432, duration: 0.174s, episode steps: 36, steps per second: 206, episode reward: 23.938, mean reward: 0.665 [0.512, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.984, 10.100], loss: 0.001670, mae: 0.045296, mean_q: 1.336320
 33033/100000: episode: 433, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 17.980, mean reward: 0.749 [0.695, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.580], loss: 0.001804, mae: 0.046821, mean_q: 1.317528
 33064/100000: episode: 434, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 24.044, mean reward: 0.776 [0.688, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.185, 10.464], loss: 0.001723, mae: 0.047033, mean_q: 1.319252
 33088/100000: episode: 435, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 20.270, mean reward: 0.845 [0.779, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.195, 10.607], loss: 0.001590, mae: 0.043433, mean_q: 1.344683
 33103/100000: episode: 436, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 12.200, mean reward: 0.813 [0.781, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.633, 10.610], loss: 0.001700, mae: 0.046212, mean_q: 1.347712
 33118/100000: episode: 437, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 11.545, mean reward: 0.770 [0.712, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.533, 10.427], loss: 0.001816, mae: 0.047665, mean_q: 1.347124
 33141/100000: episode: 438, duration: 0.132s, episode steps: 23, steps per second: 175, episode reward: 19.103, mean reward: 0.831 [0.673, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.417, 10.438], loss: 0.001693, mae: 0.044574, mean_q: 1.343660
 33178/100000: episode: 439, duration: 0.181s, episode steps: 37, steps per second: 204, episode reward: 26.061, mean reward: 0.704 [0.598, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.232, 10.290], loss: 0.001648, mae: 0.044236, mean_q: 1.344330
 33201/100000: episode: 440, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 17.680, mean reward: 0.769 [0.660, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.439], loss: 0.001534, mae: 0.042811, mean_q: 1.333172
 33237/100000: episode: 441, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 24.437, mean reward: 0.679 [0.549, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.353, 10.253], loss: 0.001811, mae: 0.045554, mean_q: 1.343570
 33259/100000: episode: 442, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 17.899, mean reward: 0.814 [0.720, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.418, 10.484], loss: 0.002052, mae: 0.048316, mean_q: 1.350746
 33295/100000: episode: 443, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 26.000, mean reward: 0.722 [0.553, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.701, 10.219], loss: 0.001753, mae: 0.046355, mean_q: 1.350375
 33318/100000: episode: 444, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 20.182, mean reward: 0.877 [0.794, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.886, 10.619], loss: 0.001386, mae: 0.040914, mean_q: 1.341688
 33341/100000: episode: 445, duration: 0.110s, episode steps: 23, steps per second: 209, episode reward: 17.500, mean reward: 0.761 [0.695, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.511], loss: 0.001862, mae: 0.047540, mean_q: 1.354421
 33368/100000: episode: 446, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 21.756, mean reward: 0.806 [0.719, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.240, 10.481], loss: 0.001702, mae: 0.045246, mean_q: 1.348479
 33405/100000: episode: 447, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 27.661, mean reward: 0.748 [0.624, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.150, 10.498], loss: 0.001503, mae: 0.042291, mean_q: 1.352308
[Info] FALSIFICATION!
 33411/100000: episode: 448, duration: 0.482s, episode steps: 6, steps per second: 12, episode reward: 5.506, mean reward: 0.918 [0.881, 1.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.020, 9.113], loss: 0.001463, mae: 0.042783, mean_q: 1.389301
 33433/100000: episode: 449, duration: 0.148s, episode steps: 22, steps per second: 149, episode reward: 17.786, mean reward: 0.808 [0.706, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.107, 10.581], loss: 0.001483, mae: 0.042932, mean_q: 1.352224
 33460/100000: episode: 450, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 21.645, mean reward: 0.802 [0.721, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.442, 10.464], loss: 0.001568, mae: 0.043764, mean_q: 1.371006
 33497/100000: episode: 451, duration: 0.178s, episode steps: 37, steps per second: 208, episode reward: 29.124, mean reward: 0.787 [0.691, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.589, 10.495], loss: 0.002188, mae: 0.049488, mean_q: 1.372025
 33533/100000: episode: 452, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 25.568, mean reward: 0.710 [0.620, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.319, 10.403], loss: 0.001688, mae: 0.045218, mean_q: 1.372248
[Info] FALSIFICATION!
 33554/100000: episode: 453, duration: 0.290s, episode steps: 21, steps per second: 72, episode reward: 17.946, mean reward: 0.855 [0.788, 1.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.842, 10.550], loss: 0.001557, mae: 0.043209, mean_q: 1.376180
 33578/100000: episode: 454, duration: 0.127s, episode steps: 24, steps per second: 190, episode reward: 16.778, mean reward: 0.699 [0.633, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.093, 10.414], loss: 0.001847, mae: 0.044060, mean_q: 1.370689
 33614/100000: episode: 455, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 27.562, mean reward: 0.766 [0.668, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.371, 10.497], loss: 0.001749, mae: 0.046771, mean_q: 1.360378
 33641/100000: episode: 456, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 22.332, mean reward: 0.827 [0.719, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.459, 10.547], loss: 0.002408, mae: 0.050106, mean_q: 1.379988
 33672/100000: episode: 457, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 23.156, mean reward: 0.747 [0.589, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.431, 10.312], loss: 0.002340, mae: 0.050900, mean_q: 1.370074
 33696/100000: episode: 458, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 19.733, mean reward: 0.822 [0.703, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.748, 10.481], loss: 0.001961, mae: 0.045538, mean_q: 1.372176
 33707/100000: episode: 459, duration: 0.069s, episode steps: 11, steps per second: 158, episode reward: 8.682, mean reward: 0.789 [0.745, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.366], loss: 0.001465, mae: 0.042240, mean_q: 1.357885
 33734/100000: episode: 460, duration: 0.129s, episode steps: 27, steps per second: 209, episode reward: 19.660, mean reward: 0.728 [0.638, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.632, 10.349], loss: 0.001809, mae: 0.047277, mean_q: 1.377266
 33749/100000: episode: 461, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 11.702, mean reward: 0.780 [0.688, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.119, 10.542], loss: 0.001526, mae: 0.042717, mean_q: 1.374390
 33786/100000: episode: 462, duration: 0.179s, episode steps: 37, steps per second: 207, episode reward: 25.775, mean reward: 0.697 [0.516, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.408, 10.140], loss: 0.001600, mae: 0.042621, mean_q: 1.377952
 33822/100000: episode: 463, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 30.062, mean reward: 0.835 [0.737, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.574, 10.648], loss: 0.001594, mae: 0.043423, mean_q: 1.386172
 33833/100000: episode: 464, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 8.702, mean reward: 0.791 [0.708, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.601], loss: 0.001470, mae: 0.042512, mean_q: 1.375670
 33848/100000: episode: 465, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 13.180, mean reward: 0.879 [0.830, 0.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.745], loss: 0.001790, mae: 0.044325, mean_q: 1.388057
 33871/100000: episode: 466, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 17.661, mean reward: 0.768 [0.679, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.251, 10.530], loss: 0.002077, mae: 0.046032, mean_q: 1.374342
 33894/100000: episode: 467, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 17.896, mean reward: 0.778 [0.685, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.469], loss: 0.001940, mae: 0.046247, mean_q: 1.390918
 33916/100000: episode: 468, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 15.510, mean reward: 0.705 [0.595, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.150, 10.338], loss: 0.001644, mae: 0.044179, mean_q: 1.378077
 33947/100000: episode: 469, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 24.895, mean reward: 0.803 [0.709, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.243, 10.456], loss: 0.001408, mae: 0.041256, mean_q: 1.392546
 33974/100000: episode: 470, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 19.814, mean reward: 0.734 [0.657, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.308, 10.478], loss: 0.001830, mae: 0.045304, mean_q: 1.388560
 34010/100000: episode: 471, duration: 0.187s, episode steps: 36, steps per second: 192, episode reward: 30.012, mean reward: 0.834 [0.746, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.869, 10.513], loss: 0.001571, mae: 0.044023, mean_q: 1.392031
[Info] FALSIFICATION!
 34018/100000: episode: 472, duration: 0.312s, episode steps: 8, steps per second: 26, episode reward: 7.342, mean reward: 0.918 [0.835, 1.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.716 [-0.192, 8.433], loss: 0.001899, mae: 0.049070, mean_q: 1.355661
 34041/100000: episode: 473, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 17.828, mean reward: 0.775 [0.698, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.933, 10.473], loss: 0.001648, mae: 0.045184, mean_q: 1.396119
 34065/100000: episode: 474, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 20.304, mean reward: 0.846 [0.775, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.236, 10.573], loss: 0.001510, mae: 0.043621, mean_q: 1.399529
 34087/100000: episode: 475, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 17.157, mean reward: 0.780 [0.688, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.075, 10.518], loss: 0.002126, mae: 0.045647, mean_q: 1.411047
 34124/100000: episode: 476, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 31.430, mean reward: 0.849 [0.764, 0.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.596, 10.604], loss: 0.001793, mae: 0.045562, mean_q: 1.405166
 34161/100000: episode: 477, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 26.444, mean reward: 0.715 [0.543, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.678, 10.190], loss: 0.001455, mae: 0.042434, mean_q: 1.416537
 34184/100000: episode: 478, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 19.383, mean reward: 0.843 [0.746, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.530, 10.536], loss: 0.001528, mae: 0.042863, mean_q: 1.402862
 34195/100000: episode: 479, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 9.400, mean reward: 0.855 [0.763, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.767, 10.628], loss: 0.001769, mae: 0.046538, mean_q: 1.398714
 34231/100000: episode: 480, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 27.230, mean reward: 0.756 [0.633, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.373, 10.367], loss: 0.001851, mae: 0.046648, mean_q: 1.418420
 34267/100000: episode: 481, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 29.828, mean reward: 0.829 [0.738, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.827, 10.595], loss: 0.001766, mae: 0.044296, mean_q: 1.407156
 34304/100000: episode: 482, duration: 0.199s, episode steps: 37, steps per second: 185, episode reward: 30.019, mean reward: 0.811 [0.638, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.788, 10.454], loss: 0.001722, mae: 0.045081, mean_q: 1.427602
 34327/100000: episode: 483, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 16.097, mean reward: 0.700 [0.619, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.419], loss: 0.001737, mae: 0.045712, mean_q: 1.404089
 34351/100000: episode: 484, duration: 0.115s, episode steps: 24, steps per second: 208, episode reward: 19.017, mean reward: 0.792 [0.683, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.308, 10.461], loss: 0.001740, mae: 0.046799, mean_q: 1.424775
 34374/100000: episode: 485, duration: 0.115s, episode steps: 23, steps per second: 201, episode reward: 19.468, mean reward: 0.846 [0.740, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.755, 10.509], loss: 0.002003, mae: 0.047066, mean_q: 1.414606
 34410/100000: episode: 486, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 28.632, mean reward: 0.795 [0.692, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.970, 10.460], loss: 0.001751, mae: 0.046363, mean_q: 1.421031
 34441/100000: episode: 487, duration: 0.167s, episode steps: 31, steps per second: 185, episode reward: 24.962, mean reward: 0.805 [0.672, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.275, 10.397], loss: 0.001852, mae: 0.046144, mean_q: 1.415633
 34465/100000: episode: 488, duration: 0.114s, episode steps: 24, steps per second: 210, episode reward: 17.143, mean reward: 0.714 [0.612, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.465], loss: 0.002105, mae: 0.045970, mean_q: 1.414029
 34501/100000: episode: 489, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 27.218, mean reward: 0.756 [0.671, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.338, 10.402], loss: 0.001584, mae: 0.044546, mean_q: 1.431111
 34532/100000: episode: 490, duration: 0.151s, episode steps: 31, steps per second: 206, episode reward: 24.198, mean reward: 0.781 [0.685, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.672, 10.514], loss: 0.002129, mae: 0.048954, mean_q: 1.410384
 34563/100000: episode: 491, duration: 0.171s, episode steps: 31, steps per second: 182, episode reward: 25.193, mean reward: 0.813 [0.683, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.505, 10.530], loss: 0.001922, mae: 0.045211, mean_q: 1.423310
 34578/100000: episode: 492, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 12.074, mean reward: 0.805 [0.685, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.449], loss: 0.002413, mae: 0.047350, mean_q: 1.415694
 34601/100000: episode: 493, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 19.593, mean reward: 0.852 [0.740, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.234, 10.575], loss: 0.001908, mae: 0.046344, mean_q: 1.408077
 34612/100000: episode: 494, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 9.231, mean reward: 0.839 [0.769, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.173, 10.629], loss: 0.001640, mae: 0.045327, mean_q: 1.431725
 34636/100000: episode: 495, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 20.637, mean reward: 0.860 [0.749, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.360, 10.576], loss: 0.001587, mae: 0.044281, mean_q: 1.415223
 34667/100000: episode: 496, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 25.647, mean reward: 0.827 [0.662, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.044, 10.544], loss: 0.001629, mae: 0.044109, mean_q: 1.418787
 34691/100000: episode: 497, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 20.069, mean reward: 0.836 [0.775, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.478, 10.565], loss: 0.001595, mae: 0.044611, mean_q: 1.438833
 34718/100000: episode: 498, duration: 0.137s, episode steps: 27, steps per second: 196, episode reward: 17.610, mean reward: 0.652 [0.573, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.035, 10.240], loss: 0.001682, mae: 0.044529, mean_q: 1.426591
 34755/100000: episode: 499, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 27.631, mean reward: 0.747 [0.639, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.117, 10.445], loss: 0.002129, mae: 0.047478, mean_q: 1.435234
 34786/100000: episode: 500, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 24.792, mean reward: 0.800 [0.685, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.418, 10.421], loss: 0.002126, mae: 0.044947, mean_q: 1.430274
 34801/100000: episode: 501, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 11.771, mean reward: 0.785 [0.678, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.490], loss: 0.001801, mae: 0.045716, mean_q: 1.419813
 34816/100000: episode: 502, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 11.480, mean reward: 0.765 [0.683, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.191, 10.487], loss: 0.001755, mae: 0.045882, mean_q: 1.437587
 34839/100000: episode: 503, duration: 0.144s, episode steps: 23, steps per second: 160, episode reward: 15.865, mean reward: 0.690 [0.528, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.219], loss: 0.001812, mae: 0.047370, mean_q: 1.432897
 34861/100000: episode: 504, duration: 0.153s, episode steps: 22, steps per second: 144, episode reward: 16.475, mean reward: 0.749 [0.677, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.210, 10.405], loss: 0.001539, mae: 0.043569, mean_q: 1.423889
 34897/100000: episode: 505, duration: 0.175s, episode steps: 36, steps per second: 206, episode reward: 27.308, mean reward: 0.759 [0.686, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.866, 10.451], loss: 0.001376, mae: 0.040758, mean_q: 1.429886
 34928/100000: episode: 506, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 21.050, mean reward: 0.679 [0.533, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.365, 10.194], loss: 0.001845, mae: 0.043332, mean_q: 1.436401
 34952/100000: episode: 507, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 18.590, mean reward: 0.775 [0.701, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.158, 10.425], loss: 0.001414, mae: 0.041816, mean_q: 1.446915
 34979/100000: episode: 508, duration: 0.130s, episode steps: 27, steps per second: 207, episode reward: 22.497, mean reward: 0.833 [0.733, 0.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.303, 10.501], loss: 0.001575, mae: 0.044620, mean_q: 1.430385
 34990/100000: episode: 509, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 9.452, mean reward: 0.859 [0.792, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.632], loss: 0.001756, mae: 0.047085, mean_q: 1.433764
[Info] Complete ISplit Iteration
[Info] Levels: [1.285855, 1.3429549, 1.480687, 1.6083795, 1.6714636]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.53]
[Info] Error Prob: 5.300000000000002e-05

 35021/100000: episode: 510, duration: 4.673s, episode steps: 31, steps per second: 7, episode reward: 25.782, mean reward: 0.832 [0.666, 0.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.388, 10.381], loss: 0.002204, mae: 0.046967, mean_q: 1.434212
 35121/100000: episode: 511, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.361, mean reward: 0.574 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.287, 10.106], loss: 0.001737, mae: 0.044853, mean_q: 1.436473
 35221/100000: episode: 512, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.494, mean reward: 0.595 [0.511, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.108, 10.404], loss: 0.001856, mae: 0.045654, mean_q: 1.435221
 35321/100000: episode: 513, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.989, mean reward: 0.580 [0.502, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.083, 10.098], loss: 0.001942, mae: 0.046423, mean_q: 1.431278
 35421/100000: episode: 514, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 62.058, mean reward: 0.621 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.702, 10.098], loss: 0.001967, mae: 0.047162, mean_q: 1.415036
 35521/100000: episode: 515, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 57.330, mean reward: 0.573 [0.506, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.091, 10.098], loss: 0.001901, mae: 0.046971, mean_q: 1.416441
 35621/100000: episode: 516, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 59.989, mean reward: 0.600 [0.514, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.590, 10.098], loss: 0.001704, mae: 0.045695, mean_q: 1.404127
 35721/100000: episode: 517, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 59.675, mean reward: 0.597 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.573, 10.098], loss: 0.002015, mae: 0.047318, mean_q: 1.407756
 35821/100000: episode: 518, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.242, mean reward: 0.572 [0.500, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.209, 10.098], loss: 0.001795, mae: 0.045210, mean_q: 1.403857
 35921/100000: episode: 519, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.496, mean reward: 0.585 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.723, 10.432], loss: 0.001792, mae: 0.045581, mean_q: 1.401016
 36021/100000: episode: 520, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 56.934, mean reward: 0.569 [0.506, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.732, 10.123], loss: 0.001878, mae: 0.046981, mean_q: 1.395564
 36121/100000: episode: 521, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.150, mean reward: 0.591 [0.510, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.197, 10.212], loss: 0.001764, mae: 0.045214, mean_q: 1.398674
 36221/100000: episode: 522, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 61.198, mean reward: 0.612 [0.499, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.783, 10.380], loss: 0.001772, mae: 0.045806, mean_q: 1.386567
 36321/100000: episode: 523, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 57.389, mean reward: 0.574 [0.512, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.604, 10.186], loss: 0.001861, mae: 0.046274, mean_q: 1.384735
 36421/100000: episode: 524, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 57.911, mean reward: 0.579 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.209, 10.098], loss: 0.001716, mae: 0.043954, mean_q: 1.379074
 36521/100000: episode: 525, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.716, mean reward: 0.577 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.777, 10.423], loss: 0.001853, mae: 0.046090, mean_q: 1.377773
 36621/100000: episode: 526, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.871, mean reward: 0.579 [0.500, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.811, 10.220], loss: 0.001859, mae: 0.044748, mean_q: 1.368019
 36721/100000: episode: 527, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.073, mean reward: 0.581 [0.498, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.579, 10.137], loss: 0.001936, mae: 0.046035, mean_q: 1.358548
 36821/100000: episode: 528, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 56.824, mean reward: 0.568 [0.504, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.652, 10.219], loss: 0.001759, mae: 0.045799, mean_q: 1.353453
 36921/100000: episode: 529, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.032, mean reward: 0.590 [0.508, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.379, 10.098], loss: 0.001823, mae: 0.045142, mean_q: 1.352424
 37021/100000: episode: 530, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.867, mean reward: 0.589 [0.514, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.367, 10.309], loss: 0.001918, mae: 0.045774, mean_q: 1.345824
 37121/100000: episode: 531, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 60.009, mean reward: 0.600 [0.513, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.919, 10.191], loss: 0.001977, mae: 0.045206, mean_q: 1.337605
 37221/100000: episode: 532, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 59.099, mean reward: 0.591 [0.500, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.057, 10.414], loss: 0.001689, mae: 0.044340, mean_q: 1.341575
 37321/100000: episode: 533, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 66.060, mean reward: 0.661 [0.519, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.188, 10.461], loss: 0.001852, mae: 0.046642, mean_q: 1.333922
 37421/100000: episode: 534, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 59.310, mean reward: 0.593 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.463, 10.098], loss: 0.001735, mae: 0.045911, mean_q: 1.329807
 37521/100000: episode: 535, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.470, mean reward: 0.575 [0.498, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.169, 10.098], loss: 0.001701, mae: 0.045093, mean_q: 1.324756
 37621/100000: episode: 536, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 60.088, mean reward: 0.601 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.935, 10.213], loss: 0.001968, mae: 0.046907, mean_q: 1.321550
 37721/100000: episode: 537, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 60.017, mean reward: 0.600 [0.501, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.013, 10.239], loss: 0.001693, mae: 0.042978, mean_q: 1.323868
 37821/100000: episode: 538, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.725, mean reward: 0.577 [0.502, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.240, 10.288], loss: 0.002107, mae: 0.047741, mean_q: 1.312207
 37921/100000: episode: 539, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.172, mean reward: 0.572 [0.501, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.098], loss: 0.001818, mae: 0.045950, mean_q: 1.305434
 38021/100000: episode: 540, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 62.218, mean reward: 0.622 [0.508, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.819, 10.198], loss: 0.001911, mae: 0.046262, mean_q: 1.294308
 38121/100000: episode: 541, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 58.798, mean reward: 0.588 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.710, 10.335], loss: 0.001891, mae: 0.045310, mean_q: 1.291930
 38221/100000: episode: 542, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 57.704, mean reward: 0.577 [0.508, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.883, 10.186], loss: 0.001874, mae: 0.045212, mean_q: 1.291717
 38321/100000: episode: 543, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.639, mean reward: 0.576 [0.507, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.841, 10.098], loss: 0.001926, mae: 0.046326, mean_q: 1.278834
 38421/100000: episode: 544, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.590, mean reward: 0.596 [0.500, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.741, 10.098], loss: 0.001989, mae: 0.045920, mean_q: 1.264405
 38521/100000: episode: 545, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.072, mean reward: 0.591 [0.503, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.850, 10.374], loss: 0.001892, mae: 0.044840, mean_q: 1.266029
 38621/100000: episode: 546, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 57.899, mean reward: 0.579 [0.501, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.437, 10.100], loss: 0.001825, mae: 0.045330, mean_q: 1.260010
 38721/100000: episode: 547, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.503, mean reward: 0.595 [0.513, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.199, 10.098], loss: 0.001602, mae: 0.043934, mean_q: 1.248154
 38821/100000: episode: 548, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 60.804, mean reward: 0.608 [0.513, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.807, 10.423], loss: 0.002111, mae: 0.047310, mean_q: 1.247890
 38921/100000: episode: 549, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 57.824, mean reward: 0.578 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.757, 10.098], loss: 0.001946, mae: 0.045250, mean_q: 1.241284
 39021/100000: episode: 550, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 60.821, mean reward: 0.608 [0.519, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.449, 10.344], loss: 0.001775, mae: 0.045304, mean_q: 1.229395
 39121/100000: episode: 551, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.806, mean reward: 0.598 [0.502, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.971, 10.136], loss: 0.001691, mae: 0.044783, mean_q: 1.224295
 39221/100000: episode: 552, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 61.475, mean reward: 0.615 [0.507, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.876, 10.196], loss: 0.001705, mae: 0.044623, mean_q: 1.215855
 39321/100000: episode: 553, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 56.736, mean reward: 0.567 [0.501, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.247, 10.098], loss: 0.001638, mae: 0.044110, mean_q: 1.211902
 39421/100000: episode: 554, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.631, mean reward: 0.596 [0.505, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.831, 10.318], loss: 0.001699, mae: 0.044474, mean_q: 1.205634
 39521/100000: episode: 555, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.453, mean reward: 0.585 [0.503, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.054, 10.193], loss: 0.001587, mae: 0.043728, mean_q: 1.197373
 39621/100000: episode: 556, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.802, mean reward: 0.608 [0.508, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.740, 10.299], loss: 0.001600, mae: 0.043379, mean_q: 1.196107
 39721/100000: episode: 557, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 59.449, mean reward: 0.594 [0.499, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.477, 10.176], loss: 0.001654, mae: 0.044306, mean_q: 1.187110
 39821/100000: episode: 558, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 58.472, mean reward: 0.585 [0.501, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.888, 10.098], loss: 0.001664, mae: 0.044281, mean_q: 1.178279
 39921/100000: episode: 559, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 58.675, mean reward: 0.587 [0.506, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.492, 10.188], loss: 0.001597, mae: 0.042953, mean_q: 1.177842
 40021/100000: episode: 560, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.248, mean reward: 0.582 [0.507, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.880, 10.183], loss: 0.001468, mae: 0.041237, mean_q: 1.168036
 40121/100000: episode: 561, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 60.923, mean reward: 0.609 [0.508, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.719, 10.098], loss: 0.001642, mae: 0.043798, mean_q: 1.170353
 40221/100000: episode: 562, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.422, mean reward: 0.594 [0.507, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.606, 10.098], loss: 0.001540, mae: 0.042773, mean_q: 1.169029
 40321/100000: episode: 563, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 58.723, mean reward: 0.587 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.185, 10.157], loss: 0.001510, mae: 0.042241, mean_q: 1.167784
 40421/100000: episode: 564, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 56.802, mean reward: 0.568 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.629, 10.195], loss: 0.001455, mae: 0.042359, mean_q: 1.166688
 40521/100000: episode: 565, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 63.069, mean reward: 0.631 [0.512, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.024, 10.098], loss: 0.001470, mae: 0.041765, mean_q: 1.168086
 40621/100000: episode: 566, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.704, mean reward: 0.597 [0.501, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.769, 10.098], loss: 0.001477, mae: 0.041917, mean_q: 1.168932
 40721/100000: episode: 567, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 60.035, mean reward: 0.600 [0.501, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.995, 10.235], loss: 0.001540, mae: 0.042443, mean_q: 1.168409
 40821/100000: episode: 568, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.423, mean reward: 0.594 [0.503, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.443, 10.098], loss: 0.001532, mae: 0.043075, mean_q: 1.171148
 40921/100000: episode: 569, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 58.174, mean reward: 0.582 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.179, 10.098], loss: 0.001586, mae: 0.043498, mean_q: 1.167396
 41021/100000: episode: 570, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.590, mean reward: 0.596 [0.500, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.819, 10.185], loss: 0.001495, mae: 0.042217, mean_q: 1.169208
 41121/100000: episode: 571, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 57.859, mean reward: 0.579 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.594, 10.207], loss: 0.001581, mae: 0.043432, mean_q: 1.169124
 41221/100000: episode: 572, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.207, mean reward: 0.582 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.384, 10.324], loss: 0.001663, mae: 0.043630, mean_q: 1.169334
 41321/100000: episode: 573, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 58.368, mean reward: 0.584 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.604, 10.098], loss: 0.001600, mae: 0.043033, mean_q: 1.167265
 41421/100000: episode: 574, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 62.193, mean reward: 0.622 [0.513, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.481, 10.235], loss: 0.001655, mae: 0.044524, mean_q: 1.168170
 41521/100000: episode: 575, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 58.550, mean reward: 0.586 [0.518, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.833, 10.132], loss: 0.001586, mae: 0.042771, mean_q: 1.170865
 41621/100000: episode: 576, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 59.882, mean reward: 0.599 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.585, 10.293], loss: 0.001544, mae: 0.043357, mean_q: 1.169905
 41721/100000: episode: 577, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 58.035, mean reward: 0.580 [0.502, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.276, 10.098], loss: 0.001609, mae: 0.043563, mean_q: 1.173935
 41821/100000: episode: 578, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 59.322, mean reward: 0.593 [0.510, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.124, 10.196], loss: 0.001657, mae: 0.044703, mean_q: 1.172406
 41921/100000: episode: 579, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.902, mean reward: 0.579 [0.501, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.122, 10.207], loss: 0.001651, mae: 0.044011, mean_q: 1.176066
 42021/100000: episode: 580, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 61.764, mean reward: 0.618 [0.522, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.808, 10.145], loss: 0.001509, mae: 0.042361, mean_q: 1.175351
 42121/100000: episode: 581, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 58.833, mean reward: 0.588 [0.511, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.456, 10.098], loss: 0.001564, mae: 0.042811, mean_q: 1.170493
 42221/100000: episode: 582, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.002, mean reward: 0.570 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.627, 10.098], loss: 0.001574, mae: 0.043286, mean_q: 1.169663
 42321/100000: episode: 583, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 59.815, mean reward: 0.598 [0.513, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.621, 10.324], loss: 0.001548, mae: 0.042615, mean_q: 1.170264
 42421/100000: episode: 584, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 60.146, mean reward: 0.601 [0.500, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.826, 10.114], loss: 0.001711, mae: 0.044766, mean_q: 1.171547
 42521/100000: episode: 585, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.476, mean reward: 0.585 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.502, 10.098], loss: 0.001440, mae: 0.041680, mean_q: 1.169036
 42621/100000: episode: 586, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.271, mean reward: 0.573 [0.505, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.284, 10.227], loss: 0.001671, mae: 0.043918, mean_q: 1.170351
 42721/100000: episode: 587, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 56.521, mean reward: 0.565 [0.506, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.404, 10.098], loss: 0.001529, mae: 0.042913, mean_q: 1.170704
 42821/100000: episode: 588, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.824, mean reward: 0.588 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.586, 10.175], loss: 0.001566, mae: 0.042859, mean_q: 1.168424
 42921/100000: episode: 589, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.883, mean reward: 0.599 [0.503, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.125, 10.250], loss: 0.001487, mae: 0.042260, mean_q: 1.168764
 43021/100000: episode: 590, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 58.108, mean reward: 0.581 [0.499, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.138, 10.172], loss: 0.001580, mae: 0.043627, mean_q: 1.170711
 43121/100000: episode: 591, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.360, mean reward: 0.574 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.940, 10.098], loss: 0.001594, mae: 0.043592, mean_q: 1.170141
 43221/100000: episode: 592, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.798, mean reward: 0.568 [0.501, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.316, 10.136], loss: 0.001485, mae: 0.041949, mean_q: 1.167319
 43321/100000: episode: 593, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.016, mean reward: 0.570 [0.509, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.708, 10.125], loss: 0.001485, mae: 0.042316, mean_q: 1.172553
 43421/100000: episode: 594, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.584, mean reward: 0.586 [0.513, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.829, 10.388], loss: 0.001593, mae: 0.043225, mean_q: 1.170836
 43521/100000: episode: 595, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 60.700, mean reward: 0.607 [0.506, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.864, 10.223], loss: 0.001625, mae: 0.044669, mean_q: 1.168083
 43621/100000: episode: 596, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.184, mean reward: 0.582 [0.502, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.308, 10.098], loss: 0.001497, mae: 0.042614, mean_q: 1.167754
 43721/100000: episode: 597, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 59.920, mean reward: 0.599 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.778, 10.098], loss: 0.001544, mae: 0.042289, mean_q: 1.165729
 43821/100000: episode: 598, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 61.288, mean reward: 0.613 [0.512, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.037, 10.414], loss: 0.001471, mae: 0.041897, mean_q: 1.167371
 43921/100000: episode: 599, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.789, mean reward: 0.578 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.771, 10.123], loss: 0.001473, mae: 0.042002, mean_q: 1.167628
 44021/100000: episode: 600, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 58.049, mean reward: 0.580 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.610, 10.098], loss: 0.001545, mae: 0.043063, mean_q: 1.164754
 44121/100000: episode: 601, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.503, mean reward: 0.575 [0.504, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.387, 10.098], loss: 0.001618, mae: 0.043663, mean_q: 1.166863
 44221/100000: episode: 602, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.325, mean reward: 0.583 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.703, 10.225], loss: 0.001421, mae: 0.041452, mean_q: 1.164345
 44321/100000: episode: 603, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.431, mean reward: 0.584 [0.509, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.024, 10.098], loss: 0.001519, mae: 0.042356, mean_q: 1.165249
 44421/100000: episode: 604, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.886, mean reward: 0.589 [0.502, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.333, 10.098], loss: 0.001550, mae: 0.043322, mean_q: 1.167274
 44521/100000: episode: 605, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.101, mean reward: 0.581 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.400, 10.098], loss: 0.001535, mae: 0.042554, mean_q: 1.166446
 44621/100000: episode: 606, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 60.136, mean reward: 0.601 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.246, 10.142], loss: 0.001526, mae: 0.042462, mean_q: 1.163917
 44721/100000: episode: 607, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 59.696, mean reward: 0.597 [0.499, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.271, 10.146], loss: 0.001464, mae: 0.042146, mean_q: 1.165299
 44821/100000: episode: 608, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.430, mean reward: 0.594 [0.518, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.282, 10.192], loss: 0.001499, mae: 0.042547, mean_q: 1.164509
 44921/100000: episode: 609, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.145, mean reward: 0.591 [0.513, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.825, 10.184], loss: 0.001425, mae: 0.041254, mean_q: 1.162906
[Info] 1-TH LEVEL FOUND: 1.3311387300491333, Considering 10/90 traces
 45021/100000: episode: 610, duration: 4.701s, episode steps: 100, steps per second: 21, episode reward: 59.303, mean reward: 0.593 [0.503, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.355, 10.230], loss: 0.001460, mae: 0.041690, mean_q: 1.167419
 45051/100000: episode: 611, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 20.679, mean reward: 0.689 [0.558, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.782, 10.291], loss: 0.001476, mae: 0.042198, mean_q: 1.169079
 45088/100000: episode: 612, duration: 0.183s, episode steps: 37, steps per second: 203, episode reward: 22.895, mean reward: 0.619 [0.562, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.269, 10.334], loss: 0.001530, mae: 0.042869, mean_q: 1.166913
 45140/100000: episode: 613, duration: 0.259s, episode steps: 52, steps per second: 201, episode reward: 32.899, mean reward: 0.633 [0.520, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.427, 10.136], loss: 0.001593, mae: 0.043363, mean_q: 1.167876
 45187/100000: episode: 614, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 29.738, mean reward: 0.633 [0.522, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.859, 10.158], loss: 0.001495, mae: 0.042018, mean_q: 1.165219
 45239/100000: episode: 615, duration: 0.285s, episode steps: 52, steps per second: 182, episode reward: 35.308, mean reward: 0.679 [0.600, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.922, 10.322], loss: 0.001785, mae: 0.045736, mean_q: 1.164911
 45275/100000: episode: 616, duration: 0.176s, episode steps: 36, steps per second: 204, episode reward: 21.070, mean reward: 0.585 [0.498, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.371, 10.100], loss: 0.001618, mae: 0.043734, mean_q: 1.171580
 45367/100000: episode: 617, duration: 0.466s, episode steps: 92, steps per second: 198, episode reward: 52.828, mean reward: 0.574 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-1.343, 10.166], loss: 0.001510, mae: 0.041655, mean_q: 1.167651
 45414/100000: episode: 618, duration: 0.244s, episode steps: 47, steps per second: 193, episode reward: 33.229, mean reward: 0.707 [0.612, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.998, 10.384], loss: 0.001501, mae: 0.042165, mean_q: 1.165799
 45461/100000: episode: 619, duration: 0.275s, episode steps: 47, steps per second: 171, episode reward: 27.868, mean reward: 0.593 [0.513, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.377, 10.128], loss: 0.001559, mae: 0.043621, mean_q: 1.170651
 45498/100000: episode: 620, duration: 0.203s, episode steps: 37, steps per second: 182, episode reward: 27.666, mean reward: 0.748 [0.657, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.249, 10.498], loss: 0.001484, mae: 0.042067, mean_q: 1.170483
 45513/100000: episode: 621, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 10.815, mean reward: 0.721 [0.662, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.971, 10.100], loss: 0.001562, mae: 0.043256, mean_q: 1.176572
 45549/100000: episode: 622, duration: 0.177s, episode steps: 36, steps per second: 204, episode reward: 22.589, mean reward: 0.627 [0.572, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.641, 10.100], loss: 0.001496, mae: 0.041805, mean_q: 1.169928
 45585/100000: episode: 623, duration: 0.216s, episode steps: 36, steps per second: 167, episode reward: 21.228, mean reward: 0.590 [0.542, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.611, 10.100], loss: 0.001642, mae: 0.043575, mean_q: 1.168867
 45637/100000: episode: 624, duration: 0.263s, episode steps: 52, steps per second: 198, episode reward: 31.822, mean reward: 0.612 [0.524, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.345, 10.178], loss: 0.001507, mae: 0.042447, mean_q: 1.170835
 45674/100000: episode: 625, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 25.272, mean reward: 0.683 [0.547, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.174, 10.200], loss: 0.001588, mae: 0.042900, mean_q: 1.171903
 45766/100000: episode: 626, duration: 0.482s, episode steps: 92, steps per second: 191, episode reward: 57.333, mean reward: 0.623 [0.523, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-0.810, 10.214], loss: 0.001637, mae: 0.043877, mean_q: 1.171723
 45858/100000: episode: 627, duration: 0.466s, episode steps: 92, steps per second: 198, episode reward: 55.068, mean reward: 0.599 [0.509, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.524 [-1.175, 10.100], loss: 0.001569, mae: 0.042656, mean_q: 1.171144
 45870/100000: episode: 628, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 7.479, mean reward: 0.623 [0.581, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.215, 10.100], loss: 0.001749, mae: 0.045551, mean_q: 1.166279
 45885/100000: episode: 629, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 10.789, mean reward: 0.719 [0.636, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.629, 10.100], loss: 0.001515, mae: 0.042823, mean_q: 1.168208
 45921/100000: episode: 630, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 22.920, mean reward: 0.637 [0.563, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.272, 10.100], loss: 0.001748, mae: 0.045884, mean_q: 1.173827
 45936/100000: episode: 631, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 9.838, mean reward: 0.656 [0.589, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.214, 10.100], loss: 0.001587, mae: 0.042929, mean_q: 1.173968
 45983/100000: episode: 632, duration: 0.222s, episode steps: 47, steps per second: 212, episode reward: 30.081, mean reward: 0.640 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.264, 10.100], loss: 0.001539, mae: 0.041904, mean_q: 1.176383
 45995/100000: episode: 633, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 7.290, mean reward: 0.608 [0.568, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.136, 10.100], loss: 0.001642, mae: 0.044155, mean_q: 1.168674
 46010/100000: episode: 634, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 10.213, mean reward: 0.681 [0.614, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.361, 10.100], loss: 0.001662, mae: 0.044805, mean_q: 1.176104
 46102/100000: episode: 635, duration: 0.476s, episode steps: 92, steps per second: 193, episode reward: 53.856, mean reward: 0.585 [0.505, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.535 [-1.151, 10.272], loss: 0.001684, mae: 0.044370, mean_q: 1.176872
 46117/100000: episode: 636, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 10.658, mean reward: 0.711 [0.652, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.491, 10.100], loss: 0.001444, mae: 0.041393, mean_q: 1.174339
 46132/100000: episode: 637, duration: 0.080s, episode steps: 15, steps per second: 189, episode reward: 11.133, mean reward: 0.742 [0.576, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.086, 10.100], loss: 0.001732, mae: 0.044395, mean_q: 1.178113
 46184/100000: episode: 638, duration: 0.282s, episode steps: 52, steps per second: 184, episode reward: 39.623, mean reward: 0.762 [0.626, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-1.043, 10.454], loss: 0.001602, mae: 0.042846, mean_q: 1.181416
 46196/100000: episode: 639, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 8.190, mean reward: 0.683 [0.649, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.333, 10.100], loss: 0.001352, mae: 0.041145, mean_q: 1.181385
 46243/100000: episode: 640, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 29.870, mean reward: 0.636 [0.547, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.263, 10.201], loss: 0.001758, mae: 0.045008, mean_q: 1.180317
 46273/100000: episode: 641, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 22.086, mean reward: 0.736 [0.655, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.605, 10.495], loss: 0.001596, mae: 0.043453, mean_q: 1.182981
 46280/100000: episode: 642, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 5.221, mean reward: 0.746 [0.687, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.397, 10.100], loss: 0.001697, mae: 0.045184, mean_q: 1.184851
 46327/100000: episode: 643, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 30.460, mean reward: 0.648 [0.564, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.458, 10.313], loss: 0.001853, mae: 0.045791, mean_q: 1.180514
 46342/100000: episode: 644, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 11.091, mean reward: 0.739 [0.673, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.642, 10.100], loss: 0.001643, mae: 0.044394, mean_q: 1.182790
 46357/100000: episode: 645, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 11.670, mean reward: 0.778 [0.630, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.415, 10.100], loss: 0.001676, mae: 0.044000, mean_q: 1.182550
 46404/100000: episode: 646, duration: 0.255s, episode steps: 47, steps per second: 184, episode reward: 31.496, mean reward: 0.670 [0.584, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.349, 10.494], loss: 0.001834, mae: 0.045296, mean_q: 1.185938
 46451/100000: episode: 647, duration: 0.235s, episode steps: 47, steps per second: 200, episode reward: 29.274, mean reward: 0.623 [0.565, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.268, 10.308], loss: 0.001820, mae: 0.045628, mean_q: 1.191400
 46479/100000: episode: 648, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 20.325, mean reward: 0.726 [0.661, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.241, 10.100], loss: 0.002187, mae: 0.048663, mean_q: 1.183090
 46491/100000: episode: 649, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 7.452, mean reward: 0.621 [0.591, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.187, 10.100], loss: 0.001541, mae: 0.042248, mean_q: 1.198493
 46528/100000: episode: 650, duration: 0.175s, episode steps: 37, steps per second: 211, episode reward: 24.520, mean reward: 0.663 [0.510, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.775, 10.165], loss: 0.001702, mae: 0.045013, mean_q: 1.188130
 46565/100000: episode: 651, duration: 0.183s, episode steps: 37, steps per second: 203, episode reward: 26.042, mean reward: 0.704 [0.559, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.035, 10.277], loss: 0.001673, mae: 0.044658, mean_q: 1.193355
 46657/100000: episode: 652, duration: 0.480s, episode steps: 92, steps per second: 192, episode reward: 53.056, mean reward: 0.577 [0.497, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.516 [-0.820, 10.100], loss: 0.001700, mae: 0.044398, mean_q: 1.191959
 46685/100000: episode: 653, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 19.312, mean reward: 0.690 [0.644, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.486, 10.100], loss: 0.001646, mae: 0.043151, mean_q: 1.194450
 46697/100000: episode: 654, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 7.894, mean reward: 0.658 [0.612, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.152, 10.100], loss: 0.001759, mae: 0.045210, mean_q: 1.195923
 46709/100000: episode: 655, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 8.406, mean reward: 0.700 [0.671, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.169, 10.100], loss: 0.001638, mae: 0.042541, mean_q: 1.181786
 46737/100000: episode: 656, duration: 0.139s, episode steps: 28, steps per second: 202, episode reward: 19.252, mean reward: 0.688 [0.625, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.339, 10.100], loss: 0.001718, mae: 0.045055, mean_q: 1.192112
 46765/100000: episode: 657, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 20.280, mean reward: 0.724 [0.625, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.627, 10.100], loss: 0.001934, mae: 0.047764, mean_q: 1.197880
 46817/100000: episode: 658, duration: 0.270s, episode steps: 52, steps per second: 193, episode reward: 34.589, mean reward: 0.665 [0.525, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-1.627, 10.168], loss: 0.001832, mae: 0.045336, mean_q: 1.189521
 46847/100000: episode: 659, duration: 0.144s, episode steps: 30, steps per second: 208, episode reward: 20.639, mean reward: 0.688 [0.542, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.160, 10.148], loss: 0.001673, mae: 0.045131, mean_q: 1.199249
 46939/100000: episode: 660, duration: 0.465s, episode steps: 92, steps per second: 198, episode reward: 62.423, mean reward: 0.679 [0.514, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-1.230, 10.100], loss: 0.001787, mae: 0.045660, mean_q: 1.194721
 46946/100000: episode: 661, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 4.872, mean reward: 0.696 [0.658, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.246, 10.100], loss: 0.001429, mae: 0.041844, mean_q: 1.207569
 46982/100000: episode: 662, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 22.209, mean reward: 0.617 [0.548, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.035, 10.100], loss: 0.001611, mae: 0.042890, mean_q: 1.201328
 47010/100000: episode: 663, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 18.287, mean reward: 0.653 [0.559, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.013, 10.100], loss: 0.001578, mae: 0.043365, mean_q: 1.208627
 47057/100000: episode: 664, duration: 0.242s, episode steps: 47, steps per second: 194, episode reward: 26.913, mean reward: 0.573 [0.509, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.318, 10.149], loss: 0.001765, mae: 0.045079, mean_q: 1.197535
 47149/100000: episode: 665, duration: 0.481s, episode steps: 92, steps per second: 191, episode reward: 56.886, mean reward: 0.618 [0.500, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-1.467, 10.100], loss: 0.001926, mae: 0.047827, mean_q: 1.199921
 47164/100000: episode: 666, duration: 0.096s, episode steps: 15, steps per second: 157, episode reward: 11.573, mean reward: 0.772 [0.676, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.647, 10.100], loss: 0.001850, mae: 0.046964, mean_q: 1.199229
 47176/100000: episode: 667, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 7.984, mean reward: 0.665 [0.599, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.262, 10.100], loss: 0.001929, mae: 0.047856, mean_q: 1.200663
 47268/100000: episode: 668, duration: 0.466s, episode steps: 92, steps per second: 197, episode reward: 53.486, mean reward: 0.581 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.529 [-0.316, 10.100], loss: 0.001727, mae: 0.044586, mean_q: 1.201707
 47296/100000: episode: 669, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 19.387, mean reward: 0.692 [0.629, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.304, 10.100], loss: 0.001813, mae: 0.045604, mean_q: 1.203152
 47326/100000: episode: 670, duration: 0.180s, episode steps: 30, steps per second: 166, episode reward: 20.234, mean reward: 0.674 [0.578, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.403, 10.407], loss: 0.001975, mae: 0.047829, mean_q: 1.198533
 47418/100000: episode: 671, duration: 0.479s, episode steps: 92, steps per second: 192, episode reward: 56.950, mean reward: 0.619 [0.499, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.520 [-1.263, 10.341], loss: 0.001845, mae: 0.045713, mean_q: 1.203395
 47454/100000: episode: 672, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 22.061, mean reward: 0.613 [0.514, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.106, 10.100], loss: 0.001612, mae: 0.043322, mean_q: 1.203035
 47546/100000: episode: 673, duration: 0.502s, episode steps: 92, steps per second: 183, episode reward: 56.366, mean reward: 0.613 [0.513, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.520 [-1.132, 10.125], loss: 0.001633, mae: 0.043888, mean_q: 1.206010
 47558/100000: episode: 674, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 8.513, mean reward: 0.709 [0.656, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.391, 10.100], loss: 0.001410, mae: 0.041746, mean_q: 1.226547
 47573/100000: episode: 675, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 10.409, mean reward: 0.694 [0.640, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.700, 10.100], loss: 0.001413, mae: 0.040978, mean_q: 1.213360
 47585/100000: episode: 676, duration: 0.060s, episode steps: 12, steps per second: 198, episode reward: 8.672, mean reward: 0.723 [0.655, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.899, 10.100], loss: 0.001421, mae: 0.041298, mean_q: 1.206505
 47613/100000: episode: 677, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 18.729, mean reward: 0.669 [0.603, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.901, 10.100], loss: 0.001862, mae: 0.046837, mean_q: 1.204937
 47643/100000: episode: 678, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 19.286, mean reward: 0.643 [0.513, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.822, 10.201], loss: 0.001823, mae: 0.046313, mean_q: 1.214239
 47658/100000: episode: 679, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 11.566, mean reward: 0.771 [0.638, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.486, 10.100], loss: 0.002073, mae: 0.050011, mean_q: 1.208048
 47686/100000: episode: 680, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 22.631, mean reward: 0.808 [0.669, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.366, 10.100], loss: 0.002146, mae: 0.050128, mean_q: 1.213518
 47778/100000: episode: 681, duration: 0.480s, episode steps: 92, steps per second: 192, episode reward: 54.460, mean reward: 0.592 [0.497, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-1.220, 10.317], loss: 0.001687, mae: 0.044078, mean_q: 1.211595
 47793/100000: episode: 682, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 11.642, mean reward: 0.776 [0.727, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.532, 10.100], loss: 0.002057, mae: 0.048989, mean_q: 1.209203
 47808/100000: episode: 683, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 10.729, mean reward: 0.715 [0.640, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.332, 10.100], loss: 0.001804, mae: 0.046040, mean_q: 1.207448
 47844/100000: episode: 684, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 23.026, mean reward: 0.640 [0.550, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.819, 10.100], loss: 0.001949, mae: 0.046663, mean_q: 1.214219
 47856/100000: episode: 685, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 8.719, mean reward: 0.727 [0.657, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.225, 10.100], loss: 0.001698, mae: 0.046007, mean_q: 1.204199
 47884/100000: episode: 686, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 22.623, mean reward: 0.808 [0.686, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.525, 10.100], loss: 0.001960, mae: 0.047465, mean_q: 1.216266
 47921/100000: episode: 687, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 24.245, mean reward: 0.655 [0.518, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.577, 10.193], loss: 0.001650, mae: 0.043607, mean_q: 1.213311
 47973/100000: episode: 688, duration: 0.262s, episode steps: 52, steps per second: 198, episode reward: 35.010, mean reward: 0.673 [0.552, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-1.130, 10.276], loss: 0.001839, mae: 0.045912, mean_q: 1.216517
 48065/100000: episode: 689, duration: 0.486s, episode steps: 92, steps per second: 189, episode reward: 55.716, mean reward: 0.606 [0.501, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-0.853, 10.100], loss: 0.001574, mae: 0.042467, mean_q: 1.221228
 48117/100000: episode: 690, duration: 0.277s, episode steps: 52, steps per second: 187, episode reward: 37.076, mean reward: 0.713 [0.624, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-1.034, 10.347], loss: 0.001804, mae: 0.046159, mean_q: 1.228011
 48129/100000: episode: 691, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 8.406, mean reward: 0.701 [0.619, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.560, 10.100], loss: 0.001918, mae: 0.045934, mean_q: 1.232890
 48136/100000: episode: 692, duration: 0.055s, episode steps: 7, steps per second: 128, episode reward: 5.048, mean reward: 0.721 [0.673, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.946, 10.100], loss: 0.001631, mae: 0.044157, mean_q: 1.237617
 48188/100000: episode: 693, duration: 0.250s, episode steps: 52, steps per second: 208, episode reward: 35.257, mean reward: 0.678 [0.516, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-1.503, 10.215], loss: 0.001729, mae: 0.044873, mean_q: 1.223895
 48280/100000: episode: 694, duration: 0.479s, episode steps: 92, steps per second: 192, episode reward: 55.563, mean reward: 0.604 [0.508, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-1.046, 10.100], loss: 0.001915, mae: 0.047197, mean_q: 1.225062
 48310/100000: episode: 695, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 20.784, mean reward: 0.693 [0.567, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.269], loss: 0.001739, mae: 0.045481, mean_q: 1.237803
 48357/100000: episode: 696, duration: 0.261s, episode steps: 47, steps per second: 180, episode reward: 32.126, mean reward: 0.684 [0.588, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.230, 10.406], loss: 0.001602, mae: 0.043191, mean_q: 1.229651
 48369/100000: episode: 697, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 8.750, mean reward: 0.729 [0.668, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.932, 10.100], loss: 0.001731, mae: 0.045846, mean_q: 1.233340
 48421/100000: episode: 698, duration: 0.261s, episode steps: 52, steps per second: 199, episode reward: 37.420, mean reward: 0.720 [0.577, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.341, 10.381], loss: 0.001706, mae: 0.044239, mean_q: 1.230800
 48457/100000: episode: 699, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 22.092, mean reward: 0.614 [0.563, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.205, 10.100], loss: 0.001912, mae: 0.046804, mean_q: 1.223736
[Info] 2-TH LEVEL FOUND: 1.553668737411499, Considering 10/90 traces
 48472/100000: episode: 700, duration: 4.268s, episode steps: 15, steps per second: 4, episode reward: 11.496, mean reward: 0.766 [0.666, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.560, 10.100], loss: 0.001626, mae: 0.043032, mean_q: 1.232040
 48480/100000: episode: 701, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 6.554, mean reward: 0.819 [0.758, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.496, 10.100], loss: 0.001581, mae: 0.041946, mean_q: 1.221181
 48500/100000: episode: 702, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 14.372, mean reward: 0.719 [0.573, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.339], loss: 0.001627, mae: 0.044098, mean_q: 1.230602
 48526/100000: episode: 703, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 20.361, mean reward: 0.783 [0.700, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.303, 10.100], loss: 0.001701, mae: 0.045228, mean_q: 1.243443
 48546/100000: episode: 704, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 15.968, mean reward: 0.798 [0.676, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.309], loss: 0.001867, mae: 0.046727, mean_q: 1.232741
 48553/100000: episode: 705, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 5.549, mean reward: 0.793 [0.718, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.323, 10.100], loss: 0.001965, mae: 0.048466, mean_q: 1.202549
 48561/100000: episode: 706, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 6.326, mean reward: 0.791 [0.740, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.393, 10.100], loss: 0.001935, mae: 0.048669, mean_q: 1.231077
 48583/100000: episode: 707, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 18.043, mean reward: 0.820 [0.729, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.418, 10.100], loss: 0.001830, mae: 0.045687, mean_q: 1.242979
 48607/100000: episode: 708, duration: 0.120s, episode steps: 24, steps per second: 199, episode reward: 19.486, mean reward: 0.812 [0.713, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.364, 10.100], loss: 0.001992, mae: 0.047698, mean_q: 1.239813
 48626/100000: episode: 709, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 13.609, mean reward: 0.716 [0.624, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.914, 10.432], loss: 0.002101, mae: 0.049680, mean_q: 1.241209
 48633/100000: episode: 710, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 5.481, mean reward: 0.783 [0.761, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.531, 10.100], loss: 0.001388, mae: 0.041685, mean_q: 1.241703
 48641/100000: episode: 711, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 6.379, mean reward: 0.797 [0.741, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.749, 10.100], loss: 0.001775, mae: 0.045089, mean_q: 1.244495
 48648/100000: episode: 712, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 5.694, mean reward: 0.813 [0.771, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.460, 10.100], loss: 0.001904, mae: 0.046803, mean_q: 1.253684
 48668/100000: episode: 713, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 14.811, mean reward: 0.741 [0.640, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.044, 10.357], loss: 0.001649, mae: 0.044399, mean_q: 1.248927
 48676/100000: episode: 714, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 6.420, mean reward: 0.802 [0.770, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.473, 10.100], loss: 0.001864, mae: 0.047523, mean_q: 1.254432
 48696/100000: episode: 715, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 16.740, mean reward: 0.837 [0.761, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.305, 10.739], loss: 0.001930, mae: 0.047141, mean_q: 1.244749
 48720/100000: episode: 716, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 19.392, mean reward: 0.808 [0.727, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.328, 10.100], loss: 0.001852, mae: 0.045948, mean_q: 1.240527
 48728/100000: episode: 717, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 6.236, mean reward: 0.779 [0.715, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.483, 10.100], loss: 0.001801, mae: 0.047009, mean_q: 1.245678
 48748/100000: episode: 718, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 15.544, mean reward: 0.777 [0.688, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.485], loss: 0.001659, mae: 0.043313, mean_q: 1.255554
 48755/100000: episode: 719, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 5.553, mean reward: 0.793 [0.763, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.554, 10.100], loss: 0.001687, mae: 0.047046, mean_q: 1.277936
 48774/100000: episode: 720, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 14.603, mean reward: 0.769 [0.689, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.365, 10.419], loss: 0.001946, mae: 0.047458, mean_q: 1.258076
 48796/100000: episode: 721, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 18.218, mean reward: 0.828 [0.745, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.464, 10.100], loss: 0.001959, mae: 0.047816, mean_q: 1.252218
 48804/100000: episode: 722, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 6.101, mean reward: 0.763 [0.734, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.408, 10.100], loss: 0.001973, mae: 0.048723, mean_q: 1.245860
 48823/100000: episode: 723, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 15.258, mean reward: 0.803 [0.763, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.727, 10.100], loss: 0.002000, mae: 0.047397, mean_q: 1.250836
 48849/100000: episode: 724, duration: 0.125s, episode steps: 26, steps per second: 209, episode reward: 18.954, mean reward: 0.729 [0.659, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.305, 10.100], loss: 0.001949, mae: 0.046721, mean_q: 1.251826
 48871/100000: episode: 725, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 18.681, mean reward: 0.849 [0.782, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.451, 10.100], loss: 0.001632, mae: 0.042832, mean_q: 1.255593
 48891/100000: episode: 726, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 14.909, mean reward: 0.745 [0.640, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.520, 10.399], loss: 0.001899, mae: 0.047874, mean_q: 1.260723
 48917/100000: episode: 727, duration: 0.130s, episode steps: 26, steps per second: 201, episode reward: 19.756, mean reward: 0.760 [0.715, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.364, 10.100], loss: 0.002200, mae: 0.050890, mean_q: 1.252527
 48925/100000: episode: 728, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 6.361, mean reward: 0.795 [0.751, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.460, 10.100], loss: 0.001772, mae: 0.044206, mean_q: 1.251427
 48951/100000: episode: 729, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 18.895, mean reward: 0.727 [0.673, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.975, 10.100], loss: 0.001844, mae: 0.046261, mean_q: 1.279121
 48959/100000: episode: 730, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 6.535, mean reward: 0.817 [0.756, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.470, 10.100], loss: 0.002053, mae: 0.047727, mean_q: 1.240857
 48967/100000: episode: 731, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 6.488, mean reward: 0.811 [0.759, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.427, 10.100], loss: 0.001654, mae: 0.044200, mean_q: 1.261548
 48974/100000: episode: 732, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 5.304, mean reward: 0.758 [0.734, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.435, 10.100], loss: 0.001797, mae: 0.045478, mean_q: 1.286986
 48982/100000: episode: 733, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 5.915, mean reward: 0.739 [0.706, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.364, 10.100], loss: 0.001857, mae: 0.046884, mean_q: 1.275754
 48989/100000: episode: 734, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 5.369, mean reward: 0.767 [0.732, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.297, 10.100], loss: 0.001717, mae: 0.045554, mean_q: 1.271201
 49011/100000: episode: 735, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 18.839, mean reward: 0.856 [0.785, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.430, 10.100], loss: 0.001781, mae: 0.046051, mean_q: 1.267532
 49031/100000: episode: 736, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 15.674, mean reward: 0.784 [0.676, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.349, 10.100], loss: 0.001662, mae: 0.043986, mean_q: 1.283164
 49039/100000: episode: 737, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 6.565, mean reward: 0.821 [0.784, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.509, 10.100], loss: 0.001787, mae: 0.044323, mean_q: 1.253662
 49047/100000: episode: 738, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 6.086, mean reward: 0.761 [0.731, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.373, 10.100], loss: 0.002313, mae: 0.051554, mean_q: 1.254891
 49066/100000: episode: 739, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 15.759, mean reward: 0.829 [0.712, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.401, 10.100], loss: 0.001960, mae: 0.048657, mean_q: 1.274398
 49074/100000: episode: 740, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 6.004, mean reward: 0.750 [0.695, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.497, 10.100], loss: 0.002106, mae: 0.050057, mean_q: 1.287147
 49094/100000: episode: 741, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 14.912, mean reward: 0.746 [0.659, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.435], loss: 0.002130, mae: 0.049466, mean_q: 1.260928
 49102/100000: episode: 742, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 6.750, mean reward: 0.844 [0.772, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.584, 10.100], loss: 0.001969, mae: 0.049187, mean_q: 1.275356
 49128/100000: episode: 743, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 19.526, mean reward: 0.751 [0.661, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.400, 10.100], loss: 0.001933, mae: 0.046168, mean_q: 1.272888
 49136/100000: episode: 744, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 6.164, mean reward: 0.770 [0.708, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.446, 10.100], loss: 0.002007, mae: 0.049178, mean_q: 1.288482
 49162/100000: episode: 745, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 21.731, mean reward: 0.836 [0.731, 0.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-1.228, 10.100], loss: 0.002064, mae: 0.049695, mean_q: 1.282325
 49169/100000: episode: 746, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 5.939, mean reward: 0.848 [0.794, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.443, 10.100], loss: 0.002271, mae: 0.050413, mean_q: 1.276984
 49176/100000: episode: 747, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 5.932, mean reward: 0.847 [0.807, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.442, 10.100], loss: 0.001723, mae: 0.045631, mean_q: 1.284905
 49202/100000: episode: 748, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 21.324, mean reward: 0.820 [0.714, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.377, 10.100], loss: 0.002188, mae: 0.050243, mean_q: 1.280603
 49209/100000: episode: 749, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 5.529, mean reward: 0.790 [0.741, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.493, 10.100], loss: 0.002005, mae: 0.048296, mean_q: 1.240094
[Info] FALSIFICATION!
 49224/100000: episode: 750, duration: 0.237s, episode steps: 15, steps per second: 63, episode reward: 13.314, mean reward: 0.888 [0.827, 1.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.358, 10.046], loss: 0.002702, mae: 0.056395, mean_q: 1.295618
 49243/100000: episode: 751, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 14.859, mean reward: 0.782 [0.741, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.412, 10.100], loss: 0.002107, mae: 0.049299, mean_q: 1.289211
 49251/100000: episode: 752, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 6.146, mean reward: 0.768 [0.749, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.570, 10.100], loss: 0.002640, mae: 0.055756, mean_q: 1.288050
[Info] FALSIFICATION!
 49259/100000: episode: 753, duration: 0.207s, episode steps: 8, steps per second: 39, episode reward: 7.153, mean reward: 0.894 [0.781, 1.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.051, 9.629], loss: 0.002469, mae: 0.056070, mean_q: 1.283726
 49283/100000: episode: 754, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 19.007, mean reward: 0.792 [0.720, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.241, 10.100], loss: 0.002391, mae: 0.052784, mean_q: 1.284961
 49307/100000: episode: 755, duration: 0.120s, episode steps: 24, steps per second: 201, episode reward: 17.807, mean reward: 0.742 [0.617, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.720, 10.100], loss: 0.002308, mae: 0.049799, mean_q: 1.300880
 49315/100000: episode: 756, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 6.701, mean reward: 0.838 [0.790, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.617, 10.100], loss: 0.001717, mae: 0.043644, mean_q: 1.282582
 49323/100000: episode: 757, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 6.218, mean reward: 0.777 [0.738, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.561, 10.100], loss: 0.001873, mae: 0.046580, mean_q: 1.291898
 49347/100000: episode: 758, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 18.434, mean reward: 0.768 [0.673, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.214, 10.100], loss: 0.002330, mae: 0.051634, mean_q: 1.298125
 49366/100000: episode: 759, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 14.808, mean reward: 0.779 [0.626, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.131, 10.419], loss: 0.002803, mae: 0.052398, mean_q: 1.283930
 49386/100000: episode: 760, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 15.774, mean reward: 0.789 [0.657, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.347, 10.100], loss: 0.002253, mae: 0.050935, mean_q: 1.290416
 49406/100000: episode: 761, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 17.491, mean reward: 0.875 [0.801, 0.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.416, 10.719], loss: 0.001927, mae: 0.047827, mean_q: 1.292528
 49413/100000: episode: 762, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 5.652, mean reward: 0.807 [0.776, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.468, 10.100], loss: 0.001915, mae: 0.048042, mean_q: 1.308044
 49432/100000: episode: 763, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 16.326, mean reward: 0.859 [0.785, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.841, 10.583], loss: 0.001999, mae: 0.045323, mean_q: 1.294629
 49451/100000: episode: 764, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 14.225, mean reward: 0.749 [0.598, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.323], loss: 0.001789, mae: 0.045333, mean_q: 1.299552
 49477/100000: episode: 765, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 21.926, mean reward: 0.843 [0.779, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.470, 10.100], loss: 0.002042, mae: 0.049141, mean_q: 1.291336
 49501/100000: episode: 766, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 19.119, mean reward: 0.797 [0.692, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.266, 10.100], loss: 0.001903, mae: 0.047853, mean_q: 1.299831
 49520/100000: episode: 767, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 14.070, mean reward: 0.741 [0.673, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.079, 10.394], loss: 0.001951, mae: 0.047915, mean_q: 1.293000
 49540/100000: episode: 768, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 14.730, mean reward: 0.737 [0.606, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.524, 10.510], loss: 0.002405, mae: 0.049499, mean_q: 1.304736
 49560/100000: episode: 769, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 17.980, mean reward: 0.899 [0.831, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.547, 10.678], loss: 0.001942, mae: 0.047014, mean_q: 1.313804
 49568/100000: episode: 770, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 6.108, mean reward: 0.764 [0.702, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.368, 10.100], loss: 0.002237, mae: 0.051997, mean_q: 1.320713
 49592/100000: episode: 771, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 17.865, mean reward: 0.744 [0.685, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.427, 10.100], loss: 0.002169, mae: 0.049558, mean_q: 1.316589
 49612/100000: episode: 772, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 16.471, mean reward: 0.824 [0.736, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.238, 10.487], loss: 0.002093, mae: 0.049090, mean_q: 1.301869
 49634/100000: episode: 773, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 19.150, mean reward: 0.870 [0.778, 0.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.756, 10.100], loss: 0.002310, mae: 0.051402, mean_q: 1.315968
 49660/100000: episode: 774, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 22.343, mean reward: 0.859 [0.754, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.507, 10.100], loss: 0.001886, mae: 0.046639, mean_q: 1.315883
 49679/100000: episode: 775, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 14.791, mean reward: 0.778 [0.722, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.467, 10.100], loss: 0.001802, mae: 0.044806, mean_q: 1.314396
 49705/100000: episode: 776, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 20.696, mean reward: 0.796 [0.695, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.807, 10.100], loss: 0.002730, mae: 0.053155, mean_q: 1.323629
 49725/100000: episode: 777, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 14.323, mean reward: 0.716 [0.623, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.335, 10.325], loss: 0.001862, mae: 0.045882, mean_q: 1.323470
 49744/100000: episode: 778, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 15.733, mean reward: 0.828 [0.743, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.562], loss: 0.002002, mae: 0.048559, mean_q: 1.315024
 49763/100000: episode: 779, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 14.073, mean reward: 0.741 [0.656, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-1.088, 10.383], loss: 0.001927, mae: 0.047116, mean_q: 1.315157
 49783/100000: episode: 780, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 15.324, mean reward: 0.766 [0.590, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.184, 10.336], loss: 0.002345, mae: 0.052947, mean_q: 1.328841
 49809/100000: episode: 781, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 19.768, mean reward: 0.760 [0.666, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.515, 10.100], loss: 0.002047, mae: 0.048770, mean_q: 1.337313
 49829/100000: episode: 782, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 16.130, mean reward: 0.806 [0.720, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.196, 10.100], loss: 0.002056, mae: 0.047185, mean_q: 1.326321
 49855/100000: episode: 783, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 19.701, mean reward: 0.758 [0.655, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.243, 10.100], loss: 0.002650, mae: 0.053074, mean_q: 1.321803
 49881/100000: episode: 784, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 18.908, mean reward: 0.727 [0.655, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.268, 10.100], loss: 0.002227, mae: 0.048005, mean_q: 1.332356
 49889/100000: episode: 785, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 6.313, mean reward: 0.789 [0.744, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.896, 10.100], loss: 0.002403, mae: 0.044662, mean_q: 1.332604
 49909/100000: episode: 786, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 16.172, mean reward: 0.809 [0.749, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.663, 10.550], loss: 0.001840, mae: 0.047057, mean_q: 1.338109
 49935/100000: episode: 787, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 19.132, mean reward: 0.736 [0.667, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.168, 10.100], loss: 0.001807, mae: 0.046497, mean_q: 1.332976
 49959/100000: episode: 788, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 20.893, mean reward: 0.871 [0.758, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.368, 10.100], loss: 0.002420, mae: 0.050683, mean_q: 1.324118
 49967/100000: episode: 789, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 6.388, mean reward: 0.798 [0.759, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.473, 10.100], loss: 0.001839, mae: 0.046428, mean_q: 1.344747
[Info] Complete ISplit Iteration
[Info] Levels: [1.3311387, 1.5536687, 1.6916802]
[Info] Cond. Prob: [0.1, 0.1, 0.14]
[Info] Error Prob: 0.0014000000000000004

 49993/100000: episode: 790, duration: 4.533s, episode steps: 26, steps per second: 6, episode reward: 20.047, mean reward: 0.771 [0.715, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.218, 10.100], loss: 0.001950, mae: 0.048786, mean_q: 1.335412
 50093/100000: episode: 791, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.890, mean reward: 0.589 [0.498, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.747, 10.336], loss: 0.002005, mae: 0.048973, mean_q: 1.333475
 50193/100000: episode: 792, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.817, mean reward: 0.578 [0.508, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.028, 10.224], loss: 0.002042, mae: 0.048803, mean_q: 1.329913
 50293/100000: episode: 793, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.662, mean reward: 0.597 [0.505, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.613, 10.268], loss: 0.002020, mae: 0.048552, mean_q: 1.336634
 50393/100000: episode: 794, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.588, mean reward: 0.586 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.112, 10.098], loss: 0.002060, mae: 0.047229, mean_q: 1.324585
 50493/100000: episode: 795, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.646, mean reward: 0.586 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.404, 10.102], loss: 0.002087, mae: 0.047195, mean_q: 1.332183
 50593/100000: episode: 796, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 57.919, mean reward: 0.579 [0.508, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.766, 10.210], loss: 0.002234, mae: 0.049725, mean_q: 1.319019
 50693/100000: episode: 797, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 61.462, mean reward: 0.615 [0.501, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.906, 10.098], loss: 0.002066, mae: 0.048532, mean_q: 1.325261
 50793/100000: episode: 798, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.509, mean reward: 0.575 [0.503, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.847, 10.098], loss: 0.002122, mae: 0.048262, mean_q: 1.328001
 50893/100000: episode: 799, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.196, mean reward: 0.582 [0.500, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.041, 10.147], loss: 0.001880, mae: 0.046077, mean_q: 1.323053
 50993/100000: episode: 800, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.858, mean reward: 0.589 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.945, 10.098], loss: 0.001839, mae: 0.045768, mean_q: 1.323417
 51093/100000: episode: 801, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.993, mean reward: 0.580 [0.509, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.105, 10.229], loss: 0.002193, mae: 0.048558, mean_q: 1.317431
 51193/100000: episode: 802, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 60.078, mean reward: 0.601 [0.510, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.016, 10.098], loss: 0.002062, mae: 0.046550, mean_q: 1.316226
 51293/100000: episode: 803, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 57.706, mean reward: 0.577 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.763, 10.107], loss: 0.001822, mae: 0.045522, mean_q: 1.305096
 51393/100000: episode: 804, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 61.559, mean reward: 0.616 [0.528, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.026, 10.098], loss: 0.001995, mae: 0.046648, mean_q: 1.313439
 51493/100000: episode: 805, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 59.757, mean reward: 0.598 [0.517, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.738, 10.280], loss: 0.002072, mae: 0.048216, mean_q: 1.310055
 51593/100000: episode: 806, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.442, mean reward: 0.594 [0.503, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.360, 10.269], loss: 0.001985, mae: 0.048250, mean_q: 1.302879
 51693/100000: episode: 807, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.127, mean reward: 0.591 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.652, 10.361], loss: 0.001928, mae: 0.046289, mean_q: 1.308531
 51793/100000: episode: 808, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: 63.918, mean reward: 0.639 [0.513, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.692, 10.511], loss: 0.002147, mae: 0.049554, mean_q: 1.300345
 51893/100000: episode: 809, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.403, mean reward: 0.594 [0.511, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.880, 10.098], loss: 0.002019, mae: 0.047042, mean_q: 1.296403
 51993/100000: episode: 810, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.800, mean reward: 0.588 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.297, 10.098], loss: 0.002065, mae: 0.048924, mean_q: 1.296778
 52093/100000: episode: 811, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.282, mean reward: 0.573 [0.500, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.970, 10.153], loss: 0.001869, mae: 0.046291, mean_q: 1.300356
 52193/100000: episode: 812, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 60.709, mean reward: 0.607 [0.508, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.989, 10.098], loss: 0.001998, mae: 0.047445, mean_q: 1.295690
 52293/100000: episode: 813, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.271, mean reward: 0.583 [0.503, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.839, 10.098], loss: 0.002026, mae: 0.047071, mean_q: 1.290931
 52393/100000: episode: 814, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.326, mean reward: 0.583 [0.512, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.526, 10.232], loss: 0.002132, mae: 0.048375, mean_q: 1.287876
 52493/100000: episode: 815, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 61.153, mean reward: 0.612 [0.502, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.092, 10.416], loss: 0.001909, mae: 0.046577, mean_q: 1.283427
 52593/100000: episode: 816, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 60.087, mean reward: 0.601 [0.506, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.824, 10.098], loss: 0.002099, mae: 0.047645, mean_q: 1.286408
 52693/100000: episode: 817, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.301, mean reward: 0.583 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.868, 10.171], loss: 0.002175, mae: 0.047376, mean_q: 1.283742
 52793/100000: episode: 818, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.276, mean reward: 0.573 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.955, 10.147], loss: 0.001928, mae: 0.047051, mean_q: 1.292663
 52893/100000: episode: 819, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 56.848, mean reward: 0.568 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.277, 10.098], loss: 0.001803, mae: 0.045903, mean_q: 1.281960
 52993/100000: episode: 820, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.632, mean reward: 0.596 [0.499, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.595, 10.214], loss: 0.001924, mae: 0.045621, mean_q: 1.280941
 53093/100000: episode: 821, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 60.849, mean reward: 0.608 [0.505, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.524, 10.098], loss: 0.001960, mae: 0.045943, mean_q: 1.274147
 53193/100000: episode: 822, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 61.310, mean reward: 0.613 [0.499, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.057, 10.386], loss: 0.001922, mae: 0.046296, mean_q: 1.273266
 53293/100000: episode: 823, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.151, mean reward: 0.592 [0.506, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.392, 10.098], loss: 0.001914, mae: 0.045244, mean_q: 1.275584
 53393/100000: episode: 824, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.658, mean reward: 0.577 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.213, 10.098], loss: 0.002089, mae: 0.047676, mean_q: 1.265474
 53493/100000: episode: 825, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 57.657, mean reward: 0.577 [0.510, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.535, 10.201], loss: 0.001766, mae: 0.044186, mean_q: 1.262464
 53593/100000: episode: 826, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.929, mean reward: 0.589 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.852, 10.109], loss: 0.001912, mae: 0.045315, mean_q: 1.253407
 53693/100000: episode: 827, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.828, mean reward: 0.588 [0.501, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.177, 10.219], loss: 0.001690, mae: 0.044379, mean_q: 1.248970
 53793/100000: episode: 828, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.661, mean reward: 0.587 [0.505, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.562, 10.098], loss: 0.001722, mae: 0.044191, mean_q: 1.239771
 53893/100000: episode: 829, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 60.556, mean reward: 0.606 [0.518, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.822, 10.227], loss: 0.001672, mae: 0.043341, mean_q: 1.243147
 53993/100000: episode: 830, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 55.874, mean reward: 0.559 [0.499, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.134, 10.104], loss: 0.001551, mae: 0.042339, mean_q: 1.232364
 54093/100000: episode: 831, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 59.288, mean reward: 0.593 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.838, 10.098], loss: 0.001686, mae: 0.043727, mean_q: 1.225184
 54193/100000: episode: 832, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 58.640, mean reward: 0.586 [0.500, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.576, 10.098], loss: 0.001685, mae: 0.043050, mean_q: 1.218727
 54293/100000: episode: 833, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.909, mean reward: 0.589 [0.509, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.751, 10.323], loss: 0.001651, mae: 0.043595, mean_q: 1.212616
 54393/100000: episode: 834, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.567, mean reward: 0.576 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.593, 10.098], loss: 0.001534, mae: 0.041934, mean_q: 1.209434
 54493/100000: episode: 835, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.357, mean reward: 0.584 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.885, 10.318], loss: 0.001569, mae: 0.042249, mean_q: 1.198191
 54593/100000: episode: 836, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.496, mean reward: 0.585 [0.503, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.185, 10.252], loss: 0.001585, mae: 0.043030, mean_q: 1.192206
 54693/100000: episode: 837, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 59.875, mean reward: 0.599 [0.498, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.310, 10.098], loss: 0.001563, mae: 0.042271, mean_q: 1.182006
 54793/100000: episode: 838, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 58.266, mean reward: 0.583 [0.505, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.428, 10.253], loss: 0.001408, mae: 0.040538, mean_q: 1.179682
 54893/100000: episode: 839, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.146, mean reward: 0.581 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.040, 10.211], loss: 0.001484, mae: 0.041570, mean_q: 1.172228
 54993/100000: episode: 840, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 59.418, mean reward: 0.594 [0.511, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.297, 10.098], loss: 0.001455, mae: 0.041487, mean_q: 1.166390
 55093/100000: episode: 841, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.878, mean reward: 0.579 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.363, 10.260], loss: 0.001391, mae: 0.040856, mean_q: 1.163727
 55193/100000: episode: 842, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.379, mean reward: 0.594 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.493, 10.098], loss: 0.001359, mae: 0.040026, mean_q: 1.167686
 55293/100000: episode: 843, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 61.477, mean reward: 0.615 [0.511, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.233, 10.649], loss: 0.001412, mae: 0.040671, mean_q: 1.164595
 55393/100000: episode: 844, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.960, mean reward: 0.580 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.040, 10.203], loss: 0.001428, mae: 0.040435, mean_q: 1.165762
 55493/100000: episode: 845, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 57.408, mean reward: 0.574 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.531, 10.098], loss: 0.001462, mae: 0.041563, mean_q: 1.168962
 55593/100000: episode: 846, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 57.423, mean reward: 0.574 [0.507, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.279, 10.289], loss: 0.001526, mae: 0.041966, mean_q: 1.167785
 55693/100000: episode: 847, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 60.028, mean reward: 0.600 [0.511, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.091, 10.165], loss: 0.001405, mae: 0.040494, mean_q: 1.167371
 55793/100000: episode: 848, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.121, mean reward: 0.601 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.880, 10.251], loss: 0.001364, mae: 0.040543, mean_q: 1.166831
 55893/100000: episode: 849, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.134, mean reward: 0.591 [0.512, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.054, 10.098], loss: 0.001569, mae: 0.042691, mean_q: 1.167615
 55993/100000: episode: 850, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 57.091, mean reward: 0.571 [0.502, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.584, 10.223], loss: 0.001438, mae: 0.041742, mean_q: 1.169630
 56093/100000: episode: 851, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 60.781, mean reward: 0.608 [0.512, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.626, 10.268], loss: 0.001519, mae: 0.041907, mean_q: 1.168170
 56193/100000: episode: 852, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 57.471, mean reward: 0.575 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.222, 10.098], loss: 0.001420, mae: 0.041226, mean_q: 1.166710
 56293/100000: episode: 853, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 56.526, mean reward: 0.565 [0.497, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.840, 10.121], loss: 0.001414, mae: 0.041622, mean_q: 1.165580
 56393/100000: episode: 854, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 67.761, mean reward: 0.678 [0.513, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.140, 10.274], loss: 0.001498, mae: 0.042127, mean_q: 1.165063
 56493/100000: episode: 855, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 60.702, mean reward: 0.607 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.778, 10.098], loss: 0.001403, mae: 0.040666, mean_q: 1.171185
 56593/100000: episode: 856, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.001, mean reward: 0.590 [0.516, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.677, 10.471], loss: 0.001332, mae: 0.039845, mean_q: 1.167720
 56693/100000: episode: 857, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 61.892, mean reward: 0.619 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.314, 10.098], loss: 0.001479, mae: 0.041514, mean_q: 1.165908
 56793/100000: episode: 858, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.149, mean reward: 0.571 [0.498, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.669, 10.259], loss: 0.001386, mae: 0.041203, mean_q: 1.166008
 56893/100000: episode: 859, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.754, mean reward: 0.578 [0.506, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.892, 10.242], loss: 0.001444, mae: 0.040909, mean_q: 1.162490
 56993/100000: episode: 860, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.045, mean reward: 0.590 [0.506, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.879, 10.098], loss: 0.001394, mae: 0.041147, mean_q: 1.167701
 57093/100000: episode: 861, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.266, mean reward: 0.583 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.405, 10.323], loss: 0.001377, mae: 0.040437, mean_q: 1.167281
 57193/100000: episode: 862, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.013, mean reward: 0.580 [0.500, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.526, 10.098], loss: 0.001416, mae: 0.041539, mean_q: 1.167210
 57293/100000: episode: 863, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 57.496, mean reward: 0.575 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.752, 10.186], loss: 0.001436, mae: 0.040891, mean_q: 1.163734
 57393/100000: episode: 864, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 63.051, mean reward: 0.631 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.305, 10.098], loss: 0.001469, mae: 0.042051, mean_q: 1.165083
 57493/100000: episode: 865, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 60.113, mean reward: 0.601 [0.518, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.647, 10.098], loss: 0.001427, mae: 0.040808, mean_q: 1.165333
 57593/100000: episode: 866, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.225, mean reward: 0.572 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.746, 10.098], loss: 0.001536, mae: 0.042173, mean_q: 1.167294
 57693/100000: episode: 867, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 60.083, mean reward: 0.601 [0.512, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.876, 10.098], loss: 0.001500, mae: 0.041777, mean_q: 1.164448
 57793/100000: episode: 868, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 61.640, mean reward: 0.616 [0.522, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.792, 10.098], loss: 0.001545, mae: 0.042881, mean_q: 1.167799
 57893/100000: episode: 869, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 60.952, mean reward: 0.610 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.535, 10.098], loss: 0.001502, mae: 0.042233, mean_q: 1.170449
 57993/100000: episode: 870, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 62.908, mean reward: 0.629 [0.510, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.782, 10.098], loss: 0.001442, mae: 0.041967, mean_q: 1.167778
 58093/100000: episode: 871, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.303, mean reward: 0.593 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.641, 10.098], loss: 0.001391, mae: 0.040966, mean_q: 1.170683
 58193/100000: episode: 872, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.913, mean reward: 0.589 [0.517, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.352, 10.098], loss: 0.001420, mae: 0.041558, mean_q: 1.168451
 58293/100000: episode: 873, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 61.468, mean reward: 0.615 [0.513, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.218], loss: 0.001531, mae: 0.043014, mean_q: 1.170270
 58393/100000: episode: 874, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.697, mean reward: 0.597 [0.511, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.890, 10.240], loss: 0.001570, mae: 0.042890, mean_q: 1.168967
 58493/100000: episode: 875, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 59.713, mean reward: 0.597 [0.506, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.808, 10.329], loss: 0.001380, mae: 0.040666, mean_q: 1.173208
 58593/100000: episode: 876, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 60.000, mean reward: 0.600 [0.512, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.099, 10.098], loss: 0.001534, mae: 0.043192, mean_q: 1.171883
 58693/100000: episode: 877, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 61.313, mean reward: 0.613 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.250, 10.365], loss: 0.001392, mae: 0.040826, mean_q: 1.172334
 58793/100000: episode: 878, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.211, mean reward: 0.572 [0.510, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.873, 10.117], loss: 0.001437, mae: 0.041398, mean_q: 1.176111
 58893/100000: episode: 879, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.939, mean reward: 0.609 [0.511, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.221, 10.268], loss: 0.001579, mae: 0.043630, mean_q: 1.172176
 58993/100000: episode: 880, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.927, mean reward: 0.589 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.695, 10.119], loss: 0.001474, mae: 0.042549, mean_q: 1.176035
 59093/100000: episode: 881, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.012, mean reward: 0.590 [0.507, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.596, 10.098], loss: 0.001488, mae: 0.042543, mean_q: 1.172931
 59193/100000: episode: 882, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 57.967, mean reward: 0.580 [0.498, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.898, 10.156], loss: 0.001491, mae: 0.042585, mean_q: 1.178646
 59293/100000: episode: 883, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.764, mean reward: 0.578 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.020, 10.098], loss: 0.001494, mae: 0.042552, mean_q: 1.176762
 59393/100000: episode: 884, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 58.013, mean reward: 0.580 [0.516, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.415, 10.098], loss: 0.001466, mae: 0.041592, mean_q: 1.174392
 59493/100000: episode: 885, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 60.357, mean reward: 0.604 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.915, 10.252], loss: 0.001411, mae: 0.041605, mean_q: 1.172100
 59593/100000: episode: 886, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 58.283, mean reward: 0.583 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.070, 10.098], loss: 0.001426, mae: 0.041963, mean_q: 1.177318
 59693/100000: episode: 887, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 56.890, mean reward: 0.569 [0.499, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.276, 10.098], loss: 0.001359, mae: 0.040575, mean_q: 1.172469
 59793/100000: episode: 888, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.880, mean reward: 0.589 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.075, 10.098], loss: 0.001548, mae: 0.043250, mean_q: 1.175427
 59893/100000: episode: 889, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.971, mean reward: 0.580 [0.506, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.406, 10.104], loss: 0.001461, mae: 0.042014, mean_q: 1.173568
[Info] 1-TH LEVEL FOUND: 1.400285005569458, Considering 10/90 traces
 59993/100000: episode: 890, duration: 4.938s, episode steps: 100, steps per second: 20, episode reward: 59.924, mean reward: 0.599 [0.504, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.683, 10.098], loss: 0.001486, mae: 0.042783, mean_q: 1.175274
 60010/100000: episode: 891, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 12.459, mean reward: 0.733 [0.642, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.359, 10.100], loss: 0.001379, mae: 0.042029, mean_q: 1.175750
 60036/100000: episode: 892, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 16.641, mean reward: 0.640 [0.533, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.583, 10.100], loss: 0.001610, mae: 0.043485, mean_q: 1.181150
 60098/100000: episode: 893, duration: 0.318s, episode steps: 62, steps per second: 195, episode reward: 38.875, mean reward: 0.627 [0.507, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.783 [-0.971, 10.100], loss: 0.001441, mae: 0.041840, mean_q: 1.178005
 60160/100000: episode: 894, duration: 0.329s, episode steps: 62, steps per second: 188, episode reward: 40.117, mean reward: 0.647 [0.530, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.797 [-0.736, 10.254], loss: 0.001528, mae: 0.042986, mean_q: 1.178623
 60186/100000: episode: 895, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 18.483, mean reward: 0.711 [0.627, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.892, 10.100], loss: 0.001563, mae: 0.044193, mean_q: 1.183099
 60218/100000: episode: 896, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 19.275, mean reward: 0.602 [0.527, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.113, 10.100], loss: 0.001565, mae: 0.043584, mean_q: 1.174145
 60234/100000: episode: 897, duration: 0.084s, episode steps: 16, steps per second: 189, episode reward: 10.804, mean reward: 0.675 [0.628, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-1.317, 10.411], loss: 0.001544, mae: 0.043836, mean_q: 1.174032
 60289/100000: episode: 898, duration: 0.281s, episode steps: 55, steps per second: 195, episode reward: 33.683, mean reward: 0.612 [0.508, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-0.361, 10.100], loss: 0.001449, mae: 0.042410, mean_q: 1.177824
 60302/100000: episode: 899, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 9.355, mean reward: 0.720 [0.687, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.375, 10.453], loss: 0.001631, mae: 0.044275, mean_q: 1.183094
 60318/100000: episode: 900, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 10.981, mean reward: 0.686 [0.630, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.388], loss: 0.001541, mae: 0.043486, mean_q: 1.171414
 60344/100000: episode: 901, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 18.149, mean reward: 0.698 [0.592, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.758, 10.100], loss: 0.001451, mae: 0.041596, mean_q: 1.174789
 60357/100000: episode: 902, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 8.973, mean reward: 0.690 [0.660, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.040, 10.436], loss: 0.001493, mae: 0.043861, mean_q: 1.192189
 60371/100000: episode: 903, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 10.023, mean reward: 0.716 [0.651, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.128, 10.100], loss: 0.001788, mae: 0.044312, mean_q: 1.181924
 60415/100000: episode: 904, duration: 0.244s, episode steps: 44, steps per second: 180, episode reward: 29.754, mean reward: 0.676 [0.570, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.530, 10.337], loss: 0.001662, mae: 0.044850, mean_q: 1.179748
 60447/100000: episode: 905, duration: 0.181s, episode steps: 32, steps per second: 177, episode reward: 20.591, mean reward: 0.643 [0.592, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.116, 10.100], loss: 0.001618, mae: 0.044002, mean_q: 1.188958
 60473/100000: episode: 906, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 17.219, mean reward: 0.662 [0.560, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.134, 10.100], loss: 0.001727, mae: 0.045454, mean_q: 1.185858
 60517/100000: episode: 907, duration: 0.219s, episode steps: 44, steps per second: 201, episode reward: 32.530, mean reward: 0.739 [0.657, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.934, 10.531], loss: 0.001612, mae: 0.044587, mean_q: 1.184665
 60531/100000: episode: 908, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.118, mean reward: 0.723 [0.667, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.112, 10.100], loss: 0.001934, mae: 0.047077, mean_q: 1.179273
 60548/100000: episode: 909, duration: 0.106s, episode steps: 17, steps per second: 161, episode reward: 12.408, mean reward: 0.730 [0.646, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.304, 10.100], loss: 0.001990, mae: 0.048564, mean_q: 1.192702
 60592/100000: episode: 910, duration: 0.242s, episode steps: 44, steps per second: 182, episode reward: 29.773, mean reward: 0.677 [0.581, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.496, 10.341], loss: 0.001578, mae: 0.043039, mean_q: 1.188159
 60606/100000: episode: 911, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 10.057, mean reward: 0.718 [0.677, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.676, 10.100], loss: 0.001567, mae: 0.042999, mean_q: 1.185275
 60622/100000: episode: 912, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 10.790, mean reward: 0.674 [0.647, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.636, 10.413], loss: 0.001886, mae: 0.046921, mean_q: 1.188998
 60636/100000: episode: 913, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 9.954, mean reward: 0.711 [0.657, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.348, 10.100], loss: 0.001810, mae: 0.046681, mean_q: 1.193130
 60691/100000: episode: 914, duration: 0.296s, episode steps: 55, steps per second: 186, episode reward: 37.423, mean reward: 0.680 [0.595, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.844, 10.297], loss: 0.001571, mae: 0.043165, mean_q: 1.189417
 60704/100000: episode: 915, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 9.255, mean reward: 0.712 [0.687, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.469], loss: 0.001240, mae: 0.039270, mean_q: 1.208123
 60720/100000: episode: 916, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 9.639, mean reward: 0.602 [0.532, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.401, 10.187], loss: 0.001683, mae: 0.043139, mean_q: 1.184242
 60782/100000: episode: 917, duration: 0.302s, episode steps: 62, steps per second: 206, episode reward: 46.484, mean reward: 0.750 [0.614, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.786 [-1.408, 10.446], loss: 0.001696, mae: 0.045206, mean_q: 1.188136
 60844/100000: episode: 918, duration: 0.333s, episode steps: 62, steps per second: 186, episode reward: 39.626, mean reward: 0.639 [0.506, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.792 [-1.144, 10.254], loss: 0.001622, mae: 0.043637, mean_q: 1.200820
 60860/100000: episode: 919, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 11.287, mean reward: 0.705 [0.629, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.061, 10.429], loss: 0.001497, mae: 0.042450, mean_q: 1.202161
 60877/100000: episode: 920, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 12.660, mean reward: 0.745 [0.669, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.266, 10.100], loss: 0.001769, mae: 0.045069, mean_q: 1.199708
 60891/100000: episode: 921, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 10.641, mean reward: 0.760 [0.727, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.289, 10.100], loss: 0.002468, mae: 0.053688, mean_q: 1.201355
 60917/100000: episode: 922, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 17.071, mean reward: 0.657 [0.571, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.239, 10.276], loss: 0.001839, mae: 0.046949, mean_q: 1.199740
 60930/100000: episode: 923, duration: 0.087s, episode steps: 13, steps per second: 150, episode reward: 9.366, mean reward: 0.720 [0.678, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.138, 10.486], loss: 0.001693, mae: 0.043740, mean_q: 1.196404
 60947/100000: episode: 924, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 13.120, mean reward: 0.772 [0.682, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.564, 10.100], loss: 0.001664, mae: 0.044211, mean_q: 1.208887
 60963/100000: episode: 925, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 9.861, mean reward: 0.616 [0.554, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.233], loss: 0.001600, mae: 0.043328, mean_q: 1.206280
 60995/100000: episode: 926, duration: 0.151s, episode steps: 32, steps per second: 212, episode reward: 23.361, mean reward: 0.730 [0.649, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.919, 10.100], loss: 0.001879, mae: 0.046541, mean_q: 1.205672
 61050/100000: episode: 927, duration: 0.265s, episode steps: 55, steps per second: 208, episode reward: 36.252, mean reward: 0.659 [0.561, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.418, 10.320], loss: 0.001662, mae: 0.044703, mean_q: 1.208181
 61066/100000: episode: 928, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 11.030, mean reward: 0.689 [0.637, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.515], loss: 0.001576, mae: 0.042599, mean_q: 1.206676
 61082/100000: episode: 929, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 11.835, mean reward: 0.740 [0.656, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.488, 10.423], loss: 0.001777, mae: 0.045252, mean_q: 1.209941
 61137/100000: episode: 930, duration: 0.271s, episode steps: 55, steps per second: 203, episode reward: 32.687, mean reward: 0.594 [0.504, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.663, 10.222], loss: 0.001762, mae: 0.046494, mean_q: 1.206637
 61192/100000: episode: 931, duration: 0.289s, episode steps: 55, steps per second: 191, episode reward: 31.913, mean reward: 0.580 [0.511, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-0.744, 10.100], loss: 0.001704, mae: 0.044804, mean_q: 1.208902
 61208/100000: episode: 932, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 11.674, mean reward: 0.730 [0.690, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.408], loss: 0.001569, mae: 0.043257, mean_q: 1.197553
 61240/100000: episode: 933, duration: 0.167s, episode steps: 32, steps per second: 192, episode reward: 20.899, mean reward: 0.653 [0.573, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.700, 10.100], loss: 0.001738, mae: 0.044960, mean_q: 1.208570
 61253/100000: episode: 934, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 9.221, mean reward: 0.709 [0.659, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.623, 10.477], loss: 0.001791, mae: 0.046411, mean_q: 1.210578
 61270/100000: episode: 935, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 11.881, mean reward: 0.699 [0.638, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.412, 10.100], loss: 0.001729, mae: 0.044839, mean_q: 1.212024
 61287/100000: episode: 936, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 10.867, mean reward: 0.639 [0.594, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.363, 10.100], loss: 0.001672, mae: 0.045367, mean_q: 1.206616
 61313/100000: episode: 937, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 17.192, mean reward: 0.661 [0.602, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.178, 10.100], loss: 0.001876, mae: 0.047683, mean_q: 1.201245
 61368/100000: episode: 938, duration: 0.276s, episode steps: 55, steps per second: 199, episode reward: 35.501, mean reward: 0.645 [0.556, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.603, 10.272], loss: 0.001842, mae: 0.045734, mean_q: 1.205466
 61423/100000: episode: 939, duration: 0.282s, episode steps: 55, steps per second: 195, episode reward: 36.664, mean reward: 0.667 [0.536, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.844 [-1.973, 10.205], loss: 0.001625, mae: 0.042858, mean_q: 1.202097
 61467/100000: episode: 940, duration: 0.219s, episode steps: 44, steps per second: 201, episode reward: 27.224, mean reward: 0.619 [0.524, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.609, 10.100], loss: 0.001677, mae: 0.044430, mean_q: 1.209170
 61522/100000: episode: 941, duration: 0.262s, episode steps: 55, steps per second: 210, episode reward: 32.672, mean reward: 0.594 [0.513, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.376, 10.100], loss: 0.001956, mae: 0.047734, mean_q: 1.213645
 61536/100000: episode: 942, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 10.878, mean reward: 0.777 [0.753, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.436, 10.100], loss: 0.001659, mae: 0.044121, mean_q: 1.203754
 61568/100000: episode: 943, duration: 0.165s, episode steps: 32, steps per second: 193, episode reward: 22.130, mean reward: 0.692 [0.621, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.395, 10.100], loss: 0.001816, mae: 0.046298, mean_q: 1.211417
 61581/100000: episode: 944, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 9.488, mean reward: 0.730 [0.678, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.492], loss: 0.001902, mae: 0.047166, mean_q: 1.192756
 61636/100000: episode: 945, duration: 0.274s, episode steps: 55, steps per second: 201, episode reward: 36.646, mean reward: 0.666 [0.584, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-0.807, 10.315], loss: 0.001546, mae: 0.042506, mean_q: 1.207688
 61662/100000: episode: 946, duration: 0.130s, episode steps: 26, steps per second: 199, episode reward: 16.831, mean reward: 0.647 [0.584, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.443], loss: 0.001536, mae: 0.042725, mean_q: 1.214818
 61676/100000: episode: 947, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 10.144, mean reward: 0.725 [0.670, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.489, 10.100], loss: 0.001711, mae: 0.044044, mean_q: 1.219099
 61690/100000: episode: 948, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 10.192, mean reward: 0.728 [0.677, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.236, 10.100], loss: 0.001688, mae: 0.044425, mean_q: 1.206191
 61707/100000: episode: 949, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 12.495, mean reward: 0.735 [0.674, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.549, 10.100], loss: 0.001732, mae: 0.046435, mean_q: 1.212762
 61720/100000: episode: 950, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 8.608, mean reward: 0.662 [0.614, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.506, 10.345], loss: 0.001628, mae: 0.043359, mean_q: 1.214412
 61764/100000: episode: 951, duration: 0.228s, episode steps: 44, steps per second: 193, episode reward: 30.672, mean reward: 0.697 [0.512, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-1.719, 10.100], loss: 0.001489, mae: 0.042066, mean_q: 1.222210
 61808/100000: episode: 952, duration: 0.223s, episode steps: 44, steps per second: 198, episode reward: 29.715, mean reward: 0.675 [0.544, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.240, 10.216], loss: 0.001728, mae: 0.045121, mean_q: 1.213426
 61821/100000: episode: 953, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 8.825, mean reward: 0.679 [0.646, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.453], loss: 0.001536, mae: 0.042220, mean_q: 1.218754
 61847/100000: episode: 954, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 16.807, mean reward: 0.646 [0.541, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.609, 10.189], loss: 0.001733, mae: 0.045460, mean_q: 1.220570
 61891/100000: episode: 955, duration: 0.222s, episode steps: 44, steps per second: 198, episode reward: 29.722, mean reward: 0.675 [0.576, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.792, 10.304], loss: 0.001836, mae: 0.046065, mean_q: 1.215695
 61907/100000: episode: 956, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 11.948, mean reward: 0.747 [0.715, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.509], loss: 0.001667, mae: 0.044998, mean_q: 1.227403
 61933/100000: episode: 957, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 17.208, mean reward: 0.662 [0.600, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.952, 10.100], loss: 0.001747, mae: 0.045758, mean_q: 1.222447
 61950/100000: episode: 958, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 11.803, mean reward: 0.694 [0.667, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.428, 10.100], loss: 0.001869, mae: 0.047674, mean_q: 1.230790
 61963/100000: episode: 959, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 8.529, mean reward: 0.656 [0.561, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.812, 10.320], loss: 0.001966, mae: 0.047233, mean_q: 1.216369
 61979/100000: episode: 960, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 9.820, mean reward: 0.614 [0.548, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.516, 10.331], loss: 0.001786, mae: 0.046520, mean_q: 1.227112
 61993/100000: episode: 961, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 9.386, mean reward: 0.670 [0.627, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.908, 10.100], loss: 0.002123, mae: 0.048377, mean_q: 1.224717
 62025/100000: episode: 962, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 21.051, mean reward: 0.658 [0.551, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.121, 10.100], loss: 0.001717, mae: 0.044429, mean_q: 1.220634
 62039/100000: episode: 963, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 10.874, mean reward: 0.777 [0.727, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.321, 10.100], loss: 0.001695, mae: 0.044498, mean_q: 1.221464
 62053/100000: episode: 964, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 10.319, mean reward: 0.737 [0.722, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.319, 10.100], loss: 0.001723, mae: 0.044284, mean_q: 1.229148
 62079/100000: episode: 965, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 18.058, mean reward: 0.695 [0.592, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.268, 10.412], loss: 0.001782, mae: 0.045854, mean_q: 1.219446
 62096/100000: episode: 966, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 12.519, mean reward: 0.736 [0.692, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.335, 10.100], loss: 0.001916, mae: 0.048336, mean_q: 1.215488
 62122/100000: episode: 967, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 18.193, mean reward: 0.700 [0.647, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.393, 10.513], loss: 0.001725, mae: 0.044410, mean_q: 1.228781
 62139/100000: episode: 968, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 12.709, mean reward: 0.748 [0.657, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.473, 10.100], loss: 0.001783, mae: 0.046502, mean_q: 1.236497
 62183/100000: episode: 969, duration: 0.222s, episode steps: 44, steps per second: 199, episode reward: 33.030, mean reward: 0.751 [0.610, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.594, 10.563], loss: 0.001685, mae: 0.043970, mean_q: 1.227678
 62199/100000: episode: 970, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 11.083, mean reward: 0.693 [0.651, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.643, 10.376], loss: 0.002002, mae: 0.048314, mean_q: 1.224624
 62261/100000: episode: 971, duration: 0.316s, episode steps: 62, steps per second: 196, episode reward: 36.427, mean reward: 0.588 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-2.103, 10.129], loss: 0.001816, mae: 0.045404, mean_q: 1.233404
 62293/100000: episode: 972, duration: 0.157s, episode steps: 32, steps per second: 203, episode reward: 22.187, mean reward: 0.693 [0.625, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.751, 10.100], loss: 0.001698, mae: 0.044781, mean_q: 1.232617
 62310/100000: episode: 973, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 12.354, mean reward: 0.727 [0.668, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.465, 10.100], loss: 0.001484, mae: 0.041490, mean_q: 1.237360
 62336/100000: episode: 974, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 17.238, mean reward: 0.663 [0.574, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.308], loss: 0.001806, mae: 0.046115, mean_q: 1.239004
 62353/100000: episode: 975, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 12.148, mean reward: 0.715 [0.641, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.545, 10.100], loss: 0.001614, mae: 0.044295, mean_q: 1.237717
 62367/100000: episode: 976, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 9.919, mean reward: 0.709 [0.655, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.403, 10.100], loss: 0.002359, mae: 0.051995, mean_q: 1.231916
 62381/100000: episode: 977, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.106, mean reward: 0.722 [0.682, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.258, 10.100], loss: 0.001888, mae: 0.048620, mean_q: 1.241933
 62395/100000: episode: 978, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 10.157, mean reward: 0.726 [0.678, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.411, 10.100], loss: 0.001827, mae: 0.045644, mean_q: 1.234890
 62439/100000: episode: 979, duration: 0.247s, episode steps: 44, steps per second: 178, episode reward: 30.414, mean reward: 0.691 [0.584, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.067, 10.492], loss: 0.001701, mae: 0.044744, mean_q: 1.235248
[Info] 2-TH LEVEL FOUND: 1.5193449258804321, Considering 10/90 traces
 62501/100000: episode: 980, duration: 4.531s, episode steps: 62, steps per second: 14, episode reward: 39.483, mean reward: 0.637 [0.503, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.789 [-1.028, 10.355], loss: 0.001777, mae: 0.045628, mean_q: 1.234708
 62525/100000: episode: 981, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 16.889, mean reward: 0.704 [0.629, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.053, 10.379], loss: 0.001653, mae: 0.045353, mean_q: 1.242977
 62549/100000: episode: 982, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 17.832, mean reward: 0.743 [0.686, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.576], loss: 0.001828, mae: 0.047153, mean_q: 1.245033
 62572/100000: episode: 983, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 18.075, mean reward: 0.786 [0.705, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.867, 10.556], loss: 0.001651, mae: 0.042859, mean_q: 1.238550
 62596/100000: episode: 984, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 16.438, mean reward: 0.685 [0.584, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.277], loss: 0.001563, mae: 0.041450, mean_q: 1.234349
[Info] FALSIFICATION!
 62599/100000: episode: 985, duration: 0.180s, episode steps: 3, steps per second: 17, episode reward: 2.758, mean reward: 0.919 [0.822, 1.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.105, 9.911], loss: 0.002450, mae: 0.051573, mean_q: 1.224710
 62613/100000: episode: 986, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 11.092, mean reward: 0.792 [0.735, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.630, 10.100], loss: 0.001596, mae: 0.042930, mean_q: 1.245924
 62625/100000: episode: 987, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 9.624, mean reward: 0.802 [0.770, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.746, 10.100], loss: 0.001472, mae: 0.042875, mean_q: 1.251299
 62633/100000: episode: 988, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 6.120, mean reward: 0.765 [0.737, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.373, 10.100], loss: 0.001613, mae: 0.043126, mean_q: 1.244300
 62647/100000: episode: 989, duration: 0.069s, episode steps: 14, steps per second: 201, episode reward: 11.121, mean reward: 0.794 [0.698, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.566, 10.100], loss: 0.001782, mae: 0.045380, mean_q: 1.243236
 62670/100000: episode: 990, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 18.288, mean reward: 0.795 [0.700, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.511, 10.404], loss: 0.001586, mae: 0.043558, mean_q: 1.240967
 62707/100000: episode: 991, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 27.252, mean reward: 0.737 [0.680, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.800, 10.424], loss: 0.001885, mae: 0.045821, mean_q: 1.240715
 62721/100000: episode: 992, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 10.188, mean reward: 0.728 [0.682, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.498, 10.100], loss: 0.001630, mae: 0.044207, mean_q: 1.254828
 62762/100000: episode: 993, duration: 0.214s, episode steps: 41, steps per second: 191, episode reward: 26.768, mean reward: 0.653 [0.554, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.497, 10.241], loss: 0.001719, mae: 0.043574, mean_q: 1.246565
 62813/100000: episode: 994, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 31.203, mean reward: 0.612 [0.510, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.373, 10.152], loss: 0.001637, mae: 0.044062, mean_q: 1.254762
 62825/100000: episode: 995, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 8.983, mean reward: 0.749 [0.702, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.259, 10.100], loss: 0.001751, mae: 0.043822, mean_q: 1.244442
 62837/100000: episode: 996, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 9.622, mean reward: 0.802 [0.732, 0.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.492, 10.100], loss: 0.001469, mae: 0.042031, mean_q: 1.267998
 62846/100000: episode: 997, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 7.079, mean reward: 0.787 [0.739, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.968, 10.100], loss: 0.001745, mae: 0.045646, mean_q: 1.239291
 62883/100000: episode: 998, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 25.906, mean reward: 0.700 [0.611, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.132, 10.318], loss: 0.001908, mae: 0.046902, mean_q: 1.249208
 62934/100000: episode: 999, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 35.037, mean reward: 0.687 [0.541, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.502, 10.274], loss: 0.001950, mae: 0.046975, mean_q: 1.250164
 62975/100000: episode: 1000, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 30.933, mean reward: 0.754 [0.716, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.579, 10.466], loss: 0.001767, mae: 0.044951, mean_q: 1.251159
 62983/100000: episode: 1001, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 6.235, mean reward: 0.779 [0.772, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.405, 10.100], loss: 0.001797, mae: 0.048263, mean_q: 1.263052
 62992/100000: episode: 1002, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 7.350, mean reward: 0.817 [0.768, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.456, 10.100], loss: 0.001880, mae: 0.045264, mean_q: 1.231209
 63006/100000: episode: 1003, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 10.565, mean reward: 0.755 [0.686, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.275, 10.100], loss: 0.001520, mae: 0.042934, mean_q: 1.273498
 63018/100000: episode: 1004, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 9.429, mean reward: 0.786 [0.761, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.495, 10.100], loss: 0.001662, mae: 0.045520, mean_q: 1.249146
 63069/100000: episode: 1005, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 32.367, mean reward: 0.635 [0.507, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.662, 10.146], loss: 0.001763, mae: 0.045721, mean_q: 1.256802
 63093/100000: episode: 1006, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 18.503, mean reward: 0.771 [0.701, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.144, 10.525], loss: 0.001792, mae: 0.046438, mean_q: 1.257309
 63101/100000: episode: 1007, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 6.602, mean reward: 0.825 [0.776, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.482, 10.100], loss: 0.001478, mae: 0.042039, mean_q: 1.263870
 63138/100000: episode: 1008, duration: 0.189s, episode steps: 37, steps per second: 195, episode reward: 23.888, mean reward: 0.646 [0.513, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.335, 10.249], loss: 0.001765, mae: 0.045135, mean_q: 1.261775
 63146/100000: episode: 1009, duration: 0.042s, episode steps: 8, steps per second: 193, episode reward: 6.677, mean reward: 0.835 [0.803, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.344, 10.100], loss: 0.001683, mae: 0.043997, mean_q: 1.270990
 63169/100000: episode: 1010, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 17.979, mean reward: 0.782 [0.712, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.474], loss: 0.001839, mae: 0.045456, mean_q: 1.251808
 63192/100000: episode: 1011, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 19.098, mean reward: 0.830 [0.694, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.435], loss: 0.001769, mae: 0.045143, mean_q: 1.269070
 63204/100000: episode: 1012, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 9.006, mean reward: 0.750 [0.665, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.270, 10.100], loss: 0.001707, mae: 0.044720, mean_q: 1.260346
 63213/100000: episode: 1013, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 7.407, mean reward: 0.823 [0.766, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.496, 10.100], loss: 0.001814, mae: 0.044219, mean_q: 1.261671
[Info] FALSIFICATION!
 63243/100000: episode: 1014, duration: 0.352s, episode steps: 30, steps per second: 85, episode reward: 24.388, mean reward: 0.813 [0.678, 1.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.629, 10.705], loss: 0.001566, mae: 0.042707, mean_q: 1.266686
 63255/100000: episode: 1015, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 9.236, mean reward: 0.770 [0.729, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.496, 10.100], loss: 0.001511, mae: 0.042581, mean_q: 1.279863
 63263/100000: episode: 1016, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 6.577, mean reward: 0.822 [0.775, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.569, 10.100], loss: 0.001675, mae: 0.046442, mean_q: 1.280346
 63314/100000: episode: 1017, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 38.082, mean reward: 0.747 [0.672, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-1.209, 10.451], loss: 0.001984, mae: 0.046293, mean_q: 1.274441
 63356/100000: episode: 1018, duration: 0.209s, episode steps: 42, steps per second: 201, episode reward: 26.559, mean reward: 0.632 [0.515, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.801, 10.173], loss: 0.001949, mae: 0.046585, mean_q: 1.268040
 63365/100000: episode: 1019, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 6.994, mean reward: 0.777 [0.746, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.278, 10.100], loss: 0.001915, mae: 0.045906, mean_q: 1.259726
 63416/100000: episode: 1020, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 33.798, mean reward: 0.663 [0.532, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.974, 10.283], loss: 0.001769, mae: 0.044870, mean_q: 1.274623
 63425/100000: episode: 1021, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 7.341, mean reward: 0.816 [0.754, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.529, 10.100], loss: 0.002045, mae: 0.050943, mean_q: 1.272675
 63439/100000: episode: 1022, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 11.351, mean reward: 0.811 [0.669, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.478, 10.100], loss: 0.001689, mae: 0.045175, mean_q: 1.271400
 63462/100000: episode: 1023, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 17.035, mean reward: 0.741 [0.676, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.362], loss: 0.001475, mae: 0.041215, mean_q: 1.283416
 63513/100000: episode: 1024, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 32.799, mean reward: 0.643 [0.541, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.733, 10.403], loss: 0.001709, mae: 0.044315, mean_q: 1.281767
 63550/100000: episode: 1025, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 23.436, mean reward: 0.633 [0.521, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.886, 10.100], loss: 0.001485, mae: 0.041481, mean_q: 1.280892
 63558/100000: episode: 1026, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 6.184, mean reward: 0.773 [0.751, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.443, 10.100], loss: 0.001376, mae: 0.040633, mean_q: 1.286279
 63609/100000: episode: 1027, duration: 0.265s, episode steps: 51, steps per second: 192, episode reward: 31.930, mean reward: 0.626 [0.516, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.380, 10.166], loss: 0.002026, mae: 0.045842, mean_q: 1.272467
 63623/100000: episode: 1028, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 11.002, mean reward: 0.786 [0.688, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.516, 10.100], loss: 0.002198, mae: 0.051324, mean_q: 1.293703
 63660/100000: episode: 1029, duration: 0.185s, episode steps: 37, steps per second: 200, episode reward: 24.145, mean reward: 0.653 [0.578, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.090, 10.278], loss: 0.002038, mae: 0.045278, mean_q: 1.273116
 63672/100000: episode: 1030, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 9.621, mean reward: 0.802 [0.768, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.392, 10.100], loss: 0.001995, mae: 0.045838, mean_q: 1.278265
 63709/100000: episode: 1031, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 24.639, mean reward: 0.666 [0.568, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.664, 10.274], loss: 0.001660, mae: 0.042849, mean_q: 1.271852
 63733/100000: episode: 1032, duration: 0.160s, episode steps: 24, steps per second: 150, episode reward: 17.407, mean reward: 0.725 [0.674, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.291, 10.474], loss: 0.001662, mae: 0.044364, mean_q: 1.274610
 63770/100000: episode: 1033, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 25.754, mean reward: 0.696 [0.510, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.086, 10.127], loss: 0.001879, mae: 0.045735, mean_q: 1.287614
 63812/100000: episode: 1034, duration: 0.225s, episode steps: 42, steps per second: 187, episode reward: 33.131, mean reward: 0.789 [0.670, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.929, 10.523], loss: 0.001549, mae: 0.042801, mean_q: 1.285511
 63820/100000: episode: 1035, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 6.697, mean reward: 0.837 [0.776, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.397, 10.100], loss: 0.001354, mae: 0.038934, mean_q: 1.281549
 63832/100000: episode: 1036, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 9.434, mean reward: 0.786 [0.749, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.352, 10.100], loss: 0.001481, mae: 0.041152, mean_q: 1.278251
 63855/100000: episode: 1037, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 16.670, mean reward: 0.725 [0.646, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.195, 10.421], loss: 0.001518, mae: 0.042818, mean_q: 1.291137
 63878/100000: episode: 1038, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 17.143, mean reward: 0.745 [0.647, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.263, 10.385], loss: 0.001705, mae: 0.043511, mean_q: 1.290194
 63890/100000: episode: 1039, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 9.695, mean reward: 0.808 [0.720, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.241, 10.100], loss: 0.001314, mae: 0.041073, mean_q: 1.291860
 63914/100000: episode: 1040, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 16.405, mean reward: 0.684 [0.622, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.234, 10.384], loss: 0.001423, mae: 0.041624, mean_q: 1.296615
 63956/100000: episode: 1041, duration: 0.207s, episode steps: 42, steps per second: 203, episode reward: 30.679, mean reward: 0.730 [0.652, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.263, 10.515], loss: 0.001701, mae: 0.043987, mean_q: 1.290496
 63968/100000: episode: 1042, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 9.355, mean reward: 0.780 [0.721, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.268, 10.100], loss: 0.001633, mae: 0.043722, mean_q: 1.289843
 64005/100000: episode: 1043, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 26.766, mean reward: 0.723 [0.656, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.085, 10.421], loss: 0.001507, mae: 0.043193, mean_q: 1.297749
 64047/100000: episode: 1044, duration: 0.257s, episode steps: 42, steps per second: 164, episode reward: 32.947, mean reward: 0.784 [0.664, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.252, 10.496], loss: 0.001577, mae: 0.043923, mean_q: 1.301679
 64056/100000: episode: 1045, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 6.816, mean reward: 0.757 [0.714, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.472, 10.100], loss: 0.001774, mae: 0.046978, mean_q: 1.289085
 64107/100000: episode: 1046, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 33.697, mean reward: 0.661 [0.560, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.569, 10.275], loss: 0.001457, mae: 0.042516, mean_q: 1.301636
 64130/100000: episode: 1047, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 17.614, mean reward: 0.766 [0.717, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.575], loss: 0.001789, mae: 0.045799, mean_q: 1.308717
 64142/100000: episode: 1048, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 9.056, mean reward: 0.755 [0.715, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.665, 10.100], loss: 0.001355, mae: 0.041050, mean_q: 1.306987
 64165/100000: episode: 1049, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 17.958, mean reward: 0.781 [0.712, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.489], loss: 0.001613, mae: 0.044072, mean_q: 1.317754
 64207/100000: episode: 1050, duration: 0.224s, episode steps: 42, steps per second: 188, episode reward: 29.040, mean reward: 0.691 [0.599, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.843, 10.593], loss: 0.001761, mae: 0.045462, mean_q: 1.304113
 64231/100000: episode: 1051, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 18.366, mean reward: 0.765 [0.702, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.738, 10.577], loss: 0.001943, mae: 0.044602, mean_q: 1.302289
 64268/100000: episode: 1052, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 23.173, mean reward: 0.626 [0.511, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.083, 10.229], loss: 0.001531, mae: 0.042799, mean_q: 1.309461
 64305/100000: episode: 1053, duration: 0.211s, episode steps: 37, steps per second: 176, episode reward: 26.816, mean reward: 0.725 [0.614, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.173, 10.329], loss: 0.001405, mae: 0.041039, mean_q: 1.307446
 64356/100000: episode: 1054, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 31.326, mean reward: 0.614 [0.512, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.705, 10.135], loss: 0.001509, mae: 0.040736, mean_q: 1.301613
[Info] FALSIFICATION!
 64382/100000: episode: 1055, duration: 0.297s, episode steps: 26, steps per second: 88, episode reward: 22.314, mean reward: 0.858 [0.746, 1.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.089, 10.073], loss: 0.001556, mae: 0.042494, mean_q: 1.302381
 64424/100000: episode: 1056, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 32.972, mean reward: 0.785 [0.669, 0.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.860, 10.433], loss: 0.001679, mae: 0.040929, mean_q: 1.316245
 64475/100000: episode: 1057, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 34.331, mean reward: 0.673 [0.505, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.930, 10.100], loss: 0.001404, mae: 0.040387, mean_q: 1.315963
 64516/100000: episode: 1058, duration: 0.210s, episode steps: 41, steps per second: 196, episode reward: 27.909, mean reward: 0.681 [0.583, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.433, 10.338], loss: 0.002251, mae: 0.045884, mean_q: 1.314691
 64553/100000: episode: 1059, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 29.016, mean reward: 0.784 [0.659, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.390, 10.418], loss: 0.001527, mae: 0.042882, mean_q: 1.324315
 64567/100000: episode: 1060, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 10.248, mean reward: 0.732 [0.666, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.367, 10.100], loss: 0.001604, mae: 0.045007, mean_q: 1.299355
 64609/100000: episode: 1061, duration: 0.209s, episode steps: 42, steps per second: 201, episode reward: 29.690, mean reward: 0.707 [0.619, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.648, 10.354], loss: 0.001628, mae: 0.044063, mean_q: 1.316171
 64651/100000: episode: 1062, duration: 0.209s, episode steps: 42, steps per second: 201, episode reward: 29.376, mean reward: 0.699 [0.577, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.270, 10.315], loss: 0.001800, mae: 0.045008, mean_q: 1.339123
 64665/100000: episode: 1063, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 11.607, mean reward: 0.829 [0.773, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.397, 10.100], loss: 0.001744, mae: 0.046216, mean_q: 1.315126
 64673/100000: episode: 1064, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 6.054, mean reward: 0.757 [0.722, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.436, 10.100], loss: 0.001638, mae: 0.044656, mean_q: 1.328877
 64697/100000: episode: 1065, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 16.734, mean reward: 0.697 [0.591, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.322], loss: 0.001885, mae: 0.046071, mean_q: 1.334918
 64711/100000: episode: 1066, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 10.328, mean reward: 0.738 [0.713, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.397, 10.100], loss: 0.002424, mae: 0.048258, mean_q: 1.325634
 64748/100000: episode: 1067, duration: 0.181s, episode steps: 37, steps per second: 205, episode reward: 25.833, mean reward: 0.698 [0.581, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.549, 10.367], loss: 0.001556, mae: 0.043772, mean_q: 1.322248
 64756/100000: episode: 1068, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 6.722, mean reward: 0.840 [0.759, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.419, 10.100], loss: 0.003020, mae: 0.049962, mean_q: 1.313447
 64793/100000: episode: 1069, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 23.796, mean reward: 0.643 [0.507, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.156, 10.244], loss: 0.001965, mae: 0.044097, mean_q: 1.326319
[Info] Complete ISplit Iteration
[Info] Levels: [1.400285, 1.5193449, 1.5735698]
[Info] Cond. Prob: [0.1, 0.1, 0.23]
[Info] Error Prob: 0.0023000000000000004

 64844/100000: episode: 1070, duration: 4.693s, episode steps: 51, steps per second: 11, episode reward: 33.156, mean reward: 0.650 [0.537, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.487, 10.351], loss: 0.001633, mae: 0.045258, mean_q: 1.340382
 64944/100000: episode: 1071, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.826, mean reward: 0.578 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.389], loss: 0.001633, mae: 0.043104, mean_q: 1.336268
 65044/100000: episode: 1072, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.034, mean reward: 0.600 [0.502, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.004, 10.239], loss: 0.001602, mae: 0.043259, mean_q: 1.332554
 65144/100000: episode: 1073, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 60.139, mean reward: 0.601 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.873, 10.220], loss: 0.001616, mae: 0.043184, mean_q: 1.332679
 65244/100000: episode: 1074, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.353, mean reward: 0.584 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.190, 10.199], loss: 0.001443, mae: 0.041451, mean_q: 1.324303
 65344/100000: episode: 1075, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 62.019, mean reward: 0.620 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.145, 10.098], loss: 0.001542, mae: 0.040875, mean_q: 1.324947
 65444/100000: episode: 1076, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.792, mean reward: 0.578 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.385, 10.147], loss: 0.001585, mae: 0.041790, mean_q: 1.324732
 65544/100000: episode: 1077, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.171, mean reward: 0.582 [0.511, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.699, 10.210], loss: 0.001314, mae: 0.039826, mean_q: 1.312431
 65644/100000: episode: 1078, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.711, mean reward: 0.587 [0.498, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.488, 10.136], loss: 0.001597, mae: 0.042056, mean_q: 1.313590
 65744/100000: episode: 1079, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 60.158, mean reward: 0.602 [0.514, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.861, 10.316], loss: 0.001428, mae: 0.041000, mean_q: 1.307832
 65844/100000: episode: 1080, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.887, mean reward: 0.589 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.425, 10.131], loss: 0.001406, mae: 0.040707, mean_q: 1.306412
 65944/100000: episode: 1081, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.197, mean reward: 0.592 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.700, 10.344], loss: 0.001587, mae: 0.042249, mean_q: 1.304260
 66044/100000: episode: 1082, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 58.032, mean reward: 0.580 [0.500, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.751, 10.132], loss: 0.001534, mae: 0.041917, mean_q: 1.300567
 66144/100000: episode: 1083, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 57.803, mean reward: 0.578 [0.502, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.236, 10.098], loss: 0.001539, mae: 0.040958, mean_q: 1.295684
 66244/100000: episode: 1084, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 57.915, mean reward: 0.579 [0.508, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.928, 10.202], loss: 0.001704, mae: 0.042534, mean_q: 1.299739
 66344/100000: episode: 1085, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 57.899, mean reward: 0.579 [0.500, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.720, 10.104], loss: 0.001567, mae: 0.042869, mean_q: 1.299129
 66444/100000: episode: 1086, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 62.111, mean reward: 0.621 [0.510, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.222, 10.295], loss: 0.001758, mae: 0.043897, mean_q: 1.296220
 66544/100000: episode: 1087, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.218, mean reward: 0.592 [0.503, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.138, 10.098], loss: 0.001782, mae: 0.043356, mean_q: 1.289333
 66644/100000: episode: 1088, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.170, mean reward: 0.582 [0.500, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.016, 10.212], loss: 0.001623, mae: 0.042932, mean_q: 1.284311
 66744/100000: episode: 1089, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.713, mean reward: 0.587 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.634, 10.198], loss: 0.001805, mae: 0.043518, mean_q: 1.284866
 66844/100000: episode: 1090, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.768, mean reward: 0.598 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.318, 10.419], loss: 0.001591, mae: 0.042988, mean_q: 1.284532
 66944/100000: episode: 1091, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 60.499, mean reward: 0.605 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.330, 10.242], loss: 0.001559, mae: 0.041534, mean_q: 1.282367
 67044/100000: episode: 1092, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 57.682, mean reward: 0.577 [0.498, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.896, 10.098], loss: 0.001685, mae: 0.044039, mean_q: 1.277994
 67144/100000: episode: 1093, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 56.913, mean reward: 0.569 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.988, 10.141], loss: 0.001812, mae: 0.044459, mean_q: 1.271634
 67244/100000: episode: 1094, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 57.421, mean reward: 0.574 [0.509, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.355, 10.231], loss: 0.001936, mae: 0.043850, mean_q: 1.271692
 67344/100000: episode: 1095, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 57.903, mean reward: 0.579 [0.510, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.506, 10.274], loss: 0.001686, mae: 0.042940, mean_q: 1.265631
 67444/100000: episode: 1096, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.921, mean reward: 0.599 [0.511, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.347, 10.214], loss: 0.001765, mae: 0.042975, mean_q: 1.264480
 67544/100000: episode: 1097, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.309, mean reward: 0.573 [0.510, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.139, 10.177], loss: 0.001916, mae: 0.044793, mean_q: 1.257930
 67644/100000: episode: 1098, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 58.807, mean reward: 0.588 [0.504, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.255, 10.299], loss: 0.001827, mae: 0.044330, mean_q: 1.256695
 67744/100000: episode: 1099, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 59.128, mean reward: 0.591 [0.507, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.623, 10.254], loss: 0.001763, mae: 0.044443, mean_q: 1.245532
 67844/100000: episode: 1100, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.179, mean reward: 0.572 [0.502, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.670, 10.098], loss: 0.001946, mae: 0.044622, mean_q: 1.244129
 67944/100000: episode: 1101, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.695, mean reward: 0.577 [0.498, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.108, 10.098], loss: 0.001778, mae: 0.044204, mean_q: 1.241994
 68044/100000: episode: 1102, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.253, mean reward: 0.583 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.474, 10.323], loss: 0.001610, mae: 0.043613, mean_q: 1.234075
 68144/100000: episode: 1103, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 60.759, mean reward: 0.608 [0.504, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.314, 10.098], loss: 0.001845, mae: 0.044975, mean_q: 1.228499
 68244/100000: episode: 1104, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.936, mean reward: 0.589 [0.520, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.309, 10.171], loss: 0.001857, mae: 0.044709, mean_q: 1.223575
 68344/100000: episode: 1105, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 57.758, mean reward: 0.578 [0.500, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.781, 10.098], loss: 0.001569, mae: 0.042361, mean_q: 1.227558
 68444/100000: episode: 1106, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.174, mean reward: 0.582 [0.509, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.554, 10.098], loss: 0.001510, mae: 0.041591, mean_q: 1.215634
 68544/100000: episode: 1107, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.319, mean reward: 0.573 [0.500, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.873, 10.103], loss: 0.001703, mae: 0.042241, mean_q: 1.215116
 68644/100000: episode: 1108, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 61.028, mean reward: 0.610 [0.517, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.541, 10.098], loss: 0.001601, mae: 0.041334, mean_q: 1.217791
 68744/100000: episode: 1109, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.727, mean reward: 0.567 [0.507, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.060, 10.122], loss: 0.001703, mae: 0.042688, mean_q: 1.213349
 68844/100000: episode: 1110, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.282, mean reward: 0.583 [0.515, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.737, 10.098], loss: 0.001783, mae: 0.043432, mean_q: 1.204295
 68944/100000: episode: 1111, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.156, mean reward: 0.572 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.553, 10.123], loss: 0.001539, mae: 0.041419, mean_q: 1.201508
 69044/100000: episode: 1112, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.430, mean reward: 0.584 [0.508, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.209, 10.098], loss: 0.001715, mae: 0.042855, mean_q: 1.194232
 69144/100000: episode: 1113, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 59.181, mean reward: 0.592 [0.509, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.878, 10.098], loss: 0.001634, mae: 0.043126, mean_q: 1.188831
 69244/100000: episode: 1114, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 62.021, mean reward: 0.620 [0.525, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.318, 10.469], loss: 0.001636, mae: 0.043022, mean_q: 1.185210
 69344/100000: episode: 1115, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.324, mean reward: 0.583 [0.502, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.406, 10.181], loss: 0.001604, mae: 0.042748, mean_q: 1.183457
 69444/100000: episode: 1116, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 62.471, mean reward: 0.625 [0.522, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.190, 10.297], loss: 0.001560, mae: 0.042404, mean_q: 1.178376
 69544/100000: episode: 1117, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 57.452, mean reward: 0.575 [0.508, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.206, 10.154], loss: 0.001505, mae: 0.041501, mean_q: 1.177728
 69644/100000: episode: 1118, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 63.550, mean reward: 0.636 [0.504, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.998, 10.098], loss: 0.001565, mae: 0.042356, mean_q: 1.171286
 69744/100000: episode: 1119, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.068, mean reward: 0.581 [0.503, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.762, 10.172], loss: 0.001378, mae: 0.039472, mean_q: 1.165630
 69844/100000: episode: 1120, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 56.171, mean reward: 0.562 [0.498, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.432, 10.188], loss: 0.001585, mae: 0.042373, mean_q: 1.165977
 69944/100000: episode: 1121, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.918, mean reward: 0.579 [0.501, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.621, 10.188], loss: 0.001374, mae: 0.040979, mean_q: 1.160756
 70044/100000: episode: 1122, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.803, mean reward: 0.578 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.156, 10.139], loss: 0.001486, mae: 0.041275, mean_q: 1.162357
 70144/100000: episode: 1123, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 59.217, mean reward: 0.592 [0.505, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.212, 10.291], loss: 0.001484, mae: 0.042116, mean_q: 1.161995
 70244/100000: episode: 1124, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 59.418, mean reward: 0.594 [0.510, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.170, 10.098], loss: 0.001435, mae: 0.040770, mean_q: 1.157713
 70344/100000: episode: 1125, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.240, mean reward: 0.592 [0.501, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.630, 10.098], loss: 0.001437, mae: 0.041188, mean_q: 1.160817
 70444/100000: episode: 1126, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.127, mean reward: 0.591 [0.509, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.715, 10.098], loss: 0.001439, mae: 0.040422, mean_q: 1.159751
 70544/100000: episode: 1127, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 60.316, mean reward: 0.603 [0.515, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.213, 10.098], loss: 0.001519, mae: 0.042021, mean_q: 1.162255
 70644/100000: episode: 1128, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.654, mean reward: 0.587 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.728, 10.098], loss: 0.001414, mae: 0.040493, mean_q: 1.160416
 70744/100000: episode: 1129, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.161, mean reward: 0.592 [0.499, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.970, 10.143], loss: 0.001403, mae: 0.040665, mean_q: 1.162435
 70844/100000: episode: 1130, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 59.352, mean reward: 0.594 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.038, 10.293], loss: 0.001637, mae: 0.043004, mean_q: 1.162846
 70944/100000: episode: 1131, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 61.311, mean reward: 0.613 [0.498, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.683, 10.323], loss: 0.001362, mae: 0.039792, mean_q: 1.163781
 71044/100000: episode: 1132, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 58.440, mean reward: 0.584 [0.509, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.098], loss: 0.001435, mae: 0.040774, mean_q: 1.162371
 71144/100000: episode: 1133, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 59.946, mean reward: 0.599 [0.507, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.750, 10.256], loss: 0.001507, mae: 0.041694, mean_q: 1.165815
 71244/100000: episode: 1134, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.559, mean reward: 0.576 [0.501, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.203], loss: 0.001461, mae: 0.040882, mean_q: 1.165316
 71344/100000: episode: 1135, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.454, mean reward: 0.585 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.217, 10.104], loss: 0.001531, mae: 0.041361, mean_q: 1.162860
 71444/100000: episode: 1136, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.120, mean reward: 0.581 [0.499, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.245, 10.098], loss: 0.001399, mae: 0.040769, mean_q: 1.162727
 71544/100000: episode: 1137, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.006, mean reward: 0.590 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.337, 10.098], loss: 0.001445, mae: 0.041285, mean_q: 1.162972
 71644/100000: episode: 1138, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.042, mean reward: 0.600 [0.507, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.914, 10.191], loss: 0.001277, mae: 0.039082, mean_q: 1.163490
 71744/100000: episode: 1139, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.478, mean reward: 0.605 [0.508, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.208, 10.144], loss: 0.001396, mae: 0.040256, mean_q: 1.164738
 71844/100000: episode: 1140, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 60.708, mean reward: 0.607 [0.500, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.750, 10.098], loss: 0.001421, mae: 0.040927, mean_q: 1.162239
 71944/100000: episode: 1141, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.162, mean reward: 0.592 [0.505, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.581, 10.363], loss: 0.001481, mae: 0.041942, mean_q: 1.163533
 72044/100000: episode: 1142, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 57.960, mean reward: 0.580 [0.507, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.282, 10.130], loss: 0.001393, mae: 0.040673, mean_q: 1.160422
 72144/100000: episode: 1143, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.594, mean reward: 0.576 [0.500, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.504, 10.098], loss: 0.001451, mae: 0.041579, mean_q: 1.164378
 72244/100000: episode: 1144, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 57.988, mean reward: 0.580 [0.501, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.172, 10.280], loss: 0.001399, mae: 0.040813, mean_q: 1.167214
 72344/100000: episode: 1145, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.688, mean reward: 0.597 [0.523, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.058, 10.098], loss: 0.001474, mae: 0.041339, mean_q: 1.165842
 72444/100000: episode: 1146, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 58.104, mean reward: 0.581 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.228, 10.098], loss: 0.001393, mae: 0.040167, mean_q: 1.163259
 72544/100000: episode: 1147, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.965, mean reward: 0.600 [0.514, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.582, 10.238], loss: 0.001527, mae: 0.042524, mean_q: 1.165289
 72644/100000: episode: 1148, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.946, mean reward: 0.609 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.027, 10.314], loss: 0.001459, mae: 0.041575, mean_q: 1.164427
 72744/100000: episode: 1149, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 60.618, mean reward: 0.606 [0.514, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.072, 10.098], loss: 0.001409, mae: 0.041132, mean_q: 1.168702
 72844/100000: episode: 1150, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.315, mean reward: 0.593 [0.505, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.393, 10.098], loss: 0.001364, mae: 0.040406, mean_q: 1.169303
 72944/100000: episode: 1151, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.132, mean reward: 0.591 [0.509, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.767, 10.271], loss: 0.001452, mae: 0.041340, mean_q: 1.166209
 73044/100000: episode: 1152, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.268, mean reward: 0.583 [0.504, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.094, 10.098], loss: 0.001408, mae: 0.040939, mean_q: 1.169351
 73144/100000: episode: 1153, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.113, mean reward: 0.591 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.473, 10.277], loss: 0.001409, mae: 0.040706, mean_q: 1.168610
 73244/100000: episode: 1154, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 65.465, mean reward: 0.655 [0.511, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.719, 10.420], loss: 0.001468, mae: 0.041147, mean_q: 1.172750
 73344/100000: episode: 1155, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 60.748, mean reward: 0.607 [0.524, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.210, 10.098], loss: 0.001470, mae: 0.041982, mean_q: 1.172535
 73444/100000: episode: 1156, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.912, mean reward: 0.579 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.618, 10.358], loss: 0.001408, mae: 0.040747, mean_q: 1.169778
 73544/100000: episode: 1157, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.025, mean reward: 0.580 [0.497, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.587, 10.190], loss: 0.001524, mae: 0.042320, mean_q: 1.171736
 73644/100000: episode: 1158, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 61.354, mean reward: 0.614 [0.516, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.301, 10.256], loss: 0.001455, mae: 0.041500, mean_q: 1.173412
 73744/100000: episode: 1159, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.216, mean reward: 0.592 [0.508, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.396, 10.356], loss: 0.001388, mae: 0.041184, mean_q: 1.173445
 73844/100000: episode: 1160, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.146, mean reward: 0.571 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.766, 10.196], loss: 0.001446, mae: 0.041616, mean_q: 1.173029
 73944/100000: episode: 1161, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 56.468, mean reward: 0.565 [0.504, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.129, 10.116], loss: 0.001403, mae: 0.041230, mean_q: 1.171742
 74044/100000: episode: 1162, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.779, mean reward: 0.578 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.612, 10.098], loss: 0.001505, mae: 0.042250, mean_q: 1.172453
 74144/100000: episode: 1163, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 62.507, mean reward: 0.625 [0.520, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.099, 10.098], loss: 0.001398, mae: 0.040339, mean_q: 1.173126
 74244/100000: episode: 1164, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 60.292, mean reward: 0.603 [0.507, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.490, 10.098], loss: 0.001374, mae: 0.040866, mean_q: 1.171347
 74344/100000: episode: 1165, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.469, mean reward: 0.595 [0.507, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.481, 10.098], loss: 0.001371, mae: 0.040500, mean_q: 1.173973
 74444/100000: episode: 1166, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.435, mean reward: 0.584 [0.503, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.694, 10.098], loss: 0.001251, mae: 0.039256, mean_q: 1.170162
 74544/100000: episode: 1167, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 57.496, mean reward: 0.575 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.724, 10.130], loss: 0.001379, mae: 0.040730, mean_q: 1.171697
 74644/100000: episode: 1168, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.851, mean reward: 0.579 [0.512, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.610, 10.098], loss: 0.001430, mae: 0.041253, mean_q: 1.170049
 74744/100000: episode: 1169, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.268, mean reward: 0.573 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.336, 10.192], loss: 0.001497, mae: 0.042781, mean_q: 1.173427
[Info] 1-TH LEVEL FOUND: 1.4338427782058716, Considering 10/90 traces
 74844/100000: episode: 1170, duration: 4.654s, episode steps: 100, steps per second: 21, episode reward: 56.075, mean reward: 0.561 [0.506, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.295, 10.157], loss: 0.001353, mae: 0.040645, mean_q: 1.168433
 74880/100000: episode: 1171, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 23.725, mean reward: 0.659 [0.616, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.197, 10.361], loss: 0.001655, mae: 0.045375, mean_q: 1.170463
 74907/100000: episode: 1172, duration: 0.150s, episode steps: 27, steps per second: 179, episode reward: 19.311, mean reward: 0.715 [0.617, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.818, 10.209], loss: 0.001503, mae: 0.042584, mean_q: 1.166124
 74942/100000: episode: 1173, duration: 0.168s, episode steps: 35, steps per second: 208, episode reward: 20.601, mean reward: 0.589 [0.511, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.084, 10.100], loss: 0.001295, mae: 0.039281, mean_q: 1.164415
 74959/100000: episode: 1174, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 13.394, mean reward: 0.788 [0.666, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.374, 10.100], loss: 0.001522, mae: 0.042683, mean_q: 1.166596
 74995/100000: episode: 1175, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 27.781, mean reward: 0.772 [0.653, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.174, 10.526], loss: 0.001352, mae: 0.040692, mean_q: 1.175047
 75033/100000: episode: 1176, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 23.395, mean reward: 0.616 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.877, 10.120], loss: 0.001354, mae: 0.039857, mean_q: 1.168406
 75068/100000: episode: 1177, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 23.855, mean reward: 0.682 [0.577, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.439, 10.333], loss: 0.001431, mae: 0.041311, mean_q: 1.177879
 75104/100000: episode: 1178, duration: 0.178s, episode steps: 36, steps per second: 202, episode reward: 23.097, mean reward: 0.642 [0.556, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.294, 10.305], loss: 0.001462, mae: 0.041842, mean_q: 1.175138
 75145/100000: episode: 1179, duration: 0.195s, episode steps: 41, steps per second: 210, episode reward: 30.263, mean reward: 0.738 [0.633, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.214, 10.100], loss: 0.001413, mae: 0.040600, mean_q: 1.174481
 75181/100000: episode: 1180, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 22.894, mean reward: 0.636 [0.559, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.813, 10.170], loss: 0.001388, mae: 0.040869, mean_q: 1.178493
 75216/100000: episode: 1181, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 21.324, mean reward: 0.609 [0.542, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.258, 10.189], loss: 0.001505, mae: 0.042181, mean_q: 1.181691
 75243/100000: episode: 1182, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 17.996, mean reward: 0.667 [0.544, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.475, 10.259], loss: 0.001391, mae: 0.041181, mean_q: 1.182356
 75270/100000: episode: 1183, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 17.692, mean reward: 0.655 [0.569, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.182, 10.243], loss: 0.001462, mae: 0.041483, mean_q: 1.177517
 75297/100000: episode: 1184, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 19.511, mean reward: 0.723 [0.652, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.486, 10.361], loss: 0.001654, mae: 0.043136, mean_q: 1.181924
 75322/100000: episode: 1185, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 16.544, mean reward: 0.662 [0.588, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.499, 10.339], loss: 0.001631, mae: 0.043839, mean_q: 1.182794
 75339/100000: episode: 1186, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 13.567, mean reward: 0.798 [0.724, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.418, 10.100], loss: 0.001494, mae: 0.041993, mean_q: 1.186090
 75364/100000: episode: 1187, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 14.886, mean reward: 0.595 [0.516, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.061, 10.154], loss: 0.001732, mae: 0.044162, mean_q: 1.180305
 75400/100000: episode: 1188, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 24.123, mean reward: 0.670 [0.619, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.210, 10.403], loss: 0.001494, mae: 0.041155, mean_q: 1.182843
 75436/100000: episode: 1189, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 21.766, mean reward: 0.605 [0.549, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.035, 10.353], loss: 0.001596, mae: 0.042262, mean_q: 1.188496
 75477/100000: episode: 1190, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 30.206, mean reward: 0.737 [0.634, 0.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.378, 10.100], loss: 0.001581, mae: 0.042660, mean_q: 1.182007
 75515/100000: episode: 1191, duration: 0.213s, episode steps: 38, steps per second: 178, episode reward: 28.231, mean reward: 0.743 [0.623, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.113, 10.427], loss: 0.001568, mae: 0.042851, mean_q: 1.189969
 75532/100000: episode: 1192, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 11.950, mean reward: 0.703 [0.637, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.323, 10.100], loss: 0.001452, mae: 0.041938, mean_q: 1.184296
 75557/100000: episode: 1193, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 15.856, mean reward: 0.634 [0.522, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.000, 10.416], loss: 0.001607, mae: 0.043632, mean_q: 1.186348
 75593/100000: episode: 1194, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 24.631, mean reward: 0.684 [0.572, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.508, 10.461], loss: 0.001388, mae: 0.041098, mean_q: 1.191116
 75610/100000: episode: 1195, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 11.376, mean reward: 0.669 [0.603, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.403, 10.100], loss: 0.001359, mae: 0.039397, mean_q: 1.189214
 75637/100000: episode: 1196, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 16.899, mean reward: 0.626 [0.510, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.441, 10.220], loss: 0.001512, mae: 0.042159, mean_q: 1.184194
 75673/100000: episode: 1197, duration: 0.213s, episode steps: 36, steps per second: 169, episode reward: 21.842, mean reward: 0.607 [0.516, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.298, 10.100], loss: 0.001509, mae: 0.041898, mean_q: 1.189098
 75698/100000: episode: 1198, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 15.327, mean reward: 0.613 [0.551, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.600, 10.182], loss: 0.001512, mae: 0.042129, mean_q: 1.191061
 75725/100000: episode: 1199, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 16.237, mean reward: 0.601 [0.545, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.126, 10.308], loss: 0.001449, mae: 0.041108, mean_q: 1.188600
 75750/100000: episode: 1200, duration: 0.150s, episode steps: 25, steps per second: 166, episode reward: 16.282, mean reward: 0.651 [0.572, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.826, 10.517], loss: 0.001607, mae: 0.043994, mean_q: 1.187184
 75767/100000: episode: 1201, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 12.592, mean reward: 0.741 [0.663, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.743, 10.100], loss: 0.001609, mae: 0.042468, mean_q: 1.195718
 75806/100000: episode: 1202, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 24.156, mean reward: 0.619 [0.526, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.793, 10.100], loss: 0.001567, mae: 0.042768, mean_q: 1.189810
 75831/100000: episode: 1203, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 16.212, mean reward: 0.648 [0.542, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.557, 10.224], loss: 0.001813, mae: 0.045650, mean_q: 1.187434
 75863/100000: episode: 1204, duration: 0.156s, episode steps: 32, steps per second: 206, episode reward: 22.922, mean reward: 0.716 [0.631, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.579, 10.474], loss: 0.001656, mae: 0.043973, mean_q: 1.192693
 75880/100000: episode: 1205, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 13.695, mean reward: 0.806 [0.694, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.211, 10.100], loss: 0.001422, mae: 0.040810, mean_q: 1.191560
 75915/100000: episode: 1206, duration: 0.174s, episode steps: 35, steps per second: 202, episode reward: 22.639, mean reward: 0.647 [0.544, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.446, 10.237], loss: 0.001635, mae: 0.043691, mean_q: 1.195621
 75954/100000: episode: 1207, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 23.255, mean reward: 0.596 [0.517, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.980, 10.166], loss: 0.001642, mae: 0.042423, mean_q: 1.190972
 75992/100000: episode: 1208, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 25.411, mean reward: 0.669 [0.548, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.384, 10.440], loss: 0.001697, mae: 0.045036, mean_q: 1.190867
 76031/100000: episode: 1209, duration: 0.192s, episode steps: 39, steps per second: 203, episode reward: 24.344, mean reward: 0.624 [0.518, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.543, 10.145], loss: 0.001574, mae: 0.042667, mean_q: 1.193445
 76056/100000: episode: 1210, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 16.230, mean reward: 0.649 [0.517, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.488, 10.201], loss: 0.001680, mae: 0.043977, mean_q: 1.188212
 76095/100000: episode: 1211, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 22.834, mean reward: 0.585 [0.512, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.295, 10.103], loss: 0.001658, mae: 0.044213, mean_q: 1.194534
 76120/100000: episode: 1212, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 16.839, mean reward: 0.674 [0.623, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.095, 10.459], loss: 0.001773, mae: 0.044796, mean_q: 1.189838
 76155/100000: episode: 1213, duration: 0.197s, episode steps: 35, steps per second: 177, episode reward: 24.561, mean reward: 0.702 [0.597, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.232, 10.418], loss: 0.001731, mae: 0.045662, mean_q: 1.193632
 76180/100000: episode: 1214, duration: 0.125s, episode steps: 25, steps per second: 199, episode reward: 15.015, mean reward: 0.601 [0.503, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.221], loss: 0.001622, mae: 0.042577, mean_q: 1.202667
 76219/100000: episode: 1215, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 24.082, mean reward: 0.617 [0.541, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.080, 10.337], loss: 0.001698, mae: 0.043470, mean_q: 1.199677
 76251/100000: episode: 1216, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 20.079, mean reward: 0.627 [0.564, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.151, 10.209], loss: 0.001723, mae: 0.044759, mean_q: 1.193388
 76289/100000: episode: 1217, duration: 0.185s, episode steps: 38, steps per second: 205, episode reward: 24.411, mean reward: 0.642 [0.558, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.444, 10.324], loss: 0.001708, mae: 0.044243, mean_q: 1.201700
 76321/100000: episode: 1218, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 23.565, mean reward: 0.736 [0.657, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.125, 10.575], loss: 0.001610, mae: 0.042823, mean_q: 1.201091
 76362/100000: episode: 1219, duration: 0.230s, episode steps: 41, steps per second: 178, episode reward: 25.404, mean reward: 0.620 [0.508, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.771, 10.100], loss: 0.001394, mae: 0.040523, mean_q: 1.202965
 76389/100000: episode: 1220, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 17.220, mean reward: 0.638 [0.548, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.268], loss: 0.001443, mae: 0.041593, mean_q: 1.203068
 76428/100000: episode: 1221, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 24.897, mean reward: 0.638 [0.524, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.730, 10.196], loss: 0.001736, mae: 0.043750, mean_q: 1.201635
 76463/100000: episode: 1222, duration: 0.209s, episode steps: 35, steps per second: 167, episode reward: 22.983, mean reward: 0.657 [0.614, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.560, 10.335], loss: 0.001669, mae: 0.043108, mean_q: 1.199446
 76498/100000: episode: 1223, duration: 0.192s, episode steps: 35, steps per second: 183, episode reward: 21.605, mean reward: 0.617 [0.537, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.578, 10.100], loss: 0.001496, mae: 0.041227, mean_q: 1.210035
 76537/100000: episode: 1224, duration: 0.216s, episode steps: 39, steps per second: 181, episode reward: 25.656, mean reward: 0.658 [0.556, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.079, 10.305], loss: 0.001868, mae: 0.046345, mean_q: 1.205485
 76569/100000: episode: 1225, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 22.039, mean reward: 0.689 [0.624, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.266, 10.332], loss: 0.001734, mae: 0.044446, mean_q: 1.207149
 76596/100000: episode: 1226, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 17.021, mean reward: 0.630 [0.521, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.820, 10.100], loss: 0.001821, mae: 0.045433, mean_q: 1.203433
 76623/100000: episode: 1227, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 17.480, mean reward: 0.647 [0.524, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.919, 10.208], loss: 0.001464, mae: 0.040373, mean_q: 1.212819
 76655/100000: episode: 1228, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 20.361, mean reward: 0.636 [0.555, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.348, 10.255], loss: 0.001422, mae: 0.040483, mean_q: 1.210101
 76691/100000: episode: 1229, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 23.241, mean reward: 0.646 [0.543, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.517, 10.224], loss: 0.001342, mae: 0.038915, mean_q: 1.203359
 76718/100000: episode: 1230, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 16.045, mean reward: 0.594 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.139, 10.100], loss: 0.001522, mae: 0.041996, mean_q: 1.204035
 76750/100000: episode: 1231, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 20.560, mean reward: 0.643 [0.584, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.712, 10.326], loss: 0.001685, mae: 0.043373, mean_q: 1.205990
 76791/100000: episode: 1232, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 29.047, mean reward: 0.708 [0.593, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.260, 10.100], loss: 0.001424, mae: 0.040673, mean_q: 1.209784
 76816/100000: episode: 1233, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 18.186, mean reward: 0.727 [0.657, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.335, 10.387], loss: 0.001495, mae: 0.042009, mean_q: 1.209731
 76841/100000: episode: 1234, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 15.898, mean reward: 0.636 [0.565, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.332, 10.255], loss: 0.001376, mae: 0.040002, mean_q: 1.206594
 76868/100000: episode: 1235, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 16.821, mean reward: 0.623 [0.500, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.162], loss: 0.001596, mae: 0.042125, mean_q: 1.202945
 76909/100000: episode: 1236, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 26.546, mean reward: 0.647 [0.509, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.838, 10.100], loss: 0.001505, mae: 0.041692, mean_q: 1.206402
 76944/100000: episode: 1237, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 23.676, mean reward: 0.676 [0.574, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.376, 10.484], loss: 0.001527, mae: 0.041988, mean_q: 1.208474
 76979/100000: episode: 1238, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 24.660, mean reward: 0.705 [0.543, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.757, 10.319], loss: 0.001678, mae: 0.044532, mean_q: 1.220300
 77018/100000: episode: 1239, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 23.755, mean reward: 0.609 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.662, 10.121], loss: 0.001814, mae: 0.044646, mean_q: 1.217356
 77045/100000: episode: 1240, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 18.372, mean reward: 0.680 [0.599, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.316], loss: 0.001827, mae: 0.045357, mean_q: 1.214936
 77080/100000: episode: 1241, duration: 0.174s, episode steps: 35, steps per second: 202, episode reward: 22.006, mean reward: 0.629 [0.545, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.547, 10.335], loss: 0.001566, mae: 0.042235, mean_q: 1.221769
 77097/100000: episode: 1242, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 14.136, mean reward: 0.832 [0.724, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.584, 10.100], loss: 0.001596, mae: 0.041852, mean_q: 1.216059
 77122/100000: episode: 1243, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 16.910, mean reward: 0.676 [0.577, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.662, 10.336], loss: 0.001727, mae: 0.043606, mean_q: 1.204031
 77160/100000: episode: 1244, duration: 0.224s, episode steps: 38, steps per second: 170, episode reward: 25.565, mean reward: 0.673 [0.630, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.478, 10.437], loss: 0.001620, mae: 0.042780, mean_q: 1.217174
 77201/100000: episode: 1245, duration: 0.215s, episode steps: 41, steps per second: 191, episode reward: 27.589, mean reward: 0.673 [0.596, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.147, 10.100], loss: 0.001427, mae: 0.040509, mean_q: 1.222672
 77218/100000: episode: 1246, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 12.876, mean reward: 0.757 [0.677, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.404, 10.100], loss: 0.001633, mae: 0.043415, mean_q: 1.223836
 77235/100000: episode: 1247, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 13.009, mean reward: 0.765 [0.649, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.928, 10.100], loss: 0.001563, mae: 0.041952, mean_q: 1.220799
 77252/100000: episode: 1248, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 12.009, mean reward: 0.706 [0.644, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.778, 10.100], loss: 0.001529, mae: 0.040686, mean_q: 1.226474
 77277/100000: episode: 1249, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 16.125, mean reward: 0.645 [0.513, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.149], loss: 0.001453, mae: 0.040821, mean_q: 1.220077
 77315/100000: episode: 1250, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 29.856, mean reward: 0.786 [0.611, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.818, 10.618], loss: 0.001675, mae: 0.044506, mean_q: 1.229955
 77342/100000: episode: 1251, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 16.552, mean reward: 0.613 [0.572, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.888, 10.290], loss: 0.001897, mae: 0.043951, mean_q: 1.219578
 77378/100000: episode: 1252, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 25.094, mean reward: 0.697 [0.527, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.270, 10.238], loss: 0.001492, mae: 0.042664, mean_q: 1.230000
 77417/100000: episode: 1253, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 24.873, mean reward: 0.638 [0.522, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.587, 10.249], loss: 0.001573, mae: 0.042087, mean_q: 1.223780
 77452/100000: episode: 1254, duration: 0.177s, episode steps: 35, steps per second: 197, episode reward: 21.620, mean reward: 0.618 [0.506, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.548, 10.160], loss: 0.001536, mae: 0.042069, mean_q: 1.226188
 77479/100000: episode: 1255, duration: 0.164s, episode steps: 27, steps per second: 164, episode reward: 16.411, mean reward: 0.608 [0.515, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.402, 10.137], loss: 0.001500, mae: 0.040668, mean_q: 1.229218
 77504/100000: episode: 1256, duration: 0.120s, episode steps: 25, steps per second: 208, episode reward: 16.363, mean reward: 0.655 [0.589, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.984, 10.399], loss: 0.001530, mae: 0.041232, mean_q: 1.222978
 77536/100000: episode: 1257, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 22.597, mean reward: 0.706 [0.615, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.869, 10.500], loss: 0.001434, mae: 0.040666, mean_q: 1.233670
 77575/100000: episode: 1258, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 26.609, mean reward: 0.682 [0.596, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.072, 10.464], loss: 0.001614, mae: 0.043069, mean_q: 1.231671
 77610/100000: episode: 1259, duration: 0.175s, episode steps: 35, steps per second: 199, episode reward: 22.882, mean reward: 0.654 [0.585, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.696, 10.412], loss: 0.001464, mae: 0.040004, mean_q: 1.224387
[Info] 2-TH LEVEL FOUND: 1.60422682762146, Considering 10/90 traces
 77627/100000: episode: 1260, duration: 4.293s, episode steps: 17, steps per second: 4, episode reward: 11.618, mean reward: 0.683 [0.643, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.280, 10.100], loss: 0.001419, mae: 0.039313, mean_q: 1.238935
 77652/100000: episode: 1261, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 19.917, mean reward: 0.797 [0.691, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.734, 10.100], loss: 0.001811, mae: 0.044315, mean_q: 1.216791
 77677/100000: episode: 1262, duration: 0.142s, episode steps: 25, steps per second: 177, episode reward: 17.384, mean reward: 0.695 [0.609, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.502, 10.100], loss: 0.001704, mae: 0.045375, mean_q: 1.239487
 77702/100000: episode: 1263, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 18.256, mean reward: 0.730 [0.666, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.405], loss: 0.001481, mae: 0.041220, mean_q: 1.239610
 77728/100000: episode: 1264, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 18.760, mean reward: 0.722 [0.635, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.181, 10.450], loss: 0.001776, mae: 0.045681, mean_q: 1.228412
 77736/100000: episode: 1265, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 6.412, mean reward: 0.801 [0.758, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.452, 10.100], loss: 0.001046, mae: 0.034163, mean_q: 1.247142
 77744/100000: episode: 1266, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 6.933, mean reward: 0.867 [0.782, 0.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.432, 10.100], loss: 0.001532, mae: 0.042022, mean_q: 1.235995
 77765/100000: episode: 1267, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 16.271, mean reward: 0.775 [0.709, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.914, 10.522], loss: 0.001581, mae: 0.041428, mean_q: 1.236944
 77775/100000: episode: 1268, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 7.993, mean reward: 0.799 [0.749, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.560, 10.100], loss: 0.001621, mae: 0.044102, mean_q: 1.235018
 77786/100000: episode: 1269, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 8.997, mean reward: 0.818 [0.788, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.226, 10.655], loss: 0.001486, mae: 0.041641, mean_q: 1.230971
 77812/100000: episode: 1270, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 20.280, mean reward: 0.780 [0.725, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.109, 10.550], loss: 0.001431, mae: 0.040887, mean_q: 1.236086
 77837/100000: episode: 1271, duration: 0.119s, episode steps: 25, steps per second: 209, episode reward: 19.994, mean reward: 0.800 [0.715, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.620, 10.495], loss: 0.001514, mae: 0.041632, mean_q: 1.252874
 77853/100000: episode: 1272, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 12.052, mean reward: 0.753 [0.673, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.537], loss: 0.001353, mae: 0.040389, mean_q: 1.250260
 77878/100000: episode: 1273, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 20.571, mean reward: 0.823 [0.743, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.382, 10.506], loss: 0.001597, mae: 0.042779, mean_q: 1.233464
 77904/100000: episode: 1274, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 20.281, mean reward: 0.780 [0.646, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.664, 10.442], loss: 0.001515, mae: 0.042283, mean_q: 1.244998
 77924/100000: episode: 1275, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 14.910, mean reward: 0.745 [0.648, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.450, 10.100], loss: 0.001575, mae: 0.043532, mean_q: 1.255941
 77944/100000: episode: 1276, duration: 0.098s, episode steps: 20, steps per second: 203, episode reward: 13.973, mean reward: 0.699 [0.633, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.278, 10.100], loss: 0.001540, mae: 0.041764, mean_q: 1.242825
 77955/100000: episode: 1277, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 8.971, mean reward: 0.816 [0.786, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.612], loss: 0.001606, mae: 0.041745, mean_q: 1.254257
 77975/100000: episode: 1278, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 14.565, mean reward: 0.728 [0.605, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.130, 10.100], loss: 0.001528, mae: 0.040998, mean_q: 1.255207
 77996/100000: episode: 1279, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 15.845, mean reward: 0.755 [0.662, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.049, 10.503], loss: 0.001423, mae: 0.041622, mean_q: 1.256152
 78017/100000: episode: 1280, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 18.345, mean reward: 0.874 [0.811, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.301, 10.681], loss: 0.001571, mae: 0.044047, mean_q: 1.258580
 78042/100000: episode: 1281, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 17.133, mean reward: 0.685 [0.518, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.086, 10.225], loss: 0.001796, mae: 0.044701, mean_q: 1.266103
 78067/100000: episode: 1282, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 18.448, mean reward: 0.738 [0.663, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-2.041, 10.449], loss: 0.001508, mae: 0.041182, mean_q: 1.267201
 78077/100000: episode: 1283, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 8.283, mean reward: 0.828 [0.779, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.401, 10.100], loss: 0.001555, mae: 0.041738, mean_q: 1.270928
 78097/100000: episode: 1284, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 17.182, mean reward: 0.859 [0.800, 0.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.551, 10.100], loss: 0.001548, mae: 0.042067, mean_q: 1.260181
 78122/100000: episode: 1285, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 20.070, mean reward: 0.803 [0.750, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.331, 10.492], loss: 0.001634, mae: 0.042316, mean_q: 1.257210
 78133/100000: episode: 1286, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 8.378, mean reward: 0.762 [0.707, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.554], loss: 0.001621, mae: 0.043508, mean_q: 1.262134
 78159/100000: episode: 1287, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 20.004, mean reward: 0.769 [0.727, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.728, 10.526], loss: 0.001651, mae: 0.043260, mean_q: 1.265048
 78179/100000: episode: 1288, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 14.356, mean reward: 0.718 [0.624, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.259, 10.100], loss: 0.001186, mae: 0.037429, mean_q: 1.262862
 78204/100000: episode: 1289, duration: 0.152s, episode steps: 25, steps per second: 165, episode reward: 19.070, mean reward: 0.763 [0.668, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.744, 10.641], loss: 0.001618, mae: 0.041870, mean_q: 1.262095
 78220/100000: episode: 1290, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 11.308, mean reward: 0.707 [0.617, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-1.014, 10.361], loss: 0.001521, mae: 0.042339, mean_q: 1.273345
 78245/100000: episode: 1291, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 19.364, mean reward: 0.775 [0.692, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.283, 10.379], loss: 0.001581, mae: 0.042448, mean_q: 1.266567
 78270/100000: episode: 1292, duration: 0.119s, episode steps: 25, steps per second: 210, episode reward: 19.007, mean reward: 0.760 [0.697, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.537], loss: 0.001655, mae: 0.043650, mean_q: 1.253946
 78295/100000: episode: 1293, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 18.954, mean reward: 0.758 [0.676, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.787, 10.488], loss: 0.001684, mae: 0.044600, mean_q: 1.268105
 78305/100000: episode: 1294, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 8.598, mean reward: 0.860 [0.806, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.480, 10.100], loss: 0.001351, mae: 0.039993, mean_q: 1.266830
 78325/100000: episode: 1295, duration: 0.115s, episode steps: 20, steps per second: 173, episode reward: 15.779, mean reward: 0.789 [0.694, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.237, 10.100], loss: 0.001375, mae: 0.040112, mean_q: 1.267385
 78345/100000: episode: 1296, duration: 0.098s, episode steps: 20, steps per second: 205, episode reward: 15.570, mean reward: 0.778 [0.658, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.297, 10.100], loss: 0.001507, mae: 0.040267, mean_q: 1.280421
 78355/100000: episode: 1297, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 8.386, mean reward: 0.839 [0.785, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.706, 10.100], loss: 0.001710, mae: 0.042309, mean_q: 1.276079
 78380/100000: episode: 1298, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 20.052, mean reward: 0.802 [0.748, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.143, 10.674], loss: 0.001650, mae: 0.043134, mean_q: 1.263003
 78388/100000: episode: 1299, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 6.863, mean reward: 0.858 [0.803, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.673, 10.100], loss: 0.001430, mae: 0.041453, mean_q: 1.268752
 78413/100000: episode: 1300, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 19.291, mean reward: 0.772 [0.657, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.657, 10.100], loss: 0.001605, mae: 0.042087, mean_q: 1.278250
 78438/100000: episode: 1301, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 18.925, mean reward: 0.757 [0.615, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.181, 10.100], loss: 0.001439, mae: 0.041402, mean_q: 1.260495
 78463/100000: episode: 1302, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 19.119, mean reward: 0.765 [0.705, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.308, 10.483], loss: 0.001412, mae: 0.040181, mean_q: 1.265839
 78483/100000: episode: 1303, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 16.652, mean reward: 0.833 [0.717, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.285, 10.100], loss: 0.001687, mae: 0.042667, mean_q: 1.294816
 78504/100000: episode: 1304, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 17.096, mean reward: 0.814 [0.755, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.609], loss: 0.001619, mae: 0.041623, mean_q: 1.279213
 78530/100000: episode: 1305, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 19.689, mean reward: 0.757 [0.667, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.566], loss: 0.001406, mae: 0.040612, mean_q: 1.275697
 78550/100000: episode: 1306, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 15.094, mean reward: 0.755 [0.676, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.296, 10.100], loss: 0.001498, mae: 0.042446, mean_q: 1.290167
 78570/100000: episode: 1307, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 14.577, mean reward: 0.729 [0.552, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.861, 10.100], loss: 0.001406, mae: 0.039394, mean_q: 1.282623
 78590/100000: episode: 1308, duration: 0.100s, episode steps: 20, steps per second: 199, episode reward: 14.577, mean reward: 0.729 [0.609, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.192, 10.100], loss: 0.001457, mae: 0.041118, mean_q: 1.298560
 78610/100000: episode: 1309, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 14.708, mean reward: 0.735 [0.664, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.515, 10.100], loss: 0.001253, mae: 0.039021, mean_q: 1.284895
 78635/100000: episode: 1310, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 18.413, mean reward: 0.737 [0.671, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.777, 10.461], loss: 0.001511, mae: 0.042526, mean_q: 1.280236
 78660/100000: episode: 1311, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 18.694, mean reward: 0.748 [0.609, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.683, 10.329], loss: 0.001467, mae: 0.040419, mean_q: 1.284271
 78680/100000: episode: 1312, duration: 0.100s, episode steps: 20, steps per second: 199, episode reward: 15.411, mean reward: 0.771 [0.673, 0.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.296, 10.100], loss: 0.001296, mae: 0.039134, mean_q: 1.285258
 78688/100000: episode: 1313, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 6.354, mean reward: 0.794 [0.774, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.502, 10.100], loss: 0.001323, mae: 0.038929, mean_q: 1.286021
 78713/100000: episode: 1314, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 19.215, mean reward: 0.769 [0.684, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.254, 10.100], loss: 0.001555, mae: 0.042916, mean_q: 1.291412
 78724/100000: episode: 1315, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 9.037, mean reward: 0.822 [0.785, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.558], loss: 0.001783, mae: 0.045235, mean_q: 1.283871
 78732/100000: episode: 1316, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 7.086, mean reward: 0.886 [0.807, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.485, 10.100], loss: 0.001817, mae: 0.047924, mean_q: 1.281698
 78757/100000: episode: 1317, duration: 0.145s, episode steps: 25, steps per second: 173, episode reward: 17.779, mean reward: 0.711 [0.626, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.371, 10.375], loss: 0.001296, mae: 0.039748, mean_q: 1.295928
[Info] FALSIFICATION!
 78765/100000: episode: 1318, duration: 0.305s, episode steps: 8, steps per second: 26, episode reward: 7.228, mean reward: 0.903 [0.816, 1.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.486, 10.093], loss: 0.001248, mae: 0.037244, mean_q: 1.303344
 78790/100000: episode: 1319, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 18.088, mean reward: 0.724 [0.665, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.367, 10.389], loss: 0.001414, mae: 0.040906, mean_q: 1.316690
 78806/100000: episode: 1320, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 12.237, mean reward: 0.765 [0.724, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.527], loss: 0.001269, mae: 0.039207, mean_q: 1.305266
 78831/100000: episode: 1321, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 19.516, mean reward: 0.781 [0.734, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.335, 10.100], loss: 0.001321, mae: 0.040029, mean_q: 1.301074
 78856/100000: episode: 1322, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 18.006, mean reward: 0.720 [0.628, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.993, 10.100], loss: 0.001531, mae: 0.041754, mean_q: 1.302825
 78877/100000: episode: 1323, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 16.285, mean reward: 0.775 [0.678, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.779, 10.499], loss: 0.001423, mae: 0.041406, mean_q: 1.306092
 78903/100000: episode: 1324, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 18.340, mean reward: 0.705 [0.597, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.758, 10.306], loss: 0.001519, mae: 0.040697, mean_q: 1.309812
 78911/100000: episode: 1325, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 6.371, mean reward: 0.796 [0.751, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.437, 10.100], loss: 0.002121, mae: 0.050105, mean_q: 1.296127
 78931/100000: episode: 1326, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 15.220, mean reward: 0.761 [0.679, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.157, 10.100], loss: 0.001314, mae: 0.039541, mean_q: 1.311138
 78951/100000: episode: 1327, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 14.816, mean reward: 0.741 [0.685, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.372, 10.100], loss: 0.001531, mae: 0.042357, mean_q: 1.312903
 78959/100000: episode: 1328, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 6.343, mean reward: 0.793 [0.761, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.533, 10.100], loss: 0.001360, mae: 0.039293, mean_q: 1.322158
 78967/100000: episode: 1329, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 6.704, mean reward: 0.838 [0.814, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.592, 10.100], loss: 0.001091, mae: 0.037502, mean_q: 1.320510
 78975/100000: episode: 1330, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 6.745, mean reward: 0.843 [0.786, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.602, 10.100], loss: 0.001267, mae: 0.038090, mean_q: 1.319743
 78986/100000: episode: 1331, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 8.662, mean reward: 0.787 [0.744, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.483], loss: 0.001254, mae: 0.038034, mean_q: 1.301195
 79011/100000: episode: 1332, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 19.714, mean reward: 0.789 [0.734, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.638, 10.390], loss: 0.001232, mae: 0.038301, mean_q: 1.306488
 79027/100000: episode: 1333, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 10.642, mean reward: 0.665 [0.602, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.322], loss: 0.001356, mae: 0.039185, mean_q: 1.297121
 79035/100000: episode: 1334, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 6.360, mean reward: 0.795 [0.753, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.486, 10.100], loss: 0.001269, mae: 0.037900, mean_q: 1.299347
 79060/100000: episode: 1335, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 19.480, mean reward: 0.779 [0.632, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.753, 10.358], loss: 0.001376, mae: 0.040766, mean_q: 1.315010
 79080/100000: episode: 1336, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 15.047, mean reward: 0.752 [0.712, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.890, 10.100], loss: 0.001533, mae: 0.043104, mean_q: 1.302750
 79096/100000: episode: 1337, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 12.003, mean reward: 0.750 [0.701, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.489], loss: 0.001874, mae: 0.043848, mean_q: 1.317574
 79104/100000: episode: 1338, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 6.175, mean reward: 0.772 [0.726, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.602, 10.100], loss: 0.001122, mae: 0.037182, mean_q: 1.320583
 79129/100000: episode: 1339, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 20.218, mean reward: 0.809 [0.650, 0.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.728, 10.537], loss: 0.001403, mae: 0.040568, mean_q: 1.321802
 79150/100000: episode: 1340, duration: 0.134s, episode steps: 21, steps per second: 157, episode reward: 17.301, mean reward: 0.824 [0.767, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.600], loss: 0.001265, mae: 0.039342, mean_q: 1.321717
 79170/100000: episode: 1341, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 14.900, mean reward: 0.745 [0.682, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.420, 10.100], loss: 0.001708, mae: 0.043113, mean_q: 1.327140
 79178/100000: episode: 1342, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 6.813, mean reward: 0.852 [0.761, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.452, 10.100], loss: 0.001414, mae: 0.040383, mean_q: 1.317509
 79203/100000: episode: 1343, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 17.731, mean reward: 0.709 [0.650, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.430, 10.454], loss: 0.001452, mae: 0.040215, mean_q: 1.325939
 79219/100000: episode: 1344, duration: 0.091s, episode steps: 16, steps per second: 177, episode reward: 11.370, mean reward: 0.711 [0.649, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.467], loss: 0.002122, mae: 0.044607, mean_q: 1.322265
 79244/100000: episode: 1345, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 17.744, mean reward: 0.710 [0.670, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.281, 10.464], loss: 0.001515, mae: 0.041639, mean_q: 1.318343
 79270/100000: episode: 1346, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 20.576, mean reward: 0.791 [0.733, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.590, 10.507], loss: 0.001393, mae: 0.040284, mean_q: 1.334160
 79295/100000: episode: 1347, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 19.166, mean reward: 0.767 [0.628, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.856, 10.360], loss: 0.001855, mae: 0.045953, mean_q: 1.316686
 79315/100000: episode: 1348, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 15.320, mean reward: 0.766 [0.631, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.275, 10.100], loss: 0.001552, mae: 0.042710, mean_q: 1.327260
 79336/100000: episode: 1349, duration: 0.101s, episode steps: 21, steps per second: 209, episode reward: 17.767, mean reward: 0.846 [0.781, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.336, 10.517], loss: 0.001370, mae: 0.040587, mean_q: 1.321884
[Info] Complete ISplit Iteration
[Info] Levels: [1.4338428, 1.6042268, 1.7174608]
[Info] Cond. Prob: [0.1, 0.1, 0.07]
[Info] Error Prob: 0.0007000000000000002

 79344/100000: episode: 1350, duration: 4.438s, episode steps: 8, steps per second: 2, episode reward: 6.401, mean reward: 0.800 [0.761, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.508, 10.100], loss: 0.001339, mae: 0.039512, mean_q: 1.323604
 79444/100000: episode: 1351, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 60.232, mean reward: 0.602 [0.525, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.061, 10.259], loss: 0.001455, mae: 0.040860, mean_q: 1.337949
 79544/100000: episode: 1352, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 56.987, mean reward: 0.570 [0.500, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.761, 10.123], loss: 0.001489, mae: 0.040831, mean_q: 1.334885
 79644/100000: episode: 1353, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.583, mean reward: 0.596 [0.502, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.944, 10.330], loss: 0.001418, mae: 0.040843, mean_q: 1.334124
 79744/100000: episode: 1354, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.979, mean reward: 0.590 [0.511, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.504, 10.098], loss: 0.001462, mae: 0.040567, mean_q: 1.334682
 79844/100000: episode: 1355, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 58.096, mean reward: 0.581 [0.508, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.534, 10.098], loss: 0.001500, mae: 0.041834, mean_q: 1.325902
 79944/100000: episode: 1356, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.204, mean reward: 0.572 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.457, 10.098], loss: 0.001416, mae: 0.040978, mean_q: 1.328007
 80044/100000: episode: 1357, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 59.077, mean reward: 0.591 [0.508, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.626, 10.293], loss: 0.001567, mae: 0.042640, mean_q: 1.319909
 80144/100000: episode: 1358, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.841, mean reward: 0.578 [0.500, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.254, 10.165], loss: 0.001534, mae: 0.041910, mean_q: 1.315431
 80244/100000: episode: 1359, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 57.845, mean reward: 0.578 [0.519, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.706, 10.206], loss: 0.001565, mae: 0.041285, mean_q: 1.321093
 80344/100000: episode: 1360, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.597, mean reward: 0.586 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.274, 10.107], loss: 0.001454, mae: 0.040948, mean_q: 1.316064
 80444/100000: episode: 1361, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.901, mean reward: 0.579 [0.512, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.967, 10.105], loss: 0.001658, mae: 0.043295, mean_q: 1.310531
 80544/100000: episode: 1362, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.072, mean reward: 0.591 [0.506, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.064, 10.098], loss: 0.001486, mae: 0.041117, mean_q: 1.307383
 80644/100000: episode: 1363, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 59.271, mean reward: 0.593 [0.516, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.953, 10.098], loss: 0.001582, mae: 0.042237, mean_q: 1.307347
 80744/100000: episode: 1364, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.002, mean reward: 0.570 [0.499, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.473, 10.136], loss: 0.001526, mae: 0.041664, mean_q: 1.303377
 80844/100000: episode: 1365, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.537, mean reward: 0.595 [0.500, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.776, 10.098], loss: 0.001570, mae: 0.041046, mean_q: 1.303172
 80944/100000: episode: 1366, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.865, mean reward: 0.589 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.024, 10.259], loss: 0.001521, mae: 0.041095, mean_q: 1.301462
 81044/100000: episode: 1367, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 59.741, mean reward: 0.597 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.528, 10.098], loss: 0.001457, mae: 0.041117, mean_q: 1.296105
 81144/100000: episode: 1368, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.298, mean reward: 0.583 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.537, 10.255], loss: 0.001697, mae: 0.042583, mean_q: 1.304282
 81244/100000: episode: 1369, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 58.156, mean reward: 0.582 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.736, 10.157], loss: 0.001466, mae: 0.040864, mean_q: 1.294169
 81344/100000: episode: 1370, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 60.722, mean reward: 0.607 [0.518, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.856, 10.257], loss: 0.001659, mae: 0.041731, mean_q: 1.297010
 81444/100000: episode: 1371, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.293, mean reward: 0.583 [0.506, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.437, 10.200], loss: 0.001659, mae: 0.043089, mean_q: 1.287533
 81544/100000: episode: 1372, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.964, mean reward: 0.590 [0.510, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.502, 10.210], loss: 0.001581, mae: 0.042258, mean_q: 1.288684
 81644/100000: episode: 1373, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.796, mean reward: 0.578 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.498, 10.098], loss: 0.001648, mae: 0.043607, mean_q: 1.290028
 81744/100000: episode: 1374, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 60.356, mean reward: 0.604 [0.512, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.553, 10.098], loss: 0.001605, mae: 0.042822, mean_q: 1.283141
 81844/100000: episode: 1375, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 59.884, mean reward: 0.599 [0.506, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.805, 10.388], loss: 0.001679, mae: 0.043060, mean_q: 1.276640
 81944/100000: episode: 1376, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.478, mean reward: 0.585 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.459, 10.098], loss: 0.001631, mae: 0.042373, mean_q: 1.285922
 82044/100000: episode: 1377, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 60.182, mean reward: 0.602 [0.511, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.349, 10.098], loss: 0.001672, mae: 0.043075, mean_q: 1.280180
 82144/100000: episode: 1378, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.044, mean reward: 0.590 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.741, 10.098], loss: 0.001512, mae: 0.042038, mean_q: 1.278013
 82244/100000: episode: 1379, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.302, mean reward: 0.593 [0.503, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.660, 10.098], loss: 0.001728, mae: 0.043540, mean_q: 1.271620
 82344/100000: episode: 1380, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.095, mean reward: 0.571 [0.509, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.100, 10.211], loss: 0.001677, mae: 0.043454, mean_q: 1.267572
 82444/100000: episode: 1381, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.819, mean reward: 0.608 [0.524, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.713, 10.209], loss: 0.001610, mae: 0.042996, mean_q: 1.267719
 82544/100000: episode: 1382, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 58.222, mean reward: 0.582 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.720, 10.098], loss: 0.001715, mae: 0.043179, mean_q: 1.264115
 82644/100000: episode: 1383, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 62.846, mean reward: 0.628 [0.526, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.327, 10.433], loss: 0.001691, mae: 0.044180, mean_q: 1.261935
 82744/100000: episode: 1384, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.367, mean reward: 0.604 [0.513, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.585, 10.098], loss: 0.001576, mae: 0.042467, mean_q: 1.259385
 82844/100000: episode: 1385, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 64.562, mean reward: 0.646 [0.514, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.509, 10.098], loss: 0.001830, mae: 0.045702, mean_q: 1.252712
 82944/100000: episode: 1386, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 60.578, mean reward: 0.606 [0.502, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.972, 10.135], loss: 0.001772, mae: 0.044335, mean_q: 1.246012
 83044/100000: episode: 1387, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.328, mean reward: 0.583 [0.514, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.304, 10.244], loss: 0.001739, mae: 0.043431, mean_q: 1.241163
 83144/100000: episode: 1388, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 58.279, mean reward: 0.583 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.312, 10.098], loss: 0.001604, mae: 0.043035, mean_q: 1.236086
 83244/100000: episode: 1389, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 58.912, mean reward: 0.589 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.076, 10.098], loss: 0.001705, mae: 0.043856, mean_q: 1.234553
 83344/100000: episode: 1390, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.665, mean reward: 0.577 [0.508, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.487, 10.279], loss: 0.001701, mae: 0.044061, mean_q: 1.223691
 83444/100000: episode: 1391, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 59.287, mean reward: 0.593 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.422, 10.155], loss: 0.001546, mae: 0.042181, mean_q: 1.216053
 83544/100000: episode: 1392, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 59.171, mean reward: 0.592 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.084, 10.196], loss: 0.001726, mae: 0.044085, mean_q: 1.206460
 83644/100000: episode: 1393, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 62.135, mean reward: 0.621 [0.510, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.837, 10.136], loss: 0.001506, mae: 0.041610, mean_q: 1.204126
 83744/100000: episode: 1394, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.732, mean reward: 0.587 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.273, 10.098], loss: 0.001603, mae: 0.043388, mean_q: 1.201194
 83844/100000: episode: 1395, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 59.466, mean reward: 0.595 [0.512, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.699, 10.098], loss: 0.001687, mae: 0.043765, mean_q: 1.196786
 83944/100000: episode: 1396, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.492, mean reward: 0.585 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.885, 10.098], loss: 0.001579, mae: 0.042531, mean_q: 1.192274
 84044/100000: episode: 1397, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 56.810, mean reward: 0.568 [0.500, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.376, 10.116], loss: 0.001664, mae: 0.044136, mean_q: 1.188894
 84144/100000: episode: 1398, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.032, mean reward: 0.570 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.609, 10.198], loss: 0.001458, mae: 0.041776, mean_q: 1.181295
 84244/100000: episode: 1399, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 59.456, mean reward: 0.595 [0.504, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.916, 10.184], loss: 0.001484, mae: 0.042108, mean_q: 1.180146
 84344/100000: episode: 1400, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 58.358, mean reward: 0.584 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.421, 10.234], loss: 0.001429, mae: 0.040883, mean_q: 1.166412
 84444/100000: episode: 1401, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.878, mean reward: 0.599 [0.508, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.499, 10.173], loss: 0.001485, mae: 0.041764, mean_q: 1.167246
 84544/100000: episode: 1402, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.695, mean reward: 0.587 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.581, 10.215], loss: 0.001505, mae: 0.042238, mean_q: 1.170850
 84644/100000: episode: 1403, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.702, mean reward: 0.597 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.646, 10.098], loss: 0.001447, mae: 0.041627, mean_q: 1.170519
 84744/100000: episode: 1404, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 57.670, mean reward: 0.577 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.698, 10.145], loss: 0.001509, mae: 0.041901, mean_q: 1.169789
 84844/100000: episode: 1405, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.265, mean reward: 0.583 [0.507, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.559, 10.111], loss: 0.001431, mae: 0.041468, mean_q: 1.169162
 84944/100000: episode: 1406, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 60.684, mean reward: 0.607 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.902, 10.098], loss: 0.001493, mae: 0.042051, mean_q: 1.168750
 85044/100000: episode: 1407, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.844, mean reward: 0.588 [0.501, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.783, 10.098], loss: 0.001498, mae: 0.042234, mean_q: 1.169659
 85144/100000: episode: 1408, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 61.554, mean reward: 0.616 [0.502, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.042, 10.148], loss: 0.001514, mae: 0.042147, mean_q: 1.168349
 85244/100000: episode: 1409, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.825, mean reward: 0.598 [0.501, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.062, 10.098], loss: 0.001497, mae: 0.041911, mean_q: 1.172349
 85344/100000: episode: 1410, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.484, mean reward: 0.575 [0.499, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.845, 10.305], loss: 0.001390, mae: 0.040621, mean_q: 1.172076
 85444/100000: episode: 1411, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.029, mean reward: 0.590 [0.501, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.866, 10.098], loss: 0.001377, mae: 0.040447, mean_q: 1.172378
 85544/100000: episode: 1412, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.836, mean reward: 0.578 [0.505, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.387, 10.217], loss: 0.001536, mae: 0.042431, mean_q: 1.172221
 85644/100000: episode: 1413, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 59.260, mean reward: 0.593 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.999, 10.233], loss: 0.001460, mae: 0.041506, mean_q: 1.169680
 85744/100000: episode: 1414, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.630, mean reward: 0.586 [0.507, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.289, 10.098], loss: 0.001410, mae: 0.041168, mean_q: 1.171401
 85844/100000: episode: 1415, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.402, mean reward: 0.574 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.259, 10.222], loss: 0.001534, mae: 0.042467, mean_q: 1.170234
 85944/100000: episode: 1416, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.449, mean reward: 0.584 [0.503, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.719, 10.166], loss: 0.001580, mae: 0.043405, mean_q: 1.171619
 86044/100000: episode: 1417, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.791, mean reward: 0.568 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.339, 10.106], loss: 0.001480, mae: 0.042068, mean_q: 1.169344
 86144/100000: episode: 1418, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.229, mean reward: 0.592 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.297, 10.162], loss: 0.001522, mae: 0.042648, mean_q: 1.168277
 86244/100000: episode: 1419, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.601, mean reward: 0.586 [0.500, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.941, 10.098], loss: 0.001508, mae: 0.042410, mean_q: 1.169680
 86344/100000: episode: 1420, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.862, mean reward: 0.579 [0.508, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.499, 10.098], loss: 0.001408, mae: 0.040844, mean_q: 1.168085
 86444/100000: episode: 1421, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 61.217, mean reward: 0.612 [0.503, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.376, 10.098], loss: 0.001565, mae: 0.043409, mean_q: 1.168890
 86544/100000: episode: 1422, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 60.242, mean reward: 0.602 [0.499, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.104, 10.098], loss: 0.001477, mae: 0.041560, mean_q: 1.168479
 86644/100000: episode: 1423, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.915, mean reward: 0.589 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.810, 10.098], loss: 0.001411, mae: 0.040633, mean_q: 1.168841
 86744/100000: episode: 1424, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 61.934, mean reward: 0.619 [0.512, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.127, 10.098], loss: 0.001600, mae: 0.043084, mean_q: 1.166839
 86844/100000: episode: 1425, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 61.811, mean reward: 0.618 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.526, 10.098], loss: 0.001597, mae: 0.043758, mean_q: 1.173336
 86944/100000: episode: 1426, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 56.761, mean reward: 0.568 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.853, 10.261], loss: 0.001652, mae: 0.044343, mean_q: 1.175463
 87044/100000: episode: 1427, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 62.803, mean reward: 0.628 [0.503, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.168, 10.098], loss: 0.001619, mae: 0.043097, mean_q: 1.172368
 87144/100000: episode: 1428, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 58.775, mean reward: 0.588 [0.499, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.255, 10.187], loss: 0.001528, mae: 0.042186, mean_q: 1.172487
 87244/100000: episode: 1429, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 60.274, mean reward: 0.603 [0.499, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.721, 10.235], loss: 0.001634, mae: 0.043996, mean_q: 1.172074
 87344/100000: episode: 1430, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.119, mean reward: 0.591 [0.509, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.001, 10.098], loss: 0.001645, mae: 0.044433, mean_q: 1.170599
 87444/100000: episode: 1431, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.867, mean reward: 0.589 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.539, 10.228], loss: 0.001695, mae: 0.044582, mean_q: 1.173055
 87544/100000: episode: 1432, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.583, mean reward: 0.576 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.462, 10.098], loss: 0.001508, mae: 0.042406, mean_q: 1.173988
 87644/100000: episode: 1433, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.662, mean reward: 0.597 [0.502, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.727, 10.208], loss: 0.001548, mae: 0.042676, mean_q: 1.172672
 87744/100000: episode: 1434, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.551, mean reward: 0.586 [0.504, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.579, 10.098], loss: 0.001709, mae: 0.044891, mean_q: 1.174659
 87844/100000: episode: 1435, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 57.086, mean reward: 0.571 [0.514, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.508, 10.283], loss: 0.001543, mae: 0.042981, mean_q: 1.167250
 87944/100000: episode: 1436, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 57.932, mean reward: 0.579 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.914, 10.143], loss: 0.001581, mae: 0.042656, mean_q: 1.168984
 88044/100000: episode: 1437, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.657, mean reward: 0.577 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.712, 10.098], loss: 0.001456, mae: 0.041316, mean_q: 1.164893
 88144/100000: episode: 1438, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.478, mean reward: 0.575 [0.504, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.435, 10.333], loss: 0.001535, mae: 0.042828, mean_q: 1.168489
 88244/100000: episode: 1439, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 57.446, mean reward: 0.574 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.587, 10.242], loss: 0.001629, mae: 0.043866, mean_q: 1.166809
 88344/100000: episode: 1440, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 59.935, mean reward: 0.599 [0.517, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.692, 10.098], loss: 0.001630, mae: 0.043060, mean_q: 1.163471
 88444/100000: episode: 1441, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 61.786, mean reward: 0.618 [0.512, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.659, 10.182], loss: 0.001647, mae: 0.043337, mean_q: 1.166298
 88544/100000: episode: 1442, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.616, mean reward: 0.586 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.494, 10.135], loss: 0.001600, mae: 0.043166, mean_q: 1.167175
 88644/100000: episode: 1443, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.578, mean reward: 0.586 [0.511, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.453, 10.210], loss: 0.001549, mae: 0.042607, mean_q: 1.165402
 88744/100000: episode: 1444, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 63.211, mean reward: 0.632 [0.512, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.208, 10.098], loss: 0.001475, mae: 0.042134, mean_q: 1.166733
 88844/100000: episode: 1445, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 59.422, mean reward: 0.594 [0.503, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.477, 10.266], loss: 0.001570, mae: 0.042583, mean_q: 1.168079
 88944/100000: episode: 1446, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 60.043, mean reward: 0.600 [0.501, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.252, 10.534], loss: 0.001537, mae: 0.042284, mean_q: 1.170162
 89044/100000: episode: 1447, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.515, mean reward: 0.595 [0.502, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.952, 10.299], loss: 0.001639, mae: 0.043821, mean_q: 1.171362
 89144/100000: episode: 1448, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 60.384, mean reward: 0.604 [0.505, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.694, 10.098], loss: 0.001611, mae: 0.043387, mean_q: 1.169385
 89244/100000: episode: 1449, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.279, mean reward: 0.583 [0.506, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.126, 10.098], loss: 0.001752, mae: 0.044951, mean_q: 1.170225
[Info] 1-TH LEVEL FOUND: 1.4074490070343018, Considering 10/90 traces
 89344/100000: episode: 1450, duration: 4.715s, episode steps: 100, steps per second: 21, episode reward: 60.513, mean reward: 0.605 [0.511, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.252, 10.367], loss: 0.001580, mae: 0.043602, mean_q: 1.169171
 89351/100000: episode: 1451, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 4.926, mean reward: 0.704 [0.690, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.390, 10.100], loss: 0.001581, mae: 0.041966, mean_q: 1.157248
 89402/100000: episode: 1452, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 35.047, mean reward: 0.687 [0.603, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.781, 10.363], loss: 0.001645, mae: 0.042115, mean_q: 1.170151
 89453/100000: episode: 1453, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 30.516, mean reward: 0.598 [0.528, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.991, 10.271], loss: 0.001548, mae: 0.042511, mean_q: 1.172596
 89531/100000: episode: 1454, duration: 0.433s, episode steps: 78, steps per second: 180, episode reward: 45.626, mean reward: 0.585 [0.510, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.673 [-0.161, 10.259], loss: 0.001582, mae: 0.043332, mean_q: 1.170872
 89586/100000: episode: 1455, duration: 0.307s, episode steps: 55, steps per second: 179, episode reward: 34.774, mean reward: 0.632 [0.519, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.791, 10.436], loss: 0.001486, mae: 0.042105, mean_q: 1.169886
 89593/100000: episode: 1456, duration: 0.047s, episode steps: 7, steps per second: 147, episode reward: 5.249, mean reward: 0.750 [0.685, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.408, 10.100], loss: 0.001806, mae: 0.045391, mean_q: 1.182426
 89622/100000: episode: 1457, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 20.331, mean reward: 0.701 [0.655, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.820, 10.100], loss: 0.001489, mae: 0.041368, mean_q: 1.170408
 89659/100000: episode: 1458, duration: 0.193s, episode steps: 37, steps per second: 191, episode reward: 25.810, mean reward: 0.698 [0.595, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.679, 10.100], loss: 0.001532, mae: 0.041834, mean_q: 1.175221
 89696/100000: episode: 1459, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 26.528, mean reward: 0.717 [0.633, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.288, 10.100], loss: 0.001586, mae: 0.042755, mean_q: 1.175488
 89705/100000: episode: 1460, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 6.189, mean reward: 0.688 [0.666, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.703, 10.100], loss: 0.001622, mae: 0.043187, mean_q: 1.180653
 89714/100000: episode: 1461, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 6.703, mean reward: 0.745 [0.710, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.176, 10.100], loss: 0.001452, mae: 0.041258, mean_q: 1.172160
 89723/100000: episode: 1462, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 6.293, mean reward: 0.699 [0.646, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.306, 10.100], loss: 0.001569, mae: 0.042552, mean_q: 1.182316
 89774/100000: episode: 1463, duration: 0.275s, episode steps: 51, steps per second: 186, episode reward: 31.279, mean reward: 0.613 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.432, 10.448], loss: 0.001632, mae: 0.043687, mean_q: 1.178474
 89852/100000: episode: 1464, duration: 0.411s, episode steps: 78, steps per second: 190, episode reward: 47.400, mean reward: 0.608 [0.511, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.669 [-0.942, 10.197], loss: 0.001692, mae: 0.044474, mean_q: 1.179681
 89861/100000: episode: 1465, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 6.921, mean reward: 0.769 [0.704, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.695, 10.100], loss: 0.001738, mae: 0.043903, mean_q: 1.177590
 89912/100000: episode: 1466, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 31.291, mean reward: 0.614 [0.523, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.491, 10.196], loss: 0.001636, mae: 0.043791, mean_q: 1.181036
 89967/100000: episode: 1467, duration: 0.285s, episode steps: 55, steps per second: 193, episode reward: 33.856, mean reward: 0.616 [0.513, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.749, 10.100], loss: 0.001586, mae: 0.043094, mean_q: 1.180170
 90045/100000: episode: 1468, duration: 0.405s, episode steps: 78, steps per second: 193, episode reward: 46.029, mean reward: 0.590 [0.511, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.662 [-0.623, 10.197], loss: 0.001681, mae: 0.044799, mean_q: 1.178034
 90074/100000: episode: 1469, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 18.416, mean reward: 0.635 [0.524, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.035, 10.100], loss: 0.001701, mae: 0.045011, mean_q: 1.180591
 90083/100000: episode: 1470, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 6.360, mean reward: 0.707 [0.659, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.507, 10.100], loss: 0.001566, mae: 0.043923, mean_q: 1.167325
 90092/100000: episode: 1471, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 6.956, mean reward: 0.773 [0.680, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.535, 10.100], loss: 0.001984, mae: 0.048719, mean_q: 1.191045
 90147/100000: episode: 1472, duration: 0.290s, episode steps: 55, steps per second: 190, episode reward: 33.494, mean reward: 0.609 [0.502, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.272, 10.100], loss: 0.001747, mae: 0.045328, mean_q: 1.182402
 90154/100000: episode: 1473, duration: 0.050s, episode steps: 7, steps per second: 141, episode reward: 5.261, mean reward: 0.752 [0.694, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.690, 10.100], loss: 0.001484, mae: 0.045626, mean_q: 1.184984
 90183/100000: episode: 1474, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 20.401, mean reward: 0.703 [0.652, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.319, 10.100], loss: 0.001623, mae: 0.043943, mean_q: 1.181561
 90212/100000: episode: 1475, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 18.516, mean reward: 0.638 [0.529, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.262, 10.100], loss: 0.001737, mae: 0.043897, mean_q: 1.183865
 90267/100000: episode: 1476, duration: 0.306s, episode steps: 55, steps per second: 180, episode reward: 34.221, mean reward: 0.622 [0.498, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.855 [-0.287, 10.175], loss: 0.001595, mae: 0.042996, mean_q: 1.184936
 90296/100000: episode: 1477, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 20.490, mean reward: 0.707 [0.596, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.381, 10.100], loss: 0.001581, mae: 0.043677, mean_q: 1.179831
 90303/100000: episode: 1478, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 5.138, mean reward: 0.734 [0.687, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.297, 10.100], loss: 0.001970, mae: 0.044658, mean_q: 1.179351
 90332/100000: episode: 1479, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 18.284, mean reward: 0.630 [0.533, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.539, 10.100], loss: 0.001728, mae: 0.044449, mean_q: 1.175637
 90383/100000: episode: 1480, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 37.085, mean reward: 0.727 [0.611, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.309, 10.586], loss: 0.001751, mae: 0.045571, mean_q: 1.185690
 90394/100000: episode: 1481, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 7.982, mean reward: 0.726 [0.677, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.295, 10.100], loss: 0.001320, mae: 0.038722, mean_q: 1.199831
 90423/100000: episode: 1482, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 19.928, mean reward: 0.687 [0.630, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.289, 10.100], loss: 0.001836, mae: 0.046400, mean_q: 1.189856
 90434/100000: episode: 1483, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 7.576, mean reward: 0.689 [0.600, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.105, 10.100], loss: 0.001885, mae: 0.046183, mean_q: 1.192606
 90459/100000: episode: 1484, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 14.460, mean reward: 0.578 [0.518, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.132, 10.184], loss: 0.001705, mae: 0.043935, mean_q: 1.191717
 90514/100000: episode: 1485, duration: 0.308s, episode steps: 55, steps per second: 179, episode reward: 33.872, mean reward: 0.616 [0.519, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-1.325, 10.168], loss: 0.001767, mae: 0.045128, mean_q: 1.191233
 90565/100000: episode: 1486, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 31.660, mean reward: 0.621 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.788, 10.176], loss: 0.001668, mae: 0.043033, mean_q: 1.188673
 90616/100000: episode: 1487, duration: 0.285s, episode steps: 51, steps per second: 179, episode reward: 31.779, mean reward: 0.623 [0.547, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.521, 10.372], loss: 0.001676, mae: 0.043918, mean_q: 1.191560
 90667/100000: episode: 1488, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 32.325, mean reward: 0.634 [0.520, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-1.044, 10.100], loss: 0.001777, mae: 0.044516, mean_q: 1.190973
 90674/100000: episode: 1489, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 4.972, mean reward: 0.710 [0.677, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.435, 10.100], loss: 0.001475, mae: 0.042099, mean_q: 1.199269
 90711/100000: episode: 1490, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 25.973, mean reward: 0.702 [0.567, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.624, 10.100], loss: 0.001697, mae: 0.044346, mean_q: 1.190489
 90718/100000: episode: 1491, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 4.888, mean reward: 0.698 [0.674, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-1.188, 10.100], loss: 0.002015, mae: 0.046284, mean_q: 1.176502
 90755/100000: episode: 1492, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 27.243, mean reward: 0.736 [0.620, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.311, 10.100], loss: 0.001752, mae: 0.044720, mean_q: 1.191371
 90806/100000: episode: 1493, duration: 0.262s, episode steps: 51, steps per second: 194, episode reward: 35.440, mean reward: 0.695 [0.625, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.449, 10.424], loss: 0.001683, mae: 0.044246, mean_q: 1.199092
 90857/100000: episode: 1494, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 31.095, mean reward: 0.610 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.336, 10.100], loss: 0.001648, mae: 0.043770, mean_q: 1.198504
 90886/100000: episode: 1495, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 21.098, mean reward: 0.728 [0.681, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.782, 10.100], loss: 0.001618, mae: 0.042844, mean_q: 1.197860
 90893/100000: episode: 1496, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 5.186, mean reward: 0.741 [0.700, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.394, 10.100], loss: 0.001427, mae: 0.039953, mean_q: 1.189037
 90904/100000: episode: 1497, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 7.802, mean reward: 0.709 [0.650, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-1.884, 10.100], loss: 0.001444, mae: 0.040675, mean_q: 1.200321
 90915/100000: episode: 1498, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 7.031, mean reward: 0.639 [0.612, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.140, 10.100], loss: 0.001827, mae: 0.045829, mean_q: 1.202662
 90922/100000: episode: 1499, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.104, mean reward: 0.729 [0.700, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.308, 10.100], loss: 0.001988, mae: 0.047152, mean_q: 1.181498
 90959/100000: episode: 1500, duration: 0.204s, episode steps: 37, steps per second: 182, episode reward: 25.824, mean reward: 0.698 [0.633, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.641, 10.100], loss: 0.001592, mae: 0.042804, mean_q: 1.191693
 90970/100000: episode: 1501, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 8.356, mean reward: 0.760 [0.661, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-1.321, 10.100], loss: 0.001496, mae: 0.040381, mean_q: 1.206443
 90977/100000: episode: 1502, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 5.459, mean reward: 0.780 [0.750, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.402, 10.100], loss: 0.001429, mae: 0.042452, mean_q: 1.201805
 90986/100000: episode: 1503, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 6.129, mean reward: 0.681 [0.646, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.877, 10.100], loss: 0.001438, mae: 0.042644, mean_q: 1.211837
 91037/100000: episode: 1504, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 32.036, mean reward: 0.628 [0.525, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-1.172, 10.120], loss: 0.001643, mae: 0.043675, mean_q: 1.201888
 91062/100000: episode: 1505, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 15.609, mean reward: 0.624 [0.553, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.286, 10.100], loss: 0.002013, mae: 0.047219, mean_q: 1.191805
 91069/100000: episode: 1506, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 4.536, mean reward: 0.648 [0.617, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.367, 10.100], loss: 0.001549, mae: 0.041399, mean_q: 1.198490
 91120/100000: episode: 1507, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 31.723, mean reward: 0.622 [0.527, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.487, 10.100], loss: 0.001725, mae: 0.044354, mean_q: 1.202071
 91198/100000: episode: 1508, duration: 0.419s, episode steps: 78, steps per second: 186, episode reward: 45.153, mean reward: 0.579 [0.504, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.660 [-0.348, 10.138], loss: 0.001799, mae: 0.044848, mean_q: 1.197705
 91205/100000: episode: 1509, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 4.865, mean reward: 0.695 [0.674, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.471, 10.100], loss: 0.001477, mae: 0.042534, mean_q: 1.213181
 91234/100000: episode: 1510, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 21.254, mean reward: 0.733 [0.677, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.698, 10.100], loss: 0.001716, mae: 0.044837, mean_q: 1.193676
 91271/100000: episode: 1511, duration: 0.205s, episode steps: 37, steps per second: 180, episode reward: 27.790, mean reward: 0.751 [0.657, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.478, 10.100], loss: 0.001763, mae: 0.044226, mean_q: 1.213680
 91322/100000: episode: 1512, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 30.146, mean reward: 0.591 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.691, 10.100], loss: 0.001863, mae: 0.045543, mean_q: 1.204584
 91351/100000: episode: 1513, duration: 0.168s, episode steps: 29, steps per second: 173, episode reward: 20.144, mean reward: 0.695 [0.572, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.563, 10.100], loss: 0.001658, mae: 0.043795, mean_q: 1.202317
 91429/100000: episode: 1514, duration: 0.405s, episode steps: 78, steps per second: 192, episode reward: 47.472, mean reward: 0.609 [0.533, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.675 [-0.221, 10.322], loss: 0.001643, mae: 0.043757, mean_q: 1.207107
 91507/100000: episode: 1515, duration: 0.396s, episode steps: 78, steps per second: 197, episode reward: 47.439, mean reward: 0.608 [0.513, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.657 [-0.674, 10.100], loss: 0.001645, mae: 0.042559, mean_q: 1.206563
 91532/100000: episode: 1516, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 17.003, mean reward: 0.680 [0.523, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.182, 10.120], loss: 0.001550, mae: 0.042285, mean_q: 1.213376
 91561/100000: episode: 1517, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 20.795, mean reward: 0.717 [0.646, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.944, 10.100], loss: 0.001741, mae: 0.045077, mean_q: 1.213762
 91612/100000: episode: 1518, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 32.504, mean reward: 0.637 [0.537, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.292, 10.229], loss: 0.001584, mae: 0.042705, mean_q: 1.208254
 91619/100000: episode: 1519, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 4.663, mean reward: 0.666 [0.634, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.587, 10.100], loss: 0.001955, mae: 0.046012, mean_q: 1.193509
 91628/100000: episode: 1520, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 6.265, mean reward: 0.696 [0.647, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.294, 10.100], loss: 0.001675, mae: 0.044384, mean_q: 1.229639
 91657/100000: episode: 1521, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 19.200, mean reward: 0.662 [0.572, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.668, 10.100], loss: 0.001671, mae: 0.044578, mean_q: 1.206462
 91694/100000: episode: 1522, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 26.938, mean reward: 0.728 [0.578, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.385, 10.100], loss: 0.001699, mae: 0.045079, mean_q: 1.206941
 91703/100000: episode: 1523, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 5.928, mean reward: 0.659 [0.618, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.361, 10.100], loss: 0.002129, mae: 0.049038, mean_q: 1.202675
 91728/100000: episode: 1524, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 15.766, mean reward: 0.631 [0.557, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.232, 10.119], loss: 0.001802, mae: 0.045755, mean_q: 1.212995
 91783/100000: episode: 1525, duration: 0.286s, episode steps: 55, steps per second: 192, episode reward: 34.064, mean reward: 0.619 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.443, 10.294], loss: 0.001564, mae: 0.042568, mean_q: 1.211073
 91812/100000: episode: 1526, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 18.908, mean reward: 0.652 [0.570, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.040, 10.100], loss: 0.001703, mae: 0.044121, mean_q: 1.211430
 91837/100000: episode: 1527, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 17.461, mean reward: 0.698 [0.586, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.175, 10.100], loss: 0.001948, mae: 0.047375, mean_q: 1.218994
 91846/100000: episode: 1528, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 6.405, mean reward: 0.712 [0.681, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.367, 10.100], loss: 0.001714, mae: 0.042809, mean_q: 1.214816
 91901/100000: episode: 1529, duration: 0.286s, episode steps: 55, steps per second: 192, episode reward: 36.145, mean reward: 0.657 [0.501, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-0.562, 10.187], loss: 0.001842, mae: 0.045688, mean_q: 1.212463
 91926/100000: episode: 1530, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 17.084, mean reward: 0.683 [0.598, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.639, 10.100], loss: 0.001719, mae: 0.044214, mean_q: 1.205688
 91951/100000: episode: 1531, duration: 0.151s, episode steps: 25, steps per second: 166, episode reward: 16.116, mean reward: 0.645 [0.559, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.074, 10.100], loss: 0.001384, mae: 0.040564, mean_q: 1.203886
 92029/100000: episode: 1532, duration: 0.412s, episode steps: 78, steps per second: 189, episode reward: 45.841, mean reward: 0.588 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.654 [-0.777, 10.100], loss: 0.001572, mae: 0.042511, mean_q: 1.215063
 92107/100000: episode: 1533, duration: 0.413s, episode steps: 78, steps per second: 189, episode reward: 47.132, mean reward: 0.604 [0.526, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.655 [-1.409, 10.100], loss: 0.001429, mae: 0.041164, mean_q: 1.217679
 92185/100000: episode: 1534, duration: 0.448s, episode steps: 78, steps per second: 174, episode reward: 49.022, mean reward: 0.628 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.656 [-0.534, 10.100], loss: 0.001570, mae: 0.042777, mean_q: 1.217276
 92222/100000: episode: 1535, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 24.533, mean reward: 0.663 [0.602, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.364, 10.100], loss: 0.001563, mae: 0.042927, mean_q: 1.214964
 92273/100000: episode: 1536, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 29.369, mean reward: 0.576 [0.502, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.278, 10.100], loss: 0.001437, mae: 0.041441, mean_q: 1.219301
 92282/100000: episode: 1537, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 6.286, mean reward: 0.698 [0.651, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.733, 10.100], loss: 0.002164, mae: 0.047244, mean_q: 1.223740
 92291/100000: episode: 1538, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 6.151, mean reward: 0.683 [0.637, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.394, 10.100], loss: 0.001916, mae: 0.045961, mean_q: 1.231494
 92300/100000: episode: 1539, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 6.541, mean reward: 0.727 [0.663, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.259, 10.100], loss: 0.001659, mae: 0.042978, mean_q: 1.205771
[Info] 2-TH LEVEL FOUND: 1.5046463012695312, Considering 10/90 traces
 92351/100000: episode: 1540, duration: 4.472s, episode steps: 51, steps per second: 11, episode reward: 31.200, mean reward: 0.612 [0.513, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.909, 10.100], loss: 0.001611, mae: 0.043345, mean_q: 1.218951
 92376/100000: episode: 1541, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 17.971, mean reward: 0.719 [0.632, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.276, 10.100], loss: 0.001575, mae: 0.042466, mean_q: 1.215676
 92391/100000: episode: 1542, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 10.069, mean reward: 0.671 [0.633, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.214, 10.100], loss: 0.001323, mae: 0.039041, mean_q: 1.217469
 92419/100000: episode: 1543, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 23.584, mean reward: 0.842 [0.758, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.520, 10.100], loss: 0.001466, mae: 0.040848, mean_q: 1.223134
 92446/100000: episode: 1544, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 18.795, mean reward: 0.696 [0.638, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-1.246, 10.100], loss: 0.001525, mae: 0.042178, mean_q: 1.232036
 92489/100000: episode: 1545, duration: 0.213s, episode steps: 43, steps per second: 202, episode reward: 28.515, mean reward: 0.663 [0.535, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.234, 10.118], loss: 0.001367, mae: 0.040336, mean_q: 1.227701
 92509/100000: episode: 1546, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 13.157, mean reward: 0.658 [0.599, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.043, 10.255], loss: 0.001435, mae: 0.040955, mean_q: 1.241598
 92560/100000: episode: 1547, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 33.163, mean reward: 0.650 [0.506, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.398, 10.100], loss: 0.001817, mae: 0.045436, mean_q: 1.224735
 92588/100000: episode: 1548, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 19.453, mean reward: 0.695 [0.567, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.222, 10.100], loss: 0.001475, mae: 0.041180, mean_q: 1.218237
 92603/100000: episode: 1549, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 10.774, mean reward: 0.718 [0.665, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.368, 10.100], loss: 0.001651, mae: 0.041470, mean_q: 1.233597
 92622/100000: episode: 1550, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 14.168, mean reward: 0.746 [0.659, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.721, 10.100], loss: 0.001318, mae: 0.039718, mean_q: 1.224833
 92642/100000: episode: 1551, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 12.372, mean reward: 0.619 [0.529, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.062, 10.162], loss: 0.001510, mae: 0.041712, mean_q: 1.234872
 92693/100000: episode: 1552, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 33.119, mean reward: 0.649 [0.509, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-1.362, 10.112], loss: 0.001391, mae: 0.040505, mean_q: 1.223325
 92713/100000: episode: 1553, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 14.725, mean reward: 0.736 [0.652, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.178, 10.100], loss: 0.002009, mae: 0.046221, mean_q: 1.233800
 92732/100000: episode: 1554, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 13.614, mean reward: 0.717 [0.629, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.672, 10.100], loss: 0.001288, mae: 0.039226, mean_q: 1.230189
 92747/100000: episode: 1555, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 11.284, mean reward: 0.752 [0.700, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.441, 10.100], loss: 0.001539, mae: 0.041340, mean_q: 1.234995
 92767/100000: episode: 1556, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 12.682, mean reward: 0.634 [0.533, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.056, 10.228], loss: 0.001587, mae: 0.041032, mean_q: 1.230064
 92810/100000: episode: 1557, duration: 0.242s, episode steps: 43, steps per second: 178, episode reward: 31.698, mean reward: 0.737 [0.669, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.224, 10.406], loss: 0.001452, mae: 0.040985, mean_q: 1.232228
 92830/100000: episode: 1558, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 14.380, mean reward: 0.719 [0.622, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.451, 10.100], loss: 0.001430, mae: 0.041369, mean_q: 1.235777
 92855/100000: episode: 1559, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 18.712, mean reward: 0.748 [0.622, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.486, 10.100], loss: 0.001486, mae: 0.042312, mean_q: 1.226693
 92906/100000: episode: 1560, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 37.739, mean reward: 0.740 [0.676, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.864, 10.477], loss: 0.001421, mae: 0.040736, mean_q: 1.243344
 92931/100000: episode: 1561, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 17.385, mean reward: 0.695 [0.594, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.286, 10.100], loss: 0.001250, mae: 0.037513, mean_q: 1.239809
 92958/100000: episode: 1562, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 20.536, mean reward: 0.761 [0.679, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.287, 10.100], loss: 0.001668, mae: 0.041685, mean_q: 1.240308
 92977/100000: episode: 1563, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 16.953, mean reward: 0.892 [0.795, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.434, 10.100], loss: 0.001839, mae: 0.045890, mean_q: 1.240875
 93004/100000: episode: 1564, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 19.790, mean reward: 0.733 [0.619, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.472, 10.100], loss: 0.001620, mae: 0.042513, mean_q: 1.243268
 93024/100000: episode: 1565, duration: 0.100s, episode steps: 20, steps per second: 199, episode reward: 13.718, mean reward: 0.686 [0.590, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.327, 10.100], loss: 0.001366, mae: 0.039557, mean_q: 1.250326
 93051/100000: episode: 1566, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 19.779, mean reward: 0.733 [0.681, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.287, 10.100], loss: 0.001351, mae: 0.040232, mean_q: 1.247504
 93102/100000: episode: 1567, duration: 0.261s, episode steps: 51, steps per second: 196, episode reward: 31.164, mean reward: 0.611 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.876, 10.272], loss: 0.001469, mae: 0.040948, mean_q: 1.238686
 93122/100000: episode: 1568, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 13.693, mean reward: 0.685 [0.529, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.096, 10.100], loss: 0.001236, mae: 0.039125, mean_q: 1.249512
 93147/100000: episode: 1569, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 18.181, mean reward: 0.727 [0.646, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.374, 10.100], loss: 0.001333, mae: 0.037785, mean_q: 1.246137
 93175/100000: episode: 1570, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 21.866, mean reward: 0.781 [0.733, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.885, 10.100], loss: 0.001388, mae: 0.040015, mean_q: 1.253966
 93226/100000: episode: 1571, duration: 0.272s, episode steps: 51, steps per second: 188, episode reward: 33.541, mean reward: 0.658 [0.525, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.329, 10.137], loss: 0.001282, mae: 0.038353, mean_q: 1.255410
 93253/100000: episode: 1572, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 21.985, mean reward: 0.814 [0.715, 0.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.483, 10.100], loss: 0.001528, mae: 0.042189, mean_q: 1.264892
 93281/100000: episode: 1573, duration: 0.175s, episode steps: 28, steps per second: 160, episode reward: 21.923, mean reward: 0.783 [0.677, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.384, 10.100], loss: 0.001591, mae: 0.042296, mean_q: 1.258036
 93300/100000: episode: 1574, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 15.410, mean reward: 0.811 [0.732, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.379, 10.100], loss: 0.001528, mae: 0.040697, mean_q: 1.255219
[Info] FALSIFICATION!
 93312/100000: episode: 1575, duration: 0.237s, episode steps: 12, steps per second: 51, episode reward: 10.644, mean reward: 0.887 [0.745, 1.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.720, 9.559], loss: 0.001124, mae: 0.036854, mean_q: 1.262120
 93340/100000: episode: 1576, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 21.445, mean reward: 0.766 [0.701, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.210, 10.100], loss: 0.001320, mae: 0.038821, mean_q: 1.257980
 93365/100000: episode: 1577, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 20.504, mean reward: 0.820 [0.747, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.888, 10.100], loss: 0.001999, mae: 0.044430, mean_q: 1.250810
 93393/100000: episode: 1578, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 23.502, mean reward: 0.839 [0.785, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.301, 10.100], loss: 0.001780, mae: 0.043287, mean_q: 1.288741
 93408/100000: episode: 1579, duration: 0.087s, episode steps: 15, steps per second: 171, episode reward: 11.547, mean reward: 0.770 [0.706, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.369, 10.100], loss: 0.001959, mae: 0.040299, mean_q: 1.240538
 93435/100000: episode: 1580, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 21.210, mean reward: 0.786 [0.694, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.526, 10.100], loss: 0.001413, mae: 0.040982, mean_q: 1.269071
 93455/100000: episode: 1581, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 14.430, mean reward: 0.721 [0.644, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.016, 10.408], loss: 0.001622, mae: 0.042017, mean_q: 1.264821
 93498/100000: episode: 1582, duration: 0.225s, episode steps: 43, steps per second: 191, episode reward: 27.881, mean reward: 0.648 [0.549, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.515, 10.363], loss: 0.001287, mae: 0.039067, mean_q: 1.266004
 93518/100000: episode: 1583, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 12.929, mean reward: 0.646 [0.584, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.512, 10.283], loss: 0.001407, mae: 0.041472, mean_q: 1.279367
 93543/100000: episode: 1584, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 17.892, mean reward: 0.716 [0.643, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.194, 10.100], loss: 0.001604, mae: 0.043781, mean_q: 1.279566
 93571/100000: episode: 1585, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 18.757, mean reward: 0.670 [0.559, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.576, 10.100], loss: 0.001487, mae: 0.041950, mean_q: 1.270348
 93622/100000: episode: 1586, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 32.920, mean reward: 0.645 [0.518, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.463, 10.100], loss: 0.001580, mae: 0.040750, mean_q: 1.268015
 93642/100000: episode: 1587, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 12.780, mean reward: 0.639 [0.537, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.103, 10.100], loss: 0.001421, mae: 0.040168, mean_q: 1.270459
 93670/100000: episode: 1588, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 19.490, mean reward: 0.696 [0.553, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.074, 10.100], loss: 0.001733, mae: 0.044751, mean_q: 1.279760
 93713/100000: episode: 1589, duration: 0.232s, episode steps: 43, steps per second: 185, episode reward: 29.447, mean reward: 0.685 [0.573, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.634, 10.245], loss: 0.001635, mae: 0.044395, mean_q: 1.272075
 93733/100000: episode: 1590, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 17.011, mean reward: 0.851 [0.726, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.457, 10.100], loss: 0.001956, mae: 0.043247, mean_q: 1.272865
 93753/100000: episode: 1591, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 13.025, mean reward: 0.651 [0.604, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.247, 10.315], loss: 0.001367, mae: 0.041749, mean_q: 1.274758
 93796/100000: episode: 1592, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 27.825, mean reward: 0.647 [0.503, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.996, 10.113], loss: 0.001612, mae: 0.041516, mean_q: 1.281258
 93823/100000: episode: 1593, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 21.272, mean reward: 0.788 [0.720, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.261, 10.100], loss: 0.001699, mae: 0.045006, mean_q: 1.276873
 93850/100000: episode: 1594, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 19.701, mean reward: 0.730 [0.671, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.240, 10.100], loss: 0.001374, mae: 0.040720, mean_q: 1.278483
 93893/100000: episode: 1595, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 29.635, mean reward: 0.689 [0.603, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.694, 10.288], loss: 0.001557, mae: 0.039648, mean_q: 1.275650
 93936/100000: episode: 1596, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 28.741, mean reward: 0.668 [0.499, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.797, 10.357], loss: 0.001320, mae: 0.039581, mean_q: 1.291645
 93987/100000: episode: 1597, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 38.556, mean reward: 0.756 [0.611, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.972, 10.545], loss: 0.001310, mae: 0.039719, mean_q: 1.283125
 94014/100000: episode: 1598, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 21.923, mean reward: 0.812 [0.722, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.511, 10.100], loss: 0.001212, mae: 0.037449, mean_q: 1.290790
 94034/100000: episode: 1599, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 13.692, mean reward: 0.685 [0.574, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.755, 10.235], loss: 0.001868, mae: 0.042043, mean_q: 1.282242
 94049/100000: episode: 1600, duration: 0.099s, episode steps: 15, steps per second: 152, episode reward: 11.182, mean reward: 0.745 [0.654, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.632, 10.100], loss: 0.001523, mae: 0.042230, mean_q: 1.284523
 94064/100000: episode: 1601, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 10.629, mean reward: 0.709 [0.614, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.146, 10.100], loss: 0.001909, mae: 0.040790, mean_q: 1.302549
 94115/100000: episode: 1602, duration: 0.281s, episode steps: 51, steps per second: 182, episode reward: 31.551, mean reward: 0.619 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.724, 10.100], loss: 0.001396, mae: 0.040839, mean_q: 1.290444
 94143/100000: episode: 1603, duration: 0.138s, episode steps: 28, steps per second: 204, episode reward: 20.106, mean reward: 0.718 [0.641, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.631, 10.100], loss: 0.001260, mae: 0.039800, mean_q: 1.275320
 94194/100000: episode: 1604, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 31.660, mean reward: 0.621 [0.534, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.333, 10.275], loss: 0.001231, mae: 0.038710, mean_q: 1.296344
 94219/100000: episode: 1605, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 18.480, mean reward: 0.739 [0.677, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.440, 10.100], loss: 0.001697, mae: 0.040966, mean_q: 1.286561
 94247/100000: episode: 1606, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 20.414, mean reward: 0.729 [0.620, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.868, 10.100], loss: 0.001574, mae: 0.039413, mean_q: 1.288251
 94275/100000: episode: 1607, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 19.283, mean reward: 0.689 [0.555, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.142, 10.100], loss: 0.001291, mae: 0.039508, mean_q: 1.300150
 94300/100000: episode: 1608, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 20.111, mean reward: 0.804 [0.739, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.488, 10.100], loss: 0.001147, mae: 0.037135, mean_q: 1.283355
 94315/100000: episode: 1609, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 10.656, mean reward: 0.710 [0.628, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.513, 10.100], loss: 0.001289, mae: 0.039461, mean_q: 1.298054
 94335/100000: episode: 1610, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 13.955, mean reward: 0.698 [0.642, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.122, 10.468], loss: 0.001347, mae: 0.039918, mean_q: 1.298179
 94354/100000: episode: 1611, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 14.924, mean reward: 0.785 [0.721, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.529, 10.100], loss: 0.001354, mae: 0.040188, mean_q: 1.289554
 94379/100000: episode: 1612, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 18.754, mean reward: 0.750 [0.634, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.311, 10.100], loss: 0.001609, mae: 0.039904, mean_q: 1.291549
 94399/100000: episode: 1613, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 14.422, mean reward: 0.721 [0.662, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.731, 10.100], loss: 0.001386, mae: 0.041888, mean_q: 1.311410
 94424/100000: episode: 1614, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 17.999, mean reward: 0.720 [0.582, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.242, 10.100], loss: 0.001400, mae: 0.037895, mean_q: 1.296301
 94444/100000: episode: 1615, duration: 0.097s, episode steps: 20, steps per second: 205, episode reward: 13.631, mean reward: 0.682 [0.600, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.416], loss: 0.001179, mae: 0.037692, mean_q: 1.294207
 94472/100000: episode: 1616, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 20.376, mean reward: 0.728 [0.608, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.147, 10.100], loss: 0.001125, mae: 0.036866, mean_q: 1.303840
 94497/100000: episode: 1617, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 19.721, mean reward: 0.789 [0.716, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.340, 10.100], loss: 0.001784, mae: 0.042518, mean_q: 1.304365
 94525/100000: episode: 1618, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 21.690, mean reward: 0.775 [0.606, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.348, 10.100], loss: 0.001141, mae: 0.037552, mean_q: 1.292131
 94550/100000: episode: 1619, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 17.375, mean reward: 0.695 [0.609, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.537, 10.100], loss: 0.001332, mae: 0.040362, mean_q: 1.299449
 94570/100000: episode: 1620, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 14.609, mean reward: 0.730 [0.702, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.791, 10.100], loss: 0.001316, mae: 0.039148, mean_q: 1.305401
 94595/100000: episode: 1621, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 18.278, mean reward: 0.731 [0.677, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.645, 10.100], loss: 0.001236, mae: 0.039238, mean_q: 1.298610
 94620/100000: episode: 1622, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 20.581, mean reward: 0.823 [0.696, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.904, 10.100], loss: 0.001161, mae: 0.037276, mean_q: 1.310103
 94647/100000: episode: 1623, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 21.236, mean reward: 0.787 [0.712, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.349, 10.100], loss: 0.001353, mae: 0.039876, mean_q: 1.300974
 94666/100000: episode: 1624, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 15.379, mean reward: 0.809 [0.739, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.245, 10.100], loss: 0.001412, mae: 0.042641, mean_q: 1.320149
 94709/100000: episode: 1625, duration: 0.225s, episode steps: 43, steps per second: 191, episode reward: 27.862, mean reward: 0.648 [0.520, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-2.142, 10.236], loss: 0.001410, mae: 0.038576, mean_q: 1.306141
 94724/100000: episode: 1626, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 11.425, mean reward: 0.762 [0.711, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.312, 10.100], loss: 0.001273, mae: 0.039362, mean_q: 1.317331
 94744/100000: episode: 1627, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 12.943, mean reward: 0.647 [0.544, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.204, 10.100], loss: 0.001590, mae: 0.037047, mean_q: 1.313796
 94764/100000: episode: 1628, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 15.497, mean reward: 0.775 [0.707, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.343, 10.100], loss: 0.001039, mae: 0.035325, mean_q: 1.315072
 94783/100000: episode: 1629, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 15.217, mean reward: 0.801 [0.768, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.412, 10.100], loss: 0.001181, mae: 0.038427, mean_q: 1.319544
[Info] Complete ISplit Iteration
[Info] Levels: [1.407449, 1.5046463, 1.8140959]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 94803/100000: episode: 1630, duration: 4.529s, episode steps: 20, steps per second: 4, episode reward: 13.457, mean reward: 0.673 [0.623, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.180, 10.348], loss: 0.001135, mae: 0.037170, mean_q: 1.320580
 94903/100000: episode: 1631, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 56.977, mean reward: 0.570 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.779, 10.182], loss: 0.001561, mae: 0.043065, mean_q: 1.314915
 95003/100000: episode: 1632, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.782, mean reward: 0.588 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.553, 10.214], loss: 0.001232, mae: 0.038442, mean_q: 1.312688
 95103/100000: episode: 1633, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.605, mean reward: 0.576 [0.508, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.471, 10.350], loss: 0.001224, mae: 0.038067, mean_q: 1.302814
 95203/100000: episode: 1634, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 60.916, mean reward: 0.609 [0.500, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.581, 10.251], loss: 0.001313, mae: 0.039155, mean_q: 1.309130
 95303/100000: episode: 1635, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.755, mean reward: 0.598 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.522, 10.449], loss: 0.001482, mae: 0.040207, mean_q: 1.304535
 95403/100000: episode: 1636, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.599, mean reward: 0.576 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.528, 10.098], loss: 0.001568, mae: 0.042186, mean_q: 1.303835
 95503/100000: episode: 1637, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 59.528, mean reward: 0.595 [0.505, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.622, 10.258], loss: 0.001423, mae: 0.039711, mean_q: 1.303097
 95603/100000: episode: 1638, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.725, mean reward: 0.577 [0.505, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.733, 10.098], loss: 0.001521, mae: 0.039838, mean_q: 1.301447
 95703/100000: episode: 1639, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.996, mean reward: 0.600 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.031, 10.098], loss: 0.001462, mae: 0.039984, mean_q: 1.292781
 95803/100000: episode: 1640, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.686, mean reward: 0.577 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.754, 10.289], loss: 0.001455, mae: 0.039566, mean_q: 1.291504
 95903/100000: episode: 1641, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.803, mean reward: 0.578 [0.500, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.811, 10.098], loss: 0.001369, mae: 0.039534, mean_q: 1.294162
 96003/100000: episode: 1642, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.399, mean reward: 0.584 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.243, 10.171], loss: 0.001466, mae: 0.040577, mean_q: 1.285985
 96103/100000: episode: 1643, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.999, mean reward: 0.580 [0.506, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.644, 10.289], loss: 0.001274, mae: 0.038436, mean_q: 1.281763
 96203/100000: episode: 1644, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 61.230, mean reward: 0.612 [0.501, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.987, 10.179], loss: 0.001401, mae: 0.040053, mean_q: 1.286188
 96303/100000: episode: 1645, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.798, mean reward: 0.588 [0.519, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.394, 10.098], loss: 0.001584, mae: 0.040660, mean_q: 1.288008
 96403/100000: episode: 1646, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.870, mean reward: 0.589 [0.503, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.346, 10.125], loss: 0.001646, mae: 0.043067, mean_q: 1.281959
 96503/100000: episode: 1647, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 62.532, mean reward: 0.625 [0.511, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.416, 10.364], loss: 0.001748, mae: 0.043699, mean_q: 1.280267
 96603/100000: episode: 1648, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.180, mean reward: 0.582 [0.508, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.814, 10.119], loss: 0.001474, mae: 0.041344, mean_q: 1.281192
 96703/100000: episode: 1649, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.923, mean reward: 0.589 [0.506, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.594, 10.098], loss: 0.001402, mae: 0.040475, mean_q: 1.274300
 96803/100000: episode: 1650, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.749, mean reward: 0.587 [0.509, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.379, 10.120], loss: 0.001420, mae: 0.040738, mean_q: 1.277428
 96903/100000: episode: 1651, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 56.830, mean reward: 0.568 [0.506, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.026, 10.112], loss: 0.001491, mae: 0.041638, mean_q: 1.271853
 97003/100000: episode: 1652, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 57.950, mean reward: 0.580 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.469, 10.098], loss: 0.001454, mae: 0.041191, mean_q: 1.265982
 97103/100000: episode: 1653, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.471, mean reward: 0.605 [0.523, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.981, 10.341], loss: 0.001500, mae: 0.041074, mean_q: 1.271527
 97203/100000: episode: 1654, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.186, mean reward: 0.582 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.748, 10.257], loss: 0.001647, mae: 0.042354, mean_q: 1.275768
 97303/100000: episode: 1655, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.767, mean reward: 0.608 [0.508, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.075, 10.098], loss: 0.001438, mae: 0.040422, mean_q: 1.265828
 97403/100000: episode: 1656, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.768, mean reward: 0.588 [0.516, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.012, 10.098], loss: 0.001509, mae: 0.041889, mean_q: 1.268549
 97503/100000: episode: 1657, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 60.613, mean reward: 0.606 [0.508, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.477, 10.098], loss: 0.001411, mae: 0.040638, mean_q: 1.263627
 97603/100000: episode: 1658, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.148, mean reward: 0.571 [0.511, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.431, 10.152], loss: 0.001470, mae: 0.040903, mean_q: 1.255879
 97703/100000: episode: 1659, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 57.047, mean reward: 0.570 [0.500, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.657, 10.098], loss: 0.001564, mae: 0.041390, mean_q: 1.250928
 97803/100000: episode: 1660, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 56.885, mean reward: 0.569 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.002, 10.193], loss: 0.001588, mae: 0.041704, mean_q: 1.247031
 97903/100000: episode: 1661, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 56.502, mean reward: 0.565 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.743, 10.098], loss: 0.001574, mae: 0.040996, mean_q: 1.246652
 98003/100000: episode: 1662, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 56.735, mean reward: 0.567 [0.504, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.995, 10.150], loss: 0.001602, mae: 0.042192, mean_q: 1.239334
 98103/100000: episode: 1663, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 56.851, mean reward: 0.569 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.680, 10.178], loss: 0.001612, mae: 0.042676, mean_q: 1.229558
 98203/100000: episode: 1664, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.457, mean reward: 0.585 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.414, 10.321], loss: 0.001503, mae: 0.041253, mean_q: 1.230127
 98303/100000: episode: 1665, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.616, mean reward: 0.586 [0.508, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.005, 10.204], loss: 0.001423, mae: 0.040634, mean_q: 1.220734
 98403/100000: episode: 1666, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.985, mean reward: 0.610 [0.513, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.272, 10.098], loss: 0.001558, mae: 0.042660, mean_q: 1.215528
 98503/100000: episode: 1667, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.784, mean reward: 0.598 [0.503, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.934, 10.098], loss: 0.001429, mae: 0.040728, mean_q: 1.215475
 98603/100000: episode: 1668, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.842, mean reward: 0.578 [0.506, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.589, 10.098], loss: 0.001502, mae: 0.041527, mean_q: 1.212527
 98703/100000: episode: 1669, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.459, mean reward: 0.595 [0.509, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.874, 10.098], loss: 0.001516, mae: 0.042503, mean_q: 1.211169
 98803/100000: episode: 1670, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.580, mean reward: 0.586 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.710, 10.098], loss: 0.001454, mae: 0.041003, mean_q: 1.204922
 98903/100000: episode: 1671, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.373, mean reward: 0.584 [0.498, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.502, 10.098], loss: 0.001504, mae: 0.042034, mean_q: 1.199050
 99003/100000: episode: 1672, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 60.028, mean reward: 0.600 [0.511, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.251, 10.098], loss: 0.001579, mae: 0.042718, mean_q: 1.196188
 99103/100000: episode: 1673, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 62.752, mean reward: 0.628 [0.501, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.322, 10.098], loss: 0.001394, mae: 0.040670, mean_q: 1.193702
 99203/100000: episode: 1674, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.617, mean reward: 0.576 [0.497, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.872, 10.156], loss: 0.001535, mae: 0.042408, mean_q: 1.189603
 99303/100000: episode: 1675, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.606, mean reward: 0.586 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.317, 10.277], loss: 0.001574, mae: 0.042842, mean_q: 1.187694
 99403/100000: episode: 1676, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 57.233, mean reward: 0.572 [0.503, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.717, 10.098], loss: 0.001552, mae: 0.042083, mean_q: 1.178450
 99503/100000: episode: 1677, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 60.213, mean reward: 0.602 [0.511, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.728, 10.098], loss: 0.001428, mae: 0.040926, mean_q: 1.179419
 99603/100000: episode: 1678, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.918, mean reward: 0.579 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.467, 10.118], loss: 0.001319, mae: 0.039718, mean_q: 1.170936
 99703/100000: episode: 1679, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.962, mean reward: 0.600 [0.526, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.854, 10.311], loss: 0.001391, mae: 0.040737, mean_q: 1.166032
 99803/100000: episode: 1680, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.625, mean reward: 0.596 [0.499, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.508, 10.560], loss: 0.001450, mae: 0.041427, mean_q: 1.164599
 99903/100000: episode: 1681, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.826, mean reward: 0.578 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.585, 10.112], loss: 0.001308, mae: 0.039518, mean_q: 1.164180
done, took 597.903 seconds
[Info] End Importance Splitting. Falsification occurred 10 times.
