Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.238s, episode steps: 100, steps per second: 420, episode reward: 60.111, mean reward: 0.601 [0.506, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.507, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.065s, episode steps: 100, steps per second: 1538, episode reward: 58.388, mean reward: 0.584 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.704, 10.149], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.065s, episode steps: 100, steps per second: 1529, episode reward: 56.826, mean reward: 0.568 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.273, 10.208], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.066s, episode steps: 100, steps per second: 1511, episode reward: 57.542, mean reward: 0.575 [0.504, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.311, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 61.650, mean reward: 0.617 [0.501, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.730, 10.317], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.063s, episode steps: 100, steps per second: 1589, episode reward: 57.574, mean reward: 0.576 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.923, 10.160], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 58.926, mean reward: 0.589 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.710, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 60.031, mean reward: 0.600 [0.498, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.087, 10.146], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.089s, episode steps: 100, steps per second: 1121, episode reward: 58.373, mean reward: 0.584 [0.508, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.634, 10.166], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.110s, episode steps: 100, steps per second: 906, episode reward: 57.022, mean reward: 0.570 [0.503, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.426, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.075s, episode steps: 100, steps per second: 1337, episode reward: 58.306, mean reward: 0.583 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.643, 10.381], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.086s, episode steps: 100, steps per second: 1162, episode reward: 57.195, mean reward: 0.572 [0.500, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.566, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.074s, episode steps: 100, steps per second: 1352, episode reward: 57.224, mean reward: 0.572 [0.498, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.085, 10.112], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.064s, episode steps: 100, steps per second: 1564, episode reward: 58.166, mean reward: 0.582 [0.511, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.465, 10.187], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.071s, episode steps: 100, steps per second: 1413, episode reward: 60.147, mean reward: 0.601 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.141, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 56.868, mean reward: 0.569 [0.499, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.291, 10.164], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.066s, episode steps: 100, steps per second: 1525, episode reward: 64.031, mean reward: 0.640 [0.512, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.567, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 59.320, mean reward: 0.593 [0.498, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.706, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 59.120, mean reward: 0.591 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.737, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.068s, episode steps: 100, steps per second: 1465, episode reward: 60.717, mean reward: 0.607 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.567, 10.179], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 58.592, mean reward: 0.586 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.913, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 58.168, mean reward: 0.582 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.864, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.070s, episode steps: 100, steps per second: 1423, episode reward: 60.520, mean reward: 0.605 [0.502, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.344, 10.420], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.073s, episode steps: 100, steps per second: 1370, episode reward: 59.107, mean reward: 0.591 [0.504, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.380, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.070s, episode steps: 100, steps per second: 1421, episode reward: 60.582, mean reward: 0.606 [0.520, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.071, 10.311], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.070s, episode steps: 100, steps per second: 1425, episode reward: 60.290, mean reward: 0.603 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.418, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.063s, episode steps: 100, steps per second: 1575, episode reward: 59.988, mean reward: 0.600 [0.505, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.662, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 57.829, mean reward: 0.578 [0.508, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.944, 10.154], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.070s, episode steps: 100, steps per second: 1423, episode reward: 59.156, mean reward: 0.592 [0.511, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.038, 10.136], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 57.643, mean reward: 0.576 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.792, 10.293], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 62.516, mean reward: 0.625 [0.498, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.340, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 57.237, mean reward: 0.572 [0.500, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.104, 10.105], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.066s, episode steps: 100, steps per second: 1511, episode reward: 59.494, mean reward: 0.595 [0.507, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.864, 10.382], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.071s, episode steps: 100, steps per second: 1409, episode reward: 60.039, mean reward: 0.600 [0.514, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.373, 10.213], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.073s, episode steps: 100, steps per second: 1368, episode reward: 59.254, mean reward: 0.593 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.258, 10.207], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.069s, episode steps: 100, steps per second: 1444, episode reward: 61.348, mean reward: 0.613 [0.499, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.264, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 58.462, mean reward: 0.585 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.942, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 59.515, mean reward: 0.595 [0.499, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.470, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: 62.454, mean reward: 0.625 [0.509, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.206, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.075s, episode steps: 100, steps per second: 1327, episode reward: 61.912, mean reward: 0.619 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.160, 10.422], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 59.262, mean reward: 0.593 [0.510, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.387, 10.100], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 57.644, mean reward: 0.576 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.667, 10.247], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.078s, episode steps: 100, steps per second: 1281, episode reward: 58.304, mean reward: 0.583 [0.501, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.272, 10.152], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 61.330, mean reward: 0.613 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.675, 10.407], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: 58.344, mean reward: 0.583 [0.506, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.664, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.063s, episode steps: 100, steps per second: 1583, episode reward: 62.858, mean reward: 0.629 [0.505, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.942, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 59.273, mean reward: 0.593 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.722, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.069s, episode steps: 100, steps per second: 1444, episode reward: 59.979, mean reward: 0.600 [0.507, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.286, 10.284], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.063s, episode steps: 100, steps per second: 1576, episode reward: 58.888, mean reward: 0.589 [0.513, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.699, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 57.947, mean reward: 0.579 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.726, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.292s, episode steps: 100, steps per second: 77, episode reward: 58.478, mean reward: 0.585 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.726, 10.182], loss: 0.047966, mae: 0.188130, mean_q: 0.119614
  5200/100000: episode: 52, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 58.882, mean reward: 0.589 [0.500, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.026, 10.098], loss: 0.003609, mae: 0.066382, mean_q: 0.553679
  5300/100000: episode: 53, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 57.518, mean reward: 0.575 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.985, 10.139], loss: 0.002920, mae: 0.058622, mean_q: 0.799488
  5400/100000: episode: 54, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 63.034, mean reward: 0.630 [0.508, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.591, 10.277], loss: 0.003231, mae: 0.058396, mean_q: 0.945649
  5500/100000: episode: 55, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 57.899, mean reward: 0.579 [0.504, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.624, 10.102], loss: 0.003280, mae: 0.057217, mean_q: 1.038954
  5600/100000: episode: 56, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 57.797, mean reward: 0.578 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.154, 10.098], loss: 0.003609, mae: 0.057196, mean_q: 1.091322
  5700/100000: episode: 57, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 59.085, mean reward: 0.591 [0.511, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.246, 10.267], loss: 0.003924, mae: 0.061140, mean_q: 1.122177
  5800/100000: episode: 58, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 58.427, mean reward: 0.584 [0.505, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.347, 10.098], loss: 0.003935, mae: 0.062882, mean_q: 1.145224
  5900/100000: episode: 59, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.426, mean reward: 0.584 [0.504, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.643, 10.193], loss: 0.003139, mae: 0.053669, mean_q: 1.156475
  6000/100000: episode: 60, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 57.901, mean reward: 0.579 [0.499, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.919, 10.098], loss: 0.003563, mae: 0.058339, mean_q: 1.161757
  6100/100000: episode: 61, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.135, mean reward: 0.571 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.381, 10.112], loss: 0.003404, mae: 0.056795, mean_q: 1.164485
  6200/100000: episode: 62, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 59.661, mean reward: 0.597 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.558, 10.231], loss: 0.003159, mae: 0.055677, mean_q: 1.171317
  6300/100000: episode: 63, duration: 0.764s, episode steps: 100, steps per second: 131, episode reward: 57.687, mean reward: 0.577 [0.501, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.911, 10.227], loss: 0.003175, mae: 0.053321, mean_q: 1.168720
  6400/100000: episode: 64, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 59.990, mean reward: 0.600 [0.511, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.713, 10.098], loss: 0.002973, mae: 0.054394, mean_q: 1.172734
  6500/100000: episode: 65, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 57.665, mean reward: 0.577 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.732, 10.157], loss: 0.002906, mae: 0.055254, mean_q: 1.178782
  6600/100000: episode: 66, duration: 0.623s, episode steps: 100, steps per second: 160, episode reward: 60.232, mean reward: 0.602 [0.519, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.777, 10.098], loss: 0.002964, mae: 0.053584, mean_q: 1.174789
  6700/100000: episode: 67, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: 61.528, mean reward: 0.615 [0.524, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.099, 10.098], loss: 0.003020, mae: 0.053450, mean_q: 1.172257
  6800/100000: episode: 68, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: 60.386, mean reward: 0.604 [0.505, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.411, 10.324], loss: 0.003259, mae: 0.056488, mean_q: 1.172735
  6900/100000: episode: 69, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 58.894, mean reward: 0.589 [0.507, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.328, 10.098], loss: 0.003090, mae: 0.055950, mean_q: 1.175490
  7000/100000: episode: 70, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: 60.868, mean reward: 0.609 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.681, 10.103], loss: 0.003242, mae: 0.056286, mean_q: 1.173114
  7100/100000: episode: 71, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: 58.766, mean reward: 0.588 [0.513, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.451, 10.169], loss: 0.003137, mae: 0.057603, mean_q: 1.176018
  7200/100000: episode: 72, duration: 1.139s, episode steps: 100, steps per second: 88, episode reward: 56.586, mean reward: 0.566 [0.499, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.469, 10.107], loss: 0.002768, mae: 0.052487, mean_q: 1.175866
  7300/100000: episode: 73, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 59.293, mean reward: 0.593 [0.510, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.147, 10.098], loss: 0.002992, mae: 0.054146, mean_q: 1.174289
  7400/100000: episode: 74, duration: 0.748s, episode steps: 100, steps per second: 134, episode reward: 59.776, mean reward: 0.598 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.124, 10.098], loss: 0.002904, mae: 0.053469, mean_q: 1.177171
  7500/100000: episode: 75, duration: 0.866s, episode steps: 100, steps per second: 116, episode reward: 58.423, mean reward: 0.584 [0.499, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.883, 10.184], loss: 0.003254, mae: 0.057449, mean_q: 1.169154
  7600/100000: episode: 76, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: 59.192, mean reward: 0.592 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.074, 10.239], loss: 0.002873, mae: 0.052545, mean_q: 1.170806
  7700/100000: episode: 77, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 57.713, mean reward: 0.577 [0.500, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.372, 10.240], loss: 0.003227, mae: 0.057677, mean_q: 1.171725
  7800/100000: episode: 78, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 58.106, mean reward: 0.581 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.429, 10.131], loss: 0.002919, mae: 0.053570, mean_q: 1.170653
  7900/100000: episode: 79, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 58.029, mean reward: 0.580 [0.511, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.793, 10.295], loss: 0.002602, mae: 0.051151, mean_q: 1.171055
  8000/100000: episode: 80, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 57.750, mean reward: 0.577 [0.497, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.776, 10.174], loss: 0.002256, mae: 0.048796, mean_q: 1.172973
  8100/100000: episode: 81, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 63.453, mean reward: 0.635 [0.519, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.653, 10.098], loss: 0.002865, mae: 0.052805, mean_q: 1.169532
  8200/100000: episode: 82, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 56.417, mean reward: 0.564 [0.502, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.548, 10.102], loss: 0.002863, mae: 0.052904, mean_q: 1.170573
  8300/100000: episode: 83, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 58.304, mean reward: 0.583 [0.510, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.422, 10.162], loss: 0.003123, mae: 0.056352, mean_q: 1.170827
  8400/100000: episode: 84, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 62.347, mean reward: 0.623 [0.508, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.515, 10.098], loss: 0.002665, mae: 0.050694, mean_q: 1.170458
  8500/100000: episode: 85, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 58.138, mean reward: 0.581 [0.504, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.985, 10.098], loss: 0.002907, mae: 0.054064, mean_q: 1.172042
  8600/100000: episode: 86, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 57.314, mean reward: 0.573 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.349, 10.345], loss: 0.002918, mae: 0.054101, mean_q: 1.172045
  8700/100000: episode: 87, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 60.704, mean reward: 0.607 [0.498, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.763, 10.098], loss: 0.003104, mae: 0.055894, mean_q: 1.168754
  8800/100000: episode: 88, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.050, mean reward: 0.580 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.815, 10.098], loss: 0.002663, mae: 0.051724, mean_q: 1.172184
  8900/100000: episode: 89, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 60.699, mean reward: 0.607 [0.498, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.774, 10.533], loss: 0.002828, mae: 0.053174, mean_q: 1.168172
  9000/100000: episode: 90, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 58.260, mean reward: 0.583 [0.506, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.623, 10.098], loss: 0.002579, mae: 0.051117, mean_q: 1.170791
  9100/100000: episode: 91, duration: 0.689s, episode steps: 100, steps per second: 145, episode reward: 60.352, mean reward: 0.604 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.797, 10.098], loss: 0.002796, mae: 0.052827, mean_q: 1.166446
  9200/100000: episode: 92, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 60.107, mean reward: 0.601 [0.506, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.764, 10.098], loss: 0.002816, mae: 0.053821, mean_q: 1.171775
  9300/100000: episode: 93, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 63.421, mean reward: 0.634 [0.512, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.031, 10.098], loss: 0.003125, mae: 0.053963, mean_q: 1.168480
  9400/100000: episode: 94, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.467, mean reward: 0.595 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.146, 10.183], loss: 0.003162, mae: 0.056013, mean_q: 1.165109
  9500/100000: episode: 95, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.518, mean reward: 0.585 [0.509, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.929, 10.098], loss: 0.002758, mae: 0.054530, mean_q: 1.171223
  9600/100000: episode: 96, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 55.921, mean reward: 0.559 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.656, 10.098], loss: 0.002661, mae: 0.050394, mean_q: 1.166407
  9700/100000: episode: 97, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 57.907, mean reward: 0.579 [0.501, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.644, 10.126], loss: 0.002360, mae: 0.049302, mean_q: 1.171452
  9800/100000: episode: 98, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 57.729, mean reward: 0.577 [0.500, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.769, 10.098], loss: 0.003108, mae: 0.056370, mean_q: 1.170180
  9900/100000: episode: 99, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 59.099, mean reward: 0.591 [0.498, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.668, 10.098], loss: 0.002957, mae: 0.053133, mean_q: 1.169566
 10000/100000: episode: 100, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 58.468, mean reward: 0.585 [0.513, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.640, 10.098], loss: 0.002392, mae: 0.049190, mean_q: 1.168583
 10100/100000: episode: 101, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: 58.726, mean reward: 0.587 [0.513, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.691, 10.181], loss: 0.002751, mae: 0.050819, mean_q: 1.166354
 10200/100000: episode: 102, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 59.525, mean reward: 0.595 [0.505, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.076, 10.098], loss: 0.003004, mae: 0.054580, mean_q: 1.168283
 10300/100000: episode: 103, duration: 0.615s, episode steps: 100, steps per second: 162, episode reward: 58.627, mean reward: 0.586 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.852, 10.168], loss: 0.002643, mae: 0.052314, mean_q: 1.167086
 10400/100000: episode: 104, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 57.839, mean reward: 0.578 [0.502, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.005, 10.123], loss: 0.002328, mae: 0.048611, mean_q: 1.167797
 10500/100000: episode: 105, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 61.443, mean reward: 0.614 [0.512, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.342, 10.186], loss: 0.002574, mae: 0.051288, mean_q: 1.170425
 10600/100000: episode: 106, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 57.701, mean reward: 0.577 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.819, 10.156], loss: 0.002720, mae: 0.052318, mean_q: 1.167746
 10700/100000: episode: 107, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.162, mean reward: 0.582 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.652, 10.206], loss: 0.002739, mae: 0.051785, mean_q: 1.167885
 10800/100000: episode: 108, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 61.414, mean reward: 0.614 [0.517, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.892, 10.392], loss: 0.002547, mae: 0.051340, mean_q: 1.166502
 10900/100000: episode: 109, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.554, mean reward: 0.596 [0.506, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.705, 10.098], loss: 0.002866, mae: 0.052903, mean_q: 1.165962
 11000/100000: episode: 110, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 58.237, mean reward: 0.582 [0.501, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.537, 10.098], loss: 0.002897, mae: 0.053918, mean_q: 1.168393
 11100/100000: episode: 111, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 59.510, mean reward: 0.595 [0.502, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.880, 10.108], loss: 0.002419, mae: 0.049603, mean_q: 1.171438
 11200/100000: episode: 112, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: 58.213, mean reward: 0.582 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.346, 10.098], loss: 0.002334, mae: 0.048886, mean_q: 1.169691
 11300/100000: episode: 113, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 58.700, mean reward: 0.587 [0.504, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.215, 10.254], loss: 0.002840, mae: 0.053292, mean_q: 1.169978
 11400/100000: episode: 114, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 60.431, mean reward: 0.604 [0.498, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.115, 10.294], loss: 0.002527, mae: 0.050578, mean_q: 1.171330
 11500/100000: episode: 115, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.488, mean reward: 0.575 [0.503, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.305, 10.201], loss: 0.002502, mae: 0.051307, mean_q: 1.173524
 11600/100000: episode: 116, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 58.279, mean reward: 0.583 [0.508, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.828, 10.098], loss: 0.002651, mae: 0.052105, mean_q: 1.168657
 11700/100000: episode: 117, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 59.343, mean reward: 0.593 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.117, 10.098], loss: 0.002551, mae: 0.051470, mean_q: 1.171401
 11800/100000: episode: 118, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 60.776, mean reward: 0.608 [0.506, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.234, 10.241], loss: 0.002201, mae: 0.047422, mean_q: 1.168734
 11900/100000: episode: 119, duration: 0.604s, episode steps: 100, steps per second: 166, episode reward: 59.080, mean reward: 0.591 [0.507, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.544, 10.098], loss: 0.002499, mae: 0.050519, mean_q: 1.166994
 12000/100000: episode: 120, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 59.593, mean reward: 0.596 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.883, 10.140], loss: 0.002367, mae: 0.049493, mean_q: 1.166605
 12100/100000: episode: 121, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.157, mean reward: 0.592 [0.508, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.623, 10.300], loss: 0.002389, mae: 0.050094, mean_q: 1.168396
 12200/100000: episode: 122, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 59.102, mean reward: 0.591 [0.510, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.205, 10.098], loss: 0.002346, mae: 0.049897, mean_q: 1.168388
 12300/100000: episode: 123, duration: 0.622s, episode steps: 100, steps per second: 161, episode reward: 58.148, mean reward: 0.581 [0.511, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.679, 10.098], loss: 0.002552, mae: 0.049336, mean_q: 1.168824
 12400/100000: episode: 124, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 60.350, mean reward: 0.603 [0.511, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.237, 10.098], loss: 0.002307, mae: 0.048935, mean_q: 1.172113
 12500/100000: episode: 125, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: 57.838, mean reward: 0.578 [0.512, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.820, 10.098], loss: 0.002309, mae: 0.048490, mean_q: 1.166665
 12600/100000: episode: 126, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 57.759, mean reward: 0.578 [0.501, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.123, 10.098], loss: 0.002429, mae: 0.048943, mean_q: 1.166338
 12700/100000: episode: 127, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 61.104, mean reward: 0.611 [0.501, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.786, 10.271], loss: 0.002690, mae: 0.052863, mean_q: 1.166821
 12800/100000: episode: 128, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 59.561, mean reward: 0.596 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.827, 10.355], loss: 0.002064, mae: 0.047241, mean_q: 1.169316
 12900/100000: episode: 129, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.398, mean reward: 0.594 [0.501, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.279, 10.098], loss: 0.002430, mae: 0.050089, mean_q: 1.171490
 13000/100000: episode: 130, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.208, mean reward: 0.602 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.309], loss: 0.002140, mae: 0.047930, mean_q: 1.174014
 13100/100000: episode: 131, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 62.209, mean reward: 0.622 [0.506, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.461, 10.326], loss: 0.002548, mae: 0.051002, mean_q: 1.169340
 13200/100000: episode: 132, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 66.283, mean reward: 0.663 [0.513, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.140, 10.476], loss: 0.002442, mae: 0.050510, mean_q: 1.172042
 13300/100000: episode: 133, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 60.296, mean reward: 0.603 [0.501, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.084, 10.163], loss: 0.002328, mae: 0.050018, mean_q: 1.174479
 13400/100000: episode: 134, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 58.673, mean reward: 0.587 [0.507, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.415, 10.152], loss: 0.002459, mae: 0.052306, mean_q: 1.173499
 13500/100000: episode: 135, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.603, mean reward: 0.596 [0.502, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.344, 10.098], loss: 0.002053, mae: 0.047568, mean_q: 1.176287
 13600/100000: episode: 136, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 61.292, mean reward: 0.613 [0.505, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.891, 10.098], loss: 0.002400, mae: 0.050560, mean_q: 1.175285
 13700/100000: episode: 137, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 61.636, mean reward: 0.616 [0.510, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.504, 10.329], loss: 0.002258, mae: 0.049988, mean_q: 1.178278
 13800/100000: episode: 138, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.937, mean reward: 0.589 [0.512, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.524, 10.263], loss: 0.002427, mae: 0.050909, mean_q: 1.177302
 13900/100000: episode: 139, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 60.189, mean reward: 0.602 [0.501, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.427], loss: 0.002291, mae: 0.049958, mean_q: 1.180499
 14000/100000: episode: 140, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 60.511, mean reward: 0.605 [0.504, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.793, 10.231], loss: 0.002169, mae: 0.049405, mean_q: 1.181200
 14100/100000: episode: 141, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.679, mean reward: 0.577 [0.503, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.496, 10.171], loss: 0.002292, mae: 0.050766, mean_q: 1.178410
 14200/100000: episode: 142, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.317, mean reward: 0.583 [0.510, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.021, 10.098], loss: 0.002226, mae: 0.049758, mean_q: 1.178986
 14300/100000: episode: 143, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 58.344, mean reward: 0.583 [0.512, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.149], loss: 0.002388, mae: 0.051000, mean_q: 1.175613
 14400/100000: episode: 144, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 60.112, mean reward: 0.601 [0.524, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.307, 10.098], loss: 0.002083, mae: 0.048284, mean_q: 1.178190
 14500/100000: episode: 145, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.157, mean reward: 0.582 [0.501, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.529, 10.098], loss: 0.002025, mae: 0.048668, mean_q: 1.174737
 14600/100000: episode: 146, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.280, mean reward: 0.573 [0.504, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.939, 10.166], loss: 0.002100, mae: 0.048531, mean_q: 1.175127
 14700/100000: episode: 147, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 56.584, mean reward: 0.566 [0.501, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.824, 10.123], loss: 0.002108, mae: 0.049108, mean_q: 1.173335
 14800/100000: episode: 148, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.943, mean reward: 0.589 [0.500, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.314, 10.100], loss: 0.002097, mae: 0.048757, mean_q: 1.175831
 14900/100000: episode: 149, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 63.350, mean reward: 0.634 [0.502, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.774, 10.592], loss: 0.001938, mae: 0.047479, mean_q: 1.174274
[Info] 1-TH LEVEL FOUND: 1.2798936367034912, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.643s, episode steps: 100, steps per second: 18, episode reward: 61.069, mean reward: 0.611 [0.499, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.279, 10.644], loss: 0.002008, mae: 0.048486, mean_q: 1.174958
 15029/100000: episode: 151, duration: 0.189s, episode steps: 29, steps per second: 153, episode reward: 20.700, mean reward: 0.714 [0.647, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.755, 10.416], loss: 0.001992, mae: 0.047200, mean_q: 1.174307
 15124/100000: episode: 152, duration: 0.639s, episode steps: 95, steps per second: 149, episode reward: 57.282, mean reward: 0.603 [0.521, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-0.816, 10.100], loss: 0.001977, mae: 0.048312, mean_q: 1.179998
 15174/100000: episode: 153, duration: 0.412s, episode steps: 50, steps per second: 121, episode reward: 31.567, mean reward: 0.631 [0.567, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.375, 10.379], loss: 0.002104, mae: 0.048864, mean_q: 1.180030
 15265/100000: episode: 154, duration: 0.873s, episode steps: 91, steps per second: 104, episode reward: 54.937, mean reward: 0.604 [0.515, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-1.846, 10.191], loss: 0.002112, mae: 0.048985, mean_q: 1.178870
 15360/100000: episode: 155, duration: 0.966s, episode steps: 95, steps per second: 98, episode reward: 55.311, mean reward: 0.582 [0.501, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-2.300, 10.103], loss: 0.002141, mae: 0.049705, mean_q: 1.180814
 15451/100000: episode: 156, duration: 0.699s, episode steps: 91, steps per second: 130, episode reward: 52.878, mean reward: 0.581 [0.501, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.534 [-2.023, 10.123], loss: 0.002022, mae: 0.047248, mean_q: 1.179950
 15482/100000: episode: 157, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 20.661, mean reward: 0.666 [0.564, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.702, 10.274], loss: 0.001806, mae: 0.045725, mean_q: 1.177293
 15571/100000: episode: 158, duration: 0.526s, episode steps: 89, steps per second: 169, episode reward: 52.207, mean reward: 0.587 [0.501, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.550 [-1.432, 10.100], loss: 0.002079, mae: 0.048587, mean_q: 1.177853
 15598/100000: episode: 159, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 16.386, mean reward: 0.607 [0.517, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.026, 10.154], loss: 0.001957, mae: 0.047516, mean_q: 1.188157
 15689/100000: episode: 160, duration: 0.510s, episode steps: 91, steps per second: 178, episode reward: 53.118, mean reward: 0.584 [0.501, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.534 [-1.267, 10.100], loss: 0.002075, mae: 0.048548, mean_q: 1.176901
 15716/100000: episode: 161, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 16.851, mean reward: 0.624 [0.556, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.050, 10.292], loss: 0.002162, mae: 0.049370, mean_q: 1.181974
 15805/100000: episode: 162, duration: 0.498s, episode steps: 89, steps per second: 179, episode reward: 53.640, mean reward: 0.603 [0.499, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.561 [-0.589, 10.268], loss: 0.002054, mae: 0.048072, mean_q: 1.180613
 15832/100000: episode: 163, duration: 0.173s, episode steps: 27, steps per second: 156, episode reward: 17.699, mean reward: 0.656 [0.595, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.801, 10.461], loss: 0.001886, mae: 0.047914, mean_q: 1.180907
 15863/100000: episode: 164, duration: 0.221s, episode steps: 31, steps per second: 140, episode reward: 21.636, mean reward: 0.698 [0.648, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.646, 10.486], loss: 0.001929, mae: 0.046835, mean_q: 1.180127
 15954/100000: episode: 165, duration: 0.571s, episode steps: 91, steps per second: 159, episode reward: 52.736, mean reward: 0.580 [0.507, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-1.311, 10.100], loss: 0.002191, mae: 0.050040, mean_q: 1.181379
 15983/100000: episode: 166, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 21.767, mean reward: 0.751 [0.637, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.640, 10.596], loss: 0.002026, mae: 0.048131, mean_q: 1.182388
 16012/100000: episode: 167, duration: 0.185s, episode steps: 29, steps per second: 156, episode reward: 17.813, mean reward: 0.614 [0.540, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.672, 10.223], loss: 0.002340, mae: 0.051328, mean_q: 1.185355
 16101/100000: episode: 168, duration: 0.538s, episode steps: 89, steps per second: 166, episode reward: 52.147, mean reward: 0.586 [0.507, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.558 [-0.592, 10.100], loss: 0.002301, mae: 0.050344, mean_q: 1.181266
 16130/100000: episode: 169, duration: 0.190s, episode steps: 29, steps per second: 152, episode reward: 19.146, mean reward: 0.660 [0.566, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.942, 10.275], loss: 0.001930, mae: 0.047039, mean_q: 1.185481
 16221/100000: episode: 170, duration: 0.519s, episode steps: 91, steps per second: 175, episode reward: 55.227, mean reward: 0.607 [0.526, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-1.053, 10.329], loss: 0.002173, mae: 0.048929, mean_q: 1.183701
 16316/100000: episode: 171, duration: 0.548s, episode steps: 95, steps per second: 173, episode reward: 55.295, mean reward: 0.582 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.750, 10.265], loss: 0.002120, mae: 0.049048, mean_q: 1.183026
 16347/100000: episode: 172, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 20.859, mean reward: 0.673 [0.594, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.644, 10.296], loss: 0.001931, mae: 0.046843, mean_q: 1.188456
 16442/100000: episode: 173, duration: 0.553s, episode steps: 95, steps per second: 172, episode reward: 54.262, mean reward: 0.571 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-1.745, 10.100], loss: 0.001983, mae: 0.046850, mean_q: 1.187543
 16537/100000: episode: 174, duration: 0.568s, episode steps: 95, steps per second: 167, episode reward: 55.895, mean reward: 0.588 [0.513, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.410, 10.100], loss: 0.002219, mae: 0.050455, mean_q: 1.182263
 16632/100000: episode: 175, duration: 0.516s, episode steps: 95, steps per second: 184, episode reward: 56.284, mean reward: 0.592 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.872, 10.100], loss: 0.002187, mae: 0.049178, mean_q: 1.181684
 16721/100000: episode: 176, duration: 0.493s, episode steps: 89, steps per second: 181, episode reward: 50.710, mean reward: 0.570 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.563 [-0.393, 10.100], loss: 0.002227, mae: 0.050617, mean_q: 1.182803
 16772/100000: episode: 177, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 36.568, mean reward: 0.717 [0.595, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.330, 10.274], loss: 0.002102, mae: 0.048674, mean_q: 1.185634
 16867/100000: episode: 178, duration: 0.535s, episode steps: 95, steps per second: 177, episode reward: 54.705, mean reward: 0.576 [0.508, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.956, 10.190], loss: 0.001978, mae: 0.047917, mean_q: 1.188329
 16958/100000: episode: 179, duration: 0.532s, episode steps: 91, steps per second: 171, episode reward: 52.859, mean reward: 0.581 [0.514, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.538 [-2.065, 10.100], loss: 0.002139, mae: 0.050299, mean_q: 1.185369
 17008/100000: episode: 180, duration: 0.286s, episode steps: 50, steps per second: 175, episode reward: 33.149, mean reward: 0.663 [0.558, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.874, 10.256], loss: 0.001900, mae: 0.046705, mean_q: 1.184107
 17059/100000: episode: 181, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 31.233, mean reward: 0.612 [0.539, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.390, 10.300], loss: 0.002056, mae: 0.048465, mean_q: 1.187872
 17150/100000: episode: 182, duration: 0.491s, episode steps: 91, steps per second: 185, episode reward: 52.135, mean reward: 0.573 [0.508, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.542 [-1.433, 10.125], loss: 0.001886, mae: 0.046616, mean_q: 1.185138
 17177/100000: episode: 183, duration: 0.173s, episode steps: 27, steps per second: 156, episode reward: 16.537, mean reward: 0.612 [0.556, 0.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.491, 10.301], loss: 0.001951, mae: 0.048745, mean_q: 1.188248
 17227/100000: episode: 184, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 32.609, mean reward: 0.652 [0.502, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.456, 10.100], loss: 0.002243, mae: 0.050490, mean_q: 1.184153
 17277/100000: episode: 185, duration: 0.296s, episode steps: 50, steps per second: 169, episode reward: 31.231, mean reward: 0.625 [0.536, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.377, 10.361], loss: 0.002155, mae: 0.049687, mean_q: 1.190049
 17328/100000: episode: 186, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 30.921, mean reward: 0.606 [0.517, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.315, 10.245], loss: 0.001914, mae: 0.047176, mean_q: 1.184428
 17423/100000: episode: 187, duration: 0.538s, episode steps: 95, steps per second: 177, episode reward: 54.004, mean reward: 0.568 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.934, 10.156], loss: 0.002062, mae: 0.048419, mean_q: 1.186927
 17452/100000: episode: 188, duration: 0.167s, episode steps: 29, steps per second: 173, episode reward: 18.982, mean reward: 0.655 [0.607, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.301, 10.342], loss: 0.001812, mae: 0.045791, mean_q: 1.186353
 17479/100000: episode: 189, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 19.036, mean reward: 0.705 [0.627, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.098, 10.499], loss: 0.002017, mae: 0.048323, mean_q: 1.183957
 17530/100000: episode: 190, duration: 0.321s, episode steps: 51, steps per second: 159, episode reward: 34.479, mean reward: 0.676 [0.552, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.343, 10.355], loss: 0.002282, mae: 0.051796, mean_q: 1.190913
 17621/100000: episode: 191, duration: 0.533s, episode steps: 91, steps per second: 171, episode reward: 54.670, mean reward: 0.601 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-0.536, 10.141], loss: 0.001910, mae: 0.047718, mean_q: 1.191483
 17650/100000: episode: 192, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 21.771, mean reward: 0.751 [0.671, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.038, 10.435], loss: 0.002110, mae: 0.049663, mean_q: 1.193033
 17739/100000: episode: 193, duration: 0.501s, episode steps: 89, steps per second: 178, episode reward: 53.004, mean reward: 0.596 [0.526, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.564 [-1.052, 10.285], loss: 0.002102, mae: 0.048641, mean_q: 1.191216
 17766/100000: episode: 194, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 16.238, mean reward: 0.601 [0.537, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.245], loss: 0.002017, mae: 0.049389, mean_q: 1.194080
 17797/100000: episode: 195, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 19.807, mean reward: 0.639 [0.557, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.174, 10.312], loss: 0.002082, mae: 0.048443, mean_q: 1.186528
 17824/100000: episode: 196, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 16.155, mean reward: 0.598 [0.504, 0.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.162], loss: 0.001701, mae: 0.044422, mean_q: 1.200491
 17913/100000: episode: 197, duration: 0.486s, episode steps: 89, steps per second: 183, episode reward: 57.109, mean reward: 0.642 [0.536, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-0.528, 10.100], loss: 0.001991, mae: 0.048432, mean_q: 1.191322
 18002/100000: episode: 198, duration: 0.498s, episode steps: 89, steps per second: 179, episode reward: 53.472, mean reward: 0.601 [0.499, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.560 [-0.923, 10.258], loss: 0.002027, mae: 0.047774, mean_q: 1.191257
 18029/100000: episode: 199, duration: 0.180s, episode steps: 27, steps per second: 150, episode reward: 16.016, mean reward: 0.593 [0.533, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.196], loss: 0.001952, mae: 0.046749, mean_q: 1.192321
 18120/100000: episode: 200, duration: 0.496s, episode steps: 91, steps per second: 184, episode reward: 56.904, mean reward: 0.625 [0.526, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.542 [-0.357, 10.100], loss: 0.002112, mae: 0.049270, mean_q: 1.193312
 18171/100000: episode: 201, duration: 0.301s, episode steps: 51, steps per second: 169, episode reward: 35.753, mean reward: 0.701 [0.582, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.318, 10.472], loss: 0.002175, mae: 0.050760, mean_q: 1.190273
 18221/100000: episode: 202, duration: 0.296s, episode steps: 50, steps per second: 169, episode reward: 31.383, mean reward: 0.628 [0.546, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.769, 10.271], loss: 0.002014, mae: 0.047717, mean_q: 1.193487
 18272/100000: episode: 203, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 35.044, mean reward: 0.687 [0.558, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.370, 10.421], loss: 0.002170, mae: 0.050241, mean_q: 1.190605
 18303/100000: episode: 204, duration: 0.190s, episode steps: 31, steps per second: 163, episode reward: 23.120, mean reward: 0.746 [0.665, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.804, 10.389], loss: 0.002162, mae: 0.050187, mean_q: 1.197995
 18332/100000: episode: 205, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 18.506, mean reward: 0.638 [0.578, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.337], loss: 0.001882, mae: 0.046197, mean_q: 1.191894
 18359/100000: episode: 206, duration: 0.166s, episode steps: 27, steps per second: 163, episode reward: 17.266, mean reward: 0.639 [0.546, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.217, 10.365], loss: 0.002290, mae: 0.051719, mean_q: 1.188871
 18386/100000: episode: 207, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 19.774, mean reward: 0.732 [0.652, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.452], loss: 0.002192, mae: 0.051749, mean_q: 1.193726
 18413/100000: episode: 208, duration: 0.162s, episode steps: 27, steps per second: 166, episode reward: 16.930, mean reward: 0.627 [0.522, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.972, 10.158], loss: 0.001965, mae: 0.047510, mean_q: 1.204294
 18442/100000: episode: 209, duration: 0.180s, episode steps: 29, steps per second: 161, episode reward: 18.755, mean reward: 0.647 [0.588, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.409, 10.347], loss: 0.002181, mae: 0.049441, mean_q: 1.188971
 18531/100000: episode: 210, duration: 0.499s, episode steps: 89, steps per second: 178, episode reward: 53.716, mean reward: 0.604 [0.540, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.552 [-0.637, 10.226], loss: 0.002104, mae: 0.049797, mean_q: 1.192581
 18562/100000: episode: 211, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 21.207, mean reward: 0.684 [0.606, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.092, 10.396], loss: 0.002030, mae: 0.049774, mean_q: 1.202760
 18589/100000: episode: 212, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 17.128, mean reward: 0.634 [0.591, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.366, 10.334], loss: 0.001905, mae: 0.046789, mean_q: 1.197085
 18680/100000: episode: 213, duration: 0.520s, episode steps: 91, steps per second: 175, episode reward: 52.974, mean reward: 0.582 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.534 [-1.219, 10.122], loss: 0.001930, mae: 0.046938, mean_q: 1.196802
 18771/100000: episode: 214, duration: 0.540s, episode steps: 91, steps per second: 168, episode reward: 52.982, mean reward: 0.582 [0.501, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-0.616, 10.216], loss: 0.001916, mae: 0.046936, mean_q: 1.195627
 18866/100000: episode: 215, duration: 0.506s, episode steps: 95, steps per second: 188, episode reward: 57.543, mean reward: 0.606 [0.502, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-0.524, 10.237], loss: 0.001933, mae: 0.048023, mean_q: 1.201251
 18895/100000: episode: 216, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 20.079, mean reward: 0.692 [0.587, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.887, 10.276], loss: 0.001810, mae: 0.046666, mean_q: 1.199215
 18990/100000: episode: 217, duration: 0.545s, episode steps: 95, steps per second: 174, episode reward: 59.012, mean reward: 0.621 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-1.013, 10.482], loss: 0.001771, mae: 0.044812, mean_q: 1.193948
 19085/100000: episode: 218, duration: 0.535s, episode steps: 95, steps per second: 177, episode reward: 56.974, mean reward: 0.600 [0.510, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.872, 10.100], loss: 0.002028, mae: 0.049230, mean_q: 1.202237
 19174/100000: episode: 219, duration: 0.507s, episode steps: 89, steps per second: 175, episode reward: 51.905, mean reward: 0.583 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.556 [-0.885, 10.385], loss: 0.001900, mae: 0.047109, mean_q: 1.198546
 19203/100000: episode: 220, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 18.949, mean reward: 0.653 [0.596, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.421, 10.303], loss: 0.001928, mae: 0.047367, mean_q: 1.201635
 19254/100000: episode: 221, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 32.566, mean reward: 0.639 [0.570, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.934, 10.375], loss: 0.001873, mae: 0.046863, mean_q: 1.203500
 19281/100000: episode: 222, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 16.233, mean reward: 0.601 [0.506, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.267], loss: 0.002004, mae: 0.046674, mean_q: 1.194718
 19372/100000: episode: 223, duration: 0.536s, episode steps: 91, steps per second: 170, episode reward: 53.887, mean reward: 0.592 [0.504, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.633, 10.264], loss: 0.002103, mae: 0.049286, mean_q: 1.197146
 19403/100000: episode: 224, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 21.319, mean reward: 0.688 [0.627, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.981, 10.443], loss: 0.001690, mae: 0.045207, mean_q: 1.204144
 19430/100000: episode: 225, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 16.212, mean reward: 0.600 [0.522, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.035, 10.182], loss: 0.002014, mae: 0.047693, mean_q: 1.198496
 19521/100000: episode: 226, duration: 0.531s, episode steps: 91, steps per second: 171, episode reward: 51.290, mean reward: 0.564 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-0.714, 10.100], loss: 0.001937, mae: 0.046933, mean_q: 1.201907
 19548/100000: episode: 227, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 17.036, mean reward: 0.631 [0.554, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.391, 10.251], loss: 0.001930, mae: 0.046979, mean_q: 1.194144
 19575/100000: episode: 228, duration: 0.166s, episode steps: 27, steps per second: 163, episode reward: 17.332, mean reward: 0.642 [0.562, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.167, 10.259], loss: 0.001817, mae: 0.047168, mean_q: 1.203510
 19604/100000: episode: 229, duration: 0.170s, episode steps: 29, steps per second: 170, episode reward: 18.528, mean reward: 0.639 [0.566, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.415, 10.270], loss: 0.001628, mae: 0.043308, mean_q: 1.205791
 19631/100000: episode: 230, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 16.237, mean reward: 0.601 [0.544, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.163, 10.240], loss: 0.002175, mae: 0.048667, mean_q: 1.199238
 19726/100000: episode: 231, duration: 0.542s, episode steps: 95, steps per second: 175, episode reward: 58.471, mean reward: 0.615 [0.510, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.990, 10.100], loss: 0.001749, mae: 0.044615, mean_q: 1.209945
 19755/100000: episode: 232, duration: 0.161s, episode steps: 29, steps per second: 181, episode reward: 19.320, mean reward: 0.666 [0.597, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.047, 10.287], loss: 0.001847, mae: 0.045961, mean_q: 1.203618
 19782/100000: episode: 233, duration: 0.165s, episode steps: 27, steps per second: 164, episode reward: 16.705, mean reward: 0.619 [0.540, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.807, 10.165], loss: 0.001989, mae: 0.048987, mean_q: 1.205549
 19873/100000: episode: 234, duration: 0.516s, episode steps: 91, steps per second: 176, episode reward: 54.149, mean reward: 0.595 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.919, 10.108], loss: 0.001985, mae: 0.048619, mean_q: 1.206338
 19962/100000: episode: 235, duration: 0.501s, episode steps: 89, steps per second: 177, episode reward: 54.438, mean reward: 0.612 [0.519, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.569 [-0.322, 10.100], loss: 0.001784, mae: 0.045635, mean_q: 1.202267
 20012/100000: episode: 236, duration: 0.297s, episode steps: 50, steps per second: 169, episode reward: 35.713, mean reward: 0.714 [0.617, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.381, 10.457], loss: 0.001686, mae: 0.044643, mean_q: 1.202751
 20041/100000: episode: 237, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 19.714, mean reward: 0.680 [0.624, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.310], loss: 0.001902, mae: 0.046850, mean_q: 1.196226
 20070/100000: episode: 238, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 19.969, mean reward: 0.689 [0.624, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.662, 10.485], loss: 0.001708, mae: 0.045421, mean_q: 1.205662
 20159/100000: episode: 239, duration: 0.508s, episode steps: 89, steps per second: 175, episode reward: 55.958, mean reward: 0.629 [0.517, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-2.097, 10.100], loss: 0.001806, mae: 0.046012, mean_q: 1.211251
[Info] 2-TH LEVEL FOUND: 1.4194554090499878, Considering 14/86 traces
 20186/100000: episode: 240, duration: 4.588s, episode steps: 27, steps per second: 6, episode reward: 16.495, mean reward: 0.611 [0.534, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.590, 10.192], loss: 0.001568, mae: 0.042877, mean_q: 1.206074
 20205/100000: episode: 241, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 15.384, mean reward: 0.810 [0.713, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.380, 10.551], loss: 0.001862, mae: 0.047088, mean_q: 1.212885
 20239/100000: episode: 242, duration: 0.201s, episode steps: 34, steps per second: 169, episode reward: 26.368, mean reward: 0.776 [0.667, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.793, 10.586], loss: 0.001726, mae: 0.045322, mean_q: 1.204571
 20273/100000: episode: 243, duration: 0.202s, episode steps: 34, steps per second: 168, episode reward: 25.579, mean reward: 0.752 [0.619, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-1.287, 10.404], loss: 0.001750, mae: 0.045129, mean_q: 1.213242
 20301/100000: episode: 244, duration: 0.178s, episode steps: 28, steps per second: 157, episode reward: 21.100, mean reward: 0.754 [0.602, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.323, 10.403], loss: 0.001695, mae: 0.045636, mean_q: 1.214314
 20329/100000: episode: 245, duration: 0.167s, episode steps: 28, steps per second: 168, episode reward: 20.494, mean reward: 0.732 [0.590, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.035, 10.318], loss: 0.001791, mae: 0.044456, mean_q: 1.221456
 20364/100000: episode: 246, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 24.138, mean reward: 0.690 [0.627, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.843, 10.390], loss: 0.001960, mae: 0.047704, mean_q: 1.217744
 20383/100000: episode: 247, duration: 0.126s, episode steps: 19, steps per second: 150, episode reward: 15.200, mean reward: 0.800 [0.752, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.198, 10.592], loss: 0.001861, mae: 0.046716, mean_q: 1.216435
 20418/100000: episode: 248, duration: 0.220s, episode steps: 35, steps per second: 159, episode reward: 22.532, mean reward: 0.644 [0.535, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.035, 10.216], loss: 0.001877, mae: 0.046664, mean_q: 1.216240
 20452/100000: episode: 249, duration: 0.206s, episode steps: 34, steps per second: 165, episode reward: 26.585, mean reward: 0.782 [0.680, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.222, 10.472], loss: 0.001598, mae: 0.043600, mean_q: 1.223658
 20487/100000: episode: 250, duration: 0.190s, episode steps: 35, steps per second: 185, episode reward: 26.106, mean reward: 0.746 [0.675, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.378, 10.551], loss: 0.001791, mae: 0.046613, mean_q: 1.223633
 20515/100000: episode: 251, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 21.828, mean reward: 0.780 [0.695, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.203, 10.572], loss: 0.001678, mae: 0.044908, mean_q: 1.218907
 20550/100000: episode: 252, duration: 0.202s, episode steps: 35, steps per second: 173, episode reward: 25.170, mean reward: 0.719 [0.583, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.873, 10.306], loss: 0.001880, mae: 0.047176, mean_q: 1.228471
 20585/100000: episode: 253, duration: 0.214s, episode steps: 35, steps per second: 164, episode reward: 24.201, mean reward: 0.691 [0.593, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.638, 10.317], loss: 0.001662, mae: 0.045168, mean_q: 1.230004
 20620/100000: episode: 254, duration: 0.204s, episode steps: 35, steps per second: 171, episode reward: 25.867, mean reward: 0.739 [0.639, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.866, 10.365], loss: 0.001641, mae: 0.044679, mean_q: 1.227372
 20655/100000: episode: 255, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 27.346, mean reward: 0.781 [0.623, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-1.220, 10.481], loss: 0.001884, mae: 0.047929, mean_q: 1.233110
 20683/100000: episode: 256, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 23.063, mean reward: 0.824 [0.778, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.052, 10.669], loss: 0.001782, mae: 0.046405, mean_q: 1.237194
 20704/100000: episode: 257, duration: 0.133s, episode steps: 21, steps per second: 158, episode reward: 18.234, mean reward: 0.868 [0.744, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.423, 10.615], loss: 0.002141, mae: 0.050649, mean_q: 1.230236
 20739/100000: episode: 258, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 25.480, mean reward: 0.728 [0.637, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.822, 10.501], loss: 0.001940, mae: 0.048352, mean_q: 1.237436
 20760/100000: episode: 259, duration: 0.137s, episode steps: 21, steps per second: 153, episode reward: 15.701, mean reward: 0.748 [0.652, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.402], loss: 0.001947, mae: 0.048135, mean_q: 1.229368
 20795/100000: episode: 260, duration: 0.210s, episode steps: 35, steps per second: 166, episode reward: 23.214, mean reward: 0.663 [0.545, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.208, 10.241], loss: 0.001996, mae: 0.049009, mean_q: 1.229577
 20830/100000: episode: 261, duration: 0.215s, episode steps: 35, steps per second: 163, episode reward: 21.362, mean reward: 0.610 [0.522, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.035, 10.212], loss: 0.002081, mae: 0.049868, mean_q: 1.231906
 20865/100000: episode: 262, duration: 0.205s, episode steps: 35, steps per second: 170, episode reward: 23.114, mean reward: 0.660 [0.550, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.718, 10.100], loss: 0.001982, mae: 0.048343, mean_q: 1.237985
 20893/100000: episode: 263, duration: 0.175s, episode steps: 28, steps per second: 160, episode reward: 20.406, mean reward: 0.729 [0.622, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.035, 10.356], loss: 0.001675, mae: 0.044280, mean_q: 1.244014
 20928/100000: episode: 264, duration: 0.205s, episode steps: 35, steps per second: 171, episode reward: 24.699, mean reward: 0.706 [0.576, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.581, 10.360], loss: 0.001763, mae: 0.045374, mean_q: 1.237339
 20963/100000: episode: 265, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 24.916, mean reward: 0.712 [0.635, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.374, 10.499], loss: 0.001552, mae: 0.043321, mean_q: 1.238248
 20984/100000: episode: 266, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 15.978, mean reward: 0.761 [0.705, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.728, 10.579], loss: 0.001637, mae: 0.044449, mean_q: 1.236796
 21003/100000: episode: 267, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 14.307, mean reward: 0.753 [0.679, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.970, 10.379], loss: 0.001796, mae: 0.045503, mean_q: 1.249837
 21038/100000: episode: 268, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 22.520, mean reward: 0.643 [0.548, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.035, 10.297], loss: 0.002009, mae: 0.048083, mean_q: 1.242284
 21073/100000: episode: 269, duration: 0.193s, episode steps: 35, steps per second: 182, episode reward: 22.425, mean reward: 0.641 [0.568, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.621, 10.268], loss: 0.002183, mae: 0.051225, mean_q: 1.248168
 21108/100000: episode: 270, duration: 0.220s, episode steps: 35, steps per second: 159, episode reward: 25.992, mean reward: 0.743 [0.655, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.087, 10.564], loss: 0.002035, mae: 0.048791, mean_q: 1.247292
 21143/100000: episode: 271, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 23.490, mean reward: 0.671 [0.562, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.987, 10.352], loss: 0.001792, mae: 0.046899, mean_q: 1.242944
 21178/100000: episode: 272, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 25.107, mean reward: 0.717 [0.623, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.390, 10.529], loss: 0.001684, mae: 0.044792, mean_q: 1.242534
 21213/100000: episode: 273, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 24.476, mean reward: 0.699 [0.537, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.568, 10.275], loss: 0.001898, mae: 0.047285, mean_q: 1.251182
 21248/100000: episode: 274, duration: 0.214s, episode steps: 35, steps per second: 163, episode reward: 22.771, mean reward: 0.651 [0.555, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.200, 10.277], loss: 0.001777, mae: 0.045615, mean_q: 1.255895
 21282/100000: episode: 275, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 24.418, mean reward: 0.718 [0.574, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.479, 10.297], loss: 0.001941, mae: 0.047834, mean_q: 1.254818
 21317/100000: episode: 276, duration: 0.218s, episode steps: 35, steps per second: 161, episode reward: 23.679, mean reward: 0.677 [0.600, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.694, 10.336], loss: 0.001860, mae: 0.047333, mean_q: 1.258662
 21351/100000: episode: 277, duration: 0.209s, episode steps: 34, steps per second: 163, episode reward: 21.854, mean reward: 0.643 [0.553, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.334, 10.366], loss: 0.001812, mae: 0.046519, mean_q: 1.251374
 21386/100000: episode: 278, duration: 0.227s, episode steps: 35, steps per second: 154, episode reward: 27.895, mean reward: 0.797 [0.628, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-1.380, 10.514], loss: 0.001675, mae: 0.044767, mean_q: 1.259497
 21421/100000: episode: 279, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 24.728, mean reward: 0.707 [0.577, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.217, 10.333], loss: 0.001716, mae: 0.045102, mean_q: 1.261228
 21456/100000: episode: 280, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 27.609, mean reward: 0.789 [0.664, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.366, 10.627], loss: 0.001647, mae: 0.044482, mean_q: 1.261857
 21475/100000: episode: 281, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 14.944, mean reward: 0.787 [0.722, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.162, 10.645], loss: 0.001904, mae: 0.048191, mean_q: 1.267224
 21509/100000: episode: 282, duration: 0.217s, episode steps: 34, steps per second: 156, episode reward: 24.396, mean reward: 0.718 [0.627, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.541, 10.387], loss: 0.001899, mae: 0.047865, mean_q: 1.259639
 21528/100000: episode: 283, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 13.487, mean reward: 0.710 [0.651, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.546, 10.389], loss: 0.001684, mae: 0.045111, mean_q: 1.262333
[Info] FALSIFICATION!
 21554/100000: episode: 284, duration: 0.609s, episode steps: 26, steps per second: 43, episode reward: 18.606, mean reward: 0.716 [0.629, 1.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-1.048, 10.024], loss: 0.002062, mae: 0.049006, mean_q: 1.254199
 21575/100000: episode: 285, duration: 0.174s, episode steps: 21, steps per second: 121, episode reward: 15.336, mean reward: 0.730 [0.657, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.443, 10.405], loss: 0.001869, mae: 0.047775, mean_q: 1.274139
 21596/100000: episode: 286, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 16.799, mean reward: 0.800 [0.731, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.783, 10.620], loss: 0.001723, mae: 0.046026, mean_q: 1.260509
 21615/100000: episode: 287, duration: 0.138s, episode steps: 19, steps per second: 138, episode reward: 15.060, mean reward: 0.793 [0.716, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.444, 10.612], loss: 0.001917, mae: 0.047674, mean_q: 1.259481
 21634/100000: episode: 288, duration: 0.199s, episode steps: 19, steps per second: 95, episode reward: 12.661, mean reward: 0.666 [0.591, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.101, 10.278], loss: 0.001706, mae: 0.045431, mean_q: 1.277856
 21669/100000: episode: 289, duration: 0.282s, episode steps: 35, steps per second: 124, episode reward: 26.644, mean reward: 0.761 [0.618, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.085, 10.513], loss: 0.002022, mae: 0.049275, mean_q: 1.269526
 21688/100000: episode: 290, duration: 0.256s, episode steps: 19, steps per second: 74, episode reward: 14.968, mean reward: 0.788 [0.727, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.560], loss: 0.001992, mae: 0.048318, mean_q: 1.270804
 21723/100000: episode: 291, duration: 0.316s, episode steps: 35, steps per second: 111, episode reward: 24.609, mean reward: 0.703 [0.572, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.464, 10.328], loss: 0.002061, mae: 0.048914, mean_q: 1.273400
 21758/100000: episode: 292, duration: 0.321s, episode steps: 35, steps per second: 109, episode reward: 23.232, mean reward: 0.664 [0.619, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.507, 10.401], loss: 0.002010, mae: 0.049053, mean_q: 1.279783
 21793/100000: episode: 293, duration: 0.388s, episode steps: 35, steps per second: 90, episode reward: 28.381, mean reward: 0.811 [0.632, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.462, 10.617], loss: 0.002243, mae: 0.051387, mean_q: 1.269405
 21821/100000: episode: 294, duration: 0.309s, episode steps: 28, steps per second: 91, episode reward: 21.626, mean reward: 0.772 [0.662, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.148, 10.488], loss: 0.001959, mae: 0.049318, mean_q: 1.280335
[Info] FALSIFICATION!
 21836/100000: episode: 295, duration: 0.525s, episode steps: 15, steps per second: 29, episode reward: 12.156, mean reward: 0.810 [0.720, 1.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.205, 10.265], loss: 0.001952, mae: 0.047995, mean_q: 1.273891
 21862/100000: episode: 296, duration: 0.194s, episode steps: 26, steps per second: 134, episode reward: 21.355, mean reward: 0.821 [0.747, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.129, 10.468], loss: 0.001745, mae: 0.045767, mean_q: 1.276874
 21883/100000: episode: 297, duration: 0.135s, episode steps: 21, steps per second: 156, episode reward: 15.341, mean reward: 0.731 [0.679, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.752, 10.562], loss: 0.001670, mae: 0.045459, mean_q: 1.288626
 21918/100000: episode: 298, duration: 0.203s, episode steps: 35, steps per second: 172, episode reward: 25.459, mean reward: 0.727 [0.612, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.587, 10.387], loss: 0.001747, mae: 0.046583, mean_q: 1.294022
 21953/100000: episode: 299, duration: 0.209s, episode steps: 35, steps per second: 167, episode reward: 23.291, mean reward: 0.665 [0.566, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.920, 10.249], loss: 0.001879, mae: 0.046949, mean_q: 1.285551
 21988/100000: episode: 300, duration: 0.188s, episode steps: 35, steps per second: 187, episode reward: 25.481, mean reward: 0.728 [0.606, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-1.076, 10.555], loss: 0.002333, mae: 0.052677, mean_q: 1.293632
 22023/100000: episode: 301, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 26.090, mean reward: 0.745 [0.650, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.866, 10.494], loss: 0.001742, mae: 0.046126, mean_q: 1.293546
 22049/100000: episode: 302, duration: 0.216s, episode steps: 26, steps per second: 120, episode reward: 20.159, mean reward: 0.775 [0.655, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.371, 10.321], loss: 0.001731, mae: 0.045491, mean_q: 1.297563
 22083/100000: episode: 303, duration: 0.203s, episode steps: 34, steps per second: 168, episode reward: 22.438, mean reward: 0.660 [0.520, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.728, 10.243], loss: 0.001697, mae: 0.045438, mean_q: 1.296332
 22118/100000: episode: 304, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 24.724, mean reward: 0.706 [0.622, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.327, 10.349], loss: 0.002161, mae: 0.048961, mean_q: 1.293052
 22153/100000: episode: 305, duration: 0.209s, episode steps: 35, steps per second: 168, episode reward: 21.890, mean reward: 0.625 [0.556, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.021, 10.382], loss: 0.001633, mae: 0.044102, mean_q: 1.285085
 22188/100000: episode: 306, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 22.972, mean reward: 0.656 [0.561, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.590, 10.399], loss: 0.001744, mae: 0.045249, mean_q: 1.284034
 22207/100000: episode: 307, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 14.136, mean reward: 0.744 [0.666, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.135, 10.376], loss: 0.002114, mae: 0.049676, mean_q: 1.291295
 22242/100000: episode: 308, duration: 0.191s, episode steps: 35, steps per second: 184, episode reward: 26.054, mean reward: 0.744 [0.616, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.569], loss: 0.002131, mae: 0.046974, mean_q: 1.306803
 22270/100000: episode: 309, duration: 0.163s, episode steps: 28, steps per second: 172, episode reward: 18.612, mean reward: 0.665 [0.579, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.307], loss: 0.001943, mae: 0.047719, mean_q: 1.295335
 22305/100000: episode: 310, duration: 0.204s, episode steps: 35, steps per second: 172, episode reward: 25.925, mean reward: 0.741 [0.624, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.035, 10.394], loss: 0.001754, mae: 0.046781, mean_q: 1.304765
 22324/100000: episode: 311, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 13.314, mean reward: 0.701 [0.581, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.371], loss: 0.001986, mae: 0.050023, mean_q: 1.300776
 22359/100000: episode: 312, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 28.045, mean reward: 0.801 [0.648, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.729, 10.644], loss: 0.001949, mae: 0.047959, mean_q: 1.297500
 22387/100000: episode: 313, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 20.520, mean reward: 0.733 [0.597, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.553, 10.315], loss: 0.002098, mae: 0.047987, mean_q: 1.288053
 22422/100000: episode: 314, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 25.212, mean reward: 0.720 [0.636, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.320, 10.450], loss: 0.001918, mae: 0.048856, mean_q: 1.302073
 22457/100000: episode: 315, duration: 0.215s, episode steps: 35, steps per second: 163, episode reward: 22.093, mean reward: 0.631 [0.539, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.035, 10.252], loss: 0.001701, mae: 0.044660, mean_q: 1.300525
 22476/100000: episode: 316, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 13.000, mean reward: 0.684 [0.554, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.278], loss: 0.001965, mae: 0.048720, mean_q: 1.310463
 22511/100000: episode: 317, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 26.209, mean reward: 0.749 [0.553, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.047, 10.408], loss: 0.001831, mae: 0.046269, mean_q: 1.313176
 22537/100000: episode: 318, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 19.075, mean reward: 0.734 [0.677, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.101, 10.417], loss: 0.001943, mae: 0.048413, mean_q: 1.298114
 22572/100000: episode: 319, duration: 0.203s, episode steps: 35, steps per second: 172, episode reward: 24.184, mean reward: 0.691 [0.613, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.380, 10.469], loss: 0.001807, mae: 0.046904, mean_q: 1.297330
 22607/100000: episode: 320, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 24.920, mean reward: 0.712 [0.619, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.060, 10.454], loss: 0.001751, mae: 0.045796, mean_q: 1.307594
 22642/100000: episode: 321, duration: 0.204s, episode steps: 35, steps per second: 171, episode reward: 25.075, mean reward: 0.716 [0.636, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.349, 10.411], loss: 0.001767, mae: 0.046236, mean_q: 1.309438
 22677/100000: episode: 322, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 25.860, mean reward: 0.739 [0.663, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.645, 10.507], loss: 0.002298, mae: 0.053240, mean_q: 1.312581
 22712/100000: episode: 323, duration: 0.222s, episode steps: 35, steps per second: 158, episode reward: 27.020, mean reward: 0.772 [0.660, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.114, 10.420], loss: 0.001852, mae: 0.047665, mean_q: 1.314956
 22747/100000: episode: 324, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 23.983, mean reward: 0.685 [0.556, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.552, 10.252], loss: 0.001894, mae: 0.047720, mean_q: 1.317326
 22781/100000: episode: 325, duration: 0.196s, episode steps: 34, steps per second: 174, episode reward: 27.808, mean reward: 0.818 [0.709, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.764, 10.556], loss: 0.001937, mae: 0.047716, mean_q: 1.315901
[Info] Complete ISplit Iteration
[Info] Levels: [1.2798936, 1.4194554, 1.6369874]
[Info] Cond. Prob: [0.1, 0.14, 0.16]
[Info] Error Prob: 0.0022400000000000002

 22816/100000: episode: 326, duration: 4.821s, episode steps: 35, steps per second: 7, episode reward: 25.030, mean reward: 0.715 [0.648, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.962, 10.392], loss: 0.002167, mae: 0.050750, mean_q: 1.324855
 22916/100000: episode: 327, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.049, mean reward: 0.580 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.527, 10.212], loss: 0.001920, mae: 0.047767, mean_q: 1.315950
 23016/100000: episode: 328, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 58.029, mean reward: 0.580 [0.512, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.517, 10.098], loss: 0.001713, mae: 0.045815, mean_q: 1.317072
 23116/100000: episode: 329, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 58.322, mean reward: 0.583 [0.514, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.832, 10.098], loss: 0.001795, mae: 0.046266, mean_q: 1.310122
 23216/100000: episode: 330, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 60.295, mean reward: 0.603 [0.507, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.831, 10.098], loss: 0.001912, mae: 0.046356, mean_q: 1.312013
 23316/100000: episode: 331, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 59.153, mean reward: 0.592 [0.506, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.478, 10.098], loss: 0.002093, mae: 0.049239, mean_q: 1.301805
 23416/100000: episode: 332, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 59.389, mean reward: 0.594 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.347, 10.098], loss: 0.001919, mae: 0.047712, mean_q: 1.305572
 23516/100000: episode: 333, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.049, mean reward: 0.580 [0.500, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.987, 10.098], loss: 0.001898, mae: 0.047349, mean_q: 1.304033
 23616/100000: episode: 334, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 57.293, mean reward: 0.573 [0.509, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.831, 10.100], loss: 0.001912, mae: 0.047436, mean_q: 1.309268
 23716/100000: episode: 335, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.264, mean reward: 0.603 [0.503, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.341, 10.098], loss: 0.002100, mae: 0.048561, mean_q: 1.305390
 23816/100000: episode: 336, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.173, mean reward: 0.572 [0.508, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.779, 10.098], loss: 0.001781, mae: 0.045306, mean_q: 1.307918
 23916/100000: episode: 337, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.767, mean reward: 0.578 [0.506, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.507, 10.098], loss: 0.002064, mae: 0.049671, mean_q: 1.304578
 24016/100000: episode: 338, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 58.109, mean reward: 0.581 [0.505, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.040, 10.215], loss: 0.001924, mae: 0.047621, mean_q: 1.295750
 24116/100000: episode: 339, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.463, mean reward: 0.585 [0.510, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.566, 10.347], loss: 0.001825, mae: 0.046918, mean_q: 1.295955
 24216/100000: episode: 340, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.424, mean reward: 0.594 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.376, 10.098], loss: 0.001835, mae: 0.046055, mean_q: 1.299936
 24316/100000: episode: 341, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.013, mean reward: 0.580 [0.507, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.796, 10.098], loss: 0.001831, mae: 0.046069, mean_q: 1.295665
 24416/100000: episode: 342, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 60.723, mean reward: 0.607 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.070, 10.238], loss: 0.001733, mae: 0.045456, mean_q: 1.294436
 24516/100000: episode: 343, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 59.088, mean reward: 0.591 [0.502, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.730, 10.098], loss: 0.001968, mae: 0.047270, mean_q: 1.292179
 24616/100000: episode: 344, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.463, mean reward: 0.595 [0.505, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.532, 10.098], loss: 0.001956, mae: 0.046908, mean_q: 1.297953
 24716/100000: episode: 345, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.308, mean reward: 0.583 [0.505, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.836, 10.141], loss: 0.001974, mae: 0.047399, mean_q: 1.294762
 24816/100000: episode: 346, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 60.299, mean reward: 0.603 [0.499, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.593, 10.098], loss: 0.002171, mae: 0.049461, mean_q: 1.300520
 24916/100000: episode: 347, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 57.904, mean reward: 0.579 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.987, 10.140], loss: 0.001869, mae: 0.045619, mean_q: 1.294999
 25016/100000: episode: 348, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 60.641, mean reward: 0.606 [0.509, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.400, 10.200], loss: 0.001739, mae: 0.045433, mean_q: 1.296297
 25116/100000: episode: 349, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.473, mean reward: 0.605 [0.503, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.526, 10.253], loss: 0.002041, mae: 0.047913, mean_q: 1.295858
 25216/100000: episode: 350, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.351, mean reward: 0.584 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.976, 10.098], loss: 0.001958, mae: 0.047159, mean_q: 1.281780
 25316/100000: episode: 351, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.228, mean reward: 0.592 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.500, 10.098], loss: 0.001820, mae: 0.045846, mean_q: 1.282634
 25416/100000: episode: 352, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 58.327, mean reward: 0.583 [0.501, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.899, 10.313], loss: 0.001775, mae: 0.045540, mean_q: 1.279425
 25516/100000: episode: 353, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.310, mean reward: 0.583 [0.511, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.802, 10.196], loss: 0.001659, mae: 0.043838, mean_q: 1.273816
 25616/100000: episode: 354, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.025, mean reward: 0.600 [0.509, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.261, 10.276], loss: 0.001973, mae: 0.047090, mean_q: 1.261770
 25716/100000: episode: 355, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 60.162, mean reward: 0.602 [0.499, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.434, 10.161], loss: 0.001734, mae: 0.045011, mean_q: 1.257590
 25816/100000: episode: 356, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 58.309, mean reward: 0.583 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.578, 10.323], loss: 0.002256, mae: 0.048274, mean_q: 1.252392
 25916/100000: episode: 357, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.695, mean reward: 0.587 [0.507, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.397, 10.098], loss: 0.001947, mae: 0.047307, mean_q: 1.254219
 26016/100000: episode: 358, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 58.359, mean reward: 0.584 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.537, 10.098], loss: 0.001744, mae: 0.044645, mean_q: 1.251622
 26116/100000: episode: 359, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.345, mean reward: 0.593 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.883, 10.098], loss: 0.001981, mae: 0.046609, mean_q: 1.238727
 26216/100000: episode: 360, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.399, mean reward: 0.574 [0.502, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.615, 10.098], loss: 0.001678, mae: 0.044347, mean_q: 1.244405
 26316/100000: episode: 361, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 60.035, mean reward: 0.600 [0.511, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.443, 10.101], loss: 0.002029, mae: 0.047673, mean_q: 1.237028
 26416/100000: episode: 362, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 59.399, mean reward: 0.594 [0.510, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.443, 10.306], loss: 0.001800, mae: 0.045830, mean_q: 1.227336
 26516/100000: episode: 363, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 56.902, mean reward: 0.569 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.500, 10.098], loss: 0.002019, mae: 0.047666, mean_q: 1.223103
 26616/100000: episode: 364, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 61.560, mean reward: 0.616 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.776, 10.098], loss: 0.001848, mae: 0.045452, mean_q: 1.225360
 26716/100000: episode: 365, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.344, mean reward: 0.573 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.757, 10.098], loss: 0.001758, mae: 0.044519, mean_q: 1.219272
 26816/100000: episode: 366, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 60.325, mean reward: 0.603 [0.510, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.614, 10.098], loss: 0.001697, mae: 0.044241, mean_q: 1.211821
 26916/100000: episode: 367, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 58.756, mean reward: 0.588 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.205, 10.098], loss: 0.001593, mae: 0.043359, mean_q: 1.209506
 27016/100000: episode: 368, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.965, mean reward: 0.590 [0.508, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.399, 10.290], loss: 0.001554, mae: 0.042360, mean_q: 1.200779
 27116/100000: episode: 369, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.505, mean reward: 0.585 [0.512, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.998, 10.270], loss: 0.001544, mae: 0.042291, mean_q: 1.200266
 27216/100000: episode: 370, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.448, mean reward: 0.594 [0.516, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.390, 10.098], loss: 0.001624, mae: 0.043261, mean_q: 1.197458
 27316/100000: episode: 371, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 58.074, mean reward: 0.581 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.904, 10.098], loss: 0.001779, mae: 0.044902, mean_q: 1.192671
 27416/100000: episode: 372, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.182, mean reward: 0.592 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.040, 10.141], loss: 0.001517, mae: 0.042725, mean_q: 1.186373
 27516/100000: episode: 373, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 59.370, mean reward: 0.594 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.739, 10.098], loss: 0.001531, mae: 0.042241, mean_q: 1.181943
 27616/100000: episode: 374, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.320, mean reward: 0.583 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.165, 10.200], loss: 0.001449, mae: 0.041100, mean_q: 1.177698
 27716/100000: episode: 375, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 56.393, mean reward: 0.564 [0.505, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.458, 10.098], loss: 0.001385, mae: 0.040739, mean_q: 1.170252
 27816/100000: episode: 376, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 61.891, mean reward: 0.619 [0.500, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.066, 10.182], loss: 0.001524, mae: 0.042352, mean_q: 1.165094
 27916/100000: episode: 377, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.633, mean reward: 0.586 [0.509, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.347, 10.098], loss: 0.001570, mae: 0.043390, mean_q: 1.162660
 28016/100000: episode: 378, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 61.095, mean reward: 0.611 [0.512, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.872, 10.395], loss: 0.001451, mae: 0.041703, mean_q: 1.165991
 28116/100000: episode: 379, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.522, mean reward: 0.595 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.324, 10.098], loss: 0.001500, mae: 0.041997, mean_q: 1.167558
 28216/100000: episode: 380, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.933, mean reward: 0.599 [0.503, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.960, 10.098], loss: 0.001605, mae: 0.043580, mean_q: 1.169678
 28316/100000: episode: 381, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 58.982, mean reward: 0.590 [0.518, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.926, 10.258], loss: 0.001463, mae: 0.040870, mean_q: 1.169235
 28416/100000: episode: 382, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.495, mean reward: 0.585 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.383, 10.098], loss: 0.001474, mae: 0.041330, mean_q: 1.166880
 28516/100000: episode: 383, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 59.502, mean reward: 0.595 [0.499, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.973, 10.191], loss: 0.001549, mae: 0.042740, mean_q: 1.164882
 28616/100000: episode: 384, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 60.152, mean reward: 0.602 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.588, 10.098], loss: 0.001542, mae: 0.043055, mean_q: 1.167820
 28716/100000: episode: 385, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.796, mean reward: 0.588 [0.503, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.703, 10.247], loss: 0.001500, mae: 0.042322, mean_q: 1.168828
 28816/100000: episode: 386, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.093, mean reward: 0.581 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.157, 10.098], loss: 0.001381, mae: 0.040541, mean_q: 1.165027
 28916/100000: episode: 387, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 59.283, mean reward: 0.593 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.719, 10.098], loss: 0.001510, mae: 0.042202, mean_q: 1.165649
 29016/100000: episode: 388, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 57.164, mean reward: 0.572 [0.503, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.901, 10.117], loss: 0.001614, mae: 0.043633, mean_q: 1.169674
 29116/100000: episode: 389, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.917, mean reward: 0.579 [0.510, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.738, 10.127], loss: 0.001493, mae: 0.041835, mean_q: 1.166024
 29216/100000: episode: 390, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 62.515, mean reward: 0.625 [0.501, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.832, 10.366], loss: 0.001570, mae: 0.043611, mean_q: 1.169617
 29316/100000: episode: 391, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 59.706, mean reward: 0.597 [0.511, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.284, 10.272], loss: 0.001527, mae: 0.042532, mean_q: 1.169071
 29416/100000: episode: 392, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.030, mean reward: 0.570 [0.508, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.846, 10.098], loss: 0.001639, mae: 0.044223, mean_q: 1.166444
 29516/100000: episode: 393, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.271, mean reward: 0.593 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.098], loss: 0.001590, mae: 0.043287, mean_q: 1.170455
 29616/100000: episode: 394, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 59.691, mean reward: 0.597 [0.513, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.928, 10.098], loss: 0.001514, mae: 0.042827, mean_q: 1.168478
 29716/100000: episode: 395, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 59.175, mean reward: 0.592 [0.501, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.784, 10.338], loss: 0.001481, mae: 0.042106, mean_q: 1.169479
 29816/100000: episode: 396, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 57.491, mean reward: 0.575 [0.511, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.445, 10.354], loss: 0.001440, mae: 0.041599, mean_q: 1.167448
 29916/100000: episode: 397, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.669, mean reward: 0.587 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.795, 10.335], loss: 0.001506, mae: 0.042271, mean_q: 1.167238
 30016/100000: episode: 398, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.128, mean reward: 0.591 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.751, 10.245], loss: 0.001506, mae: 0.042679, mean_q: 1.168704
 30116/100000: episode: 399, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 61.398, mean reward: 0.614 [0.513, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.917, 10.098], loss: 0.001455, mae: 0.041442, mean_q: 1.167543
 30216/100000: episode: 400, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.971, mean reward: 0.600 [0.505, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.947, 10.098], loss: 0.001575, mae: 0.043256, mean_q: 1.168463
 30316/100000: episode: 401, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 59.390, mean reward: 0.594 [0.500, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.809, 10.238], loss: 0.001507, mae: 0.042317, mean_q: 1.167896
 30416/100000: episode: 402, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 59.880, mean reward: 0.599 [0.502, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.259, 10.199], loss: 0.001481, mae: 0.041655, mean_q: 1.172415
 30516/100000: episode: 403, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 58.034, mean reward: 0.580 [0.509, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.427, 10.114], loss: 0.001511, mae: 0.042669, mean_q: 1.170853
 30616/100000: episode: 404, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.422, mean reward: 0.584 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.648, 10.203], loss: 0.001588, mae: 0.043342, mean_q: 1.167093
 30716/100000: episode: 405, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 56.514, mean reward: 0.565 [0.508, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.423, 10.112], loss: 0.001408, mae: 0.040728, mean_q: 1.171458
 30816/100000: episode: 406, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 62.249, mean reward: 0.622 [0.520, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.514, 10.098], loss: 0.001552, mae: 0.042804, mean_q: 1.170090
 30916/100000: episode: 407, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 56.810, mean reward: 0.568 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.989, 10.128], loss: 0.001558, mae: 0.043168, mean_q: 1.169237
 31016/100000: episode: 408, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.040, mean reward: 0.570 [0.512, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.131, 10.098], loss: 0.001480, mae: 0.041940, mean_q: 1.168635
 31116/100000: episode: 409, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.942, mean reward: 0.589 [0.499, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.447, 10.188], loss: 0.001540, mae: 0.042644, mean_q: 1.167858
 31216/100000: episode: 410, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.885, mean reward: 0.599 [0.507, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.772, 10.492], loss: 0.001539, mae: 0.042246, mean_q: 1.169193
 31316/100000: episode: 411, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 58.337, mean reward: 0.583 [0.505, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.953, 10.122], loss: 0.001464, mae: 0.041783, mean_q: 1.168622
 31416/100000: episode: 412, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.120, mean reward: 0.581 [0.505, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.665, 10.232], loss: 0.001544, mae: 0.043383, mean_q: 1.167606
 31516/100000: episode: 413, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 58.112, mean reward: 0.581 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.566, 10.127], loss: 0.001593, mae: 0.044384, mean_q: 1.168218
 31616/100000: episode: 414, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.925, mean reward: 0.579 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.354, 10.098], loss: 0.001507, mae: 0.041690, mean_q: 1.164850
 31716/100000: episode: 415, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.896, mean reward: 0.589 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.816, 10.244], loss: 0.001618, mae: 0.043310, mean_q: 1.165532
 31816/100000: episode: 416, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 60.299, mean reward: 0.603 [0.501, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.842, 10.098], loss: 0.001565, mae: 0.043268, mean_q: 1.167948
 31916/100000: episode: 417, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 58.678, mean reward: 0.587 [0.513, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.116, 10.098], loss: 0.001366, mae: 0.040630, mean_q: 1.167308
 32016/100000: episode: 418, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.713, mean reward: 0.597 [0.511, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.564, 10.098], loss: 0.001422, mae: 0.041261, mean_q: 1.164157
 32116/100000: episode: 419, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.446, mean reward: 0.594 [0.513, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.487, 10.303], loss: 0.001336, mae: 0.039956, mean_q: 1.164846
 32216/100000: episode: 420, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 60.099, mean reward: 0.601 [0.500, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.617, 10.098], loss: 0.001461, mae: 0.042173, mean_q: 1.167238
 32316/100000: episode: 421, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.068, mean reward: 0.571 [0.506, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.885, 10.211], loss: 0.001336, mae: 0.039445, mean_q: 1.166593
 32416/100000: episode: 422, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.496, mean reward: 0.595 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.877, 10.098], loss: 0.001670, mae: 0.044350, mean_q: 1.166959
 32516/100000: episode: 423, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.550, mean reward: 0.585 [0.504, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.129, 10.098], loss: 0.001592, mae: 0.043927, mean_q: 1.168964
 32616/100000: episode: 424, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.395, mean reward: 0.594 [0.497, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.172, 10.285], loss: 0.001477, mae: 0.041692, mean_q: 1.170611
 32716/100000: episode: 425, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.361, mean reward: 0.574 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.548, 10.106], loss: 0.001514, mae: 0.042257, mean_q: 1.167072
[Info] 1-TH LEVEL FOUND: 1.337253212928772, Considering 10/90 traces
 32816/100000: episode: 426, duration: 4.970s, episode steps: 100, steps per second: 20, episode reward: 57.792, mean reward: 0.578 [0.503, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.505, 10.098], loss: 0.001371, mae: 0.040923, mean_q: 1.168619
 32845/100000: episode: 427, duration: 0.173s, episode steps: 29, steps per second: 168, episode reward: 18.183, mean reward: 0.627 [0.533, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.690, 10.100], loss: 0.001382, mae: 0.040288, mean_q: 1.167872
 32874/100000: episode: 428, duration: 0.163s, episode steps: 29, steps per second: 177, episode reward: 19.944, mean reward: 0.688 [0.629, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.344, 10.100], loss: 0.001321, mae: 0.040179, mean_q: 1.170621
 32903/100000: episode: 429, duration: 0.169s, episode steps: 29, steps per second: 172, episode reward: 17.862, mean reward: 0.616 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.924, 10.199], loss: 0.001323, mae: 0.039972, mean_q: 1.164143
 32920/100000: episode: 430, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 11.294, mean reward: 0.664 [0.577, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.100, 10.100], loss: 0.001395, mae: 0.040766, mean_q: 1.167468
 32937/100000: episode: 431, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 11.651, mean reward: 0.685 [0.616, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.344, 10.100], loss: 0.001400, mae: 0.040290, mean_q: 1.164643
 32952/100000: episode: 432, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 10.152, mean reward: 0.677 [0.625, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.268, 10.100], loss: 0.001470, mae: 0.041550, mean_q: 1.166387
 32975/100000: episode: 433, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 15.335, mean reward: 0.667 [0.603, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.346, 10.370], loss: 0.001425, mae: 0.041433, mean_q: 1.164750
 33007/100000: episode: 434, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 22.460, mean reward: 0.702 [0.572, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.024, 10.100], loss: 0.001513, mae: 0.043293, mean_q: 1.168190
 33035/100000: episode: 435, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 18.383, mean reward: 0.657 [0.587, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.722, 10.100], loss: 0.001357, mae: 0.041491, mean_q: 1.175493
 33062/100000: episode: 436, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 18.621, mean reward: 0.690 [0.617, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.414, 10.100], loss: 0.001736, mae: 0.044150, mean_q: 1.169384
 33086/100000: episode: 437, duration: 0.149s, episode steps: 24, steps per second: 162, episode reward: 15.129, mean reward: 0.630 [0.559, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.072, 10.100], loss: 0.001461, mae: 0.040805, mean_q: 1.170377
 33103/100000: episode: 438, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 11.777, mean reward: 0.693 [0.627, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.444, 10.100], loss: 0.001356, mae: 0.040144, mean_q: 1.169817
 33144/100000: episode: 439, duration: 0.234s, episode steps: 41, steps per second: 175, episode reward: 27.097, mean reward: 0.661 [0.573, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.381, 10.100], loss: 0.001769, mae: 0.044844, mean_q: 1.168730
 33168/100000: episode: 440, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 18.342, mean reward: 0.764 [0.655, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.993, 10.100], loss: 0.001420, mae: 0.041314, mean_q: 1.176258
 33186/100000: episode: 441, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 13.808, mean reward: 0.767 [0.654, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.625, 10.100], loss: 0.001465, mae: 0.040379, mean_q: 1.174362
 33215/100000: episode: 442, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 19.230, mean reward: 0.663 [0.599, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.386, 10.100], loss: 0.001901, mae: 0.046418, mean_q: 1.170403
 33230/100000: episode: 443, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 10.477, mean reward: 0.698 [0.639, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.140, 10.100], loss: 0.001702, mae: 0.046942, mean_q: 1.183169
 33253/100000: episode: 444, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 16.299, mean reward: 0.709 [0.673, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.469, 10.406], loss: 0.001573, mae: 0.042556, mean_q: 1.174171
 33268/100000: episode: 445, duration: 0.096s, episode steps: 15, steps per second: 157, episode reward: 9.888, mean reward: 0.659 [0.621, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.171, 10.100], loss: 0.001721, mae: 0.043288, mean_q: 1.174960
 33309/100000: episode: 446, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 28.140, mean reward: 0.686 [0.595, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.018, 10.100], loss: 0.001651, mae: 0.042291, mean_q: 1.179196
 33338/100000: episode: 447, duration: 0.170s, episode steps: 29, steps per second: 171, episode reward: 18.013, mean reward: 0.621 [0.520, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.482, 10.100], loss: 0.002356, mae: 0.052097, mean_q: 1.183505
 33379/100000: episode: 448, duration: 0.248s, episode steps: 41, steps per second: 165, episode reward: 25.335, mean reward: 0.618 [0.507, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.185, 10.100], loss: 0.002231, mae: 0.050005, mean_q: 1.174060
 33396/100000: episode: 449, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 10.952, mean reward: 0.644 [0.570, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.100], loss: 0.001778, mae: 0.047488, mean_q: 1.175605
 33413/100000: episode: 450, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 11.810, mean reward: 0.695 [0.661, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.229, 10.100], loss: 0.001974, mae: 0.046117, mean_q: 1.175470
 33436/100000: episode: 451, duration: 0.142s, episode steps: 23, steps per second: 161, episode reward: 14.846, mean reward: 0.645 [0.594, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.433], loss: 0.001928, mae: 0.043717, mean_q: 1.177331
 33465/100000: episode: 452, duration: 0.168s, episode steps: 29, steps per second: 172, episode reward: 21.159, mean reward: 0.730 [0.637, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.155, 10.100], loss: 0.001516, mae: 0.042732, mean_q: 1.183401
 33483/100000: episode: 453, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 11.441, mean reward: 0.636 [0.592, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.403, 10.100], loss: 0.001459, mae: 0.042397, mean_q: 1.181638
 33512/100000: episode: 454, duration: 0.157s, episode steps: 29, steps per second: 184, episode reward: 18.369, mean reward: 0.633 [0.511, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.636, 10.100], loss: 0.001600, mae: 0.044092, mean_q: 1.184877
 33539/100000: episode: 455, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 17.735, mean reward: 0.657 [0.582, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.909, 10.100], loss: 0.001509, mae: 0.042420, mean_q: 1.179632
 33568/100000: episode: 456, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 18.079, mean reward: 0.623 [0.525, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.268, 10.100], loss: 0.001747, mae: 0.044094, mean_q: 1.186024
 33609/100000: episode: 457, duration: 0.232s, episode steps: 41, steps per second: 176, episode reward: 25.192, mean reward: 0.614 [0.527, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.571, 10.190], loss: 0.001463, mae: 0.042053, mean_q: 1.180568
 33638/100000: episode: 458, duration: 0.156s, episode steps: 29, steps per second: 185, episode reward: 18.104, mean reward: 0.624 [0.560, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.206, 10.100], loss: 0.001410, mae: 0.040675, mean_q: 1.168354
 33667/100000: episode: 459, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 18.677, mean reward: 0.644 [0.560, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.324, 10.100], loss: 0.001501, mae: 0.042739, mean_q: 1.185301
 33690/100000: episode: 460, duration: 0.137s, episode steps: 23, steps per second: 167, episode reward: 14.852, mean reward: 0.646 [0.523, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.223, 10.234], loss: 0.001606, mae: 0.043341, mean_q: 1.176869
 33714/100000: episode: 461, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 15.468, mean reward: 0.645 [0.578, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.218, 10.100], loss: 0.001840, mae: 0.043707, mean_q: 1.183428
 33732/100000: episode: 462, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 12.796, mean reward: 0.711 [0.619, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.525, 10.100], loss: 0.001604, mae: 0.043100, mean_q: 1.182789
 33750/100000: episode: 463, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 12.744, mean reward: 0.708 [0.564, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.323, 10.100], loss: 0.001996, mae: 0.047768, mean_q: 1.182842
 33777/100000: episode: 464, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 17.262, mean reward: 0.639 [0.530, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.488, 10.100], loss: 0.001732, mae: 0.044855, mean_q: 1.182650
 33805/100000: episode: 465, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 20.245, mean reward: 0.723 [0.612, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.306, 10.100], loss: 0.001657, mae: 0.043313, mean_q: 1.186052
 33822/100000: episode: 466, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 12.750, mean reward: 0.750 [0.668, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.201, 10.100], loss: 0.001810, mae: 0.045763, mean_q: 1.186177
 33851/100000: episode: 467, duration: 0.169s, episode steps: 29, steps per second: 172, episode reward: 19.091, mean reward: 0.658 [0.548, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.966, 10.100], loss: 0.001770, mae: 0.044748, mean_q: 1.189059
 33874/100000: episode: 468, duration: 0.141s, episode steps: 23, steps per second: 163, episode reward: 14.903, mean reward: 0.648 [0.579, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.966, 10.390], loss: 0.001373, mae: 0.041313, mean_q: 1.194532
 33897/100000: episode: 469, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 15.368, mean reward: 0.668 [0.621, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.322], loss: 0.001622, mae: 0.043496, mean_q: 1.185752
 33924/100000: episode: 470, duration: 0.162s, episode steps: 27, steps per second: 167, episode reward: 18.133, mean reward: 0.672 [0.582, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.599, 10.100], loss: 0.001582, mae: 0.042026, mean_q: 1.189992
 33947/100000: episode: 471, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 17.034, mean reward: 0.741 [0.672, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.299, 10.446], loss: 0.001672, mae: 0.044104, mean_q: 1.188777
 33964/100000: episode: 472, duration: 0.105s, episode steps: 17, steps per second: 161, episode reward: 10.371, mean reward: 0.610 [0.544, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.247, 10.100], loss: 0.001633, mae: 0.042607, mean_q: 1.183639
 33982/100000: episode: 473, duration: 0.140s, episode steps: 18, steps per second: 129, episode reward: 12.773, mean reward: 0.710 [0.641, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.289, 10.100], loss: 0.001572, mae: 0.043502, mean_q: 1.194498
 34011/100000: episode: 474, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 18.822, mean reward: 0.649 [0.531, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.715, 10.100], loss: 0.001874, mae: 0.046247, mean_q: 1.192783
 34026/100000: episode: 475, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 9.618, mean reward: 0.641 [0.559, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.236, 10.100], loss: 0.001779, mae: 0.044749, mean_q: 1.192429
 34049/100000: episode: 476, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 17.859, mean reward: 0.776 [0.675, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.491, 10.316], loss: 0.002100, mae: 0.046837, mean_q: 1.187988
 34072/100000: episode: 477, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 17.216, mean reward: 0.749 [0.695, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.255, 10.555], loss: 0.001998, mae: 0.047695, mean_q: 1.199776
 34096/100000: episode: 478, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 14.912, mean reward: 0.621 [0.518, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.665, 10.100], loss: 0.001669, mae: 0.044416, mean_q: 1.193880
 34128/100000: episode: 479, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 21.731, mean reward: 0.679 [0.591, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.159, 10.100], loss: 0.001678, mae: 0.043777, mean_q: 1.185935
 34156/100000: episode: 480, duration: 0.170s, episode steps: 28, steps per second: 165, episode reward: 16.827, mean reward: 0.601 [0.504, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.452, 10.122], loss: 0.002122, mae: 0.049556, mean_q: 1.198136
 34197/100000: episode: 481, duration: 0.327s, episode steps: 41, steps per second: 125, episode reward: 24.958, mean reward: 0.609 [0.512, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.055, 10.131], loss: 0.001952, mae: 0.046126, mean_q: 1.190966
 34226/100000: episode: 482, duration: 0.182s, episode steps: 29, steps per second: 159, episode reward: 18.832, mean reward: 0.649 [0.532, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.126, 10.100], loss: 0.001768, mae: 0.044639, mean_q: 1.195562
 34241/100000: episode: 483, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 10.214, mean reward: 0.681 [0.601, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.209, 10.100], loss: 0.001785, mae: 0.043852, mean_q: 1.184884
 34265/100000: episode: 484, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 15.439, mean reward: 0.643 [0.597, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.143, 10.100], loss: 0.001503, mae: 0.041224, mean_q: 1.192386
 34292/100000: episode: 485, duration: 0.164s, episode steps: 27, steps per second: 165, episode reward: 17.641, mean reward: 0.653 [0.585, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.802, 10.100], loss: 0.001718, mae: 0.046006, mean_q: 1.206176
 34324/100000: episode: 486, duration: 0.179s, episode steps: 32, steps per second: 178, episode reward: 19.257, mean reward: 0.602 [0.538, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.577, 10.125], loss: 0.001519, mae: 0.041872, mean_q: 1.194469
 34339/100000: episode: 487, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 9.805, mean reward: 0.654 [0.613, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.348, 10.100], loss: 0.001366, mae: 0.039963, mean_q: 1.187381
 34366/100000: episode: 488, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 17.403, mean reward: 0.645 [0.601, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.296, 10.100], loss: 0.001636, mae: 0.043201, mean_q: 1.196921
 34393/100000: episode: 489, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 16.068, mean reward: 0.595 [0.513, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.168, 10.100], loss: 0.001607, mae: 0.041542, mean_q: 1.191934
 34420/100000: episode: 490, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 17.142, mean reward: 0.635 [0.503, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.499, 10.130], loss: 0.001902, mae: 0.045888, mean_q: 1.193511
 34447/100000: episode: 491, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 18.624, mean reward: 0.690 [0.615, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.319, 10.100], loss: 0.001763, mae: 0.044535, mean_q: 1.195827
 34465/100000: episode: 492, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 12.891, mean reward: 0.716 [0.606, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.348, 10.100], loss: 0.001385, mae: 0.039710, mean_q: 1.197587
 34497/100000: episode: 493, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 20.359, mean reward: 0.636 [0.588, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.175, 10.100], loss: 0.001818, mae: 0.045026, mean_q: 1.201757
 34515/100000: episode: 494, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 11.896, mean reward: 0.661 [0.586, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.631, 10.100], loss: 0.001698, mae: 0.042697, mean_q: 1.198492
 34538/100000: episode: 495, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 17.591, mean reward: 0.765 [0.683, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.571, 10.488], loss: 0.001743, mae: 0.045054, mean_q: 1.193763
 34566/100000: episode: 496, duration: 0.184s, episode steps: 28, steps per second: 152, episode reward: 18.232, mean reward: 0.651 [0.583, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.126, 10.100], loss: 0.002504, mae: 0.052510, mean_q: 1.196998
 34583/100000: episode: 497, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 12.251, mean reward: 0.721 [0.629, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.253, 10.100], loss: 0.001962, mae: 0.047092, mean_q: 1.198993
 34600/100000: episode: 498, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 11.428, mean reward: 0.672 [0.599, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.154, 10.100], loss: 0.001692, mae: 0.044677, mean_q: 1.207750
 34629/100000: episode: 499, duration: 0.170s, episode steps: 29, steps per second: 170, episode reward: 18.039, mean reward: 0.622 [0.530, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.035, 10.100], loss: 0.001612, mae: 0.042746, mean_q: 1.202626
 34646/100000: episode: 500, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 11.880, mean reward: 0.699 [0.609, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.235, 10.100], loss: 0.001697, mae: 0.042437, mean_q: 1.195222
 34674/100000: episode: 501, duration: 0.163s, episode steps: 28, steps per second: 172, episode reward: 17.893, mean reward: 0.639 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.680, 10.251], loss: 0.001604, mae: 0.043924, mean_q: 1.205781
 34697/100000: episode: 502, duration: 0.137s, episode steps: 23, steps per second: 167, episode reward: 16.361, mean reward: 0.711 [0.652, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.396, 10.545], loss: 0.001979, mae: 0.046955, mean_q: 1.210866
 34729/100000: episode: 503, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 21.851, mean reward: 0.683 [0.602, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.529, 10.100], loss: 0.001721, mae: 0.043731, mean_q: 1.206938
 34753/100000: episode: 504, duration: 0.146s, episode steps: 24, steps per second: 164, episode reward: 15.234, mean reward: 0.635 [0.555, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.239, 10.100], loss: 0.001929, mae: 0.046695, mean_q: 1.208581
 34785/100000: episode: 505, duration: 0.189s, episode steps: 32, steps per second: 170, episode reward: 22.613, mean reward: 0.707 [0.620, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.388, 10.100], loss: 0.002131, mae: 0.047946, mean_q: 1.206911
 34812/100000: episode: 506, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 17.656, mean reward: 0.654 [0.589, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.542, 10.100], loss: 0.001971, mae: 0.046518, mean_q: 1.204978
 34844/100000: episode: 507, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 22.133, mean reward: 0.692 [0.612, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.537, 10.100], loss: 0.001728, mae: 0.043159, mean_q: 1.202465
 34862/100000: episode: 508, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 12.428, mean reward: 0.690 [0.609, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.748, 10.100], loss: 0.001999, mae: 0.048305, mean_q: 1.201432
 34889/100000: episode: 509, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 17.384, mean reward: 0.644 [0.556, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.333, 10.100], loss: 0.001744, mae: 0.045321, mean_q: 1.217587
 34916/100000: episode: 510, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 16.830, mean reward: 0.623 [0.580, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.788, 10.100], loss: 0.001859, mae: 0.046753, mean_q: 1.214009
 34934/100000: episode: 511, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 12.725, mean reward: 0.707 [0.596, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.353, 10.100], loss: 0.001795, mae: 0.045564, mean_q: 1.204839
 34975/100000: episode: 512, duration: 0.248s, episode steps: 41, steps per second: 165, episode reward: 27.859, mean reward: 0.679 [0.593, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.116, 10.100], loss: 0.002018, mae: 0.048264, mean_q: 1.210450
 35003/100000: episode: 513, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 18.841, mean reward: 0.673 [0.613, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.600, 10.100], loss: 0.001758, mae: 0.045910, mean_q: 1.221919
 35035/100000: episode: 514, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 20.400, mean reward: 0.637 [0.593, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.293, 10.100], loss: 0.001618, mae: 0.043833, mean_q: 1.218097
 35063/100000: episode: 515, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 17.378, mean reward: 0.621 [0.528, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.259, 10.100], loss: 0.001530, mae: 0.041888, mean_q: 1.207536
[Info] 2-TH LEVEL FOUND: 1.4721146821975708, Considering 10/90 traces
 35104/100000: episode: 516, duration: 4.613s, episode steps: 41, steps per second: 9, episode reward: 24.171, mean reward: 0.590 [0.531, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.035, 10.194], loss: 0.001907, mae: 0.045925, mean_q: 1.217610
 35116/100000: episode: 517, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 9.362, mean reward: 0.780 [0.747, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.549], loss: 0.001629, mae: 0.042847, mean_q: 1.209198
 35124/100000: episode: 518, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 6.124, mean reward: 0.766 [0.734, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.447, 10.100], loss: 0.001497, mae: 0.041916, mean_q: 1.214922
 35140/100000: episode: 519, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 11.211, mean reward: 0.701 [0.576, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-1.357, 10.394], loss: 0.001691, mae: 0.043769, mean_q: 1.221866
 35156/100000: episode: 520, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 12.461, mean reward: 0.779 [0.747, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.657, 10.100], loss: 0.001573, mae: 0.042447, mean_q: 1.217503
 35167/100000: episode: 521, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 8.007, mean reward: 0.728 [0.662, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.439, 10.100], loss: 0.001753, mae: 0.045302, mean_q: 1.219170
 35178/100000: episode: 522, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 8.214, mean reward: 0.747 [0.707, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.468, 10.100], loss: 0.001668, mae: 0.043139, mean_q: 1.221415
 35197/100000: episode: 523, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 13.534, mean reward: 0.712 [0.611, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.477, 10.402], loss: 0.001607, mae: 0.043036, mean_q: 1.220168
 35212/100000: episode: 524, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 11.529, mean reward: 0.769 [0.709, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.634], loss: 0.001931, mae: 0.047231, mean_q: 1.221534
 35221/100000: episode: 525, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 6.938, mean reward: 0.771 [0.720, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.557], loss: 0.001721, mae: 0.045032, mean_q: 1.210721
 35232/100000: episode: 526, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 7.979, mean reward: 0.725 [0.678, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.119, 10.100], loss: 0.001551, mae: 0.041870, mean_q: 1.218265
 35241/100000: episode: 527, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 6.995, mean reward: 0.777 [0.739, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.537], loss: 0.001920, mae: 0.045659, mean_q: 1.228498
 35252/100000: episode: 528, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 7.681, mean reward: 0.698 [0.649, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.456, 10.100], loss: 0.002268, mae: 0.050993, mean_q: 1.224522
 35263/100000: episode: 529, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 7.996, mean reward: 0.727 [0.697, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.500, 10.100], loss: 0.001820, mae: 0.045039, mean_q: 1.225452
 35275/100000: episode: 530, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 8.853, mean reward: 0.738 [0.631, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.472], loss: 0.001911, mae: 0.046200, mean_q: 1.211329
 35283/100000: episode: 531, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 6.099, mean reward: 0.762 [0.715, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.469, 10.100], loss: 0.002359, mae: 0.050653, mean_q: 1.222596
 35298/100000: episode: 532, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 10.962, mean reward: 0.731 [0.639, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.143, 10.402], loss: 0.001773, mae: 0.042204, mean_q: 1.223505
 35311/100000: episode: 533, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 9.469, mean reward: 0.728 [0.644, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.401, 10.100], loss: 0.002162, mae: 0.048333, mean_q: 1.226684
 35319/100000: episode: 534, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 6.378, mean reward: 0.797 [0.722, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.395, 10.100], loss: 0.001763, mae: 0.043253, mean_q: 1.204518
 35332/100000: episode: 535, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 10.910, mean reward: 0.839 [0.743, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.376, 10.100], loss: 0.001973, mae: 0.044690, mean_q: 1.211695
 35343/100000: episode: 536, duration: 0.070s, episode steps: 11, steps per second: 156, episode reward: 8.532, mean reward: 0.776 [0.723, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.312, 10.100], loss: 0.001858, mae: 0.046648, mean_q: 1.227197
 35354/100000: episode: 537, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 8.997, mean reward: 0.818 [0.756, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.519, 10.100], loss: 0.002020, mae: 0.047006, mean_q: 1.232047
 35365/100000: episode: 538, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 8.722, mean reward: 0.793 [0.759, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.449, 10.100], loss: 0.001934, mae: 0.045730, mean_q: 1.206518
 35378/100000: episode: 539, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 10.810, mean reward: 0.832 [0.759, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.515, 10.100], loss: 0.001966, mae: 0.048635, mean_q: 1.212304
 35389/100000: episode: 540, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 8.432, mean reward: 0.767 [0.684, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.507, 10.100], loss: 0.001744, mae: 0.043856, mean_q: 1.227174
 35408/100000: episode: 541, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 12.960, mean reward: 0.682 [0.623, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.572, 10.408], loss: 0.001973, mae: 0.048654, mean_q: 1.218445
 35427/100000: episode: 542, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 13.889, mean reward: 0.731 [0.667, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.378], loss: 0.001676, mae: 0.043549, mean_q: 1.234496
 35439/100000: episode: 543, duration: 0.077s, episode steps: 12, steps per second: 157, episode reward: 8.380, mean reward: 0.698 [0.629, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.532], loss: 0.001857, mae: 0.047030, mean_q: 1.236012
 35450/100000: episode: 544, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 8.234, mean reward: 0.749 [0.707, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.347, 10.100], loss: 0.001839, mae: 0.044840, mean_q: 1.230648
 35469/100000: episode: 545, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 13.699, mean reward: 0.721 [0.688, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.382, 10.427], loss: 0.001913, mae: 0.046523, mean_q: 1.227359
 35478/100000: episode: 546, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 6.742, mean reward: 0.749 [0.699, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.498], loss: 0.002399, mae: 0.050339, mean_q: 1.204922
 35491/100000: episode: 547, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 9.018, mean reward: 0.694 [0.645, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.409, 10.100], loss: 0.002051, mae: 0.049733, mean_q: 1.225117
 35500/100000: episode: 548, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 7.210, mean reward: 0.801 [0.714, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.511], loss: 0.002599, mae: 0.053362, mean_q: 1.215104
 35511/100000: episode: 549, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 7.877, mean reward: 0.716 [0.678, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.469, 10.100], loss: 0.001996, mae: 0.047590, mean_q: 1.231518
 35527/100000: episode: 550, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 13.848, mean reward: 0.866 [0.739, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.450, 10.100], loss: 0.002263, mae: 0.052463, mean_q: 1.231654
 35536/100000: episode: 551, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 6.827, mean reward: 0.759 [0.680, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.681, 10.458], loss: 0.002375, mae: 0.050932, mean_q: 1.222269
 35547/100000: episode: 552, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 7.855, mean reward: 0.714 [0.637, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.284, 10.100], loss: 0.001492, mae: 0.041816, mean_q: 1.237602
 35558/100000: episode: 553, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 8.568, mean reward: 0.779 [0.732, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.557, 10.100], loss: 0.001814, mae: 0.047101, mean_q: 1.240801
 35577/100000: episode: 554, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 12.670, mean reward: 0.667 [0.602, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.409], loss: 0.002217, mae: 0.050311, mean_q: 1.228877
 35593/100000: episode: 555, duration: 0.107s, episode steps: 16, steps per second: 150, episode reward: 11.973, mean reward: 0.748 [0.644, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.180, 10.490], loss: 0.002087, mae: 0.049937, mean_q: 1.237794
 35604/100000: episode: 556, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 8.884, mean reward: 0.808 [0.729, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.763, 10.100], loss: 0.001998, mae: 0.048992, mean_q: 1.233737
 35620/100000: episode: 557, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 11.377, mean reward: 0.711 [0.590, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-1.259, 10.253], loss: 0.002056, mae: 0.046895, mean_q: 1.234232
 35639/100000: episode: 558, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 14.388, mean reward: 0.757 [0.700, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.558], loss: 0.002001, mae: 0.048308, mean_q: 1.227327
 35655/100000: episode: 559, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 10.539, mean reward: 0.659 [0.624, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.095, 10.405], loss: 0.002278, mae: 0.049535, mean_q: 1.237190
 35663/100000: episode: 560, duration: 0.059s, episode steps: 8, steps per second: 135, episode reward: 6.214, mean reward: 0.777 [0.747, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.492, 10.100], loss: 0.002030, mae: 0.046387, mean_q: 1.239838
 35674/100000: episode: 561, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 7.970, mean reward: 0.725 [0.678, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.299, 10.100], loss: 0.002123, mae: 0.048980, mean_q: 1.241438
 35687/100000: episode: 562, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 10.195, mean reward: 0.784 [0.716, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.435, 10.100], loss: 0.002421, mae: 0.052174, mean_q: 1.229350
 35699/100000: episode: 563, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 8.671, mean reward: 0.723 [0.668, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.288], loss: 0.002327, mae: 0.052471, mean_q: 1.213143
 35710/100000: episode: 564, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 8.253, mean reward: 0.750 [0.708, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.416, 10.100], loss: 0.002801, mae: 0.053349, mean_q: 1.238033
 35729/100000: episode: 565, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 13.567, mean reward: 0.714 [0.626, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.500, 10.461], loss: 0.002126, mae: 0.050665, mean_q: 1.242321
 35745/100000: episode: 566, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 12.101, mean reward: 0.756 [0.699, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.436, 10.625], loss: 0.001850, mae: 0.045664, mean_q: 1.242441
 35760/100000: episode: 567, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 11.548, mean reward: 0.770 [0.608, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.142, 10.462], loss: 0.001909, mae: 0.046422, mean_q: 1.230892
 35776/100000: episode: 568, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 13.551, mean reward: 0.847 [0.777, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.375, 10.100], loss: 0.001798, mae: 0.047107, mean_q: 1.245776
 35787/100000: episode: 569, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 8.064, mean reward: 0.733 [0.669, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.333, 10.100], loss: 0.002322, mae: 0.052630, mean_q: 1.246408
 35800/100000: episode: 570, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 9.601, mean reward: 0.739 [0.696, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.131, 10.100], loss: 0.003072, mae: 0.057371, mean_q: 1.228331
 35815/100000: episode: 571, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 11.173, mean reward: 0.745 [0.681, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.436], loss: 0.001986, mae: 0.046813, mean_q: 1.246751
 35830/100000: episode: 572, duration: 0.093s, episode steps: 15, steps per second: 162, episode reward: 10.284, mean reward: 0.686 [0.596, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.326, 10.323], loss: 0.002054, mae: 0.048487, mean_q: 1.239859
 35843/100000: episode: 573, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 10.373, mean reward: 0.798 [0.726, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.475, 10.100], loss: 0.001907, mae: 0.046803, mean_q: 1.239181
 35862/100000: episode: 574, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 13.624, mean reward: 0.717 [0.604, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.535, 10.386], loss: 0.001584, mae: 0.041879, mean_q: 1.255764
 35873/100000: episode: 575, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 9.245, mean reward: 0.840 [0.755, 0.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.597, 10.100], loss: 0.001962, mae: 0.048875, mean_q: 1.248079
 35888/100000: episode: 576, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 11.278, mean reward: 0.752 [0.699, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.495, 10.456], loss: 0.002043, mae: 0.047542, mean_q: 1.245768
 35907/100000: episode: 577, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 13.046, mean reward: 0.687 [0.580, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.321, 10.267], loss: 0.002214, mae: 0.049070, mean_q: 1.246108
 35916/100000: episode: 578, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 6.607, mean reward: 0.734 [0.660, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.467], loss: 0.002281, mae: 0.049277, mean_q: 1.250991
 35925/100000: episode: 579, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 6.748, mean reward: 0.750 [0.729, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.517], loss: 0.001903, mae: 0.046799, mean_q: 1.240827
 35944/100000: episode: 580, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 13.456, mean reward: 0.708 [0.649, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.457], loss: 0.001809, mae: 0.047178, mean_q: 1.244678
 35956/100000: episode: 581, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 8.005, mean reward: 0.667 [0.626, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.362], loss: 0.001876, mae: 0.047623, mean_q: 1.244083
 35964/100000: episode: 582, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 5.701, mean reward: 0.713 [0.672, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.315, 10.100], loss: 0.002211, mae: 0.049495, mean_q: 1.255832
 35977/100000: episode: 583, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 9.962, mean reward: 0.766 [0.727, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.292, 10.100], loss: 0.002132, mae: 0.050964, mean_q: 1.266923
 35986/100000: episode: 584, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 6.832, mean reward: 0.759 [0.710, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.567], loss: 0.002048, mae: 0.048397, mean_q: 1.251715
 35994/100000: episode: 585, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 5.912, mean reward: 0.739 [0.679, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.876, 10.100], loss: 0.002078, mae: 0.047138, mean_q: 1.233707
 36010/100000: episode: 586, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 13.025, mean reward: 0.814 [0.763, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.487, 10.100], loss: 0.002081, mae: 0.047829, mean_q: 1.254116
 36029/100000: episode: 587, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 14.026, mean reward: 0.738 [0.672, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.447], loss: 0.002265, mae: 0.051524, mean_q: 1.256503
 36041/100000: episode: 588, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 8.179, mean reward: 0.682 [0.584, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.358], loss: 0.001812, mae: 0.045095, mean_q: 1.259760
 36052/100000: episode: 589, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 8.734, mean reward: 0.794 [0.742, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.287, 10.100], loss: 0.001809, mae: 0.044785, mean_q: 1.264048
 36064/100000: episode: 590, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 8.911, mean reward: 0.743 [0.694, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.495, 10.400], loss: 0.001828, mae: 0.046442, mean_q: 1.244515
 36073/100000: episode: 591, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 6.665, mean reward: 0.741 [0.705, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.494], loss: 0.002125, mae: 0.047644, mean_q: 1.245096
 36089/100000: episode: 592, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 12.571, mean reward: 0.786 [0.701, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-2.120, 10.100], loss: 0.001878, mae: 0.047761, mean_q: 1.261012
 36100/100000: episode: 593, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 8.606, mean reward: 0.782 [0.721, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.393, 10.100], loss: 0.002110, mae: 0.048600, mean_q: 1.242767
 36109/100000: episode: 594, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 6.825, mean reward: 0.758 [0.718, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.461], loss: 0.002090, mae: 0.050118, mean_q: 1.253334
 36120/100000: episode: 595, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 8.162, mean reward: 0.742 [0.726, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.456, 10.100], loss: 0.001984, mae: 0.049574, mean_q: 1.266218
 36132/100000: episode: 596, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 9.185, mean reward: 0.765 [0.709, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.514], loss: 0.002390, mae: 0.052592, mean_q: 1.269546
 36144/100000: episode: 597, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 9.252, mean reward: 0.771 [0.714, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.557], loss: 0.001923, mae: 0.047092, mean_q: 1.261473
 36152/100000: episode: 598, duration: 0.056s, episode steps: 8, steps per second: 143, episode reward: 5.643, mean reward: 0.705 [0.668, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.476, 10.100], loss: 0.001975, mae: 0.045195, mean_q: 1.236276
 36171/100000: episode: 599, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 14.675, mean reward: 0.772 [0.709, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.584], loss: 0.002131, mae: 0.050519, mean_q: 1.251250
 36184/100000: episode: 600, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 9.387, mean reward: 0.722 [0.673, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.377, 10.100], loss: 0.001924, mae: 0.047599, mean_q: 1.260790
 36192/100000: episode: 601, duration: 0.056s, episode steps: 8, steps per second: 144, episode reward: 5.868, mean reward: 0.734 [0.683, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.491, 10.100], loss: 0.002373, mae: 0.049761, mean_q: 1.252674
 36203/100000: episode: 602, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 8.432, mean reward: 0.767 [0.654, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.404, 10.100], loss: 0.001868, mae: 0.047528, mean_q: 1.267951
 36212/100000: episode: 603, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 7.004, mean reward: 0.778 [0.741, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.597], loss: 0.001780, mae: 0.043945, mean_q: 1.260014
 36225/100000: episode: 604, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 9.998, mean reward: 0.769 [0.680, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.291, 10.100], loss: 0.002127, mae: 0.048285, mean_q: 1.255994
 36244/100000: episode: 605, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 14.093, mean reward: 0.742 [0.666, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.479, 10.531], loss: 0.002103, mae: 0.049635, mean_q: 1.258075
[Info] 3-TH LEVEL FOUND: 1.5572919845581055, Considering 14/86 traces
 36253/100000: episode: 606, duration: 4.490s, episode steps: 9, steps per second: 2, episode reward: 7.174, mean reward: 0.797 [0.752, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.559], loss: 0.001814, mae: 0.044145, mean_q: 1.266156
 36264/100000: episode: 607, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 8.856, mean reward: 0.805 [0.669, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.456, 10.100], loss: 0.001462, mae: 0.043029, mean_q: 1.249123
 36275/100000: episode: 608, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 7.943, mean reward: 0.722 [0.694, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.459, 10.100], loss: 0.001506, mae: 0.043263, mean_q: 1.284751
 36286/100000: episode: 609, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 8.705, mean reward: 0.791 [0.740, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.437, 10.100], loss: 0.001787, mae: 0.045631, mean_q: 1.255755
 36297/100000: episode: 610, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 8.033, mean reward: 0.730 [0.670, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.788, 10.100], loss: 0.002039, mae: 0.048295, mean_q: 1.273620
 36308/100000: episode: 611, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 8.550, mean reward: 0.777 [0.734, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.337, 10.100], loss: 0.001905, mae: 0.048942, mean_q: 1.284404
 36319/100000: episode: 612, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 9.733, mean reward: 0.885 [0.795, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.911, 10.100], loss: 0.002074, mae: 0.049508, mean_q: 1.270471
 36330/100000: episode: 613, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 8.293, mean reward: 0.754 [0.727, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.426, 10.100], loss: 0.002548, mae: 0.054602, mean_q: 1.263140
 36341/100000: episode: 614, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 8.176, mean reward: 0.743 [0.636, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.445, 10.100], loss: 0.001660, mae: 0.044446, mean_q: 1.268051
 36352/100000: episode: 615, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 8.508, mean reward: 0.773 [0.730, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.840, 10.100], loss: 0.001589, mae: 0.044017, mean_q: 1.287786
 36363/100000: episode: 616, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 7.917, mean reward: 0.720 [0.660, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.539, 10.100], loss: 0.001689, mae: 0.044360, mean_q: 1.261512
[Info] FALSIFICATION!
 36364/100000: episode: 617, duration: 0.274s, episode steps: 1, steps per second: 4, episode reward: 1.026, mean reward: 1.026 [1.026, 1.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.121, 9.864], loss: 0.002663, mae: 0.062377, mean_q: 1.273154
 36375/100000: episode: 618, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 8.219, mean reward: 0.747 [0.676, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.442, 10.100], loss: 0.002258, mae: 0.049504, mean_q: 1.267355
 36385/100000: episode: 619, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 7.502, mean reward: 0.750 [0.698, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.329, 10.100], loss: 0.001966, mae: 0.047443, mean_q: 1.275098
 36396/100000: episode: 620, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 8.408, mean reward: 0.764 [0.726, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.336, 10.100], loss: 0.002434, mae: 0.055175, mean_q: 1.275656
 36407/100000: episode: 621, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 9.146, mean reward: 0.831 [0.698, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.550, 10.100], loss: 0.002285, mae: 0.051975, mean_q: 1.255780
 36418/100000: episode: 622, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 8.482, mean reward: 0.771 [0.665, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.554, 10.100], loss: 0.002245, mae: 0.048737, mean_q: 1.252923
 36429/100000: episode: 623, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 9.186, mean reward: 0.835 [0.741, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.428, 10.100], loss: 0.001742, mae: 0.044501, mean_q: 1.285676
 36440/100000: episode: 624, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 8.083, mean reward: 0.735 [0.680, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.901, 10.100], loss: 0.002584, mae: 0.056106, mean_q: 1.278020
 36451/100000: episode: 625, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 8.254, mean reward: 0.750 [0.679, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.266, 10.100], loss: 0.002141, mae: 0.048558, mean_q: 1.279191
 36462/100000: episode: 626, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 8.150, mean reward: 0.741 [0.687, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.434, 10.100], loss: 0.002237, mae: 0.052668, mean_q: 1.296577
 36473/100000: episode: 627, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 8.940, mean reward: 0.813 [0.706, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.377, 10.100], loss: 0.002352, mae: 0.050034, mean_q: 1.283630
 36484/100000: episode: 628, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 8.507, mean reward: 0.773 [0.745, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.341, 10.100], loss: 0.002380, mae: 0.051760, mean_q: 1.279025
 36495/100000: episode: 629, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 8.505, mean reward: 0.773 [0.722, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.448, 10.100], loss: 0.001915, mae: 0.048190, mean_q: 1.274718
 36504/100000: episode: 630, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 7.150, mean reward: 0.794 [0.750, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.631, 10.100], loss: 0.001916, mae: 0.046313, mean_q: 1.268016
 36515/100000: episode: 631, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 8.390, mean reward: 0.763 [0.703, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.850, 10.100], loss: 0.001999, mae: 0.047984, mean_q: 1.275875
 36526/100000: episode: 632, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 8.612, mean reward: 0.783 [0.767, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.414, 10.100], loss: 0.001742, mae: 0.044430, mean_q: 1.288426
 36535/100000: episode: 633, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 6.769, mean reward: 0.752 [0.641, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.519, 10.100], loss: 0.002376, mae: 0.049124, mean_q: 1.268784
 36546/100000: episode: 634, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 7.641, mean reward: 0.695 [0.645, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.336, 10.100], loss: 0.001730, mae: 0.045896, mean_q: 1.275992
 36557/100000: episode: 635, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 8.517, mean reward: 0.774 [0.669, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.434, 10.100], loss: 0.002333, mae: 0.051273, mean_q: 1.267621
 36568/100000: episode: 636, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 8.636, mean reward: 0.785 [0.732, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.654, 10.100], loss: 0.001803, mae: 0.047499, mean_q: 1.288983
 36579/100000: episode: 637, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 8.653, mean reward: 0.787 [0.724, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.282, 10.100], loss: 0.001950, mae: 0.047464, mean_q: 1.285188
 36590/100000: episode: 638, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 8.466, mean reward: 0.770 [0.710, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.267, 10.100], loss: 0.001848, mae: 0.046187, mean_q: 1.273774
 36601/100000: episode: 639, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 8.240, mean reward: 0.749 [0.717, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.564, 10.100], loss: 0.001863, mae: 0.048057, mean_q: 1.308356
 36611/100000: episode: 640, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 8.255, mean reward: 0.826 [0.773, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.469, 10.100], loss: 0.002066, mae: 0.048212, mean_q: 1.282560
 36622/100000: episode: 641, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 8.516, mean reward: 0.774 [0.727, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.674, 10.100], loss: 0.002742, mae: 0.054177, mean_q: 1.293816
 36633/100000: episode: 642, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 8.009, mean reward: 0.728 [0.677, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.410, 10.100], loss: 0.001988, mae: 0.047799, mean_q: 1.290061
 36644/100000: episode: 643, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 8.640, mean reward: 0.785 [0.745, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.265, 10.100], loss: 0.002195, mae: 0.052895, mean_q: 1.281665
 36655/100000: episode: 644, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 8.708, mean reward: 0.792 [0.710, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.477, 10.100], loss: 0.002572, mae: 0.053560, mean_q: 1.278982
 36666/100000: episode: 645, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 8.614, mean reward: 0.783 [0.732, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.324, 10.100], loss: 0.001838, mae: 0.047440, mean_q: 1.302204
 36677/100000: episode: 646, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 8.979, mean reward: 0.816 [0.706, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.554, 10.100], loss: 0.002239, mae: 0.051916, mean_q: 1.269991
 36688/100000: episode: 647, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 8.643, mean reward: 0.786 [0.740, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.534, 10.100], loss: 0.002204, mae: 0.048982, mean_q: 1.296480
 36699/100000: episode: 648, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 8.668, mean reward: 0.788 [0.743, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.411, 10.100], loss: 0.002057, mae: 0.047825, mean_q: 1.297601
 36710/100000: episode: 649, duration: 0.070s, episode steps: 11, steps per second: 156, episode reward: 8.669, mean reward: 0.788 [0.740, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.469, 10.100], loss: 0.001899, mae: 0.046655, mean_q: 1.292121
 36721/100000: episode: 650, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 7.448, mean reward: 0.677 [0.642, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.745, 10.100], loss: 0.002531, mae: 0.052653, mean_q: 1.278498
 36731/100000: episode: 651, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 7.970, mean reward: 0.797 [0.748, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.334, 10.100], loss: 0.002186, mae: 0.047293, mean_q: 1.295743
 36742/100000: episode: 652, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 8.727, mean reward: 0.793 [0.720, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.767, 10.100], loss: 0.002280, mae: 0.051700, mean_q: 1.287349
 36753/100000: episode: 653, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 7.840, mean reward: 0.713 [0.645, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.809, 10.100], loss: 0.002295, mae: 0.052666, mean_q: 1.273325
 36764/100000: episode: 654, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 8.694, mean reward: 0.790 [0.725, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.615, 10.100], loss: 0.001936, mae: 0.047261, mean_q: 1.308082
 36775/100000: episode: 655, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 7.801, mean reward: 0.709 [0.655, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.229, 10.100], loss: 0.002200, mae: 0.051287, mean_q: 1.278211
 36786/100000: episode: 656, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 8.511, mean reward: 0.774 [0.714, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.382, 10.100], loss: 0.001983, mae: 0.047873, mean_q: 1.277293
 36797/100000: episode: 657, duration: 0.080s, episode steps: 11, steps per second: 138, episode reward: 8.680, mean reward: 0.789 [0.678, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.530, 10.100], loss: 0.002112, mae: 0.049106, mean_q: 1.282824
 36808/100000: episode: 658, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 8.569, mean reward: 0.779 [0.685, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.779, 10.100], loss: 0.002109, mae: 0.049613, mean_q: 1.288043
 36819/100000: episode: 659, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 8.174, mean reward: 0.743 [0.680, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.423, 10.100], loss: 0.001958, mae: 0.046810, mean_q: 1.295079
 36830/100000: episode: 660, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 8.836, mean reward: 0.803 [0.765, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.468, 10.100], loss: 0.002165, mae: 0.049538, mean_q: 1.283852
 36841/100000: episode: 661, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 8.873, mean reward: 0.807 [0.772, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.465, 10.100], loss: 0.002596, mae: 0.054767, mean_q: 1.296033
 36852/100000: episode: 662, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 8.188, mean reward: 0.744 [0.704, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.018, 10.100], loss: 0.002851, mae: 0.058336, mean_q: 1.301722
[Info] FALSIFICATION!
 36853/100000: episode: 663, duration: 0.235s, episode steps: 1, steps per second: 4, episode reward: 1.030, mean reward: 1.030 [1.030, 1.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.137, 9.911], loss: 0.003546, mae: 0.065430, mean_q: 1.271575
 36864/100000: episode: 664, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 9.079, mean reward: 0.825 [0.760, 0.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.450, 10.100], loss: 0.002866, mae: 0.052521, mean_q: 1.310045
 36873/100000: episode: 665, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 6.992, mean reward: 0.777 [0.697, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.784, 10.100], loss: 0.002551, mae: 0.054369, mean_q: 1.300116
 36884/100000: episode: 666, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 8.935, mean reward: 0.812 [0.731, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.865, 10.100], loss: 0.002166, mae: 0.049070, mean_q: 1.317242
 36895/100000: episode: 667, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 9.829, mean reward: 0.894 [0.809, 0.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.428, 10.100], loss: 0.001841, mae: 0.045678, mean_q: 1.311810
 36906/100000: episode: 668, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 9.125, mean reward: 0.830 [0.760, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.519, 10.100], loss: 0.002244, mae: 0.048628, mean_q: 1.261207
 36917/100000: episode: 669, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 8.668, mean reward: 0.788 [0.735, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.258, 10.100], loss: 0.001774, mae: 0.045025, mean_q: 1.291835
 36928/100000: episode: 670, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 8.067, mean reward: 0.733 [0.675, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.344, 10.100], loss: 0.001925, mae: 0.047512, mean_q: 1.294906
 36939/100000: episode: 671, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 8.438, mean reward: 0.767 [0.737, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.331, 10.100], loss: 0.002024, mae: 0.047585, mean_q: 1.295367
 36950/100000: episode: 672, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 8.982, mean reward: 0.817 [0.733, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.066, 10.100], loss: 0.002211, mae: 0.047752, mean_q: 1.289480
 36961/100000: episode: 673, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 8.523, mean reward: 0.775 [0.731, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.563, 10.100], loss: 0.002234, mae: 0.048971, mean_q: 1.337273
 36972/100000: episode: 674, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 8.650, mean reward: 0.786 [0.734, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.555, 10.100], loss: 0.002006, mae: 0.048277, mean_q: 1.314314
 36983/100000: episode: 675, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 8.592, mean reward: 0.781 [0.734, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.357, 10.100], loss: 0.002233, mae: 0.046975, mean_q: 1.310006
 36994/100000: episode: 676, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 8.328, mean reward: 0.757 [0.714, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.414, 10.100], loss: 0.002915, mae: 0.053721, mean_q: 1.295184
 37005/100000: episode: 677, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 8.421, mean reward: 0.766 [0.709, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.330, 10.100], loss: 0.002253, mae: 0.051223, mean_q: 1.297781
 37016/100000: episode: 678, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 8.020, mean reward: 0.729 [0.666, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.386, 10.100], loss: 0.002040, mae: 0.050098, mean_q: 1.323802
 37025/100000: episode: 679, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 7.722, mean reward: 0.858 [0.737, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.545, 10.100], loss: 0.002092, mae: 0.049801, mean_q: 1.291505
 37036/100000: episode: 680, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 9.298, mean reward: 0.845 [0.776, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.478, 10.100], loss: 0.002054, mae: 0.049774, mean_q: 1.312520
 37047/100000: episode: 681, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 8.678, mean reward: 0.789 [0.735, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.415, 10.100], loss: 0.002360, mae: 0.053477, mean_q: 1.317070
 37056/100000: episode: 682, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 6.463, mean reward: 0.718 [0.683, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.811, 10.100], loss: 0.001986, mae: 0.047726, mean_q: 1.310896
 37067/100000: episode: 683, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 8.229, mean reward: 0.748 [0.722, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.294, 10.100], loss: 0.002646, mae: 0.055134, mean_q: 1.272910
 37076/100000: episode: 684, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 7.050, mean reward: 0.783 [0.751, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.493, 10.100], loss: 0.002136, mae: 0.050708, mean_q: 1.301852
 37085/100000: episode: 685, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 6.993, mean reward: 0.777 [0.737, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.389, 10.100], loss: 0.001909, mae: 0.047249, mean_q: 1.284326
 37096/100000: episode: 686, duration: 0.071s, episode steps: 11, steps per second: 156, episode reward: 8.469, mean reward: 0.770 [0.711, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.518, 10.100], loss: 0.002189, mae: 0.050379, mean_q: 1.320855
 37107/100000: episode: 687, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 8.455, mean reward: 0.769 [0.705, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.455, 10.100], loss: 0.001787, mae: 0.044587, mean_q: 1.310465
 37118/100000: episode: 688, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 8.215, mean reward: 0.747 [0.693, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.412, 10.100], loss: 0.002188, mae: 0.050692, mean_q: 1.308604
 37129/100000: episode: 689, duration: 0.077s, episode steps: 11, steps per second: 142, episode reward: 8.290, mean reward: 0.754 [0.710, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.427, 10.100], loss: 0.002125, mae: 0.050060, mean_q: 1.301303
 37140/100000: episode: 690, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 8.464, mean reward: 0.769 [0.738, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.479, 10.100], loss: 0.001938, mae: 0.046627, mean_q: 1.321013
 37149/100000: episode: 691, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 6.664, mean reward: 0.740 [0.712, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.687, 10.100], loss: 0.002070, mae: 0.048320, mean_q: 1.337653
[Info] Complete ISplit Iteration
[Info] Levels: [1.3372532, 1.4721147, 1.557292, 1.5739603]
[Info] Cond. Prob: [0.1, 0.1, 0.14, 0.07]
[Info] Error Prob: 9.800000000000004e-05

 37160/100000: episode: 692, duration: 4.661s, episode steps: 11, steps per second: 2, episode reward: 8.721, mean reward: 0.793 [0.743, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.464, 10.100], loss: 0.002212, mae: 0.049958, mean_q: 1.326585
 37260/100000: episode: 693, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 59.350, mean reward: 0.593 [0.500, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.991, 10.165], loss: 0.001918, mae: 0.047165, mean_q: 1.308990
 37360/100000: episode: 694, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.508, mean reward: 0.585 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.623, 10.311], loss: 0.002149, mae: 0.050339, mean_q: 1.308519
 37460/100000: episode: 695, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.767, mean reward: 0.598 [0.506, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.792, 10.257], loss: 0.002040, mae: 0.048383, mean_q: 1.313899
 37560/100000: episode: 696, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 58.717, mean reward: 0.587 [0.512, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.101, 10.098], loss: 0.002174, mae: 0.050304, mean_q: 1.311760
 37660/100000: episode: 697, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 57.867, mean reward: 0.579 [0.502, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.139, 10.298], loss: 0.002208, mae: 0.050072, mean_q: 1.313016
 37760/100000: episode: 698, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 58.010, mean reward: 0.580 [0.518, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.629, 10.214], loss: 0.002128, mae: 0.049286, mean_q: 1.307181
 37860/100000: episode: 699, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 60.276, mean reward: 0.603 [0.517, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.917, 10.103], loss: 0.002242, mae: 0.050510, mean_q: 1.308717
 37960/100000: episode: 700, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.266, mean reward: 0.573 [0.497, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.150, 10.170], loss: 0.002123, mae: 0.050144, mean_q: 1.306046
 38060/100000: episode: 701, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.083, mean reward: 0.571 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.721, 10.120], loss: 0.002015, mae: 0.049112, mean_q: 1.302096
 38160/100000: episode: 702, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 62.120, mean reward: 0.621 [0.508, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.510, 10.435], loss: 0.002048, mae: 0.048893, mean_q: 1.298006
 38260/100000: episode: 703, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.320, mean reward: 0.603 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.669, 10.098], loss: 0.001909, mae: 0.046650, mean_q: 1.294720
 38360/100000: episode: 704, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.734, mean reward: 0.587 [0.506, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.643, 10.207], loss: 0.002028, mae: 0.048829, mean_q: 1.296275
 38460/100000: episode: 705, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.565, mean reward: 0.586 [0.506, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.500, 10.098], loss: 0.002001, mae: 0.048803, mean_q: 1.298495
 38560/100000: episode: 706, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.988, mean reward: 0.580 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.600, 10.209], loss: 0.002073, mae: 0.048670, mean_q: 1.297349
 38660/100000: episode: 707, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 57.403, mean reward: 0.574 [0.502, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.627, 10.112], loss: 0.002134, mae: 0.049421, mean_q: 1.291605
 38760/100000: episode: 708, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.672, mean reward: 0.587 [0.513, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.206, 10.098], loss: 0.001927, mae: 0.047163, mean_q: 1.294691
 38860/100000: episode: 709, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.553, mean reward: 0.596 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.813, 10.215], loss: 0.001834, mae: 0.045935, mean_q: 1.273959
 38960/100000: episode: 710, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 63.828, mean reward: 0.638 [0.512, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.779, 10.485], loss: 0.001978, mae: 0.048235, mean_q: 1.283667
 39060/100000: episode: 711, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 59.610, mean reward: 0.596 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.623, 10.359], loss: 0.002081, mae: 0.048350, mean_q: 1.282248
 39160/100000: episode: 712, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 62.083, mean reward: 0.621 [0.534, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.122, 10.287], loss: 0.001918, mae: 0.046608, mean_q: 1.283248
 39260/100000: episode: 713, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 58.595, mean reward: 0.586 [0.498, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.831, 10.098], loss: 0.002063, mae: 0.048523, mean_q: 1.277124
 39360/100000: episode: 714, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 58.710, mean reward: 0.587 [0.507, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.232, 10.307], loss: 0.002184, mae: 0.049957, mean_q: 1.270019
 39460/100000: episode: 715, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.675, mean reward: 0.587 [0.508, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.515, 10.298], loss: 0.001996, mae: 0.048129, mean_q: 1.278504
 39560/100000: episode: 716, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 60.519, mean reward: 0.605 [0.509, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.532, 10.340], loss: 0.002010, mae: 0.048250, mean_q: 1.268157
 39660/100000: episode: 717, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 60.091, mean reward: 0.601 [0.517, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.782, 10.355], loss: 0.001985, mae: 0.046537, mean_q: 1.269665
 39760/100000: episode: 718, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 56.303, mean reward: 0.563 [0.501, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.764, 10.227], loss: 0.001925, mae: 0.046873, mean_q: 1.267041
 39860/100000: episode: 719, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.010, mean reward: 0.580 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.668, 10.244], loss: 0.002051, mae: 0.047785, mean_q: 1.264073
 39960/100000: episode: 720, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 59.711, mean reward: 0.597 [0.508, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.363, 10.353], loss: 0.002021, mae: 0.048574, mean_q: 1.263129
 40060/100000: episode: 721, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 59.028, mean reward: 0.590 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.526, 10.131], loss: 0.002052, mae: 0.048442, mean_q: 1.258049
 40160/100000: episode: 722, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.769, mean reward: 0.598 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.436, 10.098], loss: 0.001943, mae: 0.047477, mean_q: 1.255706
 40260/100000: episode: 723, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.783, mean reward: 0.598 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.470, 10.185], loss: 0.002164, mae: 0.048490, mean_q: 1.250102
 40360/100000: episode: 724, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 57.847, mean reward: 0.578 [0.511, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.674, 10.098], loss: 0.001914, mae: 0.046617, mean_q: 1.244128
 40460/100000: episode: 725, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 61.092, mean reward: 0.611 [0.503, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.333, 10.098], loss: 0.002143, mae: 0.048766, mean_q: 1.244105
 40560/100000: episode: 726, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.206, mean reward: 0.582 [0.505, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.666, 10.098], loss: 0.002060, mae: 0.047075, mean_q: 1.238274
 40660/100000: episode: 727, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.891, mean reward: 0.599 [0.501, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.507, 10.098], loss: 0.002088, mae: 0.047229, mean_q: 1.230498
 40760/100000: episode: 728, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.652, mean reward: 0.577 [0.510, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.379, 10.098], loss: 0.001896, mae: 0.046015, mean_q: 1.225677
 40860/100000: episode: 729, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 57.846, mean reward: 0.578 [0.503, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.639, 10.210], loss: 0.001983, mae: 0.047734, mean_q: 1.227044
 40960/100000: episode: 730, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.524, mean reward: 0.595 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.810, 10.271], loss: 0.001929, mae: 0.047090, mean_q: 1.225477
 41060/100000: episode: 731, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.344, mean reward: 0.573 [0.504, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.915, 10.098], loss: 0.001819, mae: 0.045459, mean_q: 1.223485
 41160/100000: episode: 732, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 59.717, mean reward: 0.597 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.675, 10.098], loss: 0.001799, mae: 0.044892, mean_q: 1.214855
 41260/100000: episode: 733, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 58.645, mean reward: 0.586 [0.505, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.697, 10.145], loss: 0.001579, mae: 0.043150, mean_q: 1.210803
 41360/100000: episode: 734, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.736, mean reward: 0.587 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.288, 10.489], loss: 0.001621, mae: 0.043348, mean_q: 1.203914
 41460/100000: episode: 735, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.523, mean reward: 0.585 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.749, 10.209], loss: 0.001629, mae: 0.044179, mean_q: 1.201680
 41560/100000: episode: 736, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 57.554, mean reward: 0.576 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.692, 10.192], loss: 0.001778, mae: 0.044107, mean_q: 1.197002
 41660/100000: episode: 737, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.070, mean reward: 0.581 [0.511, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.554, 10.162], loss: 0.001646, mae: 0.044006, mean_q: 1.188486
 41760/100000: episode: 738, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.654, mean reward: 0.607 [0.508, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.975, 10.098], loss: 0.001579, mae: 0.043040, mean_q: 1.187924
 41860/100000: episode: 739, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.466, mean reward: 0.575 [0.501, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.143, 10.098], loss: 0.001528, mae: 0.042276, mean_q: 1.183028
 41960/100000: episode: 740, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.805, mean reward: 0.578 [0.508, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.452, 10.098], loss: 0.001562, mae: 0.043227, mean_q: 1.175866
 42060/100000: episode: 741, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 59.470, mean reward: 0.595 [0.519, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.237], loss: 0.001386, mae: 0.041278, mean_q: 1.170753
 42160/100000: episode: 742, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 60.486, mean reward: 0.605 [0.518, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.750, 10.098], loss: 0.001437, mae: 0.041346, mean_q: 1.169109
 42260/100000: episode: 743, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.821, mean reward: 0.578 [0.505, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.303, 10.191], loss: 0.001487, mae: 0.041895, mean_q: 1.168390
 42360/100000: episode: 744, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.071, mean reward: 0.581 [0.500, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.723, 10.098], loss: 0.001418, mae: 0.041552, mean_q: 1.166048
 42460/100000: episode: 745, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.125, mean reward: 0.571 [0.509, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.025, 10.098], loss: 0.001556, mae: 0.043174, mean_q: 1.167307
 42560/100000: episode: 746, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.871, mean reward: 0.589 [0.502, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.431, 10.196], loss: 0.001414, mae: 0.041016, mean_q: 1.165701
 42660/100000: episode: 747, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 60.657, mean reward: 0.607 [0.512, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.187, 10.269], loss: 0.001410, mae: 0.040882, mean_q: 1.164999
 42760/100000: episode: 748, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 56.605, mean reward: 0.566 [0.500, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.516, 10.321], loss: 0.001285, mae: 0.039286, mean_q: 1.166008
 42860/100000: episode: 749, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 56.989, mean reward: 0.570 [0.504, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.942, 10.246], loss: 0.001374, mae: 0.040779, mean_q: 1.164480
 42960/100000: episode: 750, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 56.763, mean reward: 0.568 [0.507, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.519, 10.108], loss: 0.001330, mae: 0.040431, mean_q: 1.161655
 43060/100000: episode: 751, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 56.549, mean reward: 0.565 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.554, 10.100], loss: 0.001494, mae: 0.042101, mean_q: 1.164777
 43160/100000: episode: 752, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 59.322, mean reward: 0.593 [0.508, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.491, 10.244], loss: 0.001424, mae: 0.041545, mean_q: 1.165630
 43260/100000: episode: 753, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.908, mean reward: 0.579 [0.507, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.570, 10.098], loss: 0.001324, mae: 0.040230, mean_q: 1.164665
 43360/100000: episode: 754, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 60.735, mean reward: 0.607 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.978, 10.098], loss: 0.001467, mae: 0.042391, mean_q: 1.164536
 43460/100000: episode: 755, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 60.562, mean reward: 0.606 [0.514, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.715, 10.098], loss: 0.001434, mae: 0.041748, mean_q: 1.166997
 43560/100000: episode: 756, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 59.219, mean reward: 0.592 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.938, 10.249], loss: 0.001350, mae: 0.040692, mean_q: 1.164927
 43660/100000: episode: 757, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.523, mean reward: 0.575 [0.509, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.765, 10.098], loss: 0.001511, mae: 0.042732, mean_q: 1.165058
 43760/100000: episode: 758, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 59.408, mean reward: 0.594 [0.508, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.268, 10.098], loss: 0.001352, mae: 0.040419, mean_q: 1.163228
 43860/100000: episode: 759, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 62.366, mean reward: 0.624 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.920, 10.396], loss: 0.001329, mae: 0.040741, mean_q: 1.162491
 43960/100000: episode: 760, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.258, mean reward: 0.583 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.986, 10.170], loss: 0.001484, mae: 0.043165, mean_q: 1.165439
 44060/100000: episode: 761, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 61.267, mean reward: 0.613 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.450, 10.429], loss: 0.001404, mae: 0.041302, mean_q: 1.163903
 44160/100000: episode: 762, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.746, mean reward: 0.577 [0.502, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.155, 10.138], loss: 0.001460, mae: 0.042399, mean_q: 1.162279
 44260/100000: episode: 763, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 60.347, mean reward: 0.603 [0.504, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.685, 10.242], loss: 0.001363, mae: 0.040723, mean_q: 1.161875
 44360/100000: episode: 764, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.433, mean reward: 0.584 [0.516, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.920, 10.098], loss: 0.001353, mae: 0.041219, mean_q: 1.163730
 44460/100000: episode: 765, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.784, mean reward: 0.588 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.385, 10.210], loss: 0.001373, mae: 0.040924, mean_q: 1.161981
 44560/100000: episode: 766, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.015, mean reward: 0.590 [0.503, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.518, 10.098], loss: 0.001375, mae: 0.040871, mean_q: 1.162076
 44660/100000: episode: 767, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.775, mean reward: 0.608 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.241, 10.098], loss: 0.001319, mae: 0.040130, mean_q: 1.162327
 44760/100000: episode: 768, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 60.964, mean reward: 0.610 [0.506, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.273, 10.120], loss: 0.001342, mae: 0.040557, mean_q: 1.162587
 44860/100000: episode: 769, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 57.837, mean reward: 0.578 [0.502, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.287, 10.098], loss: 0.001535, mae: 0.043620, mean_q: 1.165654
 44960/100000: episode: 770, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 60.639, mean reward: 0.606 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.165, 10.098], loss: 0.001500, mae: 0.042717, mean_q: 1.164750
 45060/100000: episode: 771, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 57.890, mean reward: 0.579 [0.500, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.322, 10.098], loss: 0.001571, mae: 0.043457, mean_q: 1.163479
 45160/100000: episode: 772, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 56.606, mean reward: 0.566 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.904, 10.131], loss: 0.001448, mae: 0.040851, mean_q: 1.162212
 45260/100000: episode: 773, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 60.080, mean reward: 0.601 [0.507, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.971, 10.269], loss: 0.001358, mae: 0.040303, mean_q: 1.162457
 45360/100000: episode: 774, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 63.520, mean reward: 0.635 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.597, 10.360], loss: 0.001363, mae: 0.040423, mean_q: 1.166768
 45460/100000: episode: 775, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.374, mean reward: 0.594 [0.512, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.587, 10.209], loss: 0.001447, mae: 0.041946, mean_q: 1.169125
 45560/100000: episode: 776, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.247, mean reward: 0.602 [0.504, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.327, 10.243], loss: 0.001442, mae: 0.041265, mean_q: 1.168636
 45660/100000: episode: 777, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 57.882, mean reward: 0.579 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.655, 10.115], loss: 0.001369, mae: 0.040080, mean_q: 1.165677
 45760/100000: episode: 778, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.853, mean reward: 0.599 [0.516, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.280, 10.098], loss: 0.001525, mae: 0.042377, mean_q: 1.167752
 45860/100000: episode: 779, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.915, mean reward: 0.589 [0.498, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.423, 10.149], loss: 0.001467, mae: 0.042225, mean_q: 1.166820
 45960/100000: episode: 780, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.626, mean reward: 0.596 [0.511, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.416, 10.098], loss: 0.001365, mae: 0.040542, mean_q: 1.165326
 46060/100000: episode: 781, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.782, mean reward: 0.598 [0.501, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.896, 10.111], loss: 0.001552, mae: 0.042783, mean_q: 1.168939
 46160/100000: episode: 782, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.485, mean reward: 0.565 [0.510, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.340, 10.098], loss: 0.001510, mae: 0.042142, mean_q: 1.168876
 46260/100000: episode: 783, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.636, mean reward: 0.586 [0.506, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.241, 10.098], loss: 0.001362, mae: 0.040394, mean_q: 1.170849
 46360/100000: episode: 784, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 57.571, mean reward: 0.576 [0.506, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.114, 10.098], loss: 0.001419, mae: 0.040867, mean_q: 1.169962
 46460/100000: episode: 785, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.493, mean reward: 0.585 [0.510, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.477, 10.098], loss: 0.001611, mae: 0.043582, mean_q: 1.168903
 46560/100000: episode: 786, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 57.779, mean reward: 0.578 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.541, 10.098], loss: 0.001353, mae: 0.040306, mean_q: 1.167293
 46660/100000: episode: 787, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 61.372, mean reward: 0.614 [0.516, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.405, 10.098], loss: 0.001531, mae: 0.043091, mean_q: 1.166749
 46760/100000: episode: 788, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.929, mean reward: 0.589 [0.512, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.006, 10.098], loss: 0.001243, mae: 0.039201, mean_q: 1.166571
 46860/100000: episode: 789, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.845, mean reward: 0.588 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.888, 10.098], loss: 0.001435, mae: 0.041561, mean_q: 1.166473
 46960/100000: episode: 790, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.902, mean reward: 0.589 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.604, 10.248], loss: 0.001268, mae: 0.039021, mean_q: 1.167760
 47060/100000: episode: 791, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 57.255, mean reward: 0.573 [0.508, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.942, 10.098], loss: 0.001372, mae: 0.040558, mean_q: 1.167798
[Info] 1-TH LEVEL FOUND: 1.3668525218963623, Considering 10/90 traces
 47160/100000: episode: 792, duration: 4.933s, episode steps: 100, steps per second: 20, episode reward: 57.981, mean reward: 0.580 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.008, 10.304], loss: 0.001333, mae: 0.039967, mean_q: 1.166728
 47201/100000: episode: 793, duration: 0.242s, episode steps: 41, steps per second: 170, episode reward: 26.266, mean reward: 0.641 [0.515, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.419, 10.100], loss: 0.001393, mae: 0.041261, mean_q: 1.164608
 47241/100000: episode: 794, duration: 0.247s, episode steps: 40, steps per second: 162, episode reward: 25.444, mean reward: 0.636 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.994, 10.135], loss: 0.001354, mae: 0.041053, mean_q: 1.168523
 47284/100000: episode: 795, duration: 0.276s, episode steps: 43, steps per second: 156, episode reward: 30.840, mean reward: 0.717 [0.633, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.494, 10.443], loss: 0.001171, mae: 0.037571, mean_q: 1.168792
 47294/100000: episode: 796, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 6.534, mean reward: 0.653 [0.586, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-1.062, 10.416], loss: 0.001384, mae: 0.041988, mean_q: 1.166922
 47330/100000: episode: 797, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 22.820, mean reward: 0.634 [0.533, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.137, 10.344], loss: 0.001525, mae: 0.042336, mean_q: 1.167842
 47360/100000: episode: 798, duration: 0.192s, episode steps: 30, steps per second: 156, episode reward: 19.484, mean reward: 0.649 [0.542, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.402], loss: 0.001402, mae: 0.041076, mean_q: 1.173215
 47402/100000: episode: 799, duration: 0.255s, episode steps: 42, steps per second: 164, episode reward: 31.118, mean reward: 0.741 [0.671, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-2.000, 10.425], loss: 0.001341, mae: 0.040691, mean_q: 1.167317
 47412/100000: episode: 800, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 6.942, mean reward: 0.694 [0.674, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.458], loss: 0.001822, mae: 0.046218, mean_q: 1.181284
 47422/100000: episode: 801, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 6.800, mean reward: 0.680 [0.587, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.495], loss: 0.001571, mae: 0.041964, mean_q: 1.180974
 47464/100000: episode: 802, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 25.668, mean reward: 0.611 [0.526, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.851, 10.277], loss: 0.001733, mae: 0.044694, mean_q: 1.172092
 47505/100000: episode: 803, duration: 0.226s, episode steps: 41, steps per second: 182, episode reward: 25.827, mean reward: 0.630 [0.525, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.543, 10.171], loss: 0.001304, mae: 0.039520, mean_q: 1.173907
 47546/100000: episode: 804, duration: 0.231s, episode steps: 41, steps per second: 178, episode reward: 27.657, mean reward: 0.675 [0.604, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.677, 10.325], loss: 0.001428, mae: 0.041068, mean_q: 1.173092
 47589/100000: episode: 805, duration: 0.242s, episode steps: 43, steps per second: 178, episode reward: 29.982, mean reward: 0.697 [0.653, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-1.441, 10.425], loss: 0.001442, mae: 0.040916, mean_q: 1.173171
 47630/100000: episode: 806, duration: 0.226s, episode steps: 41, steps per second: 182, episode reward: 30.502, mean reward: 0.744 [0.661, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.226, 10.659], loss: 0.001289, mae: 0.038809, mean_q: 1.179159
 47637/100000: episode: 807, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 5.253, mean reward: 0.750 [0.708, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.357, 10.100], loss: 0.001184, mae: 0.038802, mean_q: 1.169471
 47679/100000: episode: 808, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 26.178, mean reward: 0.623 [0.527, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.117, 10.219], loss: 0.001494, mae: 0.041655, mean_q: 1.179513
 47689/100000: episode: 809, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 6.421, mean reward: 0.642 [0.576, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.149, 10.411], loss: 0.001283, mae: 0.038932, mean_q: 1.194455
 47725/100000: episode: 810, duration: 0.214s, episode steps: 36, steps per second: 168, episode reward: 24.000, mean reward: 0.667 [0.579, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.246, 10.451], loss: 0.001677, mae: 0.045052, mean_q: 1.188804
 47768/100000: episode: 811, duration: 0.241s, episode steps: 43, steps per second: 178, episode reward: 29.333, mean reward: 0.682 [0.594, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.146, 10.427], loss: 0.001334, mae: 0.038572, mean_q: 1.180030
 47794/100000: episode: 812, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 17.651, mean reward: 0.679 [0.613, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.258, 10.349], loss: 0.001595, mae: 0.042460, mean_q: 1.184430
 47804/100000: episode: 813, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 6.714, mean reward: 0.671 [0.593, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-1.715, 10.370], loss: 0.001514, mae: 0.042662, mean_q: 1.183193
 47830/100000: episode: 814, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 17.722, mean reward: 0.682 [0.609, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.389, 10.385], loss: 0.001523, mae: 0.043110, mean_q: 1.181157
 47837/100000: episode: 815, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 5.319, mean reward: 0.760 [0.700, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.307, 10.100], loss: 0.001449, mae: 0.038276, mean_q: 1.178747
 47863/100000: episode: 816, duration: 0.159s, episode steps: 26, steps per second: 164, episode reward: 19.113, mean reward: 0.735 [0.644, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.803, 10.537], loss: 0.001758, mae: 0.045240, mean_q: 1.192257
 47904/100000: episode: 817, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 27.785, mean reward: 0.678 [0.635, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.539, 10.411], loss: 0.001577, mae: 0.043024, mean_q: 1.190443
 47946/100000: episode: 818, duration: 0.231s, episode steps: 42, steps per second: 182, episode reward: 28.035, mean reward: 0.668 [0.578, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.416, 10.288], loss: 0.001516, mae: 0.042367, mean_q: 1.187327
 47988/100000: episode: 819, duration: 0.259s, episode steps: 42, steps per second: 162, episode reward: 26.621, mean reward: 0.634 [0.536, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.548, 10.225], loss: 0.001548, mae: 0.041398, mean_q: 1.188375
 48024/100000: episode: 820, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 21.850, mean reward: 0.607 [0.524, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.347, 10.438], loss: 0.001555, mae: 0.042388, mean_q: 1.195665
 48050/100000: episode: 821, duration: 0.159s, episode steps: 26, steps per second: 164, episode reward: 16.407, mean reward: 0.631 [0.552, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.214, 10.253], loss: 0.001462, mae: 0.039935, mean_q: 1.181132
 48057/100000: episode: 822, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 4.901, mean reward: 0.700 [0.682, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.593, 10.100], loss: 0.001983, mae: 0.044443, mean_q: 1.191225
 48064/100000: episode: 823, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 4.960, mean reward: 0.709 [0.683, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.304, 10.100], loss: 0.001815, mae: 0.045692, mean_q: 1.196186
 48105/100000: episode: 824, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 27.213, mean reward: 0.664 [0.566, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.927, 10.274], loss: 0.001635, mae: 0.043280, mean_q: 1.194165
 48135/100000: episode: 825, duration: 0.178s, episode steps: 30, steps per second: 168, episode reward: 19.344, mean reward: 0.645 [0.537, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.250], loss: 0.001391, mae: 0.039902, mean_q: 1.188409
 48177/100000: episode: 826, duration: 0.227s, episode steps: 42, steps per second: 185, episode reward: 29.851, mean reward: 0.711 [0.656, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.121, 10.395], loss: 0.001680, mae: 0.043293, mean_q: 1.194397
 48203/100000: episode: 827, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 17.100, mean reward: 0.658 [0.616, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.463, 10.459], loss: 0.001512, mae: 0.041556, mean_q: 1.198111
 48239/100000: episode: 828, duration: 0.212s, episode steps: 36, steps per second: 170, episode reward: 21.325, mean reward: 0.592 [0.519, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.060, 10.272], loss: 0.001529, mae: 0.041172, mean_q: 1.193941
 48278/100000: episode: 829, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 24.849, mean reward: 0.637 [0.594, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.509, 10.335], loss: 0.001842, mae: 0.043985, mean_q: 1.193168
 48321/100000: episode: 830, duration: 0.263s, episode steps: 43, steps per second: 163, episode reward: 27.082, mean reward: 0.630 [0.515, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.379, 10.152], loss: 0.001679, mae: 0.042919, mean_q: 1.193269
 48351/100000: episode: 831, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 17.755, mean reward: 0.592 [0.508, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.806, 10.236], loss: 0.001741, mae: 0.044399, mean_q: 1.201388
 48381/100000: episode: 832, duration: 0.185s, episode steps: 30, steps per second: 162, episode reward: 17.917, mean reward: 0.597 [0.520, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.961, 10.100], loss: 0.001734, mae: 0.043897, mean_q: 1.203735
 48391/100000: episode: 833, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 6.752, mean reward: 0.675 [0.621, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.458], loss: 0.001809, mae: 0.044617, mean_q: 1.198401
 48398/100000: episode: 834, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 4.857, mean reward: 0.694 [0.654, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.504, 10.100], loss: 0.001672, mae: 0.044464, mean_q: 1.208660
 48439/100000: episode: 835, duration: 0.243s, episode steps: 41, steps per second: 169, episode reward: 24.934, mean reward: 0.608 [0.506, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.094, 10.199], loss: 0.001632, mae: 0.042363, mean_q: 1.199072
 48469/100000: episode: 836, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 18.463, mean reward: 0.615 [0.526, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.839, 10.100], loss: 0.001705, mae: 0.043905, mean_q: 1.202405
 48509/100000: episode: 837, duration: 0.222s, episode steps: 40, steps per second: 180, episode reward: 26.357, mean reward: 0.659 [0.595, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.211, 10.248], loss: 0.001784, mae: 0.044535, mean_q: 1.200258
 48551/100000: episode: 838, duration: 0.243s, episode steps: 42, steps per second: 173, episode reward: 27.451, mean reward: 0.654 [0.512, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.177, 10.100], loss: 0.001600, mae: 0.041833, mean_q: 1.206078
 48592/100000: episode: 839, duration: 0.235s, episode steps: 41, steps per second: 174, episode reward: 24.955, mean reward: 0.609 [0.514, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.187, 10.170], loss: 0.001861, mae: 0.044627, mean_q: 1.200308
 48632/100000: episode: 840, duration: 0.219s, episode steps: 40, steps per second: 183, episode reward: 25.861, mean reward: 0.647 [0.562, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.223, 10.100], loss: 0.001703, mae: 0.042664, mean_q: 1.195162
 48672/100000: episode: 841, duration: 0.230s, episode steps: 40, steps per second: 174, episode reward: 28.385, mean reward: 0.710 [0.620, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-1.230, 10.522], loss: 0.001660, mae: 0.043697, mean_q: 1.202295
 48715/100000: episode: 842, duration: 0.249s, episode steps: 43, steps per second: 172, episode reward: 25.979, mean reward: 0.604 [0.515, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.733, 10.329], loss: 0.001660, mae: 0.042548, mean_q: 1.195812
 48755/100000: episode: 843, duration: 0.230s, episode steps: 40, steps per second: 174, episode reward: 26.431, mean reward: 0.661 [0.600, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.259, 10.399], loss: 0.002006, mae: 0.046215, mean_q: 1.201476
 48781/100000: episode: 844, duration: 0.163s, episode steps: 26, steps per second: 160, episode reward: 18.052, mean reward: 0.694 [0.577, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.577, 10.329], loss: 0.001514, mae: 0.041292, mean_q: 1.206363
 48807/100000: episode: 845, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 17.924, mean reward: 0.689 [0.621, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.490], loss: 0.001629, mae: 0.042983, mean_q: 1.203981
 48814/100000: episode: 846, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 4.891, mean reward: 0.699 [0.667, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.262, 10.100], loss: 0.001271, mae: 0.038514, mean_q: 1.184853
 48855/100000: episode: 847, duration: 0.232s, episode steps: 41, steps per second: 177, episode reward: 28.674, mean reward: 0.699 [0.605, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.110, 10.530], loss: 0.002006, mae: 0.046241, mean_q: 1.203376
 48891/100000: episode: 848, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 23.067, mean reward: 0.641 [0.544, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.649, 10.241], loss: 0.001660, mae: 0.042482, mean_q: 1.206978
 48934/100000: episode: 849, duration: 0.247s, episode steps: 43, steps per second: 174, episode reward: 28.689, mean reward: 0.667 [0.591, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.156, 10.370], loss: 0.001883, mae: 0.045779, mean_q: 1.206669
 48960/100000: episode: 850, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 18.063, mean reward: 0.695 [0.587, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.606], loss: 0.001858, mae: 0.044490, mean_q: 1.201795
 49002/100000: episode: 851, duration: 0.250s, episode steps: 42, steps per second: 168, episode reward: 25.205, mean reward: 0.600 [0.509, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.181, 10.100], loss: 0.001897, mae: 0.046065, mean_q: 1.211661
 49044/100000: episode: 852, duration: 0.249s, episode steps: 42, steps per second: 169, episode reward: 28.636, mean reward: 0.682 [0.560, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.503, 10.568], loss: 0.001650, mae: 0.042589, mean_q: 1.203216
 49074/100000: episode: 853, duration: 0.171s, episode steps: 30, steps per second: 176, episode reward: 18.212, mean reward: 0.607 [0.549, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.356, 10.214], loss: 0.001690, mae: 0.043994, mean_q: 1.211468
 49116/100000: episode: 854, duration: 0.259s, episode steps: 42, steps per second: 162, episode reward: 29.115, mean reward: 0.693 [0.593, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.953, 10.410], loss: 0.001533, mae: 0.041853, mean_q: 1.209430
 49126/100000: episode: 855, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 6.495, mean reward: 0.650 [0.584, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.426, 10.445], loss: 0.001570, mae: 0.042534, mean_q: 1.206171
 49166/100000: episode: 856, duration: 0.226s, episode steps: 40, steps per second: 177, episode reward: 25.341, mean reward: 0.634 [0.505, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.960, 10.168], loss: 0.001357, mae: 0.040483, mean_q: 1.210440
 49207/100000: episode: 857, duration: 0.240s, episode steps: 41, steps per second: 171, episode reward: 24.950, mean reward: 0.609 [0.505, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.134, 10.152], loss: 0.001581, mae: 0.041452, mean_q: 1.218525
 49233/100000: episode: 858, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 18.250, mean reward: 0.702 [0.593, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.043, 10.353], loss: 0.001802, mae: 0.044655, mean_q: 1.214809
 49274/100000: episode: 859, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 25.043, mean reward: 0.611 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.479, 10.228], loss: 0.001845, mae: 0.045311, mean_q: 1.210557
 49316/100000: episode: 860, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 26.686, mean reward: 0.635 [0.556, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.646, 10.296], loss: 0.001811, mae: 0.044925, mean_q: 1.215620
 49357/100000: episode: 861, duration: 0.231s, episode steps: 41, steps per second: 178, episode reward: 27.160, mean reward: 0.662 [0.544, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.097, 10.224], loss: 0.001465, mae: 0.041281, mean_q: 1.213413
 49387/100000: episode: 862, duration: 0.167s, episode steps: 30, steps per second: 179, episode reward: 17.905, mean reward: 0.597 [0.536, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.052, 10.166], loss: 0.001702, mae: 0.042941, mean_q: 1.212226
 49430/100000: episode: 863, duration: 0.251s, episode steps: 43, steps per second: 171, episode reward: 26.796, mean reward: 0.623 [0.548, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.743, 10.264], loss: 0.001545, mae: 0.042161, mean_q: 1.210800
 49437/100000: episode: 864, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 4.746, mean reward: 0.678 [0.645, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.189, 10.100], loss: 0.001757, mae: 0.044704, mean_q: 1.220471
 49479/100000: episode: 865, duration: 0.240s, episode steps: 42, steps per second: 175, episode reward: 28.246, mean reward: 0.673 [0.597, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.119, 10.308], loss: 0.001680, mae: 0.042280, mean_q: 1.211547
 49509/100000: episode: 866, duration: 0.182s, episode steps: 30, steps per second: 165, episode reward: 18.904, mean reward: 0.630 [0.542, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.743, 10.316], loss: 0.001548, mae: 0.041787, mean_q: 1.219495
 49548/100000: episode: 867, duration: 0.246s, episode steps: 39, steps per second: 159, episode reward: 24.233, mean reward: 0.621 [0.519, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.886, 10.319], loss: 0.001450, mae: 0.042001, mean_q: 1.218736
 49591/100000: episode: 868, duration: 0.253s, episode steps: 43, steps per second: 170, episode reward: 26.558, mean reward: 0.618 [0.514, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.666, 10.183], loss: 0.001771, mae: 0.043561, mean_q: 1.216589
 49634/100000: episode: 869, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 26.590, mean reward: 0.618 [0.558, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.872, 10.284], loss: 0.001842, mae: 0.044404, mean_q: 1.214087
 49674/100000: episode: 870, duration: 0.210s, episode steps: 40, steps per second: 190, episode reward: 25.616, mean reward: 0.640 [0.552, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.035, 10.264], loss: 0.001746, mae: 0.043056, mean_q: 1.216746
 49704/100000: episode: 871, duration: 0.161s, episode steps: 30, steps per second: 187, episode reward: 18.339, mean reward: 0.611 [0.527, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.204, 10.156], loss: 0.001777, mae: 0.044667, mean_q: 1.216084
 49744/100000: episode: 872, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 25.369, mean reward: 0.634 [0.514, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.106, 10.189], loss: 0.001749, mae: 0.045083, mean_q: 1.220877
 49774/100000: episode: 873, duration: 0.182s, episode steps: 30, steps per second: 165, episode reward: 17.810, mean reward: 0.594 [0.512, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.871, 10.100], loss: 0.001530, mae: 0.041898, mean_q: 1.209029
 49800/100000: episode: 874, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 17.342, mean reward: 0.667 [0.586, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.124, 10.322], loss: 0.001851, mae: 0.047790, mean_q: 1.224856
 49810/100000: episode: 875, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 6.918, mean reward: 0.692 [0.628, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.830, 10.438], loss: 0.002014, mae: 0.045962, mean_q: 1.222032
 49853/100000: episode: 876, duration: 0.258s, episode steps: 43, steps per second: 166, episode reward: 29.852, mean reward: 0.694 [0.564, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.409, 10.287], loss: 0.001482, mae: 0.040022, mean_q: 1.222410
 49883/100000: episode: 877, duration: 0.179s, episode steps: 30, steps per second: 168, episode reward: 18.505, mean reward: 0.617 [0.541, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.219], loss: 0.001457, mae: 0.041244, mean_q: 1.221082
 49913/100000: episode: 878, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 18.553, mean reward: 0.618 [0.537, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.394, 10.235], loss: 0.001572, mae: 0.040695, mean_q: 1.207245
 49952/100000: episode: 879, duration: 0.224s, episode steps: 39, steps per second: 174, episode reward: 26.568, mean reward: 0.681 [0.574, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.138, 10.367], loss: 0.001429, mae: 0.041005, mean_q: 1.228673
 49993/100000: episode: 880, duration: 0.244s, episode steps: 41, steps per second: 168, episode reward: 25.770, mean reward: 0.629 [0.508, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.176, 10.100], loss: 0.001373, mae: 0.040277, mean_q: 1.226264
 50035/100000: episode: 881, duration: 0.233s, episode steps: 42, steps per second: 181, episode reward: 28.403, mean reward: 0.676 [0.582, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.288, 10.384], loss: 0.001597, mae: 0.043019, mean_q: 1.221862
[Info] 2-TH LEVEL FOUND: 1.4510284662246704, Considering 10/90 traces
 50071/100000: episode: 882, duration: 4.475s, episode steps: 36, steps per second: 8, episode reward: 23.897, mean reward: 0.664 [0.578, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.511, 10.358], loss: 0.001898, mae: 0.044709, mean_q: 1.223016
 50086/100000: episode: 883, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 11.564, mean reward: 0.771 [0.715, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.496], loss: 0.001365, mae: 0.037933, mean_q: 1.236442
 50101/100000: episode: 884, duration: 0.092s, episode steps: 15, steps per second: 164, episode reward: 11.406, mean reward: 0.760 [0.703, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.374], loss: 0.001508, mae: 0.042544, mean_q: 1.220842
 50120/100000: episode: 885, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 13.148, mean reward: 0.692 [0.624, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.483], loss: 0.001467, mae: 0.040463, mean_q: 1.226204
 50146/100000: episode: 886, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 18.411, mean reward: 0.708 [0.635, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.465], loss: 0.001428, mae: 0.041758, mean_q: 1.227980
 50171/100000: episode: 887, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 19.143, mean reward: 0.766 [0.688, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.422, 10.590], loss: 0.001674, mae: 0.043163, mean_q: 1.230533
 50204/100000: episode: 888, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 20.845, mean reward: 0.632 [0.525, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.243], loss: 0.001625, mae: 0.042359, mean_q: 1.226246
 50229/100000: episode: 889, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 18.567, mean reward: 0.743 [0.644, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.512, 10.608], loss: 0.001694, mae: 0.043919, mean_q: 1.233605
 50262/100000: episode: 890, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 21.177, mean reward: 0.642 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.981, 10.196], loss: 0.001551, mae: 0.041932, mean_q: 1.231742
 50287/100000: episode: 891, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 16.486, mean reward: 0.659 [0.605, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.539, 10.325], loss: 0.001580, mae: 0.042257, mean_q: 1.234373
 50302/100000: episode: 892, duration: 0.078s, episode steps: 15, steps per second: 191, episode reward: 10.914, mean reward: 0.728 [0.677, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.548, 10.415], loss: 0.001699, mae: 0.042598, mean_q: 1.244297
 50328/100000: episode: 893, duration: 0.139s, episode steps: 26, steps per second: 186, episode reward: 20.220, mean reward: 0.778 [0.615, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.395], loss: 0.001582, mae: 0.043346, mean_q: 1.225902
 50346/100000: episode: 894, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 12.155, mean reward: 0.675 [0.631, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.379], loss: 0.001307, mae: 0.039093, mean_q: 1.241051
 50371/100000: episode: 895, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 17.361, mean reward: 0.694 [0.617, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.389], loss: 0.001471, mae: 0.041384, mean_q: 1.240300
 50395/100000: episode: 896, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 18.820, mean reward: 0.784 [0.690, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.018, 10.553], loss: 0.001511, mae: 0.039706, mean_q: 1.228528
 50428/100000: episode: 897, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 21.378, mean reward: 0.648 [0.557, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.559, 10.325], loss: 0.001462, mae: 0.041821, mean_q: 1.235749
 50446/100000: episode: 898, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 14.247, mean reward: 0.791 [0.693, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.650, 10.484], loss: 0.001722, mae: 0.043152, mean_q: 1.225005
 50465/100000: episode: 899, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 15.465, mean reward: 0.814 [0.688, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.646, 10.591], loss: 0.001477, mae: 0.040716, mean_q: 1.238136
 50480/100000: episode: 900, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 10.973, mean reward: 0.732 [0.635, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.410], loss: 0.001370, mae: 0.041237, mean_q: 1.242776
 50504/100000: episode: 901, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 17.980, mean reward: 0.749 [0.655, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.457, 10.394], loss: 0.001682, mae: 0.042744, mean_q: 1.227762
 50519/100000: episode: 902, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 11.139, mean reward: 0.743 [0.580, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.438], loss: 0.001358, mae: 0.040250, mean_q: 1.234337
 50543/100000: episode: 903, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 19.035, mean reward: 0.793 [0.687, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.097, 10.628], loss: 0.001734, mae: 0.044292, mean_q: 1.243017
 50567/100000: episode: 904, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 17.297, mean reward: 0.721 [0.666, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.150, 10.520], loss: 0.001593, mae: 0.042626, mean_q: 1.245917
 50593/100000: episode: 905, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 18.288, mean reward: 0.703 [0.655, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.408, 10.476], loss: 0.001661, mae: 0.043463, mean_q: 1.239353
 50615/100000: episode: 906, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 16.001, mean reward: 0.727 [0.669, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.327, 10.346], loss: 0.001764, mae: 0.045949, mean_q: 1.240587
 50641/100000: episode: 907, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 18.770, mean reward: 0.722 [0.582, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.105, 10.626], loss: 0.001478, mae: 0.041244, mean_q: 1.247237
 50665/100000: episode: 908, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 17.413, mean reward: 0.726 [0.664, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.859, 10.484], loss: 0.001563, mae: 0.042698, mean_q: 1.234834
 50689/100000: episode: 909, duration: 0.142s, episode steps: 24, steps per second: 170, episode reward: 16.603, mean reward: 0.692 [0.623, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.394, 10.407], loss: 0.001403, mae: 0.041515, mean_q: 1.253331
 50708/100000: episode: 910, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 12.177, mean reward: 0.641 [0.567, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.306], loss: 0.001766, mae: 0.045664, mean_q: 1.251931
 50734/100000: episode: 911, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 18.451, mean reward: 0.710 [0.664, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.130, 10.448], loss: 0.001598, mae: 0.042598, mean_q: 1.251225
 50767/100000: episode: 912, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 21.051, mean reward: 0.638 [0.545, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.301], loss: 0.001502, mae: 0.041450, mean_q: 1.243746
 50778/100000: episode: 913, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 7.723, mean reward: 0.702 [0.647, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.436], loss: 0.001586, mae: 0.043124, mean_q: 1.257063
 50804/100000: episode: 914, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 18.775, mean reward: 0.722 [0.627, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.677, 10.356], loss: 0.001708, mae: 0.043599, mean_q: 1.251662
 50828/100000: episode: 915, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 19.945, mean reward: 0.831 [0.699, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.893, 10.702], loss: 0.001727, mae: 0.044732, mean_q: 1.244655
 50846/100000: episode: 916, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 13.757, mean reward: 0.764 [0.711, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.332, 10.495], loss: 0.001663, mae: 0.043222, mean_q: 1.245596
 50879/100000: episode: 917, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 21.271, mean reward: 0.645 [0.525, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.344, 10.196], loss: 0.001582, mae: 0.042108, mean_q: 1.255947
 50897/100000: episode: 918, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 12.878, mean reward: 0.715 [0.661, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.288, 10.399], loss: 0.001234, mae: 0.038525, mean_q: 1.251146
 50922/100000: episode: 919, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 17.400, mean reward: 0.696 [0.623, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.826, 10.236], loss: 0.001477, mae: 0.041275, mean_q: 1.264526
 50940/100000: episode: 920, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 13.719, mean reward: 0.762 [0.688, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.571], loss: 0.001545, mae: 0.042042, mean_q: 1.255888
 50964/100000: episode: 921, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 18.416, mean reward: 0.767 [0.650, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.517], loss: 0.001463, mae: 0.040388, mean_q: 1.260739
 50988/100000: episode: 922, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 17.762, mean reward: 0.740 [0.658, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.423], loss: 0.001385, mae: 0.040938, mean_q: 1.260077
 51003/100000: episode: 923, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 10.583, mean reward: 0.706 [0.639, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.404, 10.377], loss: 0.001811, mae: 0.046291, mean_q: 1.252379
 51027/100000: episode: 924, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 16.752, mean reward: 0.698 [0.600, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.291, 10.322], loss: 0.001861, mae: 0.048042, mean_q: 1.249466
 51045/100000: episode: 925, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 11.187, mean reward: 0.621 [0.523, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.181], loss: 0.001403, mae: 0.040708, mean_q: 1.264037
 51063/100000: episode: 926, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 12.864, mean reward: 0.715 [0.643, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.445], loss: 0.001249, mae: 0.039335, mean_q: 1.259409
 51089/100000: episode: 927, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 17.524, mean reward: 0.674 [0.548, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.872, 10.270], loss: 0.001416, mae: 0.040601, mean_q: 1.256666
 51115/100000: episode: 928, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 16.846, mean reward: 0.648 [0.522, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.725, 10.224], loss: 0.001471, mae: 0.041759, mean_q: 1.253149
 51140/100000: episode: 929, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 18.300, mean reward: 0.732 [0.688, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.265, 10.442], loss: 0.001712, mae: 0.044426, mean_q: 1.262307
 51165/100000: episode: 930, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 19.122, mean reward: 0.765 [0.668, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.497, 10.531], loss: 0.001428, mae: 0.041265, mean_q: 1.269623
 51191/100000: episode: 931, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 18.358, mean reward: 0.706 [0.645, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.522, 10.344], loss: 0.001496, mae: 0.041875, mean_q: 1.268798
 51224/100000: episode: 932, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 22.706, mean reward: 0.688 [0.601, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.035, 10.266], loss: 0.001331, mae: 0.039961, mean_q: 1.253104
 51242/100000: episode: 933, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 12.635, mean reward: 0.702 [0.632, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.430, 10.435], loss: 0.001687, mae: 0.044466, mean_q: 1.276160
 51275/100000: episode: 934, duration: 0.229s, episode steps: 33, steps per second: 144, episode reward: 21.837, mean reward: 0.662 [0.586, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.035, 10.329], loss: 0.001367, mae: 0.040354, mean_q: 1.279160
 51300/100000: episode: 935, duration: 0.162s, episode steps: 25, steps per second: 155, episode reward: 18.684, mean reward: 0.747 [0.651, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.402], loss: 0.001383, mae: 0.041130, mean_q: 1.272137
 51333/100000: episode: 936, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: 23.099, mean reward: 0.700 [0.612, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.690, 10.365], loss: 0.001457, mae: 0.041171, mean_q: 1.276310
 51357/100000: episode: 937, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 16.286, mean reward: 0.679 [0.580, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.469, 10.307], loss: 0.001749, mae: 0.043683, mean_q: 1.272839
 51379/100000: episode: 938, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 15.814, mean reward: 0.719 [0.682, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.517, 10.510], loss: 0.001592, mae: 0.042367, mean_q: 1.267024
 51401/100000: episode: 939, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 16.440, mean reward: 0.747 [0.669, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.625, 10.349], loss: 0.001302, mae: 0.039974, mean_q: 1.280550
 51434/100000: episode: 940, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 23.805, mean reward: 0.721 [0.647, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.469, 10.435], loss: 0.001417, mae: 0.040411, mean_q: 1.278850
 51456/100000: episode: 941, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 16.588, mean reward: 0.754 [0.707, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.364, 10.507], loss: 0.001355, mae: 0.040220, mean_q: 1.281468
 51471/100000: episode: 942, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 10.158, mean reward: 0.677 [0.618, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.340], loss: 0.001570, mae: 0.043339, mean_q: 1.277127
 51482/100000: episode: 943, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 8.061, mean reward: 0.733 [0.698, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.409], loss: 0.001287, mae: 0.039759, mean_q: 1.274240
 51500/100000: episode: 944, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 13.976, mean reward: 0.776 [0.690, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.522, 10.545], loss: 0.001419, mae: 0.040499, mean_q: 1.293694
 51524/100000: episode: 945, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 15.940, mean reward: 0.664 [0.587, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.521, 10.302], loss: 0.001409, mae: 0.040588, mean_q: 1.279999
 51543/100000: episode: 946, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 13.367, mean reward: 0.704 [0.642, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.351], loss: 0.002004, mae: 0.049261, mean_q: 1.279313
 51567/100000: episode: 947, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 16.747, mean reward: 0.698 [0.602, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.274, 10.389], loss: 0.001949, mae: 0.048011, mean_q: 1.284906
 51600/100000: episode: 948, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 22.539, mean reward: 0.683 [0.614, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.503], loss: 0.001295, mae: 0.039473, mean_q: 1.295116
 51624/100000: episode: 949, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 16.711, mean reward: 0.696 [0.540, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.310], loss: 0.001369, mae: 0.040014, mean_q: 1.279924
 51635/100000: episode: 950, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 7.852, mean reward: 0.714 [0.659, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.445], loss: 0.001364, mae: 0.040295, mean_q: 1.297784
 51646/100000: episode: 951, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 7.941, mean reward: 0.722 [0.665, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.418], loss: 0.001399, mae: 0.040223, mean_q: 1.301158
 51679/100000: episode: 952, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 23.201, mean reward: 0.703 [0.638, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.827, 10.423], loss: 0.001295, mae: 0.039778, mean_q: 1.281636
 51701/100000: episode: 953, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 16.109, mean reward: 0.732 [0.683, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.202, 10.490], loss: 0.001487, mae: 0.041385, mean_q: 1.286230
 51725/100000: episode: 954, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 17.376, mean reward: 0.724 [0.642, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.376], loss: 0.001568, mae: 0.042400, mean_q: 1.297078
 51751/100000: episode: 955, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 20.091, mean reward: 0.773 [0.659, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.224, 10.601], loss: 0.001565, mae: 0.043093, mean_q: 1.295368
 51775/100000: episode: 956, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 17.086, mean reward: 0.712 [0.634, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.471, 10.370], loss: 0.001387, mae: 0.041033, mean_q: 1.298203
 51794/100000: episode: 957, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 13.705, mean reward: 0.721 [0.626, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.363], loss: 0.001570, mae: 0.042612, mean_q: 1.288639
 51813/100000: episode: 958, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 13.901, mean reward: 0.732 [0.648, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.505], loss: 0.001458, mae: 0.042282, mean_q: 1.303242
 51831/100000: episode: 959, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 11.595, mean reward: 0.644 [0.555, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.333], loss: 0.001350, mae: 0.040729, mean_q: 1.294073
 51856/100000: episode: 960, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 16.043, mean reward: 0.642 [0.557, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.588, 10.278], loss: 0.001874, mae: 0.049074, mean_q: 1.297321
 51880/100000: episode: 961, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 16.126, mean reward: 0.672 [0.618, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.065, 10.370], loss: 0.001494, mae: 0.042224, mean_q: 1.295899
 51906/100000: episode: 962, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 19.951, mean reward: 0.767 [0.646, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.584], loss: 0.001345, mae: 0.039825, mean_q: 1.299642
 51939/100000: episode: 963, duration: 0.189s, episode steps: 33, steps per second: 175, episode reward: 20.308, mean reward: 0.615 [0.545, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.713, 10.309], loss: 0.001510, mae: 0.042003, mean_q: 1.298430
 51950/100000: episode: 964, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 7.968, mean reward: 0.724 [0.678, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.509, 10.524], loss: 0.001226, mae: 0.037657, mean_q: 1.310649
 51968/100000: episode: 965, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 12.843, mean reward: 0.714 [0.667, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.129, 10.529], loss: 0.001303, mae: 0.039614, mean_q: 1.308383
 51986/100000: episode: 966, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 13.124, mean reward: 0.729 [0.682, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.205, 10.455], loss: 0.001185, mae: 0.037590, mean_q: 1.313497
 52010/100000: episode: 967, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 16.390, mean reward: 0.683 [0.586, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.663, 10.365], loss: 0.001333, mae: 0.041279, mean_q: 1.290570
 52036/100000: episode: 968, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 19.279, mean reward: 0.741 [0.583, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.414], loss: 0.001697, mae: 0.045917, mean_q: 1.291300
 52060/100000: episode: 969, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 16.538, mean reward: 0.689 [0.579, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.711, 10.302], loss: 0.001331, mae: 0.040743, mean_q: 1.296355
 52082/100000: episode: 970, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 15.903, mean reward: 0.723 [0.665, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.577, 10.530], loss: 0.001506, mae: 0.042510, mean_q: 1.314034
 52104/100000: episode: 971, duration: 0.125s, episode steps: 22, steps per second: 175, episode reward: 16.361, mean reward: 0.744 [0.622, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.064, 10.387], loss: 0.001277, mae: 0.039598, mean_q: 1.306191
[Info] 3-TH LEVEL FOUND: 1.595839500427246, Considering 10/90 traces
 52137/100000: episode: 972, duration: 4.628s, episode steps: 33, steps per second: 7, episode reward: 20.003, mean reward: 0.606 [0.509, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.104], loss: 0.001528, mae: 0.043184, mean_q: 1.298966
 52155/100000: episode: 973, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 13.759, mean reward: 0.764 [0.663, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.150, 10.527], loss: 0.001308, mae: 0.039833, mean_q: 1.308719
 52162/100000: episode: 974, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 5.833, mean reward: 0.833 [0.805, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.589], loss: 0.001990, mae: 0.049914, mean_q: 1.280977
 52180/100000: episode: 975, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 14.050, mean reward: 0.781 [0.703, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.563, 10.511], loss: 0.001396, mae: 0.040407, mean_q: 1.312015
 52195/100000: episode: 976, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 12.533, mean reward: 0.836 [0.739, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.089, 10.572], loss: 0.001512, mae: 0.042092, mean_q: 1.306576
 52210/100000: episode: 977, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 12.453, mean reward: 0.830 [0.728, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.690, 10.434], loss: 0.001249, mae: 0.038190, mean_q: 1.299817
 52217/100000: episode: 978, duration: 0.052s, episode steps: 7, steps per second: 134, episode reward: 5.489, mean reward: 0.784 [0.774, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.663, 10.507], loss: 0.001035, mae: 0.036450, mean_q: 1.312730
 52230/100000: episode: 979, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 11.456, mean reward: 0.881 [0.805, 0.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.658], loss: 0.001395, mae: 0.041808, mean_q: 1.289003
 52239/100000: episode: 980, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 7.185, mean reward: 0.798 [0.769, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.275, 10.444], loss: 0.001361, mae: 0.041699, mean_q: 1.297955
 52257/100000: episode: 981, duration: 0.108s, episode steps: 18, steps per second: 167, episode reward: 14.066, mean reward: 0.781 [0.704, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.039, 10.570], loss: 0.001603, mae: 0.043743, mean_q: 1.318220
 52266/100000: episode: 982, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 7.451, mean reward: 0.828 [0.751, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.497], loss: 0.001649, mae: 0.044761, mean_q: 1.307912
 52284/100000: episode: 983, duration: 0.111s, episode steps: 18, steps per second: 161, episode reward: 13.900, mean reward: 0.772 [0.670, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.446, 10.391], loss: 0.001389, mae: 0.040548, mean_q: 1.316550
 52297/100000: episode: 984, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 10.999, mean reward: 0.846 [0.770, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.570], loss: 0.001478, mae: 0.041497, mean_q: 1.296990
 52310/100000: episode: 985, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 10.623, mean reward: 0.817 [0.764, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.584], loss: 0.001299, mae: 0.039901, mean_q: 1.300195
 52331/100000: episode: 986, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 15.911, mean reward: 0.758 [0.638, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.259, 10.468], loss: 0.001284, mae: 0.040004, mean_q: 1.317559
 52338/100000: episode: 987, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 5.544, mean reward: 0.792 [0.736, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.519], loss: 0.001321, mae: 0.039763, mean_q: 1.312726
 52350/100000: episode: 988, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 9.817, mean reward: 0.818 [0.741, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.570, 10.556], loss: 0.001329, mae: 0.039729, mean_q: 1.317440
 52367/100000: episode: 989, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 14.861, mean reward: 0.874 [0.810, 0.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.915, 10.669], loss: 0.001357, mae: 0.041106, mean_q: 1.306206
 52388/100000: episode: 990, duration: 0.125s, episode steps: 21, steps per second: 168, episode reward: 17.574, mean reward: 0.837 [0.730, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.165, 10.600], loss: 0.001266, mae: 0.039751, mean_q: 1.315871
 52406/100000: episode: 991, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 13.199, mean reward: 0.733 [0.676, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.459], loss: 0.001385, mae: 0.040585, mean_q: 1.314532
 52424/100000: episode: 992, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 14.220, mean reward: 0.790 [0.734, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.334, 10.560], loss: 0.001244, mae: 0.039666, mean_q: 1.293434
 52445/100000: episode: 993, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 17.523, mean reward: 0.834 [0.739, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.484], loss: 0.001327, mae: 0.039990, mean_q: 1.307339
 52460/100000: episode: 994, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 11.647, mean reward: 0.776 [0.693, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.421], loss: 0.001330, mae: 0.040924, mean_q: 1.318881
 52478/100000: episode: 995, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 14.521, mean reward: 0.807 [0.697, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.187, 10.492], loss: 0.001338, mae: 0.039694, mean_q: 1.313725
 52491/100000: episode: 996, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 11.287, mean reward: 0.868 [0.832, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.325, 10.681], loss: 0.001285, mae: 0.039399, mean_q: 1.308443
 52509/100000: episode: 997, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 13.548, mean reward: 0.753 [0.606, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.474], loss: 0.001223, mae: 0.037697, mean_q: 1.322114
 52527/100000: episode: 998, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 13.209, mean reward: 0.734 [0.664, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.247, 10.424], loss: 0.001530, mae: 0.043427, mean_q: 1.320862
 52540/100000: episode: 999, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 10.794, mean reward: 0.830 [0.741, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.544], loss: 0.001269, mae: 0.037760, mean_q: 1.321395
 52552/100000: episode: 1000, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 10.660, mean reward: 0.888 [0.827, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.970, 10.566], loss: 0.001552, mae: 0.043492, mean_q: 1.316930
[Info] FALSIFICATION!
 52562/100000: episode: 1001, duration: 0.225s, episode steps: 10, steps per second: 44, episode reward: 9.057, mean reward: 0.906 [0.837, 1.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.393, 10.597], loss: 0.001894, mae: 0.049004, mean_q: 1.336240
[Info] FALSIFICATION!
 52574/100000: episode: 1002, duration: 0.337s, episode steps: 12, steps per second: 36, episode reward: 10.218, mean reward: 0.852 [0.730, 1.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.245, 10.533], loss: 0.001390, mae: 0.040185, mean_q: 1.314042
 52587/100000: episode: 1003, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 9.917, mean reward: 0.763 [0.728, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.508], loss: 0.001183, mae: 0.037955, mean_q: 1.327605
 52599/100000: episode: 1004, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 9.961, mean reward: 0.830 [0.735, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.586], loss: 0.001196, mae: 0.037962, mean_q: 1.339085
 52617/100000: episode: 1005, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 13.232, mean reward: 0.735 [0.615, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-1.352, 10.355], loss: 0.001292, mae: 0.040247, mean_q: 1.338542
 52630/100000: episode: 1006, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 10.706, mean reward: 0.824 [0.772, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.447], loss: 0.001547, mae: 0.042255, mean_q: 1.332434
[Info] FALSIFICATION!
 52637/100000: episode: 1007, duration: 0.256s, episode steps: 7, steps per second: 27, episode reward: 6.116, mean reward: 0.874 [0.786, 1.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-1.263, 10.676], loss: 0.001794, mae: 0.045971, mean_q: 1.288116
 52652/100000: episode: 1008, duration: 0.098s, episode steps: 15, steps per second: 153, episode reward: 12.383, mean reward: 0.826 [0.736, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.270, 10.580], loss: 0.001604, mae: 0.044468, mean_q: 1.315453
 52664/100000: episode: 1009, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 10.064, mean reward: 0.839 [0.773, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.650], loss: 0.001461, mae: 0.042455, mean_q: 1.302903
 52677/100000: episode: 1010, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 10.480, mean reward: 0.806 [0.687, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.314, 10.617], loss: 0.001188, mae: 0.038664, mean_q: 1.333020
 52690/100000: episode: 1011, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 10.553, mean reward: 0.812 [0.749, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.478], loss: 0.001493, mae: 0.042242, mean_q: 1.316225
 52708/100000: episode: 1012, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 14.895, mean reward: 0.827 [0.756, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.410, 10.653], loss: 0.001413, mae: 0.042066, mean_q: 1.327416
 52721/100000: episode: 1013, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 10.549, mean reward: 0.811 [0.765, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.150, 10.543], loss: 0.001916, mae: 0.043327, mean_q: 1.322518
 52739/100000: episode: 1014, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 14.178, mean reward: 0.788 [0.719, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.603, 10.566], loss: 0.001810, mae: 0.042470, mean_q: 1.329412
[Info] FALSIFICATION!
 52750/100000: episode: 1015, duration: 0.327s, episode steps: 11, steps per second: 34, episode reward: 8.912, mean reward: 0.810 [0.693, 1.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.204, 10.768], loss: 0.001539, mae: 0.043389, mean_q: 1.313911
 52768/100000: episode: 1016, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 14.838, mean reward: 0.824 [0.776, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.632], loss: 0.001363, mae: 0.040991, mean_q: 1.331745
 52789/100000: episode: 1017, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 18.063, mean reward: 0.860 [0.765, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.651], loss: 0.001310, mae: 0.040878, mean_q: 1.333374
 52806/100000: episode: 1018, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 14.931, mean reward: 0.878 [0.804, 0.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.655], loss: 0.001944, mae: 0.042996, mean_q: 1.334419
 52819/100000: episode: 1019, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 10.784, mean reward: 0.830 [0.788, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.642], loss: 0.002064, mae: 0.044993, mean_q: 1.347483
 52831/100000: episode: 1020, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 9.776, mean reward: 0.815 [0.758, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.567], loss: 0.002094, mae: 0.045942, mean_q: 1.322603
 52846/100000: episode: 1021, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 12.254, mean reward: 0.817 [0.784, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.904, 10.564], loss: 0.001569, mae: 0.043735, mean_q: 1.339036
 52859/100000: episode: 1022, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 10.624, mean reward: 0.817 [0.768, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-1.090, 10.623], loss: 0.001428, mae: 0.041194, mean_q: 1.337917
 52872/100000: episode: 1023, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 10.960, mean reward: 0.843 [0.774, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.629], loss: 0.001399, mae: 0.041767, mean_q: 1.332714
 52881/100000: episode: 1024, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 7.742, mean reward: 0.860 [0.813, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.630], loss: 0.001258, mae: 0.038824, mean_q: 1.332416
 52899/100000: episode: 1025, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 15.196, mean reward: 0.844 [0.764, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.182, 10.552], loss: 0.001280, mae: 0.038702, mean_q: 1.338459
 52914/100000: episode: 1026, duration: 0.100s, episode steps: 15, steps per second: 150, episode reward: 11.749, mean reward: 0.783 [0.670, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.051, 10.640], loss: 0.001208, mae: 0.038866, mean_q: 1.338186
 52923/100000: episode: 1027, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 7.143, mean reward: 0.794 [0.752, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.540], loss: 0.002333, mae: 0.044781, mean_q: 1.345085
 52940/100000: episode: 1028, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 14.113, mean reward: 0.830 [0.779, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.690, 10.525], loss: 0.002393, mae: 0.046775, mean_q: 1.326882
 52952/100000: episode: 1029, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 10.014, mean reward: 0.835 [0.797, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.447, 10.466], loss: 0.001465, mae: 0.041531, mean_q: 1.332811
 52970/100000: episode: 1030, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 13.829, mean reward: 0.768 [0.705, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.397, 10.432], loss: 0.001823, mae: 0.041780, mean_q: 1.349514
 52985/100000: episode: 1031, duration: 0.093s, episode steps: 15, steps per second: 160, episode reward: 11.559, mean reward: 0.771 [0.691, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.683, 10.506], loss: 0.001179, mae: 0.038820, mean_q: 1.352093
 53002/100000: episode: 1032, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 13.711, mean reward: 0.807 [0.773, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.162, 10.582], loss: 0.001657, mae: 0.040210, mean_q: 1.342888
 53023/100000: episode: 1033, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 16.224, mean reward: 0.773 [0.691, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.545, 10.456], loss: 0.002414, mae: 0.051628, mean_q: 1.357981
 53041/100000: episode: 1034, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 15.369, mean reward: 0.854 [0.757, 0.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.069, 10.532], loss: 0.001523, mae: 0.043219, mean_q: 1.326235
 53062/100000: episode: 1035, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 16.477, mean reward: 0.785 [0.732, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.336, 10.578], loss: 0.001461, mae: 0.042855, mean_q: 1.343918
 53079/100000: episode: 1036, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 13.544, mean reward: 0.797 [0.673, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.077, 10.516], loss: 0.001463, mae: 0.040997, mean_q: 1.351088
 53092/100000: episode: 1037, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 9.445, mean reward: 0.727 [0.663, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.425], loss: 0.001329, mae: 0.040764, mean_q: 1.358794
 53110/100000: episode: 1038, duration: 0.121s, episode steps: 18, steps per second: 148, episode reward: 13.749, mean reward: 0.764 [0.608, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.948, 10.371], loss: 0.001309, mae: 0.039769, mean_q: 1.342829
 53128/100000: episode: 1039, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 13.805, mean reward: 0.767 [0.676, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-1.089, 10.375], loss: 0.001432, mae: 0.041873, mean_q: 1.339640
 53146/100000: episode: 1040, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 13.221, mean reward: 0.735 [0.640, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.569, 10.387], loss: 0.001606, mae: 0.044021, mean_q: 1.347193
 53153/100000: episode: 1041, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 5.244, mean reward: 0.749 [0.703, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.419], loss: 0.001422, mae: 0.043638, mean_q: 1.354458
 53165/100000: episode: 1042, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 10.723, mean reward: 0.894 [0.761, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.622], loss: 0.002033, mae: 0.044001, mean_q: 1.359685
 53182/100000: episode: 1043, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 13.740, mean reward: 0.808 [0.718, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.492, 10.570], loss: 0.001571, mae: 0.042495, mean_q: 1.350910
 53194/100000: episode: 1044, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 8.810, mean reward: 0.734 [0.681, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.426], loss: 0.001253, mae: 0.040219, mean_q: 1.344287
 53201/100000: episode: 1045, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 5.456, mean reward: 0.779 [0.758, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.532], loss: 0.001430, mae: 0.042147, mean_q: 1.351255
 53219/100000: episode: 1046, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 15.795, mean reward: 0.878 [0.804, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.067, 10.720], loss: 0.001561, mae: 0.043341, mean_q: 1.345279
 53237/100000: episode: 1047, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 14.443, mean reward: 0.802 [0.739, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.115, 10.552], loss: 0.002582, mae: 0.045132, mean_q: 1.365896
 53252/100000: episode: 1048, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 12.613, mean reward: 0.841 [0.769, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.672], loss: 0.002027, mae: 0.045322, mean_q: 1.355273
 53270/100000: episode: 1049, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 14.731, mean reward: 0.818 [0.749, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.513], loss: 0.001579, mae: 0.041220, mean_q: 1.359457
 53285/100000: episode: 1050, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 10.854, mean reward: 0.724 [0.682, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.539], loss: 0.001553, mae: 0.044544, mean_q: 1.350661
 53303/100000: episode: 1051, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 14.584, mean reward: 0.810 [0.760, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.072, 10.564], loss: 0.001693, mae: 0.046581, mean_q: 1.357867
 53310/100000: episode: 1052, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 5.719, mean reward: 0.817 [0.804, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.356, 10.609], loss: 0.001549, mae: 0.044069, mean_q: 1.364775
 53328/100000: episode: 1053, duration: 0.106s, episode steps: 18, steps per second: 171, episode reward: 13.630, mean reward: 0.757 [0.701, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.175, 10.497], loss: 0.001624, mae: 0.043552, mean_q: 1.357913
 53341/100000: episode: 1054, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 9.964, mean reward: 0.766 [0.710, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.335, 10.525], loss: 0.001779, mae: 0.046749, mean_q: 1.358663
 53353/100000: episode: 1055, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 10.339, mean reward: 0.862 [0.834, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.544], loss: 0.002058, mae: 0.045434, mean_q: 1.372853
 53371/100000: episode: 1056, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 14.732, mean reward: 0.818 [0.771, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.261, 10.636], loss: 0.001671, mae: 0.040710, mean_q: 1.386636
 53384/100000: episode: 1057, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 10.528, mean reward: 0.810 [0.769, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.216, 10.534], loss: 0.001918, mae: 0.041749, mean_q: 1.381803
 53401/100000: episode: 1058, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 14.861, mean reward: 0.874 [0.799, 0.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.838, 10.679], loss: 0.001193, mae: 0.038070, mean_q: 1.366188
 53410/100000: episode: 1059, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 7.362, mean reward: 0.818 [0.800, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.548], loss: 0.001350, mae: 0.039600, mean_q: 1.371393
 53422/100000: episode: 1060, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 10.302, mean reward: 0.858 [0.809, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.581], loss: 0.001363, mae: 0.041306, mean_q: 1.377781
 53437/100000: episode: 1061, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 11.670, mean reward: 0.778 [0.691, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.702], loss: 0.001455, mae: 0.041194, mean_q: 1.358637
[Info] Complete ISplit Iteration
[Info] Levels: [1.3668525, 1.4510285, 1.5958395, 1.5926942]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.86]
[Info] Error Prob: 0.0008600000000000002

 53450/100000: episode: 1062, duration: 4.618s, episode steps: 13, steps per second: 3, episode reward: 10.448, mean reward: 0.804 [0.753, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.628], loss: 0.001424, mae: 0.042072, mean_q: 1.357185
 53550/100000: episode: 1063, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.397, mean reward: 0.584 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.513, 10.098], loss: 0.001581, mae: 0.041948, mean_q: 1.367676
 53650/100000: episode: 1064, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.557, mean reward: 0.586 [0.513, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.143, 10.098], loss: 0.001501, mae: 0.042198, mean_q: 1.363008
 53750/100000: episode: 1065, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.206, mean reward: 0.572 [0.503, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.352, 10.098], loss: 0.001465, mae: 0.041660, mean_q: 1.353521
 53850/100000: episode: 1066, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.368, mean reward: 0.584 [0.507, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.000, 10.257], loss: 0.001648, mae: 0.042701, mean_q: 1.360693
 53950/100000: episode: 1067, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 60.130, mean reward: 0.601 [0.506, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.605, 10.392], loss: 0.001430, mae: 0.041542, mean_q: 1.358725
 54050/100000: episode: 1068, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 58.285, mean reward: 0.583 [0.511, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.903, 10.098], loss: 0.001546, mae: 0.042742, mean_q: 1.357601
 54150/100000: episode: 1069, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.146, mean reward: 0.581 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.550, 10.242], loss: 0.001296, mae: 0.039915, mean_q: 1.350541
 54250/100000: episode: 1070, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.102, mean reward: 0.581 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.896, 10.098], loss: 0.001787, mae: 0.044018, mean_q: 1.356633
 54350/100000: episode: 1071, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.318, mean reward: 0.583 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.435, 10.196], loss: 0.001687, mae: 0.045020, mean_q: 1.342485
 54450/100000: episode: 1072, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.149, mean reward: 0.581 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.157, 10.098], loss: 0.001562, mae: 0.042463, mean_q: 1.338901
 54550/100000: episode: 1073, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.955, mean reward: 0.590 [0.498, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.268, 10.098], loss: 0.001631, mae: 0.042636, mean_q: 1.341253
 54650/100000: episode: 1074, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 62.038, mean reward: 0.620 [0.512, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.092, 10.098], loss: 0.001403, mae: 0.041254, mean_q: 1.349017
 54750/100000: episode: 1075, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 60.681, mean reward: 0.607 [0.502, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.101, 10.377], loss: 0.001331, mae: 0.039803, mean_q: 1.345125
 54850/100000: episode: 1076, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 61.363, mean reward: 0.614 [0.505, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.430, 10.098], loss: 0.001561, mae: 0.041703, mean_q: 1.345617
 54950/100000: episode: 1077, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 56.610, mean reward: 0.566 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.184, 10.098], loss: 0.001650, mae: 0.042588, mean_q: 1.338382
 55050/100000: episode: 1078, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.665, mean reward: 0.587 [0.506, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.535, 10.281], loss: 0.001541, mae: 0.041267, mean_q: 1.329395
 55150/100000: episode: 1079, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 57.766, mean reward: 0.578 [0.504, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.791, 10.098], loss: 0.001774, mae: 0.044334, mean_q: 1.329908
 55250/100000: episode: 1080, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 61.575, mean reward: 0.616 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.738, 10.199], loss: 0.001494, mae: 0.041648, mean_q: 1.321453
 55350/100000: episode: 1081, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 58.900, mean reward: 0.589 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.992, 10.098], loss: 0.001459, mae: 0.040641, mean_q: 1.312013
 55450/100000: episode: 1082, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.752, mean reward: 0.588 [0.503, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.158, 10.098], loss: 0.001548, mae: 0.040982, mean_q: 1.317134
 55550/100000: episode: 1083, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 55.886, mean reward: 0.559 [0.500, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.368, 10.098], loss: 0.001666, mae: 0.043342, mean_q: 1.306286
 55650/100000: episode: 1084, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.511, mean reward: 0.585 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.731, 10.158], loss: 0.001751, mae: 0.043885, mean_q: 1.308801
 55750/100000: episode: 1085, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 59.497, mean reward: 0.595 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.583, 10.295], loss: 0.001670, mae: 0.043686, mean_q: 1.305521
 55850/100000: episode: 1086, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.575, mean reward: 0.586 [0.498, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.418, 10.105], loss: 0.001633, mae: 0.042435, mean_q: 1.294905
 55950/100000: episode: 1087, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.344, mean reward: 0.593 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.654, 10.158], loss: 0.001460, mae: 0.041053, mean_q: 1.296635
 56050/100000: episode: 1088, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.794, mean reward: 0.588 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.183, 10.228], loss: 0.001678, mae: 0.043561, mean_q: 1.290198
 56150/100000: episode: 1089, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.490, mean reward: 0.585 [0.497, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.910, 10.315], loss: 0.001402, mae: 0.041290, mean_q: 1.292713
 56250/100000: episode: 1090, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 57.375, mean reward: 0.574 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.606, 10.214], loss: 0.001530, mae: 0.041233, mean_q: 1.285585
 56350/100000: episode: 1091, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 60.190, mean reward: 0.602 [0.506, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.964, 10.352], loss: 0.001538, mae: 0.042336, mean_q: 1.279844
 56450/100000: episode: 1092, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.684, mean reward: 0.587 [0.501, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.640, 10.191], loss: 0.001730, mae: 0.042857, mean_q: 1.278336
 56550/100000: episode: 1093, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.585, mean reward: 0.586 [0.505, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.313, 10.098], loss: 0.001734, mae: 0.043309, mean_q: 1.268520
 56650/100000: episode: 1094, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 60.380, mean reward: 0.604 [0.502, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.924, 10.344], loss: 0.001769, mae: 0.044198, mean_q: 1.272093
 56750/100000: episode: 1095, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.544, mean reward: 0.595 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.092, 10.098], loss: 0.001704, mae: 0.042117, mean_q: 1.268556
 56850/100000: episode: 1096, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.467, mean reward: 0.585 [0.506, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.109, 10.150], loss: 0.001733, mae: 0.042479, mean_q: 1.265021
 56950/100000: episode: 1097, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.379, mean reward: 0.574 [0.500, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.057, 10.164], loss: 0.001657, mae: 0.042733, mean_q: 1.260291
 57050/100000: episode: 1098, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 59.428, mean reward: 0.594 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.316, 10.217], loss: 0.001549, mae: 0.041913, mean_q: 1.256660
 57150/100000: episode: 1099, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.265, mean reward: 0.583 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.528, 10.147], loss: 0.001676, mae: 0.042936, mean_q: 1.244347
 57250/100000: episode: 1100, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 58.754, mean reward: 0.588 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.404, 10.098], loss: 0.001542, mae: 0.040831, mean_q: 1.243703
 57350/100000: episode: 1101, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.412, mean reward: 0.594 [0.501, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.625, 10.217], loss: 0.001481, mae: 0.040468, mean_q: 1.238778
 57450/100000: episode: 1102, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.760, mean reward: 0.588 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.491, 10.098], loss: 0.001565, mae: 0.042901, mean_q: 1.224494
 57550/100000: episode: 1103, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 59.088, mean reward: 0.591 [0.498, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.540, 10.098], loss: 0.001418, mae: 0.040798, mean_q: 1.217827
 57650/100000: episode: 1104, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.082, mean reward: 0.581 [0.508, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.794, 10.180], loss: 0.001626, mae: 0.043101, mean_q: 1.220326
 57750/100000: episode: 1105, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.746, mean reward: 0.587 [0.498, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.141, 10.450], loss: 0.001508, mae: 0.042880, mean_q: 1.207038
 57850/100000: episode: 1106, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.609, mean reward: 0.586 [0.516, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.465, 10.098], loss: 0.001319, mae: 0.039994, mean_q: 1.197758
 57950/100000: episode: 1107, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.897, mean reward: 0.609 [0.519, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.473, 10.223], loss: 0.001338, mae: 0.040136, mean_q: 1.192441
 58050/100000: episode: 1108, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.561, mean reward: 0.586 [0.503, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.607, 10.257], loss: 0.001291, mae: 0.039678, mean_q: 1.191853
 58150/100000: episode: 1109, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.295, mean reward: 0.573 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.651, 10.299], loss: 0.001427, mae: 0.041283, mean_q: 1.185099
 58250/100000: episode: 1110, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.825, mean reward: 0.598 [0.512, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.095, 10.281], loss: 0.001369, mae: 0.040561, mean_q: 1.173824
 58350/100000: episode: 1111, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.968, mean reward: 0.580 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.982, 10.122], loss: 0.001303, mae: 0.039670, mean_q: 1.175939
 58450/100000: episode: 1112, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 57.020, mean reward: 0.570 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.432, 10.166], loss: 0.001261, mae: 0.038856, mean_q: 1.163040
 58550/100000: episode: 1113, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 56.939, mean reward: 0.569 [0.501, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.690, 10.098], loss: 0.001206, mae: 0.038441, mean_q: 1.160318
 58650/100000: episode: 1114, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.256, mean reward: 0.583 [0.503, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.602, 10.102], loss: 0.001230, mae: 0.038723, mean_q: 1.161173
 58750/100000: episode: 1115, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 61.444, mean reward: 0.614 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.008, 10.247], loss: 0.001300, mae: 0.039113, mean_q: 1.160848
 58850/100000: episode: 1116, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.179, mean reward: 0.582 [0.501, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.066, 10.246], loss: 0.001349, mae: 0.040340, mean_q: 1.164354
 58950/100000: episode: 1117, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 61.331, mean reward: 0.613 [0.512, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.842, 10.372], loss: 0.001405, mae: 0.041269, mean_q: 1.165860
 59050/100000: episode: 1118, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.455, mean reward: 0.585 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.816, 10.098], loss: 0.001344, mae: 0.040379, mean_q: 1.163730
 59150/100000: episode: 1119, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 62.043, mean reward: 0.620 [0.511, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.903, 10.293], loss: 0.001418, mae: 0.041652, mean_q: 1.164005
 59250/100000: episode: 1120, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.821, mean reward: 0.578 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.822, 10.098], loss: 0.001342, mae: 0.040265, mean_q: 1.166148
 59350/100000: episode: 1121, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.354, mean reward: 0.584 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.746, 10.098], loss: 0.001362, mae: 0.040236, mean_q: 1.166906
 59450/100000: episode: 1122, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.273, mean reward: 0.583 [0.514, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.642, 10.098], loss: 0.001328, mae: 0.040340, mean_q: 1.168382
 59550/100000: episode: 1123, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 56.060, mean reward: 0.561 [0.504, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.156, 10.098], loss: 0.001339, mae: 0.039986, mean_q: 1.166400
 59650/100000: episode: 1124, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.250, mean reward: 0.583 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.845, 10.098], loss: 0.001393, mae: 0.040761, mean_q: 1.162082
 59750/100000: episode: 1125, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.658, mean reward: 0.587 [0.500, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.449, 10.238], loss: 0.001398, mae: 0.041496, mean_q: 1.164062
 59850/100000: episode: 1126, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.759, mean reward: 0.578 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.534, 10.098], loss: 0.001406, mae: 0.041445, mean_q: 1.160418
 59950/100000: episode: 1127, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.195, mean reward: 0.592 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.664, 10.098], loss: 0.001334, mae: 0.040350, mean_q: 1.161550
 60050/100000: episode: 1128, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.367, mean reward: 0.594 [0.507, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.602, 10.098], loss: 0.001361, mae: 0.040608, mean_q: 1.162813
 60150/100000: episode: 1129, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.663, mean reward: 0.577 [0.504, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.202, 10.114], loss: 0.001346, mae: 0.040804, mean_q: 1.160661
 60250/100000: episode: 1130, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.297, mean reward: 0.573 [0.509, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.813, 10.102], loss: 0.001329, mae: 0.040189, mean_q: 1.161950
 60350/100000: episode: 1131, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.512, mean reward: 0.585 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.670, 10.243], loss: 0.001434, mae: 0.041348, mean_q: 1.158791
 60450/100000: episode: 1132, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: 59.062, mean reward: 0.591 [0.508, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.298, 10.098], loss: 0.001443, mae: 0.041814, mean_q: 1.163195
 60550/100000: episode: 1133, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.712, mean reward: 0.597 [0.502, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.317, 10.098], loss: 0.001417, mae: 0.041130, mean_q: 1.161354
 60650/100000: episode: 1134, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.847, mean reward: 0.598 [0.504, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.974, 10.221], loss: 0.001317, mae: 0.039985, mean_q: 1.162487
 60750/100000: episode: 1135, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.073, mean reward: 0.591 [0.505, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.264], loss: 0.001388, mae: 0.040501, mean_q: 1.165651
 60850/100000: episode: 1136, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.861, mean reward: 0.579 [0.500, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.507, 10.098], loss: 0.001414, mae: 0.041437, mean_q: 1.159871
 60950/100000: episode: 1137, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.157, mean reward: 0.572 [0.500, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.327, 10.098], loss: 0.001353, mae: 0.040145, mean_q: 1.162874
 61050/100000: episode: 1138, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.361, mean reward: 0.584 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.805, 10.183], loss: 0.001384, mae: 0.040592, mean_q: 1.160947
 61150/100000: episode: 1139, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 60.615, mean reward: 0.606 [0.508, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.771, 10.098], loss: 0.001317, mae: 0.039282, mean_q: 1.161854
 61250/100000: episode: 1140, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 58.052, mean reward: 0.581 [0.513, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.906, 10.105], loss: 0.001401, mae: 0.041139, mean_q: 1.162392
 61350/100000: episode: 1141, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 59.685, mean reward: 0.597 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.029, 10.403], loss: 0.001406, mae: 0.040784, mean_q: 1.159137
 61450/100000: episode: 1142, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.886, mean reward: 0.589 [0.507, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.603, 10.098], loss: 0.001449, mae: 0.041815, mean_q: 1.162851
 61550/100000: episode: 1143, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 59.121, mean reward: 0.591 [0.504, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.199, 10.239], loss: 0.001414, mae: 0.041008, mean_q: 1.161229
 61650/100000: episode: 1144, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 57.624, mean reward: 0.576 [0.505, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.555, 10.098], loss: 0.001456, mae: 0.041509, mean_q: 1.160356
 61750/100000: episode: 1145, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 61.406, mean reward: 0.614 [0.503, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.860, 10.098], loss: 0.001406, mae: 0.040953, mean_q: 1.161461
 61850/100000: episode: 1146, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.690, mean reward: 0.577 [0.502, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.461, 10.098], loss: 0.001411, mae: 0.041222, mean_q: 1.163229
 61950/100000: episode: 1147, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.969, mean reward: 0.590 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.488, 10.098], loss: 0.001410, mae: 0.041218, mean_q: 1.164190
 62050/100000: episode: 1148, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.355, mean reward: 0.584 [0.506, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.274, 10.237], loss: 0.001335, mae: 0.040141, mean_q: 1.161458
 62150/100000: episode: 1149, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 61.163, mean reward: 0.612 [0.512, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.821, 10.243], loss: 0.001347, mae: 0.040222, mean_q: 1.165677
 62250/100000: episode: 1150, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.245, mean reward: 0.572 [0.505, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.214, 10.098], loss: 0.001498, mae: 0.042150, mean_q: 1.160678
 62350/100000: episode: 1151, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 59.821, mean reward: 0.598 [0.508, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.735, 10.098], loss: 0.001348, mae: 0.039959, mean_q: 1.157089
 62450/100000: episode: 1152, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.241, mean reward: 0.592 [0.500, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.924, 10.213], loss: 0.001369, mae: 0.040637, mean_q: 1.159866
 62550/100000: episode: 1153, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.165, mean reward: 0.592 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.525, 10.098], loss: 0.001448, mae: 0.041878, mean_q: 1.160200
 62650/100000: episode: 1154, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 56.871, mean reward: 0.569 [0.502, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.770, 10.098], loss: 0.001374, mae: 0.040902, mean_q: 1.163329
 62750/100000: episode: 1155, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 60.573, mean reward: 0.606 [0.500, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.543, 10.323], loss: 0.001508, mae: 0.042587, mean_q: 1.162538
 62850/100000: episode: 1156, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 56.450, mean reward: 0.564 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.309, 10.098], loss: 0.001450, mae: 0.041518, mean_q: 1.162165
 62950/100000: episode: 1157, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.541, mean reward: 0.585 [0.504, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.406, 10.165], loss: 0.001523, mae: 0.042659, mean_q: 1.158067
 63050/100000: episode: 1158, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.071, mean reward: 0.571 [0.512, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.529, 10.141], loss: 0.001390, mae: 0.040172, mean_q: 1.158625
 63150/100000: episode: 1159, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 58.372, mean reward: 0.584 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.533, 10.158], loss: 0.001499, mae: 0.042227, mean_q: 1.157591
 63250/100000: episode: 1160, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.341, mean reward: 0.583 [0.514, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.828, 10.134], loss: 0.001393, mae: 0.040789, mean_q: 1.159785
 63350/100000: episode: 1161, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.984, mean reward: 0.590 [0.505, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.068, 10.165], loss: 0.001457, mae: 0.041771, mean_q: 1.162440
[Info] 1-TH LEVEL FOUND: 1.3672348260879517, Considering 10/90 traces
 63450/100000: episode: 1162, duration: 4.891s, episode steps: 100, steps per second: 20, episode reward: 63.587, mean reward: 0.636 [0.503, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.424, 10.098], loss: 0.001368, mae: 0.040462, mean_q: 1.159180
 63464/100000: episode: 1163, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 11.070, mean reward: 0.791 [0.739, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.369, 10.100], loss: 0.001360, mae: 0.040239, mean_q: 1.162123
 63476/100000: episode: 1164, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 7.543, mean reward: 0.629 [0.558, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.944, 10.275], loss: 0.001613, mae: 0.042104, mean_q: 1.162661
 63571/100000: episode: 1165, duration: 0.514s, episode steps: 95, steps per second: 185, episode reward: 57.526, mean reward: 0.606 [0.499, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-1.594, 10.122], loss: 0.001528, mae: 0.042975, mean_q: 1.164365
 63592/100000: episode: 1166, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 15.929, mean reward: 0.759 [0.693, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.571, 10.523], loss: 0.001710, mae: 0.044904, mean_q: 1.164186
 63604/100000: episode: 1167, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 8.360, mean reward: 0.697 [0.638, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.537, 10.373], loss: 0.001808, mae: 0.044559, mean_q: 1.169090
 63639/100000: episode: 1168, duration: 0.185s, episode steps: 35, steps per second: 190, episode reward: 24.188, mean reward: 0.691 [0.571, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.132, 10.308], loss: 0.001300, mae: 0.039884, mean_q: 1.167556
 63731/100000: episode: 1169, duration: 0.492s, episode steps: 92, steps per second: 187, episode reward: 54.857, mean reward: 0.596 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-0.804, 10.100], loss: 0.001553, mae: 0.042998, mean_q: 1.164868
 63826/100000: episode: 1170, duration: 0.513s, episode steps: 95, steps per second: 185, episode reward: 54.230, mean reward: 0.571 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.921, 10.100], loss: 0.001621, mae: 0.043683, mean_q: 1.169801
 63838/100000: episode: 1171, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 7.856, mean reward: 0.655 [0.584, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.409], loss: 0.001642, mae: 0.045247, mean_q: 1.167664
 63870/100000: episode: 1172, duration: 0.181s, episode steps: 32, steps per second: 177, episode reward: 21.192, mean reward: 0.662 [0.592, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.601, 10.421], loss: 0.001787, mae: 0.044886, mean_q: 1.170107
 63889/100000: episode: 1173, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 13.618, mean reward: 0.717 [0.654, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.180, 10.334], loss: 0.001784, mae: 0.043313, mean_q: 1.167380
 63901/100000: episode: 1174, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 8.411, mean reward: 0.701 [0.657, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.089, 10.477], loss: 0.001507, mae: 0.042422, mean_q: 1.163775
 63933/100000: episode: 1175, duration: 0.164s, episode steps: 32, steps per second: 196, episode reward: 22.212, mean reward: 0.694 [0.586, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.762, 10.298], loss: 0.001455, mae: 0.041083, mean_q: 1.162668
 63947/100000: episode: 1176, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 10.626, mean reward: 0.759 [0.741, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.390, 10.100], loss: 0.001792, mae: 0.044836, mean_q: 1.161899
 63959/100000: episode: 1177, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 7.763, mean reward: 0.647 [0.602, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.376], loss: 0.001619, mae: 0.043293, mean_q: 1.161237
 63978/100000: episode: 1178, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 12.787, mean reward: 0.673 [0.626, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.875, 10.399], loss: 0.001696, mae: 0.044288, mean_q: 1.170537
 64073/100000: episode: 1179, duration: 0.519s, episode steps: 95, steps per second: 183, episode reward: 58.019, mean reward: 0.611 [0.520, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.855, 10.100], loss: 0.001719, mae: 0.044558, mean_q: 1.169222
 64167/100000: episode: 1180, duration: 0.508s, episode steps: 94, steps per second: 185, episode reward: 56.936, mean reward: 0.606 [0.512, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.507 [-0.174, 10.176], loss: 0.001619, mae: 0.042841, mean_q: 1.167101
 64179/100000: episode: 1181, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 7.936, mean reward: 0.661 [0.630, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.500, 10.357], loss: 0.001347, mae: 0.040022, mean_q: 1.175803
 64193/100000: episode: 1182, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 10.517, mean reward: 0.751 [0.693, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.488, 10.100], loss: 0.001323, mae: 0.038331, mean_q: 1.167851
 64212/100000: episode: 1183, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 13.903, mean reward: 0.732 [0.666, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.443], loss: 0.001436, mae: 0.040595, mean_q: 1.167572
 64231/100000: episode: 1184, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 12.316, mean reward: 0.648 [0.600, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.311], loss: 0.001265, mae: 0.039565, mean_q: 1.167597
 64323/100000: episode: 1185, duration: 0.488s, episode steps: 92, steps per second: 188, episode reward: 55.392, mean reward: 0.602 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-1.485, 10.100], loss: 0.001693, mae: 0.043885, mean_q: 1.175640
 64355/100000: episode: 1186, duration: 0.175s, episode steps: 32, steps per second: 182, episode reward: 21.866, mean reward: 0.683 [0.617, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.822, 10.452], loss: 0.001539, mae: 0.041869, mean_q: 1.173527
 64369/100000: episode: 1187, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 10.418, mean reward: 0.744 [0.664, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.251, 10.648], loss: 0.001656, mae: 0.043529, mean_q: 1.171949
 64401/100000: episode: 1188, duration: 0.181s, episode steps: 32, steps per second: 177, episode reward: 19.519, mean reward: 0.610 [0.504, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.080, 10.205], loss: 0.001662, mae: 0.044559, mean_q: 1.176810
 64433/100000: episode: 1189, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 19.732, mean reward: 0.617 [0.531, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.594, 10.208], loss: 0.001738, mae: 0.042911, mean_q: 1.165653
 64447/100000: episode: 1190, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 8.976, mean reward: 0.641 [0.590, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.307], loss: 0.001391, mae: 0.040963, mean_q: 1.168974
 64542/100000: episode: 1191, duration: 0.501s, episode steps: 95, steps per second: 190, episode reward: 55.388, mean reward: 0.583 [0.511, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-1.269, 10.100], loss: 0.001812, mae: 0.045325, mean_q: 1.174360
 64634/100000: episode: 1192, duration: 0.458s, episode steps: 92, steps per second: 201, episode reward: 55.632, mean reward: 0.605 [0.508, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.522 [-1.067, 10.107], loss: 0.001828, mae: 0.045124, mean_q: 1.173765
 64726/100000: episode: 1193, duration: 0.512s, episode steps: 92, steps per second: 180, episode reward: 52.679, mean reward: 0.573 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.992, 10.179], loss: 0.001616, mae: 0.041707, mean_q: 1.176145
 64761/100000: episode: 1194, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 24.542, mean reward: 0.701 [0.613, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.822, 10.331], loss: 0.001565, mae: 0.041872, mean_q: 1.174627
 64775/100000: episode: 1195, duration: 0.095s, episode steps: 14, steps per second: 148, episode reward: 10.287, mean reward: 0.735 [0.642, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.459, 10.450], loss: 0.001779, mae: 0.045746, mean_q: 1.167956
 64789/100000: episode: 1196, duration: 0.072s, episode steps: 14, steps per second: 196, episode reward: 9.582, mean reward: 0.684 [0.652, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.252, 10.442], loss: 0.001811, mae: 0.045997, mean_q: 1.181828
 64810/100000: episode: 1197, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 14.613, mean reward: 0.696 [0.630, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.227, 10.488], loss: 0.001790, mae: 0.044376, mean_q: 1.179556
 64824/100000: episode: 1198, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 10.585, mean reward: 0.756 [0.716, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.393, 10.100], loss: 0.001917, mae: 0.047496, mean_q: 1.169735
 64859/100000: episode: 1199, duration: 0.179s, episode steps: 35, steps per second: 195, episode reward: 22.002, mean reward: 0.629 [0.544, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.258], loss: 0.001822, mae: 0.045408, mean_q: 1.188492
 64953/100000: episode: 1200, duration: 0.518s, episode steps: 94, steps per second: 182, episode reward: 54.881, mean reward: 0.584 [0.502, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.507 [-1.759, 10.125], loss: 0.001741, mae: 0.043103, mean_q: 1.179613
 65047/100000: episode: 1201, duration: 0.494s, episode steps: 94, steps per second: 190, episode reward: 55.116, mean reward: 0.586 [0.506, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.959, 10.239], loss: 0.001657, mae: 0.043468, mean_q: 1.179606
 65082/100000: episode: 1202, duration: 0.180s, episode steps: 35, steps per second: 195, episode reward: 24.686, mean reward: 0.705 [0.613, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.591, 10.318], loss: 0.001536, mae: 0.042266, mean_q: 1.176855
 65096/100000: episode: 1203, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 9.534, mean reward: 0.681 [0.631, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.091, 10.467], loss: 0.001659, mae: 0.044555, mean_q: 1.182132
 65110/100000: episode: 1204, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 9.380, mean reward: 0.670 [0.626, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.299, 10.412], loss: 0.001363, mae: 0.038705, mean_q: 1.175043
 65205/100000: episode: 1205, duration: 0.469s, episode steps: 95, steps per second: 202, episode reward: 57.552, mean reward: 0.606 [0.518, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-1.387, 10.405], loss: 0.001564, mae: 0.042699, mean_q: 1.183455
 65219/100000: episode: 1206, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 8.903, mean reward: 0.636 [0.563, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.656, 10.296], loss: 0.001554, mae: 0.040991, mean_q: 1.173596
 65233/100000: episode: 1207, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 10.216, mean reward: 0.730 [0.618, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.284, 10.100], loss: 0.001646, mae: 0.043124, mean_q: 1.194067
 65247/100000: episode: 1208, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 8.885, mean reward: 0.635 [0.594, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.361], loss: 0.001607, mae: 0.044437, mean_q: 1.185688
 65268/100000: episode: 1209, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 15.571, mean reward: 0.741 [0.628, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.734, 10.369], loss: 0.001770, mae: 0.044525, mean_q: 1.181705
 65282/100000: episode: 1210, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 9.292, mean reward: 0.664 [0.624, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.276, 10.416], loss: 0.001576, mae: 0.041068, mean_q: 1.179497
 65314/100000: episode: 1211, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 22.680, mean reward: 0.709 [0.624, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.046, 10.452], loss: 0.001361, mae: 0.040386, mean_q: 1.194206
 65326/100000: episode: 1212, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 8.523, mean reward: 0.710 [0.641, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.164, 10.455], loss: 0.001391, mae: 0.038177, mean_q: 1.185929
 65340/100000: episode: 1213, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 9.719, mean reward: 0.694 [0.659, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.669, 10.406], loss: 0.001698, mae: 0.043090, mean_q: 1.188796
 65372/100000: episode: 1214, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 20.563, mean reward: 0.643 [0.568, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.381], loss: 0.001460, mae: 0.040474, mean_q: 1.192393
 65386/100000: episode: 1215, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 9.111, mean reward: 0.651 [0.604, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.582, 10.394], loss: 0.001556, mae: 0.043263, mean_q: 1.192367
 65480/100000: episode: 1216, duration: 0.500s, episode steps: 94, steps per second: 188, episode reward: 55.635, mean reward: 0.592 [0.505, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.511 [-0.453, 10.100], loss: 0.001817, mae: 0.045706, mean_q: 1.190275
 65512/100000: episode: 1217, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 21.490, mean reward: 0.672 [0.590, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.035, 10.432], loss: 0.001637, mae: 0.042849, mean_q: 1.191798
 65526/100000: episode: 1218, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 10.548, mean reward: 0.753 [0.674, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.622], loss: 0.001742, mae: 0.042003, mean_q: 1.185228
 65561/100000: episode: 1219, duration: 0.189s, episode steps: 35, steps per second: 186, episode reward: 22.077, mean reward: 0.631 [0.548, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.035, 10.290], loss: 0.001601, mae: 0.042671, mean_q: 1.188749
 65575/100000: episode: 1220, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 10.082, mean reward: 0.720 [0.638, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.810, 10.464], loss: 0.001716, mae: 0.044740, mean_q: 1.186100
 65669/100000: episode: 1221, duration: 0.505s, episode steps: 94, steps per second: 186, episode reward: 56.155, mean reward: 0.597 [0.510, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-1.043, 10.100], loss: 0.001681, mae: 0.043380, mean_q: 1.190409
 65683/100000: episode: 1222, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 10.081, mean reward: 0.720 [0.681, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.479, 10.552], loss: 0.001647, mae: 0.042691, mean_q: 1.180776
[Info] FALSIFICATION!
 65700/100000: episode: 1223, duration: 0.333s, episode steps: 17, steps per second: 51, episode reward: 13.984, mean reward: 0.823 [0.661, 1.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.046, 10.815], loss: 0.001518, mae: 0.040798, mean_q: 1.186772
 65792/100000: episode: 1224, duration: 0.479s, episode steps: 92, steps per second: 192, episode reward: 54.306, mean reward: 0.590 [0.502, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.534 [-0.739, 10.163], loss: 0.001561, mae: 0.042827, mean_q: 1.189488
 65804/100000: episode: 1225, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 8.287, mean reward: 0.691 [0.667, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.418], loss: 0.001939, mae: 0.047406, mean_q: 1.201748
 65816/100000: episode: 1226, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 8.507, mean reward: 0.709 [0.659, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.075, 10.452], loss: 0.001973, mae: 0.048929, mean_q: 1.191139
 65851/100000: episode: 1227, duration: 0.214s, episode steps: 35, steps per second: 163, episode reward: 27.392, mean reward: 0.783 [0.712, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.326, 10.707], loss: 0.001421, mae: 0.040847, mean_q: 1.199129
 65870/100000: episode: 1228, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 12.195, mean reward: 0.642 [0.589, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.291], loss: 0.001659, mae: 0.042632, mean_q: 1.207095
 65905/100000: episode: 1229, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 23.254, mean reward: 0.664 [0.569, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.322, 10.326], loss: 0.001514, mae: 0.042140, mean_q: 1.203833
 65926/100000: episode: 1230, duration: 0.122s, episode steps: 21, steps per second: 173, episode reward: 13.922, mean reward: 0.663 [0.617, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.285], loss: 0.001331, mae: 0.038715, mean_q: 1.199250
 66020/100000: episode: 1231, duration: 0.496s, episode steps: 94, steps per second: 190, episode reward: 58.670, mean reward: 0.624 [0.512, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.419, 10.472], loss: 0.001886, mae: 0.045507, mean_q: 1.200361
 66041/100000: episode: 1232, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 14.498, mean reward: 0.690 [0.650, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.184, 10.405], loss: 0.002597, mae: 0.051336, mean_q: 1.193092
 66055/100000: episode: 1233, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 9.680, mean reward: 0.691 [0.633, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.757, 10.414], loss: 0.001717, mae: 0.043968, mean_q: 1.194269
 66069/100000: episode: 1234, duration: 0.091s, episode steps: 14, steps per second: 153, episode reward: 9.978, mean reward: 0.713 [0.624, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.851, 10.100], loss: 0.001564, mae: 0.040632, mean_q: 1.195871
 66088/100000: episode: 1235, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 13.399, mean reward: 0.705 [0.650, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.403], loss: 0.001492, mae: 0.040917, mean_q: 1.201572
 66102/100000: episode: 1236, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 9.532, mean reward: 0.681 [0.615, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.321], loss: 0.001982, mae: 0.045540, mean_q: 1.204611
 66123/100000: episode: 1237, duration: 0.107s, episode steps: 21, steps per second: 195, episode reward: 13.073, mean reward: 0.623 [0.547, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.605, 10.240], loss: 0.001621, mae: 0.043015, mean_q: 1.203357
 66155/100000: episode: 1238, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 21.965, mean reward: 0.686 [0.518, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.211, 10.175], loss: 0.001575, mae: 0.041645, mean_q: 1.202167
 66187/100000: episode: 1239, duration: 0.190s, episode steps: 32, steps per second: 168, episode reward: 20.794, mean reward: 0.650 [0.579, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.559, 10.245], loss: 0.001769, mae: 0.045269, mean_q: 1.213708
 66219/100000: episode: 1240, duration: 0.178s, episode steps: 32, steps per second: 179, episode reward: 21.961, mean reward: 0.686 [0.533, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.352, 10.207], loss: 0.001627, mae: 0.041250, mean_q: 1.209052
 66314/100000: episode: 1241, duration: 0.499s, episode steps: 95, steps per second: 190, episode reward: 55.561, mean reward: 0.585 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.512 [-0.459, 10.227], loss: 0.001596, mae: 0.042379, mean_q: 1.202342
 66346/100000: episode: 1242, duration: 0.189s, episode steps: 32, steps per second: 169, episode reward: 20.743, mean reward: 0.648 [0.534, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.342, 10.203], loss: 0.001618, mae: 0.042746, mean_q: 1.194873
 66358/100000: episode: 1243, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 7.577, mean reward: 0.631 [0.603, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.325], loss: 0.001320, mae: 0.039658, mean_q: 1.207598
 66372/100000: episode: 1244, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 10.493, mean reward: 0.749 [0.634, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.750, 10.100], loss: 0.001512, mae: 0.041172, mean_q: 1.206008
 66464/100000: episode: 1245, duration: 0.486s, episode steps: 92, steps per second: 189, episode reward: 54.940, mean reward: 0.597 [0.505, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-0.991, 10.291], loss: 0.001551, mae: 0.042187, mean_q: 1.206548
 66496/100000: episode: 1246, duration: 0.179s, episode steps: 32, steps per second: 179, episode reward: 20.673, mean reward: 0.646 [0.573, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.318, 10.417], loss: 0.001525, mae: 0.042402, mean_q: 1.205050
 66590/100000: episode: 1247, duration: 0.499s, episode steps: 94, steps per second: 189, episode reward: 57.030, mean reward: 0.607 [0.510, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.907, 10.100], loss: 0.001605, mae: 0.042816, mean_q: 1.206969
 66604/100000: episode: 1248, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 10.196, mean reward: 0.728 [0.695, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.518, 10.100], loss: 0.001365, mae: 0.040404, mean_q: 1.208611
 66616/100000: episode: 1249, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 7.794, mean reward: 0.649 [0.602, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.284], loss: 0.001478, mae: 0.041734, mean_q: 1.209514
 66708/100000: episode: 1250, duration: 0.492s, episode steps: 92, steps per second: 187, episode reward: 54.927, mean reward: 0.597 [0.524, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.538 [-0.821, 10.206], loss: 0.001603, mae: 0.042936, mean_q: 1.206034
 66727/100000: episode: 1251, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 14.952, mean reward: 0.787 [0.659, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.651, 10.578], loss: 0.001752, mae: 0.044708, mean_q: 1.207160
[Info] Complete ISplit Iteration
[Info] Levels: [1.3672348, 1.5976658]
[Info] Cond. Prob: [0.1, 0.02]
[Info] Error Prob: 0.002

 66822/100000: episode: 1252, duration: 4.959s, episode steps: 95, steps per second: 19, episode reward: 54.965, mean reward: 0.579 [0.504, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.386, 10.100], loss: 0.001510, mae: 0.041693, mean_q: 1.210980
 66922/100000: episode: 1253, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.405, mean reward: 0.594 [0.501, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.121, 10.098], loss: 0.001499, mae: 0.042067, mean_q: 1.209773
 67022/100000: episode: 1254, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 56.310, mean reward: 0.563 [0.505, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.692, 10.130], loss: 0.001588, mae: 0.042417, mean_q: 1.209273
 67122/100000: episode: 1255, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 61.202, mean reward: 0.612 [0.519, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.264, 10.322], loss: 0.001579, mae: 0.042977, mean_q: 1.203114
 67222/100000: episode: 1256, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.183, mean reward: 0.582 [0.504, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.056, 10.098], loss: 0.001520, mae: 0.041744, mean_q: 1.208904
 67322/100000: episode: 1257, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.144, mean reward: 0.581 [0.504, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.537, 10.098], loss: 0.001510, mae: 0.041856, mean_q: 1.205803
 67422/100000: episode: 1258, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 60.279, mean reward: 0.603 [0.504, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.342, 10.286], loss: 0.001395, mae: 0.040128, mean_q: 1.209288
 67522/100000: episode: 1259, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.643, mean reward: 0.576 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.835, 10.098], loss: 0.001553, mae: 0.042102, mean_q: 1.207160
 67622/100000: episode: 1260, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.707, mean reward: 0.587 [0.509, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.783, 10.098], loss: 0.001537, mae: 0.042272, mean_q: 1.208561
 67722/100000: episode: 1261, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.263, mean reward: 0.593 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.981, 10.098], loss: 0.001494, mae: 0.041634, mean_q: 1.209796
 67822/100000: episode: 1262, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.222, mean reward: 0.582 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.310, 10.211], loss: 0.001537, mae: 0.042342, mean_q: 1.206790
 67922/100000: episode: 1263, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 62.280, mean reward: 0.623 [0.512, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.198, 10.098], loss: 0.001499, mae: 0.041077, mean_q: 1.205406
 68022/100000: episode: 1264, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 56.820, mean reward: 0.568 [0.504, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.647, 10.144], loss: 0.001458, mae: 0.041370, mean_q: 1.206440
 68122/100000: episode: 1265, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 61.717, mean reward: 0.617 [0.510, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.504, 10.253], loss: 0.001455, mae: 0.040659, mean_q: 1.208799
 68222/100000: episode: 1266, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.934, mean reward: 0.579 [0.505, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.029, 10.104], loss: 0.001465, mae: 0.041041, mean_q: 1.208877
 68322/100000: episode: 1267, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.720, mean reward: 0.577 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.883, 10.098], loss: 0.001425, mae: 0.041112, mean_q: 1.211314
 68422/100000: episode: 1268, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.917, mean reward: 0.579 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.962, 10.263], loss: 0.001479, mae: 0.041451, mean_q: 1.211579
 68522/100000: episode: 1269, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 62.576, mean reward: 0.626 [0.504, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.094, 10.098], loss: 0.001601, mae: 0.043258, mean_q: 1.205101
 68622/100000: episode: 1270, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.944, mean reward: 0.579 [0.507, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.561, 10.098], loss: 0.001398, mae: 0.040418, mean_q: 1.208184
 68722/100000: episode: 1271, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 59.454, mean reward: 0.595 [0.507, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.644, 10.222], loss: 0.001472, mae: 0.041644, mean_q: 1.203515
 68822/100000: episode: 1272, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.921, mean reward: 0.579 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.506, 10.098], loss: 0.001348, mae: 0.039625, mean_q: 1.204145
 68922/100000: episode: 1273, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.714, mean reward: 0.577 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.376, 10.269], loss: 0.001604, mae: 0.043152, mean_q: 1.206331
 69022/100000: episode: 1274, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 56.993, mean reward: 0.570 [0.503, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.494, 10.098], loss: 0.001520, mae: 0.041829, mean_q: 1.202678
 69122/100000: episode: 1275, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 60.120, mean reward: 0.601 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.102, 10.098], loss: 0.001530, mae: 0.042059, mean_q: 1.199657
 69222/100000: episode: 1276, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.550, mean reward: 0.585 [0.501, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.941, 10.098], loss: 0.001475, mae: 0.041622, mean_q: 1.201370
 69322/100000: episode: 1277, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.027, mean reward: 0.590 [0.503, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.580, 10.101], loss: 0.001520, mae: 0.041448, mean_q: 1.198522
 69422/100000: episode: 1278, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 60.216, mean reward: 0.602 [0.499, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.137, 10.461], loss: 0.001634, mae: 0.043598, mean_q: 1.197033
 69522/100000: episode: 1279, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.574, mean reward: 0.606 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.495, 10.098], loss: 0.001442, mae: 0.041765, mean_q: 1.196337
 69622/100000: episode: 1280, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.995, mean reward: 0.580 [0.499, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.114, 10.098], loss: 0.001407, mae: 0.040431, mean_q: 1.196100
 69722/100000: episode: 1281, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 56.173, mean reward: 0.562 [0.498, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.213, 10.098], loss: 0.001622, mae: 0.043100, mean_q: 1.193385
 69822/100000: episode: 1282, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.204, mean reward: 0.572 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.418, 10.284], loss: 0.001526, mae: 0.041827, mean_q: 1.187640
 69922/100000: episode: 1283, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.910, mean reward: 0.589 [0.507, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.938, 10.114], loss: 0.001486, mae: 0.041800, mean_q: 1.191652
 70022/100000: episode: 1284, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: 58.514, mean reward: 0.585 [0.504, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.563, 10.098], loss: 0.001642, mae: 0.043912, mean_q: 1.184055
 70122/100000: episode: 1285, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.418, mean reward: 0.584 [0.500, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.030, 10.107], loss: 0.001699, mae: 0.043899, mean_q: 1.184787
 70222/100000: episode: 1286, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.644, mean reward: 0.576 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.457, 10.098], loss: 0.001533, mae: 0.042052, mean_q: 1.186546
 70322/100000: episode: 1287, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.729, mean reward: 0.567 [0.502, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.400, 10.167], loss: 0.001458, mae: 0.041450, mean_q: 1.182167
 70422/100000: episode: 1288, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.383, mean reward: 0.594 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.483, 10.098], loss: 0.001536, mae: 0.042190, mean_q: 1.181699
 70522/100000: episode: 1289, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.389, mean reward: 0.594 [0.511, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.917, 10.343], loss: 0.001449, mae: 0.041377, mean_q: 1.179962
 70622/100000: episode: 1290, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.046, mean reward: 0.580 [0.502, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.933, 10.172], loss: 0.001402, mae: 0.040870, mean_q: 1.181090
 70722/100000: episode: 1291, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.603, mean reward: 0.606 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.367, 10.098], loss: 0.001564, mae: 0.042425, mean_q: 1.176266
 70822/100000: episode: 1292, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.864, mean reward: 0.589 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.820, 10.098], loss: 0.001433, mae: 0.041553, mean_q: 1.179061
 70922/100000: episode: 1293, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 66.876, mean reward: 0.669 [0.501, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-0.963, 10.098], loss: 0.001471, mae: 0.040966, mean_q: 1.174022
 71022/100000: episode: 1294, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.374, mean reward: 0.584 [0.517, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.578, 10.098], loss: 0.001451, mae: 0.041709, mean_q: 1.173395
 71122/100000: episode: 1295, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.643, mean reward: 0.576 [0.500, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.689, 10.099], loss: 0.001516, mae: 0.042391, mean_q: 1.173666
 71222/100000: episode: 1296, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 59.227, mean reward: 0.592 [0.513, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.157, 10.098], loss: 0.001570, mae: 0.042322, mean_q: 1.166006
 71322/100000: episode: 1297, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.867, mean reward: 0.599 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.933, 10.285], loss: 0.001521, mae: 0.042300, mean_q: 1.171290
 71422/100000: episode: 1298, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 59.311, mean reward: 0.593 [0.507, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.899, 10.330], loss: 0.001493, mae: 0.041805, mean_q: 1.172757
 71522/100000: episode: 1299, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 60.682, mean reward: 0.607 [0.514, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.904, 10.098], loss: 0.001392, mae: 0.040476, mean_q: 1.167068
 71622/100000: episode: 1300, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.586, mean reward: 0.596 [0.507, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.805, 10.229], loss: 0.001548, mae: 0.042600, mean_q: 1.168604
 71722/100000: episode: 1301, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.183, mean reward: 0.602 [0.512, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.841, 10.238], loss: 0.001405, mae: 0.040860, mean_q: 1.166599
 71822/100000: episode: 1302, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.490, mean reward: 0.575 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.533, 10.098], loss: 0.001430, mae: 0.041483, mean_q: 1.163637
 71922/100000: episode: 1303, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.690, mean reward: 0.587 [0.507, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.640, 10.098], loss: 0.001369, mae: 0.040712, mean_q: 1.164725
 72022/100000: episode: 1304, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 60.484, mean reward: 0.605 [0.518, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.634, 10.497], loss: 0.001390, mae: 0.041103, mean_q: 1.166043
 72122/100000: episode: 1305, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.747, mean reward: 0.587 [0.503, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.145, 10.183], loss: 0.001415, mae: 0.041115, mean_q: 1.166260
 72222/100000: episode: 1306, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.440, mean reward: 0.594 [0.508, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.828, 10.105], loss: 0.001308, mae: 0.039952, mean_q: 1.163250
 72322/100000: episode: 1307, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.288, mean reward: 0.593 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.636, 10.098], loss: 0.001431, mae: 0.040876, mean_q: 1.166706
 72422/100000: episode: 1308, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.731, mean reward: 0.597 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.990, 10.356], loss: 0.001353, mae: 0.039911, mean_q: 1.169158
 72522/100000: episode: 1309, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.716, mean reward: 0.587 [0.504, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.583, 10.169], loss: 0.001660, mae: 0.044846, mean_q: 1.170420
 72622/100000: episode: 1310, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.363, mean reward: 0.584 [0.514, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.146, 10.098], loss: 0.001457, mae: 0.041768, mean_q: 1.167668
 72722/100000: episode: 1311, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.350, mean reward: 0.584 [0.507, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.236, 10.145], loss: 0.001374, mae: 0.040705, mean_q: 1.165160
 72822/100000: episode: 1312, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.612, mean reward: 0.566 [0.507, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.527, 10.098], loss: 0.001378, mae: 0.040298, mean_q: 1.167242
 72922/100000: episode: 1313, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.850, mean reward: 0.579 [0.516, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.259, 10.155], loss: 0.001501, mae: 0.042384, mean_q: 1.168365
 73022/100000: episode: 1314, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.210, mean reward: 0.582 [0.512, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.801, 10.098], loss: 0.001518, mae: 0.042130, mean_q: 1.164560
 73122/100000: episode: 1315, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 58.132, mean reward: 0.581 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.991, 10.098], loss: 0.001329, mae: 0.040187, mean_q: 1.162776
 73222/100000: episode: 1316, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.504, mean reward: 0.595 [0.502, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.294, 10.098], loss: 0.001281, mae: 0.039343, mean_q: 1.161352
 73322/100000: episode: 1317, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.935, mean reward: 0.589 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.082, 10.098], loss: 0.001406, mae: 0.040773, mean_q: 1.164397
 73422/100000: episode: 1318, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 59.132, mean reward: 0.591 [0.507, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.481, 10.246], loss: 0.001368, mae: 0.040420, mean_q: 1.166303
 73522/100000: episode: 1319, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 60.976, mean reward: 0.610 [0.507, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.480, 10.122], loss: 0.001437, mae: 0.040974, mean_q: 1.166931
 73622/100000: episode: 1320, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 56.955, mean reward: 0.570 [0.505, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.367, 10.287], loss: 0.001573, mae: 0.043027, mean_q: 1.164926
 73722/100000: episode: 1321, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.076, mean reward: 0.571 [0.505, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.645, 10.170], loss: 0.001446, mae: 0.041435, mean_q: 1.165211
 73822/100000: episode: 1322, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.495, mean reward: 0.595 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.951, 10.132], loss: 0.001385, mae: 0.040194, mean_q: 1.165910
 73922/100000: episode: 1323, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.353, mean reward: 0.584 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.328, 10.098], loss: 0.001463, mae: 0.041690, mean_q: 1.167371
 74022/100000: episode: 1324, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.541, mean reward: 0.575 [0.509, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.347, 10.098], loss: 0.001404, mae: 0.040487, mean_q: 1.165153
 74122/100000: episode: 1325, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.433, mean reward: 0.604 [0.519, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.370, 10.098], loss: 0.001351, mae: 0.039790, mean_q: 1.163887
 74222/100000: episode: 1326, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.795, mean reward: 0.598 [0.515, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.394, 10.392], loss: 0.001385, mae: 0.040888, mean_q: 1.164546
 74322/100000: episode: 1327, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.476, mean reward: 0.595 [0.503, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.787, 10.303], loss: 0.001498, mae: 0.041796, mean_q: 1.165958
 74422/100000: episode: 1328, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.782, mean reward: 0.588 [0.500, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.145, 10.130], loss: 0.001393, mae: 0.040822, mean_q: 1.166330
 74522/100000: episode: 1329, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.054, mean reward: 0.591 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.986, 10.126], loss: 0.001522, mae: 0.042468, mean_q: 1.166300
 74622/100000: episode: 1330, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.398, mean reward: 0.584 [0.512, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.877, 10.098], loss: 0.001383, mae: 0.040767, mean_q: 1.161614
 74722/100000: episode: 1331, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.295, mean reward: 0.603 [0.505, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.284, 10.098], loss: 0.001390, mae: 0.040658, mean_q: 1.165028
 74822/100000: episode: 1332, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.148, mean reward: 0.591 [0.504, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.801, 10.185], loss: 0.001455, mae: 0.040447, mean_q: 1.165035
 74922/100000: episode: 1333, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 62.337, mean reward: 0.623 [0.500, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.887, 10.098], loss: 0.001363, mae: 0.040186, mean_q: 1.164749
 75022/100000: episode: 1334, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 60.400, mean reward: 0.604 [0.516, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.440, 10.098], loss: 0.001369, mae: 0.040307, mean_q: 1.168979
 75122/100000: episode: 1335, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.608, mean reward: 0.586 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.468, 10.098], loss: 0.001469, mae: 0.041895, mean_q: 1.170330
 75222/100000: episode: 1336, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.390, mean reward: 0.594 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.739, 10.497], loss: 0.001439, mae: 0.041185, mean_q: 1.168711
 75322/100000: episode: 1337, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.771, mean reward: 0.588 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.213, 10.293], loss: 0.001481, mae: 0.042054, mean_q: 1.172571
 75422/100000: episode: 1338, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.236, mean reward: 0.592 [0.500, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.354, 10.098], loss: 0.001359, mae: 0.039938, mean_q: 1.170691
 75522/100000: episode: 1339, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.125, mean reward: 0.581 [0.498, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.748, 10.173], loss: 0.001432, mae: 0.041370, mean_q: 1.169696
 75622/100000: episode: 1340, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.859, mean reward: 0.599 [0.509, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.256, 10.098], loss: 0.001447, mae: 0.041879, mean_q: 1.168752
 75722/100000: episode: 1341, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.580, mean reward: 0.596 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.464, 10.133], loss: 0.001383, mae: 0.040680, mean_q: 1.168089
 75822/100000: episode: 1342, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.494, mean reward: 0.585 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.518, 10.129], loss: 0.001392, mae: 0.041228, mean_q: 1.173289
 75922/100000: episode: 1343, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 62.586, mean reward: 0.626 [0.517, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.760, 10.098], loss: 0.001416, mae: 0.041595, mean_q: 1.168184
 76022/100000: episode: 1344, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.534, mean reward: 0.575 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.950, 10.098], loss: 0.001295, mae: 0.039763, mean_q: 1.166753
 76122/100000: episode: 1345, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.235, mean reward: 0.562 [0.499, 0.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.515, 10.117], loss: 0.001431, mae: 0.041315, mean_q: 1.167261
 76222/100000: episode: 1346, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.614, mean reward: 0.586 [0.503, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.498, 10.245], loss: 0.001453, mae: 0.041637, mean_q: 1.165785
 76322/100000: episode: 1347, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.603, mean reward: 0.586 [0.502, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.168, 10.129], loss: 0.001383, mae: 0.040955, mean_q: 1.170600
 76422/100000: episode: 1348, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.273, mean reward: 0.593 [0.506, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.466, 10.098], loss: 0.001382, mae: 0.040545, mean_q: 1.165958
 76522/100000: episode: 1349, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.963, mean reward: 0.600 [0.514, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.942, 10.098], loss: 0.001397, mae: 0.040777, mean_q: 1.167423
 76622/100000: episode: 1350, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.424, mean reward: 0.594 [0.499, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.949, 10.098], loss: 0.001375, mae: 0.040477, mean_q: 1.166409
 76722/100000: episode: 1351, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 61.450, mean reward: 0.614 [0.517, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.948, 10.098], loss: 0.001430, mae: 0.041722, mean_q: 1.167246
[Info] 1-TH LEVEL FOUND: 1.3869037628173828, Considering 10/90 traces
 76822/100000: episode: 1352, duration: 4.861s, episode steps: 100, steps per second: 21, episode reward: 62.136, mean reward: 0.621 [0.503, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.446, 10.291], loss: 0.001365, mae: 0.041441, mean_q: 1.170122
 76858/100000: episode: 1353, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 25.356, mean reward: 0.704 [0.551, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.476, 10.362], loss: 0.001494, mae: 0.042585, mean_q: 1.164023
 76865/100000: episode: 1354, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 4.641, mean reward: 0.663 [0.592, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.227, 10.100], loss: 0.001376, mae: 0.040425, mean_q: 1.170974
 76888/100000: episode: 1355, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 16.335, mean reward: 0.710 [0.674, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.295, 10.448], loss: 0.001541, mae: 0.043304, mean_q: 1.171095
 76974/100000: episode: 1356, duration: 0.441s, episode steps: 86, steps per second: 195, episode reward: 54.697, mean reward: 0.636 [0.557, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.590 [-1.016, 10.100], loss: 0.001438, mae: 0.041595, mean_q: 1.174301
 77013/100000: episode: 1357, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 26.452, mean reward: 0.678 [0.585, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.879, 10.100], loss: 0.001485, mae: 0.041001, mean_q: 1.175039
 77025/100000: episode: 1358, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 8.391, mean reward: 0.699 [0.645, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.863, 10.100], loss: 0.001601, mae: 0.043754, mean_q: 1.167170
 77032/100000: episode: 1359, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 4.756, mean reward: 0.679 [0.622, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.816, 10.100], loss: 0.001602, mae: 0.043992, mean_q: 1.170949
 77055/100000: episode: 1360, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 16.018, mean reward: 0.696 [0.639, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.310, 10.307], loss: 0.001395, mae: 0.041204, mean_q: 1.167205
 77151/100000: episode: 1361, duration: 0.534s, episode steps: 96, steps per second: 180, episode reward: 56.683, mean reward: 0.590 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-0.833, 10.100], loss: 0.001463, mae: 0.041345, mean_q: 1.173747
 77174/100000: episode: 1362, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 17.074, mean reward: 0.742 [0.672, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.435, 10.435], loss: 0.001425, mae: 0.041011, mean_q: 1.181172
 77181/100000: episode: 1363, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 4.733, mean reward: 0.676 [0.615, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.214, 10.100], loss: 0.001466, mae: 0.040604, mean_q: 1.179990
 77188/100000: episode: 1364, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 4.392, mean reward: 0.627 [0.558, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.162, 10.100], loss: 0.001781, mae: 0.044523, mean_q: 1.176629
 77211/100000: episode: 1365, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 15.318, mean reward: 0.666 [0.599, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.278], loss: 0.001535, mae: 0.042648, mean_q: 1.180465
 77297/100000: episode: 1366, duration: 0.457s, episode steps: 86, steps per second: 188, episode reward: 51.157, mean reward: 0.595 [0.508, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.580 [-0.702, 10.100], loss: 0.001454, mae: 0.041810, mean_q: 1.178960
 77320/100000: episode: 1367, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 16.750, mean reward: 0.728 [0.602, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.828, 10.326], loss: 0.001392, mae: 0.040139, mean_q: 1.174944
 77375/100000: episode: 1368, duration: 0.308s, episode steps: 55, steps per second: 178, episode reward: 36.701, mean reward: 0.667 [0.542, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.230, 10.100], loss: 0.001581, mae: 0.042359, mean_q: 1.176589
 77398/100000: episode: 1369, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 16.307, mean reward: 0.709 [0.646, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.385], loss: 0.001513, mae: 0.041722, mean_q: 1.180740
 77427/100000: episode: 1370, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 18.380, mean reward: 0.634 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.338, 10.160], loss: 0.001586, mae: 0.043199, mean_q: 1.176432
 77434/100000: episode: 1371, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.633, mean reward: 0.662 [0.584, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.556, 10.100], loss: 0.001422, mae: 0.042040, mean_q: 1.179277
 77457/100000: episode: 1372, duration: 0.127s, episode steps: 23, steps per second: 180, episode reward: 16.609, mean reward: 0.722 [0.682, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-1.176, 10.493], loss: 0.001608, mae: 0.042930, mean_q: 1.170265
 77553/100000: episode: 1373, duration: 0.512s, episode steps: 96, steps per second: 187, episode reward: 56.922, mean reward: 0.593 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-1.161, 10.251], loss: 0.001570, mae: 0.042961, mean_q: 1.181863
 77576/100000: episode: 1374, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 17.339, mean reward: 0.754 [0.696, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.362, 10.518], loss: 0.001828, mae: 0.045990, mean_q: 1.183270
 77612/100000: episode: 1375, duration: 0.195s, episode steps: 36, steps per second: 184, episode reward: 21.869, mean reward: 0.607 [0.520, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.035, 10.281], loss: 0.001362, mae: 0.040767, mean_q: 1.184556
 77641/100000: episode: 1376, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 18.832, mean reward: 0.649 [0.578, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.168, 10.150], loss: 0.001334, mae: 0.040221, mean_q: 1.177624
 77677/100000: episode: 1377, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 24.359, mean reward: 0.677 [0.602, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.191, 10.325], loss: 0.001363, mae: 0.040676, mean_q: 1.179197
 77732/100000: episode: 1378, duration: 0.293s, episode steps: 55, steps per second: 188, episode reward: 34.998, mean reward: 0.636 [0.571, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.247, 10.100], loss: 0.001399, mae: 0.040216, mean_q: 1.187624
 77739/100000: episode: 1379, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 4.846, mean reward: 0.692 [0.640, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.475, 10.100], loss: 0.001413, mae: 0.040835, mean_q: 1.184027
 77778/100000: episode: 1380, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 27.820, mean reward: 0.713 [0.648, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.001, 10.100], loss: 0.001369, mae: 0.039929, mean_q: 1.189440
 77817/100000: episode: 1381, duration: 0.219s, episode steps: 39, steps per second: 178, episode reward: 29.099, mean reward: 0.746 [0.668, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.427, 10.100], loss: 0.001734, mae: 0.045218, mean_q: 1.191866
 77856/100000: episode: 1382, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 25.594, mean reward: 0.656 [0.525, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.114, 10.135], loss: 0.001450, mae: 0.041661, mean_q: 1.191641
 77868/100000: episode: 1383, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 7.950, mean reward: 0.663 [0.596, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.087, 10.100], loss: 0.001404, mae: 0.039605, mean_q: 1.184204
 77911/100000: episode: 1384, duration: 0.218s, episode steps: 43, steps per second: 198, episode reward: 26.428, mean reward: 0.615 [0.504, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.399, 10.118], loss: 0.001674, mae: 0.044162, mean_q: 1.190718
 77918/100000: episode: 1385, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 5.026, mean reward: 0.718 [0.663, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.652, 10.100], loss: 0.001216, mae: 0.038976, mean_q: 1.198948
 77930/100000: episode: 1386, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 8.526, mean reward: 0.710 [0.613, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.493, 10.100], loss: 0.001841, mae: 0.046267, mean_q: 1.195967
 78016/100000: episode: 1387, duration: 0.481s, episode steps: 86, steps per second: 179, episode reward: 52.013, mean reward: 0.605 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.578 [-0.374, 10.162], loss: 0.001881, mae: 0.045739, mean_q: 1.192913
 78071/100000: episode: 1388, duration: 0.287s, episode steps: 55, steps per second: 192, episode reward: 34.566, mean reward: 0.628 [0.507, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.655, 10.100], loss: 0.001493, mae: 0.042239, mean_q: 1.195007
 78126/100000: episode: 1389, duration: 0.274s, episode steps: 55, steps per second: 201, episode reward: 37.471, mean reward: 0.681 [0.607, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.847 [-1.090, 10.100], loss: 0.001717, mae: 0.044077, mean_q: 1.195973
 78155/100000: episode: 1390, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 18.295, mean reward: 0.631 [0.523, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.528, 10.264], loss: 0.001895, mae: 0.043441, mean_q: 1.194513
 78191/100000: episode: 1391, duration: 0.197s, episode steps: 36, steps per second: 182, episode reward: 22.279, mean reward: 0.619 [0.540, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.756, 10.255], loss: 0.001923, mae: 0.047978, mean_q: 1.204275
 78227/100000: episode: 1392, duration: 0.211s, episode steps: 36, steps per second: 171, episode reward: 22.920, mean reward: 0.637 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.035, 10.158], loss: 0.001433, mae: 0.041484, mean_q: 1.197434
 78239/100000: episode: 1393, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 8.432, mean reward: 0.703 [0.668, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.306, 10.100], loss: 0.001601, mae: 0.042691, mean_q: 1.188337
 78268/100000: episode: 1394, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 18.745, mean reward: 0.646 [0.600, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.444], loss: 0.001510, mae: 0.041155, mean_q: 1.189139
 78297/100000: episode: 1395, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 18.351, mean reward: 0.633 [0.543, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.304, 10.205], loss: 0.001554, mae: 0.042086, mean_q: 1.199965
 78340/100000: episode: 1396, duration: 0.222s, episode steps: 43, steps per second: 193, episode reward: 29.379, mean reward: 0.683 [0.626, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.726, 10.388], loss: 0.001567, mae: 0.042256, mean_q: 1.198275
 78383/100000: episode: 1397, duration: 0.223s, episode steps: 43, steps per second: 193, episode reward: 29.859, mean reward: 0.694 [0.624, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-1.076, 10.398], loss: 0.001766, mae: 0.043656, mean_q: 1.197950
 78426/100000: episode: 1398, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 29.018, mean reward: 0.675 [0.526, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.335, 10.191], loss: 0.001442, mae: 0.040954, mean_q: 1.197761
 78449/100000: episode: 1399, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 15.637, mean reward: 0.680 [0.600, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.494, 10.429], loss: 0.001475, mae: 0.041521, mean_q: 1.201523
 78535/100000: episode: 1400, duration: 0.446s, episode steps: 86, steps per second: 193, episode reward: 50.196, mean reward: 0.584 [0.505, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.591 [-0.684, 10.228], loss: 0.001479, mae: 0.041258, mean_q: 1.203904
 78574/100000: episode: 1401, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 27.340, mean reward: 0.701 [0.591, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.915, 10.100], loss: 0.001519, mae: 0.040303, mean_q: 1.205416
 78660/100000: episode: 1402, duration: 0.437s, episode steps: 86, steps per second: 197, episode reward: 54.452, mean reward: 0.633 [0.535, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.593 [-0.301, 10.100], loss: 0.001534, mae: 0.041870, mean_q: 1.206614
 78667/100000: episode: 1403, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 4.770, mean reward: 0.681 [0.595, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.134, 10.100], loss: 0.001523, mae: 0.040046, mean_q: 1.206641
 78763/100000: episode: 1404, duration: 0.528s, episode steps: 96, steps per second: 182, episode reward: 55.321, mean reward: 0.576 [0.508, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.603, 10.100], loss: 0.001557, mae: 0.042007, mean_q: 1.204138
 78799/100000: episode: 1405, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 22.714, mean reward: 0.631 [0.549, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.931, 10.250], loss: 0.001565, mae: 0.042186, mean_q: 1.204249
 78822/100000: episode: 1406, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 14.135, mean reward: 0.615 [0.531, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.179], loss: 0.001385, mae: 0.039636, mean_q: 1.211028
 78858/100000: episode: 1407, duration: 0.174s, episode steps: 36, steps per second: 207, episode reward: 22.427, mean reward: 0.623 [0.497, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.822, 10.114], loss: 0.001598, mae: 0.042030, mean_q: 1.205549
 78954/100000: episode: 1408, duration: 0.526s, episode steps: 96, steps per second: 183, episode reward: 56.237, mean reward: 0.586 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.220, 10.186], loss: 0.001478, mae: 0.041537, mean_q: 1.207340
 78977/100000: episode: 1409, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 17.008, mean reward: 0.739 [0.658, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.759, 10.337], loss: 0.001394, mae: 0.040762, mean_q: 1.217424
 79063/100000: episode: 1410, duration: 0.468s, episode steps: 86, steps per second: 184, episode reward: 50.438, mean reward: 0.586 [0.509, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.590 [-1.049, 10.207], loss: 0.001505, mae: 0.041670, mean_q: 1.208727
 79099/100000: episode: 1411, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 24.536, mean reward: 0.682 [0.597, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.491, 10.428], loss: 0.001558, mae: 0.042571, mean_q: 1.213877
 79135/100000: episode: 1412, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 24.078, mean reward: 0.669 [0.581, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.939, 10.348], loss: 0.001564, mae: 0.042356, mean_q: 1.202638
 79190/100000: episode: 1413, duration: 0.289s, episode steps: 55, steps per second: 191, episode reward: 40.094, mean reward: 0.729 [0.505, 0.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.667, 10.100], loss: 0.001434, mae: 0.041341, mean_q: 1.215264
 79276/100000: episode: 1414, duration: 0.461s, episode steps: 86, steps per second: 186, episode reward: 52.293, mean reward: 0.608 [0.534, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.598 [-0.703, 10.310], loss: 0.001600, mae: 0.043184, mean_q: 1.217149
 79372/100000: episode: 1415, duration: 0.529s, episode steps: 96, steps per second: 182, episode reward: 61.125, mean reward: 0.637 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-1.024, 10.122], loss: 0.001455, mae: 0.041306, mean_q: 1.208651
 79427/100000: episode: 1416, duration: 0.309s, episode steps: 55, steps per second: 178, episode reward: 37.192, mean reward: 0.676 [0.546, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.383, 10.100], loss: 0.001485, mae: 0.041764, mean_q: 1.212714
 79470/100000: episode: 1417, duration: 0.221s, episode steps: 43, steps per second: 194, episode reward: 28.730, mean reward: 0.668 [0.580, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.694, 10.408], loss: 0.001461, mae: 0.040558, mean_q: 1.217558
 79566/100000: episode: 1418, duration: 0.529s, episode steps: 96, steps per second: 181, episode reward: 57.713, mean reward: 0.601 [0.499, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-0.659, 10.100], loss: 0.001528, mae: 0.041872, mean_q: 1.217733
 79595/100000: episode: 1419, duration: 0.172s, episode steps: 29, steps per second: 168, episode reward: 18.137, mean reward: 0.625 [0.512, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.084, 10.125], loss: 0.001444, mae: 0.040951, mean_q: 1.214592
 79607/100000: episode: 1420, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 8.586, mean reward: 0.716 [0.652, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.290, 10.100], loss: 0.001202, mae: 0.037971, mean_q: 1.223895
 79630/100000: episode: 1421, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 16.500, mean reward: 0.717 [0.637, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.173, 10.477], loss: 0.001321, mae: 0.039323, mean_q: 1.223180
 79659/100000: episode: 1422, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 18.230, mean reward: 0.629 [0.540, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.235], loss: 0.001560, mae: 0.043069, mean_q: 1.217158
 79682/100000: episode: 1423, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 16.028, mean reward: 0.697 [0.642, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.099, 10.459], loss: 0.001985, mae: 0.046814, mean_q: 1.215976
 79768/100000: episode: 1424, duration: 0.457s, episode steps: 86, steps per second: 188, episode reward: 50.618, mean reward: 0.589 [0.507, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.586 [-0.539, 10.132], loss: 0.001400, mae: 0.040403, mean_q: 1.220811
 79775/100000: episode: 1425, duration: 0.059s, episode steps: 7, steps per second: 119, episode reward: 4.837, mean reward: 0.691 [0.671, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.164, 10.100], loss: 0.001395, mae: 0.040547, mean_q: 1.205810
 79871/100000: episode: 1426, duration: 0.497s, episode steps: 96, steps per second: 193, episode reward: 56.075, mean reward: 0.584 [0.516, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-1.153, 10.170], loss: 0.001392, mae: 0.040616, mean_q: 1.214086
 79907/100000: episode: 1427, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 24.060, mean reward: 0.668 [0.508, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.569, 10.124], loss: 0.001337, mae: 0.040245, mean_q: 1.219594
 79914/100000: episode: 1428, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 4.572, mean reward: 0.653 [0.589, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.311, 10.100], loss: 0.001989, mae: 0.043504, mean_q: 1.205209
 80000/100000: episode: 1429, duration: 0.470s, episode steps: 86, steps per second: 183, episode reward: 52.200, mean reward: 0.607 [0.514, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.576 [-0.775, 10.100], loss: 0.001522, mae: 0.041894, mean_q: 1.216685
 80023/100000: episode: 1430, duration: 0.147s, episode steps: 23, steps per second: 157, episode reward: 16.239, mean reward: 0.706 [0.653, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.604, 10.416], loss: 0.001476, mae: 0.039762, mean_q: 1.220880
 80052/100000: episode: 1431, duration: 0.175s, episode steps: 29, steps per second: 166, episode reward: 19.370, mean reward: 0.668 [0.597, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.485, 10.340], loss: 0.001677, mae: 0.042634, mean_q: 1.218351
 80148/100000: episode: 1432, duration: 0.536s, episode steps: 96, steps per second: 179, episode reward: 56.226, mean reward: 0.586 [0.510, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.744, 10.100], loss: 0.001458, mae: 0.041333, mean_q: 1.225350
 80187/100000: episode: 1433, duration: 0.230s, episode steps: 39, steps per second: 169, episode reward: 25.338, mean reward: 0.650 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.484, 10.201], loss: 0.001446, mae: 0.040417, mean_q: 1.211400
 80230/100000: episode: 1434, duration: 0.248s, episode steps: 43, steps per second: 174, episode reward: 27.794, mean reward: 0.646 [0.517, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-2.003, 10.183], loss: 0.001215, mae: 0.038423, mean_q: 1.221233
 80259/100000: episode: 1435, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 18.864, mean reward: 0.650 [0.556, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.035, 10.268], loss: 0.001500, mae: 0.041473, mean_q: 1.226691
 80271/100000: episode: 1436, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 8.384, mean reward: 0.699 [0.632, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.304, 10.100], loss: 0.001426, mae: 0.041693, mean_q: 1.205005
 80294/100000: episode: 1437, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 16.105, mean reward: 0.700 [0.638, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.065, 10.327], loss: 0.001340, mae: 0.039971, mean_q: 1.216632
 80317/100000: episode: 1438, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 17.824, mean reward: 0.775 [0.683, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.510], loss: 0.001483, mae: 0.040958, mean_q: 1.216847
 80403/100000: episode: 1439, duration: 0.485s, episode steps: 86, steps per second: 177, episode reward: 50.061, mean reward: 0.582 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.577 [-0.873, 10.189], loss: 0.001315, mae: 0.040124, mean_q: 1.222627
 80426/100000: episode: 1440, duration: 0.115s, episode steps: 23, steps per second: 201, episode reward: 16.037, mean reward: 0.697 [0.623, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.355], loss: 0.001524, mae: 0.041762, mean_q: 1.221051
 80455/100000: episode: 1441, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 18.761, mean reward: 0.647 [0.583, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.345, 10.345], loss: 0.001453, mae: 0.040868, mean_q: 1.231591
[Info] 2-TH LEVEL FOUND: 1.499558687210083, Considering 10/90 traces
 80462/100000: episode: 1442, duration: 4.332s, episode steps: 7, steps per second: 2, episode reward: 4.829, mean reward: 0.690 [0.627, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.510, 10.100], loss: 0.001126, mae: 0.037603, mean_q: 1.210461
 80479/100000: episode: 1443, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 13.430, mean reward: 0.790 [0.715, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.073, 10.533], loss: 0.001314, mae: 0.037934, mean_q: 1.228622
 80491/100000: episode: 1444, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 8.513, mean reward: 0.709 [0.653, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.338], loss: 0.001459, mae: 0.041109, mean_q: 1.212922
 80526/100000: episode: 1445, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 27.570, mean reward: 0.788 [0.674, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.616, 10.100], loss: 0.001218, mae: 0.038587, mean_q: 1.233702
[Info] FALSIFICATION!
 80556/100000: episode: 1446, duration: 0.415s, episode steps: 30, steps per second: 72, episode reward: 22.890, mean reward: 0.763 [0.623, 1.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.342, 9.988], loss: 0.001476, mae: 0.041441, mean_q: 1.229850
 80591/100000: episode: 1447, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 23.758, mean reward: 0.679 [0.524, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.355, 10.100], loss: 0.001225, mae: 0.037661, mean_q: 1.234360
 80628/100000: episode: 1448, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 27.661, mean reward: 0.748 [0.604, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.549, 10.100], loss: 0.001305, mae: 0.039833, mean_q: 1.244968
 80640/100000: episode: 1449, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 8.434, mean reward: 0.703 [0.631, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.058, 10.387], loss: 0.001284, mae: 0.038051, mean_q: 1.227878
 80675/100000: episode: 1450, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 24.001, mean reward: 0.686 [0.589, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.330, 10.100], loss: 0.001388, mae: 0.039599, mean_q: 1.237664
 80710/100000: episode: 1451, duration: 0.185s, episode steps: 35, steps per second: 190, episode reward: 24.811, mean reward: 0.709 [0.625, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.307, 10.100], loss: 0.001371, mae: 0.039116, mean_q: 1.234843
 80745/100000: episode: 1452, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 22.489, mean reward: 0.643 [0.583, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.322, 10.100], loss: 0.001423, mae: 0.039712, mean_q: 1.235316
 80785/100000: episode: 1453, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 28.062, mean reward: 0.702 [0.639, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.613, 10.100], loss: 0.001493, mae: 0.040804, mean_q: 1.238125
 80825/100000: episode: 1454, duration: 0.233s, episode steps: 40, steps per second: 172, episode reward: 32.080, mean reward: 0.802 [0.687, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.620, 10.100], loss: 0.001536, mae: 0.043404, mean_q: 1.238168
 80839/100000: episode: 1455, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 11.251, mean reward: 0.804 [0.743, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.568], loss: 0.001550, mae: 0.043333, mean_q: 1.250237
 80876/100000: episode: 1456, duration: 0.221s, episode steps: 37, steps per second: 167, episode reward: 26.317, mean reward: 0.711 [0.594, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.854, 10.100], loss: 0.001396, mae: 0.040765, mean_q: 1.241956
 80912/100000: episode: 1457, duration: 0.197s, episode steps: 36, steps per second: 182, episode reward: 22.283, mean reward: 0.619 [0.531, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.578, 10.100], loss: 0.001400, mae: 0.040303, mean_q: 1.243927
 80929/100000: episode: 1458, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 12.838, mean reward: 0.755 [0.707, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.941, 10.528], loss: 0.001372, mae: 0.040377, mean_q: 1.241634
 80965/100000: episode: 1459, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 23.700, mean reward: 0.658 [0.546, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.487, 10.100], loss: 0.001387, mae: 0.040478, mean_q: 1.249623
 81000/100000: episode: 1460, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 23.286, mean reward: 0.665 [0.577, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.078, 10.100], loss: 0.001671, mae: 0.044663, mean_q: 1.251721
 81036/100000: episode: 1461, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 29.306, mean reward: 0.814 [0.678, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.377, 10.100], loss: 0.001420, mae: 0.042073, mean_q: 1.240709
 81053/100000: episode: 1462, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 13.407, mean reward: 0.789 [0.640, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.356], loss: 0.001279, mae: 0.040173, mean_q: 1.249368
 81070/100000: episode: 1463, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 13.575, mean reward: 0.799 [0.735, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.352, 10.465], loss: 0.001444, mae: 0.039407, mean_q: 1.257722
 81084/100000: episode: 1464, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 10.694, mean reward: 0.764 [0.719, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.546], loss: 0.001329, mae: 0.040191, mean_q: 1.254792
 81120/100000: episode: 1465, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 26.548, mean reward: 0.737 [0.620, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.216, 10.100], loss: 0.001352, mae: 0.040740, mean_q: 1.265903
 81156/100000: episode: 1466, duration: 0.183s, episode steps: 36, steps per second: 196, episode reward: 28.424, mean reward: 0.790 [0.715, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-1.788, 10.100], loss: 0.001530, mae: 0.042436, mean_q: 1.257941
 81191/100000: episode: 1467, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 25.353, mean reward: 0.724 [0.619, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.531, 10.100], loss: 0.001301, mae: 0.038886, mean_q: 1.254571
 81210/100000: episode: 1468, duration: 0.119s, episode steps: 19, steps per second: 159, episode reward: 14.696, mean reward: 0.773 [0.691, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.435], loss: 0.001293, mae: 0.039979, mean_q: 1.272319
 81245/100000: episode: 1469, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 23.016, mean reward: 0.658 [0.561, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.068, 10.100], loss: 0.001324, mae: 0.039582, mean_q: 1.266393
 81285/100000: episode: 1470, duration: 0.230s, episode steps: 40, steps per second: 174, episode reward: 29.693, mean reward: 0.742 [0.588, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.224, 10.100], loss: 0.001332, mae: 0.039936, mean_q: 1.256171
 81320/100000: episode: 1471, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 23.264, mean reward: 0.665 [0.528, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.506, 10.100], loss: 0.001345, mae: 0.040458, mean_q: 1.270015
 81332/100000: episode: 1472, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 9.042, mean reward: 0.754 [0.673, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.452], loss: 0.001158, mae: 0.037359, mean_q: 1.261752
 81367/100000: episode: 1473, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 24.436, mean reward: 0.698 [0.633, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.255, 10.100], loss: 0.001383, mae: 0.039244, mean_q: 1.272118
 81386/100000: episode: 1474, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 14.316, mean reward: 0.753 [0.673, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.447, 10.418], loss: 0.001646, mae: 0.042945, mean_q: 1.277813
 81421/100000: episode: 1475, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 21.420, mean reward: 0.612 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.344, 10.100], loss: 0.001360, mae: 0.040537, mean_q: 1.269820
 81457/100000: episode: 1476, duration: 0.187s, episode steps: 36, steps per second: 192, episode reward: 24.356, mean reward: 0.677 [0.542, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.493, 10.100], loss: 0.001526, mae: 0.042169, mean_q: 1.261666
 81469/100000: episode: 1477, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 8.913, mean reward: 0.743 [0.716, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.479], loss: 0.001241, mae: 0.039407, mean_q: 1.279118
 81505/100000: episode: 1478, duration: 0.203s, episode steps: 36, steps per second: 178, episode reward: 29.821, mean reward: 0.828 [0.758, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.953, 10.100], loss: 0.001178, mae: 0.038079, mean_q: 1.284958
 81524/100000: episode: 1479, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 14.329, mean reward: 0.754 [0.694, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.524], loss: 0.001214, mae: 0.039090, mean_q: 1.275901
 81538/100000: episode: 1480, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 10.599, mean reward: 0.757 [0.724, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.248, 10.409], loss: 0.001319, mae: 0.038646, mean_q: 1.274584
 81555/100000: episode: 1481, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 12.430, mean reward: 0.731 [0.631, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.354], loss: 0.001347, mae: 0.039293, mean_q: 1.269783
 81590/100000: episode: 1482, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 23.084, mean reward: 0.660 [0.578, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.482, 10.100], loss: 0.001223, mae: 0.039258, mean_q: 1.271648
 81625/100000: episode: 1483, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 26.347, mean reward: 0.753 [0.702, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.346, 10.100], loss: 0.001294, mae: 0.038647, mean_q: 1.281348
[Info] FALSIFICATION!
 81643/100000: episode: 1484, duration: 0.351s, episode steps: 18, steps per second: 51, episode reward: 15.448, mean reward: 0.858 [0.765, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.017, 9.404], loss: 0.001355, mae: 0.041386, mean_q: 1.270504
 81655/100000: episode: 1485, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 9.873, mean reward: 0.823 [0.755, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.638], loss: 0.001890, mae: 0.044756, mean_q: 1.276200
 81690/100000: episode: 1486, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 22.922, mean reward: 0.655 [0.566, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.525, 10.100], loss: 0.001269, mae: 0.039749, mean_q: 1.284535
 81727/100000: episode: 1487, duration: 0.211s, episode steps: 37, steps per second: 176, episode reward: 28.340, mean reward: 0.766 [0.645, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.527, 10.100], loss: 0.001244, mae: 0.039214, mean_q: 1.274287
 81744/100000: episode: 1488, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 12.068, mean reward: 0.710 [0.612, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.554, 10.307], loss: 0.001468, mae: 0.039711, mean_q: 1.286261
 81763/100000: episode: 1489, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 13.560, mean reward: 0.714 [0.609, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.365], loss: 0.001426, mae: 0.038847, mean_q: 1.285611
 81799/100000: episode: 1490, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 25.315, mean reward: 0.703 [0.614, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.741, 10.100], loss: 0.001302, mae: 0.039096, mean_q: 1.279417
 81813/100000: episode: 1491, duration: 0.093s, episode steps: 14, steps per second: 151, episode reward: 11.358, mean reward: 0.811 [0.752, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.718, 10.626], loss: 0.001061, mae: 0.035639, mean_q: 1.277521
 81849/100000: episode: 1492, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 25.794, mean reward: 0.716 [0.632, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.570, 10.100], loss: 0.001086, mae: 0.036326, mean_q: 1.283856
 81866/100000: episode: 1493, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 12.058, mean reward: 0.709 [0.571, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.469, 10.283], loss: 0.001110, mae: 0.036502, mean_q: 1.270087
 81901/100000: episode: 1494, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 22.253, mean reward: 0.636 [0.512, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.261, 10.280], loss: 0.001101, mae: 0.036837, mean_q: 1.278776
 81938/100000: episode: 1495, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 28.118, mean reward: 0.760 [0.684, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.590, 10.100], loss: 0.001423, mae: 0.040059, mean_q: 1.287333
 81955/100000: episode: 1496, duration: 0.106s, episode steps: 17, steps per second: 161, episode reward: 10.930, mean reward: 0.643 [0.536, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.282], loss: 0.001185, mae: 0.036269, mean_q: 1.277152
 81991/100000: episode: 1497, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 24.034, mean reward: 0.668 [0.531, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.174, 10.100], loss: 0.001284, mae: 0.038456, mean_q: 1.282905
 82005/100000: episode: 1498, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 11.579, mean reward: 0.827 [0.777, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.257, 10.594], loss: 0.001310, mae: 0.039185, mean_q: 1.291233
 82022/100000: episode: 1499, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 12.074, mean reward: 0.710 [0.657, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.799, 10.519], loss: 0.001219, mae: 0.039033, mean_q: 1.292318
 82058/100000: episode: 1500, duration: 0.204s, episode steps: 36, steps per second: 176, episode reward: 23.894, mean reward: 0.664 [0.543, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.679, 10.142], loss: 0.001291, mae: 0.040009, mean_q: 1.285854
 82072/100000: episode: 1501, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 10.477, mean reward: 0.748 [0.675, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.387], loss: 0.001147, mae: 0.037288, mean_q: 1.278904
 82108/100000: episode: 1502, duration: 0.188s, episode steps: 36, steps per second: 192, episode reward: 28.645, mean reward: 0.796 [0.664, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.142, 10.100], loss: 0.001361, mae: 0.039447, mean_q: 1.284696
 82144/100000: episode: 1503, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 25.947, mean reward: 0.721 [0.579, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.444, 10.100], loss: 0.001242, mae: 0.038718, mean_q: 1.286361
 82179/100000: episode: 1504, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 24.923, mean reward: 0.712 [0.611, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.776, 10.100], loss: 0.001269, mae: 0.039989, mean_q: 1.300382
 82215/100000: episode: 1505, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 27.769, mean reward: 0.771 [0.703, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.189, 10.100], loss: 0.001345, mae: 0.040549, mean_q: 1.289053
 82234/100000: episode: 1506, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 14.920, mean reward: 0.785 [0.687, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.079, 10.455], loss: 0.001323, mae: 0.040380, mean_q: 1.298751
 82270/100000: episode: 1507, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 25.213, mean reward: 0.700 [0.578, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.386, 10.100], loss: 0.001369, mae: 0.039569, mean_q: 1.297725
 82289/100000: episode: 1508, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 15.343, mean reward: 0.808 [0.729, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.219, 10.588], loss: 0.001223, mae: 0.038284, mean_q: 1.293816
 82303/100000: episode: 1509, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 10.606, mean reward: 0.758 [0.718, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.467], loss: 0.001249, mae: 0.039642, mean_q: 1.282252
 82338/100000: episode: 1510, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 24.282, mean reward: 0.694 [0.610, 0.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.605, 10.100], loss: 0.001678, mae: 0.042256, mean_q: 1.290879
 82378/100000: episode: 1511, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 29.586, mean reward: 0.740 [0.550, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.688, 10.100], loss: 0.001340, mae: 0.039386, mean_q: 1.296994
 82414/100000: episode: 1512, duration: 0.240s, episode steps: 36, steps per second: 150, episode reward: 26.410, mean reward: 0.734 [0.629, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.404, 10.100], loss: 0.001244, mae: 0.039556, mean_q: 1.290015
 82433/100000: episode: 1513, duration: 0.154s, episode steps: 19, steps per second: 123, episode reward: 13.240, mean reward: 0.697 [0.658, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.083, 10.405], loss: 0.001600, mae: 0.041545, mean_q: 1.309036
 82468/100000: episode: 1514, duration: 0.224s, episode steps: 35, steps per second: 156, episode reward: 24.281, mean reward: 0.694 [0.550, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.350, 10.104], loss: 0.001453, mae: 0.039185, mean_q: 1.302088
 82485/100000: episode: 1515, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 12.888, mean reward: 0.758 [0.692, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.549], loss: 0.001637, mae: 0.043618, mean_q: 1.288466
 82521/100000: episode: 1516, duration: 0.215s, episode steps: 36, steps per second: 168, episode reward: 24.439, mean reward: 0.679 [0.510, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.611, 10.100], loss: 0.001460, mae: 0.040905, mean_q: 1.305565
 82561/100000: episode: 1517, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 27.162, mean reward: 0.679 [0.517, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.275, 10.230], loss: 0.001329, mae: 0.039373, mean_q: 1.308284
 82580/100000: episode: 1518, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 14.687, mean reward: 0.773 [0.676, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.630], loss: 0.001326, mae: 0.040720, mean_q: 1.296046
 82616/100000: episode: 1519, duration: 0.213s, episode steps: 36, steps per second: 169, episode reward: 26.829, mean reward: 0.745 [0.633, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.895, 10.100], loss: 0.001389, mae: 0.039646, mean_q: 1.309225
 82656/100000: episode: 1520, duration: 0.223s, episode steps: 40, steps per second: 180, episode reward: 26.556, mean reward: 0.664 [0.559, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.636, 10.100], loss: 0.001194, mae: 0.038272, mean_q: 1.296709
 82691/100000: episode: 1521, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 21.374, mean reward: 0.611 [0.531, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.507, 10.100], loss: 0.001270, mae: 0.039707, mean_q: 1.302168
 82728/100000: episode: 1522, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 25.352, mean reward: 0.685 [0.515, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.185, 10.123], loss: 0.001275, mae: 0.039832, mean_q: 1.311337
 82747/100000: episode: 1523, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 15.436, mean reward: 0.812 [0.728, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.257, 10.682], loss: 0.001093, mae: 0.036392, mean_q: 1.303643
 82787/100000: episode: 1524, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 29.561, mean reward: 0.739 [0.550, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.251, 10.100], loss: 0.001502, mae: 0.040764, mean_q: 1.300396
 82801/100000: episode: 1525, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 9.955, mean reward: 0.711 [0.567, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.403], loss: 0.001124, mae: 0.038004, mean_q: 1.312892
 82837/100000: episode: 1526, duration: 0.241s, episode steps: 36, steps per second: 150, episode reward: 23.803, mean reward: 0.661 [0.537, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.058, 10.100], loss: 0.001467, mae: 0.039958, mean_q: 1.304593
 82873/100000: episode: 1527, duration: 0.213s, episode steps: 36, steps per second: 169, episode reward: 24.608, mean reward: 0.684 [0.620, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.737, 10.100], loss: 0.001289, mae: 0.037827, mean_q: 1.318643
 82910/100000: episode: 1528, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 27.805, mean reward: 0.751 [0.693, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.604, 10.100], loss: 0.001199, mae: 0.037194, mean_q: 1.305304
 82945/100000: episode: 1529, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 22.511, mean reward: 0.643 [0.510, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-1.289, 10.149], loss: 0.001255, mae: 0.039596, mean_q: 1.313181
 82980/100000: episode: 1530, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 22.564, mean reward: 0.645 [0.555, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.120, 10.106], loss: 0.001846, mae: 0.042298, mean_q: 1.310430
 83017/100000: episode: 1531, duration: 0.223s, episode steps: 37, steps per second: 166, episode reward: 26.513, mean reward: 0.717 [0.608, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.232, 10.100], loss: 0.001477, mae: 0.040966, mean_q: 1.315640
[Info] Complete ISplit Iteration
[Info] Levels: [1.3869038, 1.4995587, 1.585777]
[Info] Cond. Prob: [0.1, 0.1, 0.33]
[Info] Error Prob: 0.003300000000000001

 83057/100000: episode: 1532, duration: 5.360s, episode steps: 40, steps per second: 7, episode reward: 26.759, mean reward: 0.669 [0.540, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.373, 10.100], loss: 0.001243, mae: 0.039475, mean_q: 1.306058
 83157/100000: episode: 1533, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 61.223, mean reward: 0.612 [0.504, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.777, 10.394], loss: 0.001411, mae: 0.040591, mean_q: 1.315513
 83257/100000: episode: 1534, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 59.450, mean reward: 0.594 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.882, 10.098], loss: 0.001489, mae: 0.041378, mean_q: 1.309272
 83357/100000: episode: 1535, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 61.411, mean reward: 0.614 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.190, 10.098], loss: 0.001415, mae: 0.040773, mean_q: 1.307969
 83457/100000: episode: 1536, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 57.969, mean reward: 0.580 [0.505, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.458, 10.274], loss: 0.001320, mae: 0.038391, mean_q: 1.302137
 83557/100000: episode: 1537, duration: 0.676s, episode steps: 100, steps per second: 148, episode reward: 59.436, mean reward: 0.594 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.949, 10.312], loss: 0.001366, mae: 0.040266, mean_q: 1.305484
 83657/100000: episode: 1538, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: 57.403, mean reward: 0.574 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.441, 10.198], loss: 0.001374, mae: 0.041052, mean_q: 1.302017
 83757/100000: episode: 1539, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: 59.466, mean reward: 0.595 [0.510, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.336, 10.098], loss: 0.001504, mae: 0.042349, mean_q: 1.305984
 83857/100000: episode: 1540, duration: 0.958s, episode steps: 100, steps per second: 104, episode reward: 57.519, mean reward: 0.575 [0.509, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.576, 10.098], loss: 0.001393, mae: 0.040224, mean_q: 1.299978
 83957/100000: episode: 1541, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 58.635, mean reward: 0.586 [0.508, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.128, 10.257], loss: 0.001300, mae: 0.039666, mean_q: 1.297749
 84057/100000: episode: 1542, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: 58.868, mean reward: 0.589 [0.511, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.580, 10.098], loss: 0.001474, mae: 0.041104, mean_q: 1.300570
 84157/100000: episode: 1543, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: 60.152, mean reward: 0.602 [0.511, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.147, 10.361], loss: 0.001426, mae: 0.040749, mean_q: 1.294518
 84257/100000: episode: 1544, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 60.509, mean reward: 0.605 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.924, 10.171], loss: 0.001601, mae: 0.041459, mean_q: 1.292339
 84357/100000: episode: 1545, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.853, mean reward: 0.589 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.547, 10.098], loss: 0.001300, mae: 0.039296, mean_q: 1.297460
 84457/100000: episode: 1546, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.882, mean reward: 0.569 [0.497, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.922, 10.098], loss: 0.001477, mae: 0.041071, mean_q: 1.294605
 84557/100000: episode: 1547, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.568, mean reward: 0.586 [0.497, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.511, 10.253], loss: 0.001659, mae: 0.042393, mean_q: 1.282721
 84657/100000: episode: 1548, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 59.475, mean reward: 0.595 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.723, 10.301], loss: 0.001456, mae: 0.040396, mean_q: 1.290683
 84757/100000: episode: 1549, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.386, mean reward: 0.584 [0.503, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.025, 10.098], loss: 0.001417, mae: 0.039705, mean_q: 1.284150
 84857/100000: episode: 1550, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.265, mean reward: 0.573 [0.501, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.443, 10.133], loss: 0.001452, mae: 0.041414, mean_q: 1.291890
 84957/100000: episode: 1551, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.979, mean reward: 0.590 [0.503, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.607, 10.210], loss: 0.001844, mae: 0.045512, mean_q: 1.282634
 85057/100000: episode: 1552, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 59.260, mean reward: 0.593 [0.509, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.658, 10.098], loss: 0.001613, mae: 0.042697, mean_q: 1.279524
 85157/100000: episode: 1553, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.040, mean reward: 0.580 [0.505, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.755, 10.137], loss: 0.001554, mae: 0.041850, mean_q: 1.281140
 85257/100000: episode: 1554, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 55.954, mean reward: 0.560 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.848, 10.280], loss: 0.001704, mae: 0.043597, mean_q: 1.276531
 85357/100000: episode: 1555, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.047, mean reward: 0.580 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.098, 10.098], loss: 0.001559, mae: 0.042638, mean_q: 1.281274
 85457/100000: episode: 1556, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.426, mean reward: 0.584 [0.504, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.347, 10.098], loss: 0.001723, mae: 0.043408, mean_q: 1.277427
 85557/100000: episode: 1557, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 59.607, mean reward: 0.596 [0.512, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.791, 10.098], loss: 0.001694, mae: 0.043885, mean_q: 1.271324
 85657/100000: episode: 1558, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.616, mean reward: 0.586 [0.501, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.981, 10.330], loss: 0.001857, mae: 0.045143, mean_q: 1.262469
 85757/100000: episode: 1559, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.512, mean reward: 0.585 [0.517, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.313, 10.098], loss: 0.001619, mae: 0.042058, mean_q: 1.266208
 85857/100000: episode: 1560, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.259, mean reward: 0.583 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.102, 10.098], loss: 0.001593, mae: 0.042231, mean_q: 1.256189
 85957/100000: episode: 1561, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.462, mean reward: 0.585 [0.505, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.453, 10.129], loss: 0.001529, mae: 0.041744, mean_q: 1.252228
 86057/100000: episode: 1562, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.318, mean reward: 0.573 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.340, 10.098], loss: 0.001720, mae: 0.044325, mean_q: 1.245027
 86157/100000: episode: 1563, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.678, mean reward: 0.587 [0.508, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.002, 10.098], loss: 0.001618, mae: 0.043264, mean_q: 1.240209
 86257/100000: episode: 1564, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.268, mean reward: 0.583 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.563, 10.290], loss: 0.001597, mae: 0.042384, mean_q: 1.237584
 86357/100000: episode: 1565, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.710, mean reward: 0.587 [0.518, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.510, 10.098], loss: 0.001708, mae: 0.043623, mean_q: 1.230327
 86457/100000: episode: 1566, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.591, mean reward: 0.586 [0.504, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.884, 10.114], loss: 0.001613, mae: 0.042506, mean_q: 1.231707
 86557/100000: episode: 1567, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.867, mean reward: 0.589 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.402, 10.098], loss: 0.001563, mae: 0.042845, mean_q: 1.219483
 86657/100000: episode: 1568, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 60.668, mean reward: 0.607 [0.511, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.839, 10.098], loss: 0.001675, mae: 0.044064, mean_q: 1.221071
 86757/100000: episode: 1569, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 61.833, mean reward: 0.618 [0.519, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.985, 10.237], loss: 0.001590, mae: 0.043200, mean_q: 1.215507
 86857/100000: episode: 1570, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.629, mean reward: 0.606 [0.502, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.374, 10.098], loss: 0.001674, mae: 0.044053, mean_q: 1.214184
 86957/100000: episode: 1571, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.532, mean reward: 0.605 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.668, 10.124], loss: 0.001678, mae: 0.044405, mean_q: 1.205183
 87057/100000: episode: 1572, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.268, mean reward: 0.593 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.970, 10.098], loss: 0.001732, mae: 0.044321, mean_q: 1.199977
 87157/100000: episode: 1573, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 63.067, mean reward: 0.631 [0.500, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.425, 10.315], loss: 0.001593, mae: 0.043000, mean_q: 1.198936
 87257/100000: episode: 1574, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 57.890, mean reward: 0.579 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.030, 10.254], loss: 0.001675, mae: 0.044220, mean_q: 1.188529
 87357/100000: episode: 1575, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.326, mean reward: 0.573 [0.505, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.749, 10.220], loss: 0.001574, mae: 0.042768, mean_q: 1.190188
 87457/100000: episode: 1576, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.833, mean reward: 0.608 [0.505, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.870, 10.098], loss: 0.001470, mae: 0.042185, mean_q: 1.186017
 87557/100000: episode: 1577, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.503, mean reward: 0.605 [0.509, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.784, 10.233], loss: 0.001555, mae: 0.042601, mean_q: 1.181080
 87657/100000: episode: 1578, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.412, mean reward: 0.584 [0.506, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.994, 10.098], loss: 0.001663, mae: 0.044889, mean_q: 1.183767
 87757/100000: episode: 1579, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.580, mean reward: 0.596 [0.501, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.315, 10.260], loss: 0.001437, mae: 0.041841, mean_q: 1.177249
 87857/100000: episode: 1580, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 56.851, mean reward: 0.569 [0.499, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.435, 10.098], loss: 0.001567, mae: 0.042415, mean_q: 1.177934
 87957/100000: episode: 1581, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.037, mean reward: 0.580 [0.509, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.778, 10.098], loss: 0.001541, mae: 0.042421, mean_q: 1.169603
 88057/100000: episode: 1582, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 64.837, mean reward: 0.648 [0.501, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.835, 10.098], loss: 0.001405, mae: 0.041116, mean_q: 1.166686
 88157/100000: episode: 1583, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.861, mean reward: 0.579 [0.506, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.389, 10.360], loss: 0.001470, mae: 0.042112, mean_q: 1.166664
 88257/100000: episode: 1584, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 56.327, mean reward: 0.563 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.781, 10.099], loss: 0.001487, mae: 0.042221, mean_q: 1.168531
 88357/100000: episode: 1585, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.001, mean reward: 0.590 [0.515, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.768, 10.098], loss: 0.001263, mae: 0.038840, mean_q: 1.160962
 88457/100000: episode: 1586, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.890, mean reward: 0.589 [0.506, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.725, 10.098], loss: 0.001376, mae: 0.040089, mean_q: 1.163418
 88557/100000: episode: 1587, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 61.665, mean reward: 0.617 [0.515, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.767, 10.098], loss: 0.001434, mae: 0.041092, mean_q: 1.163939
 88657/100000: episode: 1588, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 64.764, mean reward: 0.648 [0.516, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.498, 10.098], loss: 0.001422, mae: 0.041905, mean_q: 1.163790
 88757/100000: episode: 1589, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 57.924, mean reward: 0.579 [0.503, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.497, 10.212], loss: 0.001442, mae: 0.041227, mean_q: 1.166756
 88857/100000: episode: 1590, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.642, mean reward: 0.586 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.092, 10.098], loss: 0.001515, mae: 0.042967, mean_q: 1.168384
 88957/100000: episode: 1591, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.366, mean reward: 0.584 [0.507, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.493, 10.127], loss: 0.001595, mae: 0.044075, mean_q: 1.171894
 89057/100000: episode: 1592, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 59.930, mean reward: 0.599 [0.499, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.709, 10.098], loss: 0.001436, mae: 0.041754, mean_q: 1.163912
 89157/100000: episode: 1593, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.785, mean reward: 0.578 [0.503, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.848, 10.098], loss: 0.001401, mae: 0.041645, mean_q: 1.167219
 89257/100000: episode: 1594, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.214, mean reward: 0.592 [0.502, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.775, 10.184], loss: 0.001467, mae: 0.041845, mean_q: 1.166709
 89357/100000: episode: 1595, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.649, mean reward: 0.586 [0.504, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.360, 10.186], loss: 0.001516, mae: 0.042496, mean_q: 1.166176
 89457/100000: episode: 1596, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.139, mean reward: 0.581 [0.501, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.000, 10.098], loss: 0.001542, mae: 0.043331, mean_q: 1.167330
 89557/100000: episode: 1597, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 61.436, mean reward: 0.614 [0.508, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.392, 10.311], loss: 0.001498, mae: 0.042293, mean_q: 1.170453
 89657/100000: episode: 1598, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.037, mean reward: 0.580 [0.511, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.765, 10.112], loss: 0.001484, mae: 0.041654, mean_q: 1.165238
 89757/100000: episode: 1599, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.334, mean reward: 0.593 [0.504, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.004, 10.125], loss: 0.001373, mae: 0.040504, mean_q: 1.165689
 89857/100000: episode: 1600, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.689, mean reward: 0.587 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.870, 10.304], loss: 0.001517, mae: 0.042964, mean_q: 1.167489
 89957/100000: episode: 1601, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.301, mean reward: 0.603 [0.516, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.845, 10.277], loss: 0.001428, mae: 0.041456, mean_q: 1.170906
 90057/100000: episode: 1602, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.077, mean reward: 0.571 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.429, 10.141], loss: 0.001400, mae: 0.041338, mean_q: 1.171237
 90157/100000: episode: 1603, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 56.915, mean reward: 0.569 [0.507, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.986, 10.098], loss: 0.001394, mae: 0.041142, mean_q: 1.169016
 90257/100000: episode: 1604, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.148, mean reward: 0.571 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.948, 10.098], loss: 0.001421, mae: 0.040999, mean_q: 1.171880
 90357/100000: episode: 1605, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 57.904, mean reward: 0.579 [0.501, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.699, 10.131], loss: 0.001372, mae: 0.040361, mean_q: 1.167853
 90457/100000: episode: 1606, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 56.459, mean reward: 0.565 [0.501, 0.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.267, 10.158], loss: 0.001324, mae: 0.040001, mean_q: 1.167179
 90557/100000: episode: 1607, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 62.238, mean reward: 0.622 [0.506, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.348, 10.098], loss: 0.001382, mae: 0.040570, mean_q: 1.164325
 90657/100000: episode: 1608, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.097, mean reward: 0.601 [0.505, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.821, 10.098], loss: 0.001389, mae: 0.041170, mean_q: 1.167712
 90757/100000: episode: 1609, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.484, mean reward: 0.585 [0.506, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.659, 10.224], loss: 0.001382, mae: 0.040893, mean_q: 1.170497
 90857/100000: episode: 1610, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 60.863, mean reward: 0.609 [0.512, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.172, 10.146], loss: 0.001429, mae: 0.040933, mean_q: 1.169538
 90957/100000: episode: 1611, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 58.011, mean reward: 0.580 [0.509, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.539, 10.140], loss: 0.001515, mae: 0.042587, mean_q: 1.174684
 91057/100000: episode: 1612, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.774, mean reward: 0.598 [0.504, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.127, 10.098], loss: 0.001458, mae: 0.041765, mean_q: 1.167759
 91157/100000: episode: 1613, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.986, mean reward: 0.580 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.136, 10.118], loss: 0.001565, mae: 0.042866, mean_q: 1.173098
 91257/100000: episode: 1614, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.697, mean reward: 0.587 [0.500, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.068, 10.098], loss: 0.001518, mae: 0.042638, mean_q: 1.172123
 91357/100000: episode: 1615, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 56.959, mean reward: 0.570 [0.504, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.109, 10.098], loss: 0.001588, mae: 0.043135, mean_q: 1.170685
 91457/100000: episode: 1616, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 62.374, mean reward: 0.624 [0.524, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.013, 10.098], loss: 0.001473, mae: 0.041843, mean_q: 1.173965
 91557/100000: episode: 1617, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 60.390, mean reward: 0.604 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.562, 10.098], loss: 0.001538, mae: 0.042947, mean_q: 1.178414
 91657/100000: episode: 1618, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.471, mean reward: 0.585 [0.505, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.516, 10.184], loss: 0.001482, mae: 0.041928, mean_q: 1.172110
 91757/100000: episode: 1619, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.931, mean reward: 0.579 [0.502, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.763, 10.127], loss: 0.001523, mae: 0.042775, mean_q: 1.170527
 91857/100000: episode: 1620, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.869, mean reward: 0.589 [0.509, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.004, 10.098], loss: 0.001474, mae: 0.041853, mean_q: 1.168653
 91957/100000: episode: 1621, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 63.398, mean reward: 0.634 [0.520, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.613, 10.281], loss: 0.001514, mae: 0.042337, mean_q: 1.167133
 92057/100000: episode: 1622, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 61.532, mean reward: 0.615 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.925, 10.299], loss: 0.001553, mae: 0.042896, mean_q: 1.170563
 92157/100000: episode: 1623, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.817, mean reward: 0.588 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.655, 10.098], loss: 0.001416, mae: 0.040836, mean_q: 1.169474
 92257/100000: episode: 1624, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.946, mean reward: 0.589 [0.512, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.665, 10.098], loss: 0.001531, mae: 0.042880, mean_q: 1.168489
 92357/100000: episode: 1625, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.286, mean reward: 0.573 [0.502, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.556, 10.098], loss: 0.001482, mae: 0.042566, mean_q: 1.170401
 92457/100000: episode: 1626, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 56.611, mean reward: 0.566 [0.501, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.499, 10.142], loss: 0.001596, mae: 0.043792, mean_q: 1.170692
 92557/100000: episode: 1627, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.072, mean reward: 0.581 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.437, 10.182], loss: 0.001463, mae: 0.041242, mean_q: 1.168628
 92657/100000: episode: 1628, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.791, mean reward: 0.588 [0.510, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.538, 10.250], loss: 0.001450, mae: 0.041533, mean_q: 1.166908
 92757/100000: episode: 1629, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.160, mean reward: 0.572 [0.514, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.118, 10.124], loss: 0.001539, mae: 0.042351, mean_q: 1.168708
 92857/100000: episode: 1630, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.327, mean reward: 0.583 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.865, 10.098], loss: 0.001377, mae: 0.041086, mean_q: 1.166578
 92957/100000: episode: 1631, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 61.266, mean reward: 0.613 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.503, 10.304], loss: 0.001438, mae: 0.041589, mean_q: 1.166334
[Info] 1-TH LEVEL FOUND: 1.3753942251205444, Considering 10/90 traces
 93057/100000: episode: 1632, duration: 4.860s, episode steps: 100, steps per second: 21, episode reward: 59.011, mean reward: 0.590 [0.503, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.098], loss: 0.001455, mae: 0.041649, mean_q: 1.166849
 93102/100000: episode: 1633, duration: 0.249s, episode steps: 45, steps per second: 180, episode reward: 29.156, mean reward: 0.648 [0.585, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.694, 10.100], loss: 0.001647, mae: 0.043905, mean_q: 1.172335
 93141/100000: episode: 1634, duration: 0.214s, episode steps: 39, steps per second: 182, episode reward: 26.244, mean reward: 0.673 [0.575, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.612, 10.204], loss: 0.001428, mae: 0.041940, mean_q: 1.159365
 93161/100000: episode: 1635, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 15.437, mean reward: 0.772 [0.685, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.375, 10.100], loss: 0.001298, mae: 0.039790, mean_q: 1.169150
 93200/100000: episode: 1636, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 25.761, mean reward: 0.661 [0.543, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.440, 10.199], loss: 0.001424, mae: 0.040313, mean_q: 1.173024
 93234/100000: episode: 1637, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 21.318, mean reward: 0.627 [0.517, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.042, 10.139], loss: 0.001559, mae: 0.042465, mean_q: 1.171280
 93268/100000: episode: 1638, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 20.167, mean reward: 0.593 [0.513, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.092, 10.123], loss: 0.001484, mae: 0.042009, mean_q: 1.171401
 93313/100000: episode: 1639, duration: 0.231s, episode steps: 45, steps per second: 195, episode reward: 28.639, mean reward: 0.636 [0.537, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.791, 10.100], loss: 0.001438, mae: 0.040867, mean_q: 1.172385
 93347/100000: episode: 1640, duration: 0.203s, episode steps: 34, steps per second: 167, episode reward: 20.774, mean reward: 0.611 [0.503, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.102, 10.243], loss: 0.001406, mae: 0.040868, mean_q: 1.167818
 93378/100000: episode: 1641, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 19.976, mean reward: 0.644 [0.568, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.318, 10.290], loss: 0.001791, mae: 0.045411, mean_q: 1.173053
 93417/100000: episode: 1642, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 24.306, mean reward: 0.623 [0.523, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.323, 10.100], loss: 0.001389, mae: 0.040902, mean_q: 1.167835
 93451/100000: episode: 1643, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 21.574, mean reward: 0.635 [0.509, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.633, 10.167], loss: 0.001594, mae: 0.041591, mean_q: 1.169617
 93477/100000: episode: 1644, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 16.876, mean reward: 0.649 [0.588, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.473, 10.336], loss: 0.001368, mae: 0.039563, mean_q: 1.173756
 93504/100000: episode: 1645, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 18.584, mean reward: 0.688 [0.595, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.916, 10.100], loss: 0.001377, mae: 0.040231, mean_q: 1.168476
 93530/100000: episode: 1646, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 18.123, mean reward: 0.697 [0.650, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.393], loss: 0.001448, mae: 0.041679, mean_q: 1.175693
 93559/100000: episode: 1647, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 18.784, mean reward: 0.648 [0.553, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.249, 10.100], loss: 0.001566, mae: 0.041239, mean_q: 1.172687
 93579/100000: episode: 1648, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 13.565, mean reward: 0.678 [0.608, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.282, 10.100], loss: 0.001689, mae: 0.043063, mean_q: 1.180072
 93618/100000: episode: 1649, duration: 0.207s, episode steps: 39, steps per second: 188, episode reward: 28.753, mean reward: 0.737 [0.655, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.302, 10.100], loss: 0.001686, mae: 0.043635, mean_q: 1.169344
 93657/100000: episode: 1650, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 27.037, mean reward: 0.693 [0.563, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-1.083, 10.102], loss: 0.001623, mae: 0.042095, mean_q: 1.177618
 93683/100000: episode: 1651, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 16.609, mean reward: 0.639 [0.524, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.303, 10.187], loss: 0.001749, mae: 0.043835, mean_q: 1.171399
 93722/100000: episode: 1652, duration: 0.243s, episode steps: 39, steps per second: 161, episode reward: 28.308, mean reward: 0.726 [0.640, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.434, 10.334], loss: 0.001575, mae: 0.042276, mean_q: 1.175359
 93749/100000: episode: 1653, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 17.954, mean reward: 0.665 [0.593, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.150, 10.100], loss: 0.001591, mae: 0.042012, mean_q: 1.178353
 93776/100000: episode: 1654, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 19.938, mean reward: 0.738 [0.611, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.391, 10.100], loss: 0.001892, mae: 0.046867, mean_q: 1.180080
 93803/100000: episode: 1655, duration: 0.162s, episode steps: 27, steps per second: 166, episode reward: 17.514, mean reward: 0.649 [0.544, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.488, 10.100], loss: 0.001737, mae: 0.044554, mean_q: 1.181268
 93837/100000: episode: 1656, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 23.697, mean reward: 0.697 [0.627, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.504, 10.374], loss: 0.001697, mae: 0.044229, mean_q: 1.183741
 93863/100000: episode: 1657, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 16.581, mean reward: 0.638 [0.576, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.405], loss: 0.001642, mae: 0.043039, mean_q: 1.187517
 93900/100000: episode: 1658, duration: 0.222s, episode steps: 37, steps per second: 166, episode reward: 23.861, mean reward: 0.645 [0.540, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.035, 10.419], loss: 0.001689, mae: 0.043666, mean_q: 1.183346
 93931/100000: episode: 1659, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 22.113, mean reward: 0.713 [0.627, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.849, 10.412], loss: 0.001437, mae: 0.040464, mean_q: 1.183350
 93960/100000: episode: 1660, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 18.657, mean reward: 0.643 [0.512, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.411, 10.100], loss: 0.001810, mae: 0.044750, mean_q: 1.182726
 93980/100000: episode: 1661, duration: 0.121s, episode steps: 20, steps per second: 166, episode reward: 13.210, mean reward: 0.660 [0.621, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.273, 10.100], loss: 0.001680, mae: 0.044482, mean_q: 1.192157
 94017/100000: episode: 1662, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 23.190, mean reward: 0.627 [0.505, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.629, 10.161], loss: 0.001489, mae: 0.041366, mean_q: 1.192714
 94046/100000: episode: 1663, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 20.270, mean reward: 0.699 [0.619, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.182, 10.100], loss: 0.001511, mae: 0.040780, mean_q: 1.194462
 94085/100000: episode: 1664, duration: 0.196s, episode steps: 39, steps per second: 199, episode reward: 24.277, mean reward: 0.622 [0.511, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.417, 10.100], loss: 0.001469, mae: 0.040992, mean_q: 1.190637
 94112/100000: episode: 1665, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 18.105, mean reward: 0.671 [0.570, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.200, 10.100], loss: 0.001848, mae: 0.045512, mean_q: 1.181900
 94143/100000: episode: 1666, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 19.754, mean reward: 0.637 [0.534, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.615, 10.255], loss: 0.001746, mae: 0.043903, mean_q: 1.188745
 94174/100000: episode: 1667, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 20.005, mean reward: 0.645 [0.522, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.356, 10.184], loss: 0.001714, mae: 0.043419, mean_q: 1.188369
 94200/100000: episode: 1668, duration: 0.160s, episode steps: 26, steps per second: 163, episode reward: 15.695, mean reward: 0.604 [0.519, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.833, 10.156], loss: 0.001606, mae: 0.042626, mean_q: 1.200061
 94226/100000: episode: 1669, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 17.299, mean reward: 0.665 [0.610, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.422, 10.301], loss: 0.001841, mae: 0.045196, mean_q: 1.186983
 94260/100000: episode: 1670, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 21.425, mean reward: 0.630 [0.545, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.361, 10.451], loss: 0.001878, mae: 0.045115, mean_q: 1.184360
 94280/100000: episode: 1671, duration: 0.097s, episode steps: 20, steps per second: 205, episode reward: 16.405, mean reward: 0.820 [0.708, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.916, 10.100], loss: 0.001513, mae: 0.041747, mean_q: 1.197463
 94314/100000: episode: 1672, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 20.879, mean reward: 0.614 [0.530, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.591, 10.230], loss: 0.001944, mae: 0.046391, mean_q: 1.189533
 94343/100000: episode: 1673, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 20.062, mean reward: 0.692 [0.626, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.780, 10.100], loss: 0.001521, mae: 0.042349, mean_q: 1.194632
 94370/100000: episode: 1674, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 18.500, mean reward: 0.685 [0.532, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.441, 10.100], loss: 0.001897, mae: 0.045584, mean_q: 1.188951
 94409/100000: episode: 1675, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 28.887, mean reward: 0.741 [0.663, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.379, 10.100], loss: 0.001938, mae: 0.045808, mean_q: 1.196352
 94443/100000: episode: 1676, duration: 0.194s, episode steps: 34, steps per second: 175, episode reward: 22.284, mean reward: 0.655 [0.590, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.692, 10.311], loss: 0.001797, mae: 0.044530, mean_q: 1.193040
 94482/100000: episode: 1677, duration: 0.201s, episode steps: 39, steps per second: 194, episode reward: 24.831, mean reward: 0.637 [0.519, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.264, 10.100], loss: 0.001865, mae: 0.045565, mean_q: 1.195135
 94519/100000: episode: 1678, duration: 0.192s, episode steps: 37, steps per second: 192, episode reward: 23.668, mean reward: 0.640 [0.561, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.594, 10.361], loss: 0.001568, mae: 0.042224, mean_q: 1.196881
 94548/100000: episode: 1679, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 19.029, mean reward: 0.656 [0.590, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.548, 10.100], loss: 0.001692, mae: 0.043762, mean_q: 1.191437
 94593/100000: episode: 1680, duration: 0.250s, episode steps: 45, steps per second: 180, episode reward: 30.682, mean reward: 0.682 [0.622, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.378, 10.100], loss: 0.001490, mae: 0.041267, mean_q: 1.195114
 94620/100000: episode: 1681, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 19.606, mean reward: 0.726 [0.659, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.844, 10.100], loss: 0.001652, mae: 0.043520, mean_q: 1.199605
 94659/100000: episode: 1682, duration: 0.227s, episode steps: 39, steps per second: 172, episode reward: 24.545, mean reward: 0.629 [0.510, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.035, 10.172], loss: 0.001661, mae: 0.043818, mean_q: 1.194556
 94696/100000: episode: 1683, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 25.394, mean reward: 0.686 [0.543, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.064, 10.282], loss: 0.001504, mae: 0.041069, mean_q: 1.197868
 94725/100000: episode: 1684, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 18.979, mean reward: 0.654 [0.562, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.667, 10.100], loss: 0.001637, mae: 0.042799, mean_q: 1.192695
 94759/100000: episode: 1685, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 20.339, mean reward: 0.598 [0.500, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.765, 10.169], loss: 0.001956, mae: 0.046382, mean_q: 1.200725
 94793/100000: episode: 1686, duration: 0.172s, episode steps: 34, steps per second: 198, episode reward: 24.422, mean reward: 0.718 [0.620, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.677, 10.621], loss: 0.001523, mae: 0.042494, mean_q: 1.209445
 94832/100000: episode: 1687, duration: 0.221s, episode steps: 39, steps per second: 177, episode reward: 25.869, mean reward: 0.663 [0.526, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.729, 10.226], loss: 0.001689, mae: 0.044068, mean_q: 1.204078
 94869/100000: episode: 1688, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 23.044, mean reward: 0.623 [0.529, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.140, 10.198], loss: 0.001656, mae: 0.042343, mean_q: 1.206108
 94903/100000: episode: 1689, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 22.393, mean reward: 0.659 [0.578, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.035, 10.466], loss: 0.001529, mae: 0.042048, mean_q: 1.199758
 94930/100000: episode: 1690, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 18.931, mean reward: 0.701 [0.649, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.340, 10.100], loss: 0.001399, mae: 0.040534, mean_q: 1.211599
 94967/100000: episode: 1691, duration: 0.185s, episode steps: 37, steps per second: 200, episode reward: 23.638, mean reward: 0.639 [0.517, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.126, 10.399], loss: 0.001661, mae: 0.044294, mean_q: 1.212993
 94998/100000: episode: 1692, duration: 0.173s, episode steps: 31, steps per second: 180, episode reward: 19.523, mean reward: 0.630 [0.533, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.069, 10.189], loss: 0.001705, mae: 0.044966, mean_q: 1.216974
 95032/100000: episode: 1693, duration: 0.177s, episode steps: 34, steps per second: 193, episode reward: 22.720, mean reward: 0.668 [0.581, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.709, 10.401], loss: 0.001734, mae: 0.043651, mean_q: 1.213548
 95077/100000: episode: 1694, duration: 0.245s, episode steps: 45, steps per second: 184, episode reward: 26.762, mean reward: 0.595 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.847, 10.100], loss: 0.001464, mae: 0.040430, mean_q: 1.210377
 95108/100000: episode: 1695, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 19.923, mean reward: 0.643 [0.529, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.972, 10.100], loss: 0.001575, mae: 0.041680, mean_q: 1.214177
 95147/100000: episode: 1696, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 30.293, mean reward: 0.777 [0.587, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.132, 10.100], loss: 0.001382, mae: 0.039881, mean_q: 1.212869
 95186/100000: episode: 1697, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 25.793, mean reward: 0.661 [0.576, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-1.444, 10.332], loss: 0.001501, mae: 0.040903, mean_q: 1.210838
 95225/100000: episode: 1698, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 24.411, mean reward: 0.626 [0.524, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.411, 10.100], loss: 0.001653, mae: 0.043189, mean_q: 1.214855
 95270/100000: episode: 1699, duration: 0.253s, episode steps: 45, steps per second: 178, episode reward: 31.805, mean reward: 0.707 [0.598, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.355, 10.100], loss: 0.001697, mae: 0.044387, mean_q: 1.222172
 95309/100000: episode: 1700, duration: 0.207s, episode steps: 39, steps per second: 189, episode reward: 27.879, mean reward: 0.715 [0.607, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.452, 10.344], loss: 0.001423, mae: 0.040847, mean_q: 1.223243
 95329/100000: episode: 1701, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 13.552, mean reward: 0.678 [0.619, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.406, 10.100], loss: 0.001551, mae: 0.042189, mean_q: 1.216785
 95360/100000: episode: 1702, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 20.535, mean reward: 0.662 [0.610, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.398, 10.341], loss: 0.001745, mae: 0.044075, mean_q: 1.221061
 95397/100000: episode: 1703, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 23.853, mean reward: 0.645 [0.532, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.035, 10.211], loss: 0.001519, mae: 0.041345, mean_q: 1.227353
 95424/100000: episode: 1704, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 20.638, mean reward: 0.764 [0.660, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.919, 10.100], loss: 0.001444, mae: 0.041965, mean_q: 1.229093
 95444/100000: episode: 1705, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 15.900, mean reward: 0.795 [0.659, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.685, 10.100], loss: 0.001387, mae: 0.039749, mean_q: 1.228328
 95470/100000: episode: 1706, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 17.520, mean reward: 0.674 [0.612, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.421], loss: 0.001708, mae: 0.044082, mean_q: 1.224126
 95509/100000: episode: 1707, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 24.920, mean reward: 0.639 [0.511, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.333, 10.170], loss: 0.001607, mae: 0.042725, mean_q: 1.227756
 95548/100000: episode: 1708, duration: 0.207s, episode steps: 39, steps per second: 188, episode reward: 25.725, mean reward: 0.660 [0.608, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.813, 10.100], loss: 0.001657, mae: 0.043869, mean_q: 1.226767
 95593/100000: episode: 1709, duration: 0.240s, episode steps: 45, steps per second: 187, episode reward: 30.108, mean reward: 0.669 [0.557, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.454, 10.100], loss: 0.001509, mae: 0.042711, mean_q: 1.227222
 95619/100000: episode: 1710, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 18.125, mean reward: 0.697 [0.595, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.196, 10.278], loss: 0.001537, mae: 0.042071, mean_q: 1.236682
 95664/100000: episode: 1711, duration: 0.247s, episode steps: 45, steps per second: 183, episode reward: 30.183, mean reward: 0.671 [0.602, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.860, 10.100], loss: 0.001703, mae: 0.043086, mean_q: 1.233936
 95693/100000: episode: 1712, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 19.098, mean reward: 0.659 [0.568, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.442, 10.100], loss: 0.001811, mae: 0.044883, mean_q: 1.232911
 95727/100000: episode: 1713, duration: 0.172s, episode steps: 34, steps per second: 198, episode reward: 21.163, mean reward: 0.622 [0.529, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.458, 10.162], loss: 0.001586, mae: 0.042056, mean_q: 1.235615
 95756/100000: episode: 1714, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 18.622, mean reward: 0.642 [0.542, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.057, 10.100], loss: 0.001712, mae: 0.043558, mean_q: 1.239866
 95782/100000: episode: 1715, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 16.725, mean reward: 0.643 [0.553, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.048, 10.234], loss: 0.001893, mae: 0.046563, mean_q: 1.231050
 95802/100000: episode: 1716, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 14.599, mean reward: 0.730 [0.642, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.760, 10.100], loss: 0.001531, mae: 0.042497, mean_q: 1.234771
 95822/100000: episode: 1717, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 13.134, mean reward: 0.657 [0.570, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.446, 10.100], loss: 0.001526, mae: 0.041295, mean_q: 1.233240
 95859/100000: episode: 1718, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 24.714, mean reward: 0.668 [0.507, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.189, 10.108], loss: 0.001583, mae: 0.042390, mean_q: 1.238166
 95885/100000: episode: 1719, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 18.104, mean reward: 0.696 [0.604, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.485, 10.349], loss: 0.001335, mae: 0.038594, mean_q: 1.238275
 95924/100000: episode: 1720, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 24.575, mean reward: 0.630 [0.533, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.429, 10.284], loss: 0.001710, mae: 0.044205, mean_q: 1.232489
 95963/100000: episode: 1721, duration: 0.203s, episode steps: 39, steps per second: 193, episode reward: 26.418, mean reward: 0.677 [0.535, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.667, 10.100], loss: 0.001554, mae: 0.042421, mean_q: 1.236268
[Info] 2-TH LEVEL FOUND: 1.5629205703735352, Considering 10/90 traces
 95997/100000: episode: 1722, duration: 4.475s, episode steps: 34, steps per second: 8, episode reward: 21.258, mean reward: 0.625 [0.564, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.180, 10.324], loss: 0.001509, mae: 0.041013, mean_q: 1.236539
 96009/100000: episode: 1723, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 9.354, mean reward: 0.780 [0.698, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.379, 10.100], loss: 0.001395, mae: 0.041262, mean_q: 1.229254
 96040/100000: episode: 1724, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 22.315, mean reward: 0.720 [0.590, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.365, 10.100], loss: 0.001392, mae: 0.040153, mean_q: 1.239377
 96056/100000: episode: 1725, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 11.586, mean reward: 0.724 [0.628, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.267, 10.100], loss: 0.001541, mae: 0.041891, mean_q: 1.246993
 96072/100000: episode: 1726, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 13.637, mean reward: 0.852 [0.789, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.963, 10.100], loss: 0.001415, mae: 0.040746, mean_q: 1.235693
 96088/100000: episode: 1727, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 11.568, mean reward: 0.723 [0.654, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.364, 10.100], loss: 0.001269, mae: 0.038845, mean_q: 1.260477
 96105/100000: episode: 1728, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 14.015, mean reward: 0.824 [0.719, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.970, 10.100], loss: 0.001451, mae: 0.039745, mean_q: 1.250497
 96140/100000: episode: 1729, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 24.564, mean reward: 0.702 [0.619, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.632, 10.100], loss: 0.001763, mae: 0.044659, mean_q: 1.240088
 96155/100000: episode: 1730, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 12.017, mean reward: 0.801 [0.723, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.504, 10.100], loss: 0.001610, mae: 0.041592, mean_q: 1.235075
 96186/100000: episode: 1731, duration: 0.159s, episode steps: 31, steps per second: 194, episode reward: 21.805, mean reward: 0.703 [0.624, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.571, 10.100], loss: 0.001342, mae: 0.040054, mean_q: 1.253208
 96217/100000: episode: 1732, duration: 0.189s, episode steps: 31, steps per second: 164, episode reward: 23.321, mean reward: 0.752 [0.611, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.267, 10.100], loss: 0.001535, mae: 0.041781, mean_q: 1.248372
 96252/100000: episode: 1733, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 27.453, mean reward: 0.784 [0.637, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.614, 10.100], loss: 0.001627, mae: 0.043599, mean_q: 1.249914
 96267/100000: episode: 1734, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 11.858, mean reward: 0.791 [0.742, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.309, 10.100], loss: 0.001207, mae: 0.039068, mean_q: 1.251642
 96302/100000: episode: 1735, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 24.127, mean reward: 0.689 [0.562, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.206, 10.100], loss: 0.001288, mae: 0.038814, mean_q: 1.252818
 96316/100000: episode: 1736, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 10.590, mean reward: 0.756 [0.666, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.325, 10.100], loss: 0.001517, mae: 0.042433, mean_q: 1.251366
 96330/100000: episode: 1737, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 11.763, mean reward: 0.840 [0.752, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.511, 10.100], loss: 0.001405, mae: 0.039902, mean_q: 1.258803
 96365/100000: episode: 1738, duration: 0.178s, episode steps: 35, steps per second: 196, episode reward: 25.367, mean reward: 0.725 [0.635, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.848, 10.100], loss: 0.001479, mae: 0.041041, mean_q: 1.253459
 96382/100000: episode: 1739, duration: 0.087s, episode steps: 17, steps per second: 194, episode reward: 14.488, mean reward: 0.852 [0.760, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.088, 10.100], loss: 0.001508, mae: 0.041999, mean_q: 1.253230
 96413/100000: episode: 1740, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 20.745, mean reward: 0.669 [0.541, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.548, 10.100], loss: 0.001702, mae: 0.044040, mean_q: 1.250363
 96448/100000: episode: 1741, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 23.359, mean reward: 0.667 [0.503, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.048, 10.113], loss: 0.001513, mae: 0.041769, mean_q: 1.261685
 96453/100000: episode: 1742, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 3.712, mean reward: 0.742 [0.700, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.419, 10.100], loss: 0.001724, mae: 0.044727, mean_q: 1.264123
 96468/100000: episode: 1743, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 11.884, mean reward: 0.792 [0.739, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.425, 10.100], loss: 0.001504, mae: 0.043188, mean_q: 1.259184
 96503/100000: episode: 1744, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 26.502, mean reward: 0.757 [0.689, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.517, 10.100], loss: 0.001371, mae: 0.039806, mean_q: 1.249717
 96519/100000: episode: 1745, duration: 0.090s, episode steps: 16, steps per second: 179, episode reward: 12.385, mean reward: 0.774 [0.682, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.280, 10.100], loss: 0.001589, mae: 0.042707, mean_q: 1.274771
 96535/100000: episode: 1746, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 13.100, mean reward: 0.819 [0.744, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.158, 10.100], loss: 0.001476, mae: 0.041335, mean_q: 1.262621
 96552/100000: episode: 1747, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 13.516, mean reward: 0.795 [0.737, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.617, 10.100], loss: 0.001765, mae: 0.045172, mean_q: 1.269886
 96583/100000: episode: 1748, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 19.620, mean reward: 0.633 [0.516, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.327, 10.100], loss: 0.001662, mae: 0.043278, mean_q: 1.276816
 96600/100000: episode: 1749, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 12.709, mean reward: 0.748 [0.681, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.174, 10.100], loss: 0.001457, mae: 0.041780, mean_q: 1.262894
 96617/100000: episode: 1750, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 12.912, mean reward: 0.760 [0.693, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.428, 10.100], loss: 0.001460, mae: 0.041358, mean_q: 1.280872
 96632/100000: episode: 1751, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 11.141, mean reward: 0.743 [0.680, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.462, 10.100], loss: 0.001399, mae: 0.042019, mean_q: 1.269628
 96637/100000: episode: 1752, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 3.913, mean reward: 0.783 [0.740, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.475, 10.100], loss: 0.001376, mae: 0.040142, mean_q: 1.290490
 96642/100000: episode: 1753, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 3.875, mean reward: 0.775 [0.739, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.056, 10.100], loss: 0.001698, mae: 0.041770, mean_q: 1.246603
 96654/100000: episode: 1754, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 9.343, mean reward: 0.779 [0.740, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.695, 10.100], loss: 0.001406, mae: 0.039242, mean_q: 1.275664
 96671/100000: episode: 1755, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 12.679, mean reward: 0.746 [0.652, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.511, 10.100], loss: 0.001608, mae: 0.042491, mean_q: 1.262073
 96707/100000: episode: 1756, duration: 0.190s, episode steps: 36, steps per second: 190, episode reward: 26.198, mean reward: 0.728 [0.606, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.295, 10.100], loss: 0.001392, mae: 0.039990, mean_q: 1.283383
 96722/100000: episode: 1757, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 10.648, mean reward: 0.710 [0.653, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.214, 10.100], loss: 0.001506, mae: 0.042198, mean_q: 1.257206
 96757/100000: episode: 1758, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 27.234, mean reward: 0.778 [0.693, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.303, 10.100], loss: 0.001394, mae: 0.040919, mean_q: 1.278419
 96773/100000: episode: 1759, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 11.704, mean reward: 0.732 [0.675, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.411, 10.100], loss: 0.001389, mae: 0.039787, mean_q: 1.279803
 96809/100000: episode: 1760, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 24.937, mean reward: 0.693 [0.585, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.241, 10.100], loss: 0.001376, mae: 0.039443, mean_q: 1.275529
 96845/100000: episode: 1761, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 24.916, mean reward: 0.692 [0.581, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.205, 10.100], loss: 0.001462, mae: 0.040079, mean_q: 1.281972
 96860/100000: episode: 1762, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 11.806, mean reward: 0.787 [0.743, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.677, 10.100], loss: 0.001507, mae: 0.040145, mean_q: 1.276851
 96876/100000: episode: 1763, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 12.619, mean reward: 0.789 [0.681, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.291, 10.100], loss: 0.001410, mae: 0.041098, mean_q: 1.268004
 96890/100000: episode: 1764, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 10.998, mean reward: 0.786 [0.731, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.326, 10.100], loss: 0.001322, mae: 0.040922, mean_q: 1.279831
 96925/100000: episode: 1765, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 26.243, mean reward: 0.750 [0.616, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.772, 10.100], loss: 0.001562, mae: 0.041614, mean_q: 1.280804
 96940/100000: episode: 1766, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 10.989, mean reward: 0.733 [0.664, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.404, 10.100], loss: 0.001509, mae: 0.040921, mean_q: 1.294606
 96957/100000: episode: 1767, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 12.507, mean reward: 0.736 [0.685, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.297, 10.100], loss: 0.001182, mae: 0.035840, mean_q: 1.280761
 96993/100000: episode: 1768, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 25.874, mean reward: 0.719 [0.607, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-1.199, 10.100], loss: 0.001322, mae: 0.039008, mean_q: 1.289251
 97008/100000: episode: 1769, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 11.189, mean reward: 0.746 [0.707, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.348, 10.100], loss: 0.001572, mae: 0.043652, mean_q: 1.293820
 97020/100000: episode: 1770, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 8.891, mean reward: 0.741 [0.691, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.393, 10.100], loss: 0.001585, mae: 0.041711, mean_q: 1.267122
 97035/100000: episode: 1771, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 12.116, mean reward: 0.808 [0.757, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.375, 10.100], loss: 0.001196, mae: 0.037651, mean_q: 1.276981
 97070/100000: episode: 1772, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 24.642, mean reward: 0.704 [0.620, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.842, 10.100], loss: 0.001409, mae: 0.040589, mean_q: 1.296724
 97075/100000: episode: 1773, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 3.695, mean reward: 0.739 [0.700, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.399, 10.100], loss: 0.001514, mae: 0.041951, mean_q: 1.308814
 97111/100000: episode: 1774, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 25.691, mean reward: 0.714 [0.529, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.917, 10.100], loss: 0.001551, mae: 0.040981, mean_q: 1.291579
 97146/100000: episode: 1775, duration: 0.212s, episode steps: 35, steps per second: 165, episode reward: 24.913, mean reward: 0.712 [0.551, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.904, 10.100], loss: 0.001203, mae: 0.038383, mean_q: 1.290264
 97160/100000: episode: 1776, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 11.090, mean reward: 0.792 [0.750, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.274, 10.100], loss: 0.001195, mae: 0.036709, mean_q: 1.299166
 97195/100000: episode: 1777, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 24.646, mean reward: 0.704 [0.596, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.468, 10.100], loss: 0.001803, mae: 0.046407, mean_q: 1.290060
 97209/100000: episode: 1778, duration: 0.088s, episode steps: 14, steps per second: 158, episode reward: 11.008, mean reward: 0.786 [0.702, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.568, 10.100], loss: 0.001542, mae: 0.045412, mean_q: 1.302442
 97224/100000: episode: 1779, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 11.917, mean reward: 0.794 [0.734, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.408, 10.100], loss: 0.001355, mae: 0.041259, mean_q: 1.292973
 97238/100000: episode: 1780, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 11.036, mean reward: 0.788 [0.750, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.282, 10.100], loss: 0.001219, mae: 0.039312, mean_q: 1.294066
 97269/100000: episode: 1781, duration: 0.163s, episode steps: 31, steps per second: 191, episode reward: 21.882, mean reward: 0.706 [0.593, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.768, 10.100], loss: 0.001438, mae: 0.040445, mean_q: 1.290960
 97305/100000: episode: 1782, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 24.299, mean reward: 0.675 [0.556, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.343, 10.211], loss: 0.001340, mae: 0.039341, mean_q: 1.305962
 97341/100000: episode: 1783, duration: 0.198s, episode steps: 36, steps per second: 181, episode reward: 24.432, mean reward: 0.679 [0.610, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.177, 10.100], loss: 0.001337, mae: 0.039220, mean_q: 1.300926
 97353/100000: episode: 1784, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 9.331, mean reward: 0.778 [0.731, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.475, 10.100], loss: 0.001466, mae: 0.042908, mean_q: 1.300325
 97358/100000: episode: 1785, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 3.884, mean reward: 0.777 [0.746, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.346, 10.100], loss: 0.001430, mae: 0.041721, mean_q: 1.305097
 97372/100000: episode: 1786, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 11.410, mean reward: 0.815 [0.720, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.645, 10.100], loss: 0.001400, mae: 0.041565, mean_q: 1.314394
 97377/100000: episode: 1787, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 3.922, mean reward: 0.784 [0.765, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.270, 10.100], loss: 0.001197, mae: 0.037470, mean_q: 1.310853
 97382/100000: episode: 1788, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 3.842, mean reward: 0.768 [0.735, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.398, 10.100], loss: 0.001541, mae: 0.042410, mean_q: 1.303405
 97418/100000: episode: 1789, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 26.267, mean reward: 0.730 [0.645, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.379, 10.100], loss: 0.001454, mae: 0.041059, mean_q: 1.301473
 97432/100000: episode: 1790, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 10.920, mean reward: 0.780 [0.730, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.738, 10.100], loss: 0.001293, mae: 0.038506, mean_q: 1.320015
 97467/100000: episode: 1791, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 25.091, mean reward: 0.717 [0.602, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.614, 10.100], loss: 0.001254, mae: 0.038298, mean_q: 1.304244
 97483/100000: episode: 1792, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 12.404, mean reward: 0.775 [0.667, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.245, 10.100], loss: 0.001258, mae: 0.038214, mean_q: 1.317296
 97488/100000: episode: 1793, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 4.002, mean reward: 0.800 [0.770, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.490, 10.100], loss: 0.001412, mae: 0.039408, mean_q: 1.306732
 97519/100000: episode: 1794, duration: 0.155s, episode steps: 31, steps per second: 201, episode reward: 20.784, mean reward: 0.670 [0.525, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.782, 10.100], loss: 0.001337, mae: 0.040111, mean_q: 1.325821
 97535/100000: episode: 1795, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 13.048, mean reward: 0.815 [0.768, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.652, 10.100], loss: 0.001186, mae: 0.038276, mean_q: 1.329705
 97552/100000: episode: 1796, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 13.458, mean reward: 0.792 [0.739, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.343, 10.100], loss: 0.001312, mae: 0.040804, mean_q: 1.321080
 97567/100000: episode: 1797, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 11.806, mean reward: 0.787 [0.709, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.633, 10.100], loss: 0.001405, mae: 0.039554, mean_q: 1.307878
 97579/100000: episode: 1798, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 9.906, mean reward: 0.825 [0.759, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.420, 10.100], loss: 0.001455, mae: 0.040564, mean_q: 1.322581
 97595/100000: episode: 1799, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 12.274, mean reward: 0.767 [0.713, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.287, 10.100], loss: 0.001246, mae: 0.038043, mean_q: 1.307160
 97631/100000: episode: 1800, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 25.779, mean reward: 0.716 [0.621, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.618, 10.100], loss: 0.001205, mae: 0.038212, mean_q: 1.310899
 97646/100000: episode: 1801, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 11.758, mean reward: 0.784 [0.707, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.824, 10.100], loss: 0.001106, mae: 0.037610, mean_q: 1.318548
 97661/100000: episode: 1802, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 11.105, mean reward: 0.740 [0.647, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.333, 10.100], loss: 0.001243, mae: 0.039116, mean_q: 1.319790
 97676/100000: episode: 1803, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 11.273, mean reward: 0.752 [0.710, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.335, 10.100], loss: 0.001089, mae: 0.036794, mean_q: 1.306483
[Info] FALSIFICATION!
 97682/100000: episode: 1804, duration: 0.294s, episode steps: 6, steps per second: 20, episode reward: 4.866, mean reward: 0.811 [0.703, 1.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.186, 9.911], loss: 0.001276, mae: 0.039927, mean_q: 1.308521
 97694/100000: episode: 1805, duration: 0.075s, episode steps: 12, steps per second: 159, episode reward: 9.746, mean reward: 0.812 [0.735, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.320, 10.100], loss: 0.001446, mae: 0.042114, mean_q: 1.324653
 97709/100000: episode: 1806, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 11.769, mean reward: 0.785 [0.733, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.909, 10.100], loss: 0.001225, mae: 0.037547, mean_q: 1.315915
 97724/100000: episode: 1807, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 11.165, mean reward: 0.744 [0.700, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.639, 10.100], loss: 0.001427, mae: 0.040837, mean_q: 1.340490
 97729/100000: episode: 1808, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 4.110, mean reward: 0.822 [0.797, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.521, 10.100], loss: 0.001343, mae: 0.042882, mean_q: 1.324724
 97745/100000: episode: 1809, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 11.995, mean reward: 0.750 [0.691, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.632, 10.100], loss: 0.001257, mae: 0.039818, mean_q: 1.316371
 97760/100000: episode: 1810, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 12.555, mean reward: 0.837 [0.781, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.506, 10.100], loss: 0.001552, mae: 0.042862, mean_q: 1.324070
 97777/100000: episode: 1811, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 13.607, mean reward: 0.800 [0.668, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.341, 10.100], loss: 0.001530, mae: 0.041735, mean_q: 1.327172
[Info] Complete ISplit Iteration
[Info] Levels: [1.3753942, 1.5629206, 1.650091]
[Info] Cond. Prob: [0.1, 0.1, 0.04]
[Info] Error Prob: 0.0004000000000000001

 97808/100000: episode: 1812, duration: 4.673s, episode steps: 31, steps per second: 7, episode reward: 23.682, mean reward: 0.764 [0.703, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-2.429, 10.100], loss: 0.001551, mae: 0.041659, mean_q: 1.317099
 97908/100000: episode: 1813, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 65.505, mean reward: 0.655 [0.509, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.775, 10.098], loss: 0.001240, mae: 0.038344, mean_q: 1.329350
 98008/100000: episode: 1814, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.452, mean reward: 0.575 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.220, 10.295], loss: 0.001376, mae: 0.040097, mean_q: 1.330170
 98108/100000: episode: 1815, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.076, mean reward: 0.581 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.743, 10.158], loss: 0.001217, mae: 0.038502, mean_q: 1.328272
 98208/100000: episode: 1816, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 59.203, mean reward: 0.592 [0.514, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.382, 10.098], loss: 0.001545, mae: 0.041241, mean_q: 1.324947
 98308/100000: episode: 1817, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.100, mean reward: 0.581 [0.509, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.834, 10.129], loss: 0.001371, mae: 0.040425, mean_q: 1.322608
 98408/100000: episode: 1818, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.207, mean reward: 0.582 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.889, 10.187], loss: 0.001316, mae: 0.038794, mean_q: 1.322661
 98508/100000: episode: 1819, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.419, mean reward: 0.594 [0.508, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.317, 10.219], loss: 0.001413, mae: 0.039913, mean_q: 1.317980
 98608/100000: episode: 1820, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 60.017, mean reward: 0.600 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.341, 10.098], loss: 0.001717, mae: 0.042562, mean_q: 1.316920
 98708/100000: episode: 1821, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.134, mean reward: 0.581 [0.503, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.871, 10.227], loss: 0.001386, mae: 0.039983, mean_q: 1.315796
 98808/100000: episode: 1822, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.529, mean reward: 0.595 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.656, 10.280], loss: 0.001417, mae: 0.040479, mean_q: 1.308089
 98908/100000: episode: 1823, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.189, mean reward: 0.582 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.343, 10.259], loss: 0.001341, mae: 0.039319, mean_q: 1.309570
 99008/100000: episode: 1824, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 60.601, mean reward: 0.606 [0.517, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.661, 10.432], loss: 0.001554, mae: 0.040881, mean_q: 1.316687
 99108/100000: episode: 1825, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.655, mean reward: 0.597 [0.503, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.764, 10.256], loss: 0.001502, mae: 0.040985, mean_q: 1.306864
 99208/100000: episode: 1826, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.213, mean reward: 0.592 [0.502, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.858, 10.098], loss: 0.001504, mae: 0.040281, mean_q: 1.305536
 99308/100000: episode: 1827, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 61.066, mean reward: 0.611 [0.511, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.858, 10.209], loss: 0.001745, mae: 0.044055, mean_q: 1.294063
 99408/100000: episode: 1828, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.261, mean reward: 0.583 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.843, 10.140], loss: 0.001368, mae: 0.039620, mean_q: 1.292421
 99508/100000: episode: 1829, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.359, mean reward: 0.574 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.580, 10.141], loss: 0.001560, mae: 0.041237, mean_q: 1.293031
 99608/100000: episode: 1830, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 59.860, mean reward: 0.599 [0.509, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.447, 10.395], loss: 0.001381, mae: 0.039714, mean_q: 1.295712
 99708/100000: episode: 1831, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.167, mean reward: 0.582 [0.509, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.588, 10.264], loss: 0.001577, mae: 0.041870, mean_q: 1.290840
 99808/100000: episode: 1832, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 56.639, mean reward: 0.566 [0.499, 0.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.160, 10.244], loss: 0.001389, mae: 0.039094, mean_q: 1.291994
 99908/100000: episode: 1833, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.089, mean reward: 0.581 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.837, 10.098], loss: 0.001458, mae: 0.040850, mean_q: 1.287966
done, took 628.693 seconds
[Info] End Importance Splitting. Falsification occurred 12 times.
