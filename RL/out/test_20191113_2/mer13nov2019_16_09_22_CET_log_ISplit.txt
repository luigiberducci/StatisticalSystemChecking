Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.168s, episode steps: 100, steps per second: 594, episode reward: 57.591, mean reward: 0.576 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.868, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1581, episode reward: 59.182, mean reward: 0.592 [0.507, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.099, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.076s, episode steps: 100, steps per second: 1324, episode reward: 62.516, mean reward: 0.625 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.347, 10.445], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: 58.553, mean reward: 0.586 [0.501, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.882, 10.303], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.074s, episode steps: 100, steps per second: 1356, episode reward: 59.490, mean reward: 0.595 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.467, 10.370], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 59.662, mean reward: 0.597 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.473, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 59.484, mean reward: 0.595 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.941, 10.191], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.063s, episode steps: 100, steps per second: 1580, episode reward: 58.181, mean reward: 0.582 [0.499, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.668, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.070s, episode steps: 100, steps per second: 1438, episode reward: 59.372, mean reward: 0.594 [0.503, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.624, 10.199], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 56.774, mean reward: 0.568 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.768, 10.102], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 58.968, mean reward: 0.590 [0.503, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.303, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.090s, episode steps: 100, steps per second: 1105, episode reward: 57.264, mean reward: 0.573 [0.508, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.369, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 56.724, mean reward: 0.567 [0.503, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.419, 10.257], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 57.920, mean reward: 0.579 [0.507, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.417, 10.335], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 58.420, mean reward: 0.584 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.793, 10.126], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 59.714, mean reward: 0.597 [0.510, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.589, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.069s, episode steps: 100, steps per second: 1443, episode reward: 65.412, mean reward: 0.654 [0.509, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.582, 10.305], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.073s, episode steps: 100, steps per second: 1373, episode reward: 58.841, mean reward: 0.588 [0.500, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.597, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 57.815, mean reward: 0.578 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.038, 10.104], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 59.438, mean reward: 0.594 [0.520, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.771, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.069s, episode steps: 100, steps per second: 1451, episode reward: 61.216, mean reward: 0.612 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.361, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.063s, episode steps: 100, steps per second: 1588, episode reward: 58.221, mean reward: 0.582 [0.517, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.482, 10.244], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 56.558, mean reward: 0.566 [0.503, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.971, 10.290], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.063s, episode steps: 100, steps per second: 1576, episode reward: 60.528, mean reward: 0.605 [0.512, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.810, 10.190], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 58.412, mean reward: 0.584 [0.511, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.010, 10.333], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.063s, episode steps: 100, steps per second: 1578, episode reward: 60.274, mean reward: 0.603 [0.504, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.824, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.063s, episode steps: 100, steps per second: 1580, episode reward: 56.736, mean reward: 0.567 [0.509, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.202, 10.159], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.068s, episode steps: 100, steps per second: 1480, episode reward: 57.613, mean reward: 0.576 [0.504, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.823, 10.113], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.075s, episode steps: 100, steps per second: 1339, episode reward: 57.779, mean reward: 0.578 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.054, 10.153], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.063s, episode steps: 100, steps per second: 1576, episode reward: 60.709, mean reward: 0.607 [0.512, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.803, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 57.511, mean reward: 0.575 [0.508, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.204, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 59.268, mean reward: 0.593 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.743, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.063s, episode steps: 100, steps per second: 1580, episode reward: 57.950, mean reward: 0.579 [0.509, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.742, 10.235], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.070s, episode steps: 100, steps per second: 1438, episode reward: 56.963, mean reward: 0.570 [0.501, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.244, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.064s, episode steps: 100, steps per second: 1569, episode reward: 58.566, mean reward: 0.586 [0.505, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.658, 10.149], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.063s, episode steps: 100, steps per second: 1576, episode reward: 59.894, mean reward: 0.599 [0.508, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.009, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.063s, episode steps: 100, steps per second: 1575, episode reward: 58.114, mean reward: 0.581 [0.500, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.719, 10.139], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.065s, episode steps: 100, steps per second: 1530, episode reward: 62.808, mean reward: 0.628 [0.509, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.833, 10.341], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.067s, episode steps: 100, steps per second: 1490, episode reward: 57.714, mean reward: 0.577 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.034, 10.180], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.069s, episode steps: 100, steps per second: 1447, episode reward: 62.195, mean reward: 0.622 [0.516, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-1.640, 10.212], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.075s, episode steps: 100, steps per second: 1338, episode reward: 63.649, mean reward: 0.636 [0.513, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.937, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.063s, episode steps: 100, steps per second: 1582, episode reward: 62.126, mean reward: 0.621 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.223, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 59.059, mean reward: 0.591 [0.500, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.547, 10.148], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.063s, episode steps: 100, steps per second: 1581, episode reward: 62.482, mean reward: 0.625 [0.505, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.752, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.063s, episode steps: 100, steps per second: 1584, episode reward: 60.084, mean reward: 0.601 [0.509, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.347, 10.242], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 59.330, mean reward: 0.593 [0.512, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.861, 10.121], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.063s, episode steps: 100, steps per second: 1580, episode reward: 61.489, mean reward: 0.615 [0.506, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.908, 10.328], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 57.520, mean reward: 0.575 [0.502, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.975, 10.221], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.076s, episode steps: 100, steps per second: 1318, episode reward: 59.834, mean reward: 0.598 [0.500, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.149, 10.119], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 60.080, mean reward: 0.601 [0.513, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.359, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.294s, episode steps: 100, steps per second: 77, episode reward: 60.637, mean reward: 0.606 [0.523, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.517, 10.098], loss: 0.012044, mae: 0.102825, mean_q: 0.700527
  5200/100000: episode: 52, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 57.148, mean reward: 0.571 [0.499, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.990, 10.098], loss: 0.002954, mae: 0.054726, mean_q: 0.891282
  5300/100000: episode: 53, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 57.285, mean reward: 0.573 [0.505, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.721, 10.098], loss: 0.002920, mae: 0.054962, mean_q: 0.998019
  5400/100000: episode: 54, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.114, mean reward: 0.581 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.689, 10.098], loss: 0.003026, mae: 0.054172, mean_q: 1.063685
  5500/100000: episode: 55, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.957, mean reward: 0.590 [0.500, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.068, 10.098], loss: 0.003153, mae: 0.054572, mean_q: 1.103338
  5600/100000: episode: 56, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.142, mean reward: 0.591 [0.500, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.519, 10.330], loss: 0.003204, mae: 0.054416, mean_q: 1.130541
  5700/100000: episode: 57, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.780, mean reward: 0.598 [0.504, 0.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.546, 10.195], loss: 0.003076, mae: 0.053784, mean_q: 1.145802
  5800/100000: episode: 58, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 57.461, mean reward: 0.575 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.000, 10.098], loss: 0.003290, mae: 0.054927, mean_q: 1.157629
  5900/100000: episode: 59, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.729, mean reward: 0.597 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.799, 10.233], loss: 0.003224, mae: 0.054685, mean_q: 1.163299
  6000/100000: episode: 60, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.643, mean reward: 0.586 [0.521, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.549, 10.191], loss: 0.003361, mae: 0.055968, mean_q: 1.167501
  6100/100000: episode: 61, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.188, mean reward: 0.602 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.673, 10.433], loss: 0.002832, mae: 0.051700, mean_q: 1.169251
  6200/100000: episode: 62, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.842, mean reward: 0.578 [0.505, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.775, 10.270], loss: 0.002999, mae: 0.052853, mean_q: 1.171750
  6300/100000: episode: 63, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.928, mean reward: 0.599 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.902, 10.098], loss: 0.003056, mae: 0.054168, mean_q: 1.171805
  6400/100000: episode: 64, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.173, mean reward: 0.592 [0.520, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.645, 10.331], loss: 0.002836, mae: 0.052022, mean_q: 1.173543
  6500/100000: episode: 65, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.981, mean reward: 0.600 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.788, 10.282], loss: 0.003016, mae: 0.052858, mean_q: 1.171415
  6600/100000: episode: 66, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 58.329, mean reward: 0.583 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.074, 10.207], loss: 0.003418, mae: 0.056251, mean_q: 1.175019
  6700/100000: episode: 67, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 57.924, mean reward: 0.579 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.347, 10.113], loss: 0.002893, mae: 0.050926, mean_q: 1.170632
  6800/100000: episode: 68, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.288, mean reward: 0.583 [0.508, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.820, 10.211], loss: 0.002689, mae: 0.051365, mean_q: 1.172690
  6900/100000: episode: 69, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.153, mean reward: 0.582 [0.510, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.631, 10.192], loss: 0.002651, mae: 0.050592, mean_q: 1.174310
  7000/100000: episode: 70, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.660, mean reward: 0.587 [0.507, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.240, 10.098], loss: 0.002957, mae: 0.053818, mean_q: 1.173299
  7100/100000: episode: 71, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 58.440, mean reward: 0.584 [0.509, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.444, 10.098], loss: 0.002711, mae: 0.049939, mean_q: 1.171186
  7200/100000: episode: 72, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.428, mean reward: 0.584 [0.505, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.819, 10.105], loss: 0.003070, mae: 0.051644, mean_q: 1.167276
  7300/100000: episode: 73, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.737, mean reward: 0.607 [0.505, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.561, 10.162], loss: 0.002924, mae: 0.052564, mean_q: 1.168836
  7400/100000: episode: 74, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.167, mean reward: 0.582 [0.512, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.948, 10.223], loss: 0.002393, mae: 0.048635, mean_q: 1.169524
  7500/100000: episode: 75, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 64.399, mean reward: 0.644 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.551, 10.098], loss: 0.002492, mae: 0.049334, mean_q: 1.170515
  7600/100000: episode: 76, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.868, mean reward: 0.589 [0.508, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.003, 10.098], loss: 0.003018, mae: 0.052935, mean_q: 1.170458
  7700/100000: episode: 77, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.581, mean reward: 0.576 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.354, 10.238], loss: 0.002651, mae: 0.050466, mean_q: 1.171120
  7800/100000: episode: 78, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.685, mean reward: 0.597 [0.503, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.979, 10.098], loss: 0.002799, mae: 0.051504, mean_q: 1.173516
  7900/100000: episode: 79, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.937, mean reward: 0.579 [0.497, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.542, 10.098], loss: 0.002304, mae: 0.049047, mean_q: 1.175881
  8000/100000: episode: 80, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 60.696, mean reward: 0.607 [0.515, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.935, 10.098], loss: 0.002877, mae: 0.052338, mean_q: 1.174884
  8100/100000: episode: 81, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.743, mean reward: 0.587 [0.510, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.421, 10.146], loss: 0.002552, mae: 0.049550, mean_q: 1.177115
  8200/100000: episode: 82, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 59.934, mean reward: 0.599 [0.508, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.520, 10.098], loss: 0.002609, mae: 0.050302, mean_q: 1.173535
  8300/100000: episode: 83, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.590, mean reward: 0.586 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.086, 10.428], loss: 0.003069, mae: 0.055113, mean_q: 1.175701
  8400/100000: episode: 84, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.012, mean reward: 0.580 [0.510, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.287, 10.296], loss: 0.002970, mae: 0.052245, mean_q: 1.171230
  8500/100000: episode: 85, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.766, mean reward: 0.608 [0.525, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.239, 10.114], loss: 0.002702, mae: 0.050913, mean_q: 1.174780
  8600/100000: episode: 86, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.229, mean reward: 0.602 [0.510, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.525, 10.272], loss: 0.002917, mae: 0.052943, mean_q: 1.176212
  8700/100000: episode: 87, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.910, mean reward: 0.579 [0.498, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.620, 10.200], loss: 0.002712, mae: 0.051528, mean_q: 1.175452
  8800/100000: episode: 88, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.520, mean reward: 0.575 [0.504, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.611, 10.122], loss: 0.002638, mae: 0.050388, mean_q: 1.173590
  8900/100000: episode: 89, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.514, mean reward: 0.595 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.364, 10.098], loss: 0.002695, mae: 0.051392, mean_q: 1.176537
  9000/100000: episode: 90, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 61.616, mean reward: 0.616 [0.514, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.000, 10.133], loss: 0.002456, mae: 0.050064, mean_q: 1.175156
  9100/100000: episode: 91, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.936, mean reward: 0.589 [0.506, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.885, 10.098], loss: 0.002813, mae: 0.051464, mean_q: 1.172114
  9200/100000: episode: 92, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.990, mean reward: 0.600 [0.504, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.920, 10.098], loss: 0.002659, mae: 0.049628, mean_q: 1.171002
  9300/100000: episode: 93, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 56.690, mean reward: 0.567 [0.508, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.378, 10.272], loss: 0.002706, mae: 0.051187, mean_q: 1.172074
  9400/100000: episode: 94, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.505, mean reward: 0.605 [0.505, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.701, 10.314], loss: 0.002692, mae: 0.050078, mean_q: 1.169994
  9500/100000: episode: 95, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.526, mean reward: 0.605 [0.510, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.631, 10.098], loss: 0.002453, mae: 0.049781, mean_q: 1.169863
  9600/100000: episode: 96, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.399, mean reward: 0.604 [0.512, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.397, 10.185], loss: 0.002326, mae: 0.047657, mean_q: 1.171694
  9700/100000: episode: 97, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 57.332, mean reward: 0.573 [0.511, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.472, 10.222], loss: 0.002466, mae: 0.049968, mean_q: 1.171845
  9800/100000: episode: 98, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.983, mean reward: 0.600 [0.504, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.921, 10.254], loss: 0.002314, mae: 0.048685, mean_q: 1.171130
  9900/100000: episode: 99, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 57.480, mean reward: 0.575 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.788, 10.098], loss: 0.002441, mae: 0.048274, mean_q: 1.169051
 10000/100000: episode: 100, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 60.026, mean reward: 0.600 [0.510, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.079, 10.366], loss: 0.002545, mae: 0.049478, mean_q: 1.168062
 10100/100000: episode: 101, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.443, mean reward: 0.574 [0.498, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.603, 10.207], loss: 0.002434, mae: 0.048311, mean_q: 1.168856
 10200/100000: episode: 102, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 60.981, mean reward: 0.610 [0.519, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.936, 10.098], loss: 0.002652, mae: 0.050467, mean_q: 1.169639
 10300/100000: episode: 103, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 59.234, mean reward: 0.592 [0.500, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.800, 10.124], loss: 0.002709, mae: 0.050510, mean_q: 1.168637
 10400/100000: episode: 104, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.391, mean reward: 0.584 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.049, 10.098], loss: 0.002639, mae: 0.050698, mean_q: 1.169063
 10500/100000: episode: 105, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.429, mean reward: 0.574 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.439, 10.267], loss: 0.002341, mae: 0.048068, mean_q: 1.171721
 10600/100000: episode: 106, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.595, mean reward: 0.596 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.916, 10.228], loss: 0.002594, mae: 0.049752, mean_q: 1.170849
 10700/100000: episode: 107, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 61.032, mean reward: 0.610 [0.506, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.430, 10.098], loss: 0.002629, mae: 0.050209, mean_q: 1.171836
 10800/100000: episode: 108, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.180, mean reward: 0.572 [0.504, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.763, 10.098], loss: 0.002681, mae: 0.050282, mean_q: 1.170337
 10900/100000: episode: 109, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 59.364, mean reward: 0.594 [0.505, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.045, 10.098], loss: 0.002807, mae: 0.052363, mean_q: 1.171252
 11000/100000: episode: 110, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.689, mean reward: 0.587 [0.501, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.092, 10.098], loss: 0.002478, mae: 0.048278, mean_q: 1.167608
 11100/100000: episode: 111, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.036, mean reward: 0.580 [0.512, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.706, 10.100], loss: 0.002506, mae: 0.049513, mean_q: 1.169510
 11200/100000: episode: 112, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.277, mean reward: 0.583 [0.501, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.825, 10.098], loss: 0.002524, mae: 0.050666, mean_q: 1.169469
 11300/100000: episode: 113, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 58.443, mean reward: 0.584 [0.501, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.885, 10.098], loss: 0.002405, mae: 0.048631, mean_q: 1.170877
 11400/100000: episode: 114, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.403, mean reward: 0.584 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.438, 10.294], loss: 0.002804, mae: 0.051557, mean_q: 1.167752
 11500/100000: episode: 115, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.547, mean reward: 0.585 [0.508, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.656, 10.098], loss: 0.002589, mae: 0.050197, mean_q: 1.170485
 11600/100000: episode: 116, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.001, mean reward: 0.600 [0.506, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.220, 10.307], loss: 0.002685, mae: 0.050858, mean_q: 1.168901
 11700/100000: episode: 117, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.836, mean reward: 0.598 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.327, 10.307], loss: 0.002204, mae: 0.047539, mean_q: 1.170601
 11800/100000: episode: 118, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.724, mean reward: 0.587 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.279, 10.298], loss: 0.002161, mae: 0.047023, mean_q: 1.172498
 11900/100000: episode: 119, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.662, mean reward: 0.597 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.011, 10.098], loss: 0.002350, mae: 0.047958, mean_q: 1.171982
 12000/100000: episode: 120, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 56.395, mean reward: 0.564 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.635, 10.098], loss: 0.002726, mae: 0.050664, mean_q: 1.170736
 12100/100000: episode: 121, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.786, mean reward: 0.608 [0.507, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.015, 10.098], loss: 0.002468, mae: 0.049567, mean_q: 1.171392
 12200/100000: episode: 122, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.193, mean reward: 0.602 [0.518, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.757, 10.098], loss: 0.002508, mae: 0.048517, mean_q: 1.171195
 12300/100000: episode: 123, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.353, mean reward: 0.584 [0.507, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.666, 10.215], loss: 0.002401, mae: 0.048697, mean_q: 1.171061
 12400/100000: episode: 124, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.397, mean reward: 0.584 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.608, 10.098], loss: 0.002424, mae: 0.048700, mean_q: 1.170900
 12500/100000: episode: 125, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 62.172, mean reward: 0.622 [0.510, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.809, 10.349], loss: 0.002442, mae: 0.049577, mean_q: 1.170082
 12600/100000: episode: 126, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.371, mean reward: 0.584 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.859, 10.196], loss: 0.002426, mae: 0.048787, mean_q: 1.173963
 12700/100000: episode: 127, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.139, mean reward: 0.581 [0.510, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.242, 10.098], loss: 0.002373, mae: 0.048510, mean_q: 1.171758
 12800/100000: episode: 128, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 59.997, mean reward: 0.600 [0.513, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.708, 10.340], loss: 0.002300, mae: 0.047770, mean_q: 1.172149
 12900/100000: episode: 129, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.418, mean reward: 0.584 [0.518, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.635, 10.211], loss: 0.002632, mae: 0.048994, mean_q: 1.170798
 13000/100000: episode: 130, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 57.298, mean reward: 0.573 [0.504, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.952, 10.169], loss: 0.002702, mae: 0.049662, mean_q: 1.168674
 13100/100000: episode: 131, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.338, mean reward: 0.573 [0.506, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.129, 10.258], loss: 0.002661, mae: 0.050858, mean_q: 1.168639
 13200/100000: episode: 132, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.677, mean reward: 0.577 [0.514, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.877, 10.098], loss: 0.002364, mae: 0.048064, mean_q: 1.166733
 13300/100000: episode: 133, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.074, mean reward: 0.591 [0.503, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.476, 10.182], loss: 0.002369, mae: 0.048772, mean_q: 1.168536
 13400/100000: episode: 134, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 61.385, mean reward: 0.614 [0.501, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.394, 10.235], loss: 0.002483, mae: 0.048342, mean_q: 1.167883
 13500/100000: episode: 135, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.043, mean reward: 0.600 [0.509, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.737, 10.098], loss: 0.002373, mae: 0.047564, mean_q: 1.166602
 13600/100000: episode: 136, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 59.713, mean reward: 0.597 [0.501, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.013, 10.098], loss: 0.002656, mae: 0.050574, mean_q: 1.164713
 13700/100000: episode: 137, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.860, mean reward: 0.589 [0.500, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.393, 10.176], loss: 0.002126, mae: 0.046464, mean_q: 1.168147
 13800/100000: episode: 138, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.412, mean reward: 0.574 [0.507, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.631, 10.098], loss: 0.002672, mae: 0.050121, mean_q: 1.166410
 13900/100000: episode: 139, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 56.472, mean reward: 0.565 [0.504, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.550, 10.217], loss: 0.002285, mae: 0.047302, mean_q: 1.169502
 14000/100000: episode: 140, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.784, mean reward: 0.588 [0.505, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.452, 10.098], loss: 0.002477, mae: 0.049221, mean_q: 1.167977
 14100/100000: episode: 141, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 56.744, mean reward: 0.567 [0.500, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.522, 10.200], loss: 0.002715, mae: 0.051763, mean_q: 1.166153
 14200/100000: episode: 142, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.156, mean reward: 0.592 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.344, 10.129], loss: 0.002499, mae: 0.049080, mean_q: 1.165488
 14300/100000: episode: 143, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 59.719, mean reward: 0.597 [0.524, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.795, 10.218], loss: 0.002381, mae: 0.048142, mean_q: 1.165853
 14400/100000: episode: 144, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 56.102, mean reward: 0.561 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.427, 10.151], loss: 0.002347, mae: 0.048323, mean_q: 1.167262
 14500/100000: episode: 145, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.388, mean reward: 0.574 [0.510, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.098], loss: 0.002234, mae: 0.048160, mean_q: 1.165546
 14600/100000: episode: 146, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 56.954, mean reward: 0.570 [0.502, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.796, 10.098], loss: 0.002512, mae: 0.048770, mean_q: 1.163124
 14700/100000: episode: 147, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 63.456, mean reward: 0.635 [0.507, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.854, 10.324], loss: 0.002354, mae: 0.048328, mean_q: 1.163390
 14800/100000: episode: 148, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.599, mean reward: 0.586 [0.510, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.816, 10.183], loss: 0.002429, mae: 0.048313, mean_q: 1.162824
 14900/100000: episode: 149, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 56.987, mean reward: 0.570 [0.509, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.431, 10.110], loss: 0.002274, mae: 0.048280, mean_q: 1.162741
[Info] 1-TH LEVEL FOUND: 1.2088379859924316, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.145s, episode steps: 100, steps per second: 19, episode reward: 59.973, mean reward: 0.600 [0.501, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.091, 10.098], loss: 0.002329, mae: 0.048712, mean_q: 1.165151
 15051/100000: episode: 151, duration: 0.273s, episode steps: 51, steps per second: 187, episode reward: 29.287, mean reward: 0.574 [0.507, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.393, 10.188], loss: 0.002435, mae: 0.050361, mean_q: 1.163125
 15102/100000: episode: 152, duration: 0.257s, episode steps: 51, steps per second: 198, episode reward: 29.083, mean reward: 0.570 [0.499, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.319, 10.100], loss: 0.002476, mae: 0.049312, mean_q: 1.158780
 15153/100000: episode: 153, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 31.308, mean reward: 0.614 [0.530, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.360, 10.100], loss: 0.002316, mae: 0.047663, mean_q: 1.160823
 15247/100000: episode: 154, duration: 0.494s, episode steps: 94, steps per second: 190, episode reward: 57.865, mean reward: 0.616 [0.503, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.954, 10.100], loss: 0.002270, mae: 0.047628, mean_q: 1.162610
 15295/100000: episode: 155, duration: 0.242s, episode steps: 48, steps per second: 198, episode reward: 29.721, mean reward: 0.619 [0.514, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.553, 10.167], loss: 0.002176, mae: 0.048013, mean_q: 1.163289
 15350/100000: episode: 156, duration: 0.257s, episode steps: 55, steps per second: 214, episode reward: 33.586, mean reward: 0.611 [0.520, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.443, 10.196], loss: 0.002251, mae: 0.047463, mean_q: 1.162047
 15401/100000: episode: 157, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 31.486, mean reward: 0.617 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-1.231, 10.408], loss: 0.002380, mae: 0.048555, mean_q: 1.163696
 15446/100000: episode: 158, duration: 0.248s, episode steps: 45, steps per second: 182, episode reward: 29.091, mean reward: 0.646 [0.521, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.634, 10.301], loss: 0.002284, mae: 0.048219, mean_q: 1.166622
 15497/100000: episode: 159, duration: 0.243s, episode steps: 51, steps per second: 210, episode reward: 31.494, mean reward: 0.618 [0.504, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.269, 10.100], loss: 0.002480, mae: 0.048769, mean_q: 1.164729
 15548/100000: episode: 160, duration: 0.284s, episode steps: 51, steps per second: 180, episode reward: 34.034, mean reward: 0.667 [0.500, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.355, 10.458], loss: 0.002336, mae: 0.049231, mean_q: 1.168177
 15603/100000: episode: 161, duration: 0.284s, episode steps: 55, steps per second: 193, episode reward: 34.389, mean reward: 0.625 [0.543, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-1.159, 10.286], loss: 0.002147, mae: 0.047952, mean_q: 1.171185
 15654/100000: episode: 162, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 32.619, mean reward: 0.640 [0.524, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.709, 10.411], loss: 0.002338, mae: 0.047801, mean_q: 1.169379
 15705/100000: episode: 163, duration: 0.248s, episode steps: 51, steps per second: 206, episode reward: 30.143, mean reward: 0.591 [0.510, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.528, 10.197], loss: 0.002813, mae: 0.052299, mean_q: 1.163604
 15756/100000: episode: 164, duration: 0.260s, episode steps: 51, steps per second: 197, episode reward: 29.508, mean reward: 0.579 [0.499, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.636, 10.133], loss: 0.002286, mae: 0.048399, mean_q: 1.169890
 15807/100000: episode: 165, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 32.761, mean reward: 0.642 [0.512, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.120, 10.354], loss: 0.002545, mae: 0.051166, mean_q: 1.169615
 15901/100000: episode: 166, duration: 0.465s, episode steps: 94, steps per second: 202, episode reward: 54.622, mean reward: 0.581 [0.502, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-1.001, 10.122], loss: 0.002405, mae: 0.049145, mean_q: 1.168945
 15952/100000: episode: 167, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 32.355, mean reward: 0.634 [0.521, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.674, 10.100], loss: 0.002798, mae: 0.052357, mean_q: 1.166002
 16045/100000: episode: 168, duration: 0.450s, episode steps: 93, steps per second: 207, episode reward: 57.674, mean reward: 0.620 [0.506, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.520 [-0.788, 10.452], loss: 0.002520, mae: 0.051503, mean_q: 1.171342
 16095/100000: episode: 169, duration: 0.247s, episode steps: 50, steps per second: 203, episode reward: 30.357, mean reward: 0.607 [0.521, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.675, 10.100], loss: 0.002014, mae: 0.046423, mean_q: 1.172112
 16140/100000: episode: 170, duration: 0.222s, episode steps: 45, steps per second: 202, episode reward: 27.671, mean reward: 0.615 [0.541, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.129, 10.213], loss: 0.002311, mae: 0.049421, mean_q: 1.171374
 16195/100000: episode: 171, duration: 0.283s, episode steps: 55, steps per second: 194, episode reward: 33.378, mean reward: 0.607 [0.513, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.652, 10.326], loss: 0.002120, mae: 0.047261, mean_q: 1.171517
 16246/100000: episode: 172, duration: 0.257s, episode steps: 51, steps per second: 199, episode reward: 30.233, mean reward: 0.593 [0.519, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-1.266, 10.227], loss: 0.002634, mae: 0.052928, mean_q: 1.171730
 16301/100000: episode: 173, duration: 0.265s, episode steps: 55, steps per second: 207, episode reward: 32.309, mean reward: 0.587 [0.512, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-1.346, 10.194], loss: 0.002248, mae: 0.047422, mean_q: 1.171913
 16346/100000: episode: 174, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 25.589, mean reward: 0.569 [0.512, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.196, 10.202], loss: 0.002546, mae: 0.050367, mean_q: 1.170495
 16440/100000: episode: 175, duration: 0.470s, episode steps: 94, steps per second: 200, episode reward: 53.151, mean reward: 0.565 [0.502, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.355, 10.100], loss: 0.002223, mae: 0.048454, mean_q: 1.175616
 16495/100000: episode: 176, duration: 0.269s, episode steps: 55, steps per second: 204, episode reward: 31.744, mean reward: 0.577 [0.504, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.603, 10.133], loss: 0.002549, mae: 0.052320, mean_q: 1.174466
 16545/100000: episode: 177, duration: 0.244s, episode steps: 50, steps per second: 205, episode reward: 30.447, mean reward: 0.609 [0.527, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.841, 10.165], loss: 0.002271, mae: 0.047802, mean_q: 1.170657
 16638/100000: episode: 178, duration: 0.444s, episode steps: 93, steps per second: 210, episode reward: 56.174, mean reward: 0.604 [0.499, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.530 [-0.265, 10.278], loss: 0.002431, mae: 0.050316, mean_q: 1.172582
 16689/100000: episode: 179, duration: 0.273s, episode steps: 51, steps per second: 187, episode reward: 32.892, mean reward: 0.645 [0.515, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.538, 10.335], loss: 0.002603, mae: 0.052048, mean_q: 1.172165
 16737/100000: episode: 180, duration: 0.241s, episode steps: 48, steps per second: 199, episode reward: 29.245, mean reward: 0.609 [0.505, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.556, 10.207], loss: 0.002334, mae: 0.050035, mean_q: 1.173868
 16785/100000: episode: 181, duration: 0.248s, episode steps: 48, steps per second: 193, episode reward: 30.432, mean reward: 0.634 [0.556, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.926, 10.303], loss: 0.002309, mae: 0.048758, mean_q: 1.174517
 16830/100000: episode: 182, duration: 0.233s, episode steps: 45, steps per second: 193, episode reward: 31.636, mean reward: 0.703 [0.528, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.102, 10.426], loss: 0.002443, mae: 0.050828, mean_q: 1.174591
 16924/100000: episode: 183, duration: 0.482s, episode steps: 94, steps per second: 195, episode reward: 54.799, mean reward: 0.583 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.514 [-1.749, 10.263], loss: 0.002401, mae: 0.049704, mean_q: 1.173900
 16972/100000: episode: 184, duration: 0.240s, episode steps: 48, steps per second: 200, episode reward: 29.269, mean reward: 0.610 [0.503, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.701, 10.100], loss: 0.002370, mae: 0.049968, mean_q: 1.172505
 17066/100000: episode: 185, duration: 0.473s, episode steps: 94, steps per second: 199, episode reward: 54.963, mean reward: 0.585 [0.507, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.518 [-0.441, 10.464], loss: 0.002197, mae: 0.048695, mean_q: 1.176104
 17121/100000: episode: 186, duration: 0.269s, episode steps: 55, steps per second: 204, episode reward: 30.720, mean reward: 0.559 [0.504, 0.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.716, 10.100], loss: 0.002716, mae: 0.053501, mean_q: 1.170809
 17172/100000: episode: 187, duration: 0.245s, episode steps: 51, steps per second: 208, episode reward: 29.843, mean reward: 0.585 [0.500, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.584, 10.262], loss: 0.002398, mae: 0.051462, mean_q: 1.175602
 17227/100000: episode: 188, duration: 0.263s, episode steps: 55, steps per second: 209, episode reward: 32.071, mean reward: 0.583 [0.502, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.447, 10.151], loss: 0.002595, mae: 0.053367, mean_q: 1.173264
 17278/100000: episode: 189, duration: 0.247s, episode steps: 51, steps per second: 207, episode reward: 32.086, mean reward: 0.629 [0.511, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-1.108, 10.423], loss: 0.002220, mae: 0.049190, mean_q: 1.174037
 17371/100000: episode: 190, duration: 0.453s, episode steps: 93, steps per second: 205, episode reward: 56.006, mean reward: 0.602 [0.530, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.518 [-1.637, 10.176], loss: 0.002272, mae: 0.049652, mean_q: 1.173296
 17422/100000: episode: 191, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 29.225, mean reward: 0.573 [0.506, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.717, 10.304], loss: 0.002079, mae: 0.048305, mean_q: 1.172847
 17470/100000: episode: 192, duration: 0.235s, episode steps: 48, steps per second: 204, episode reward: 29.302, mean reward: 0.610 [0.523, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.708, 10.158], loss: 0.002210, mae: 0.048363, mean_q: 1.170568
 17525/100000: episode: 193, duration: 0.279s, episode steps: 55, steps per second: 197, episode reward: 33.314, mean reward: 0.606 [0.518, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-0.928, 10.100], loss: 0.002161, mae: 0.047932, mean_q: 1.175681
 17576/100000: episode: 194, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 30.978, mean reward: 0.607 [0.509, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-1.161, 10.100], loss: 0.002208, mae: 0.049182, mean_q: 1.172423
 17627/100000: episode: 195, duration: 0.249s, episode steps: 51, steps per second: 205, episode reward: 28.920, mean reward: 0.567 [0.504, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.614, 10.100], loss: 0.002276, mae: 0.050053, mean_q: 1.172263
 17677/100000: episode: 196, duration: 0.271s, episode steps: 50, steps per second: 185, episode reward: 29.650, mean reward: 0.593 [0.520, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.394, 10.100], loss: 0.002333, mae: 0.051095, mean_q: 1.174258
 17771/100000: episode: 197, duration: 0.464s, episode steps: 94, steps per second: 203, episode reward: 54.940, mean reward: 0.584 [0.503, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.511 [-0.711, 10.271], loss: 0.002194, mae: 0.049368, mean_q: 1.175969
 17821/100000: episode: 198, duration: 0.243s, episode steps: 50, steps per second: 206, episode reward: 29.157, mean reward: 0.583 [0.510, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.189, 10.100], loss: 0.002356, mae: 0.050620, mean_q: 1.174317
 17869/100000: episode: 199, duration: 0.240s, episode steps: 48, steps per second: 200, episode reward: 29.312, mean reward: 0.611 [0.525, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-1.360, 10.261], loss: 0.002261, mae: 0.049991, mean_q: 1.169883
 17920/100000: episode: 200, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 29.412, mean reward: 0.577 [0.502, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.453, 10.100], loss: 0.002108, mae: 0.048504, mean_q: 1.178518
 17971/100000: episode: 201, duration: 0.250s, episode steps: 51, steps per second: 204, episode reward: 30.210, mean reward: 0.592 [0.513, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.149, 10.100], loss: 0.002712, mae: 0.053197, mean_q: 1.169279
 18022/100000: episode: 202, duration: 0.250s, episode steps: 51, steps per second: 204, episode reward: 30.418, mean reward: 0.596 [0.521, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-1.183, 10.100], loss: 0.002247, mae: 0.050114, mean_q: 1.178266
 18115/100000: episode: 203, duration: 0.477s, episode steps: 93, steps per second: 195, episode reward: 57.510, mean reward: 0.618 [0.520, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.518 [-0.309, 10.162], loss: 0.002355, mae: 0.050233, mean_q: 1.172153
 18166/100000: episode: 204, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 29.038, mean reward: 0.569 [0.499, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.823, 10.161], loss: 0.002411, mae: 0.051975, mean_q: 1.176547
 18216/100000: episode: 205, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 30.207, mean reward: 0.604 [0.530, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.881, 10.172], loss: 0.002297, mae: 0.049610, mean_q: 1.177614
 18266/100000: episode: 206, duration: 0.273s, episode steps: 50, steps per second: 183, episode reward: 31.337, mean reward: 0.627 [0.512, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-1.662, 10.100], loss: 0.002645, mae: 0.054602, mean_q: 1.175464
 18317/100000: episode: 207, duration: 0.241s, episode steps: 51, steps per second: 212, episode reward: 29.816, mean reward: 0.585 [0.508, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.115, 10.219], loss: 0.002441, mae: 0.052073, mean_q: 1.175021
 18365/100000: episode: 208, duration: 0.232s, episode steps: 48, steps per second: 207, episode reward: 27.994, mean reward: 0.583 [0.513, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.030, 10.100], loss: 0.002083, mae: 0.048849, mean_q: 1.174668
 18416/100000: episode: 209, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 31.335, mean reward: 0.614 [0.511, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-1.083, 10.100], loss: 0.002262, mae: 0.048401, mean_q: 1.173369
 18471/100000: episode: 210, duration: 0.272s, episode steps: 55, steps per second: 203, episode reward: 32.433, mean reward: 0.590 [0.515, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-1.118, 10.247], loss: 0.002186, mae: 0.049624, mean_q: 1.180520
 18522/100000: episode: 211, duration: 0.248s, episode steps: 51, steps per second: 206, episode reward: 29.059, mean reward: 0.570 [0.503, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.764, 10.113], loss: 0.002502, mae: 0.051587, mean_q: 1.172430
 18616/100000: episode: 212, duration: 0.464s, episode steps: 94, steps per second: 203, episode reward: 54.163, mean reward: 0.576 [0.509, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.432, 10.120], loss: 0.002317, mae: 0.049991, mean_q: 1.179719
 18671/100000: episode: 213, duration: 0.277s, episode steps: 55, steps per second: 199, episode reward: 31.579, mean reward: 0.574 [0.510, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.647, 10.100], loss: 0.002402, mae: 0.051166, mean_q: 1.176937
 18722/100000: episode: 214, duration: 0.245s, episode steps: 51, steps per second: 208, episode reward: 30.174, mean reward: 0.592 [0.525, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.157, 10.230], loss: 0.002193, mae: 0.049396, mean_q: 1.174779
 18777/100000: episode: 215, duration: 0.270s, episode steps: 55, steps per second: 203, episode reward: 33.698, mean reward: 0.613 [0.514, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-1.182, 10.155], loss: 0.002322, mae: 0.050613, mean_q: 1.178293
 18828/100000: episode: 216, duration: 0.246s, episode steps: 51, steps per second: 208, episode reward: 30.065, mean reward: 0.590 [0.509, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.178, 10.105], loss: 0.002394, mae: 0.050490, mean_q: 1.174100
 18873/100000: episode: 217, duration: 0.223s, episode steps: 45, steps per second: 202, episode reward: 28.864, mean reward: 0.641 [0.527, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.816, 10.121], loss: 0.002165, mae: 0.048936, mean_q: 1.175072
 18924/100000: episode: 218, duration: 0.257s, episode steps: 51, steps per second: 198, episode reward: 29.920, mean reward: 0.587 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.581, 10.100], loss: 0.001994, mae: 0.048006, mean_q: 1.176085
 18975/100000: episode: 219, duration: 0.278s, episode steps: 51, steps per second: 184, episode reward: 32.378, mean reward: 0.635 [0.499, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.941, 10.337], loss: 0.002178, mae: 0.048957, mean_q: 1.171393
 19020/100000: episode: 220, duration: 0.235s, episode steps: 45, steps per second: 192, episode reward: 28.558, mean reward: 0.635 [0.561, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.742, 10.492], loss: 0.002105, mae: 0.048565, mean_q: 1.178218
 19075/100000: episode: 221, duration: 0.280s, episode steps: 55, steps per second: 196, episode reward: 31.838, mean reward: 0.579 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.203, 10.100], loss: 0.002250, mae: 0.049880, mean_q: 1.174072
 19125/100000: episode: 222, duration: 0.255s, episode steps: 50, steps per second: 196, episode reward: 28.886, mean reward: 0.578 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.670, 10.156], loss: 0.002468, mae: 0.049385, mean_q: 1.178816
 19175/100000: episode: 223, duration: 0.255s, episode steps: 50, steps per second: 196, episode reward: 30.711, mean reward: 0.614 [0.527, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.680, 10.146], loss: 0.002383, mae: 0.050712, mean_q: 1.176280
 19268/100000: episode: 224, duration: 0.453s, episode steps: 93, steps per second: 205, episode reward: 53.804, mean reward: 0.579 [0.498, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.515 [-1.467, 10.100], loss: 0.002298, mae: 0.050106, mean_q: 1.176346
 19319/100000: episode: 225, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 30.322, mean reward: 0.595 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.777, 10.292], loss: 0.002306, mae: 0.050437, mean_q: 1.174716
 19370/100000: episode: 226, duration: 0.275s, episode steps: 51, steps per second: 186, episode reward: 31.492, mean reward: 0.617 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.506, 10.367], loss: 0.002288, mae: 0.050952, mean_q: 1.181773
 19421/100000: episode: 227, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 30.802, mean reward: 0.604 [0.503, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.174, 10.454], loss: 0.002006, mae: 0.048287, mean_q: 1.178437
 19471/100000: episode: 228, duration: 0.252s, episode steps: 50, steps per second: 199, episode reward: 30.532, mean reward: 0.611 [0.520, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.513, 10.100], loss: 0.002470, mae: 0.052871, mean_q: 1.180348
 19522/100000: episode: 229, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 29.307, mean reward: 0.575 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.697, 10.235], loss: 0.002064, mae: 0.047926, mean_q: 1.178884
 19616/100000: episode: 230, duration: 0.464s, episode steps: 94, steps per second: 203, episode reward: 55.533, mean reward: 0.591 [0.510, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.511 [-0.683, 10.221], loss: 0.002312, mae: 0.050011, mean_q: 1.175490
 19664/100000: episode: 231, duration: 0.253s, episode steps: 48, steps per second: 190, episode reward: 31.774, mean reward: 0.662 [0.593, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.461, 10.335], loss: 0.002239, mae: 0.049888, mean_q: 1.181468
 19715/100000: episode: 232, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 29.258, mean reward: 0.574 [0.501, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.492, 10.105], loss: 0.002178, mae: 0.049599, mean_q: 1.178309
 19770/100000: episode: 233, duration: 0.279s, episode steps: 55, steps per second: 197, episode reward: 35.169, mean reward: 0.639 [0.520, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.283, 10.347], loss: 0.002229, mae: 0.050648, mean_q: 1.178142
 19825/100000: episode: 234, duration: 0.297s, episode steps: 55, steps per second: 185, episode reward: 31.974, mean reward: 0.581 [0.509, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.815, 10.100], loss: 0.002240, mae: 0.051028, mean_q: 1.178399
 19876/100000: episode: 235, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 28.975, mean reward: 0.568 [0.503, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.473, 10.142], loss: 0.001871, mae: 0.047047, mean_q: 1.179209
 19927/100000: episode: 236, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 29.111, mean reward: 0.571 [0.511, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.156, 10.100], loss: 0.002239, mae: 0.049808, mean_q: 1.177060
 19975/100000: episode: 237, duration: 0.231s, episode steps: 48, steps per second: 208, episode reward: 31.146, mean reward: 0.649 [0.559, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.953, 10.321], loss: 0.001992, mae: 0.048646, mean_q: 1.185831
 20068/100000: episode: 238, duration: 0.472s, episode steps: 93, steps per second: 197, episode reward: 53.553, mean reward: 0.576 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-0.477, 10.100], loss: 0.003013, mae: 0.053411, mean_q: 1.180735
 20161/100000: episode: 239, duration: 0.467s, episode steps: 93, steps per second: 199, episode reward: 54.982, mean reward: 0.591 [0.510, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-0.958, 10.100], loss: 0.003639, mae: 0.057509, mean_q: 1.181596
[Info] 2-TH LEVEL FOUND: 1.297936201095581, Considering 10/90 traces
 20212/100000: episode: 240, duration: 4.584s, episode steps: 51, steps per second: 11, episode reward: 30.251, mean reward: 0.593 [0.510, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.376, 10.143], loss: 0.002810, mae: 0.054191, mean_q: 1.180037
 20238/100000: episode: 241, duration: 0.162s, episode steps: 26, steps per second: 160, episode reward: 16.291, mean reward: 0.627 [0.556, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.243], loss: 0.002932, mae: 0.054719, mean_q: 1.184285
 20267/100000: episode: 242, duration: 0.172s, episode steps: 29, steps per second: 169, episode reward: 18.735, mean reward: 0.646 [0.568, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.243, 10.352], loss: 0.002899, mae: 0.054470, mean_q: 1.182079
 20301/100000: episode: 243, duration: 0.229s, episode steps: 34, steps per second: 149, episode reward: 21.208, mean reward: 0.624 [0.546, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.293, 10.188], loss: 0.002276, mae: 0.048587, mean_q: 1.181981
 20335/100000: episode: 244, duration: 0.202s, episode steps: 34, steps per second: 169, episode reward: 21.484, mean reward: 0.632 [0.579, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.822, 10.283], loss: 0.002055, mae: 0.047164, mean_q: 1.178162
 20371/100000: episode: 245, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 23.843, mean reward: 0.662 [0.554, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.592, 10.292], loss: 0.002303, mae: 0.049450, mean_q: 1.182299
 20401/100000: episode: 246, duration: 0.191s, episode steps: 30, steps per second: 157, episode reward: 19.296, mean reward: 0.643 [0.580, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.035, 10.281], loss: 0.002118, mae: 0.049723, mean_q: 1.182497
 20435/100000: episode: 247, duration: 0.221s, episode steps: 34, steps per second: 154, episode reward: 21.094, mean reward: 0.620 [0.539, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.136, 10.206], loss: 0.002575, mae: 0.052933, mean_q: 1.179784
 20465/100000: episode: 248, duration: 0.182s, episode steps: 30, steps per second: 165, episode reward: 20.742, mean reward: 0.691 [0.604, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.293, 10.329], loss: 0.002142, mae: 0.048810, mean_q: 1.177431
 20499/100000: episode: 249, duration: 0.232s, episode steps: 34, steps per second: 146, episode reward: 21.048, mean reward: 0.619 [0.523, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.212, 10.106], loss: 0.002366, mae: 0.053424, mean_q: 1.176583
 20535/100000: episode: 250, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 23.454, mean reward: 0.651 [0.554, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.959, 10.238], loss: 0.002053, mae: 0.048791, mean_q: 1.175653
 20567/100000: episode: 251, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 21.494, mean reward: 0.672 [0.610, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.600, 10.347], loss: 0.001980, mae: 0.048226, mean_q: 1.176991
 20599/100000: episode: 252, duration: 0.192s, episode steps: 32, steps per second: 167, episode reward: 20.681, mean reward: 0.646 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.578, 10.160], loss: 0.002344, mae: 0.051674, mean_q: 1.180420
 20625/100000: episode: 253, duration: 0.156s, episode steps: 26, steps per second: 166, episode reward: 16.711, mean reward: 0.643 [0.578, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.453], loss: 0.002047, mae: 0.048765, mean_q: 1.179998
 20654/100000: episode: 254, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 19.973, mean reward: 0.689 [0.606, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.626, 10.347], loss: 0.002015, mae: 0.049356, mean_q: 1.187280
 20688/100000: episode: 255, duration: 0.204s, episode steps: 34, steps per second: 167, episode reward: 22.158, mean reward: 0.652 [0.592, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.392], loss: 0.001974, mae: 0.047882, mean_q: 1.178438
 20718/100000: episode: 256, duration: 0.176s, episode steps: 30, steps per second: 171, episode reward: 18.512, mean reward: 0.617 [0.553, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.519, 10.252], loss: 0.001926, mae: 0.047466, mean_q: 1.187299
 20750/100000: episode: 257, duration: 0.201s, episode steps: 32, steps per second: 159, episode reward: 20.734, mean reward: 0.648 [0.600, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.300, 10.334], loss: 0.002234, mae: 0.050711, mean_q: 1.179347
 20786/100000: episode: 258, duration: 0.216s, episode steps: 36, steps per second: 167, episode reward: 21.950, mean reward: 0.610 [0.514, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.332, 10.100], loss: 0.002024, mae: 0.047874, mean_q: 1.177459
 20820/100000: episode: 259, duration: 0.199s, episode steps: 34, steps per second: 171, episode reward: 23.132, mean reward: 0.680 [0.588, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.608, 10.483], loss: 0.001872, mae: 0.047066, mean_q: 1.184612
 20847/100000: episode: 260, duration: 0.170s, episode steps: 27, steps per second: 159, episode reward: 17.564, mean reward: 0.651 [0.559, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.324], loss: 0.001875, mae: 0.046208, mean_q: 1.178474
 20881/100000: episode: 261, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 22.952, mean reward: 0.675 [0.574, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.516, 10.266], loss: 0.002238, mae: 0.050498, mean_q: 1.179981
 20915/100000: episode: 262, duration: 0.195s, episode steps: 34, steps per second: 174, episode reward: 23.718, mean reward: 0.698 [0.623, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.778, 10.447], loss: 0.001816, mae: 0.046938, mean_q: 1.187338
 20951/100000: episode: 263, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 21.867, mean reward: 0.607 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.428, 10.100], loss: 0.001687, mae: 0.045227, mean_q: 1.186867
 20985/100000: episode: 264, duration: 0.170s, episode steps: 34, steps per second: 200, episode reward: 21.819, mean reward: 0.642 [0.592, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.255, 10.340], loss: 0.001925, mae: 0.046255, mean_q: 1.184051
 21014/100000: episode: 265, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 18.926, mean reward: 0.653 [0.595, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.081, 10.281], loss: 0.002174, mae: 0.051016, mean_q: 1.186736
 21040/100000: episode: 266, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 17.484, mean reward: 0.672 [0.564, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.175, 10.310], loss: 0.002056, mae: 0.048172, mean_q: 1.177472
 21072/100000: episode: 267, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 22.480, mean reward: 0.703 [0.567, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.921, 10.324], loss: 0.001875, mae: 0.046824, mean_q: 1.185329
 21106/100000: episode: 268, duration: 0.170s, episode steps: 34, steps per second: 200, episode reward: 23.507, mean reward: 0.691 [0.622, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.146, 10.456], loss: 0.002220, mae: 0.051454, mean_q: 1.192378
 21136/100000: episode: 269, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 19.420, mean reward: 0.647 [0.527, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.459, 10.199], loss: 0.002104, mae: 0.048238, mean_q: 1.184011
 21163/100000: episode: 270, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 18.623, mean reward: 0.690 [0.622, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.188, 10.465], loss: 0.001682, mae: 0.046786, mean_q: 1.195984
 21189/100000: episode: 271, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 15.615, mean reward: 0.601 [0.506, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.268, 10.100], loss: 0.002093, mae: 0.049564, mean_q: 1.185664
 21230/100000: episode: 272, duration: 0.203s, episode steps: 41, steps per second: 202, episode reward: 25.393, mean reward: 0.619 [0.519, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.321, 10.100], loss: 0.002047, mae: 0.048664, mean_q: 1.185246
 21260/100000: episode: 273, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 18.291, mean reward: 0.610 [0.511, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.076, 10.177], loss: 0.002217, mae: 0.050714, mean_q: 1.180881
 21296/100000: episode: 274, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 24.159, mean reward: 0.671 [0.581, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.686, 10.455], loss: 0.001831, mae: 0.045799, mean_q: 1.187618
 21322/100000: episode: 275, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 17.645, mean reward: 0.679 [0.615, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.863, 10.405], loss: 0.001905, mae: 0.047465, mean_q: 1.194491
 21349/100000: episode: 276, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 17.976, mean reward: 0.666 [0.606, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.624, 10.276], loss: 0.001841, mae: 0.047898, mean_q: 1.197803
 21381/100000: episode: 277, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 21.755, mean reward: 0.680 [0.577, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.052, 10.248], loss: 0.002260, mae: 0.051156, mean_q: 1.197053
 21410/100000: episode: 278, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 20.883, mean reward: 0.720 [0.607, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.459], loss: 0.002200, mae: 0.051410, mean_q: 1.188704
 21444/100000: episode: 279, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 21.275, mean reward: 0.626 [0.527, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.361, 10.372], loss: 0.001813, mae: 0.045710, mean_q: 1.192958
 21480/100000: episode: 280, duration: 0.188s, episode steps: 36, steps per second: 192, episode reward: 25.882, mean reward: 0.719 [0.649, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.035, 10.399], loss: 0.001917, mae: 0.047968, mean_q: 1.194902
 21509/100000: episode: 281, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 20.014, mean reward: 0.690 [0.620, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.208, 10.409], loss: 0.001984, mae: 0.048190, mean_q: 1.199208
 21538/100000: episode: 282, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 20.954, mean reward: 0.723 [0.623, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.547, 10.464], loss: 0.002521, mae: 0.054402, mean_q: 1.192345
 21572/100000: episode: 283, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 23.223, mean reward: 0.683 [0.612, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.364, 10.385], loss: 0.002533, mae: 0.054075, mean_q: 1.196762
 21613/100000: episode: 284, duration: 0.208s, episode steps: 41, steps per second: 198, episode reward: 25.005, mean reward: 0.610 [0.509, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-1.500, 10.100], loss: 0.002218, mae: 0.050639, mean_q: 1.199624
 21639/100000: episode: 285, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 16.475, mean reward: 0.634 [0.567, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.403], loss: 0.002130, mae: 0.050674, mean_q: 1.192101
 21669/100000: episode: 286, duration: 0.174s, episode steps: 30, steps per second: 172, episode reward: 19.475, mean reward: 0.649 [0.568, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.230], loss: 0.001969, mae: 0.047818, mean_q: 1.201990
 21696/100000: episode: 287, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 19.609, mean reward: 0.726 [0.645, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.224, 10.418], loss: 0.002032, mae: 0.048722, mean_q: 1.195109
 21737/100000: episode: 288, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 24.776, mean reward: 0.604 [0.515, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.456, 10.173], loss: 0.002048, mae: 0.048839, mean_q: 1.198356
 21764/100000: episode: 289, duration: 0.129s, episode steps: 27, steps per second: 210, episode reward: 17.943, mean reward: 0.665 [0.627, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.181, 10.397], loss: 0.001675, mae: 0.045085, mean_q: 1.201653
 21798/100000: episode: 290, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 22.454, mean reward: 0.660 [0.542, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.344, 10.250], loss: 0.001839, mae: 0.046893, mean_q: 1.191367
 21839/100000: episode: 291, duration: 0.202s, episode steps: 41, steps per second: 203, episode reward: 27.911, mean reward: 0.681 [0.610, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.118, 10.361], loss: 0.001819, mae: 0.045494, mean_q: 1.201931
 21873/100000: episode: 292, duration: 0.164s, episode steps: 34, steps per second: 207, episode reward: 22.943, mean reward: 0.675 [0.607, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.503, 10.434], loss: 0.001883, mae: 0.047658, mean_q: 1.199914
 21907/100000: episode: 293, duration: 0.162s, episode steps: 34, steps per second: 210, episode reward: 23.665, mean reward: 0.696 [0.601, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.197, 10.379], loss: 0.001989, mae: 0.048416, mean_q: 1.205002
 21936/100000: episode: 294, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 20.255, mean reward: 0.698 [0.613, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.723, 10.539], loss: 0.001957, mae: 0.048608, mean_q: 1.194714
 21962/100000: episode: 295, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 17.503, mean reward: 0.673 [0.611, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.385, 10.348], loss: 0.002128, mae: 0.051161, mean_q: 1.205320
 21996/100000: episode: 296, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 22.860, mean reward: 0.672 [0.578, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-1.068, 10.412], loss: 0.002261, mae: 0.050422, mean_q: 1.206393
 22022/100000: episode: 297, duration: 0.125s, episode steps: 26, steps per second: 207, episode reward: 15.549, mean reward: 0.598 [0.536, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.443, 10.190], loss: 0.001768, mae: 0.046944, mean_q: 1.201357
 22049/100000: episode: 298, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 18.486, mean reward: 0.685 [0.636, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.164, 10.447], loss: 0.002148, mae: 0.049469, mean_q: 1.200295
 22085/100000: episode: 299, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 21.304, mean reward: 0.592 [0.526, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.182, 10.183], loss: 0.001904, mae: 0.046842, mean_q: 1.206999
 22112/100000: episode: 300, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 16.626, mean reward: 0.616 [0.515, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.192, 10.211], loss: 0.001840, mae: 0.046686, mean_q: 1.211872
 22141/100000: episode: 301, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 18.203, mean reward: 0.628 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.114, 10.100], loss: 0.001787, mae: 0.045680, mean_q: 1.205903
 22171/100000: episode: 302, duration: 0.163s, episode steps: 30, steps per second: 185, episode reward: 19.643, mean reward: 0.655 [0.521, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.674, 10.189], loss: 0.001737, mae: 0.046295, mean_q: 1.208236
 22201/100000: episode: 303, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 21.101, mean reward: 0.703 [0.605, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.100, 10.401], loss: 0.001858, mae: 0.047613, mean_q: 1.207466
 22242/100000: episode: 304, duration: 0.196s, episode steps: 41, steps per second: 209, episode reward: 30.915, mean reward: 0.754 [0.643, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.509, 10.507], loss: 0.002360, mae: 0.051031, mean_q: 1.213191
 22268/100000: episode: 305, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 17.448, mean reward: 0.671 [0.603, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.452, 10.468], loss: 0.001894, mae: 0.047618, mean_q: 1.210316
 22294/100000: episode: 306, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 16.262, mean reward: 0.625 [0.565, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.052, 10.255], loss: 0.002055, mae: 0.049310, mean_q: 1.217307
 22328/100000: episode: 307, duration: 0.166s, episode steps: 34, steps per second: 205, episode reward: 25.153, mean reward: 0.740 [0.652, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.977, 10.470], loss: 0.002204, mae: 0.051818, mean_q: 1.209485
 22355/100000: episode: 308, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 18.236, mean reward: 0.675 [0.591, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.442, 10.347], loss: 0.002121, mae: 0.049171, mean_q: 1.212438
 22381/100000: episode: 309, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 15.773, mean reward: 0.607 [0.528, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.630, 10.133], loss: 0.001759, mae: 0.045717, mean_q: 1.216744
 22410/100000: episode: 310, duration: 0.141s, episode steps: 29, steps per second: 205, episode reward: 19.289, mean reward: 0.665 [0.583, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.693, 10.380], loss: 0.001717, mae: 0.044992, mean_q: 1.214535
 22442/100000: episode: 311, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 24.774, mean reward: 0.774 [0.666, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.203, 10.510], loss: 0.001958, mae: 0.048467, mean_q: 1.208680
 22476/100000: episode: 312, duration: 0.182s, episode steps: 34, steps per second: 186, episode reward: 22.278, mean reward: 0.655 [0.591, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.366, 10.321], loss: 0.001728, mae: 0.045043, mean_q: 1.216018
 22503/100000: episode: 313, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 20.188, mean reward: 0.748 [0.644, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.180, 10.667], loss: 0.001796, mae: 0.046988, mean_q: 1.207886
 22529/100000: episode: 314, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 16.275, mean reward: 0.626 [0.522, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.805, 10.165], loss: 0.001577, mae: 0.043674, mean_q: 1.219682
 22559/100000: episode: 315, duration: 0.161s, episode steps: 30, steps per second: 187, episode reward: 19.134, mean reward: 0.638 [0.572, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.713, 10.359], loss: 0.001981, mae: 0.048633, mean_q: 1.211869
 22593/100000: episode: 316, duration: 0.167s, episode steps: 34, steps per second: 204, episode reward: 22.210, mean reward: 0.653 [0.562, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.330, 10.222], loss: 0.001891, mae: 0.046607, mean_q: 1.218009
 22623/100000: episode: 317, duration: 0.144s, episode steps: 30, steps per second: 208, episode reward: 19.196, mean reward: 0.640 [0.555, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.429, 10.396], loss: 0.002057, mae: 0.049842, mean_q: 1.214692
 22659/100000: episode: 318, duration: 0.188s, episode steps: 36, steps per second: 192, episode reward: 23.457, mean reward: 0.652 [0.590, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.789, 10.426], loss: 0.001595, mae: 0.042909, mean_q: 1.229948
 22700/100000: episode: 319, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 27.955, mean reward: 0.682 [0.611, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.307, 10.414], loss: 0.002071, mae: 0.048028, mean_q: 1.222685
 22727/100000: episode: 320, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 20.287, mean reward: 0.751 [0.643, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.156, 10.428], loss: 0.001829, mae: 0.045002, mean_q: 1.216936
 22759/100000: episode: 321, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 21.070, mean reward: 0.658 [0.545, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.443, 10.269], loss: 0.002105, mae: 0.049490, mean_q: 1.227297
 22793/100000: episode: 322, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 22.411, mean reward: 0.659 [0.543, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.213, 10.242], loss: 0.001988, mae: 0.047052, mean_q: 1.228505
 22827/100000: episode: 323, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 23.949, mean reward: 0.704 [0.619, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.876, 10.495], loss: 0.002273, mae: 0.051769, mean_q: 1.235431
 22861/100000: episode: 324, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 21.633, mean reward: 0.636 [0.530, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.910, 10.183], loss: 0.001990, mae: 0.047417, mean_q: 1.226774
 22893/100000: episode: 325, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 23.267, mean reward: 0.727 [0.673, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.071, 10.640], loss: 0.002060, mae: 0.049067, mean_q: 1.230784
 22927/100000: episode: 326, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 23.516, mean reward: 0.692 [0.569, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.249], loss: 0.001868, mae: 0.047890, mean_q: 1.232899
 22957/100000: episode: 327, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 19.764, mean reward: 0.659 [0.620, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.452], loss: 0.002431, mae: 0.052198, mean_q: 1.229493
 22984/100000: episode: 328, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 18.828, mean reward: 0.697 [0.640, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.458], loss: 0.001870, mae: 0.046220, mean_q: 1.234779
 23011/100000: episode: 329, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 18.155, mean reward: 0.672 [0.613, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.205, 10.325], loss: 0.001351, mae: 0.040693, mean_q: 1.236980
[Info] 3-TH LEVEL FOUND: 1.4387925863265991, Considering 10/90 traces
 23047/100000: episode: 330, duration: 4.337s, episode steps: 36, steps per second: 8, episode reward: 22.882, mean reward: 0.636 [0.516, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.428, 10.147], loss: 0.001728, mae: 0.044684, mean_q: 1.233399
 23067/100000: episode: 331, duration: 0.099s, episode steps: 20, steps per second: 203, episode reward: 16.053, mean reward: 0.803 [0.741, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.696], loss: 0.001793, mae: 0.046522, mean_q: 1.224781
 23082/100000: episode: 332, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 11.251, mean reward: 0.750 [0.669, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.407], loss: 0.001923, mae: 0.045695, mean_q: 1.223516
 23102/100000: episode: 333, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: 15.660, mean reward: 0.783 [0.641, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.606, 10.517], loss: 0.001983, mae: 0.047917, mean_q: 1.238699
 23128/100000: episode: 334, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 19.290, mean reward: 0.742 [0.648, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.392], loss: 0.001788, mae: 0.045924, mean_q: 1.245492
 23148/100000: episode: 335, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 15.841, mean reward: 0.792 [0.651, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.527], loss: 0.001766, mae: 0.044595, mean_q: 1.236646
[Info] FALSIFICATION!
 23176/100000: episode: 336, duration: 0.532s, episode steps: 28, steps per second: 53, episode reward: 22.403, mean reward: 0.800 [0.703, 1.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.474, 10.422], loss: 0.002021, mae: 0.048216, mean_q: 1.246541
 23191/100000: episode: 337, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 11.829, mean reward: 0.789 [0.704, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.607], loss: 0.001868, mae: 0.047023, mean_q: 1.254739
 23213/100000: episode: 338, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 17.903, mean reward: 0.814 [0.688, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.743, 10.518], loss: 0.001651, mae: 0.043135, mean_q: 1.240904
 23239/100000: episode: 339, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 20.387, mean reward: 0.784 [0.676, 0.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.016, 10.537], loss: 0.002600, mae: 0.054062, mean_q: 1.235709
 23269/100000: episode: 340, duration: 0.151s, episode steps: 30, steps per second: 198, episode reward: 21.634, mean reward: 0.721 [0.594, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.035, 10.350], loss: 0.001920, mae: 0.047939, mean_q: 1.256517
 23299/100000: episode: 341, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 22.022, mean reward: 0.734 [0.588, 0.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.743, 10.283], loss: 0.001553, mae: 0.042814, mean_q: 1.239300
 23325/100000: episode: 342, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 18.819, mean reward: 0.724 [0.658, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.384, 10.360], loss: 0.001836, mae: 0.046619, mean_q: 1.244790
 23354/100000: episode: 343, duration: 0.141s, episode steps: 29, steps per second: 206, episode reward: 20.381, mean reward: 0.703 [0.641, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.459, 10.461], loss: 0.002298, mae: 0.050740, mean_q: 1.250967
 23376/100000: episode: 344, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 18.662, mean reward: 0.848 [0.736, 0.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.703], loss: 0.002318, mae: 0.050974, mean_q: 1.260081
 23391/100000: episode: 345, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 10.023, mean reward: 0.668 [0.608, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.338, 10.326], loss: 0.001848, mae: 0.048456, mean_q: 1.266636
 23416/100000: episode: 346, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 16.408, mean reward: 0.656 [0.572, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.398], loss: 0.001965, mae: 0.046931, mean_q: 1.246617
 23441/100000: episode: 347, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 16.176, mean reward: 0.647 [0.598, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.308], loss: 0.001563, mae: 0.041946, mean_q: 1.257807
 23456/100000: episode: 348, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 10.980, mean reward: 0.732 [0.669, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.547], loss: 0.002274, mae: 0.051413, mean_q: 1.273315
 23479/100000: episode: 349, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 18.068, mean reward: 0.786 [0.691, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.307, 10.532], loss: 0.002683, mae: 0.054895, mean_q: 1.271210
 23508/100000: episode: 350, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 25.162, mean reward: 0.868 [0.717, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.725, 10.520], loss: 0.002046, mae: 0.047602, mean_q: 1.260068
 23534/100000: episode: 351, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 19.778, mean reward: 0.761 [0.616, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.351, 10.421], loss: 0.001865, mae: 0.046199, mean_q: 1.259616
 23559/100000: episode: 352, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 16.111, mean reward: 0.644 [0.569, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.957, 10.312], loss: 0.001625, mae: 0.043016, mean_q: 1.270631
 23588/100000: episode: 353, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 21.270, mean reward: 0.733 [0.664, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.949, 10.475], loss: 0.002194, mae: 0.051780, mean_q: 1.266839
 23611/100000: episode: 354, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 15.774, mean reward: 0.686 [0.641, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-1.236, 10.409], loss: 0.001883, mae: 0.046864, mean_q: 1.272498
 23631/100000: episode: 355, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 16.677, mean reward: 0.834 [0.742, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.580, 10.592], loss: 0.001656, mae: 0.043554, mean_q: 1.271866
 23651/100000: episode: 356, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 14.450, mean reward: 0.722 [0.648, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.409], loss: 0.001709, mae: 0.044195, mean_q: 1.283652
 23673/100000: episode: 357, duration: 0.114s, episode steps: 22, steps per second: 192, episode reward: 16.940, mean reward: 0.770 [0.718, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.488], loss: 0.001715, mae: 0.045317, mean_q: 1.282453
 23693/100000: episode: 358, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 12.866, mean reward: 0.643 [0.537, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.243], loss: 0.002146, mae: 0.049611, mean_q: 1.248386
 23716/100000: episode: 359, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 16.649, mean reward: 0.724 [0.663, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.446], loss: 0.001690, mae: 0.045278, mean_q: 1.274748
 23736/100000: episode: 360, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 13.720, mean reward: 0.686 [0.578, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.556, 10.335], loss: 0.002062, mae: 0.050017, mean_q: 1.262808
 23751/100000: episode: 361, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 11.291, mean reward: 0.753 [0.698, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.512], loss: 0.001899, mae: 0.047746, mean_q: 1.275890
 23771/100000: episode: 362, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 13.844, mean reward: 0.692 [0.636, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.515], loss: 0.001687, mae: 0.045612, mean_q: 1.264621
 23800/100000: episode: 363, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 24.602, mean reward: 0.848 [0.703, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.500, 10.523], loss: 0.002139, mae: 0.050851, mean_q: 1.273735
 23823/100000: episode: 364, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 15.742, mean reward: 0.684 [0.610, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.488], loss: 0.001817, mae: 0.045687, mean_q: 1.275356
 23838/100000: episode: 365, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 12.059, mean reward: 0.804 [0.701, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.452, 10.378], loss: 0.001590, mae: 0.043495, mean_q: 1.271596
 23858/100000: episode: 366, duration: 0.122s, episode steps: 20, steps per second: 163, episode reward: 14.290, mean reward: 0.715 [0.623, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.332], loss: 0.001790, mae: 0.046454, mean_q: 1.281106
 23887/100000: episode: 367, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 21.177, mean reward: 0.730 [0.674, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.249, 10.536], loss: 0.001869, mae: 0.047094, mean_q: 1.277689
 23913/100000: episode: 368, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 18.306, mean reward: 0.704 [0.637, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.404], loss: 0.001909, mae: 0.047230, mean_q: 1.273694
 23933/100000: episode: 369, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 15.420, mean reward: 0.771 [0.708, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.055, 10.421], loss: 0.001611, mae: 0.044046, mean_q: 1.276604
 23962/100000: episode: 370, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 23.563, mean reward: 0.813 [0.713, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.557, 10.613], loss: 0.001807, mae: 0.046331, mean_q: 1.285807
 23982/100000: episode: 371, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 14.577, mean reward: 0.729 [0.667, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.075, 10.537], loss: 0.001831, mae: 0.045645, mean_q: 1.289343
 24002/100000: episode: 372, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 13.827, mean reward: 0.691 [0.618, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.065, 10.364], loss: 0.001865, mae: 0.046142, mean_q: 1.266665
 24022/100000: episode: 373, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 13.984, mean reward: 0.699 [0.597, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.299, 10.277], loss: 0.002016, mae: 0.048312, mean_q: 1.302282
 24044/100000: episode: 374, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 16.486, mean reward: 0.749 [0.642, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.803, 10.385], loss: 0.001758, mae: 0.045486, mean_q: 1.294627
 24074/100000: episode: 375, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 19.962, mean reward: 0.665 [0.542, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.406], loss: 0.001669, mae: 0.044799, mean_q: 1.297966
 24094/100000: episode: 376, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 14.132, mean reward: 0.707 [0.615, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.358, 10.397], loss: 0.001867, mae: 0.045977, mean_q: 1.298247
 24109/100000: episode: 377, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 10.573, mean reward: 0.705 [0.654, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.354, 10.473], loss: 0.001847, mae: 0.046493, mean_q: 1.302766
 24134/100000: episode: 378, duration: 0.120s, episode steps: 25, steps per second: 209, episode reward: 18.078, mean reward: 0.723 [0.646, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.369, 10.495], loss: 0.001675, mae: 0.044333, mean_q: 1.291083
 24154/100000: episode: 379, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 14.922, mean reward: 0.746 [0.640, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.373], loss: 0.001767, mae: 0.044961, mean_q: 1.284645
 24169/100000: episode: 380, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 11.512, mean reward: 0.767 [0.684, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.112, 10.427], loss: 0.001916, mae: 0.047454, mean_q: 1.283375
 24189/100000: episode: 381, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 14.402, mean reward: 0.720 [0.665, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.427], loss: 0.001748, mae: 0.044833, mean_q: 1.290501
 24212/100000: episode: 382, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 16.713, mean reward: 0.727 [0.645, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.224, 10.462], loss: 0.002008, mae: 0.049135, mean_q: 1.295790
 24235/100000: episode: 383, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 16.950, mean reward: 0.737 [0.674, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.194, 10.367], loss: 0.002224, mae: 0.051979, mean_q: 1.290286
 24255/100000: episode: 384, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 14.461, mean reward: 0.723 [0.615, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.573, 10.464], loss: 0.002125, mae: 0.049819, mean_q: 1.293699
 24275/100000: episode: 385, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 13.249, mean reward: 0.662 [0.601, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.342], loss: 0.002142, mae: 0.051076, mean_q: 1.310261
 24301/100000: episode: 386, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 19.139, mean reward: 0.736 [0.644, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.338], loss: 0.001677, mae: 0.045163, mean_q: 1.301870
 24324/100000: episode: 387, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 16.657, mean reward: 0.724 [0.656, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.059, 10.545], loss: 0.001909, mae: 0.046143, mean_q: 1.297924
 24350/100000: episode: 388, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 18.880, mean reward: 0.726 [0.667, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.171, 10.442], loss: 0.001647, mae: 0.043751, mean_q: 1.303376
 24379/100000: episode: 389, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 23.515, mean reward: 0.811 [0.720, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.169, 10.469], loss: 0.001800, mae: 0.045648, mean_q: 1.292687
 24409/100000: episode: 390, duration: 0.176s, episode steps: 30, steps per second: 171, episode reward: 20.424, mean reward: 0.681 [0.550, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.014, 10.323], loss: 0.001727, mae: 0.045385, mean_q: 1.301464
 24434/100000: episode: 391, duration: 0.154s, episode steps: 25, steps per second: 163, episode reward: 18.286, mean reward: 0.731 [0.654, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.241, 10.383], loss: 0.001902, mae: 0.046806, mean_q: 1.298305
[Info] FALSIFICATION!
 24444/100000: episode: 392, duration: 0.229s, episode steps: 10, steps per second: 44, episode reward: 8.728, mean reward: 0.873 [0.730, 1.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.240, 10.137], loss: 0.002039, mae: 0.049553, mean_q: 1.302803
 24464/100000: episode: 393, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 14.017, mean reward: 0.701 [0.628, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.089, 10.355], loss: 0.001729, mae: 0.046025, mean_q: 1.308215
 24490/100000: episode: 394, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 21.400, mean reward: 0.823 [0.727, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.620, 10.525], loss: 0.001696, mae: 0.044774, mean_q: 1.314689
 24510/100000: episode: 395, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 14.254, mean reward: 0.713 [0.595, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.406, 10.382], loss: 0.001969, mae: 0.047256, mean_q: 1.306753
 24535/100000: episode: 396, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 19.761, mean reward: 0.790 [0.652, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.552], loss: 0.001520, mae: 0.042652, mean_q: 1.327545
 24561/100000: episode: 397, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 20.453, mean reward: 0.787 [0.724, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.411, 10.564], loss: 0.002091, mae: 0.047583, mean_q: 1.311104
 24581/100000: episode: 398, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 14.929, mean reward: 0.746 [0.669, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.202, 10.517], loss: 0.002108, mae: 0.047471, mean_q: 1.299889
 24611/100000: episode: 399, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 20.204, mean reward: 0.673 [0.587, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.311], loss: 0.001912, mae: 0.046596, mean_q: 1.312910
 24631/100000: episode: 400, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 15.408, mean reward: 0.770 [0.717, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.525], loss: 0.001811, mae: 0.045521, mean_q: 1.304664
 24660/100000: episode: 401, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 22.623, mean reward: 0.780 [0.661, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.130, 10.502], loss: 0.001950, mae: 0.045147, mean_q: 1.319910
 24675/100000: episode: 402, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 11.260, mean reward: 0.751 [0.673, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.135, 10.428], loss: 0.001529, mae: 0.040893, mean_q: 1.328122
 24697/100000: episode: 403, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 15.933, mean reward: 0.724 [0.669, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.404], loss: 0.001758, mae: 0.043291, mean_q: 1.315734
 24717/100000: episode: 404, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 15.923, mean reward: 0.796 [0.711, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.099, 10.596], loss: 0.001851, mae: 0.046242, mean_q: 1.310342
 24739/100000: episode: 405, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 15.075, mean reward: 0.685 [0.627, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.316, 10.331], loss: 0.001428, mae: 0.041468, mean_q: 1.326443
 24768/100000: episode: 406, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 23.258, mean reward: 0.802 [0.754, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.854, 10.523], loss: 0.001813, mae: 0.045880, mean_q: 1.315035
 24790/100000: episode: 407, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 17.908, mean reward: 0.814 [0.695, 0.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.314, 10.613], loss: 0.001671, mae: 0.044466, mean_q: 1.325044
 24813/100000: episode: 408, duration: 0.119s, episode steps: 23, steps per second: 192, episode reward: 16.123, mean reward: 0.701 [0.642, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.440], loss: 0.001738, mae: 0.045497, mean_q: 1.335536
 24843/100000: episode: 409, duration: 0.177s, episode steps: 30, steps per second: 170, episode reward: 23.129, mean reward: 0.771 [0.723, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.623, 10.479], loss: 0.001801, mae: 0.045522, mean_q: 1.319297
 24858/100000: episode: 410, duration: 0.101s, episode steps: 15, steps per second: 148, episode reward: 11.036, mean reward: 0.736 [0.677, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-1.028, 10.515], loss: 0.002757, mae: 0.055931, mean_q: 1.319772
 24873/100000: episode: 411, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 10.317, mean reward: 0.688 [0.603, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.156, 10.377], loss: 0.001802, mae: 0.045972, mean_q: 1.335421
 24899/100000: episode: 412, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 18.714, mean reward: 0.720 [0.635, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.190, 10.417], loss: 0.001644, mae: 0.043951, mean_q: 1.322575
 24924/100000: episode: 413, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 17.629, mean reward: 0.705 [0.616, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.291], loss: 0.001629, mae: 0.043966, mean_q: 1.336496
 24946/100000: episode: 414, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 16.329, mean reward: 0.742 [0.670, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.134, 10.599], loss: 0.001795, mae: 0.047224, mean_q: 1.344388
 24976/100000: episode: 415, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 20.284, mean reward: 0.676 [0.598, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.162, 10.414], loss: 0.001579, mae: 0.043179, mean_q: 1.335737
 24996/100000: episode: 416, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 14.300, mean reward: 0.715 [0.552, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.967, 10.389], loss: 0.001643, mae: 0.043744, mean_q: 1.334233
 25018/100000: episode: 417, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 16.813, mean reward: 0.764 [0.692, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.529], loss: 0.001835, mae: 0.046831, mean_q: 1.337017
 25047/100000: episode: 418, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 23.560, mean reward: 0.812 [0.714, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.449, 10.684], loss: 0.002893, mae: 0.057444, mean_q: 1.339031
 25072/100000: episode: 419, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 17.055, mean reward: 0.682 [0.600, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.326], loss: 0.002518, mae: 0.051345, mean_q: 1.338956
[Info] Complete ISplit Iteration
[Info] Levels: [1.208838, 1.2979362, 1.4387926, 1.5453699]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.24]
[Info] Error Prob: 0.00024000000000000006

 25094/100000: episode: 420, duration: 4.416s, episode steps: 22, steps per second: 5, episode reward: 17.046, mean reward: 0.775 [0.691, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.088, 10.499], loss: 0.001684, mae: 0.044131, mean_q: 1.342575
 25194/100000: episode: 421, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.383, mean reward: 0.584 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.604, 10.176], loss: 0.001780, mae: 0.044743, mean_q: 1.339655
 25294/100000: episode: 422, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 57.091, mean reward: 0.571 [0.502, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.819, 10.223], loss: 0.002217, mae: 0.050005, mean_q: 1.335765
 25394/100000: episode: 423, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.522, mean reward: 0.585 [0.513, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.195, 10.098], loss: 0.001890, mae: 0.046913, mean_q: 1.331532
 25494/100000: episode: 424, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.778, mean reward: 0.608 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.002, 10.098], loss: 0.002159, mae: 0.049979, mean_q: 1.333307
 25594/100000: episode: 425, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.285, mean reward: 0.583 [0.500, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.832, 10.256], loss: 0.001915, mae: 0.046844, mean_q: 1.332480
 25694/100000: episode: 426, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 64.495, mean reward: 0.645 [0.504, 0.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.900, 10.593], loss: 0.002120, mae: 0.048687, mean_q: 1.332746
 25794/100000: episode: 427, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.336, mean reward: 0.593 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.403, 10.170], loss: 0.002179, mae: 0.050259, mean_q: 1.321151
 25894/100000: episode: 428, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.568, mean reward: 0.576 [0.503, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.610, 10.098], loss: 0.002034, mae: 0.048246, mean_q: 1.328398
 25994/100000: episode: 429, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.089, mean reward: 0.581 [0.497, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.148, 10.098], loss: 0.001759, mae: 0.045394, mean_q: 1.328768
 26094/100000: episode: 430, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.548, mean reward: 0.575 [0.497, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.111, 10.195], loss: 0.001751, mae: 0.045813, mean_q: 1.318936
 26194/100000: episode: 431, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 56.396, mean reward: 0.564 [0.501, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.458, 10.098], loss: 0.002049, mae: 0.048551, mean_q: 1.321817
 26294/100000: episode: 432, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 58.614, mean reward: 0.586 [0.505, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.833, 10.147], loss: 0.001757, mae: 0.045209, mean_q: 1.317049
 26394/100000: episode: 433, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.646, mean reward: 0.586 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.807, 10.141], loss: 0.001949, mae: 0.047180, mean_q: 1.312253
 26494/100000: episode: 434, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.353, mean reward: 0.584 [0.500, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.689, 10.098], loss: 0.001956, mae: 0.047133, mean_q: 1.307455
 26594/100000: episode: 435, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 62.168, mean reward: 0.622 [0.513, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.351, 10.098], loss: 0.001754, mae: 0.045397, mean_q: 1.306549
 26694/100000: episode: 436, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 60.905, mean reward: 0.609 [0.498, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.670, 10.338], loss: 0.001805, mae: 0.046300, mean_q: 1.306354
 26794/100000: episode: 437, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.191, mean reward: 0.592 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.830, 10.128], loss: 0.001896, mae: 0.047164, mean_q: 1.302877
 26894/100000: episode: 438, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.183, mean reward: 0.572 [0.500, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.875, 10.185], loss: 0.002169, mae: 0.050360, mean_q: 1.298829
 26994/100000: episode: 439, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.844, mean reward: 0.578 [0.501, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.203, 10.312], loss: 0.002137, mae: 0.049010, mean_q: 1.301528
 27094/100000: episode: 440, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 57.792, mean reward: 0.578 [0.500, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.225, 10.098], loss: 0.002319, mae: 0.050942, mean_q: 1.299795
 27194/100000: episode: 441, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.865, mean reward: 0.599 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.477, 10.098], loss: 0.001852, mae: 0.047069, mean_q: 1.296829
 27294/100000: episode: 442, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.889, mean reward: 0.599 [0.504, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.647, 10.098], loss: 0.002060, mae: 0.049535, mean_q: 1.287959
 27394/100000: episode: 443, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 57.833, mean reward: 0.578 [0.500, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.710, 10.098], loss: 0.001974, mae: 0.048002, mean_q: 1.290326
 27494/100000: episode: 444, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 60.713, mean reward: 0.607 [0.500, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.195, 10.437], loss: 0.001960, mae: 0.047990, mean_q: 1.280410
 27594/100000: episode: 445, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.676, mean reward: 0.607 [0.513, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.636, 10.098], loss: 0.001883, mae: 0.047307, mean_q: 1.288949
 27694/100000: episode: 446, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 56.074, mean reward: 0.561 [0.501, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.376, 10.101], loss: 0.001876, mae: 0.046382, mean_q: 1.285630
 27794/100000: episode: 447, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.880, mean reward: 0.589 [0.510, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.849, 10.226], loss: 0.002100, mae: 0.049088, mean_q: 1.270289
 27894/100000: episode: 448, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.220, mean reward: 0.592 [0.508, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-3.216, 10.098], loss: 0.001972, mae: 0.047604, mean_q: 1.277454
 27994/100000: episode: 449, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 67.761, mean reward: 0.678 [0.512, 0.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.406 [-1.350, 10.098], loss: 0.001750, mae: 0.046128, mean_q: 1.269175
 28094/100000: episode: 450, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 61.558, mean reward: 0.616 [0.504, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.266, 10.295], loss: 0.002378, mae: 0.052164, mean_q: 1.266440
 28194/100000: episode: 451, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.536, mean reward: 0.575 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.900, 10.183], loss: 0.002265, mae: 0.050818, mean_q: 1.264274
 28294/100000: episode: 452, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.347, mean reward: 0.583 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.250, 10.098], loss: 0.001953, mae: 0.047794, mean_q: 1.254946
 28394/100000: episode: 453, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 57.691, mean reward: 0.577 [0.507, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.493, 10.262], loss: 0.002030, mae: 0.047105, mean_q: 1.253811
 28494/100000: episode: 454, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 57.668, mean reward: 0.577 [0.498, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.727, 10.205], loss: 0.001959, mae: 0.047527, mean_q: 1.247423
 28594/100000: episode: 455, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.982, mean reward: 0.580 [0.504, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.217, 10.098], loss: 0.002050, mae: 0.048147, mean_q: 1.244932
 28694/100000: episode: 456, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 57.796, mean reward: 0.578 [0.510, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.449, 10.194], loss: 0.002237, mae: 0.050403, mean_q: 1.233139
 28794/100000: episode: 457, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.565, mean reward: 0.576 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.316, 10.125], loss: 0.001923, mae: 0.047060, mean_q: 1.231441
 28894/100000: episode: 458, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 57.668, mean reward: 0.577 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.177, 10.098], loss: 0.001833, mae: 0.046668, mean_q: 1.223950
 28994/100000: episode: 459, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 62.191, mean reward: 0.622 [0.508, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.075, 10.098], loss: 0.001903, mae: 0.046762, mean_q: 1.217248
 29094/100000: episode: 460, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 58.876, mean reward: 0.589 [0.517, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.061, 10.216], loss: 0.002117, mae: 0.048821, mean_q: 1.218235
 29194/100000: episode: 461, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 58.480, mean reward: 0.585 [0.508, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.631, 10.098], loss: 0.001945, mae: 0.046107, mean_q: 1.215670
 29294/100000: episode: 462, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.218, mean reward: 0.592 [0.507, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.372, 10.252], loss: 0.002061, mae: 0.047508, mean_q: 1.206734
 29394/100000: episode: 463, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.316, mean reward: 0.593 [0.500, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.103, 10.098], loss: 0.001855, mae: 0.045705, mean_q: 1.200837
 29494/100000: episode: 464, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.799, mean reward: 0.608 [0.508, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.692, 10.463], loss: 0.001871, mae: 0.046290, mean_q: 1.194537
 29594/100000: episode: 465, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 58.624, mean reward: 0.586 [0.510, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.584, 10.098], loss: 0.001749, mae: 0.044795, mean_q: 1.191217
 29694/100000: episode: 466, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.396, mean reward: 0.584 [0.501, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.778, 10.194], loss: 0.001808, mae: 0.045087, mean_q: 1.185917
 29794/100000: episode: 467, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.402, mean reward: 0.584 [0.505, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.769, 10.098], loss: 0.001864, mae: 0.046020, mean_q: 1.185073
 29894/100000: episode: 468, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 60.017, mean reward: 0.600 [0.498, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.056, 10.244], loss: 0.001957, mae: 0.046643, mean_q: 1.176664
 29994/100000: episode: 469, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 64.312, mean reward: 0.643 [0.529, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.712, 10.278], loss: 0.001944, mae: 0.047497, mean_q: 1.171003
 30094/100000: episode: 470, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 56.500, mean reward: 0.565 [0.498, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.218, 10.098], loss: 0.001858, mae: 0.046846, mean_q: 1.168695
 30194/100000: episode: 471, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.171, mean reward: 0.582 [0.501, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.911, 10.321], loss: 0.001741, mae: 0.045432, mean_q: 1.167235
 30294/100000: episode: 472, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 57.196, mean reward: 0.572 [0.508, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.657, 10.098], loss: 0.001736, mae: 0.045069, mean_q: 1.167877
 30394/100000: episode: 473, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.947, mean reward: 0.599 [0.510, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.773, 10.098], loss: 0.001982, mae: 0.048069, mean_q: 1.166450
 30494/100000: episode: 474, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.834, mean reward: 0.578 [0.509, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.045, 10.204], loss: 0.001830, mae: 0.046721, mean_q: 1.165344
 30594/100000: episode: 475, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 60.842, mean reward: 0.608 [0.502, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.374, 10.098], loss: 0.002054, mae: 0.048536, mean_q: 1.164901
 30694/100000: episode: 476, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.288, mean reward: 0.573 [0.499, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.331, 10.098], loss: 0.001740, mae: 0.044721, mean_q: 1.165724
 30794/100000: episode: 477, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 60.503, mean reward: 0.605 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.256, 10.328], loss: 0.001738, mae: 0.045019, mean_q: 1.165306
 30894/100000: episode: 478, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 60.498, mean reward: 0.605 [0.505, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.072, 10.311], loss: 0.001860, mae: 0.045928, mean_q: 1.167542
 30994/100000: episode: 479, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.526, mean reward: 0.585 [0.509, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.527, 10.308], loss: 0.001742, mae: 0.045991, mean_q: 1.168761
 31094/100000: episode: 480, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 62.714, mean reward: 0.627 [0.507, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.624, 10.098], loss: 0.001694, mae: 0.045260, mean_q: 1.166946
 31194/100000: episode: 481, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.690, mean reward: 0.577 [0.504, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.574, 10.098], loss: 0.002006, mae: 0.047482, mean_q: 1.166955
 31294/100000: episode: 482, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.297, mean reward: 0.573 [0.507, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.197, 10.098], loss: 0.001907, mae: 0.047387, mean_q: 1.169361
 31394/100000: episode: 483, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.100, mean reward: 0.581 [0.507, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.943, 10.233], loss: 0.001804, mae: 0.046308, mean_q: 1.168945
 31494/100000: episode: 484, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 57.032, mean reward: 0.570 [0.507, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.320, 10.098], loss: 0.001709, mae: 0.044970, mean_q: 1.170697
 31594/100000: episode: 485, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 56.639, mean reward: 0.566 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.714, 10.123], loss: 0.001619, mae: 0.043715, mean_q: 1.165972
 31694/100000: episode: 486, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 56.873, mean reward: 0.569 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.260, 10.098], loss: 0.001686, mae: 0.044391, mean_q: 1.166173
 31794/100000: episode: 487, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 63.317, mean reward: 0.633 [0.519, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.431, 10.098], loss: 0.001778, mae: 0.045640, mean_q: 1.164986
 31894/100000: episode: 488, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 60.129, mean reward: 0.601 [0.509, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.743, 10.307], loss: 0.001660, mae: 0.044538, mean_q: 1.170419
 31994/100000: episode: 489, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.134, mean reward: 0.591 [0.506, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.777, 10.155], loss: 0.001586, mae: 0.042928, mean_q: 1.164339
 32094/100000: episode: 490, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.330, mean reward: 0.603 [0.514, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.838, 10.403], loss: 0.002104, mae: 0.048237, mean_q: 1.164272
 32194/100000: episode: 491, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.999, mean reward: 0.590 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.466, 10.313], loss: 0.001753, mae: 0.045425, mean_q: 1.170065
 32294/100000: episode: 492, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.315, mean reward: 0.583 [0.509, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.761, 10.214], loss: 0.001634, mae: 0.044104, mean_q: 1.168833
 32394/100000: episode: 493, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.349, mean reward: 0.583 [0.508, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.015, 10.113], loss: 0.001572, mae: 0.043118, mean_q: 1.168947
 32494/100000: episode: 494, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.256, mean reward: 0.583 [0.517, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.492, 10.098], loss: 0.001537, mae: 0.043099, mean_q: 1.170025
 32594/100000: episode: 495, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.563, mean reward: 0.596 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.885, 10.098], loss: 0.001526, mae: 0.042817, mean_q: 1.168348
 32694/100000: episode: 496, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 58.524, mean reward: 0.585 [0.502, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.745, 10.198], loss: 0.001539, mae: 0.042421, mean_q: 1.166483
 32794/100000: episode: 497, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.136, mean reward: 0.591 [0.511, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.258, 10.098], loss: 0.001587, mae: 0.042893, mean_q: 1.167539
 32894/100000: episode: 498, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 58.246, mean reward: 0.582 [0.506, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.093, 10.098], loss: 0.001759, mae: 0.044485, mean_q: 1.167172
 32994/100000: episode: 499, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.159, mean reward: 0.572 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.780, 10.166], loss: 0.001533, mae: 0.042163, mean_q: 1.163338
 33094/100000: episode: 500, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.474, mean reward: 0.595 [0.512, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.148, 10.279], loss: 0.001717, mae: 0.044614, mean_q: 1.165439
 33194/100000: episode: 501, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.426, mean reward: 0.584 [0.498, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.474, 10.098], loss: 0.001596, mae: 0.043145, mean_q: 1.163840
 33294/100000: episode: 502, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 61.589, mean reward: 0.616 [0.500, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.524, 10.098], loss: 0.001712, mae: 0.045110, mean_q: 1.163151
 33394/100000: episode: 503, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 59.295, mean reward: 0.593 [0.513, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.545, 10.098], loss: 0.001543, mae: 0.042202, mean_q: 1.166904
 33494/100000: episode: 504, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.376, mean reward: 0.584 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.742, 10.101], loss: 0.001556, mae: 0.042710, mean_q: 1.163852
 33594/100000: episode: 505, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 60.357, mean reward: 0.604 [0.519, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.024, 10.098], loss: 0.001510, mae: 0.042605, mean_q: 1.164879
 33694/100000: episode: 506, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.134, mean reward: 0.581 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.304, 10.141], loss: 0.001546, mae: 0.042608, mean_q: 1.164719
 33794/100000: episode: 507, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.012, mean reward: 0.600 [0.515, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.761, 10.098], loss: 0.001641, mae: 0.044236, mean_q: 1.168283
 33894/100000: episode: 508, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.540, mean reward: 0.585 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.135, 10.098], loss: 0.001715, mae: 0.044818, mean_q: 1.167553
 33994/100000: episode: 509, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 60.190, mean reward: 0.602 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.925, 10.378], loss: 0.001657, mae: 0.043663, mean_q: 1.167858
 34094/100000: episode: 510, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 61.655, mean reward: 0.617 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.040, 10.098], loss: 0.001760, mae: 0.044402, mean_q: 1.167982
 34194/100000: episode: 511, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.370, mean reward: 0.574 [0.498, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.568, 10.118], loss: 0.001581, mae: 0.043118, mean_q: 1.170070
 34294/100000: episode: 512, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 57.487, mean reward: 0.575 [0.503, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.422, 10.098], loss: 0.001435, mae: 0.041587, mean_q: 1.170677
 34394/100000: episode: 513, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.276, mean reward: 0.593 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.449, 10.172], loss: 0.001679, mae: 0.044040, mean_q: 1.168099
 34494/100000: episode: 514, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 63.058, mean reward: 0.631 [0.503, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.049, 10.355], loss: 0.001584, mae: 0.043580, mean_q: 1.169333
 34594/100000: episode: 515, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.754, mean reward: 0.608 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.725, 10.098], loss: 0.001697, mae: 0.044383, mean_q: 1.166519
 34694/100000: episode: 516, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.563, mean reward: 0.586 [0.498, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.479, 10.098], loss: 0.001585, mae: 0.043489, mean_q: 1.172626
 34794/100000: episode: 517, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 59.817, mean reward: 0.598 [0.499, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.843, 10.179], loss: 0.001495, mae: 0.042279, mean_q: 1.171011
 34894/100000: episode: 518, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.426, mean reward: 0.584 [0.509, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.527, 10.098], loss: 0.001501, mae: 0.042207, mean_q: 1.167549
 34994/100000: episode: 519, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 58.083, mean reward: 0.581 [0.504, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.327, 10.098], loss: 0.001497, mae: 0.042613, mean_q: 1.171067
[Info] 1-TH LEVEL FOUND: 1.3796989917755127, Considering 10/90 traces
 35094/100000: episode: 520, duration: 4.722s, episode steps: 100, steps per second: 21, episode reward: 56.907, mean reward: 0.569 [0.502, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.307, 10.116], loss: 0.001534, mae: 0.043017, mean_q: 1.169067
 35145/100000: episode: 521, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 32.943, mean reward: 0.646 [0.533, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.473, 10.100], loss: 0.001450, mae: 0.041327, mean_q: 1.169089
 35196/100000: episode: 522, duration: 0.307s, episode steps: 51, steps per second: 166, episode reward: 33.245, mean reward: 0.652 [0.585, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.341, 10.100], loss: 0.001559, mae: 0.042364, mean_q: 1.167838
 35211/100000: episode: 523, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 11.119, mean reward: 0.741 [0.685, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.313, 10.100], loss: 0.002048, mae: 0.046461, mean_q: 1.167542
 35253/100000: episode: 524, duration: 0.222s, episode steps: 42, steps per second: 190, episode reward: 26.078, mean reward: 0.621 [0.547, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.636, 10.100], loss: 0.001831, mae: 0.046755, mean_q: 1.171021
 35275/100000: episode: 525, duration: 0.130s, episode steps: 22, steps per second: 170, episode reward: 14.583, mean reward: 0.663 [0.580, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.296, 10.359], loss: 0.001502, mae: 0.043145, mean_q: 1.168597
 35297/100000: episode: 526, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 15.937, mean reward: 0.724 [0.672, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.095, 10.423], loss: 0.001516, mae: 0.041973, mean_q: 1.164534
 35322/100000: episode: 527, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 15.840, mean reward: 0.634 [0.569, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.292, 10.100], loss: 0.001548, mae: 0.043108, mean_q: 1.175444
 35373/100000: episode: 528, duration: 0.291s, episode steps: 51, steps per second: 175, episode reward: 31.703, mean reward: 0.622 [0.524, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.313, 10.219], loss: 0.001591, mae: 0.043914, mean_q: 1.174294
 35397/100000: episode: 529, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 17.249, mean reward: 0.719 [0.678, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.750, 10.100], loss: 0.001431, mae: 0.040588, mean_q: 1.169106
 35436/100000: episode: 530, duration: 0.228s, episode steps: 39, steps per second: 171, episode reward: 26.844, mean reward: 0.688 [0.542, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.896, 10.100], loss: 0.001491, mae: 0.042671, mean_q: 1.177063
 35460/100000: episode: 531, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 16.917, mean reward: 0.705 [0.613, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.317, 10.100], loss: 0.002465, mae: 0.051456, mean_q: 1.176545
 35482/100000: episode: 532, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 13.662, mean reward: 0.621 [0.581, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.494, 10.254], loss: 0.002003, mae: 0.046770, mean_q: 1.179491
 35507/100000: episode: 533, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 16.446, mean reward: 0.658 [0.608, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.162, 10.100], loss: 0.001672, mae: 0.044620, mean_q: 1.180090
 35522/100000: episode: 534, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 10.639, mean reward: 0.709 [0.649, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.325, 10.100], loss: 0.001684, mae: 0.044035, mean_q: 1.167877
 35537/100000: episode: 535, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 10.269, mean reward: 0.685 [0.600, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.075, 10.100], loss: 0.001550, mae: 0.042752, mean_q: 1.180421
 35579/100000: episode: 536, duration: 0.230s, episode steps: 42, steps per second: 183, episode reward: 26.877, mean reward: 0.640 [0.514, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.574, 10.100], loss: 0.001676, mae: 0.043861, mean_q: 1.177104
 35618/100000: episode: 537, duration: 0.214s, episode steps: 39, steps per second: 183, episode reward: 24.552, mean reward: 0.630 [0.528, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.035, 10.188], loss: 0.001709, mae: 0.044398, mean_q: 1.178200
 35669/100000: episode: 538, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 35.513, mean reward: 0.696 [0.557, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.177, 10.100], loss: 0.001640, mae: 0.043692, mean_q: 1.178138
 35711/100000: episode: 539, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 30.269, mean reward: 0.721 [0.626, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-1.298, 10.100], loss: 0.001666, mae: 0.045094, mean_q: 1.178605
 35735/100000: episode: 540, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 15.208, mean reward: 0.634 [0.573, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.115, 10.100], loss: 0.001799, mae: 0.045993, mean_q: 1.176314
 35774/100000: episode: 541, duration: 0.232s, episode steps: 39, steps per second: 168, episode reward: 24.602, mean reward: 0.631 [0.520, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.020, 10.100], loss: 0.001721, mae: 0.044630, mean_q: 1.180478
 35798/100000: episode: 542, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 16.198, mean reward: 0.675 [0.585, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.672, 10.100], loss: 0.001736, mae: 0.044349, mean_q: 1.184416
 35840/100000: episode: 543, duration: 0.225s, episode steps: 42, steps per second: 187, episode reward: 26.299, mean reward: 0.626 [0.517, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.271, 10.100], loss: 0.001593, mae: 0.042874, mean_q: 1.189629
 35865/100000: episode: 544, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 16.992, mean reward: 0.680 [0.560, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.432, 10.100], loss: 0.001513, mae: 0.041618, mean_q: 1.182526
 35916/100000: episode: 545, duration: 0.271s, episode steps: 51, steps per second: 189, episode reward: 34.225, mean reward: 0.671 [0.561, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-1.277, 10.100], loss: 0.001902, mae: 0.046864, mean_q: 1.183563
 35931/100000: episode: 546, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 10.697, mean reward: 0.713 [0.655, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.324, 10.100], loss: 0.001991, mae: 0.048464, mean_q: 1.175216
 35958/100000: episode: 547, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 20.516, mean reward: 0.760 [0.682, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.468], loss: 0.002147, mae: 0.050435, mean_q: 1.192591
 35985/100000: episode: 548, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 17.611, mean reward: 0.652 [0.561, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.256], loss: 0.001656, mae: 0.044548, mean_q: 1.191268
 36027/100000: episode: 549, duration: 0.246s, episode steps: 42, steps per second: 171, episode reward: 27.850, mean reward: 0.663 [0.522, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.142, 10.100], loss: 0.001604, mae: 0.042841, mean_q: 1.189602
 36051/100000: episode: 550, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 15.279, mean reward: 0.637 [0.500, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.639, 10.100], loss: 0.001648, mae: 0.042194, mean_q: 1.193124
 36075/100000: episode: 551, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 16.735, mean reward: 0.697 [0.625, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.309, 10.100], loss: 0.001862, mae: 0.045712, mean_q: 1.184410
 36099/100000: episode: 552, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 18.653, mean reward: 0.777 [0.679, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.392, 10.100], loss: 0.001953, mae: 0.046045, mean_q: 1.183426
 36141/100000: episode: 553, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 27.157, mean reward: 0.647 [0.530, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.185, 10.100], loss: 0.001799, mae: 0.045597, mean_q: 1.186083
 36183/100000: episode: 554, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 27.163, mean reward: 0.647 [0.571, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.199, 10.100], loss: 0.001751, mae: 0.045012, mean_q: 1.195869
 36205/100000: episode: 555, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 15.242, mean reward: 0.693 [0.657, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.085, 10.437], loss: 0.001841, mae: 0.044936, mean_q: 1.191681
 36227/100000: episode: 556, duration: 0.132s, episode steps: 22, steps per second: 167, episode reward: 16.106, mean reward: 0.732 [0.663, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.408], loss: 0.002004, mae: 0.047430, mean_q: 1.195037
 36266/100000: episode: 557, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 24.171, mean reward: 0.620 [0.526, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.327, 10.100], loss: 0.001668, mae: 0.042427, mean_q: 1.188451
 36293/100000: episode: 558, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 18.630, mean reward: 0.690 [0.615, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.305, 10.473], loss: 0.001613, mae: 0.043453, mean_q: 1.196502
 36318/100000: episode: 559, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 18.016, mean reward: 0.721 [0.665, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.287, 10.100], loss: 0.001540, mae: 0.042153, mean_q: 1.203929
 36343/100000: episode: 560, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 17.493, mean reward: 0.700 [0.595, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.765, 10.100], loss: 0.001793, mae: 0.046028, mean_q: 1.193303
 36382/100000: episode: 561, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 25.901, mean reward: 0.664 [0.577, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.788, 10.100], loss: 0.001788, mae: 0.045518, mean_q: 1.194217
 36433/100000: episode: 562, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 33.509, mean reward: 0.657 [0.570, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.421, 10.100], loss: 0.001802, mae: 0.044293, mean_q: 1.204572
 36457/100000: episode: 563, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 16.589, mean reward: 0.691 [0.620, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.677, 10.100], loss: 0.001840, mae: 0.045877, mean_q: 1.197248
 36479/100000: episode: 564, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 15.273, mean reward: 0.694 [0.622, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.521], loss: 0.001901, mae: 0.046682, mean_q: 1.204727
 36494/100000: episode: 565, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 10.314, mean reward: 0.688 [0.630, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.370, 10.100], loss: 0.001756, mae: 0.045079, mean_q: 1.202578
 36516/100000: episode: 566, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 15.631, mean reward: 0.711 [0.630, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.907, 10.407], loss: 0.001691, mae: 0.043787, mean_q: 1.206470
 36567/100000: episode: 567, duration: 0.273s, episode steps: 51, steps per second: 187, episode reward: 35.696, mean reward: 0.700 [0.577, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.432, 10.100], loss: 0.002084, mae: 0.048159, mean_q: 1.203510
 36589/100000: episode: 568, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 14.882, mean reward: 0.676 [0.610, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.047, 10.324], loss: 0.001723, mae: 0.045194, mean_q: 1.217247
 36614/100000: episode: 569, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 16.771, mean reward: 0.671 [0.576, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.564, 10.100], loss: 0.001797, mae: 0.046548, mean_q: 1.210984
 36636/100000: episode: 570, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 13.623, mean reward: 0.619 [0.536, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.297, 10.226], loss: 0.001501, mae: 0.041630, mean_q: 1.208825
 36687/100000: episode: 571, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 32.093, mean reward: 0.629 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.599, 10.128], loss: 0.001702, mae: 0.043728, mean_q: 1.206707
 36711/100000: episode: 572, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 16.734, mean reward: 0.697 [0.604, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.014, 10.100], loss: 0.001687, mae: 0.043896, mean_q: 1.209918
 36733/100000: episode: 573, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 14.316, mean reward: 0.651 [0.585, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.152, 10.272], loss: 0.001966, mae: 0.048511, mean_q: 1.209635
 36755/100000: episode: 574, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 15.733, mean reward: 0.715 [0.667, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.414], loss: 0.002049, mae: 0.046651, mean_q: 1.211730
 36797/100000: episode: 575, duration: 0.236s, episode steps: 42, steps per second: 178, episode reward: 27.999, mean reward: 0.667 [0.546, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.370, 10.212], loss: 0.001767, mae: 0.044990, mean_q: 1.208215
 36812/100000: episode: 576, duration: 0.103s, episode steps: 15, steps per second: 146, episode reward: 9.995, mean reward: 0.666 [0.629, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.114, 10.100], loss: 0.001828, mae: 0.047013, mean_q: 1.216475
 36863/100000: episode: 577, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 33.023, mean reward: 0.648 [0.549, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.728, 10.100], loss: 0.001490, mae: 0.041384, mean_q: 1.210206
 36887/100000: episode: 578, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 14.970, mean reward: 0.624 [0.575, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.151, 10.100], loss: 0.002282, mae: 0.051168, mean_q: 1.209701
 36929/100000: episode: 579, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 29.093, mean reward: 0.693 [0.604, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.548, 10.100], loss: 0.001843, mae: 0.045791, mean_q: 1.211692
 36968/100000: episode: 580, duration: 0.234s, episode steps: 39, steps per second: 167, episode reward: 25.870, mean reward: 0.663 [0.523, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.843, 10.100], loss: 0.001891, mae: 0.045626, mean_q: 1.214061
 37010/100000: episode: 581, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 27.103, mean reward: 0.645 [0.524, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.450, 10.100], loss: 0.001511, mae: 0.042567, mean_q: 1.216863
 37061/100000: episode: 582, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 32.912, mean reward: 0.645 [0.521, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.499, 10.100], loss: 0.001686, mae: 0.043797, mean_q: 1.214715
 37083/100000: episode: 583, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 14.398, mean reward: 0.654 [0.587, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.251, 10.337], loss: 0.001561, mae: 0.042552, mean_q: 1.225526
 37134/100000: episode: 584, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 36.367, mean reward: 0.713 [0.643, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.606, 10.100], loss: 0.001773, mae: 0.044857, mean_q: 1.218716
 37161/100000: episode: 585, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 18.628, mean reward: 0.690 [0.639, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.147, 10.386], loss: 0.002254, mae: 0.049768, mean_q: 1.211681
 37183/100000: episode: 586, duration: 0.141s, episode steps: 22, steps per second: 156, episode reward: 16.614, mean reward: 0.755 [0.677, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.597], loss: 0.001908, mae: 0.046854, mean_q: 1.212430
 37234/100000: episode: 587, duration: 0.284s, episode steps: 51, steps per second: 180, episode reward: 34.360, mean reward: 0.674 [0.531, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.792, 10.100], loss: 0.001817, mae: 0.045764, mean_q: 1.225288
 37276/100000: episode: 588, duration: 0.216s, episode steps: 42, steps per second: 194, episode reward: 28.763, mean reward: 0.685 [0.564, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.261, 10.100], loss: 0.001679, mae: 0.044298, mean_q: 1.230820
 37298/100000: episode: 589, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 15.678, mean reward: 0.713 [0.619, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.231, 10.439], loss: 0.001931, mae: 0.045895, mean_q: 1.227035
 37337/100000: episode: 590, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 25.677, mean reward: 0.658 [0.554, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.629, 10.100], loss: 0.001823, mae: 0.044857, mean_q: 1.225650
 37364/100000: episode: 591, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 18.418, mean reward: 0.682 [0.564, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.208, 10.272], loss: 0.001892, mae: 0.046742, mean_q: 1.233814
 37415/100000: episode: 592, duration: 0.279s, episode steps: 51, steps per second: 183, episode reward: 36.033, mean reward: 0.707 [0.595, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-1.235, 10.100], loss: 0.001988, mae: 0.045848, mean_q: 1.223144
 37442/100000: episode: 593, duration: 0.145s, episode steps: 27, steps per second: 187, episode reward: 18.175, mean reward: 0.673 [0.578, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.373, 10.392], loss: 0.001645, mae: 0.044202, mean_q: 1.231081
 37484/100000: episode: 594, duration: 0.249s, episode steps: 42, steps per second: 168, episode reward: 28.328, mean reward: 0.674 [0.551, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.785, 10.100], loss: 0.002150, mae: 0.049547, mean_q: 1.234083
 37535/100000: episode: 595, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 31.483, mean reward: 0.617 [0.529, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-1.197, 10.100], loss: 0.002277, mae: 0.050209, mean_q: 1.224982
 37560/100000: episode: 596, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 18.255, mean reward: 0.730 [0.673, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.454, 10.100], loss: 0.002644, mae: 0.055371, mean_q: 1.235740
 37602/100000: episode: 597, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 26.579, mean reward: 0.633 [0.566, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-2.052, 10.100], loss: 0.001740, mae: 0.044870, mean_q: 1.226325
 37629/100000: episode: 598, duration: 0.150s, episode steps: 27, steps per second: 181, episode reward: 18.855, mean reward: 0.698 [0.609, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.588, 10.374], loss: 0.001852, mae: 0.045897, mean_q: 1.244962
 37653/100000: episode: 599, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 16.789, mean reward: 0.700 [0.613, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.446, 10.100], loss: 0.001622, mae: 0.043156, mean_q: 1.240706
 37692/100000: episode: 600, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 26.365, mean reward: 0.676 [0.582, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.799, 10.100], loss: 0.001603, mae: 0.043517, mean_q: 1.241423
 37734/100000: episode: 601, duration: 0.244s, episode steps: 42, steps per second: 172, episode reward: 25.927, mean reward: 0.617 [0.513, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.141, 10.100], loss: 0.001702, mae: 0.044421, mean_q: 1.241957
 37776/100000: episode: 602, duration: 0.226s, episode steps: 42, steps per second: 185, episode reward: 27.699, mean reward: 0.660 [0.569, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.568, 10.100], loss: 0.002076, mae: 0.047980, mean_q: 1.244760
 37791/100000: episode: 603, duration: 0.092s, episode steps: 15, steps per second: 162, episode reward: 10.675, mean reward: 0.712 [0.611, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.576, 10.100], loss: 0.002412, mae: 0.050076, mean_q: 1.231984
 37830/100000: episode: 604, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 25.804, mean reward: 0.662 [0.520, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-1.146, 10.100], loss: 0.002001, mae: 0.047778, mean_q: 1.233780
 37881/100000: episode: 605, duration: 0.303s, episode steps: 51, steps per second: 168, episode reward: 37.223, mean reward: 0.730 [0.644, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.672, 10.100], loss: 0.001636, mae: 0.043998, mean_q: 1.246414
 37932/100000: episode: 606, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 35.611, mean reward: 0.698 [0.565, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.677, 10.100], loss: 0.001765, mae: 0.044756, mean_q: 1.242030
 37954/100000: episode: 607, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 16.344, mean reward: 0.743 [0.681, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.552], loss: 0.001937, mae: 0.046555, mean_q: 1.251035
 37996/100000: episode: 608, duration: 0.237s, episode steps: 42, steps per second: 177, episode reward: 31.411, mean reward: 0.748 [0.647, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-1.148, 10.100], loss: 0.001962, mae: 0.048112, mean_q: 1.253149
 38018/100000: episode: 609, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 14.928, mean reward: 0.679 [0.601, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.569, 10.351], loss: 0.001734, mae: 0.044902, mean_q: 1.255900
[Info] 2-TH LEVEL FOUND: 1.4974632263183594, Considering 10/90 traces
 38040/100000: episode: 610, duration: 4.262s, episode steps: 22, steps per second: 5, episode reward: 15.281, mean reward: 0.695 [0.628, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.909, 10.394], loss: 0.001969, mae: 0.046188, mean_q: 1.242671
 38084/100000: episode: 611, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 29.504, mean reward: 0.671 [0.528, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-1.008, 10.258], loss: 0.001941, mae: 0.046954, mean_q: 1.250115
 38125/100000: episode: 612, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 26.271, mean reward: 0.641 [0.555, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.644, 10.100], loss: 0.001817, mae: 0.046112, mean_q: 1.261547
 38172/100000: episode: 613, duration: 0.261s, episode steps: 47, steps per second: 180, episode reward: 31.417, mean reward: 0.668 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.640, 10.246], loss: 0.001682, mae: 0.044554, mean_q: 1.253487
 38190/100000: episode: 614, duration: 0.111s, episode steps: 18, steps per second: 163, episode reward: 12.620, mean reward: 0.701 [0.640, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.240, 10.464], loss: 0.002117, mae: 0.049877, mean_q: 1.257465
 38234/100000: episode: 615, duration: 0.259s, episode steps: 44, steps per second: 170, episode reward: 36.170, mean reward: 0.822 [0.748, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.307, 10.100], loss: 0.001537, mae: 0.042581, mean_q: 1.263284
 38278/100000: episode: 616, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 29.443, mean reward: 0.669 [0.531, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.253, 10.100], loss: 0.001618, mae: 0.043821, mean_q: 1.261917
 38316/100000: episode: 617, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 28.982, mean reward: 0.763 [0.699, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.534, 10.100], loss: 0.001589, mae: 0.043531, mean_q: 1.263379
 38332/100000: episode: 618, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 13.646, mean reward: 0.853 [0.812, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.059, 10.605], loss: 0.001769, mae: 0.045971, mean_q: 1.261570
 38376/100000: episode: 619, duration: 0.246s, episode steps: 44, steps per second: 179, episode reward: 31.596, mean reward: 0.718 [0.586, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.251, 10.100], loss: 0.001952, mae: 0.048049, mean_q: 1.265263
 38415/100000: episode: 620, duration: 0.225s, episode steps: 39, steps per second: 173, episode reward: 30.796, mean reward: 0.790 [0.706, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.499, 10.100], loss: 0.001722, mae: 0.044219, mean_q: 1.261063
 38456/100000: episode: 621, duration: 0.221s, episode steps: 41, steps per second: 186, episode reward: 30.871, mean reward: 0.753 [0.649, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.832, 10.100], loss: 0.001544, mae: 0.043624, mean_q: 1.266283
 38489/100000: episode: 622, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 25.407, mean reward: 0.770 [0.630, 0.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.236, 10.100], loss: 0.001464, mae: 0.041614, mean_q: 1.277508
 38522/100000: episode: 623, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 21.661, mean reward: 0.656 [0.549, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.496, 10.100], loss: 0.002034, mae: 0.048061, mean_q: 1.274519
 38538/100000: episode: 624, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 12.730, mean reward: 0.796 [0.719, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.886, 10.546], loss: 0.001790, mae: 0.045089, mean_q: 1.278634
 38554/100000: episode: 625, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 13.861, mean reward: 0.866 [0.787, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.688], loss: 0.001537, mae: 0.043008, mean_q: 1.266271
 38595/100000: episode: 626, duration: 0.222s, episode steps: 41, steps per second: 185, episode reward: 27.846, mean reward: 0.679 [0.516, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.322, 10.100], loss: 0.001682, mae: 0.043750, mean_q: 1.275115
 38642/100000: episode: 627, duration: 0.254s, episode steps: 47, steps per second: 185, episode reward: 31.562, mean reward: 0.672 [0.520, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.247, 10.398], loss: 0.001661, mae: 0.044101, mean_q: 1.276082
 38677/100000: episode: 628, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 25.897, mean reward: 0.740 [0.596, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.886, 10.100], loss: 0.001697, mae: 0.043986, mean_q: 1.279747
 38718/100000: episode: 629, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 26.632, mean reward: 0.650 [0.521, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.217, 10.100], loss: 0.002206, mae: 0.050875, mean_q: 1.283106
 38753/100000: episode: 630, duration: 0.205s, episode steps: 35, steps per second: 171, episode reward: 24.822, mean reward: 0.709 [0.613, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.806, 10.100], loss: 0.001745, mae: 0.046388, mean_q: 1.276285
 38769/100000: episode: 631, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 12.443, mean reward: 0.778 [0.698, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.579, 10.506], loss: 0.001723, mae: 0.043810, mean_q: 1.290318
 38787/100000: episode: 632, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 12.978, mean reward: 0.721 [0.660, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.504, 10.425], loss: 0.002290, mae: 0.051026, mean_q: 1.280120
 38803/100000: episode: 633, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 13.550, mean reward: 0.847 [0.758, 0.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.743], loss: 0.001907, mae: 0.048137, mean_q: 1.282766
 38836/100000: episode: 634, duration: 0.189s, episode steps: 33, steps per second: 174, episode reward: 22.641, mean reward: 0.686 [0.596, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.557, 10.100], loss: 0.001939, mae: 0.046776, mean_q: 1.287547
 38880/100000: episode: 635, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 34.188, mean reward: 0.777 [0.657, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-1.451, 10.100], loss: 0.001737, mae: 0.045554, mean_q: 1.297904
 38896/100000: episode: 636, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 12.443, mean reward: 0.778 [0.726, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.534], loss: 0.001755, mae: 0.044952, mean_q: 1.293785
 38935/100000: episode: 637, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 27.847, mean reward: 0.714 [0.629, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.490, 10.100], loss: 0.001850, mae: 0.047394, mean_q: 1.296185
 38974/100000: episode: 638, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 25.583, mean reward: 0.656 [0.505, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.602, 10.158], loss: 0.001785, mae: 0.046370, mean_q: 1.293026
 39009/100000: episode: 639, duration: 0.202s, episode steps: 35, steps per second: 173, episode reward: 24.466, mean reward: 0.699 [0.571, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.109, 10.218], loss: 0.001829, mae: 0.046299, mean_q: 1.292453
 39047/100000: episode: 640, duration: 0.225s, episode steps: 38, steps per second: 169, episode reward: 28.680, mean reward: 0.755 [0.641, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.796, 10.100], loss: 0.001872, mae: 0.047472, mean_q: 1.299157
 39063/100000: episode: 641, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 11.128, mean reward: 0.696 [0.637, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.400], loss: 0.002288, mae: 0.051614, mean_q: 1.296543
 39110/100000: episode: 642, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 38.060, mean reward: 0.810 [0.712, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.572, 10.100], loss: 0.001836, mae: 0.046300, mean_q: 1.294076
 39145/100000: episode: 643, duration: 0.185s, episode steps: 35, steps per second: 189, episode reward: 25.808, mean reward: 0.737 [0.663, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.457, 10.100], loss: 0.002178, mae: 0.049897, mean_q: 1.299085
 39189/100000: episode: 644, duration: 0.252s, episode steps: 44, steps per second: 174, episode reward: 30.692, mean reward: 0.698 [0.597, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.353, 10.100], loss: 0.001757, mae: 0.045885, mean_q: 1.307725
 39227/100000: episode: 645, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 27.358, mean reward: 0.720 [0.652, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.546, 10.100], loss: 0.002081, mae: 0.049161, mean_q: 1.309835
 39245/100000: episode: 646, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 12.751, mean reward: 0.708 [0.618, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.413], loss: 0.001836, mae: 0.047461, mean_q: 1.321580
 39280/100000: episode: 647, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 23.702, mean reward: 0.677 [0.519, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.035, 10.184], loss: 0.001589, mae: 0.044348, mean_q: 1.311991
 39327/100000: episode: 648, duration: 0.260s, episode steps: 47, steps per second: 181, episode reward: 29.188, mean reward: 0.621 [0.519, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.214, 10.247], loss: 0.001962, mae: 0.046917, mean_q: 1.312996
 39366/100000: episode: 649, duration: 0.219s, episode steps: 39, steps per second: 178, episode reward: 26.579, mean reward: 0.682 [0.516, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.878, 10.140], loss: 0.001718, mae: 0.045223, mean_q: 1.308207
 39410/100000: episode: 650, duration: 0.245s, episode steps: 44, steps per second: 180, episode reward: 31.958, mean reward: 0.726 [0.636, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.863, 10.100], loss: 0.001752, mae: 0.045635, mean_q: 1.305077
 39445/100000: episode: 651, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 27.690, mean reward: 0.791 [0.733, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.831, 10.100], loss: 0.001796, mae: 0.045456, mean_q: 1.312690
 39489/100000: episode: 652, duration: 0.241s, episode steps: 44, steps per second: 183, episode reward: 34.887, mean reward: 0.793 [0.703, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.817, 10.100], loss: 0.001885, mae: 0.047576, mean_q: 1.309520
 39527/100000: episode: 653, duration: 0.225s, episode steps: 38, steps per second: 169, episode reward: 26.590, mean reward: 0.700 [0.545, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.919, 10.100], loss: 0.001776, mae: 0.046403, mean_q: 1.313664
 39545/100000: episode: 654, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 12.814, mean reward: 0.712 [0.672, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.237, 10.371], loss: 0.001628, mae: 0.045082, mean_q: 1.311300
 39584/100000: episode: 655, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 28.565, mean reward: 0.732 [0.569, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.193, 10.100], loss: 0.001893, mae: 0.047754, mean_q: 1.320960
 39631/100000: episode: 656, duration: 0.296s, episode steps: 47, steps per second: 159, episode reward: 30.465, mean reward: 0.648 [0.559, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-1.173, 10.100], loss: 0.001562, mae: 0.043364, mean_q: 1.325424
 39649/100000: episode: 657, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 13.702, mean reward: 0.761 [0.699, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.573], loss: 0.001755, mae: 0.045910, mean_q: 1.323475
 39687/100000: episode: 658, duration: 0.186s, episode steps: 38, steps per second: 205, episode reward: 27.157, mean reward: 0.715 [0.657, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.352, 10.100], loss: 0.001821, mae: 0.046334, mean_q: 1.316725
 39725/100000: episode: 659, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 26.058, mean reward: 0.686 [0.626, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.896, 10.100], loss: 0.002030, mae: 0.049142, mean_q: 1.314088
 39772/100000: episode: 660, duration: 0.252s, episode steps: 47, steps per second: 187, episode reward: 32.446, mean reward: 0.690 [0.585, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.403, 10.100], loss: 0.002116, mae: 0.050125, mean_q: 1.335331
 39813/100000: episode: 661, duration: 0.233s, episode steps: 41, steps per second: 176, episode reward: 26.192, mean reward: 0.639 [0.513, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-1.331, 10.100], loss: 0.001751, mae: 0.045962, mean_q: 1.329881
 39853/100000: episode: 662, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 28.154, mean reward: 0.704 [0.539, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.905, 10.100], loss: 0.001734, mae: 0.045517, mean_q: 1.335709
 39894/100000: episode: 663, duration: 0.239s, episode steps: 41, steps per second: 172, episode reward: 26.192, mean reward: 0.639 [0.534, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.500, 10.100], loss: 0.001649, mae: 0.044648, mean_q: 1.330092
 39935/100000: episode: 664, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 28.394, mean reward: 0.693 [0.545, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.780, 10.100], loss: 0.001746, mae: 0.046293, mean_q: 1.333279
 39982/100000: episode: 665, duration: 0.271s, episode steps: 47, steps per second: 173, episode reward: 32.588, mean reward: 0.693 [0.511, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-1.003, 10.161], loss: 0.001985, mae: 0.048191, mean_q: 1.325895
 40021/100000: episode: 666, duration: 0.229s, episode steps: 39, steps per second: 171, episode reward: 26.088, mean reward: 0.669 [0.600, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.226, 10.100], loss: 0.001640, mae: 0.044653, mean_q: 1.329861
 40061/100000: episode: 667, duration: 0.217s, episode steps: 40, steps per second: 184, episode reward: 27.687, mean reward: 0.692 [0.594, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.391, 10.100], loss: 0.001789, mae: 0.047291, mean_q: 1.339682
 40108/100000: episode: 668, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 36.702, mean reward: 0.781 [0.661, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.252, 10.100], loss: 0.001740, mae: 0.045501, mean_q: 1.334261
 40126/100000: episode: 669, duration: 0.111s, episode steps: 18, steps per second: 163, episode reward: 13.424, mean reward: 0.746 [0.696, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.071, 10.645], loss: 0.002227, mae: 0.051665, mean_q: 1.328295
 40142/100000: episode: 670, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 11.716, mean reward: 0.732 [0.610, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.429], loss: 0.001764, mae: 0.046020, mean_q: 1.339111
 40183/100000: episode: 671, duration: 0.215s, episode steps: 41, steps per second: 191, episode reward: 27.281, mean reward: 0.665 [0.526, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.237, 10.100], loss: 0.001528, mae: 0.042656, mean_q: 1.337745
 40227/100000: episode: 672, duration: 0.257s, episode steps: 44, steps per second: 171, episode reward: 30.404, mean reward: 0.691 [0.548, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.514, 10.100], loss: 0.001606, mae: 0.044036, mean_q: 1.337234
 40260/100000: episode: 673, duration: 0.189s, episode steps: 33, steps per second: 175, episode reward: 23.005, mean reward: 0.697 [0.564, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.853, 10.100], loss: 0.001826, mae: 0.046515, mean_q: 1.339718
 40298/100000: episode: 674, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 29.369, mean reward: 0.773 [0.655, 0.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.344, 10.100], loss: 0.001836, mae: 0.046683, mean_q: 1.339672
 40331/100000: episode: 675, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 25.392, mean reward: 0.769 [0.648, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-1.489, 10.100], loss: 0.001797, mae: 0.046944, mean_q: 1.348155
 40375/100000: episode: 676, duration: 0.241s, episode steps: 44, steps per second: 183, episode reward: 32.172, mean reward: 0.731 [0.609, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.733, 10.100], loss: 0.001723, mae: 0.045076, mean_q: 1.332161
 40393/100000: episode: 677, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 12.959, mean reward: 0.720 [0.680, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.347, 10.475], loss: 0.001684, mae: 0.045377, mean_q: 1.336006
 40428/100000: episode: 678, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 27.066, mean reward: 0.773 [0.629, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.980, 10.100], loss: 0.001943, mae: 0.047860, mean_q: 1.346594
 40463/100000: episode: 679, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 25.705, mean reward: 0.734 [0.644, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.201, 10.100], loss: 0.001703, mae: 0.045466, mean_q: 1.345091
[Info] FALSIFICATION!
 40483/100000: episode: 680, duration: 0.269s, episode steps: 20, steps per second: 74, episode reward: 15.963, mean reward: 0.798 [0.661, 1.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.786 [-1.560, 9.139], loss: 0.001486, mae: 0.042922, mean_q: 1.355537
 40521/100000: episode: 681, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 25.820, mean reward: 0.679 [0.539, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.068, 10.100], loss: 0.001768, mae: 0.046105, mean_q: 1.348953
 40568/100000: episode: 682, duration: 0.254s, episode steps: 47, steps per second: 185, episode reward: 35.797, mean reward: 0.762 [0.654, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.314, 10.100], loss: 0.001749, mae: 0.044327, mean_q: 1.344016
 40606/100000: episode: 683, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 26.979, mean reward: 0.710 [0.539, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.162, 10.100], loss: 0.001666, mae: 0.044977, mean_q: 1.351918
 40650/100000: episode: 684, duration: 0.255s, episode steps: 44, steps per second: 173, episode reward: 28.905, mean reward: 0.657 [0.543, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.286, 10.100], loss: 0.001722, mae: 0.045111, mean_q: 1.350773
 40688/100000: episode: 685, duration: 0.223s, episode steps: 38, steps per second: 170, episode reward: 29.034, mean reward: 0.764 [0.674, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.451, 10.100], loss: 0.001773, mae: 0.046807, mean_q: 1.353049
 40706/100000: episode: 686, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 13.056, mean reward: 0.725 [0.661, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.215, 10.407], loss: 0.001532, mae: 0.043793, mean_q: 1.358607
 40722/100000: episode: 687, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 13.204, mean reward: 0.825 [0.761, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.558], loss: 0.001573, mae: 0.046023, mean_q: 1.365330
 40740/100000: episode: 688, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 14.541, mean reward: 0.808 [0.724, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.069, 10.627], loss: 0.001582, mae: 0.042650, mean_q: 1.354810
 40780/100000: episode: 689, duration: 0.229s, episode steps: 40, steps per second: 175, episode reward: 26.938, mean reward: 0.673 [0.558, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.496, 10.100], loss: 0.001649, mae: 0.045262, mean_q: 1.361622
 40827/100000: episode: 690, duration: 0.260s, episode steps: 47, steps per second: 181, episode reward: 29.379, mean reward: 0.625 [0.502, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.363, 10.267], loss: 0.001877, mae: 0.046225, mean_q: 1.349565
 40845/100000: episode: 691, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 12.961, mean reward: 0.720 [0.665, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.468], loss: 0.001601, mae: 0.044158, mean_q: 1.348839
 40892/100000: episode: 692, duration: 0.252s, episode steps: 47, steps per second: 186, episode reward: 29.862, mean reward: 0.635 [0.520, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-1.634, 10.138], loss: 0.001668, mae: 0.045377, mean_q: 1.350625
 40933/100000: episode: 693, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 25.341, mean reward: 0.618 [0.523, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.158, 10.100], loss: 0.001684, mae: 0.045113, mean_q: 1.359762
 40949/100000: episode: 694, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 12.844, mean reward: 0.803 [0.755, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.534], loss: 0.001474, mae: 0.042350, mean_q: 1.362515
 40967/100000: episode: 695, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 13.068, mean reward: 0.726 [0.681, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.479], loss: 0.001601, mae: 0.044532, mean_q: 1.357072
 41002/100000: episode: 696, duration: 0.187s, episode steps: 35, steps per second: 188, episode reward: 26.949, mean reward: 0.770 [0.638, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.243, 10.100], loss: 0.001515, mae: 0.043364, mean_q: 1.354829
 41040/100000: episode: 697, duration: 0.201s, episode steps: 38, steps per second: 189, episode reward: 25.891, mean reward: 0.681 [0.511, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.552, 10.126], loss: 0.001756, mae: 0.044034, mean_q: 1.353937
 41078/100000: episode: 698, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 28.975, mean reward: 0.762 [0.621, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-1.003, 10.100], loss: 0.001731, mae: 0.044676, mean_q: 1.353811
 41096/100000: episode: 699, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 13.158, mean reward: 0.731 [0.685, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.695, 10.529], loss: 0.001639, mae: 0.043730, mean_q: 1.343732
[Info] Complete ISplit Iteration
[Info] Levels: [1.379699, 1.4974632, 1.704717]
[Info] Cond. Prob: [0.1, 0.1, 0.05]
[Info] Error Prob: 0.0005000000000000001

 41143/100000: episode: 700, duration: 4.475s, episode steps: 47, steps per second: 11, episode reward: 37.960, mean reward: 0.808 [0.703, 0.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.791, 10.100], loss: 0.001558, mae: 0.043060, mean_q: 1.363747
 41243/100000: episode: 701, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.893, mean reward: 0.589 [0.502, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.531, 10.259], loss: 0.001903, mae: 0.047310, mean_q: 1.356836
 41343/100000: episode: 702, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.024, mean reward: 0.590 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.978, 10.098], loss: 0.001805, mae: 0.046304, mean_q: 1.354937
 41443/100000: episode: 703, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.262, mean reward: 0.583 [0.502, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.115, 10.098], loss: 0.001700, mae: 0.045090, mean_q: 1.355548
 41543/100000: episode: 704, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.831, mean reward: 0.588 [0.509, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.577, 10.155], loss: 0.001901, mae: 0.046738, mean_q: 1.347547
 41643/100000: episode: 705, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 57.108, mean reward: 0.571 [0.500, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.118, 10.129], loss: 0.001640, mae: 0.045007, mean_q: 1.344745
 41743/100000: episode: 706, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 56.836, mean reward: 0.568 [0.497, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.741, 10.152], loss: 0.001677, mae: 0.044037, mean_q: 1.345768
 41843/100000: episode: 707, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 60.429, mean reward: 0.604 [0.500, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.716, 10.098], loss: 0.001708, mae: 0.044560, mean_q: 1.339255
 41943/100000: episode: 708, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 64.905, mean reward: 0.649 [0.512, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.978, 10.366], loss: 0.001913, mae: 0.046690, mean_q: 1.337132
 42043/100000: episode: 709, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.125, mean reward: 0.591 [0.501, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.088, 10.181], loss: 0.001557, mae: 0.042837, mean_q: 1.337120
 42143/100000: episode: 710, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.808, mean reward: 0.578 [0.498, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.649, 10.098], loss: 0.001747, mae: 0.045335, mean_q: 1.327658
 42243/100000: episode: 711, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.278, mean reward: 0.563 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.864, 10.131], loss: 0.001710, mae: 0.045034, mean_q: 1.325695
 42343/100000: episode: 712, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 58.700, mean reward: 0.587 [0.507, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.354, 10.122], loss: 0.001774, mae: 0.046198, mean_q: 1.325626
 42443/100000: episode: 713, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.597, mean reward: 0.576 [0.504, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.617, 10.234], loss: 0.001678, mae: 0.044124, mean_q: 1.317167
 42543/100000: episode: 714, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.386, mean reward: 0.574 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.309, 10.098], loss: 0.001730, mae: 0.044696, mean_q: 1.320776
 42643/100000: episode: 715, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.482, mean reward: 0.575 [0.499, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.025, 10.098], loss: 0.001852, mae: 0.044333, mean_q: 1.310843
 42743/100000: episode: 716, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.576, mean reward: 0.586 [0.502, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.628, 10.249], loss: 0.001643, mae: 0.043372, mean_q: 1.314172
 42843/100000: episode: 717, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.310, mean reward: 0.593 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.215, 10.098], loss: 0.001681, mae: 0.043703, mean_q: 1.312164
 42943/100000: episode: 718, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.896, mean reward: 0.599 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.870, 10.098], loss: 0.001813, mae: 0.045550, mean_q: 1.305927
 43043/100000: episode: 719, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.477, mean reward: 0.585 [0.507, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.553, 10.101], loss: 0.001801, mae: 0.045164, mean_q: 1.301744
 43143/100000: episode: 720, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 56.896, mean reward: 0.569 [0.500, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.679, 10.098], loss: 0.001889, mae: 0.044900, mean_q: 1.299479
 43243/100000: episode: 721, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.267, mean reward: 0.593 [0.504, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.919, 10.098], loss: 0.001889, mae: 0.046625, mean_q: 1.292294
 43343/100000: episode: 722, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 56.871, mean reward: 0.569 [0.504, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.522, 10.098], loss: 0.001706, mae: 0.044272, mean_q: 1.285508
 43443/100000: episode: 723, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 57.118, mean reward: 0.571 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.801, 10.098], loss: 0.001860, mae: 0.045804, mean_q: 1.278594
 43543/100000: episode: 724, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 64.142, mean reward: 0.641 [0.510, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.085, 10.385], loss: 0.001611, mae: 0.043172, mean_q: 1.271394
 43643/100000: episode: 725, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: 58.333, mean reward: 0.583 [0.499, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.804, 10.205], loss: 0.001972, mae: 0.045707, mean_q: 1.268408
 43743/100000: episode: 726, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 58.241, mean reward: 0.582 [0.503, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.790, 10.132], loss: 0.002014, mae: 0.045695, mean_q: 1.262771
 43843/100000: episode: 727, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: 60.664, mean reward: 0.607 [0.511, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.428, 10.222], loss: 0.001897, mae: 0.045057, mean_q: 1.264214
 43943/100000: episode: 728, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 60.056, mean reward: 0.601 [0.523, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.652, 10.512], loss: 0.001713, mae: 0.044099, mean_q: 1.254218
 44043/100000: episode: 729, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: 59.572, mean reward: 0.596 [0.510, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.964, 10.098], loss: 0.001728, mae: 0.044485, mean_q: 1.249907
 44143/100000: episode: 730, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 55.832, mean reward: 0.558 [0.506, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.214, 10.098], loss: 0.001753, mae: 0.044385, mean_q: 1.247594
 44243/100000: episode: 731, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 61.899, mean reward: 0.619 [0.520, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.714, 10.098], loss: 0.001749, mae: 0.043671, mean_q: 1.242798
 44343/100000: episode: 732, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.509, mean reward: 0.605 [0.520, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.451, 10.201], loss: 0.001981, mae: 0.045953, mean_q: 1.245517
 44443/100000: episode: 733, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 60.177, mean reward: 0.602 [0.515, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.822, 10.098], loss: 0.001845, mae: 0.046206, mean_q: 1.232796
 44543/100000: episode: 734, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.876, mean reward: 0.589 [0.500, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.956, 10.098], loss: 0.001935, mae: 0.045558, mean_q: 1.229608
 44643/100000: episode: 735, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.511, mean reward: 0.585 [0.506, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.805, 10.098], loss: 0.001945, mae: 0.045863, mean_q: 1.220136
 44743/100000: episode: 736, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.236, mean reward: 0.572 [0.500, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.708, 10.098], loss: 0.001802, mae: 0.045226, mean_q: 1.222080
 44843/100000: episode: 737, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.986, mean reward: 0.600 [0.513, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.537, 10.098], loss: 0.001729, mae: 0.043824, mean_q: 1.220409
 44943/100000: episode: 738, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 64.189, mean reward: 0.642 [0.512, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.532, 10.098], loss: 0.001690, mae: 0.043090, mean_q: 1.220265
 45043/100000: episode: 739, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 62.369, mean reward: 0.624 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.885, 10.098], loss: 0.001634, mae: 0.043212, mean_q: 1.221944
 45143/100000: episode: 740, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.938, mean reward: 0.589 [0.517, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.522, 10.098], loss: 0.001616, mae: 0.042862, mean_q: 1.212253
 45243/100000: episode: 741, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.651, mean reward: 0.587 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.810, 10.098], loss: 0.001794, mae: 0.044694, mean_q: 1.211567
 45343/100000: episode: 742, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.883, mean reward: 0.599 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.949, 10.397], loss: 0.001804, mae: 0.045249, mean_q: 1.204636
 45443/100000: episode: 743, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 58.674, mean reward: 0.587 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.236, 10.437], loss: 0.001734, mae: 0.044409, mean_q: 1.199881
 45543/100000: episode: 744, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.815, mean reward: 0.578 [0.498, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.346, 10.180], loss: 0.001741, mae: 0.044705, mean_q: 1.194228
 45643/100000: episode: 745, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 59.030, mean reward: 0.590 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.753, 10.274], loss: 0.001586, mae: 0.043733, mean_q: 1.189405
 45743/100000: episode: 746, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 61.498, mean reward: 0.615 [0.499, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.401, 10.098], loss: 0.001793, mae: 0.045366, mean_q: 1.182535
 45843/100000: episode: 747, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.315, mean reward: 0.593 [0.509, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.726, 10.098], loss: 0.001645, mae: 0.043605, mean_q: 1.185048
 45943/100000: episode: 748, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.901, mean reward: 0.609 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.541, 10.119], loss: 0.001732, mae: 0.045086, mean_q: 1.183218
 46043/100000: episode: 749, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 61.909, mean reward: 0.619 [0.512, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.777, 10.098], loss: 0.001629, mae: 0.043997, mean_q: 1.179885
 46143/100000: episode: 750, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.889, mean reward: 0.589 [0.516, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.509, 10.098], loss: 0.001483, mae: 0.041662, mean_q: 1.172342
 46243/100000: episode: 751, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.578, mean reward: 0.596 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.710, 10.205], loss: 0.001636, mae: 0.043976, mean_q: 1.170309
 46343/100000: episode: 752, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 59.876, mean reward: 0.599 [0.512, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.763, 10.098], loss: 0.001584, mae: 0.043350, mean_q: 1.174775
 46443/100000: episode: 753, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 57.821, mean reward: 0.578 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.222, 10.115], loss: 0.001572, mae: 0.042901, mean_q: 1.171697
 46543/100000: episode: 754, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.432, mean reward: 0.584 [0.511, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.123, 10.098], loss: 0.001457, mae: 0.041786, mean_q: 1.170970
 46643/100000: episode: 755, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.589, mean reward: 0.596 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.334, 10.098], loss: 0.001530, mae: 0.042388, mean_q: 1.173237
 46743/100000: episode: 756, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: 58.292, mean reward: 0.583 [0.504, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.356, 10.146], loss: 0.001472, mae: 0.041630, mean_q: 1.171013
 46843/100000: episode: 757, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.924, mean reward: 0.609 [0.506, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.734, 10.098], loss: 0.001500, mae: 0.042271, mean_q: 1.171644
 46943/100000: episode: 758, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 57.012, mean reward: 0.570 [0.512, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.202, 10.098], loss: 0.001485, mae: 0.041877, mean_q: 1.171411
 47043/100000: episode: 759, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.050, mean reward: 0.591 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.614, 10.315], loss: 0.001661, mae: 0.043955, mean_q: 1.165785
 47143/100000: episode: 760, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 63.028, mean reward: 0.630 [0.504, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.031, 10.098], loss: 0.001466, mae: 0.042116, mean_q: 1.170387
 47243/100000: episode: 761, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 65.877, mean reward: 0.659 [0.503, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.208, 10.098], loss: 0.001532, mae: 0.042872, mean_q: 1.170839
 47343/100000: episode: 762, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 56.262, mean reward: 0.563 [0.500, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.446, 10.146], loss: 0.001502, mae: 0.042244, mean_q: 1.174421
 47443/100000: episode: 763, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 64.167, mean reward: 0.642 [0.506, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.178, 10.306], loss: 0.001648, mae: 0.044394, mean_q: 1.176384
 47543/100000: episode: 764, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 57.858, mean reward: 0.579 [0.506, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.700, 10.098], loss: 0.001536, mae: 0.042711, mean_q: 1.178531
 47643/100000: episode: 765, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 59.928, mean reward: 0.599 [0.505, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.621, 10.355], loss: 0.001562, mae: 0.043459, mean_q: 1.176167
 47743/100000: episode: 766, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.785, mean reward: 0.588 [0.522, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.250, 10.283], loss: 0.001573, mae: 0.043338, mean_q: 1.177400
 47843/100000: episode: 767, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 57.155, mean reward: 0.572 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.094, 10.098], loss: 0.001622, mae: 0.043853, mean_q: 1.177914
 47943/100000: episode: 768, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.611, mean reward: 0.586 [0.500, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.683, 10.098], loss: 0.001659, mae: 0.043446, mean_q: 1.176026
 48043/100000: episode: 769, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: 59.872, mean reward: 0.599 [0.507, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.484, 10.197], loss: 0.001550, mae: 0.042705, mean_q: 1.178238
 48143/100000: episode: 770, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 58.045, mean reward: 0.580 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.774, 10.098], loss: 0.001602, mae: 0.043871, mean_q: 1.179013
 48243/100000: episode: 771, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 56.808, mean reward: 0.568 [0.504, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.797, 10.119], loss: 0.001448, mae: 0.041786, mean_q: 1.175732
 48343/100000: episode: 772, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 57.377, mean reward: 0.574 [0.500, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.226, 10.098], loss: 0.001625, mae: 0.043105, mean_q: 1.176551
 48443/100000: episode: 773, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.859, mean reward: 0.579 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.715, 10.284], loss: 0.001651, mae: 0.044404, mean_q: 1.177026
 48543/100000: episode: 774, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.079, mean reward: 0.581 [0.498, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.671, 10.098], loss: 0.001556, mae: 0.042988, mean_q: 1.174626
 48643/100000: episode: 775, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.886, mean reward: 0.579 [0.505, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.950, 10.098], loss: 0.001526, mae: 0.042797, mean_q: 1.173003
 48743/100000: episode: 776, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.799, mean reward: 0.578 [0.509, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.771, 10.290], loss: 0.001463, mae: 0.041533, mean_q: 1.176144
 48843/100000: episode: 777, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 59.139, mean reward: 0.591 [0.499, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.177, 10.098], loss: 0.001748, mae: 0.045395, mean_q: 1.175208
 48943/100000: episode: 778, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.206, mean reward: 0.582 [0.503, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.201, 10.098], loss: 0.001493, mae: 0.042615, mean_q: 1.175065
 49043/100000: episode: 779, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.529, mean reward: 0.605 [0.511, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.976, 10.239], loss: 0.001397, mae: 0.040993, mean_q: 1.171263
 49143/100000: episode: 780, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.485, mean reward: 0.585 [0.502, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.356, 10.098], loss: 0.001474, mae: 0.041430, mean_q: 1.177272
 49243/100000: episode: 781, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 60.824, mean reward: 0.608 [0.503, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.291, 10.217], loss: 0.001419, mae: 0.041065, mean_q: 1.177123
 49343/100000: episode: 782, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 61.732, mean reward: 0.617 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.145, 10.403], loss: 0.001527, mae: 0.042267, mean_q: 1.173068
 49443/100000: episode: 783, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: 58.292, mean reward: 0.583 [0.499, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.530, 10.130], loss: 0.001509, mae: 0.042311, mean_q: 1.175151
 49543/100000: episode: 784, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.429, mean reward: 0.594 [0.517, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.177, 10.167], loss: 0.001399, mae: 0.041057, mean_q: 1.175683
 49643/100000: episode: 785, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 61.720, mean reward: 0.617 [0.509, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.332, 10.357], loss: 0.001443, mae: 0.040916, mean_q: 1.173327
 49743/100000: episode: 786, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 61.638, mean reward: 0.616 [0.499, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.640, 10.338], loss: 0.001511, mae: 0.042638, mean_q: 1.178598
 49843/100000: episode: 787, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 60.670, mean reward: 0.607 [0.512, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.483, 10.098], loss: 0.001472, mae: 0.041373, mean_q: 1.180920
 49943/100000: episode: 788, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 58.430, mean reward: 0.584 [0.506, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.774, 10.098], loss: 0.001505, mae: 0.041956, mean_q: 1.176059
 50043/100000: episode: 789, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 56.697, mean reward: 0.567 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.597, 10.171], loss: 0.001554, mae: 0.042634, mean_q: 1.176948
 50143/100000: episode: 790, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 61.844, mean reward: 0.618 [0.504, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.314, 10.098], loss: 0.001526, mae: 0.042232, mean_q: 1.173641
 50243/100000: episode: 791, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 59.660, mean reward: 0.597 [0.503, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.274, 10.098], loss: 0.001430, mae: 0.041304, mean_q: 1.175666
 50343/100000: episode: 792, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 57.131, mean reward: 0.571 [0.505, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.157, 10.248], loss: 0.001364, mae: 0.040036, mean_q: 1.177570
 50443/100000: episode: 793, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.373, mean reward: 0.584 [0.506, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.372, 10.170], loss: 0.001508, mae: 0.041496, mean_q: 1.175410
 50543/100000: episode: 794, duration: 0.597s, episode steps: 100, steps per second: 167, episode reward: 59.775, mean reward: 0.598 [0.500, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.919, 10.238], loss: 0.001604, mae: 0.043401, mean_q: 1.176777
 50643/100000: episode: 795, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 59.287, mean reward: 0.593 [0.504, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.767, 10.118], loss: 0.001516, mae: 0.042546, mean_q: 1.175241
 50743/100000: episode: 796, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 58.952, mean reward: 0.590 [0.498, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.064, 10.098], loss: 0.001522, mae: 0.042451, mean_q: 1.174014
 50843/100000: episode: 797, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.789, mean reward: 0.598 [0.506, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.718, 10.098], loss: 0.001490, mae: 0.042278, mean_q: 1.172927
 50943/100000: episode: 798, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.186, mean reward: 0.602 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.794, 10.098], loss: 0.001482, mae: 0.041544, mean_q: 1.174885
 51043/100000: episode: 799, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.699, mean reward: 0.597 [0.501, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.772, 10.145], loss: 0.001502, mae: 0.042246, mean_q: 1.174013
[Info] 1-TH LEVEL FOUND: 1.4348793029785156, Considering 10/90 traces
 51143/100000: episode: 800, duration: 4.720s, episode steps: 100, steps per second: 21, episode reward: 56.913, mean reward: 0.569 [0.506, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.804, 10.098], loss: 0.001384, mae: 0.041092, mean_q: 1.174181
 51167/100000: episode: 801, duration: 0.142s, episode steps: 24, steps per second: 170, episode reward: 16.576, mean reward: 0.691 [0.579, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.471, 10.100], loss: 0.001559, mae: 0.043321, mean_q: 1.170730
 51209/100000: episode: 802, duration: 0.226s, episode steps: 42, steps per second: 186, episode reward: 27.956, mean reward: 0.666 [0.527, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.167, 10.100], loss: 0.001651, mae: 0.043687, mean_q: 1.170768
 51233/100000: episode: 803, duration: 0.153s, episode steps: 24, steps per second: 157, episode reward: 16.189, mean reward: 0.675 [0.624, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.620, 10.100], loss: 0.001344, mae: 0.040058, mean_q: 1.176493
 51253/100000: episode: 804, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 14.615, mean reward: 0.731 [0.692, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.462, 10.100], loss: 0.001511, mae: 0.041349, mean_q: 1.178617
 51278/100000: episode: 805, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 16.595, mean reward: 0.664 [0.540, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.142, 10.100], loss: 0.001554, mae: 0.043079, mean_q: 1.177844
 51302/100000: episode: 806, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 16.673, mean reward: 0.695 [0.630, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.384, 10.100], loss: 0.001385, mae: 0.040844, mean_q: 1.178027
 51340/100000: episode: 807, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 27.598, mean reward: 0.726 [0.588, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.798, 10.348], loss: 0.001619, mae: 0.043898, mean_q: 1.181716
 51360/100000: episode: 808, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 15.017, mean reward: 0.751 [0.693, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.300, 10.100], loss: 0.001934, mae: 0.045869, mean_q: 1.175984
 51402/100000: episode: 809, duration: 0.239s, episode steps: 42, steps per second: 176, episode reward: 26.997, mean reward: 0.643 [0.579, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.653, 10.283], loss: 0.001480, mae: 0.042009, mean_q: 1.182600
 51426/100000: episode: 810, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 17.964, mean reward: 0.749 [0.636, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.090, 10.100], loss: 0.001615, mae: 0.044137, mean_q: 1.176197
 51450/100000: episode: 811, duration: 0.152s, episode steps: 24, steps per second: 158, episode reward: 17.044, mean reward: 0.710 [0.645, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.249, 10.100], loss: 0.001704, mae: 0.044620, mean_q: 1.180339
 51492/100000: episode: 812, duration: 0.238s, episode steps: 42, steps per second: 176, episode reward: 32.659, mean reward: 0.778 [0.698, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.852, 10.100], loss: 0.001444, mae: 0.041996, mean_q: 1.186971
 51516/100000: episode: 813, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 16.507, mean reward: 0.688 [0.614, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.593, 10.411], loss: 0.001555, mae: 0.041808, mean_q: 1.182941
 51545/100000: episode: 814, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 22.810, mean reward: 0.787 [0.653, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.850, 10.100], loss: 0.001993, mae: 0.045443, mean_q: 1.190664
 51559/100000: episode: 815, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 10.111, mean reward: 0.722 [0.655, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.369], loss: 0.002129, mae: 0.046634, mean_q: 1.188216
 51583/100000: episode: 816, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 16.742, mean reward: 0.698 [0.647, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.852, 10.386], loss: 0.001910, mae: 0.046975, mean_q: 1.182009
 51625/100000: episode: 817, duration: 0.242s, episode steps: 42, steps per second: 173, episode reward: 28.902, mean reward: 0.688 [0.582, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.200, 10.242], loss: 0.001699, mae: 0.043633, mean_q: 1.189923
 51645/100000: episode: 818, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 15.345, mean reward: 0.767 [0.703, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.375, 10.100], loss: 0.001602, mae: 0.042502, mean_q: 1.176739
 51674/100000: episode: 819, duration: 0.170s, episode steps: 29, steps per second: 171, episode reward: 20.654, mean reward: 0.712 [0.650, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.428, 10.100], loss: 0.001693, mae: 0.044723, mean_q: 1.199923
 51699/100000: episode: 820, duration: 0.142s, episode steps: 25, steps per second: 175, episode reward: 16.589, mean reward: 0.664 [0.590, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.795, 10.100], loss: 0.001693, mae: 0.043692, mean_q: 1.198578
 51723/100000: episode: 821, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 17.789, mean reward: 0.741 [0.653, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.029, 10.100], loss: 0.001954, mae: 0.044717, mean_q: 1.194508
 51747/100000: episode: 822, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 14.875, mean reward: 0.620 [0.532, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.131, 10.100], loss: 0.001737, mae: 0.043858, mean_q: 1.191149
 51771/100000: episode: 823, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 16.530, mean reward: 0.689 [0.623, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.455, 10.100], loss: 0.001887, mae: 0.045058, mean_q: 1.192596
 51785/100000: episode: 824, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 10.152, mean reward: 0.725 [0.683, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.584], loss: 0.001705, mae: 0.044589, mean_q: 1.189478
 51827/100000: episode: 825, duration: 0.226s, episode steps: 42, steps per second: 186, episode reward: 28.102, mean reward: 0.669 [0.542, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.572, 10.258], loss: 0.001813, mae: 0.046166, mean_q: 1.197404
 51851/100000: episode: 826, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 15.024, mean reward: 0.626 [0.567, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.544, 10.320], loss: 0.001772, mae: 0.044270, mean_q: 1.199667
 51876/100000: episode: 827, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 16.899, mean reward: 0.676 [0.629, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.532, 10.100], loss: 0.001541, mae: 0.042036, mean_q: 1.201351
 51896/100000: episode: 828, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 14.163, mean reward: 0.708 [0.651, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.333, 10.100], loss: 0.002123, mae: 0.049121, mean_q: 1.193897
 51934/100000: episode: 829, duration: 0.230s, episode steps: 38, steps per second: 165, episode reward: 24.015, mean reward: 0.632 [0.515, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.380, 10.195], loss: 0.001832, mae: 0.046396, mean_q: 1.204219
 51963/100000: episode: 830, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 21.067, mean reward: 0.726 [0.682, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.225, 10.100], loss: 0.002048, mae: 0.046749, mean_q: 1.192934
 52001/100000: episode: 831, duration: 0.210s, episode steps: 38, steps per second: 181, episode reward: 26.799, mean reward: 0.705 [0.634, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-1.973, 10.488], loss: 0.001975, mae: 0.047338, mean_q: 1.201056
 52015/100000: episode: 832, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 9.417, mean reward: 0.673 [0.584, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.402, 10.280], loss: 0.001823, mae: 0.044951, mean_q: 1.196111
 52057/100000: episode: 833, duration: 0.223s, episode steps: 42, steps per second: 188, episode reward: 27.895, mean reward: 0.664 [0.600, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.387, 10.100], loss: 0.001709, mae: 0.044156, mean_q: 1.213033
 52077/100000: episode: 834, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 14.359, mean reward: 0.718 [0.666, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.264, 10.100], loss: 0.001825, mae: 0.044681, mean_q: 1.207802
 52119/100000: episode: 835, duration: 0.237s, episode steps: 42, steps per second: 177, episode reward: 26.331, mean reward: 0.627 [0.567, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.163, 10.226], loss: 0.001676, mae: 0.043483, mean_q: 1.202294
 52161/100000: episode: 836, duration: 0.239s, episode steps: 42, steps per second: 176, episode reward: 27.138, mean reward: 0.646 [0.512, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.644, 10.100], loss: 0.001848, mae: 0.045385, mean_q: 1.203347
 52203/100000: episode: 837, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 29.487, mean reward: 0.702 [0.607, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.720, 10.288], loss: 0.001850, mae: 0.045023, mean_q: 1.198705
 52217/100000: episode: 838, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 10.071, mean reward: 0.719 [0.663, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.459, 10.357], loss: 0.001975, mae: 0.046518, mean_q: 1.187793
 52259/100000: episode: 839, duration: 0.230s, episode steps: 42, steps per second: 182, episode reward: 30.674, mean reward: 0.730 [0.565, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.556, 10.100], loss: 0.001729, mae: 0.044684, mean_q: 1.209673
 52283/100000: episode: 840, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 17.560, mean reward: 0.732 [0.675, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.436], loss: 0.001752, mae: 0.043487, mean_q: 1.212823
 52325/100000: episode: 841, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 28.636, mean reward: 0.682 [0.582, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.313, 10.245], loss: 0.001789, mae: 0.045060, mean_q: 1.211249
 52350/100000: episode: 842, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 17.183, mean reward: 0.687 [0.616, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.219, 10.100], loss: 0.001992, mae: 0.046620, mean_q: 1.210951
 52374/100000: episode: 843, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 16.903, mean reward: 0.704 [0.663, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.775, 10.100], loss: 0.001501, mae: 0.041930, mean_q: 1.216330
 52388/100000: episode: 844, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 9.947, mean reward: 0.710 [0.688, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.427], loss: 0.001564, mae: 0.042727, mean_q: 1.213172
 52430/100000: episode: 845, duration: 0.249s, episode steps: 42, steps per second: 169, episode reward: 27.874, mean reward: 0.664 [0.522, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.471, 10.230], loss: 0.001707, mae: 0.044564, mean_q: 1.210831
 52454/100000: episode: 846, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 16.790, mean reward: 0.700 [0.618, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.404, 10.100], loss: 0.002120, mae: 0.048016, mean_q: 1.212298
 52496/100000: episode: 847, duration: 0.241s, episode steps: 42, steps per second: 175, episode reward: 28.998, mean reward: 0.690 [0.584, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.432, 10.289], loss: 0.001781, mae: 0.044640, mean_q: 1.213287
 52520/100000: episode: 848, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 15.971, mean reward: 0.665 [0.620, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-1.089, 10.100], loss: 0.001772, mae: 0.044966, mean_q: 1.216148
 52544/100000: episode: 849, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 18.568, mean reward: 0.774 [0.670, 0.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.415, 10.100], loss: 0.001738, mae: 0.043685, mean_q: 1.214944
 52582/100000: episode: 850, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 27.638, mean reward: 0.727 [0.533, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-1.328, 10.291], loss: 0.001597, mae: 0.043512, mean_q: 1.215238
 52624/100000: episode: 851, duration: 0.231s, episode steps: 42, steps per second: 182, episode reward: 31.115, mean reward: 0.741 [0.637, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.365, 10.517], loss: 0.001592, mae: 0.042372, mean_q: 1.218941
 52644/100000: episode: 852, duration: 0.130s, episode steps: 20, steps per second: 153, episode reward: 14.466, mean reward: 0.723 [0.689, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.748, 10.100], loss: 0.001790, mae: 0.045617, mean_q: 1.224951
 52686/100000: episode: 853, duration: 0.212s, episode steps: 42, steps per second: 198, episode reward: 27.596, mean reward: 0.657 [0.525, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.194, 10.100], loss: 0.001632, mae: 0.042521, mean_q: 1.225052
 52710/100000: episode: 854, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 15.624, mean reward: 0.651 [0.569, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.889, 10.320], loss: 0.002011, mae: 0.047084, mean_q: 1.222209
 52748/100000: episode: 855, duration: 0.216s, episode steps: 38, steps per second: 176, episode reward: 28.258, mean reward: 0.744 [0.638, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.291, 10.508], loss: 0.001780, mae: 0.045442, mean_q: 1.228811
 52790/100000: episode: 856, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 28.325, mean reward: 0.674 [0.546, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.382, 10.300], loss: 0.001739, mae: 0.044206, mean_q: 1.227673
 52804/100000: episode: 857, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.934, mean reward: 0.781 [0.724, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.216, 10.465], loss: 0.002039, mae: 0.048249, mean_q: 1.224882
 52846/100000: episode: 858, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 28.168, mean reward: 0.671 [0.599, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.170, 10.356], loss: 0.001824, mae: 0.045794, mean_q: 1.218890
 52860/100000: episode: 859, duration: 0.105s, episode steps: 14, steps per second: 134, episode reward: 10.203, mean reward: 0.729 [0.676, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.444], loss: 0.001694, mae: 0.045379, mean_q: 1.225786
 52884/100000: episode: 860, duration: 0.176s, episode steps: 24, steps per second: 136, episode reward: 17.544, mean reward: 0.731 [0.637, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.422, 10.100], loss: 0.001960, mae: 0.046575, mean_q: 1.235557
 52926/100000: episode: 861, duration: 0.248s, episode steps: 42, steps per second: 169, episode reward: 33.064, mean reward: 0.787 [0.672, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.332, 10.513], loss: 0.001743, mae: 0.044812, mean_q: 1.233941
 52968/100000: episode: 862, duration: 0.250s, episode steps: 42, steps per second: 168, episode reward: 27.303, mean reward: 0.650 [0.554, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.600, 10.100], loss: 0.001914, mae: 0.045782, mean_q: 1.224574
 52992/100000: episode: 863, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 16.849, mean reward: 0.702 [0.598, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.323, 10.100], loss: 0.001829, mae: 0.046587, mean_q: 1.235533
 53034/100000: episode: 864, duration: 0.248s, episode steps: 42, steps per second: 169, episode reward: 30.657, mean reward: 0.730 [0.618, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.963, 10.100], loss: 0.002136, mae: 0.047933, mean_q: 1.231313
 53048/100000: episode: 865, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 10.718, mean reward: 0.766 [0.704, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.421], loss: 0.001706, mae: 0.043965, mean_q: 1.232327
 53072/100000: episode: 866, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 17.752, mean reward: 0.740 [0.664, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.573, 10.100], loss: 0.001684, mae: 0.045323, mean_q: 1.239534
 53114/100000: episode: 867, duration: 0.257s, episode steps: 42, steps per second: 163, episode reward: 27.867, mean reward: 0.664 [0.519, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-1.179, 10.100], loss: 0.001951, mae: 0.046357, mean_q: 1.239983
 53152/100000: episode: 868, duration: 0.224s, episode steps: 38, steps per second: 170, episode reward: 25.342, mean reward: 0.667 [0.537, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.423, 10.210], loss: 0.001849, mae: 0.045742, mean_q: 1.240708
 53177/100000: episode: 869, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 19.386, mean reward: 0.775 [0.692, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.497, 10.100], loss: 0.001774, mae: 0.044475, mean_q: 1.239795
 53219/100000: episode: 870, duration: 0.249s, episode steps: 42, steps per second: 169, episode reward: 27.007, mean reward: 0.643 [0.541, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.685, 10.210], loss: 0.002180, mae: 0.049273, mean_q: 1.235683
 53261/100000: episode: 871, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 27.290, mean reward: 0.650 [0.562, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.674, 10.353], loss: 0.002017, mae: 0.046866, mean_q: 1.239969
 53303/100000: episode: 872, duration: 0.239s, episode steps: 42, steps per second: 176, episode reward: 28.455, mean reward: 0.678 [0.547, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.155, 10.100], loss: 0.001772, mae: 0.044497, mean_q: 1.249849
 53328/100000: episode: 873, duration: 0.206s, episode steps: 25, steps per second: 122, episode reward: 17.720, mean reward: 0.709 [0.639, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.215, 10.100], loss: 0.001928, mae: 0.046241, mean_q: 1.250650
 53370/100000: episode: 874, duration: 0.389s, episode steps: 42, steps per second: 108, episode reward: 26.050, mean reward: 0.620 [0.540, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.673, 10.276], loss: 0.002103, mae: 0.048101, mean_q: 1.242727
 53399/100000: episode: 875, duration: 0.206s, episode steps: 29, steps per second: 141, episode reward: 22.481, mean reward: 0.775 [0.707, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.257, 10.100], loss: 0.001861, mae: 0.046439, mean_q: 1.241928
 53428/100000: episode: 876, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 21.972, mean reward: 0.758 [0.668, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.719, 10.100], loss: 0.001698, mae: 0.044051, mean_q: 1.248479
 53452/100000: episode: 877, duration: 0.168s, episode steps: 24, steps per second: 143, episode reward: 16.671, mean reward: 0.695 [0.645, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-1.289, 10.388], loss: 0.001639, mae: 0.043236, mean_q: 1.255765
 53494/100000: episode: 878, duration: 0.233s, episode steps: 42, steps per second: 180, episode reward: 32.573, mean reward: 0.776 [0.673, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-1.433, 10.100], loss: 0.001878, mae: 0.046822, mean_q: 1.244346
 53508/100000: episode: 879, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 9.872, mean reward: 0.705 [0.660, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.513, 10.458], loss: 0.001883, mae: 0.046118, mean_q: 1.250593
 53532/100000: episode: 880, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 18.698, mean reward: 0.779 [0.691, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.163, 10.100], loss: 0.001955, mae: 0.046705, mean_q: 1.256440
 53546/100000: episode: 881, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 9.228, mean reward: 0.659 [0.623, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.422], loss: 0.001672, mae: 0.044581, mean_q: 1.264501
 53571/100000: episode: 882, duration: 0.156s, episode steps: 25, steps per second: 160, episode reward: 17.863, mean reward: 0.715 [0.604, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.508, 10.100], loss: 0.001810, mae: 0.044842, mean_q: 1.260446
 53609/100000: episode: 883, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 24.940, mean reward: 0.656 [0.574, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.551, 10.321], loss: 0.001955, mae: 0.047276, mean_q: 1.264024
 53651/100000: episode: 884, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 27.063, mean reward: 0.644 [0.553, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.560, 10.100], loss: 0.001939, mae: 0.046465, mean_q: 1.262501
 53693/100000: episode: 885, duration: 0.233s, episode steps: 42, steps per second: 181, episode reward: 29.251, mean reward: 0.696 [0.603, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.716, 10.528], loss: 0.001787, mae: 0.044952, mean_q: 1.269892
 53722/100000: episode: 886, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 21.725, mean reward: 0.749 [0.672, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.414, 10.100], loss: 0.002033, mae: 0.046289, mean_q: 1.261471
 53742/100000: episode: 887, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 13.427, mean reward: 0.671 [0.544, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.577, 10.100], loss: 0.001808, mae: 0.045984, mean_q: 1.266150
 53784/100000: episode: 888, duration: 0.264s, episode steps: 42, steps per second: 159, episode reward: 26.751, mean reward: 0.637 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.196, 10.146], loss: 0.002012, mae: 0.047904, mean_q: 1.271551
 53826/100000: episode: 889, duration: 0.270s, episode steps: 42, steps per second: 156, episode reward: 28.535, mean reward: 0.679 [0.586, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.195, 10.241], loss: 0.002014, mae: 0.047564, mean_q: 1.269102
[Info] 2-TH LEVEL FOUND: 1.5999317169189453, Considering 10/90 traces
 53850/100000: episode: 890, duration: 4.595s, episode steps: 24, steps per second: 5, episode reward: 16.215, mean reward: 0.676 [0.623, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.358, 10.372], loss: 0.001969, mae: 0.047440, mean_q: 1.262183
 53863/100000: episode: 891, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 10.910, mean reward: 0.839 [0.808, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.503, 10.100], loss: 0.002798, mae: 0.053999, mean_q: 1.284656
 53900/100000: episode: 892, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 26.021, mean reward: 0.703 [0.547, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-1.252, 10.264], loss: 0.002001, mae: 0.047313, mean_q: 1.276336
 53913/100000: episode: 893, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 11.059, mean reward: 0.851 [0.767, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.760, 10.100], loss: 0.002172, mae: 0.048189, mean_q: 1.273776
 53944/100000: episode: 894, duration: 0.193s, episode steps: 31, steps per second: 161, episode reward: 23.720, mean reward: 0.765 [0.689, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.046, 10.601], loss: 0.001700, mae: 0.044078, mean_q: 1.274109
 53979/100000: episode: 895, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 26.867, mean reward: 0.768 [0.707, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.772, 10.555], loss: 0.002044, mae: 0.048907, mean_q: 1.267723
 53992/100000: episode: 896, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 11.160, mean reward: 0.858 [0.788, 0.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.625, 10.100], loss: 0.001933, mae: 0.045643, mean_q: 1.270540
 54027/100000: episode: 897, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 26.643, mean reward: 0.761 [0.662, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.923, 10.469], loss: 0.001892, mae: 0.046255, mean_q: 1.281360
 54058/100000: episode: 898, duration: 0.197s, episode steps: 31, steps per second: 157, episode reward: 23.921, mean reward: 0.772 [0.596, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.467, 10.415], loss: 0.001994, mae: 0.047092, mean_q: 1.280269
 54071/100000: episode: 899, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 10.939, mean reward: 0.841 [0.785, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.605, 10.100], loss: 0.001807, mae: 0.044356, mean_q: 1.297556
 54102/100000: episode: 900, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 24.737, mean reward: 0.798 [0.647, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.035, 10.399], loss: 0.001688, mae: 0.044097, mean_q: 1.289172
[Info] FALSIFICATION!
 54120/100000: episode: 901, duration: 0.366s, episode steps: 18, steps per second: 49, episode reward: 14.428, mean reward: 0.802 [0.701, 1.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.024, 10.513], loss: 0.001925, mae: 0.047428, mean_q: 1.285043
 54155/100000: episode: 902, duration: 0.222s, episode steps: 35, steps per second: 157, episode reward: 27.400, mean reward: 0.783 [0.696, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.681, 10.550], loss: 0.001909, mae: 0.047042, mean_q: 1.287825
 54188/100000: episode: 903, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 25.017, mean reward: 0.758 [0.677, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.585, 10.545], loss: 0.001952, mae: 0.046719, mean_q: 1.292393
 54223/100000: episode: 904, duration: 0.207s, episode steps: 35, steps per second: 169, episode reward: 26.376, mean reward: 0.754 [0.649, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.605, 10.355], loss: 0.002081, mae: 0.048815, mean_q: 1.292797
 54245/100000: episode: 905, duration: 0.132s, episode steps: 22, steps per second: 166, episode reward: 17.042, mean reward: 0.775 [0.702, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.458, 10.100], loss: 0.002139, mae: 0.048496, mean_q: 1.281247
 54260/100000: episode: 906, duration: 0.091s, episode steps: 15, steps per second: 166, episode reward: 11.984, mean reward: 0.799 [0.710, 0.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.759, 10.590], loss: 0.001903, mae: 0.045141, mean_q: 1.291144
 54289/100000: episode: 907, duration: 0.166s, episode steps: 29, steps per second: 174, episode reward: 20.475, mean reward: 0.706 [0.554, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.618, 10.203], loss: 0.001808, mae: 0.045482, mean_q: 1.292957
 54324/100000: episode: 908, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 24.274, mean reward: 0.694 [0.580, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.035, 10.298], loss: 0.001735, mae: 0.045242, mean_q: 1.298484
 54361/100000: episode: 909, duration: 0.215s, episode steps: 37, steps per second: 172, episode reward: 28.266, mean reward: 0.764 [0.698, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.317, 10.459], loss: 0.001917, mae: 0.046806, mean_q: 1.305702
 54383/100000: episode: 910, duration: 0.130s, episode steps: 22, steps per second: 170, episode reward: 17.412, mean reward: 0.791 [0.683, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.291, 10.100], loss: 0.002181, mae: 0.048194, mean_q: 1.295152
 54416/100000: episode: 911, duration: 0.227s, episode steps: 33, steps per second: 145, episode reward: 21.399, mean reward: 0.648 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.170, 10.100], loss: 0.002291, mae: 0.049656, mean_q: 1.305769
 54447/100000: episode: 912, duration: 0.188s, episode steps: 31, steps per second: 165, episode reward: 24.535, mean reward: 0.791 [0.678, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.276, 10.482], loss: 0.002020, mae: 0.046995, mean_q: 1.306656
 54478/100000: episode: 913, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 23.519, mean reward: 0.759 [0.651, 0.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.159, 10.420], loss: 0.001539, mae: 0.043510, mean_q: 1.300567
 54507/100000: episode: 914, duration: 0.171s, episode steps: 29, steps per second: 170, episode reward: 22.285, mean reward: 0.768 [0.646, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.747, 10.560], loss: 0.001958, mae: 0.046760, mean_q: 1.321512
 54538/100000: episode: 915, duration: 0.166s, episode steps: 31, steps per second: 186, episode reward: 25.908, mean reward: 0.836 [0.758, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.699, 10.493], loss: 0.001955, mae: 0.047230, mean_q: 1.322711
 54573/100000: episode: 916, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 26.025, mean reward: 0.744 [0.638, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-1.177, 10.440], loss: 0.001868, mae: 0.045032, mean_q: 1.314941
 54602/100000: episode: 917, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 20.337, mean reward: 0.701 [0.596, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.321, 10.317], loss: 0.002175, mae: 0.050072, mean_q: 1.322628
 54639/100000: episode: 918, duration: 0.231s, episode steps: 37, steps per second: 160, episode reward: 26.015, mean reward: 0.703 [0.540, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.842, 10.245], loss: 0.001987, mae: 0.047740, mean_q: 1.318141
 54668/100000: episode: 919, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 21.469, mean reward: 0.740 [0.661, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.433, 10.461], loss: 0.001903, mae: 0.046153, mean_q: 1.319850
 54690/100000: episode: 920, duration: 0.142s, episode steps: 22, steps per second: 155, episode reward: 16.543, mean reward: 0.752 [0.627, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.086, 10.100], loss: 0.002651, mae: 0.054680, mean_q: 1.313041
 54722/100000: episode: 921, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 20.126, mean reward: 0.629 [0.514, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.172, 10.156], loss: 0.001791, mae: 0.045401, mean_q: 1.325892
 54759/100000: episode: 922, duration: 0.215s, episode steps: 37, steps per second: 172, episode reward: 24.657, mean reward: 0.666 [0.541, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.382, 10.394], loss: 0.001861, mae: 0.046440, mean_q: 1.324100
 54794/100000: episode: 923, duration: 0.197s, episode steps: 35, steps per second: 178, episode reward: 25.445, mean reward: 0.727 [0.636, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.338, 10.454], loss: 0.002550, mae: 0.050512, mean_q: 1.315047
 54807/100000: episode: 924, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 10.031, mean reward: 0.772 [0.713, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.383, 10.100], loss: 0.001767, mae: 0.046710, mean_q: 1.331231
 54829/100000: episode: 925, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 18.281, mean reward: 0.831 [0.757, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.089, 10.100], loss: 0.002206, mae: 0.047646, mean_q: 1.315507
 54861/100000: episode: 926, duration: 0.203s, episode steps: 32, steps per second: 158, episode reward: 22.105, mean reward: 0.691 [0.583, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.684, 10.436], loss: 0.002096, mae: 0.047586, mean_q: 1.327187
 54890/100000: episode: 927, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 21.962, mean reward: 0.757 [0.656, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.424], loss: 0.001853, mae: 0.046542, mean_q: 1.322272
 54927/100000: episode: 928, duration: 0.220s, episode steps: 37, steps per second: 168, episode reward: 25.231, mean reward: 0.682 [0.510, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.269, 10.149], loss: 0.001791, mae: 0.044727, mean_q: 1.321959
 54962/100000: episode: 929, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 23.432, mean reward: 0.669 [0.566, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.617, 10.456], loss: 0.001815, mae: 0.045454, mean_q: 1.333127
 54999/100000: episode: 930, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 25.046, mean reward: 0.677 [0.549, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.234, 10.260], loss: 0.001739, mae: 0.043968, mean_q: 1.330448
 55021/100000: episode: 931, duration: 0.139s, episode steps: 22, steps per second: 158, episode reward: 17.807, mean reward: 0.809 [0.755, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.690, 10.100], loss: 0.001482, mae: 0.041794, mean_q: 1.334383
 55058/100000: episode: 932, duration: 0.230s, episode steps: 37, steps per second: 161, episode reward: 25.399, mean reward: 0.686 [0.523, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.962, 10.170], loss: 0.001769, mae: 0.044959, mean_q: 1.342813
 55087/100000: episode: 933, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 21.247, mean reward: 0.733 [0.664, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.922, 10.402], loss: 0.001618, mae: 0.045514, mean_q: 1.336879
 55100/100000: episode: 934, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 10.396, mean reward: 0.800 [0.749, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.348, 10.100], loss: 0.001681, mae: 0.045325, mean_q: 1.340681
 55137/100000: episode: 935, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 26.516, mean reward: 0.717 [0.576, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.394, 10.254], loss: 0.001894, mae: 0.046423, mean_q: 1.342345
 55172/100000: episode: 936, duration: 0.207s, episode steps: 35, steps per second: 169, episode reward: 23.727, mean reward: 0.678 [0.573, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-1.446, 10.338], loss: 0.001710, mae: 0.044551, mean_q: 1.335301
 55207/100000: episode: 937, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 25.335, mean reward: 0.724 [0.635, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.411, 10.399], loss: 0.001972, mae: 0.048382, mean_q: 1.334211
 55240/100000: episode: 938, duration: 0.198s, episode steps: 33, steps per second: 167, episode reward: 25.299, mean reward: 0.767 [0.660, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.483, 10.381], loss: 0.001772, mae: 0.045549, mean_q: 1.336475
 55255/100000: episode: 939, duration: 0.096s, episode steps: 15, steps per second: 157, episode reward: 12.308, mean reward: 0.821 [0.734, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-1.086, 10.588], loss: 0.001767, mae: 0.044475, mean_q: 1.352877
 55277/100000: episode: 940, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 18.565, mean reward: 0.844 [0.784, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.440, 10.100], loss: 0.002161, mae: 0.046490, mean_q: 1.338852
 55299/100000: episode: 941, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 18.307, mean reward: 0.832 [0.762, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.033, 10.100], loss: 0.001794, mae: 0.045978, mean_q: 1.347173
 55331/100000: episode: 942, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 22.124, mean reward: 0.691 [0.640, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.118, 10.368], loss: 0.002022, mae: 0.048040, mean_q: 1.352048
 55364/100000: episode: 943, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 23.910, mean reward: 0.725 [0.661, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.596, 10.468], loss: 0.001944, mae: 0.045974, mean_q: 1.346313
 55399/100000: episode: 944, duration: 0.206s, episode steps: 35, steps per second: 170, episode reward: 25.039, mean reward: 0.715 [0.651, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.343, 10.586], loss: 0.001774, mae: 0.044665, mean_q: 1.347299
 55432/100000: episode: 945, duration: 0.202s, episode steps: 33, steps per second: 163, episode reward: 23.223, mean reward: 0.704 [0.598, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.066, 10.307], loss: 0.001626, mae: 0.044764, mean_q: 1.356099
 55454/100000: episode: 946, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 16.385, mean reward: 0.745 [0.692, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.409, 10.100], loss: 0.001550, mae: 0.043387, mean_q: 1.353794
 55487/100000: episode: 947, duration: 0.204s, episode steps: 33, steps per second: 162, episode reward: 23.109, mean reward: 0.700 [0.572, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.035, 10.291], loss: 0.001685, mae: 0.044330, mean_q: 1.365905
 55524/100000: episode: 948, duration: 0.230s, episode steps: 37, steps per second: 161, episode reward: 28.254, mean reward: 0.764 [0.617, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.281, 10.395], loss: 0.001936, mae: 0.048596, mean_q: 1.356344
 55559/100000: episode: 949, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 24.790, mean reward: 0.708 [0.597, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.312, 10.306], loss: 0.001762, mae: 0.044941, mean_q: 1.364209
 55590/100000: episode: 950, duration: 0.191s, episode steps: 31, steps per second: 162, episode reward: 22.570, mean reward: 0.728 [0.601, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.557, 10.188], loss: 0.002133, mae: 0.050870, mean_q: 1.358749
 55627/100000: episode: 951, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 24.008, mean reward: 0.649 [0.522, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.709, 10.100], loss: 0.001810, mae: 0.046769, mean_q: 1.350987
 55659/100000: episode: 952, duration: 0.187s, episode steps: 32, steps per second: 171, episode reward: 22.998, mean reward: 0.719 [0.582, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.204, 10.388], loss: 0.001695, mae: 0.045729, mean_q: 1.371426
 55692/100000: episode: 953, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 24.364, mean reward: 0.738 [0.665, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.559, 10.402], loss: 0.001818, mae: 0.045356, mean_q: 1.349984
 55724/100000: episode: 954, duration: 0.177s, episode steps: 32, steps per second: 180, episode reward: 21.894, mean reward: 0.684 [0.610, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.501, 10.416], loss: 0.001738, mae: 0.044909, mean_q: 1.373376
 55737/100000: episode: 955, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 11.118, mean reward: 0.855 [0.781, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.922, 10.100], loss: 0.001827, mae: 0.046429, mean_q: 1.373415
 55774/100000: episode: 956, duration: 0.212s, episode steps: 37, steps per second: 174, episode reward: 25.401, mean reward: 0.687 [0.528, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.109, 10.200], loss: 0.001999, mae: 0.047791, mean_q: 1.371988
 55787/100000: episode: 957, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 10.239, mean reward: 0.788 [0.721, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.393, 10.100], loss: 0.001774, mae: 0.046572, mean_q: 1.361787
 55802/100000: episode: 958, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 10.311, mean reward: 0.687 [0.630, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.403], loss: 0.001795, mae: 0.045512, mean_q: 1.369356
 55824/100000: episode: 959, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 18.674, mean reward: 0.849 [0.781, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.429, 10.100], loss: 0.002285, mae: 0.049370, mean_q: 1.373237
 55853/100000: episode: 960, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 21.439, mean reward: 0.739 [0.628, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.508, 10.400], loss: 0.001787, mae: 0.045500, mean_q: 1.358169
 55886/100000: episode: 961, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 26.977, mean reward: 0.817 [0.722, 0.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.951, 10.479], loss: 0.001665, mae: 0.045835, mean_q: 1.375373
 55899/100000: episode: 962, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 10.474, mean reward: 0.806 [0.742, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.355, 10.100], loss: 0.001673, mae: 0.045565, mean_q: 1.381835
 55914/100000: episode: 963, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 12.118, mean reward: 0.808 [0.749, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.476, 10.647], loss: 0.001656, mae: 0.044061, mean_q: 1.370647
 55927/100000: episode: 964, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 10.042, mean reward: 0.772 [0.734, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.820, 10.100], loss: 0.001558, mae: 0.042748, mean_q: 1.358958
 55940/100000: episode: 965, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 9.610, mean reward: 0.739 [0.646, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.414, 10.100], loss: 0.001835, mae: 0.046624, mean_q: 1.362867
 55969/100000: episode: 966, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 20.900, mean reward: 0.721 [0.630, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.469], loss: 0.001586, mae: 0.043371, mean_q: 1.372515
 56006/100000: episode: 967, duration: 0.211s, episode steps: 37, steps per second: 176, episode reward: 26.289, mean reward: 0.711 [0.545, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.551, 10.272], loss: 0.001639, mae: 0.044492, mean_q: 1.379229
 56021/100000: episode: 968, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 12.388, mean reward: 0.826 [0.761, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.839, 10.593], loss: 0.002799, mae: 0.052377, mean_q: 1.393130
 56043/100000: episode: 969, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 19.247, mean reward: 0.875 [0.791, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.682, 10.100], loss: 0.001579, mae: 0.043658, mean_q: 1.377635
 56065/100000: episode: 970, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 16.432, mean reward: 0.747 [0.655, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.362, 10.100], loss: 0.001449, mae: 0.041379, mean_q: 1.377123
 56096/100000: episode: 971, duration: 0.183s, episode steps: 31, steps per second: 170, episode reward: 21.356, mean reward: 0.689 [0.530, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.194, 10.218], loss: 0.002151, mae: 0.048644, mean_q: 1.383428
 56133/100000: episode: 972, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 24.528, mean reward: 0.663 [0.544, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.865, 10.224], loss: 0.002381, mae: 0.052132, mean_q: 1.387361
 56146/100000: episode: 973, duration: 0.089s, episode steps: 13, steps per second: 146, episode reward: 11.155, mean reward: 0.858 [0.787, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.420, 10.100], loss: 0.002215, mae: 0.052223, mean_q: 1.372619
 56181/100000: episode: 974, duration: 0.208s, episode steps: 35, steps per second: 168, episode reward: 28.913, mean reward: 0.826 [0.728, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.517, 10.533], loss: 0.002006, mae: 0.047485, mean_q: 1.374025
 56214/100000: episode: 975, duration: 0.209s, episode steps: 33, steps per second: 158, episode reward: 25.106, mean reward: 0.761 [0.666, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.321, 10.548], loss: 0.001741, mae: 0.045370, mean_q: 1.376383
 56251/100000: episode: 976, duration: 0.204s, episode steps: 37, steps per second: 182, episode reward: 30.393, mean reward: 0.821 [0.736, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.993, 10.464], loss: 0.001882, mae: 0.048261, mean_q: 1.386616
 56280/100000: episode: 977, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 22.559, mean reward: 0.778 [0.682, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.693, 10.493], loss: 0.001793, mae: 0.046912, mean_q: 1.384868
 56313/100000: episode: 978, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 24.042, mean reward: 0.729 [0.602, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.054, 10.325], loss: 0.002254, mae: 0.050871, mean_q: 1.387937
 56342/100000: episode: 979, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 20.140, mean reward: 0.694 [0.565, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.585, 10.318], loss: 0.001840, mae: 0.047011, mean_q: 1.389713
[Info] Complete ISplit Iteration
[Info] Levels: [1.4348793, 1.5999317, 1.6507273]
[Info] Cond. Prob: [0.1, 0.1, 0.26]
[Info] Error Prob: 0.0026000000000000007

 56355/100000: episode: 980, duration: 4.638s, episode steps: 13, steps per second: 3, episode reward: 10.698, mean reward: 0.823 [0.744, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.358, 10.100], loss: 0.001293, mae: 0.040058, mean_q: 1.401920
 56455/100000: episode: 981, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 57.853, mean reward: 0.579 [0.498, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.295, 10.124], loss: 0.001977, mae: 0.048034, mean_q: 1.388761
 56555/100000: episode: 982, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.936, mean reward: 0.589 [0.503, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.650, 10.193], loss: 0.001839, mae: 0.046532, mean_q: 1.378955
 56655/100000: episode: 983, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.759, mean reward: 0.578 [0.507, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.717, 10.147], loss: 0.001736, mae: 0.045272, mean_q: 1.377639
 56755/100000: episode: 984, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 60.916, mean reward: 0.609 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.829, 10.098], loss: 0.001904, mae: 0.047605, mean_q: 1.374165
 56855/100000: episode: 985, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.229, mean reward: 0.582 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.881, 10.098], loss: 0.002053, mae: 0.047320, mean_q: 1.375722
 56955/100000: episode: 986, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 58.642, mean reward: 0.586 [0.516, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.940, 10.226], loss: 0.001813, mae: 0.045498, mean_q: 1.363937
 57055/100000: episode: 987, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 58.532, mean reward: 0.585 [0.514, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.796, 10.261], loss: 0.002024, mae: 0.046710, mean_q: 1.361915
 57155/100000: episode: 988, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 56.948, mean reward: 0.569 [0.499, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.541, 10.098], loss: 0.001681, mae: 0.044922, mean_q: 1.360949
 57255/100000: episode: 989, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 61.154, mean reward: 0.612 [0.502, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.326, 10.098], loss: 0.002069, mae: 0.048034, mean_q: 1.357997
 57355/100000: episode: 990, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.506, mean reward: 0.595 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.246, 10.294], loss: 0.001758, mae: 0.045425, mean_q: 1.352281
 57455/100000: episode: 991, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 59.963, mean reward: 0.600 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.659, 10.460], loss: 0.001867, mae: 0.046113, mean_q: 1.354017
 57555/100000: episode: 992, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 60.224, mean reward: 0.602 [0.514, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.767, 10.548], loss: 0.001783, mae: 0.046143, mean_q: 1.343605
 57655/100000: episode: 993, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 60.439, mean reward: 0.604 [0.511, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.267, 10.207], loss: 0.001722, mae: 0.045341, mean_q: 1.350310
 57755/100000: episode: 994, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 58.401, mean reward: 0.584 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.978, 10.335], loss: 0.002181, mae: 0.048642, mean_q: 1.332425
 57855/100000: episode: 995, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.111, mean reward: 0.581 [0.498, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.322, 10.098], loss: 0.002172, mae: 0.049412, mean_q: 1.329179
 57955/100000: episode: 996, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.649, mean reward: 0.576 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.674, 10.111], loss: 0.001650, mae: 0.044304, mean_q: 1.333280
 58055/100000: episode: 997, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.706, mean reward: 0.587 [0.502, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.317, 10.098], loss: 0.001885, mae: 0.046121, mean_q: 1.325853
 58155/100000: episode: 998, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 61.264, mean reward: 0.613 [0.509, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.747, 10.277], loss: 0.001666, mae: 0.043973, mean_q: 1.325481
 58255/100000: episode: 999, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.859, mean reward: 0.579 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.013, 10.226], loss: 0.002011, mae: 0.048011, mean_q: 1.323853
 58355/100000: episode: 1000, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 61.891, mean reward: 0.619 [0.500, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.709, 10.321], loss: 0.001852, mae: 0.045860, mean_q: 1.314821
 58455/100000: episode: 1001, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 61.599, mean reward: 0.616 [0.529, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.290, 10.098], loss: 0.001876, mae: 0.046061, mean_q: 1.309187
 58555/100000: episode: 1002, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 61.419, mean reward: 0.614 [0.512, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.752, 10.245], loss: 0.001690, mae: 0.043832, mean_q: 1.313130
 58655/100000: episode: 1003, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.235, mean reward: 0.582 [0.503, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.558, 10.345], loss: 0.001979, mae: 0.047207, mean_q: 1.307070
 58755/100000: episode: 1004, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 57.931, mean reward: 0.579 [0.501, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.630, 10.098], loss: 0.001675, mae: 0.044432, mean_q: 1.307213
 58855/100000: episode: 1005, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 58.441, mean reward: 0.584 [0.503, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.421, 10.144], loss: 0.001735, mae: 0.044393, mean_q: 1.298383
 58955/100000: episode: 1006, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 64.068, mean reward: 0.641 [0.511, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.371, 10.098], loss: 0.001943, mae: 0.045849, mean_q: 1.291241
 59055/100000: episode: 1007, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.380, mean reward: 0.584 [0.500, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.524, 10.098], loss: 0.001596, mae: 0.043846, mean_q: 1.285042
 59155/100000: episode: 1008, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 56.954, mean reward: 0.570 [0.508, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.680, 10.212], loss: 0.001805, mae: 0.045855, mean_q: 1.286316
 59255/100000: episode: 1009, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.207, mean reward: 0.582 [0.507, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.999, 10.102], loss: 0.001819, mae: 0.046062, mean_q: 1.272470
 59355/100000: episode: 1010, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.263, mean reward: 0.583 [0.514, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.922, 10.142], loss: 0.001671, mae: 0.044526, mean_q: 1.266767
 59455/100000: episode: 1011, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 62.030, mean reward: 0.620 [0.518, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.196, 10.152], loss: 0.001665, mae: 0.043999, mean_q: 1.268742
 59555/100000: episode: 1012, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.634, mean reward: 0.586 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.488, 10.235], loss: 0.001718, mae: 0.045209, mean_q: 1.265312
 59655/100000: episode: 1013, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.522, mean reward: 0.605 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.400, 10.457], loss: 0.001618, mae: 0.043162, mean_q: 1.253920
 59755/100000: episode: 1014, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 58.029, mean reward: 0.580 [0.500, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.332, 10.098], loss: 0.001623, mae: 0.043460, mean_q: 1.250954
 59855/100000: episode: 1015, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 63.310, mean reward: 0.633 [0.520, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.537, 10.276], loss: 0.001558, mae: 0.042789, mean_q: 1.249056
 59955/100000: episode: 1016, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.864, mean reward: 0.599 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.308, 10.098], loss: 0.001697, mae: 0.044313, mean_q: 1.243475
 60055/100000: episode: 1017, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.289, mean reward: 0.583 [0.506, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.013, 10.242], loss: 0.001647, mae: 0.044529, mean_q: 1.242500
 60155/100000: episode: 1018, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 61.562, mean reward: 0.616 [0.511, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.498, 10.564], loss: 0.001698, mae: 0.044007, mean_q: 1.231835
 60255/100000: episode: 1019, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 58.663, mean reward: 0.587 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.907, 10.201], loss: 0.001862, mae: 0.045786, mean_q: 1.233325
 60355/100000: episode: 1020, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 57.970, mean reward: 0.580 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.942, 10.226], loss: 0.001673, mae: 0.044217, mean_q: 1.225662
 60455/100000: episode: 1021, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.649, mean reward: 0.596 [0.503, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.756, 10.269], loss: 0.001691, mae: 0.043952, mean_q: 1.228849
 60555/100000: episode: 1022, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 56.700, mean reward: 0.567 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.215, 10.207], loss: 0.001622, mae: 0.042771, mean_q: 1.212171
 60655/100000: episode: 1023, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 56.921, mean reward: 0.569 [0.506, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.096, 10.098], loss: 0.001714, mae: 0.043802, mean_q: 1.208094
 60755/100000: episode: 1024, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 60.346, mean reward: 0.603 [0.500, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.791, 10.098], loss: 0.001662, mae: 0.043400, mean_q: 1.202878
 60855/100000: episode: 1025, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.800, mean reward: 0.588 [0.502, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.862, 10.098], loss: 0.001589, mae: 0.043006, mean_q: 1.199757
 60955/100000: episode: 1026, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 60.186, mean reward: 0.602 [0.503, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.760, 10.273], loss: 0.001741, mae: 0.045874, mean_q: 1.190327
 61055/100000: episode: 1027, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.072, mean reward: 0.581 [0.499, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.528, 10.098], loss: 0.001579, mae: 0.042645, mean_q: 1.185383
 61155/100000: episode: 1028, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 61.695, mean reward: 0.617 [0.512, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.472, 10.098], loss: 0.001488, mae: 0.042041, mean_q: 1.186267
 61255/100000: episode: 1029, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.220, mean reward: 0.592 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.463, 10.098], loss: 0.001511, mae: 0.041794, mean_q: 1.179263
 61355/100000: episode: 1030, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 58.376, mean reward: 0.584 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.527, 10.098], loss: 0.001466, mae: 0.041933, mean_q: 1.173086
 61455/100000: episode: 1031, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.896, mean reward: 0.589 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.456, 10.184], loss: 0.001436, mae: 0.041521, mean_q: 1.177174
 61555/100000: episode: 1032, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 61.269, mean reward: 0.613 [0.512, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.148, 10.098], loss: 0.001470, mae: 0.042001, mean_q: 1.175526
 61655/100000: episode: 1033, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 59.841, mean reward: 0.598 [0.504, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.384, 10.098], loss: 0.001432, mae: 0.041527, mean_q: 1.174389
 61755/100000: episode: 1034, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.733, mean reward: 0.597 [0.511, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.416, 10.098], loss: 0.001372, mae: 0.040749, mean_q: 1.173916
 61855/100000: episode: 1035, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 63.813, mean reward: 0.638 [0.507, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.395, 10.322], loss: 0.001424, mae: 0.041739, mean_q: 1.175598
 61955/100000: episode: 1036, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 61.609, mean reward: 0.616 [0.512, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.085, 10.392], loss: 0.001604, mae: 0.043860, mean_q: 1.175497
 62055/100000: episode: 1037, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.255, mean reward: 0.593 [0.510, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.314, 10.418], loss: 0.001404, mae: 0.040824, mean_q: 1.179282
 62155/100000: episode: 1038, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 61.227, mean reward: 0.612 [0.505, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.668, 10.323], loss: 0.001508, mae: 0.042270, mean_q: 1.180966
 62255/100000: episode: 1039, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 63.011, mean reward: 0.630 [0.503, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.601, 10.098], loss: 0.001662, mae: 0.044334, mean_q: 1.182673
 62355/100000: episode: 1040, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 56.949, mean reward: 0.569 [0.505, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.611, 10.098], loss: 0.001548, mae: 0.042577, mean_q: 1.182674
 62455/100000: episode: 1041, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 59.806, mean reward: 0.598 [0.501, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.414, 10.113], loss: 0.001435, mae: 0.042066, mean_q: 1.180451
 62555/100000: episode: 1042, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.034, mean reward: 0.590 [0.504, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.646, 10.098], loss: 0.001509, mae: 0.042678, mean_q: 1.176793
 62655/100000: episode: 1043, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.800, mean reward: 0.588 [0.514, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.420, 10.234], loss: 0.001424, mae: 0.041628, mean_q: 1.178409
 62755/100000: episode: 1044, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 57.303, mean reward: 0.573 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.715, 10.104], loss: 0.001532, mae: 0.042820, mean_q: 1.177935
 62855/100000: episode: 1045, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 60.979, mean reward: 0.610 [0.510, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.801, 10.488], loss: 0.001404, mae: 0.041701, mean_q: 1.181796
 62955/100000: episode: 1046, duration: 0.903s, episode steps: 100, steps per second: 111, episode reward: 61.391, mean reward: 0.614 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.319, 10.098], loss: 0.001532, mae: 0.042771, mean_q: 1.179674
 63055/100000: episode: 1047, duration: 1.061s, episode steps: 100, steps per second: 94, episode reward: 59.315, mean reward: 0.593 [0.509, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.341, 10.098], loss: 0.001534, mae: 0.042832, mean_q: 1.181504
 63155/100000: episode: 1048, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 59.365, mean reward: 0.594 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.740, 10.098], loss: 0.001443, mae: 0.041793, mean_q: 1.178558
 63255/100000: episode: 1049, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: 58.747, mean reward: 0.587 [0.499, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.833, 10.160], loss: 0.001456, mae: 0.041588, mean_q: 1.177726
 63355/100000: episode: 1050, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 57.566, mean reward: 0.576 [0.503, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.579, 10.098], loss: 0.001481, mae: 0.042252, mean_q: 1.176329
 63455/100000: episode: 1051, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 56.969, mean reward: 0.570 [0.505, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.727, 10.241], loss: 0.001488, mae: 0.042533, mean_q: 1.176407
 63555/100000: episode: 1052, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.446, mean reward: 0.574 [0.507, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.533, 10.098], loss: 0.001439, mae: 0.041655, mean_q: 1.178062
 63655/100000: episode: 1053, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 65.514, mean reward: 0.655 [0.513, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.179, 10.098], loss: 0.001451, mae: 0.041424, mean_q: 1.173877
 63755/100000: episode: 1054, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.589, mean reward: 0.586 [0.506, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.692, 10.308], loss: 0.001655, mae: 0.044010, mean_q: 1.180142
 63855/100000: episode: 1055, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 59.356, mean reward: 0.594 [0.502, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.328, 10.255], loss: 0.001413, mae: 0.041255, mean_q: 1.181578
 63955/100000: episode: 1056, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 60.368, mean reward: 0.604 [0.500, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.212, 10.098], loss: 0.001523, mae: 0.042573, mean_q: 1.176235
 64055/100000: episode: 1057, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 59.502, mean reward: 0.595 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.000, 10.382], loss: 0.001466, mae: 0.042013, mean_q: 1.180624
 64155/100000: episode: 1058, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.210, mean reward: 0.572 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.086, 10.104], loss: 0.001383, mae: 0.040727, mean_q: 1.178914
 64255/100000: episode: 1059, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 57.109, mean reward: 0.571 [0.503, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.521, 10.229], loss: 0.001477, mae: 0.042616, mean_q: 1.178619
 64355/100000: episode: 1060, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 57.412, mean reward: 0.574 [0.504, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.784, 10.098], loss: 0.001452, mae: 0.041364, mean_q: 1.178770
 64455/100000: episode: 1061, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 61.224, mean reward: 0.612 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.564, 10.098], loss: 0.001713, mae: 0.044793, mean_q: 1.177092
 64555/100000: episode: 1062, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.024, mean reward: 0.600 [0.514, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.543, 10.267], loss: 0.001433, mae: 0.041669, mean_q: 1.181151
 64655/100000: episode: 1063, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.194, mean reward: 0.582 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.487, 10.192], loss: 0.001476, mae: 0.041816, mean_q: 1.178286
 64755/100000: episode: 1064, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.064, mean reward: 0.581 [0.505, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.097, 10.168], loss: 0.001450, mae: 0.041902, mean_q: 1.181133
 64855/100000: episode: 1065, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.066, mean reward: 0.571 [0.502, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.960, 10.137], loss: 0.001469, mae: 0.041975, mean_q: 1.172993
 64955/100000: episode: 1066, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.143, mean reward: 0.581 [0.503, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.663, 10.098], loss: 0.001509, mae: 0.042293, mean_q: 1.174721
 65055/100000: episode: 1067, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.105, mean reward: 0.601 [0.515, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.711, 10.098], loss: 0.001411, mae: 0.041541, mean_q: 1.173024
 65155/100000: episode: 1068, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.102, mean reward: 0.581 [0.514, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.375, 10.098], loss: 0.001400, mae: 0.040540, mean_q: 1.170568
 65255/100000: episode: 1069, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.586, mean reward: 0.586 [0.506, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.496, 10.143], loss: 0.001395, mae: 0.041164, mean_q: 1.170815
 65355/100000: episode: 1070, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.870, mean reward: 0.589 [0.510, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.353, 10.098], loss: 0.001397, mae: 0.040935, mean_q: 1.169153
 65455/100000: episode: 1071, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 61.022, mean reward: 0.610 [0.515, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.930, 10.098], loss: 0.001376, mae: 0.040801, mean_q: 1.174319
 65555/100000: episode: 1072, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 58.576, mean reward: 0.586 [0.506, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.833, 10.098], loss: 0.001519, mae: 0.042449, mean_q: 1.177231
 65655/100000: episode: 1073, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 57.114, mean reward: 0.571 [0.500, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.440, 10.098], loss: 0.001385, mae: 0.041165, mean_q: 1.175579
 65755/100000: episode: 1074, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.844, mean reward: 0.588 [0.507, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.576, 10.098], loss: 0.001552, mae: 0.042883, mean_q: 1.175135
 65855/100000: episode: 1075, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 56.828, mean reward: 0.568 [0.501, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.327, 10.131], loss: 0.001462, mae: 0.041909, mean_q: 1.173582
 65955/100000: episode: 1076, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.211, mean reward: 0.572 [0.498, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.285, 10.140], loss: 0.001430, mae: 0.041840, mean_q: 1.171956
 66055/100000: episode: 1077, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.016, mean reward: 0.590 [0.501, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.568, 10.107], loss: 0.001458, mae: 0.041996, mean_q: 1.167559
 66155/100000: episode: 1078, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.543, mean reward: 0.595 [0.503, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.099, 10.098], loss: 0.001402, mae: 0.040996, mean_q: 1.171561
 66255/100000: episode: 1079, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.190, mean reward: 0.582 [0.505, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.852, 10.240], loss: 0.001359, mae: 0.040314, mean_q: 1.171770
[Info] 1-TH LEVEL FOUND: 1.4070782661437988, Considering 10/90 traces
 66355/100000: episode: 1080, duration: 4.780s, episode steps: 100, steps per second: 21, episode reward: 57.168, mean reward: 0.572 [0.499, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.758, 10.098], loss: 0.001393, mae: 0.040625, mean_q: 1.170565
 66367/100000: episode: 1081, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 8.464, mean reward: 0.705 [0.639, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.457, 10.100], loss: 0.001388, mae: 0.039790, mean_q: 1.161968
 66390/100000: episode: 1082, duration: 0.133s, episode steps: 23, steps per second: 172, episode reward: 14.953, mean reward: 0.650 [0.575, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.468, 10.100], loss: 0.001344, mae: 0.039763, mean_q: 1.162500
 66412/100000: episode: 1083, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 15.818, mean reward: 0.719 [0.635, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.243, 10.444], loss: 0.001459, mae: 0.041868, mean_q: 1.176924
 66435/100000: episode: 1084, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 17.121, mean reward: 0.744 [0.675, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.322, 10.100], loss: 0.001279, mae: 0.039581, mean_q: 1.174020
 66468/100000: episode: 1085, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 22.213, mean reward: 0.673 [0.592, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.609, 10.100], loss: 0.001533, mae: 0.042480, mean_q: 1.168635
 66508/100000: episode: 1086, duration: 0.231s, episode steps: 40, steps per second: 173, episode reward: 31.109, mean reward: 0.778 [0.671, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.723, 10.100], loss: 0.001383, mae: 0.040795, mean_q: 1.170581
 66517/100000: episode: 1087, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 6.529, mean reward: 0.725 [0.697, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.784, 10.390], loss: 0.002006, mae: 0.044356, mean_q: 1.176894
 66539/100000: episode: 1088, duration: 0.142s, episode steps: 22, steps per second: 155, episode reward: 14.711, mean reward: 0.669 [0.561, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.245], loss: 0.001443, mae: 0.040525, mean_q: 1.178374
 66548/100000: episode: 1089, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 6.882, mean reward: 0.765 [0.734, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.470, 10.497], loss: 0.001281, mae: 0.040222, mean_q: 1.177793
 66568/100000: episode: 1090, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 14.401, mean reward: 0.720 [0.634, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.385, 10.100], loss: 0.001591, mae: 0.042324, mean_q: 1.180680
 66588/100000: episode: 1091, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 13.728, mean reward: 0.686 [0.636, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.655, 10.100], loss: 0.001657, mae: 0.044147, mean_q: 1.176284
 66621/100000: episode: 1092, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 23.797, mean reward: 0.721 [0.642, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.941, 10.100], loss: 0.001401, mae: 0.040982, mean_q: 1.184047
 66644/100000: episode: 1093, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 17.264, mean reward: 0.751 [0.645, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.531, 10.100], loss: 0.001422, mae: 0.041010, mean_q: 1.178082
 66656/100000: episode: 1094, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 9.249, mean reward: 0.771 [0.690, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.439, 10.100], loss: 0.001417, mae: 0.041982, mean_q: 1.193238
 66689/100000: episode: 1095, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 23.986, mean reward: 0.727 [0.595, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.815, 10.100], loss: 0.001781, mae: 0.044707, mean_q: 1.178801
 66729/100000: episode: 1096, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 26.932, mean reward: 0.673 [0.616, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.314, 10.100], loss: 0.001781, mae: 0.045260, mean_q: 1.184433
 66752/100000: episode: 1097, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 15.147, mean reward: 0.659 [0.564, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.795, 10.100], loss: 0.002092, mae: 0.047316, mean_q: 1.173642
 66761/100000: episode: 1098, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 6.739, mean reward: 0.749 [0.714, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.452, 10.462], loss: 0.002366, mae: 0.052623, mean_q: 1.174150
 66801/100000: episode: 1099, duration: 0.236s, episode steps: 40, steps per second: 170, episode reward: 24.843, mean reward: 0.621 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.610, 10.207], loss: 0.001793, mae: 0.044776, mean_q: 1.186691
 66813/100000: episode: 1100, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 9.331, mean reward: 0.778 [0.668, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.653, 10.100], loss: 0.001549, mae: 0.043663, mean_q: 1.170620
 66836/100000: episode: 1101, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 14.304, mean reward: 0.622 [0.542, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.185, 10.100], loss: 0.001841, mae: 0.045284, mean_q: 1.178415
 66858/100000: episode: 1102, duration: 0.136s, episode steps: 22, steps per second: 161, episode reward: 15.130, mean reward: 0.688 [0.598, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.306, 10.363], loss: 0.001505, mae: 0.042377, mean_q: 1.180166
 66891/100000: episode: 1103, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 23.107, mean reward: 0.700 [0.654, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.017, 10.100], loss: 0.001751, mae: 0.044162, mean_q: 1.181473
 66931/100000: episode: 1104, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 26.597, mean reward: 0.665 [0.560, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.058, 10.100], loss: 0.001528, mae: 0.042883, mean_q: 1.183621
 66951/100000: episode: 1105, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 14.256, mean reward: 0.713 [0.652, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.206, 10.100], loss: 0.001774, mae: 0.043680, mean_q: 1.187355
 66981/100000: episode: 1106, duration: 0.167s, episode steps: 30, steps per second: 179, episode reward: 20.642, mean reward: 0.688 [0.616, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.464, 10.100], loss: 0.001662, mae: 0.043157, mean_q: 1.192680
 67003/100000: episode: 1107, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 15.045, mean reward: 0.684 [0.612, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.381], loss: 0.001656, mae: 0.044254, mean_q: 1.183762
 67036/100000: episode: 1108, duration: 0.197s, episode steps: 33, steps per second: 168, episode reward: 22.698, mean reward: 0.688 [0.571, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.052, 10.100], loss: 0.001700, mae: 0.044479, mean_q: 1.188898
 67069/100000: episode: 1109, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 24.905, mean reward: 0.755 [0.656, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.242, 10.100], loss: 0.001716, mae: 0.044983, mean_q: 1.192540
 67091/100000: episode: 1110, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 16.655, mean reward: 0.757 [0.673, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.635, 10.598], loss: 0.001643, mae: 0.044024, mean_q: 1.187177
 67113/100000: episode: 1111, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 15.202, mean reward: 0.691 [0.646, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.179, 10.441], loss: 0.001510, mae: 0.042602, mean_q: 1.196462
 67135/100000: episode: 1112, duration: 0.133s, episode steps: 22, steps per second: 166, episode reward: 16.137, mean reward: 0.733 [0.667, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.383], loss: 0.001700, mae: 0.042814, mean_q: 1.192566
 67175/100000: episode: 1113, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 28.183, mean reward: 0.705 [0.616, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.347, 10.100], loss: 0.001703, mae: 0.043829, mean_q: 1.193781
 67198/100000: episode: 1114, duration: 0.125s, episode steps: 23, steps per second: 185, episode reward: 14.971, mean reward: 0.651 [0.529, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.163, 10.100], loss: 0.001798, mae: 0.044051, mean_q: 1.205570
 67228/100000: episode: 1115, duration: 0.162s, episode steps: 30, steps per second: 186, episode reward: 22.390, mean reward: 0.746 [0.657, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.145, 10.100], loss: 0.001675, mae: 0.043540, mean_q: 1.196302
 67237/100000: episode: 1116, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 6.823, mean reward: 0.758 [0.718, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.509], loss: 0.001887, mae: 0.047848, mean_q: 1.200463
 67267/100000: episode: 1117, duration: 0.161s, episode steps: 30, steps per second: 187, episode reward: 20.075, mean reward: 0.669 [0.577, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.321, 10.100], loss: 0.001747, mae: 0.044580, mean_q: 1.202452
 67300/100000: episode: 1118, duration: 0.180s, episode steps: 33, steps per second: 184, episode reward: 21.472, mean reward: 0.651 [0.521, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.035, 10.147], loss: 0.001602, mae: 0.043147, mean_q: 1.198626
 67309/100000: episode: 1119, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 6.601, mean reward: 0.733 [0.694, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.346, 10.483], loss: 0.001505, mae: 0.040932, mean_q: 1.199065
 67339/100000: episode: 1120, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 21.324, mean reward: 0.711 [0.609, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.813, 10.100], loss: 0.001706, mae: 0.043150, mean_q: 1.197352
 67359/100000: episode: 1121, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 13.592, mean reward: 0.680 [0.593, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.790, 10.100], loss: 0.001831, mae: 0.045869, mean_q: 1.198078
 67392/100000: episode: 1122, duration: 0.189s, episode steps: 33, steps per second: 174, episode reward: 22.987, mean reward: 0.697 [0.606, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.438, 10.100], loss: 0.001811, mae: 0.045086, mean_q: 1.203365
 67414/100000: episode: 1123, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 14.456, mean reward: 0.657 [0.591, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.942, 10.326], loss: 0.002034, mae: 0.046454, mean_q: 1.188473
 67423/100000: episode: 1124, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 6.430, mean reward: 0.714 [0.683, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.447], loss: 0.001990, mae: 0.046935, mean_q: 1.183481
 67445/100000: episode: 1125, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 17.896, mean reward: 0.813 [0.649, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.920, 10.752], loss: 0.001721, mae: 0.045307, mean_q: 1.191648
 67467/100000: episode: 1126, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 17.530, mean reward: 0.797 [0.716, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.268, 10.686], loss: 0.001857, mae: 0.046955, mean_q: 1.207999
 67489/100000: episode: 1127, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 14.270, mean reward: 0.649 [0.560, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.384, 10.315], loss: 0.001358, mae: 0.039848, mean_q: 1.206157
 67522/100000: episode: 1128, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 22.763, mean reward: 0.690 [0.580, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.313, 10.100], loss: 0.001849, mae: 0.044991, mean_q: 1.203992
 67555/100000: episode: 1129, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 23.891, mean reward: 0.724 [0.612, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.931, 10.100], loss: 0.001697, mae: 0.044749, mean_q: 1.210530
 67575/100000: episode: 1130, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 13.266, mean reward: 0.663 [0.581, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.666, 10.100], loss: 0.001860, mae: 0.045823, mean_q: 1.206556
 67595/100000: episode: 1131, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 13.982, mean reward: 0.699 [0.611, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.035, 10.100], loss: 0.001686, mae: 0.044077, mean_q: 1.205524
 67625/100000: episode: 1132, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 21.870, mean reward: 0.729 [0.658, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.848, 10.100], loss: 0.001413, mae: 0.039827, mean_q: 1.213365
 67665/100000: episode: 1133, duration: 0.211s, episode steps: 40, steps per second: 190, episode reward: 27.661, mean reward: 0.692 [0.603, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-2.030, 10.100], loss: 0.001676, mae: 0.043732, mean_q: 1.206008
 67695/100000: episode: 1134, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 20.387, mean reward: 0.680 [0.617, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.531, 10.100], loss: 0.001739, mae: 0.045266, mean_q: 1.217157
 67718/100000: episode: 1135, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 14.983, mean reward: 0.651 [0.587, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.667, 10.100], loss: 0.002087, mae: 0.049476, mean_q: 1.219555
 67730/100000: episode: 1136, duration: 0.065s, episode steps: 12, steps per second: 183, episode reward: 8.105, mean reward: 0.675 [0.616, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.207, 10.100], loss: 0.001609, mae: 0.043984, mean_q: 1.213124
 67752/100000: episode: 1137, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 16.558, mean reward: 0.753 [0.653, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.612], loss: 0.001636, mae: 0.042072, mean_q: 1.216385
 67764/100000: episode: 1138, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 8.446, mean reward: 0.704 [0.652, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.849, 10.100], loss: 0.001434, mae: 0.041103, mean_q: 1.216730
 67784/100000: episode: 1139, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 13.905, mean reward: 0.695 [0.605, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.878, 10.100], loss: 0.001588, mae: 0.042351, mean_q: 1.215513
 67824/100000: episode: 1140, duration: 0.219s, episode steps: 40, steps per second: 183, episode reward: 28.808, mean reward: 0.720 [0.624, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.672, 10.100], loss: 0.001777, mae: 0.044385, mean_q: 1.216597
 67844/100000: episode: 1141, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 13.493, mean reward: 0.675 [0.609, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.262, 10.100], loss: 0.002072, mae: 0.045928, mean_q: 1.199946
 67866/100000: episode: 1142, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 17.346, mean reward: 0.788 [0.680, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.546, 10.512], loss: 0.002001, mae: 0.046745, mean_q: 1.219916
 67899/100000: episode: 1143, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 22.511, mean reward: 0.682 [0.576, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.152, 10.100], loss: 0.001815, mae: 0.044607, mean_q: 1.217686
 67921/100000: episode: 1144, duration: 0.114s, episode steps: 22, steps per second: 192, episode reward: 15.164, mean reward: 0.689 [0.620, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.449, 10.390], loss: 0.001692, mae: 0.044224, mean_q: 1.238425
 67944/100000: episode: 1145, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 15.927, mean reward: 0.692 [0.641, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.176, 10.100], loss: 0.001755, mae: 0.043216, mean_q: 1.224307
 67977/100000: episode: 1146, duration: 0.186s, episode steps: 33, steps per second: 178, episode reward: 25.126, mean reward: 0.761 [0.659, 0.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.812, 10.100], loss: 0.001691, mae: 0.044185, mean_q: 1.225284
 68000/100000: episode: 1147, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 16.322, mean reward: 0.710 [0.637, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.489, 10.100], loss: 0.001573, mae: 0.042382, mean_q: 1.222874
 68033/100000: episode: 1148, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 22.156, mean reward: 0.671 [0.553, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.075, 10.100], loss: 0.001586, mae: 0.043324, mean_q: 1.228255
 68056/100000: episode: 1149, duration: 0.115s, episode steps: 23, steps per second: 199, episode reward: 15.399, mean reward: 0.670 [0.574, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.177, 10.100], loss: 0.001505, mae: 0.042550, mean_q: 1.228700
 68078/100000: episode: 1150, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 15.889, mean reward: 0.722 [0.682, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.517, 10.528], loss: 0.001805, mae: 0.044673, mean_q: 1.229804
 68118/100000: episode: 1151, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 27.698, mean reward: 0.692 [0.550, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-1.269, 10.100], loss: 0.001695, mae: 0.043973, mean_q: 1.225829
 68138/100000: episode: 1152, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 14.464, mean reward: 0.723 [0.673, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.257, 10.100], loss: 0.001853, mae: 0.047006, mean_q: 1.227540
 68161/100000: episode: 1153, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 14.120, mean reward: 0.614 [0.543, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.142, 10.100], loss: 0.001806, mae: 0.046451, mean_q: 1.216781
 68194/100000: episode: 1154, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 23.643, mean reward: 0.716 [0.623, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.189, 10.100], loss: 0.001693, mae: 0.043964, mean_q: 1.236605
 68206/100000: episode: 1155, duration: 0.080s, episode steps: 12, steps per second: 149, episode reward: 8.982, mean reward: 0.749 [0.717, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.362, 10.100], loss: 0.001788, mae: 0.046213, mean_q: 1.231995
 68228/100000: episode: 1156, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 15.590, mean reward: 0.709 [0.649, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.448, 10.517], loss: 0.001608, mae: 0.042805, mean_q: 1.239519
 68250/100000: episode: 1157, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 16.854, mean reward: 0.766 [0.705, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.860, 10.427], loss: 0.001626, mae: 0.042625, mean_q: 1.241783
 68280/100000: episode: 1158, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 18.381, mean reward: 0.613 [0.508, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.547, 10.152], loss: 0.001560, mae: 0.042559, mean_q: 1.231388
 68313/100000: episode: 1159, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 22.990, mean reward: 0.697 [0.625, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.352, 10.100], loss: 0.001497, mae: 0.042232, mean_q: 1.235503
 68336/100000: episode: 1160, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 15.846, mean reward: 0.689 [0.646, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.375, 10.100], loss: 0.001483, mae: 0.040619, mean_q: 1.230494
 68358/100000: episode: 1161, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 13.927, mean reward: 0.633 [0.585, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.362], loss: 0.001602, mae: 0.042391, mean_q: 1.254828
 68380/100000: episode: 1162, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 18.096, mean reward: 0.823 [0.713, 0.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.404, 10.691], loss: 0.001506, mae: 0.041345, mean_q: 1.234642
 68389/100000: episode: 1163, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 6.755, mean reward: 0.751 [0.725, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.865, 10.478], loss: 0.001781, mae: 0.046858, mean_q: 1.243775
 68412/100000: episode: 1164, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 15.100, mean reward: 0.657 [0.579, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.307, 10.100], loss: 0.001558, mae: 0.041590, mean_q: 1.248496
 68452/100000: episode: 1165, duration: 0.223s, episode steps: 40, steps per second: 179, episode reward: 26.173, mean reward: 0.654 [0.592, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-1.250, 10.100], loss: 0.002009, mae: 0.047563, mean_q: 1.230793
 68461/100000: episode: 1166, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 6.744, mean reward: 0.749 [0.718, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.535], loss: 0.001680, mae: 0.044045, mean_q: 1.236356
 68501/100000: episode: 1167, duration: 0.216s, episode steps: 40, steps per second: 186, episode reward: 26.524, mean reward: 0.663 [0.521, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.400, 10.100], loss: 0.001846, mae: 0.045877, mean_q: 1.248246
 68534/100000: episode: 1168, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 21.936, mean reward: 0.665 [0.507, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.330, 10.139], loss: 0.001936, mae: 0.045839, mean_q: 1.240676
 68556/100000: episode: 1169, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 16.411, mean reward: 0.746 [0.702, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.851, 10.497], loss: 0.001682, mae: 0.044286, mean_q: 1.240910
[Info] 2-TH LEVEL FOUND: 1.5832575559616089, Considering 10/90 traces
 68576/100000: episode: 1170, duration: 4.309s, episode steps: 20, steps per second: 5, episode reward: 14.979, mean reward: 0.749 [0.703, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.891, 10.100], loss: 0.001797, mae: 0.045958, mean_q: 1.238749
 68604/100000: episode: 1171, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 20.755, mean reward: 0.741 [0.679, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.305, 10.100], loss: 0.001466, mae: 0.040992, mean_q: 1.251841
 68613/100000: episode: 1172, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 7.507, mean reward: 0.834 [0.790, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.364, 10.641], loss: 0.002264, mae: 0.049611, mean_q: 1.269855
 68639/100000: episode: 1173, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 20.287, mean reward: 0.780 [0.700, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.221, 10.100], loss: 0.001699, mae: 0.044417, mean_q: 1.251526
 68650/100000: episode: 1174, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 8.747, mean reward: 0.795 [0.756, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.488], loss: 0.001782, mae: 0.045516, mean_q: 1.239620
 68674/100000: episode: 1175, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 19.937, mean reward: 0.831 [0.737, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.665, 10.100], loss: 0.002183, mae: 0.050299, mean_q: 1.253789
 68702/100000: episode: 1176, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 20.402, mean reward: 0.729 [0.606, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.587, 10.100], loss: 0.001757, mae: 0.045009, mean_q: 1.241844
 68713/100000: episode: 1177, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 8.226, mean reward: 0.748 [0.691, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.271, 10.480], loss: 0.001591, mae: 0.042973, mean_q: 1.251849
 68739/100000: episode: 1178, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 19.152, mean reward: 0.737 [0.657, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.752, 10.100], loss: 0.001733, mae: 0.043970, mean_q: 1.250548
 68765/100000: episode: 1179, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 18.966, mean reward: 0.729 [0.675, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.271, 10.100], loss: 0.001693, mae: 0.043547, mean_q: 1.240041
 68787/100000: episode: 1180, duration: 0.144s, episode steps: 22, steps per second: 153, episode reward: 17.331, mean reward: 0.788 [0.690, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.500, 10.100], loss: 0.001921, mae: 0.045996, mean_q: 1.263660
 68813/100000: episode: 1181, duration: 0.158s, episode steps: 26, steps per second: 164, episode reward: 18.234, mean reward: 0.701 [0.584, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.666, 10.100], loss: 0.001644, mae: 0.043892, mean_q: 1.274635
 68839/100000: episode: 1182, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 20.244, mean reward: 0.779 [0.624, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.271, 10.100], loss: 0.001550, mae: 0.042362, mean_q: 1.261825
 68861/100000: episode: 1183, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 18.492, mean reward: 0.841 [0.764, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.452, 10.100], loss: 0.001671, mae: 0.044283, mean_q: 1.272582
 68878/100000: episode: 1184, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 13.827, mean reward: 0.813 [0.766, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.101, 10.533], loss: 0.001730, mae: 0.042970, mean_q: 1.259070
 68904/100000: episode: 1185, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 19.603, mean reward: 0.754 [0.672, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.527, 10.100], loss: 0.002383, mae: 0.053202, mean_q: 1.258300
 68932/100000: episode: 1186, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 20.661, mean reward: 0.738 [0.660, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.105, 10.100], loss: 0.001900, mae: 0.046035, mean_q: 1.265541
 68943/100000: episode: 1187, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 8.322, mean reward: 0.757 [0.684, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-1.067, 10.526], loss: 0.001755, mae: 0.043168, mean_q: 1.264492
 68969/100000: episode: 1188, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 20.408, mean reward: 0.785 [0.674, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.049, 10.100], loss: 0.001949, mae: 0.046336, mean_q: 1.259376
 68978/100000: episode: 1189, duration: 0.070s, episode steps: 9, steps per second: 128, episode reward: 6.837, mean reward: 0.760 [0.727, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.513], loss: 0.001559, mae: 0.041577, mean_q: 1.253539
 69004/100000: episode: 1190, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 19.732, mean reward: 0.759 [0.582, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.402, 10.100], loss: 0.002282, mae: 0.050824, mean_q: 1.264065
 69030/100000: episode: 1191, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 18.926, mean reward: 0.728 [0.610, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-1.311, 10.100], loss: 0.002265, mae: 0.050942, mean_q: 1.268460
 69047/100000: episode: 1192, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 13.472, mean reward: 0.792 [0.705, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.104, 10.414], loss: 0.002195, mae: 0.050347, mean_q: 1.264335
 69056/100000: episode: 1193, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 7.043, mean reward: 0.783 [0.742, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.444], loss: 0.002202, mae: 0.049599, mean_q: 1.271782
 69082/100000: episode: 1194, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 20.798, mean reward: 0.800 [0.760, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.389, 10.100], loss: 0.001845, mae: 0.046629, mean_q: 1.281089
 69093/100000: episode: 1195, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 8.182, mean reward: 0.744 [0.716, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.516], loss: 0.001739, mae: 0.043792, mean_q: 1.262999
 69121/100000: episode: 1196, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 21.417, mean reward: 0.765 [0.681, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.789, 10.100], loss: 0.001523, mae: 0.042562, mean_q: 1.278650
 69149/100000: episode: 1197, duration: 0.161s, episode steps: 28, steps per second: 173, episode reward: 19.580, mean reward: 0.699 [0.544, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.078, 10.100], loss: 0.001538, mae: 0.043336, mean_q: 1.280994
 69173/100000: episode: 1198, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 18.917, mean reward: 0.788 [0.684, 0.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.354, 10.100], loss: 0.001801, mae: 0.045375, mean_q: 1.269518
 69201/100000: episode: 1199, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 20.601, mean reward: 0.736 [0.552, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.712, 10.100], loss: 0.001718, mae: 0.044036, mean_q: 1.272993
 69218/100000: episode: 1200, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 13.497, mean reward: 0.794 [0.681, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.474], loss: 0.001755, mae: 0.043803, mean_q: 1.281921
 69244/100000: episode: 1201, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 20.057, mean reward: 0.771 [0.634, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.492, 10.100], loss: 0.001620, mae: 0.044202, mean_q: 1.281653
 69253/100000: episode: 1202, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 7.555, mean reward: 0.839 [0.776, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.659], loss: 0.001642, mae: 0.040944, mean_q: 1.286882
 69262/100000: episode: 1203, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 7.045, mean reward: 0.783 [0.755, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.512], loss: 0.001798, mae: 0.045245, mean_q: 1.279078
 69286/100000: episode: 1204, duration: 0.147s, episode steps: 24, steps per second: 163, episode reward: 17.438, mean reward: 0.727 [0.641, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.262, 10.100], loss: 0.001962, mae: 0.046475, mean_q: 1.292319
 69303/100000: episode: 1205, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 13.117, mean reward: 0.772 [0.716, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.041, 10.487], loss: 0.001735, mae: 0.043043, mean_q: 1.299827
 69329/100000: episode: 1206, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 19.344, mean reward: 0.744 [0.628, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.414, 10.100], loss: 0.001674, mae: 0.043493, mean_q: 1.278698
 69355/100000: episode: 1207, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 19.850, mean reward: 0.763 [0.672, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.872, 10.100], loss: 0.001570, mae: 0.042529, mean_q: 1.287135
 69381/100000: episode: 1208, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 19.242, mean reward: 0.740 [0.666, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.516, 10.100], loss: 0.002079, mae: 0.048295, mean_q: 1.288853
 69398/100000: episode: 1209, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 14.059, mean reward: 0.827 [0.761, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.092, 10.536], loss: 0.001848, mae: 0.047072, mean_q: 1.296484
 69426/100000: episode: 1210, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 24.021, mean reward: 0.858 [0.798, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.446, 10.100], loss: 0.001770, mae: 0.046488, mean_q: 1.290346
 69452/100000: episode: 1211, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 19.596, mean reward: 0.754 [0.699, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.165, 10.100], loss: 0.001665, mae: 0.044909, mean_q: 1.294119
 69478/100000: episode: 1212, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 21.569, mean reward: 0.830 [0.719, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.388, 10.100], loss: 0.001952, mae: 0.045610, mean_q: 1.292048
 69504/100000: episode: 1213, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 20.124, mean reward: 0.774 [0.732, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.419, 10.100], loss: 0.001954, mae: 0.047904, mean_q: 1.295830
 69530/100000: episode: 1214, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 21.249, mean reward: 0.817 [0.716, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.629, 10.100], loss: 0.001577, mae: 0.044099, mean_q: 1.291835
 69558/100000: episode: 1215, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 23.842, mean reward: 0.852 [0.773, 0.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.867, 10.100], loss: 0.001852, mae: 0.046008, mean_q: 1.305767
 69569/100000: episode: 1216, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 8.575, mean reward: 0.780 [0.703, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.126, 10.505], loss: 0.001660, mae: 0.044149, mean_q: 1.293031
 69591/100000: episode: 1217, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 17.057, mean reward: 0.775 [0.640, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.580, 10.100], loss: 0.001573, mae: 0.042289, mean_q: 1.308128
 69617/100000: episode: 1218, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 17.173, mean reward: 0.660 [0.563, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.329, 10.100], loss: 0.001477, mae: 0.041874, mean_q: 1.305389
 69634/100000: episode: 1219, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 13.355, mean reward: 0.786 [0.738, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.708, 10.543], loss: 0.001713, mae: 0.045130, mean_q: 1.305167
 69651/100000: episode: 1220, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 14.463, mean reward: 0.851 [0.741, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.648, 10.522], loss: 0.001461, mae: 0.041932, mean_q: 1.308959
 69677/100000: episode: 1221, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 18.268, mean reward: 0.703 [0.568, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.293, 10.100], loss: 0.001569, mae: 0.043008, mean_q: 1.308206
 69694/100000: episode: 1222, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 12.336, mean reward: 0.726 [0.559, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.116, 10.339], loss: 0.001624, mae: 0.042940, mean_q: 1.313403
 69720/100000: episode: 1223, duration: 0.146s, episode steps: 26, steps per second: 177, episode reward: 17.998, mean reward: 0.692 [0.571, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.071, 10.100], loss: 0.001581, mae: 0.043321, mean_q: 1.310017
 69729/100000: episode: 1224, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 7.150, mean reward: 0.794 [0.755, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.561], loss: 0.001530, mae: 0.042071, mean_q: 1.323888
 69740/100000: episode: 1225, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 8.752, mean reward: 0.796 [0.726, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.497], loss: 0.001651, mae: 0.044818, mean_q: 1.326881
 69766/100000: episode: 1226, duration: 0.153s, episode steps: 26, steps per second: 169, episode reward: 20.205, mean reward: 0.777 [0.703, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.526, 10.100], loss: 0.001759, mae: 0.044977, mean_q: 1.321440
 69775/100000: episode: 1227, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 7.223, mean reward: 0.803 [0.724, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.120, 10.575], loss: 0.001855, mae: 0.046040, mean_q: 1.308660
 69801/100000: episode: 1228, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 21.334, mean reward: 0.821 [0.607, 0.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.282, 10.100], loss: 0.001886, mae: 0.046267, mean_q: 1.330255
 69829/100000: episode: 1229, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 20.706, mean reward: 0.740 [0.660, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.300, 10.100], loss: 0.001531, mae: 0.042604, mean_q: 1.329907
 69857/100000: episode: 1230, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 19.604, mean reward: 0.700 [0.546, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.700, 10.100], loss: 0.001756, mae: 0.045005, mean_q: 1.324104
 69879/100000: episode: 1231, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 17.645, mean reward: 0.802 [0.672, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.581, 10.100], loss: 0.001510, mae: 0.042629, mean_q: 1.323640
 69903/100000: episode: 1232, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 20.037, mean reward: 0.835 [0.755, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.409, 10.100], loss: 0.001724, mae: 0.044150, mean_q: 1.332327
 69925/100000: episode: 1233, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 17.379, mean reward: 0.790 [0.750, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.894, 10.100], loss: 0.001873, mae: 0.045358, mean_q: 1.297439
 69951/100000: episode: 1234, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 18.394, mean reward: 0.707 [0.642, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.603, 10.100], loss: 0.001847, mae: 0.046955, mean_q: 1.337688
 69960/100000: episode: 1235, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 7.316, mean reward: 0.813 [0.750, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.452], loss: 0.001378, mae: 0.041420, mean_q: 1.323989
 69971/100000: episode: 1236, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 7.808, mean reward: 0.710 [0.639, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.379], loss: 0.001719, mae: 0.043461, mean_q: 1.330852
 69999/100000: episode: 1237, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 20.398, mean reward: 0.729 [0.618, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.339, 10.100], loss: 0.001820, mae: 0.045550, mean_q: 1.328909
 70025/100000: episode: 1238, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 19.820, mean reward: 0.762 [0.695, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.382, 10.100], loss: 0.001955, mae: 0.046374, mean_q: 1.324375
 70053/100000: episode: 1239, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 20.182, mean reward: 0.721 [0.588, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.650, 10.100], loss: 0.001730, mae: 0.045048, mean_q: 1.325071
 70062/100000: episode: 1240, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 7.420, mean reward: 0.824 [0.763, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.473], loss: 0.001620, mae: 0.044947, mean_q: 1.357102
 70088/100000: episode: 1241, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 20.798, mean reward: 0.800 [0.758, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.060, 10.100], loss: 0.001688, mae: 0.045140, mean_q: 1.337979
 70099/100000: episode: 1242, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 9.156, mean reward: 0.832 [0.807, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.652], loss: 0.001665, mae: 0.042921, mean_q: 1.325504
 70125/100000: episode: 1243, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 19.296, mean reward: 0.742 [0.581, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.504, 10.100], loss: 0.001967, mae: 0.048417, mean_q: 1.340016
 70142/100000: episode: 1244, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 13.477, mean reward: 0.793 [0.693, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.194, 10.501], loss: 0.001556, mae: 0.042625, mean_q: 1.341270
 70168/100000: episode: 1245, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 21.501, mean reward: 0.827 [0.780, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.971, 10.100], loss: 0.001744, mae: 0.043779, mean_q: 1.354587
 70192/100000: episode: 1246, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 20.698, mean reward: 0.862 [0.749, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.221, 10.100], loss: 0.001636, mae: 0.043262, mean_q: 1.329984
 70220/100000: episode: 1247, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 19.043, mean reward: 0.680 [0.555, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.195, 10.100], loss: 0.001627, mae: 0.043776, mean_q: 1.340569
 70229/100000: episode: 1248, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 7.443, mean reward: 0.827 [0.757, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.197, 10.665], loss: 0.001799, mae: 0.045365, mean_q: 1.362132
 70238/100000: episode: 1249, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 7.253, mean reward: 0.806 [0.756, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.663], loss: 0.001815, mae: 0.046489, mean_q: 1.358808
 70260/100000: episode: 1250, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 17.244, mean reward: 0.784 [0.710, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.421, 10.100], loss: 0.001820, mae: 0.046446, mean_q: 1.331808
 70286/100000: episode: 1251, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 19.503, mean reward: 0.750 [0.637, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.240, 10.100], loss: 0.001685, mae: 0.044129, mean_q: 1.359193
 70295/100000: episode: 1252, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 6.824, mean reward: 0.758 [0.725, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.491], loss: 0.002348, mae: 0.053114, mean_q: 1.356783
 70304/100000: episode: 1253, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 6.402, mean reward: 0.711 [0.665, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.481], loss: 0.001973, mae: 0.047937, mean_q: 1.353168
 70330/100000: episode: 1254, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 19.536, mean reward: 0.751 [0.590, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.296, 10.100], loss: 0.001807, mae: 0.046917, mean_q: 1.352025
 70354/100000: episode: 1255, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 17.941, mean reward: 0.748 [0.685, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.526, 10.100], loss: 0.001660, mae: 0.044118, mean_q: 1.357524
 70380/100000: episode: 1256, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 22.704, mean reward: 0.873 [0.809, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.523, 10.100], loss: 0.001705, mae: 0.044565, mean_q: 1.361832
 70408/100000: episode: 1257, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 21.486, mean reward: 0.767 [0.640, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.294, 10.100], loss: 0.001705, mae: 0.044452, mean_q: 1.349386
 70432/100000: episode: 1258, duration: 0.151s, episode steps: 24, steps per second: 159, episode reward: 20.615, mean reward: 0.859 [0.784, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.727, 10.100], loss: 0.001770, mae: 0.044948, mean_q: 1.344485
 70458/100000: episode: 1259, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 18.140, mean reward: 0.698 [0.567, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.671, 10.100], loss: 0.001828, mae: 0.045055, mean_q: 1.367980
[Info] 3-TH LEVEL FOUND: 1.70057213306427, Considering 10/90 traces
 70486/100000: episode: 1260, duration: 4.348s, episode steps: 28, steps per second: 6, episode reward: 21.525, mean reward: 0.769 [0.652, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.050, 10.100], loss: 0.001777, mae: 0.045068, mean_q: 1.360136
 70507/100000: episode: 1261, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 17.033, mean reward: 0.811 [0.727, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.887, 10.100], loss: 0.001887, mae: 0.047039, mean_q: 1.364381
 70526/100000: episode: 1262, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 15.791, mean reward: 0.831 [0.756, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.242, 10.100], loss: 0.001643, mae: 0.044667, mean_q: 1.361308
 70545/100000: episode: 1263, duration: 0.108s, episode steps: 19, steps per second: 177, episode reward: 16.407, mean reward: 0.864 [0.812, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.183, 10.100], loss: 0.001875, mae: 0.046093, mean_q: 1.363843
[Info] FALSIFICATION!
 70546/100000: episode: 1264, duration: 0.180s, episode steps: 1, steps per second: 6, episode reward: 1.003, mean reward: 1.003 [1.003, 1.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.027, 9.629], loss: 0.001534, mae: 0.042910, mean_q: 1.425587
 70561/100000: episode: 1265, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 12.606, mean reward: 0.840 [0.765, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.478, 10.100], loss: 0.001802, mae: 0.046672, mean_q: 1.373304
 70585/100000: episode: 1266, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 20.364, mean reward: 0.848 [0.760, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.106, 10.100], loss: 0.001697, mae: 0.043715, mean_q: 1.382185
 70600/100000: episode: 1267, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 12.889, mean reward: 0.859 [0.811, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.385, 10.100], loss: 0.001649, mae: 0.044359, mean_q: 1.364080
 70624/100000: episode: 1268, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 19.993, mean reward: 0.833 [0.703, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.299, 10.100], loss: 0.001552, mae: 0.042096, mean_q: 1.375329
[Info] FALSIFICATION!
 70626/100000: episode: 1269, duration: 0.193s, episode steps: 2, steps per second: 10, episode reward: 1.943, mean reward: 0.972 [0.942, 1.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.063, 9.695], loss: 0.001177, mae: 0.037126, mean_q: 1.379891
 70649/100000: episode: 1270, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 19.761, mean reward: 0.859 [0.792, 0.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.441, 10.100], loss: 0.001693, mae: 0.044803, mean_q: 1.397273
 70669/100000: episode: 1271, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 16.940, mean reward: 0.847 [0.750, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.469, 10.100], loss: 0.001552, mae: 0.042358, mean_q: 1.381828
 70688/100000: episode: 1272, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 16.917, mean reward: 0.890 [0.803, 0.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.411, 10.100], loss: 0.001791, mae: 0.045468, mean_q: 1.382066
 70703/100000: episode: 1273, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 12.137, mean reward: 0.809 [0.749, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.681, 10.100], loss: 0.003041, mae: 0.051420, mean_q: 1.372919
 70727/100000: episode: 1274, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 21.169, mean reward: 0.882 [0.844, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.479, 10.100], loss: 0.001797, mae: 0.046366, mean_q: 1.387830
 70751/100000: episode: 1275, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 17.330, mean reward: 0.722 [0.587, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.151, 10.100], loss: 0.001953, mae: 0.047822, mean_q: 1.378051
 70774/100000: episode: 1276, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 18.401, mean reward: 0.800 [0.742, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.450, 10.100], loss: 0.001665, mae: 0.044303, mean_q: 1.383495
[Info] FALSIFICATION!
 70785/100000: episode: 1277, duration: 0.323s, episode steps: 11, steps per second: 34, episode reward: 9.784, mean reward: 0.889 [0.817, 1.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.440, 9.695], loss: 0.001598, mae: 0.044717, mean_q: 1.370005
 70804/100000: episode: 1278, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 15.178, mean reward: 0.799 [0.720, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.483, 10.100], loss: 0.001626, mae: 0.043851, mean_q: 1.387851
 70819/100000: episode: 1279, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 13.817, mean reward: 0.921 [0.872, 0.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.418, 10.100], loss: 0.001703, mae: 0.045570, mean_q: 1.397513
 70834/100000: episode: 1280, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 11.959, mean reward: 0.797 [0.668, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.150, 10.100], loss: 0.002613, mae: 0.049805, mean_q: 1.400146
 70857/100000: episode: 1281, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 18.513, mean reward: 0.805 [0.683, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.357, 10.100], loss: 0.002222, mae: 0.048140, mean_q: 1.399453
 70877/100000: episode: 1282, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 16.730, mean reward: 0.837 [0.776, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.511, 10.100], loss: 0.002023, mae: 0.047708, mean_q: 1.386512
 70898/100000: episode: 1283, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 16.340, mean reward: 0.778 [0.658, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.189, 10.100], loss: 0.001620, mae: 0.043421, mean_q: 1.411414
 70922/100000: episode: 1284, duration: 0.132s, episode steps: 24, steps per second: 181, episode reward: 19.704, mean reward: 0.821 [0.691, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.297, 10.100], loss: 0.002071, mae: 0.046938, mean_q: 1.395267
 70941/100000: episode: 1285, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 15.946, mean reward: 0.839 [0.782, 0.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.498, 10.100], loss: 0.001503, mae: 0.041632, mean_q: 1.407172
 70963/100000: episode: 1286, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 16.972, mean reward: 0.771 [0.658, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.844, 10.100], loss: 0.002202, mae: 0.051446, mean_q: 1.409132
 70982/100000: episode: 1287, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 15.415, mean reward: 0.811 [0.694, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.309, 10.100], loss: 0.001953, mae: 0.048813, mean_q: 1.417784
 71003/100000: episode: 1288, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 17.184, mean reward: 0.818 [0.736, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.688, 10.100], loss: 0.002349, mae: 0.046231, mean_q: 1.399524
 71022/100000: episode: 1289, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 15.691, mean reward: 0.826 [0.742, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.348, 10.100], loss: 0.001684, mae: 0.044879, mean_q: 1.414130
 71037/100000: episode: 1290, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 13.267, mean reward: 0.884 [0.824, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.630, 10.100], loss: 0.001450, mae: 0.041243, mean_q: 1.432086
 71061/100000: episode: 1291, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 19.609, mean reward: 0.817 [0.745, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.292, 10.100], loss: 0.001637, mae: 0.044336, mean_q: 1.406955
 71076/100000: episode: 1292, duration: 0.098s, episode steps: 15, steps per second: 153, episode reward: 12.842, mean reward: 0.856 [0.725, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.630, 10.100], loss: 0.002008, mae: 0.049272, mean_q: 1.405681
 71099/100000: episode: 1293, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 19.750, mean reward: 0.859 [0.781, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.436, 10.100], loss: 0.001523, mae: 0.043182, mean_q: 1.416500
 71123/100000: episode: 1294, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 21.976, mean reward: 0.916 [0.870, 0.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.541, 10.100], loss: 0.001817, mae: 0.045896, mean_q: 1.415011
 71142/100000: episode: 1295, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 14.954, mean reward: 0.787 [0.670, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.855, 10.100], loss: 0.001972, mae: 0.048891, mean_q: 1.431034
 71157/100000: episode: 1296, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 12.061, mean reward: 0.804 [0.703, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.255, 10.100], loss: 0.001915, mae: 0.045725, mean_q: 1.420074
 71172/100000: episode: 1297, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 12.757, mean reward: 0.850 [0.764, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.224, 10.100], loss: 0.002270, mae: 0.046657, mean_q: 1.443945
[Info] FALSIFICATION!
 71186/100000: episode: 1298, duration: 0.259s, episode steps: 14, steps per second: 54, episode reward: 12.727, mean reward: 0.909 [0.865, 1.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.495, 10.098], loss: 0.002164, mae: 0.050348, mean_q: 1.420616
 71209/100000: episode: 1299, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 19.346, mean reward: 0.841 [0.777, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.708, 10.100], loss: 0.002133, mae: 0.050454, mean_q: 1.423051
 71228/100000: episode: 1300, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 15.779, mean reward: 0.830 [0.755, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.354, 10.100], loss: 0.002075, mae: 0.045416, mean_q: 1.421685
 71243/100000: episode: 1301, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 12.669, mean reward: 0.845 [0.755, 0.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.345, 10.100], loss: 0.001549, mae: 0.042611, mean_q: 1.402616
 71266/100000: episode: 1302, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 18.122, mean reward: 0.788 [0.724, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.464, 10.100], loss: 0.001737, mae: 0.045507, mean_q: 1.420344
 71281/100000: episode: 1303, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 12.415, mean reward: 0.828 [0.728, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.267, 10.100], loss: 0.001925, mae: 0.048268, mean_q: 1.409254
 71296/100000: episode: 1304, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 13.553, mean reward: 0.904 [0.837, 0.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.298, 10.100], loss: 0.001746, mae: 0.045516, mean_q: 1.419721
 71319/100000: episode: 1305, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 20.630, mean reward: 0.897 [0.793, 0.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.367, 10.100], loss: 0.001783, mae: 0.045239, mean_q: 1.440409
 71340/100000: episode: 1306, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 17.677, mean reward: 0.842 [0.761, 0.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.559, 10.100], loss: 0.001592, mae: 0.042749, mean_q: 1.435826
 71355/100000: episode: 1307, duration: 0.100s, episode steps: 15, steps per second: 151, episode reward: 13.127, mean reward: 0.875 [0.756, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.298, 10.100], loss: 0.001614, mae: 0.043176, mean_q: 1.430495
 71370/100000: episode: 1308, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 13.400, mean reward: 0.893 [0.810, 0.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.391, 10.100], loss: 0.001640, mae: 0.043816, mean_q: 1.443066
 71389/100000: episode: 1309, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 16.223, mean reward: 0.854 [0.773, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.611, 10.100], loss: 0.001610, mae: 0.043642, mean_q: 1.428279
 71409/100000: episode: 1310, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 18.588, mean reward: 0.929 [0.852, 0.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.682, 10.100], loss: 0.001745, mae: 0.044920, mean_q: 1.445820
 71424/100000: episode: 1311, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 12.864, mean reward: 0.858 [0.766, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.468, 10.100], loss: 0.001678, mae: 0.044258, mean_q: 1.449352
[Info] FALSIFICATION!
 71429/100000: episode: 1312, duration: 0.200s, episode steps: 5, steps per second: 25, episode reward: 4.639, mean reward: 0.928 [0.826, 1.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.003, 9.559], loss: 0.001417, mae: 0.041189, mean_q: 1.438810
[Info] FALSIFICATION!
 71451/100000: episode: 1313, duration: 0.310s, episode steps: 22, steps per second: 71, episode reward: 19.318, mean reward: 0.878 [0.790, 1.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.545, 10.100], loss: 0.002211, mae: 0.046624, mean_q: 1.425407
 71466/100000: episode: 1314, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 12.853, mean reward: 0.857 [0.802, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.548, 10.100], loss: 0.002110, mae: 0.044248, mean_q: 1.428204
 71489/100000: episode: 1315, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 19.601, mean reward: 0.852 [0.783, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.842, 10.100], loss: 0.001619, mae: 0.044359, mean_q: 1.438026
 71504/100000: episode: 1316, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 13.806, mean reward: 0.920 [0.865, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.884, 10.100], loss: 0.001812, mae: 0.045813, mean_q: 1.437183
 71523/100000: episode: 1317, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 15.116, mean reward: 0.796 [0.644, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.261, 10.100], loss: 0.002260, mae: 0.053209, mean_q: 1.436901
 71538/100000: episode: 1318, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 12.684, mean reward: 0.846 [0.804, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.311, 10.100], loss: 0.001777, mae: 0.046287, mean_q: 1.445166
 71558/100000: episode: 1319, duration: 0.134s, episode steps: 20, steps per second: 149, episode reward: 16.025, mean reward: 0.801 [0.740, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.333, 10.100], loss: 0.003072, mae: 0.050544, mean_q: 1.456248
 71580/100000: episode: 1320, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 18.120, mean reward: 0.824 [0.730, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.536, 10.100], loss: 0.002355, mae: 0.049983, mean_q: 1.449510
 71603/100000: episode: 1321, duration: 0.132s, episode steps: 23, steps per second: 175, episode reward: 18.620, mean reward: 0.810 [0.748, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.302, 10.100], loss: 0.002488, mae: 0.047994, mean_q: 1.442496
 71618/100000: episode: 1322, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 11.832, mean reward: 0.789 [0.747, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.401, 10.100], loss: 0.002923, mae: 0.053930, mean_q: 1.445453
 71640/100000: episode: 1323, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 16.475, mean reward: 0.749 [0.686, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.607, 10.100], loss: 0.001619, mae: 0.043844, mean_q: 1.439371
 71664/100000: episode: 1324, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 19.458, mean reward: 0.811 [0.688, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.603, 10.100], loss: 0.002072, mae: 0.045963, mean_q: 1.454383
 71683/100000: episode: 1325, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 15.054, mean reward: 0.792 [0.737, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.987, 10.100], loss: 0.001852, mae: 0.048168, mean_q: 1.449947
 71706/100000: episode: 1326, duration: 0.114s, episode steps: 23, steps per second: 201, episode reward: 17.568, mean reward: 0.764 [0.612, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.251, 10.100], loss: 0.002442, mae: 0.053497, mean_q: 1.444632
[Info] FALSIFICATION!
 71708/100000: episode: 1327, duration: 0.224s, episode steps: 2, steps per second: 9, episode reward: 2.020, mean reward: 1.010 [0.969, 1.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.006, 8.942], loss: 0.002406, mae: 0.052400, mean_q: 1.451851
 71727/100000: episode: 1328, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 15.841, mean reward: 0.834 [0.663, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.495, 10.100], loss: 0.002427, mae: 0.049347, mean_q: 1.452003
 71742/100000: episode: 1329, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 12.644, mean reward: 0.843 [0.736, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.318, 10.100], loss: 0.002234, mae: 0.045740, mean_q: 1.452966
 71761/100000: episode: 1330, duration: 0.120s, episode steps: 19, steps per second: 159, episode reward: 15.701, mean reward: 0.826 [0.718, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.313, 10.100], loss: 0.001699, mae: 0.044213, mean_q: 1.443701
[Info] FALSIFICATION!
 71765/100000: episode: 1331, duration: 0.195s, episode steps: 4, steps per second: 20, episode reward: 3.662, mean reward: 0.915 [0.865, 1.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.005, 9.231], loss: 0.002240, mae: 0.048334, mean_q: 1.455902
 71784/100000: episode: 1332, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 16.508, mean reward: 0.869 [0.812, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.383, 10.100], loss: 0.002818, mae: 0.048739, mean_q: 1.452084
 71806/100000: episode: 1333, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 17.241, mean reward: 0.784 [0.710, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.308, 10.100], loss: 0.002339, mae: 0.049640, mean_q: 1.462186
 71821/100000: episode: 1334, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 12.366, mean reward: 0.824 [0.792, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.330, 10.100], loss: 0.002661, mae: 0.049594, mean_q: 1.444418
[Info] FALSIFICATION!
 71837/100000: episode: 1335, duration: 0.353s, episode steps: 16, steps per second: 45, episode reward: 13.374, mean reward: 0.836 [0.758, 1.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.457, 9.988], loss: 0.001607, mae: 0.045404, mean_q: 1.462813
 71856/100000: episode: 1336, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 15.632, mean reward: 0.823 [0.759, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.502, 10.100], loss: 0.003032, mae: 0.048951, mean_q: 1.452108
 71879/100000: episode: 1337, duration: 0.140s, episode steps: 23, steps per second: 164, episode reward: 19.049, mean reward: 0.828 [0.774, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.360, 10.100], loss: 0.001687, mae: 0.045923, mean_q: 1.477939
 71894/100000: episode: 1338, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 13.170, mean reward: 0.878 [0.794, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.400, 10.100], loss: 0.001992, mae: 0.041634, mean_q: 1.451172
 71918/100000: episode: 1339, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 18.843, mean reward: 0.785 [0.650, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.209, 10.100], loss: 0.002302, mae: 0.045794, mean_q: 1.453340
 71941/100000: episode: 1340, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 19.778, mean reward: 0.860 [0.776, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.310, 10.100], loss: 0.002557, mae: 0.047695, mean_q: 1.456744
 71965/100000: episode: 1341, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 17.539, mean reward: 0.731 [0.660, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.062, 10.100], loss: 0.001866, mae: 0.046390, mean_q: 1.470883
 71980/100000: episode: 1342, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 12.697, mean reward: 0.846 [0.805, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.675, 10.100], loss: 0.001626, mae: 0.044672, mean_q: 1.454955
[Info] FALSIFICATION!
 71990/100000: episode: 1343, duration: 0.276s, episode steps: 10, steps per second: 36, episode reward: 9.215, mean reward: 0.922 [0.827, 1.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.305, 9.864], loss: 0.002861, mae: 0.052198, mean_q: 1.425653
 72009/100000: episode: 1344, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 14.948, mean reward: 0.787 [0.659, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.305, 10.100], loss: 0.002688, mae: 0.050670, mean_q: 1.464502
 72024/100000: episode: 1345, duration: 0.093s, episode steps: 15, steps per second: 160, episode reward: 12.044, mean reward: 0.803 [0.664, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.727, 10.100], loss: 0.001721, mae: 0.045953, mean_q: 1.471897
 72045/100000: episode: 1346, duration: 0.134s, episode steps: 21, steps per second: 157, episode reward: 16.231, mean reward: 0.773 [0.631, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.317, 10.100], loss: 0.002139, mae: 0.046382, mean_q: 1.438071
[Info] FALSIFICATION!
 72053/100000: episode: 1347, duration: 0.228s, episode steps: 8, steps per second: 35, episode reward: 7.532, mean reward: 0.941 [0.896, 1.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.141, 9.813], loss: 0.001582, mae: 0.044798, mean_q: 1.442100
[Info] FALSIFICATION!
 72069/100000: episode: 1348, duration: 0.264s, episode steps: 16, steps per second: 61, episode reward: 14.005, mean reward: 0.875 [0.802, 1.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.530, 9.988], loss: 0.002558, mae: 0.049947, mean_q: 1.480165
 72090/100000: episode: 1349, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 17.533, mean reward: 0.835 [0.786, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.400, 10.100], loss: 0.001823, mae: 0.046130, mean_q: 1.475497
[Info] Complete ISplit Iteration
[Info] Levels: [1.4070783, 1.5832576, 1.7005721, 1.7487347]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.49]
[Info] Error Prob: 0.0004900000000000001

 72110/100000: episode: 1350, duration: 4.561s, episode steps: 20, steps per second: 4, episode reward: 16.003, mean reward: 0.800 [0.731, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.213, 10.100], loss: 0.001558, mae: 0.043030, mean_q: 1.464077
 72210/100000: episode: 1351, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 59.794, mean reward: 0.598 [0.506, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.819, 10.329], loss: 0.002991, mae: 0.051675, mean_q: 1.469262
 72310/100000: episode: 1352, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.734, mean reward: 0.577 [0.500, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.585, 10.291], loss: 0.001892, mae: 0.045162, mean_q: 1.468241
 72410/100000: episode: 1353, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 57.073, mean reward: 0.571 [0.507, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.207, 10.098], loss: 0.002025, mae: 0.045474, mean_q: 1.454686
 72510/100000: episode: 1354, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.316, mean reward: 0.583 [0.509, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.831, 10.191], loss: 0.002542, mae: 0.048256, mean_q: 1.457518
 72610/100000: episode: 1355, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.599, mean reward: 0.576 [0.501, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.587, 10.098], loss: 0.002024, mae: 0.045460, mean_q: 1.451728
 72710/100000: episode: 1356, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 60.886, mean reward: 0.609 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.662, 10.273], loss: 0.002004, mae: 0.045549, mean_q: 1.447190
 72810/100000: episode: 1357, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.858, mean reward: 0.609 [0.501, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.677, 10.098], loss: 0.002304, mae: 0.047867, mean_q: 1.447285
 72910/100000: episode: 1358, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 59.739, mean reward: 0.597 [0.514, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.960, 10.172], loss: 0.002218, mae: 0.047879, mean_q: 1.439160
 73010/100000: episode: 1359, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 60.098, mean reward: 0.601 [0.528, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.721, 10.392], loss: 0.002567, mae: 0.049229, mean_q: 1.436417
 73110/100000: episode: 1360, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 56.783, mean reward: 0.568 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.224, 10.206], loss: 0.002309, mae: 0.050041, mean_q: 1.442007
 73210/100000: episode: 1361, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.632, mean reward: 0.586 [0.510, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.686, 10.098], loss: 0.001712, mae: 0.044874, mean_q: 1.428214
 73310/100000: episode: 1362, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.163, mean reward: 0.592 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.728, 10.158], loss: 0.001920, mae: 0.045246, mean_q: 1.427709
 73410/100000: episode: 1363, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 55.768, mean reward: 0.558 [0.501, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.164, 10.129], loss: 0.002797, mae: 0.048767, mean_q: 1.424223
 73510/100000: episode: 1364, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.288, mean reward: 0.593 [0.505, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.493, 10.142], loss: 0.002234, mae: 0.046452, mean_q: 1.406265
 73610/100000: episode: 1365, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.836, mean reward: 0.588 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.821, 10.346], loss: 0.002727, mae: 0.051518, mean_q: 1.405503
 73710/100000: episode: 1366, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.179, mean reward: 0.592 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.807, 10.098], loss: 0.002418, mae: 0.048877, mean_q: 1.410635
 73810/100000: episode: 1367, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.539, mean reward: 0.575 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.672, 10.227], loss: 0.002552, mae: 0.047675, mean_q: 1.388994
 73910/100000: episode: 1368, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.022, mean reward: 0.590 [0.498, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.829, 10.166], loss: 0.002281, mae: 0.045897, mean_q: 1.387369
 74010/100000: episode: 1369, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.701, mean reward: 0.587 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.977, 10.098], loss: 0.001886, mae: 0.045514, mean_q: 1.378627
 74110/100000: episode: 1370, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.945, mean reward: 0.589 [0.506, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.950, 10.098], loss: 0.002330, mae: 0.045884, mean_q: 1.381077
 74210/100000: episode: 1371, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.291, mean reward: 0.593 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.259, 10.098], loss: 0.002176, mae: 0.045956, mean_q: 1.373538
 74310/100000: episode: 1372, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.478, mean reward: 0.605 [0.511, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.887, 10.098], loss: 0.002375, mae: 0.047757, mean_q: 1.368981
 74410/100000: episode: 1373, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.561, mean reward: 0.586 [0.500, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.171, 10.098], loss: 0.002055, mae: 0.046801, mean_q: 1.360222
 74510/100000: episode: 1374, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.233, mean reward: 0.582 [0.504, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.657, 10.098], loss: 0.002044, mae: 0.046737, mean_q: 1.354989
 74610/100000: episode: 1375, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.789, mean reward: 0.588 [0.510, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.254, 10.098], loss: 0.001902, mae: 0.044125, mean_q: 1.340836
 74710/100000: episode: 1376, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.224, mean reward: 0.582 [0.505, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.966, 10.359], loss: 0.002060, mae: 0.046064, mean_q: 1.335019
 74810/100000: episode: 1377, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.390, mean reward: 0.584 [0.504, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.256, 10.108], loss: 0.002351, mae: 0.046075, mean_q: 1.341439
 74910/100000: episode: 1378, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 57.484, mean reward: 0.575 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.460, 10.360], loss: 0.002049, mae: 0.044467, mean_q: 1.323507
 75010/100000: episode: 1379, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 60.393, mean reward: 0.604 [0.499, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.172, 10.173], loss: 0.002105, mae: 0.045828, mean_q: 1.321370
 75110/100000: episode: 1380, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 59.439, mean reward: 0.594 [0.499, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.880, 10.098], loss: 0.002273, mae: 0.046419, mean_q: 1.314737
 75210/100000: episode: 1381, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.467, mean reward: 0.575 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.573, 10.175], loss: 0.002361, mae: 0.046585, mean_q: 1.311014
 75310/100000: episode: 1382, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 62.176, mean reward: 0.622 [0.507, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.406, 10.098], loss: 0.002378, mae: 0.046227, mean_q: 1.297824
 75410/100000: episode: 1383, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 56.287, mean reward: 0.563 [0.505, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.666, 10.098], loss: 0.002293, mae: 0.046163, mean_q: 1.294902
 75510/100000: episode: 1384, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 56.920, mean reward: 0.569 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.946, 10.098], loss: 0.002112, mae: 0.045073, mean_q: 1.288974
 75610/100000: episode: 1385, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 61.780, mean reward: 0.618 [0.509, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.710, 10.098], loss: 0.002422, mae: 0.047118, mean_q: 1.278783
 75710/100000: episode: 1386, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 60.339, mean reward: 0.603 [0.511, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.650, 10.231], loss: 0.001943, mae: 0.044497, mean_q: 1.276387
 75810/100000: episode: 1387, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.657, mean reward: 0.577 [0.504, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.714, 10.098], loss: 0.002091, mae: 0.044470, mean_q: 1.266728
 75910/100000: episode: 1388, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.546, mean reward: 0.585 [0.498, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.434, 10.098], loss: 0.002151, mae: 0.045514, mean_q: 1.262739
 76010/100000: episode: 1389, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.403, mean reward: 0.574 [0.502, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.164, 10.098], loss: 0.001957, mae: 0.043615, mean_q: 1.250760
 76110/100000: episode: 1390, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 60.526, mean reward: 0.605 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.365, 10.156], loss: 0.001823, mae: 0.042807, mean_q: 1.239672
 76210/100000: episode: 1391, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.875, mean reward: 0.579 [0.509, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.913, 10.098], loss: 0.001626, mae: 0.042471, mean_q: 1.238093
 76310/100000: episode: 1392, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.527, mean reward: 0.575 [0.501, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.784, 10.202], loss: 0.001711, mae: 0.042362, mean_q: 1.226399
 76410/100000: episode: 1393, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.494, mean reward: 0.575 [0.500, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.679, 10.244], loss: 0.001673, mae: 0.042481, mean_q: 1.215576
 76510/100000: episode: 1394, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.789, mean reward: 0.598 [0.504, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.863, 10.098], loss: 0.002009, mae: 0.045384, mean_q: 1.211360
 76610/100000: episode: 1395, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.717, mean reward: 0.587 [0.514, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.503, 10.098], loss: 0.001744, mae: 0.042806, mean_q: 1.205407
 76710/100000: episode: 1396, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.332, mean reward: 0.593 [0.508, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.702, 10.098], loss: 0.001519, mae: 0.042292, mean_q: 1.198993
 76810/100000: episode: 1397, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 62.855, mean reward: 0.629 [0.507, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.575, 10.397], loss: 0.001701, mae: 0.042390, mean_q: 1.180921
 76910/100000: episode: 1398, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.732, mean reward: 0.587 [0.507, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.524, 10.166], loss: 0.001837, mae: 0.043419, mean_q: 1.178609
 77010/100000: episode: 1399, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.494, mean reward: 0.575 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.402, 10.115], loss: 0.001402, mae: 0.040951, mean_q: 1.176294
 77110/100000: episode: 1400, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 57.744, mean reward: 0.577 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.757, 10.151], loss: 0.001451, mae: 0.041583, mean_q: 1.167772
 77210/100000: episode: 1401, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 55.645, mean reward: 0.556 [0.499, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.041, 10.098], loss: 0.001257, mae: 0.038959, mean_q: 1.163274
 77310/100000: episode: 1402, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 60.040, mean reward: 0.600 [0.501, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.343, 10.485], loss: 0.001338, mae: 0.040423, mean_q: 1.162263
 77410/100000: episode: 1403, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.919, mean reward: 0.579 [0.503, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.881, 10.145], loss: 0.001362, mae: 0.040626, mean_q: 1.159381
 77510/100000: episode: 1404, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.369, mean reward: 0.584 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.072, 10.108], loss: 0.001359, mae: 0.039983, mean_q: 1.158922
 77610/100000: episode: 1405, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 62.796, mean reward: 0.628 [0.501, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.911, 10.538], loss: 0.001274, mae: 0.039468, mean_q: 1.161726
 77710/100000: episode: 1406, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.277, mean reward: 0.573 [0.501, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.284, 10.098], loss: 0.001261, mae: 0.039039, mean_q: 1.161704
 77810/100000: episode: 1407, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.350, mean reward: 0.584 [0.512, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.187, 10.125], loss: 0.001320, mae: 0.040061, mean_q: 1.160021
 77910/100000: episode: 1408, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 59.286, mean reward: 0.593 [0.516, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.444, 10.319], loss: 0.001382, mae: 0.040588, mean_q: 1.162401
 78010/100000: episode: 1409, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.119, mean reward: 0.581 [0.515, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.355, 10.098], loss: 0.001377, mae: 0.040905, mean_q: 1.157845
 78110/100000: episode: 1410, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 62.202, mean reward: 0.622 [0.514, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.013, 10.098], loss: 0.001463, mae: 0.041481, mean_q: 1.162170
 78210/100000: episode: 1411, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.047, mean reward: 0.580 [0.502, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.870, 10.116], loss: 0.001498, mae: 0.042551, mean_q: 1.162546
 78310/100000: episode: 1412, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.548, mean reward: 0.595 [0.516, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.257], loss: 0.001336, mae: 0.040195, mean_q: 1.165477
 78410/100000: episode: 1413, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.756, mean reward: 0.588 [0.505, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.435, 10.098], loss: 0.001421, mae: 0.041453, mean_q: 1.165999
 78510/100000: episode: 1414, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.676, mean reward: 0.587 [0.504, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.844, 10.298], loss: 0.001368, mae: 0.040746, mean_q: 1.162318
 78610/100000: episode: 1415, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.577, mean reward: 0.576 [0.504, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.369, 10.152], loss: 0.001361, mae: 0.040693, mean_q: 1.162775
 78710/100000: episode: 1416, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.551, mean reward: 0.586 [0.504, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.077, 10.145], loss: 0.001360, mae: 0.040359, mean_q: 1.159948
 78810/100000: episode: 1417, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.047, mean reward: 0.600 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.085, 10.098], loss: 0.001361, mae: 0.040284, mean_q: 1.160875
 78910/100000: episode: 1418, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 56.869, mean reward: 0.569 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.060, 10.098], loss: 0.001470, mae: 0.041661, mean_q: 1.165868
 79010/100000: episode: 1419, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.807, mean reward: 0.578 [0.499, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.692, 10.098], loss: 0.001431, mae: 0.041635, mean_q: 1.162981
 79110/100000: episode: 1420, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.148, mean reward: 0.601 [0.499, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.806, 10.312], loss: 0.001464, mae: 0.041991, mean_q: 1.164253
 79210/100000: episode: 1421, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.751, mean reward: 0.578 [0.503, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.610, 10.234], loss: 0.001557, mae: 0.043086, mean_q: 1.162070
 79310/100000: episode: 1422, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 61.483, mean reward: 0.615 [0.503, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.161, 10.098], loss: 0.001448, mae: 0.041545, mean_q: 1.162109
 79410/100000: episode: 1423, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 56.782, mean reward: 0.568 [0.501, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.996, 10.098], loss: 0.001336, mae: 0.040448, mean_q: 1.162905
 79510/100000: episode: 1424, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.597, mean reward: 0.606 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.250, 10.408], loss: 0.001517, mae: 0.042584, mean_q: 1.161029
 79610/100000: episode: 1425, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 56.679, mean reward: 0.567 [0.500, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.822, 10.098], loss: 0.001475, mae: 0.041919, mean_q: 1.163381
 79710/100000: episode: 1426, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 61.286, mean reward: 0.613 [0.524, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.963, 10.288], loss: 0.001441, mae: 0.041484, mean_q: 1.163402
 79810/100000: episode: 1427, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.234, mean reward: 0.592 [0.498, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.702, 10.392], loss: 0.001410, mae: 0.041182, mean_q: 1.165805
 79910/100000: episode: 1428, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 61.081, mean reward: 0.611 [0.507, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.442, 10.098], loss: 0.001312, mae: 0.040230, mean_q: 1.165573
 80010/100000: episode: 1429, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.311, mean reward: 0.583 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.814, 10.155], loss: 0.001458, mae: 0.041678, mean_q: 1.163153
 80110/100000: episode: 1430, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.081, mean reward: 0.581 [0.503, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.541, 10.098], loss: 0.001481, mae: 0.041733, mean_q: 1.162957
 80210/100000: episode: 1431, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.830, mean reward: 0.608 [0.518, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.533, 10.442], loss: 0.001368, mae: 0.041125, mean_q: 1.166003
 80310/100000: episode: 1432, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.628, mean reward: 0.586 [0.509, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.466, 10.203], loss: 0.001350, mae: 0.040106, mean_q: 1.163132
 80410/100000: episode: 1433, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.160, mean reward: 0.582 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.552, 10.098], loss: 0.001467, mae: 0.041612, mean_q: 1.162873
 80510/100000: episode: 1434, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.811, mean reward: 0.588 [0.505, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.858, 10.220], loss: 0.001494, mae: 0.042439, mean_q: 1.168025
 80610/100000: episode: 1435, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.715, mean reward: 0.577 [0.510, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.036, 10.199], loss: 0.001590, mae: 0.043026, mean_q: 1.164205
 80710/100000: episode: 1436, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.100, mean reward: 0.591 [0.522, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.376, 10.192], loss: 0.001381, mae: 0.040719, mean_q: 1.159945
 80810/100000: episode: 1437, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.314, mean reward: 0.573 [0.503, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.310, 10.098], loss: 0.001319, mae: 0.039926, mean_q: 1.163361
 80910/100000: episode: 1438, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.685, mean reward: 0.577 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.924, 10.187], loss: 0.001409, mae: 0.041717, mean_q: 1.164304
 81010/100000: episode: 1439, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.929, mean reward: 0.599 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.341, 10.246], loss: 0.001347, mae: 0.040032, mean_q: 1.163466
 81110/100000: episode: 1440, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.280, mean reward: 0.583 [0.498, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.488, 10.098], loss: 0.001305, mae: 0.039971, mean_q: 1.165563
 81210/100000: episode: 1441, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.776, mean reward: 0.578 [0.507, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.205, 10.128], loss: 0.001325, mae: 0.040177, mean_q: 1.160080
 81310/100000: episode: 1442, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.746, mean reward: 0.587 [0.512, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.793, 10.098], loss: 0.001459, mae: 0.042069, mean_q: 1.163583
 81410/100000: episode: 1443, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 61.432, mean reward: 0.614 [0.516, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.876, 10.098], loss: 0.001384, mae: 0.040700, mean_q: 1.163850
 81510/100000: episode: 1444, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.972, mean reward: 0.580 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.998, 10.108], loss: 0.001370, mae: 0.040231, mean_q: 1.164831
 81610/100000: episode: 1445, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.419, mean reward: 0.594 [0.509, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.923, 10.098], loss: 0.001413, mae: 0.041266, mean_q: 1.165207
 81710/100000: episode: 1446, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.516, mean reward: 0.575 [0.500, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.886, 10.221], loss: 0.001453, mae: 0.041741, mean_q: 1.162166
 81810/100000: episode: 1447, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.717, mean reward: 0.587 [0.509, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.537, 10.277], loss: 0.001495, mae: 0.041917, mean_q: 1.165602
 81910/100000: episode: 1448, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.774, mean reward: 0.598 [0.503, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.625, 10.098], loss: 0.001440, mae: 0.040983, mean_q: 1.162494
 82010/100000: episode: 1449, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.510, mean reward: 0.575 [0.504, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.134, 10.098], loss: 0.001389, mae: 0.040684, mean_q: 1.163797
[Info] 1-TH LEVEL FOUND: 1.34108567237854, Considering 10/90 traces
 82110/100000: episode: 1450, duration: 4.748s, episode steps: 100, steps per second: 21, episode reward: 57.482, mean reward: 0.575 [0.502, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.436, 10.098], loss: 0.001517, mae: 0.042226, mean_q: 1.163764
 82182/100000: episode: 1451, duration: 0.384s, episode steps: 72, steps per second: 188, episode reward: 44.077, mean reward: 0.612 [0.512, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.724 [-0.482, 10.142], loss: 0.001251, mae: 0.038999, mean_q: 1.169582
 82215/100000: episode: 1452, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 21.753, mean reward: 0.659 [0.563, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.546, 10.344], loss: 0.001402, mae: 0.041013, mean_q: 1.161796
 82241/100000: episode: 1453, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 17.654, mean reward: 0.679 [0.601, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.394], loss: 0.001514, mae: 0.042105, mean_q: 1.173670
 82269/100000: episode: 1454, duration: 0.171s, episode steps: 28, steps per second: 164, episode reward: 17.595, mean reward: 0.628 [0.560, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.725, 10.100], loss: 0.001747, mae: 0.043962, mean_q: 1.164225
 82295/100000: episode: 1455, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 18.041, mean reward: 0.694 [0.576, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.232], loss: 0.001755, mae: 0.044359, mean_q: 1.159818
 82334/100000: episode: 1456, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 25.558, mean reward: 0.655 [0.534, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.978, 10.100], loss: 0.001619, mae: 0.042531, mean_q: 1.166500
 82406/100000: episode: 1457, duration: 0.410s, episode steps: 72, steps per second: 176, episode reward: 43.449, mean reward: 0.603 [0.504, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.717 [-0.686, 10.192], loss: 0.001371, mae: 0.040892, mean_q: 1.166870
 82444/100000: episode: 1458, duration: 0.205s, episode steps: 38, steps per second: 185, episode reward: 23.150, mean reward: 0.609 [0.520, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.813, 10.239], loss: 0.001421, mae: 0.041273, mean_q: 1.170034
 82483/100000: episode: 1459, duration: 0.215s, episode steps: 39, steps per second: 181, episode reward: 27.358, mean reward: 0.701 [0.649, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.684, 10.100], loss: 0.001314, mae: 0.038966, mean_q: 1.175250
 82509/100000: episode: 1460, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 17.132, mean reward: 0.659 [0.606, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.351], loss: 0.001360, mae: 0.040693, mean_q: 1.175831
 82547/100000: episode: 1461, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 25.902, mean reward: 0.682 [0.521, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.235, 10.250], loss: 0.001545, mae: 0.042581, mean_q: 1.171416
 82586/100000: episode: 1462, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 27.168, mean reward: 0.697 [0.617, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.411, 10.100], loss: 0.001590, mae: 0.042768, mean_q: 1.170026
 82633/100000: episode: 1463, duration: 0.262s, episode steps: 47, steps per second: 179, episode reward: 33.053, mean reward: 0.703 [0.593, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.248, 10.100], loss: 0.001504, mae: 0.042544, mean_q: 1.173276
 82680/100000: episode: 1464, duration: 0.248s, episode steps: 47, steps per second: 189, episode reward: 29.862, mean reward: 0.635 [0.518, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.212, 10.100], loss: 0.001697, mae: 0.045303, mean_q: 1.175741
 82706/100000: episode: 1465, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 19.173, mean reward: 0.737 [0.654, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.507, 10.313], loss: 0.001465, mae: 0.041285, mean_q: 1.182215
 82734/100000: episode: 1466, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 19.370, mean reward: 0.692 [0.571, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.593, 10.100], loss: 0.001481, mae: 0.040889, mean_q: 1.170457
 82787/100000: episode: 1467, duration: 0.298s, episode steps: 53, steps per second: 178, episode reward: 31.056, mean reward: 0.586 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.515, 10.100], loss: 0.001508, mae: 0.042031, mean_q: 1.174330
 82834/100000: episode: 1468, duration: 0.285s, episode steps: 47, steps per second: 165, episode reward: 29.021, mean reward: 0.617 [0.502, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.541, 10.165], loss: 0.001705, mae: 0.043495, mean_q: 1.173946
 82887/100000: episode: 1469, duration: 0.284s, episode steps: 53, steps per second: 187, episode reward: 32.068, mean reward: 0.605 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.957, 10.103], loss: 0.001558, mae: 0.042285, mean_q: 1.182195
 82934/100000: episode: 1470, duration: 0.259s, episode steps: 47, steps per second: 182, episode reward: 28.270, mean reward: 0.601 [0.510, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.604, 10.201], loss: 0.001687, mae: 0.043987, mean_q: 1.175570
 82960/100000: episode: 1471, duration: 0.132s, episode steps: 26, steps per second: 197, episode reward: 16.553, mean reward: 0.637 [0.580, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.210, 10.392], loss: 0.001257, mae: 0.039187, mean_q: 1.177975
 82986/100000: episode: 1472, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 18.750, mean reward: 0.721 [0.634, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.307, 10.515], loss: 0.001538, mae: 0.042534, mean_q: 1.182752
 83025/100000: episode: 1473, duration: 0.227s, episode steps: 39, steps per second: 172, episode reward: 25.601, mean reward: 0.656 [0.587, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-1.467, 10.100], loss: 0.001650, mae: 0.043439, mean_q: 1.168454
 83077/100000: episode: 1474, duration: 0.297s, episode steps: 52, steps per second: 175, episode reward: 32.795, mean reward: 0.631 [0.545, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.530, 10.100], loss: 0.001469, mae: 0.041126, mean_q: 1.180845
 83110/100000: episode: 1475, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 25.516, mean reward: 0.773 [0.677, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.558, 10.507], loss: 0.001718, mae: 0.045128, mean_q: 1.178081
 83148/100000: episode: 1476, duration: 0.234s, episode steps: 38, steps per second: 163, episode reward: 24.879, mean reward: 0.655 [0.574, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.731, 10.490], loss: 0.001653, mae: 0.043118, mean_q: 1.178461
 83181/100000: episode: 1477, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 22.447, mean reward: 0.680 [0.613, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.186, 10.422], loss: 0.001529, mae: 0.042014, mean_q: 1.177955
 83214/100000: episode: 1478, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 23.967, mean reward: 0.726 [0.678, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.523, 10.397], loss: 0.001601, mae: 0.043407, mean_q: 1.189392
 83242/100000: episode: 1479, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 19.720, mean reward: 0.704 [0.560, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.900, 10.100], loss: 0.001312, mae: 0.039019, mean_q: 1.175381
 83268/100000: episode: 1480, duration: 0.157s, episode steps: 26, steps per second: 166, episode reward: 18.956, mean reward: 0.729 [0.652, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.452], loss: 0.001461, mae: 0.039969, mean_q: 1.182298
 83340/100000: episode: 1481, duration: 0.386s, episode steps: 72, steps per second: 187, episode reward: 43.109, mean reward: 0.599 [0.506, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-0.752, 10.346], loss: 0.001542, mae: 0.042006, mean_q: 1.186752
 83412/100000: episode: 1482, duration: 0.406s, episode steps: 72, steps per second: 177, episode reward: 43.649, mean reward: 0.606 [0.516, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.722 [-0.958, 10.199], loss: 0.001501, mae: 0.042108, mean_q: 1.187688
 83459/100000: episode: 1483, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 31.712, mean reward: 0.675 [0.592, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.033, 10.100], loss: 0.001520, mae: 0.042264, mean_q: 1.192751
 83487/100000: episode: 1484, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 20.630, mean reward: 0.737 [0.614, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.400, 10.100], loss: 0.001599, mae: 0.043252, mean_q: 1.189273
 83525/100000: episode: 1485, duration: 0.214s, episode steps: 38, steps per second: 177, episode reward: 27.020, mean reward: 0.711 [0.661, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.400, 10.437], loss: 0.001620, mae: 0.043616, mean_q: 1.195273
 83597/100000: episode: 1486, duration: 0.392s, episode steps: 72, steps per second: 184, episode reward: 42.703, mean reward: 0.593 [0.500, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.725 [-0.676, 10.299], loss: 0.001542, mae: 0.042345, mean_q: 1.197815
 83636/100000: episode: 1487, duration: 0.226s, episode steps: 39, steps per second: 173, episode reward: 24.571, mean reward: 0.630 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.489, 10.100], loss: 0.001463, mae: 0.042173, mean_q: 1.189605
 83683/100000: episode: 1488, duration: 0.246s, episode steps: 47, steps per second: 191, episode reward: 28.685, mean reward: 0.610 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.480, 10.100], loss: 0.001496, mae: 0.041797, mean_q: 1.198310
 83730/100000: episode: 1489, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 29.356, mean reward: 0.625 [0.534, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-1.086, 10.161], loss: 0.001526, mae: 0.041702, mean_q: 1.196740
 83777/100000: episode: 1490, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 31.109, mean reward: 0.662 [0.544, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.310, 10.209], loss: 0.001555, mae: 0.042335, mean_q: 1.190407
 83849/100000: episode: 1491, duration: 0.379s, episode steps: 72, steps per second: 190, episode reward: 46.478, mean reward: 0.646 [0.520, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-0.546, 10.110], loss: 0.001584, mae: 0.042291, mean_q: 1.195042
 83896/100000: episode: 1492, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 28.510, mean reward: 0.607 [0.517, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.668, 10.100], loss: 0.001610, mae: 0.043067, mean_q: 1.196751
 83922/100000: episode: 1493, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 20.016, mean reward: 0.770 [0.644, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.609, 10.477], loss: 0.001619, mae: 0.042034, mean_q: 1.202800
 83994/100000: episode: 1494, duration: 0.388s, episode steps: 72, steps per second: 186, episode reward: 42.999, mean reward: 0.597 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.716 [-0.493, 10.100], loss: 0.001565, mae: 0.042348, mean_q: 1.199978
 84027/100000: episode: 1495, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 23.935, mean reward: 0.725 [0.634, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.930, 10.431], loss: 0.001569, mae: 0.042584, mean_q: 1.203551
 84053/100000: episode: 1496, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 17.150, mean reward: 0.660 [0.600, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.429], loss: 0.001409, mae: 0.041029, mean_q: 1.202629
 84096/100000: episode: 1497, duration: 0.246s, episode steps: 43, steps per second: 175, episode reward: 27.366, mean reward: 0.636 [0.516, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-1.198, 10.152], loss: 0.001488, mae: 0.040738, mean_q: 1.203025
 84124/100000: episode: 1498, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 17.125, mean reward: 0.612 [0.533, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.107, 10.100], loss: 0.001405, mae: 0.040008, mean_q: 1.205718
 84157/100000: episode: 1499, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 21.115, mean reward: 0.640 [0.547, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.481, 10.377], loss: 0.001778, mae: 0.044720, mean_q: 1.206882
 84200/100000: episode: 1500, duration: 0.229s, episode steps: 43, steps per second: 188, episode reward: 25.999, mean reward: 0.605 [0.551, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.478, 10.100], loss: 0.001521, mae: 0.041513, mean_q: 1.208492
 84238/100000: episode: 1501, duration: 0.197s, episode steps: 38, steps per second: 192, episode reward: 25.026, mean reward: 0.659 [0.570, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.066, 10.261], loss: 0.001499, mae: 0.041465, mean_q: 1.208147
 84285/100000: episode: 1502, duration: 0.245s, episode steps: 47, steps per second: 192, episode reward: 27.021, mean reward: 0.575 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.704, 10.116], loss: 0.001539, mae: 0.042462, mean_q: 1.200200
 84332/100000: episode: 1503, duration: 0.270s, episode steps: 47, steps per second: 174, episode reward: 32.998, mean reward: 0.702 [0.580, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-1.694, 10.100], loss: 0.001498, mae: 0.040813, mean_q: 1.203646
 84404/100000: episode: 1504, duration: 0.382s, episode steps: 72, steps per second: 188, episode reward: 42.648, mean reward: 0.592 [0.500, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.716 [-1.732, 10.100], loss: 0.001594, mae: 0.042393, mean_q: 1.204243
 84430/100000: episode: 1505, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 17.588, mean reward: 0.676 [0.616, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.408], loss: 0.001616, mae: 0.041893, mean_q: 1.208313
 84456/100000: episode: 1506, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 20.255, mean reward: 0.779 [0.655, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.388, 10.544], loss: 0.001587, mae: 0.042599, mean_q: 1.197408
 84499/100000: episode: 1507, duration: 0.244s, episode steps: 43, steps per second: 176, episode reward: 27.519, mean reward: 0.640 [0.548, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.248, 10.100], loss: 0.001515, mae: 0.042036, mean_q: 1.211560
 84551/100000: episode: 1508, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 38.372, mean reward: 0.738 [0.609, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.764, 10.100], loss: 0.001584, mae: 0.043308, mean_q: 1.211945
 84604/100000: episode: 1509, duration: 0.270s, episode steps: 53, steps per second: 196, episode reward: 32.367, mean reward: 0.611 [0.509, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.907, 10.321], loss: 0.001875, mae: 0.046263, mean_q: 1.212957
 84630/100000: episode: 1510, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 18.733, mean reward: 0.721 [0.647, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.411], loss: 0.001642, mae: 0.043070, mean_q: 1.211952
 84669/100000: episode: 1511, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 24.948, mean reward: 0.640 [0.565, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.187, 10.100], loss: 0.001706, mae: 0.044756, mean_q: 1.217004
 84741/100000: episode: 1512, duration: 0.384s, episode steps: 72, steps per second: 188, episode reward: 42.971, mean reward: 0.597 [0.511, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.721 [-1.178, 10.100], loss: 0.001580, mae: 0.042278, mean_q: 1.212993
 84788/100000: episode: 1513, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 29.192, mean reward: 0.621 [0.503, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-1.179, 10.100], loss: 0.001489, mae: 0.041623, mean_q: 1.213750
 84841/100000: episode: 1514, duration: 0.280s, episode steps: 53, steps per second: 189, episode reward: 33.648, mean reward: 0.635 [0.549, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.112, 10.100], loss: 0.001504, mae: 0.041679, mean_q: 1.220662
 84874/100000: episode: 1515, duration: 0.202s, episode steps: 33, steps per second: 163, episode reward: 24.186, mean reward: 0.733 [0.613, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.277, 10.466], loss: 0.001456, mae: 0.041402, mean_q: 1.220737
 84907/100000: episode: 1516, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 23.717, mean reward: 0.719 [0.642, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.360], loss: 0.001490, mae: 0.041633, mean_q: 1.217911
 84954/100000: episode: 1517, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 28.303, mean reward: 0.602 [0.507, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-1.005, 10.100], loss: 0.001949, mae: 0.046952, mean_q: 1.214718
 85007/100000: episode: 1518, duration: 0.287s, episode steps: 53, steps per second: 185, episode reward: 35.633, mean reward: 0.672 [0.580, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.450, 10.100], loss: 0.001743, mae: 0.045399, mean_q: 1.219200
 85040/100000: episode: 1519, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 22.969, mean reward: 0.696 [0.524, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.035, 10.209], loss: 0.001591, mae: 0.042967, mean_q: 1.218135
 85087/100000: episode: 1520, duration: 0.251s, episode steps: 47, steps per second: 188, episode reward: 30.939, mean reward: 0.658 [0.580, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.839, 10.100], loss: 0.001678, mae: 0.044113, mean_q: 1.221754
 85139/100000: episode: 1521, duration: 0.269s, episode steps: 52, steps per second: 193, episode reward: 37.728, mean reward: 0.726 [0.599, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.843, 10.100], loss: 0.001731, mae: 0.045014, mean_q: 1.217417
 85177/100000: episode: 1522, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 26.088, mean reward: 0.687 [0.621, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.039, 10.408], loss: 0.001723, mae: 0.044766, mean_q: 1.224829
 85215/100000: episode: 1523, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 27.081, mean reward: 0.713 [0.654, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.773, 10.435], loss: 0.001690, mae: 0.043913, mean_q: 1.222739
 85258/100000: episode: 1524, duration: 0.242s, episode steps: 43, steps per second: 177, episode reward: 27.513, mean reward: 0.640 [0.540, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.839, 10.100], loss: 0.001678, mae: 0.043204, mean_q: 1.222677
 85310/100000: episode: 1525, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 35.743, mean reward: 0.687 [0.524, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.864, 10.285], loss: 0.001731, mae: 0.044168, mean_q: 1.221887
 85357/100000: episode: 1526, duration: 0.265s, episode steps: 47, steps per second: 178, episode reward: 32.227, mean reward: 0.686 [0.605, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.972, 10.100], loss: 0.001939, mae: 0.047091, mean_q: 1.231299
 85410/100000: episode: 1527, duration: 0.285s, episode steps: 53, steps per second: 186, episode reward: 30.327, mean reward: 0.572 [0.521, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.502, 10.144], loss: 0.001895, mae: 0.046937, mean_q: 1.235693
 85482/100000: episode: 1528, duration: 0.409s, episode steps: 72, steps per second: 176, episode reward: 41.875, mean reward: 0.582 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.711 [-1.034, 10.100], loss: 0.001628, mae: 0.043455, mean_q: 1.231022
 85534/100000: episode: 1529, duration: 0.283s, episode steps: 52, steps per second: 184, episode reward: 30.540, mean reward: 0.587 [0.511, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.820, 10.100], loss: 0.001497, mae: 0.041259, mean_q: 1.232514
 85572/100000: episode: 1530, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 26.527, mean reward: 0.698 [0.623, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.399, 10.308], loss: 0.001624, mae: 0.043373, mean_q: 1.236116
 85644/100000: episode: 1531, duration: 0.380s, episode steps: 72, steps per second: 190, episode reward: 43.895, mean reward: 0.610 [0.510, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.730 [-0.826, 10.370], loss: 0.001787, mae: 0.044632, mean_q: 1.235067
 85682/100000: episode: 1532, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 23.669, mean reward: 0.623 [0.512, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.564, 10.100], loss: 0.001638, mae: 0.043610, mean_q: 1.230615
 85725/100000: episode: 1533, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 28.130, mean reward: 0.654 [0.562, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.822, 10.100], loss: 0.001575, mae: 0.042668, mean_q: 1.236208
 85753/100000: episode: 1534, duration: 0.149s, episode steps: 28, steps per second: 187, episode reward: 20.469, mean reward: 0.731 [0.566, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.216, 10.100], loss: 0.001623, mae: 0.042899, mean_q: 1.230374
 85806/100000: episode: 1535, duration: 0.298s, episode steps: 53, steps per second: 178, episode reward: 32.155, mean reward: 0.607 [0.522, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.035, 10.368], loss: 0.001804, mae: 0.044311, mean_q: 1.237368
 85858/100000: episode: 1536, duration: 0.296s, episode steps: 52, steps per second: 175, episode reward: 32.043, mean reward: 0.616 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.209, 10.100], loss: 0.001641, mae: 0.043734, mean_q: 1.235170
 85891/100000: episode: 1537, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 22.756, mean reward: 0.690 [0.637, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.035, 10.264], loss: 0.001589, mae: 0.042541, mean_q: 1.246405
 85924/100000: episode: 1538, duration: 0.172s, episode steps: 33, steps per second: 191, episode reward: 21.294, mean reward: 0.645 [0.547, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.197], loss: 0.001381, mae: 0.039648, mean_q: 1.245146
 85950/100000: episode: 1539, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 18.635, mean reward: 0.717 [0.627, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.499, 10.395], loss: 0.001716, mae: 0.045085, mean_q: 1.244359
[Info] 2-TH LEVEL FOUND: 1.5467543601989746, Considering 10/90 traces
 85978/100000: episode: 1540, duration: 4.389s, episode steps: 28, steps per second: 6, episode reward: 18.838, mean reward: 0.673 [0.578, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.192, 10.100], loss: 0.001455, mae: 0.041114, mean_q: 1.239904
 86007/100000: episode: 1541, duration: 0.161s, episode steps: 29, steps per second: 181, episode reward: 21.532, mean reward: 0.742 [0.705, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.886, 10.531], loss: 0.001681, mae: 0.044143, mean_q: 1.240816
 86021/100000: episode: 1542, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 10.587, mean reward: 0.756 [0.677, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.475], loss: 0.001560, mae: 0.042161, mean_q: 1.230286
 86033/100000: episode: 1543, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 8.265, mean reward: 0.689 [0.646, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.082, 10.419], loss: 0.001626, mae: 0.043034, mean_q: 1.252341
 86064/100000: episode: 1544, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 24.624, mean reward: 0.794 [0.697, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.317, 10.466], loss: 0.001538, mae: 0.041937, mean_q: 1.242824
 86096/100000: episode: 1545, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 21.192, mean reward: 0.662 [0.525, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.262, 10.156], loss: 0.001601, mae: 0.043425, mean_q: 1.251148
 86121/100000: episode: 1546, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 18.662, mean reward: 0.746 [0.606, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.035, 10.408], loss: 0.001911, mae: 0.045230, mean_q: 1.252816
 86143/100000: episode: 1547, duration: 0.125s, episode steps: 22, steps per second: 177, episode reward: 16.640, mean reward: 0.756 [0.650, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.340], loss: 0.001515, mae: 0.042773, mean_q: 1.256263
 86161/100000: episode: 1548, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 12.686, mean reward: 0.705 [0.596, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.331], loss: 0.001677, mae: 0.045178, mean_q: 1.244893
 86179/100000: episode: 1549, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 13.690, mean reward: 0.761 [0.681, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.506], loss: 0.001504, mae: 0.042230, mean_q: 1.258690
 86191/100000: episode: 1550, duration: 0.079s, episode steps: 12, steps per second: 152, episode reward: 9.288, mean reward: 0.774 [0.719, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.373, 10.100], loss: 0.001707, mae: 0.042511, mean_q: 1.248647
 86216/100000: episode: 1551, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 19.628, mean reward: 0.785 [0.666, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.361], loss: 0.001765, mae: 0.044119, mean_q: 1.243100
 86245/100000: episode: 1552, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 20.533, mean reward: 0.708 [0.637, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.452], loss: 0.001659, mae: 0.043812, mean_q: 1.256633
 86267/100000: episode: 1553, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 16.742, mean reward: 0.761 [0.590, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.171, 10.311], loss: 0.001852, mae: 0.046707, mean_q: 1.266190
 86279/100000: episode: 1554, duration: 0.073s, episode steps: 12, steps per second: 163, episode reward: 9.968, mean reward: 0.831 [0.771, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.483, 10.100], loss: 0.001388, mae: 0.041114, mean_q: 1.255306
 86310/100000: episode: 1555, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 23.856, mean reward: 0.770 [0.685, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.955, 10.555], loss: 0.001775, mae: 0.045624, mean_q: 1.256106
 86332/100000: episode: 1556, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 16.337, mean reward: 0.743 [0.639, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.358], loss: 0.001692, mae: 0.045139, mean_q: 1.267983
 86357/100000: episode: 1557, duration: 0.127s, episode steps: 25, steps per second: 198, episode reward: 17.383, mean reward: 0.695 [0.556, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.222, 10.244], loss: 0.001616, mae: 0.043835, mean_q: 1.260055
 86382/100000: episode: 1558, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 20.439, mean reward: 0.818 [0.711, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.388, 10.533], loss: 0.001620, mae: 0.041949, mean_q: 1.257435
 86394/100000: episode: 1559, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 9.323, mean reward: 0.777 [0.709, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.655], loss: 0.001418, mae: 0.040061, mean_q: 1.276220
 86423/100000: episode: 1560, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 21.746, mean reward: 0.750 [0.627, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.448, 10.612], loss: 0.001645, mae: 0.043514, mean_q: 1.276484
 86448/100000: episode: 1561, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 19.096, mean reward: 0.764 [0.651, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.051, 10.389], loss: 0.001892, mae: 0.045219, mean_q: 1.267772
 86460/100000: episode: 1562, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 10.437, mean reward: 0.870 [0.832, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.312, 10.100], loss: 0.001511, mae: 0.042013, mean_q: 1.267497
 86472/100000: episode: 1563, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 9.053, mean reward: 0.754 [0.726, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.935, 10.605], loss: 0.001511, mae: 0.041971, mean_q: 1.266811
 86503/100000: episode: 1564, duration: 0.179s, episode steps: 31, steps per second: 174, episode reward: 26.219, mean reward: 0.846 [0.764, 0.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.167, 10.485], loss: 0.001660, mae: 0.043939, mean_q: 1.275552
 86521/100000: episode: 1565, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 14.156, mean reward: 0.786 [0.731, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.488], loss: 0.001809, mae: 0.045458, mean_q: 1.269583
 86550/100000: episode: 1566, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 20.241, mean reward: 0.698 [0.544, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.591, 10.274], loss: 0.001823, mae: 0.045671, mean_q: 1.267033
 86562/100000: episode: 1567, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 10.029, mean reward: 0.836 [0.782, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.449, 10.100], loss: 0.001974, mae: 0.045672, mean_q: 1.254136
 86594/100000: episode: 1568, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 22.918, mean reward: 0.716 [0.609, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.102, 10.381], loss: 0.002023, mae: 0.048755, mean_q: 1.280401
 86606/100000: episode: 1569, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 9.678, mean reward: 0.807 [0.745, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.421, 10.100], loss: 0.001531, mae: 0.042645, mean_q: 1.283399
 86620/100000: episode: 1570, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 10.633, mean reward: 0.759 [0.710, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.254, 10.479], loss: 0.001623, mae: 0.043497, mean_q: 1.277444
 86645/100000: episode: 1571, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 17.766, mean reward: 0.711 [0.646, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.561, 10.387], loss: 0.001680, mae: 0.043389, mean_q: 1.278749
 86667/100000: episode: 1572, duration: 0.106s, episode steps: 22, steps per second: 207, episode reward: 17.297, mean reward: 0.786 [0.742, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.275, 10.582], loss: 0.001717, mae: 0.044693, mean_q: 1.281728
 86696/100000: episode: 1573, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 21.198, mean reward: 0.731 [0.608, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.101, 10.385], loss: 0.001722, mae: 0.044663, mean_q: 1.287412
 86728/100000: episode: 1574, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 22.760, mean reward: 0.711 [0.582, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.015, 10.288], loss: 0.001542, mae: 0.041805, mean_q: 1.280841
 86757/100000: episode: 1575, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 18.225, mean reward: 0.628 [0.553, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.473, 10.230], loss: 0.001891, mae: 0.047807, mean_q: 1.287269
 86782/100000: episode: 1576, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 21.416, mean reward: 0.857 [0.773, 0.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.484, 10.540], loss: 0.001547, mae: 0.043120, mean_q: 1.277494
 86804/100000: episode: 1577, duration: 0.110s, episode steps: 22, steps per second: 199, episode reward: 17.231, mean reward: 0.783 [0.736, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.073, 10.442], loss: 0.001578, mae: 0.043345, mean_q: 1.266363
 86816/100000: episode: 1578, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 10.220, mean reward: 0.852 [0.779, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.471, 10.100], loss: 0.001389, mae: 0.040943, mean_q: 1.278812
 86848/100000: episode: 1579, duration: 0.186s, episode steps: 32, steps per second: 172, episode reward: 23.966, mean reward: 0.749 [0.698, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.167, 10.417], loss: 0.001365, mae: 0.040429, mean_q: 1.300038
 86866/100000: episode: 1580, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 15.679, mean reward: 0.871 [0.822, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-1.238, 10.628], loss: 0.001536, mae: 0.043615, mean_q: 1.266755
 86898/100000: episode: 1581, duration: 0.165s, episode steps: 32, steps per second: 193, episode reward: 24.471, mean reward: 0.765 [0.658, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.613, 10.563], loss: 0.001595, mae: 0.043999, mean_q: 1.308758
 86910/100000: episode: 1582, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 9.317, mean reward: 0.776 [0.737, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.536], loss: 0.001517, mae: 0.042472, mean_q: 1.290319
 86941/100000: episode: 1583, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 23.154, mean reward: 0.747 [0.683, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.114, 10.550], loss: 0.001747, mae: 0.046304, mean_q: 1.306305
 86955/100000: episode: 1584, duration: 0.090s, episode steps: 14, steps per second: 156, episode reward: 10.044, mean reward: 0.717 [0.656, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.423], loss: 0.002098, mae: 0.050216, mean_q: 1.297994
 86969/100000: episode: 1585, duration: 0.090s, episode steps: 14, steps per second: 156, episode reward: 10.940, mean reward: 0.781 [0.730, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.487], loss: 0.001473, mae: 0.042110, mean_q: 1.311988
 86998/100000: episode: 1586, duration: 0.183s, episode steps: 29, steps per second: 159, episode reward: 19.516, mean reward: 0.673 [0.573, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.043, 10.285], loss: 0.001459, mae: 0.041921, mean_q: 1.299345
 87020/100000: episode: 1587, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 15.718, mean reward: 0.714 [0.671, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.538], loss: 0.001485, mae: 0.041219, mean_q: 1.301693
 87049/100000: episode: 1588, duration: 0.171s, episode steps: 29, steps per second: 169, episode reward: 22.150, mean reward: 0.764 [0.670, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.180, 10.528], loss: 0.001733, mae: 0.046025, mean_q: 1.306391
 87061/100000: episode: 1589, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 10.057, mean reward: 0.838 [0.790, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.295, 10.100], loss: 0.001482, mae: 0.043072, mean_q: 1.312629
 87090/100000: episode: 1590, duration: 0.199s, episode steps: 29, steps per second: 146, episode reward: 22.962, mean reward: 0.792 [0.713, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.676, 10.481], loss: 0.001554, mae: 0.043192, mean_q: 1.308471
 87108/100000: episode: 1591, duration: 0.124s, episode steps: 18, steps per second: 145, episode reward: 12.681, mean reward: 0.704 [0.593, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.318], loss: 0.001745, mae: 0.044599, mean_q: 1.308944
 87126/100000: episode: 1592, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 12.998, mean reward: 0.722 [0.629, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.317], loss: 0.001583, mae: 0.042650, mean_q: 1.299819
[Info] FALSIFICATION!
 87128/100000: episode: 1593, duration: 0.288s, episode steps: 2, steps per second: 7, episode reward: 1.991, mean reward: 0.996 [0.972, 1.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.015, 10.573], loss: 0.001225, mae: 0.040912, mean_q: 1.330405
 87159/100000: episode: 1594, duration: 0.195s, episode steps: 31, steps per second: 159, episode reward: 22.226, mean reward: 0.717 [0.662, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.464, 10.421], loss: 0.001809, mae: 0.044390, mean_q: 1.312111
 87188/100000: episode: 1595, duration: 0.190s, episode steps: 29, steps per second: 152, episode reward: 21.638, mean reward: 0.746 [0.656, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.319, 10.344], loss: 0.001636, mae: 0.044096, mean_q: 1.306086
 87210/100000: episode: 1596, duration: 0.157s, episode steps: 22, steps per second: 140, episode reward: 17.339, mean reward: 0.788 [0.736, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.518], loss: 0.001821, mae: 0.044215, mean_q: 1.312952
 87235/100000: episode: 1597, duration: 0.237s, episode steps: 25, steps per second: 105, episode reward: 20.331, mean reward: 0.813 [0.736, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.603], loss: 0.001612, mae: 0.043396, mean_q: 1.302295
 87264/100000: episode: 1598, duration: 0.428s, episode steps: 29, steps per second: 68, episode reward: 21.997, mean reward: 0.759 [0.638, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.046, 10.370], loss: 0.001603, mae: 0.043641, mean_q: 1.310605
 87286/100000: episode: 1599, duration: 0.280s, episode steps: 22, steps per second: 79, episode reward: 15.516, mean reward: 0.705 [0.613, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.552, 10.366], loss: 0.002188, mae: 0.048749, mean_q: 1.315621
 87304/100000: episode: 1600, duration: 0.197s, episode steps: 18, steps per second: 91, episode reward: 14.630, mean reward: 0.813 [0.737, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.441, 10.486], loss: 0.001539, mae: 0.042207, mean_q: 1.313973
 87329/100000: episode: 1601, duration: 0.289s, episode steps: 25, steps per second: 86, episode reward: 16.754, mean reward: 0.670 [0.515, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.812, 10.123], loss: 0.001674, mae: 0.042207, mean_q: 1.309763
 87351/100000: episode: 1602, duration: 0.199s, episode steps: 22, steps per second: 111, episode reward: 16.929, mean reward: 0.769 [0.727, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.481, 10.490], loss: 0.001670, mae: 0.044561, mean_q: 1.307045
 87363/100000: episode: 1603, duration: 0.110s, episode steps: 12, steps per second: 109, episode reward: 9.654, mean reward: 0.805 [0.734, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.594], loss: 0.001482, mae: 0.041196, mean_q: 1.310085
 87375/100000: episode: 1604, duration: 0.106s, episode steps: 12, steps per second: 113, episode reward: 9.806, mean reward: 0.817 [0.723, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.706], loss: 0.001557, mae: 0.045044, mean_q: 1.301247
 87404/100000: episode: 1605, duration: 0.254s, episode steps: 29, steps per second: 114, episode reward: 20.402, mean reward: 0.704 [0.589, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.142, 10.304], loss: 0.001583, mae: 0.043567, mean_q: 1.321972
 87416/100000: episode: 1606, duration: 0.102s, episode steps: 12, steps per second: 117, episode reward: 8.580, mean reward: 0.715 [0.678, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-2.139, 10.408], loss: 0.001575, mae: 0.043182, mean_q: 1.306909
 87430/100000: episode: 1607, duration: 0.134s, episode steps: 14, steps per second: 105, episode reward: 10.696, mean reward: 0.764 [0.725, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.091, 10.559], loss: 0.001567, mae: 0.041972, mean_q: 1.327217
 87459/100000: episode: 1608, duration: 0.190s, episode steps: 29, steps per second: 152, episode reward: 20.911, mean reward: 0.721 [0.641, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.431], loss: 0.001680, mae: 0.045078, mean_q: 1.324688
 87484/100000: episode: 1609, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 17.710, mean reward: 0.708 [0.649, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.442], loss: 0.001504, mae: 0.042316, mean_q: 1.316001
 87509/100000: episode: 1610, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 19.333, mean reward: 0.773 [0.675, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.806, 10.412], loss: 0.001686, mae: 0.043353, mean_q: 1.310719
 87521/100000: episode: 1611, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 9.062, mean reward: 0.755 [0.719, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.811, 10.435], loss: 0.001528, mae: 0.042984, mean_q: 1.320511
 87543/100000: episode: 1612, duration: 0.177s, episode steps: 22, steps per second: 125, episode reward: 18.224, mean reward: 0.828 [0.735, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.294, 10.520], loss: 0.001542, mae: 0.043334, mean_q: 1.316851
 87555/100000: episode: 1613, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 10.347, mean reward: 0.862 [0.765, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.517, 10.100], loss: 0.001498, mae: 0.042295, mean_q: 1.311891
 87567/100000: episode: 1614, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 9.659, mean reward: 0.805 [0.718, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.741, 10.100], loss: 0.001462, mae: 0.042637, mean_q: 1.332463
 87592/100000: episode: 1615, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 18.445, mean reward: 0.738 [0.615, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.749, 10.367], loss: 0.001634, mae: 0.044307, mean_q: 1.318746
 87614/100000: episode: 1616, duration: 0.141s, episode steps: 22, steps per second: 156, episode reward: 15.072, mean reward: 0.685 [0.550, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.669, 10.248], loss: 0.001797, mae: 0.043416, mean_q: 1.317692
 87646/100000: episode: 1617, duration: 0.208s, episode steps: 32, steps per second: 154, episode reward: 23.095, mean reward: 0.722 [0.662, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.629, 10.392], loss: 0.001785, mae: 0.044846, mean_q: 1.312511
 87675/100000: episode: 1618, duration: 0.172s, episode steps: 29, steps per second: 169, episode reward: 23.116, mean reward: 0.797 [0.704, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.313, 10.557], loss: 0.001714, mae: 0.045673, mean_q: 1.316911
 87693/100000: episode: 1619, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 13.000, mean reward: 0.722 [0.645, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.372], loss: 0.001753, mae: 0.045827, mean_q: 1.327313
 87722/100000: episode: 1620, duration: 0.185s, episode steps: 29, steps per second: 157, episode reward: 20.617, mean reward: 0.711 [0.622, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.303, 10.409], loss: 0.001310, mae: 0.040410, mean_q: 1.319336
 87751/100000: episode: 1621, duration: 0.181s, episode steps: 29, steps per second: 160, episode reward: 20.987, mean reward: 0.724 [0.637, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.581, 10.282], loss: 0.001796, mae: 0.046699, mean_q: 1.320392
 87765/100000: episode: 1622, duration: 0.100s, episode steps: 14, steps per second: 140, episode reward: 11.241, mean reward: 0.803 [0.742, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-1.018, 10.511], loss: 0.001504, mae: 0.042176, mean_q: 1.335659
 87783/100000: episode: 1623, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 13.411, mean reward: 0.745 [0.604, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.713, 10.414], loss: 0.001665, mae: 0.044548, mean_q: 1.329544
 87795/100000: episode: 1624, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 9.304, mean reward: 0.775 [0.722, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.530, 10.100], loss: 0.001631, mae: 0.044772, mean_q: 1.326290
 87827/100000: episode: 1625, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 24.987, mean reward: 0.781 [0.696, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.152, 10.546], loss: 0.002049, mae: 0.049140, mean_q: 1.321625
 87856/100000: episode: 1626, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 20.870, mean reward: 0.720 [0.638, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.379, 10.377], loss: 0.001978, mae: 0.048774, mean_q: 1.332389
 87878/100000: episode: 1627, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 17.208, mean reward: 0.782 [0.701, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.395, 10.568], loss: 0.001733, mae: 0.045305, mean_q: 1.329045
 87910/100000: episode: 1628, duration: 0.196s, episode steps: 32, steps per second: 163, episode reward: 25.223, mean reward: 0.788 [0.712, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.353, 10.460], loss: 0.001612, mae: 0.042931, mean_q: 1.336535
 87939/100000: episode: 1629, duration: 0.198s, episode steps: 29, steps per second: 146, episode reward: 20.462, mean reward: 0.706 [0.519, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.191, 10.248], loss: 0.001467, mae: 0.042736, mean_q: 1.345132
[Info] Complete ISplit Iteration
[Info] Levels: [1.3410857, 1.5467544, 1.6034713]
[Info] Cond. Prob: [0.1, 0.1, 0.4]
[Info] Error Prob: 0.004000000000000001

 87951/100000: episode: 1630, duration: 5.057s, episode steps: 12, steps per second: 2, episode reward: 8.455, mean reward: 0.705 [0.585, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.443, 10.389], loss: 0.001602, mae: 0.044806, mean_q: 1.330977
 88051/100000: episode: 1631, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 63.576, mean reward: 0.636 [0.509, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.289, 10.098], loss: 0.001552, mae: 0.043098, mean_q: 1.339547
 88151/100000: episode: 1632, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 60.921, mean reward: 0.609 [0.511, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.382, 10.098], loss: 0.001616, mae: 0.044302, mean_q: 1.331740
 88251/100000: episode: 1633, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.603, mean reward: 0.576 [0.513, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.113, 10.098], loss: 0.001695, mae: 0.044908, mean_q: 1.330022
 88351/100000: episode: 1634, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.784, mean reward: 0.588 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.014, 10.138], loss: 0.001466, mae: 0.041759, mean_q: 1.328896
 88451/100000: episode: 1635, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.641, mean reward: 0.586 [0.507, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.298, 10.201], loss: 0.001901, mae: 0.046705, mean_q: 1.321627
 88551/100000: episode: 1636, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 62.287, mean reward: 0.623 [0.501, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.240, 10.098], loss: 0.001794, mae: 0.045926, mean_q: 1.324754
 88651/100000: episode: 1637, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.070, mean reward: 0.601 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.034, 10.214], loss: 0.001762, mae: 0.045005, mean_q: 1.323365
 88751/100000: episode: 1638, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.546, mean reward: 0.605 [0.499, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.158, 10.098], loss: 0.001651, mae: 0.043765, mean_q: 1.322598
 88851/100000: episode: 1639, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 59.824, mean reward: 0.598 [0.509, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.474, 10.193], loss: 0.001569, mae: 0.043510, mean_q: 1.320020
 88951/100000: episode: 1640, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.423, mean reward: 0.604 [0.500, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.677, 10.098], loss: 0.001678, mae: 0.043851, mean_q: 1.321704
 89051/100000: episode: 1641, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.960, mean reward: 0.590 [0.504, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.736, 10.186], loss: 0.001617, mae: 0.044175, mean_q: 1.320327
 89151/100000: episode: 1642, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.935, mean reward: 0.589 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.787, 10.433], loss: 0.001648, mae: 0.043342, mean_q: 1.313517
 89251/100000: episode: 1643, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.894, mean reward: 0.589 [0.504, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.400, 10.193], loss: 0.001873, mae: 0.046483, mean_q: 1.314228
 89351/100000: episode: 1644, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.963, mean reward: 0.610 [0.508, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.199, 10.239], loss: 0.001659, mae: 0.044151, mean_q: 1.314698
 89451/100000: episode: 1645, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.915, mean reward: 0.599 [0.503, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.998, 10.145], loss: 0.001783, mae: 0.044747, mean_q: 1.316385
 89551/100000: episode: 1646, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 57.378, mean reward: 0.574 [0.505, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.800, 10.122], loss: 0.001716, mae: 0.045081, mean_q: 1.305326
 89651/100000: episode: 1647, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.652, mean reward: 0.597 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.722, 10.398], loss: 0.001791, mae: 0.044756, mean_q: 1.305316
 89751/100000: episode: 1648, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.108, mean reward: 0.591 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.246, 10.102], loss: 0.001680, mae: 0.044117, mean_q: 1.308487
 89851/100000: episode: 1649, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 61.101, mean reward: 0.611 [0.512, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.668, 10.098], loss: 0.001812, mae: 0.046088, mean_q: 1.304592
 89951/100000: episode: 1650, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.906, mean reward: 0.589 [0.512, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.641, 10.510], loss: 0.001838, mae: 0.045738, mean_q: 1.299925
 90051/100000: episode: 1651, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.773, mean reward: 0.588 [0.509, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.568, 10.227], loss: 0.001731, mae: 0.044726, mean_q: 1.298095
 90151/100000: episode: 1652, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 64.661, mean reward: 0.647 [0.502, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.168, 10.300], loss: 0.001696, mae: 0.044422, mean_q: 1.289361
 90251/100000: episode: 1653, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.251, mean reward: 0.603 [0.501, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.139, 10.098], loss: 0.001844, mae: 0.045661, mean_q: 1.293208
 90351/100000: episode: 1654, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.636, mean reward: 0.586 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.003, 10.098], loss: 0.001849, mae: 0.046137, mean_q: 1.293881
 90451/100000: episode: 1655, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 57.673, mean reward: 0.577 [0.507, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.901, 10.169], loss: 0.001726, mae: 0.044453, mean_q: 1.291175
 90551/100000: episode: 1656, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.344, mean reward: 0.583 [0.511, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.254, 10.267], loss: 0.001586, mae: 0.042595, mean_q: 1.285280
 90651/100000: episode: 1657, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.326, mean reward: 0.593 [0.498, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.371, 10.113], loss: 0.001670, mae: 0.043909, mean_q: 1.289911
 90751/100000: episode: 1658, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 63.895, mean reward: 0.639 [0.507, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.128], loss: 0.001638, mae: 0.043621, mean_q: 1.288463
 90851/100000: episode: 1659, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.078, mean reward: 0.591 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.615, 10.352], loss: 0.001901, mae: 0.047471, mean_q: 1.290241
 90951/100000: episode: 1660, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.235, mean reward: 0.602 [0.512, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.670, 10.224], loss: 0.001871, mae: 0.045638, mean_q: 1.285139
 91051/100000: episode: 1661, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 61.429, mean reward: 0.614 [0.509, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.792, 10.098], loss: 0.001620, mae: 0.043073, mean_q: 1.284977
 91151/100000: episode: 1662, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.921, mean reward: 0.599 [0.515, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.961, 10.270], loss: 0.001879, mae: 0.045797, mean_q: 1.272377
 91251/100000: episode: 1663, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 64.309, mean reward: 0.643 [0.516, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.790, 10.098], loss: 0.001850, mae: 0.045523, mean_q: 1.269603
 91351/100000: episode: 1664, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.040, mean reward: 0.590 [0.507, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.110, 10.323], loss: 0.001708, mae: 0.044206, mean_q: 1.262579
 91451/100000: episode: 1665, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.177, mean reward: 0.592 [0.498, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.547, 10.098], loss: 0.001851, mae: 0.045772, mean_q: 1.259079
 91551/100000: episode: 1666, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 59.877, mean reward: 0.599 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.258, 10.306], loss: 0.001930, mae: 0.046507, mean_q: 1.252755
 91651/100000: episode: 1667, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.098, mean reward: 0.581 [0.501, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.267, 10.098], loss: 0.001806, mae: 0.045113, mean_q: 1.240817
 91751/100000: episode: 1668, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 59.408, mean reward: 0.594 [0.510, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.608, 10.098], loss: 0.001823, mae: 0.045509, mean_q: 1.246607
 91851/100000: episode: 1669, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 57.774, mean reward: 0.578 [0.502, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.273, 10.126], loss: 0.001745, mae: 0.044697, mean_q: 1.237373
 91951/100000: episode: 1670, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 61.321, mean reward: 0.613 [0.506, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.428, 10.534], loss: 0.001608, mae: 0.043037, mean_q: 1.228835
 92051/100000: episode: 1671, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.256, mean reward: 0.583 [0.508, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.266, 10.175], loss: 0.001724, mae: 0.044229, mean_q: 1.228595
 92151/100000: episode: 1672, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.069, mean reward: 0.601 [0.506, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.281, 10.361], loss: 0.001681, mae: 0.044024, mean_q: 1.221351
 92251/100000: episode: 1673, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 57.533, mean reward: 0.575 [0.507, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.040, 10.098], loss: 0.001652, mae: 0.043479, mean_q: 1.217429
 92351/100000: episode: 1674, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.159, mean reward: 0.582 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.730, 10.098], loss: 0.001625, mae: 0.043283, mean_q: 1.212878
 92451/100000: episode: 1675, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 61.408, mean reward: 0.614 [0.504, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.369, 10.098], loss: 0.001628, mae: 0.043621, mean_q: 1.207385
 92551/100000: episode: 1676, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 60.305, mean reward: 0.603 [0.503, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.539, 10.658], loss: 0.001676, mae: 0.044177, mean_q: 1.204111
 92651/100000: episode: 1677, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.083, mean reward: 0.571 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.244, 10.237], loss: 0.001652, mae: 0.043745, mean_q: 1.195253
 92751/100000: episode: 1678, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.422, mean reward: 0.574 [0.502, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.969, 10.098], loss: 0.001560, mae: 0.042976, mean_q: 1.191789
 92851/100000: episode: 1679, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.418, mean reward: 0.584 [0.502, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.475, 10.379], loss: 0.001640, mae: 0.044481, mean_q: 1.185776
 92951/100000: episode: 1680, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 59.027, mean reward: 0.590 [0.502, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.573, 10.098], loss: 0.001593, mae: 0.043310, mean_q: 1.178240
 93051/100000: episode: 1681, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.138, mean reward: 0.581 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.863, 10.098], loss: 0.001571, mae: 0.043238, mean_q: 1.179957
 93151/100000: episode: 1682, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.531, mean reward: 0.585 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.133, 10.098], loss: 0.001499, mae: 0.042058, mean_q: 1.180705
 93251/100000: episode: 1683, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.531, mean reward: 0.595 [0.504, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.828, 10.098], loss: 0.001534, mae: 0.042934, mean_q: 1.179332
 93351/100000: episode: 1684, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.405, mean reward: 0.594 [0.514, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.937, 10.143], loss: 0.001632, mae: 0.044247, mean_q: 1.178985
 93451/100000: episode: 1685, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.643, mean reward: 0.596 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.135, 10.233], loss: 0.001524, mae: 0.042657, mean_q: 1.181140
 93551/100000: episode: 1686, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 59.226, mean reward: 0.592 [0.500, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.769, 10.122], loss: 0.001514, mae: 0.042607, mean_q: 1.177891
 93651/100000: episode: 1687, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.543, mean reward: 0.585 [0.500, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.613, 10.098], loss: 0.001538, mae: 0.042679, mean_q: 1.176174
 93751/100000: episode: 1688, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 62.597, mean reward: 0.626 [0.501, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.968, 10.652], loss: 0.001565, mae: 0.043453, mean_q: 1.178277
 93851/100000: episode: 1689, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.362, mean reward: 0.584 [0.503, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.356, 10.098], loss: 0.001513, mae: 0.042753, mean_q: 1.175060
 93951/100000: episode: 1690, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.146, mean reward: 0.601 [0.507, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.611, 10.437], loss: 0.001574, mae: 0.042905, mean_q: 1.174865
 94051/100000: episode: 1691, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.123, mean reward: 0.571 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.313, 10.216], loss: 0.001503, mae: 0.042104, mean_q: 1.178837
 94151/100000: episode: 1692, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.456, mean reward: 0.605 [0.501, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.882, 10.205], loss: 0.001574, mae: 0.043610, mean_q: 1.176709
 94251/100000: episode: 1693, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.366, mean reward: 0.574 [0.498, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.023, 10.118], loss: 0.001673, mae: 0.044249, mean_q: 1.177184
 94351/100000: episode: 1694, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.464, mean reward: 0.585 [0.512, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.848, 10.259], loss: 0.001585, mae: 0.043380, mean_q: 1.176368
 94451/100000: episode: 1695, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 65.177, mean reward: 0.652 [0.506, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.049, 10.346], loss: 0.001556, mae: 0.043430, mean_q: 1.173203
 94551/100000: episode: 1696, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 61.819, mean reward: 0.618 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.942, 10.346], loss: 0.001586, mae: 0.042967, mean_q: 1.174718
 94651/100000: episode: 1697, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.189, mean reward: 0.592 [0.506, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.456, 10.098], loss: 0.001612, mae: 0.043835, mean_q: 1.180846
 94751/100000: episode: 1698, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.647, mean reward: 0.596 [0.498, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.191, 10.155], loss: 0.001559, mae: 0.043709, mean_q: 1.175797
 94851/100000: episode: 1699, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.493, mean reward: 0.575 [0.503, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.113, 10.270], loss: 0.001552, mae: 0.043032, mean_q: 1.177991
 94951/100000: episode: 1700, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.826, mean reward: 0.588 [0.507, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.158, 10.209], loss: 0.001583, mae: 0.043575, mean_q: 1.177731
 95051/100000: episode: 1701, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 58.653, mean reward: 0.587 [0.500, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.455, 10.098], loss: 0.001498, mae: 0.042268, mean_q: 1.178223
 95151/100000: episode: 1702, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 58.914, mean reward: 0.589 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.953, 10.309], loss: 0.001495, mae: 0.042741, mean_q: 1.176379
 95251/100000: episode: 1703, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.358, mean reward: 0.574 [0.499, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.180, 10.098], loss: 0.001645, mae: 0.044943, mean_q: 1.175401
 95351/100000: episode: 1704, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.748, mean reward: 0.597 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.053, 10.098], loss: 0.001594, mae: 0.043806, mean_q: 1.177714
 95451/100000: episode: 1705, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 62.304, mean reward: 0.623 [0.527, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.460, 10.281], loss: 0.001574, mae: 0.043633, mean_q: 1.177091
 95551/100000: episode: 1706, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.363, mean reward: 0.594 [0.511, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.623, 10.319], loss: 0.001591, mae: 0.043983, mean_q: 1.175725
 95651/100000: episode: 1707, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.380, mean reward: 0.584 [0.501, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.585, 10.098], loss: 0.001541, mae: 0.043478, mean_q: 1.177171
 95751/100000: episode: 1708, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 57.445, mean reward: 0.574 [0.505, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.263, 10.194], loss: 0.001481, mae: 0.042150, mean_q: 1.174610
 95851/100000: episode: 1709, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.165, mean reward: 0.602 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.097, 10.111], loss: 0.001545, mae: 0.043431, mean_q: 1.174931
 95951/100000: episode: 1710, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.823, mean reward: 0.598 [0.512, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.212, 10.098], loss: 0.001499, mae: 0.043153, mean_q: 1.177136
 96051/100000: episode: 1711, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.425, mean reward: 0.584 [0.510, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.670, 10.179], loss: 0.001601, mae: 0.043764, mean_q: 1.172071
 96151/100000: episode: 1712, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 60.333, mean reward: 0.603 [0.513, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.400, 10.098], loss: 0.001486, mae: 0.042701, mean_q: 1.173746
 96251/100000: episode: 1713, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 61.489, mean reward: 0.615 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.011, 10.098], loss: 0.001503, mae: 0.042300, mean_q: 1.171706
 96351/100000: episode: 1714, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.523, mean reward: 0.585 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.268, 10.098], loss: 0.001699, mae: 0.044790, mean_q: 1.172443
 96451/100000: episode: 1715, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.678, mean reward: 0.577 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.087, 10.098], loss: 0.001529, mae: 0.042429, mean_q: 1.169507
 96551/100000: episode: 1716, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.874, mean reward: 0.589 [0.512, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.353, 10.098], loss: 0.001507, mae: 0.042840, mean_q: 1.170514
 96651/100000: episode: 1717, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.182, mean reward: 0.592 [0.508, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.635, 10.098], loss: 0.001595, mae: 0.043557, mean_q: 1.168546
 96751/100000: episode: 1718, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.355, mean reward: 0.584 [0.502, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.791, 10.117], loss: 0.001423, mae: 0.041429, mean_q: 1.169481
 96851/100000: episode: 1719, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 60.361, mean reward: 0.604 [0.513, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.791, 10.287], loss: 0.001512, mae: 0.042652, mean_q: 1.172319
 96951/100000: episode: 1720, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.110, mean reward: 0.581 [0.500, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.548, 10.098], loss: 0.001585, mae: 0.043636, mean_q: 1.171046
 97051/100000: episode: 1721, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.043, mean reward: 0.590 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.037, 10.099], loss: 0.001519, mae: 0.042369, mean_q: 1.173839
 97151/100000: episode: 1722, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 61.748, mean reward: 0.617 [0.508, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.573, 10.098], loss: 0.001503, mae: 0.042016, mean_q: 1.168803
 97251/100000: episode: 1723, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.830, mean reward: 0.578 [0.498, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.053, 10.098], loss: 0.001416, mae: 0.041356, mean_q: 1.170532
 97351/100000: episode: 1724, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.692, mean reward: 0.577 [0.507, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.097, 10.098], loss: 0.001439, mae: 0.041741, mean_q: 1.176428
 97451/100000: episode: 1725, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.162, mean reward: 0.582 [0.511, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.340, 10.098], loss: 0.001300, mae: 0.039848, mean_q: 1.169074
 97551/100000: episode: 1726, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 59.737, mean reward: 0.597 [0.504, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.389, 10.129], loss: 0.001446, mae: 0.041642, mean_q: 1.172083
 97651/100000: episode: 1727, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 59.368, mean reward: 0.594 [0.498, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.273, 10.098], loss: 0.001486, mae: 0.041884, mean_q: 1.169779
 97751/100000: episode: 1728, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 60.405, mean reward: 0.604 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.607, 10.098], loss: 0.001468, mae: 0.041845, mean_q: 1.174405
 97851/100000: episode: 1729, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.250, mean reward: 0.572 [0.504, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.614, 10.277], loss: 0.001522, mae: 0.042958, mean_q: 1.171266
[Info] 1-TH LEVEL FOUND: 1.4210573434829712, Considering 10/90 traces
 97951/100000: episode: 1730, duration: 4.696s, episode steps: 100, steps per second: 21, episode reward: 56.889, mean reward: 0.569 [0.506, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.068, 10.098], loss: 0.001593, mae: 0.043968, mean_q: 1.169613
 97969/100000: episode: 1731, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 13.306, mean reward: 0.739 [0.702, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.339, 10.505], loss: 0.001684, mae: 0.045434, mean_q: 1.175509
 97984/100000: episode: 1732, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 10.913, mean reward: 0.728 [0.702, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.870, 10.100], loss: 0.001633, mae: 0.043690, mean_q: 1.162633
 97999/100000: episode: 1733, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 11.333, mean reward: 0.756 [0.670, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.415, 10.100], loss: 0.001750, mae: 0.047109, mean_q: 1.178428
 98038/100000: episode: 1734, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 28.325, mean reward: 0.726 [0.627, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.861, 10.437], loss: 0.001526, mae: 0.043155, mean_q: 1.169520
 98077/100000: episode: 1735, duration: 0.191s, episode steps: 39, steps per second: 205, episode reward: 27.724, mean reward: 0.711 [0.614, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.669, 10.423], loss: 0.001582, mae: 0.044088, mean_q: 1.181955
 98091/100000: episode: 1736, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 10.172, mean reward: 0.727 [0.686, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.233, 10.100], loss: 0.001419, mae: 0.042394, mean_q: 1.177617
 98130/100000: episode: 1737, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 26.726, mean reward: 0.685 [0.555, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.594, 10.240], loss: 0.001456, mae: 0.042200, mean_q: 1.179923
 98148/100000: episode: 1738, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 12.445, mean reward: 0.691 [0.623, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.525, 10.406], loss: 0.001507, mae: 0.042616, mean_q: 1.181767
 98163/100000: episode: 1739, duration: 0.093s, episode steps: 15, steps per second: 162, episode reward: 10.332, mean reward: 0.689 [0.648, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.133, 10.100], loss: 0.001493, mae: 0.042031, mean_q: 1.173495
 98258/100000: episode: 1740, duration: 0.521s, episode steps: 95, steps per second: 182, episode reward: 55.556, mean reward: 0.585 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.848, 10.137], loss: 0.001617, mae: 0.043843, mean_q: 1.180189
 98272/100000: episode: 1741, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 9.617, mean reward: 0.687 [0.638, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.658, 10.100], loss: 0.001630, mae: 0.043147, mean_q: 1.178136
 98286/100000: episode: 1742, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 9.598, mean reward: 0.686 [0.611, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.934, 10.100], loss: 0.001767, mae: 0.045461, mean_q: 1.173744
 98381/100000: episode: 1743, duration: 0.515s, episode steps: 95, steps per second: 184, episode reward: 54.091, mean reward: 0.569 [0.500, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.511 [-0.869, 10.296], loss: 0.001658, mae: 0.043444, mean_q: 1.182631
 98395/100000: episode: 1744, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 10.341, mean reward: 0.739 [0.682, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.352, 10.100], loss: 0.001664, mae: 0.044153, mean_q: 1.177455
 98413/100000: episode: 1745, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 13.815, mean reward: 0.767 [0.690, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.036, 10.638], loss: 0.001685, mae: 0.045523, mean_q: 1.178150
 98432/100000: episode: 1746, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 12.684, mean reward: 0.668 [0.598, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.220, 10.100], loss: 0.001551, mae: 0.042958, mean_q: 1.175864
 98446/100000: episode: 1747, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 9.639, mean reward: 0.689 [0.630, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.410, 10.100], loss: 0.001825, mae: 0.045317, mean_q: 1.179751
 98541/100000: episode: 1748, duration: 0.489s, episode steps: 95, steps per second: 194, episode reward: 56.499, mean reward: 0.595 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.868, 10.100], loss: 0.001779, mae: 0.045918, mean_q: 1.183394
 98550/100000: episode: 1749, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 6.547, mean reward: 0.727 [0.674, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.545], loss: 0.001349, mae: 0.039231, mean_q: 1.187924
 98645/100000: episode: 1750, duration: 0.489s, episode steps: 95, steps per second: 194, episode reward: 54.803, mean reward: 0.577 [0.505, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-1.094, 10.194], loss: 0.001721, mae: 0.044383, mean_q: 1.177396
 98660/100000: episode: 1751, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 11.503, mean reward: 0.767 [0.713, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.445, 10.100], loss: 0.001415, mae: 0.041461, mean_q: 1.175387
 98672/100000: episode: 1752, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 7.817, mean reward: 0.651 [0.565, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.434, 10.100], loss: 0.001323, mae: 0.040062, mean_q: 1.184768
 98686/100000: episode: 1753, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 9.815, mean reward: 0.701 [0.645, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.371, 10.100], loss: 0.001616, mae: 0.043340, mean_q: 1.182834
 98698/100000: episode: 1754, duration: 0.076s, episode steps: 12, steps per second: 159, episode reward: 8.250, mean reward: 0.687 [0.608, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.230, 10.100], loss: 0.001784, mae: 0.045797, mean_q: 1.182305
 98713/100000: episode: 1755, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 10.483, mean reward: 0.699 [0.615, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.480, 10.100], loss: 0.001673, mae: 0.046490, mean_q: 1.185230
 98727/100000: episode: 1756, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 9.814, mean reward: 0.701 [0.623, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.420, 10.100], loss: 0.001283, mae: 0.039890, mean_q: 1.183514
 98745/100000: episode: 1757, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 13.261, mean reward: 0.737 [0.673, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.420], loss: 0.001575, mae: 0.041679, mean_q: 1.185074
 98780/100000: episode: 1758, duration: 0.192s, episode steps: 35, steps per second: 183, episode reward: 23.073, mean reward: 0.659 [0.541, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.718, 10.239], loss: 0.001677, mae: 0.044880, mean_q: 1.179950
 98815/100000: episode: 1759, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 24.422, mean reward: 0.698 [0.627, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.446], loss: 0.001690, mae: 0.044093, mean_q: 1.178243
 98829/100000: episode: 1760, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 10.467, mean reward: 0.748 [0.677, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.527, 10.100], loss: 0.002006, mae: 0.048744, mean_q: 1.188898
 98838/100000: episode: 1761, duration: 0.050s, episode steps: 9, steps per second: 182, episode reward: 6.321, mean reward: 0.702 [0.678, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.387, 10.359], loss: 0.001704, mae: 0.046576, mean_q: 1.203883
 98852/100000: episode: 1762, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 9.631, mean reward: 0.688 [0.645, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.337, 10.100], loss: 0.001514, mae: 0.042122, mean_q: 1.192781
 98864/100000: episode: 1763, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 7.437, mean reward: 0.620 [0.567, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.210, 10.100], loss: 0.001517, mae: 0.041743, mean_q: 1.186546
 98876/100000: episode: 1764, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 7.927, mean reward: 0.661 [0.632, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.315, 10.100], loss: 0.001380, mae: 0.041168, mean_q: 1.186626
 98911/100000: episode: 1765, duration: 0.205s, episode steps: 35, steps per second: 171, episode reward: 24.968, mean reward: 0.713 [0.661, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.425, 10.509], loss: 0.001632, mae: 0.043870, mean_q: 1.182546
 98950/100000: episode: 1766, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 27.766, mean reward: 0.712 [0.571, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.692, 10.361], loss: 0.001658, mae: 0.043840, mean_q: 1.183157
 98968/100000: episode: 1767, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 12.843, mean reward: 0.713 [0.658, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.093, 10.525], loss: 0.001691, mae: 0.044603, mean_q: 1.196920
 99003/100000: episode: 1768, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 22.541, mean reward: 0.644 [0.582, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.416, 10.271], loss: 0.001595, mae: 0.042570, mean_q: 1.189303
 99038/100000: episode: 1769, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 24.949, mean reward: 0.713 [0.578, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.950, 10.267], loss: 0.001698, mae: 0.044334, mean_q: 1.188510
 99057/100000: episode: 1770, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 15.183, mean reward: 0.799 [0.701, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.574, 10.100], loss: 0.001401, mae: 0.040641, mean_q: 1.187741
 99069/100000: episode: 1771, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 8.663, mean reward: 0.722 [0.661, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.860, 10.100], loss: 0.001795, mae: 0.045591, mean_q: 1.178106
 99083/100000: episode: 1772, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 9.390, mean reward: 0.671 [0.618, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.300, 10.100], loss: 0.001959, mae: 0.047849, mean_q: 1.203269
 99097/100000: episode: 1773, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 9.353, mean reward: 0.668 [0.562, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.213, 10.100], loss: 0.001749, mae: 0.043696, mean_q: 1.201632
 99112/100000: episode: 1774, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 10.915, mean reward: 0.728 [0.609, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.213, 10.100], loss: 0.001660, mae: 0.044224, mean_q: 1.195415
 99207/100000: episode: 1775, duration: 0.492s, episode steps: 95, steps per second: 193, episode reward: 58.793, mean reward: 0.619 [0.502, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.627, 10.100], loss: 0.001755, mae: 0.044842, mean_q: 1.198129
 99219/100000: episode: 1776, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 8.229, mean reward: 0.686 [0.624, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.797, 10.100], loss: 0.001291, mae: 0.039918, mean_q: 1.193485
 99254/100000: episode: 1777, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 24.054, mean reward: 0.687 [0.519, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.646, 10.195], loss: 0.001666, mae: 0.043767, mean_q: 1.194430
 99289/100000: episode: 1778, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 24.847, mean reward: 0.710 [0.585, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.621, 10.521], loss: 0.001698, mae: 0.044482, mean_q: 1.200104
 99303/100000: episode: 1779, duration: 0.094s, episode steps: 14, steps per second: 149, episode reward: 9.396, mean reward: 0.671 [0.571, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.245, 10.100], loss: 0.001690, mae: 0.045317, mean_q: 1.199336
 99342/100000: episode: 1780, duration: 0.218s, episode steps: 39, steps per second: 179, episode reward: 24.874, mean reward: 0.638 [0.501, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.638, 10.136], loss: 0.001606, mae: 0.043411, mean_q: 1.198258
 99361/100000: episode: 1781, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 14.749, mean reward: 0.776 [0.670, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.262, 10.100], loss: 0.001696, mae: 0.042990, mean_q: 1.189618
 99400/100000: episode: 1782, duration: 0.216s, episode steps: 39, steps per second: 181, episode reward: 26.615, mean reward: 0.682 [0.597, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.375, 10.386], loss: 0.001754, mae: 0.045843, mean_q: 1.205819
 99412/100000: episode: 1783, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 8.024, mean reward: 0.669 [0.636, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.470, 10.100], loss: 0.001592, mae: 0.042913, mean_q: 1.183362
 99431/100000: episode: 1784, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 14.090, mean reward: 0.742 [0.642, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.497, 10.100], loss: 0.001642, mae: 0.043606, mean_q: 1.210081
 99445/100000: episode: 1785, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 9.804, mean reward: 0.700 [0.648, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.428, 10.100], loss: 0.001805, mae: 0.045495, mean_q: 1.210814
 99484/100000: episode: 1786, duration: 0.207s, episode steps: 39, steps per second: 189, episode reward: 26.578, mean reward: 0.681 [0.613, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.443, 10.279], loss: 0.001740, mae: 0.045069, mean_q: 1.201839
 99502/100000: episode: 1787, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 13.060, mean reward: 0.726 [0.682, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.625, 10.437], loss: 0.001621, mae: 0.043245, mean_q: 1.208020
 99520/100000: episode: 1788, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 13.401, mean reward: 0.745 [0.676, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.134, 10.446], loss: 0.001564, mae: 0.043141, mean_q: 1.210060
 99534/100000: episode: 1789, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 9.540, mean reward: 0.681 [0.632, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.316, 10.100], loss: 0.001628, mae: 0.044064, mean_q: 1.198966
 99573/100000: episode: 1790, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 25.844, mean reward: 0.663 [0.568, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.077, 10.399], loss: 0.001731, mae: 0.044668, mean_q: 1.206609
 99587/100000: episode: 1791, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 9.471, mean reward: 0.677 [0.639, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.114, 10.100], loss: 0.001684, mae: 0.044165, mean_q: 1.210152
 99596/100000: episode: 1792, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 6.266, mean reward: 0.696 [0.638, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.376], loss: 0.001702, mae: 0.045893, mean_q: 1.211802
 99691/100000: episode: 1793, duration: 0.499s, episode steps: 95, steps per second: 190, episode reward: 60.705, mean reward: 0.639 [0.517, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-1.652, 10.346], loss: 0.001658, mae: 0.044086, mean_q: 1.204514
 99786/100000: episode: 1794, duration: 0.530s, episode steps: 95, steps per second: 179, episode reward: 55.120, mean reward: 0.580 [0.501, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.966, 10.226], loss: 0.001697, mae: 0.044594, mean_q: 1.204724
 99800/100000: episode: 1795, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 9.630, mean reward: 0.688 [0.642, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.313, 10.100], loss: 0.001714, mae: 0.045045, mean_q: 1.212611
 99814/100000: episode: 1796, duration: 0.069s, episode steps: 14, steps per second: 201, episode reward: 10.321, mean reward: 0.737 [0.649, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.345, 10.100], loss: 0.001645, mae: 0.042991, mean_q: 1.216342
 99823/100000: episode: 1797, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 6.699, mean reward: 0.744 [0.709, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-1.006, 10.472], loss: 0.001558, mae: 0.045409, mean_q: 1.204217
 99835/100000: episode: 1798, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 8.387, mean reward: 0.699 [0.637, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.270, 10.100], loss: 0.001724, mae: 0.043641, mean_q: 1.202652
 99930/100000: episode: 1799, duration: 0.508s, episode steps: 95, steps per second: 187, episode reward: 57.438, mean reward: 0.605 [0.506, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.592, 10.100], loss: 0.001713, mae: 0.044475, mean_q: 1.212991
 99948/100000: episode: 1800, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 13.002, mean reward: 0.722 [0.668, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.441, 10.418], loss: 0.001582, mae: 0.043527, mean_q: 1.200357
 99962/100000: episode: 1801, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 10.172, mean reward: 0.727 [0.659, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.586, 10.100], loss: 0.001634, mae: 0.044954, mean_q: 1.213702
 99997/100000: episode: 1802, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 22.862, mean reward: 0.653 [0.573, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.095, 10.266], loss: 0.001710, mae: 0.044671, mean_q: 1.214268
done, took 612.689 seconds
[Info] End Importance Splitting. Falsification occurred 17 times.
