Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.180s, episode steps: 100, steps per second: 556, episode reward: 56.878, mean reward: 0.569 [0.499, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.792, 10.150], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1582, episode reward: 57.484, mean reward: 0.575 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.801, 10.162], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 62.112, mean reward: 0.621 [0.505, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.081, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 59.610, mean reward: 0.596 [0.523, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.135, 10.403], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.067s, episode steps: 100, steps per second: 1484, episode reward: 58.039, mean reward: 0.580 [0.505, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.572, 10.311], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.063s, episode steps: 100, steps per second: 1579, episode reward: 58.071, mean reward: 0.581 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.297, 10.201], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.070s, episode steps: 100, steps per second: 1420, episode reward: 58.600, mean reward: 0.586 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.100, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.066s, episode steps: 100, steps per second: 1509, episode reward: 58.001, mean reward: 0.580 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.835, 10.272], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.067s, episode steps: 100, steps per second: 1499, episode reward: 62.182, mean reward: 0.622 [0.515, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.635, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.071s, episode steps: 100, steps per second: 1413, episode reward: 59.459, mean reward: 0.595 [0.511, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.153, 10.142], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.068s, episode steps: 100, steps per second: 1478, episode reward: 59.483, mean reward: 0.595 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.988, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.072s, episode steps: 100, steps per second: 1390, episode reward: 58.025, mean reward: 0.580 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.426, 10.124], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.067s, episode steps: 100, steps per second: 1503, episode reward: 57.869, mean reward: 0.579 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.648, 10.106], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.066s, episode steps: 100, steps per second: 1514, episode reward: 61.044, mean reward: 0.610 [0.502, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.504, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.074s, episode steps: 100, steps per second: 1346, episode reward: 57.277, mean reward: 0.573 [0.500, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.945, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.067s, episode steps: 100, steps per second: 1484, episode reward: 59.678, mean reward: 0.597 [0.505, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.460, 10.234], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.067s, episode steps: 100, steps per second: 1488, episode reward: 58.923, mean reward: 0.589 [0.504, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.914, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.070s, episode steps: 100, steps per second: 1438, episode reward: 58.563, mean reward: 0.586 [0.507, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.862, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.067s, episode steps: 100, steps per second: 1501, episode reward: 58.387, mean reward: 0.584 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.652, 10.125], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.067s, episode steps: 100, steps per second: 1497, episode reward: 60.365, mean reward: 0.604 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.831, 10.234], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.067s, episode steps: 100, steps per second: 1488, episode reward: 57.701, mean reward: 0.577 [0.503, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.288, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.067s, episode steps: 100, steps per second: 1494, episode reward: 60.015, mean reward: 0.600 [0.498, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.533, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 60.018, mean reward: 0.600 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.668, 10.132], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.067s, episode steps: 100, steps per second: 1491, episode reward: 59.937, mean reward: 0.599 [0.502, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.120, 10.449], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.081s, episode steps: 100, steps per second: 1233, episode reward: 58.958, mean reward: 0.590 [0.503, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.477, 10.423], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.067s, episode steps: 100, steps per second: 1503, episode reward: 60.038, mean reward: 0.600 [0.509, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.816, 10.186], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.067s, episode steps: 100, steps per second: 1486, episode reward: 59.046, mean reward: 0.590 [0.506, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.861, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.066s, episode steps: 100, steps per second: 1505, episode reward: 60.004, mean reward: 0.600 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.370, 10.380], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.066s, episode steps: 100, steps per second: 1512, episode reward: 56.754, mean reward: 0.568 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.331, 10.161], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.066s, episode steps: 100, steps per second: 1507, episode reward: 58.213, mean reward: 0.582 [0.510, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.267, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.071s, episode steps: 100, steps per second: 1400, episode reward: 59.493, mean reward: 0.595 [0.506, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.069, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.066s, episode steps: 100, steps per second: 1518, episode reward: 60.164, mean reward: 0.602 [0.516, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.954, 10.154], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.074s, episode steps: 100, steps per second: 1349, episode reward: 56.698, mean reward: 0.567 [0.502, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.470, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.071s, episode steps: 100, steps per second: 1404, episode reward: 58.077, mean reward: 0.581 [0.512, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.547, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.067s, episode steps: 100, steps per second: 1504, episode reward: 58.227, mean reward: 0.582 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.895, 10.213], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.072s, episode steps: 100, steps per second: 1395, episode reward: 58.262, mean reward: 0.583 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.326, 10.140], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.067s, episode steps: 100, steps per second: 1487, episode reward: 58.120, mean reward: 0.581 [0.502, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.395, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.066s, episode steps: 100, steps per second: 1504, episode reward: 60.541, mean reward: 0.605 [0.524, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.000, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.067s, episode steps: 100, steps per second: 1503, episode reward: 57.989, mean reward: 0.580 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.158, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.066s, episode steps: 100, steps per second: 1506, episode reward: 58.332, mean reward: 0.583 [0.510, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.905, 10.213], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.066s, episode steps: 100, steps per second: 1516, episode reward: 58.111, mean reward: 0.581 [0.509, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.441, 10.270], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.066s, episode steps: 100, steps per second: 1505, episode reward: 60.880, mean reward: 0.609 [0.511, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.821, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.066s, episode steps: 100, steps per second: 1510, episode reward: 56.964, mean reward: 0.570 [0.511, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.211, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.081s, episode steps: 100, steps per second: 1228, episode reward: 57.528, mean reward: 0.575 [0.497, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.636, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.079s, episode steps: 100, steps per second: 1262, episode reward: 60.600, mean reward: 0.606 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.920, 10.148], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.067s, episode steps: 100, steps per second: 1501, episode reward: 58.185, mean reward: 0.582 [0.499, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.502, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.077s, episode steps: 100, steps per second: 1300, episode reward: 58.566, mean reward: 0.586 [0.513, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.176, 10.204], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.066s, episode steps: 100, steps per second: 1509, episode reward: 58.216, mean reward: 0.582 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.908, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.076s, episode steps: 100, steps per second: 1321, episode reward: 58.936, mean reward: 0.589 [0.500, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.791, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.066s, episode steps: 100, steps per second: 1512, episode reward: 62.322, mean reward: 0.623 [0.500, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.479, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.196s, episode steps: 100, steps per second: 84, episode reward: 57.857, mean reward: 0.579 [0.499, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.704, 10.098], loss: 0.013172, mae: 0.103559, mean_q: 0.776964
  5200/100000: episode: 52, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.386, mean reward: 0.594 [0.516, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.560, 10.098], loss: 0.003524, mae: 0.058031, mean_q: 0.943329
  5300/100000: episode: 53, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.360, mean reward: 0.574 [0.498, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.708, 10.098], loss: 0.003326, mae: 0.055398, mean_q: 1.026646
  5400/100000: episode: 54, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 64.528, mean reward: 0.645 [0.529, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.228, 10.098], loss: 0.002903, mae: 0.052578, mean_q: 1.080602
  5500/100000: episode: 55, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.804, mean reward: 0.608 [0.505, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.342, 10.265], loss: 0.002999, mae: 0.053562, mean_q: 1.113983
  5600/100000: episode: 56, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.932, mean reward: 0.579 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.761, 10.098], loss: 0.003288, mae: 0.055808, mean_q: 1.134298
  5700/100000: episode: 57, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.236, mean reward: 0.572 [0.504, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.613, 10.186], loss: 0.003076, mae: 0.054454, mean_q: 1.145153
  5800/100000: episode: 58, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 56.834, mean reward: 0.568 [0.499, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.904, 10.098], loss: 0.002532, mae: 0.050500, mean_q: 1.158439
  5900/100000: episode: 59, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 59.250, mean reward: 0.593 [0.502, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.537, 10.098], loss: 0.002869, mae: 0.052280, mean_q: 1.159632
  6000/100000: episode: 60, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 57.308, mean reward: 0.573 [0.504, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.107, 10.098], loss: 0.002609, mae: 0.050776, mean_q: 1.164241
  6100/100000: episode: 61, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 61.873, mean reward: 0.619 [0.511, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.714, 10.098], loss: 0.002755, mae: 0.052138, mean_q: 1.166518
  6200/100000: episode: 62, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.289, mean reward: 0.593 [0.499, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.737, 10.098], loss: 0.003163, mae: 0.055424, mean_q: 1.163622
  6300/100000: episode: 63, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 63.301, mean reward: 0.633 [0.512, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.656, 10.098], loss: 0.002679, mae: 0.051540, mean_q: 1.165909
  6400/100000: episode: 64, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.794, mean reward: 0.588 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.891, 10.249], loss: 0.003439, mae: 0.057819, mean_q: 1.169601
  6500/100000: episode: 65, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.859, mean reward: 0.579 [0.503, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.188, 10.105], loss: 0.002845, mae: 0.052555, mean_q: 1.170375
  6600/100000: episode: 66, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.940, mean reward: 0.589 [0.512, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.828, 10.189], loss: 0.003003, mae: 0.054315, mean_q: 1.172135
  6700/100000: episode: 67, duration: 0.639s, episode steps: 100, steps per second: 157, episode reward: 58.783, mean reward: 0.588 [0.509, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.424, 10.098], loss: 0.002917, mae: 0.053637, mean_q: 1.168787
  6800/100000: episode: 68, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 58.625, mean reward: 0.586 [0.516, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.635, 10.098], loss: 0.003071, mae: 0.054620, mean_q: 1.167907
  6900/100000: episode: 69, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.791, mean reward: 0.578 [0.510, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.102, 10.195], loss: 0.002741, mae: 0.051101, mean_q: 1.169584
  7000/100000: episode: 70, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.180, mean reward: 0.582 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.566, 10.098], loss: 0.002439, mae: 0.049046, mean_q: 1.167785
  7100/100000: episode: 71, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.132, mean reward: 0.591 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.060, 10.120], loss: 0.002580, mae: 0.049990, mean_q: 1.169865
  7200/100000: episode: 72, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.079, mean reward: 0.591 [0.511, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.728, 10.098], loss: 0.002555, mae: 0.051275, mean_q: 1.169877
  7300/100000: episode: 73, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.829, mean reward: 0.588 [0.505, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.867, 10.098], loss: 0.002409, mae: 0.048274, mean_q: 1.166157
  7400/100000: episode: 74, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.884, mean reward: 0.579 [0.502, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.197, 10.108], loss: 0.002560, mae: 0.050396, mean_q: 1.167689
  7500/100000: episode: 75, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.798, mean reward: 0.588 [0.504, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.685, 10.281], loss: 0.002339, mae: 0.049188, mean_q: 1.166130
  7600/100000: episode: 76, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 58.029, mean reward: 0.580 [0.500, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.092, 10.098], loss: 0.002360, mae: 0.050313, mean_q: 1.168243
  7700/100000: episode: 77, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.411, mean reward: 0.594 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.571, 10.279], loss: 0.002824, mae: 0.053520, mean_q: 1.165514
  7800/100000: episode: 78, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.177, mean reward: 0.582 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.055, 10.098], loss: 0.002306, mae: 0.049774, mean_q: 1.167189
  7900/100000: episode: 79, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.369, mean reward: 0.594 [0.497, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.237, 10.125], loss: 0.002704, mae: 0.052404, mean_q: 1.165941
  8000/100000: episode: 80, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 62.244, mean reward: 0.622 [0.519, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.160, 10.492], loss: 0.002108, mae: 0.048205, mean_q: 1.168192
  8100/100000: episode: 81, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.674, mean reward: 0.577 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.165], loss: 0.002450, mae: 0.050079, mean_q: 1.165079
  8200/100000: episode: 82, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.316, mean reward: 0.583 [0.497, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.247, 10.098], loss: 0.002268, mae: 0.049561, mean_q: 1.163500
  8300/100000: episode: 83, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 59.002, mean reward: 0.590 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.163, 10.098], loss: 0.002186, mae: 0.047879, mean_q: 1.163992
  8400/100000: episode: 84, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 64.603, mean reward: 0.646 [0.519, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.813, 10.098], loss: 0.002483, mae: 0.051187, mean_q: 1.167230
  8500/100000: episode: 85, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 57.020, mean reward: 0.570 [0.500, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.852, 10.208], loss: 0.002422, mae: 0.050270, mean_q: 1.167171
  8600/100000: episode: 86, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.802, mean reward: 0.608 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.001, 10.251], loss: 0.002422, mae: 0.051760, mean_q: 1.169863
  8700/100000: episode: 87, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 61.271, mean reward: 0.613 [0.517, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.419, 10.098], loss: 0.002418, mae: 0.050898, mean_q: 1.172456
  8800/100000: episode: 88, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 61.336, mean reward: 0.613 [0.499, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.873, 10.572], loss: 0.002763, mae: 0.053604, mean_q: 1.167879
  8900/100000: episode: 89, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.784, mean reward: 0.568 [0.498, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.540, 10.102], loss: 0.002467, mae: 0.051938, mean_q: 1.172574
  9000/100000: episode: 90, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 62.445, mean reward: 0.624 [0.501, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.847, 10.374], loss: 0.002335, mae: 0.049785, mean_q: 1.169661
  9100/100000: episode: 91, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 62.151, mean reward: 0.622 [0.516, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.790, 10.222], loss: 0.002521, mae: 0.053086, mean_q: 1.173483
  9200/100000: episode: 92, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 60.113, mean reward: 0.601 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.061, 10.419], loss: 0.002513, mae: 0.052149, mean_q: 1.173003
  9300/100000: episode: 93, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 61.627, mean reward: 0.616 [0.505, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.743, 10.098], loss: 0.002982, mae: 0.056928, mean_q: 1.177940
  9400/100000: episode: 94, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.573, mean reward: 0.576 [0.501, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.612, 10.205], loss: 0.002398, mae: 0.051986, mean_q: 1.176139
  9500/100000: episode: 95, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 60.764, mean reward: 0.608 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.213, 10.370], loss: 0.002440, mae: 0.052058, mean_q: 1.177845
  9600/100000: episode: 96, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.163, mean reward: 0.592 [0.513, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.310, 10.151], loss: 0.002548, mae: 0.053511, mean_q: 1.177862
  9700/100000: episode: 97, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.010, mean reward: 0.580 [0.504, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.264, 10.355], loss: 0.002666, mae: 0.053311, mean_q: 1.173675
  9800/100000: episode: 98, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.178, mean reward: 0.592 [0.506, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.588, 10.118], loss: 0.002350, mae: 0.051497, mean_q: 1.177497
  9900/100000: episode: 99, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 66.736, mean reward: 0.667 [0.539, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.805, 10.527], loss: 0.002313, mae: 0.051286, mean_q: 1.176441
 10000/100000: episode: 100, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.146, mean reward: 0.591 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.155, 10.098], loss: 0.002673, mae: 0.053740, mean_q: 1.174014
 10100/100000: episode: 101, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.543, mean reward: 0.575 [0.505, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.475, 10.214], loss: 0.002974, mae: 0.056880, mean_q: 1.177012
 10200/100000: episode: 102, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 57.532, mean reward: 0.575 [0.500, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.262, 10.098], loss: 0.002618, mae: 0.053072, mean_q: 1.175159
 10300/100000: episode: 103, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 59.535, mean reward: 0.595 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.030, 10.276], loss: 0.002366, mae: 0.051360, mean_q: 1.176916
 10400/100000: episode: 104, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.455, mean reward: 0.595 [0.515, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.251, 10.098], loss: 0.002384, mae: 0.051217, mean_q: 1.179944
 10500/100000: episode: 105, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 62.042, mean reward: 0.620 [0.512, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.293, 10.415], loss: 0.002137, mae: 0.049469, mean_q: 1.178419
 10600/100000: episode: 106, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.341, mean reward: 0.583 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.758, 10.445], loss: 0.002179, mae: 0.048762, mean_q: 1.176859
 10700/100000: episode: 107, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 56.791, mean reward: 0.568 [0.512, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.362, 10.098], loss: 0.002146, mae: 0.048759, mean_q: 1.176500
 10800/100000: episode: 108, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.377, mean reward: 0.594 [0.505, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.752, 10.257], loss: 0.002544, mae: 0.053537, mean_q: 1.178197
 10900/100000: episode: 109, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.811, mean reward: 0.578 [0.502, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.964, 10.106], loss: 0.002551, mae: 0.053036, mean_q: 1.179456
 11000/100000: episode: 110, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 61.188, mean reward: 0.612 [0.515, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.814, 10.098], loss: 0.002504, mae: 0.052156, mean_q: 1.176066
 11100/100000: episode: 111, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.268, mean reward: 0.583 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.281, 10.217], loss: 0.002317, mae: 0.051141, mean_q: 1.177983
 11200/100000: episode: 112, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 56.614, mean reward: 0.566 [0.498, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.132, 10.098], loss: 0.002261, mae: 0.049701, mean_q: 1.173707
 11300/100000: episode: 113, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.856, mean reward: 0.579 [0.497, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.992, 10.098], loss: 0.002212, mae: 0.049564, mean_q: 1.174440
 11400/100000: episode: 114, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.586, mean reward: 0.566 [0.499, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.686, 10.098], loss: 0.002253, mae: 0.050288, mean_q: 1.172912
 11500/100000: episode: 115, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.228, mean reward: 0.582 [0.507, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.053, 10.098], loss: 0.002103, mae: 0.048530, mean_q: 1.172972
 11600/100000: episode: 116, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.563, mean reward: 0.596 [0.517, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.086, 10.207], loss: 0.002250, mae: 0.050009, mean_q: 1.174439
 11700/100000: episode: 117, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.790, mean reward: 0.598 [0.501, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.211, 10.335], loss: 0.002377, mae: 0.050376, mean_q: 1.175357
 11800/100000: episode: 118, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 58.387, mean reward: 0.584 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.663, 10.098], loss: 0.002013, mae: 0.047673, mean_q: 1.174487
 11900/100000: episode: 119, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.657, mean reward: 0.577 [0.499, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.773, 10.134], loss: 0.002074, mae: 0.048317, mean_q: 1.175179
 12000/100000: episode: 120, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.161, mean reward: 0.582 [0.499, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.394, 10.354], loss: 0.002216, mae: 0.049438, mean_q: 1.176594
 12100/100000: episode: 121, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 59.216, mean reward: 0.592 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.537, 10.098], loss: 0.002370, mae: 0.051068, mean_q: 1.175363
 12200/100000: episode: 122, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 56.982, mean reward: 0.570 [0.504, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.062, 10.098], loss: 0.002093, mae: 0.048838, mean_q: 1.175596
 12300/100000: episode: 123, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.454, mean reward: 0.595 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.179, 10.098], loss: 0.002394, mae: 0.051418, mean_q: 1.174720
 12400/100000: episode: 124, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.880, mean reward: 0.579 [0.500, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.241, 10.208], loss: 0.001918, mae: 0.046690, mean_q: 1.174410
 12500/100000: episode: 125, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 56.640, mean reward: 0.566 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.216, 10.098], loss: 0.002132, mae: 0.048484, mean_q: 1.176191
 12600/100000: episode: 126, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.652, mean reward: 0.597 [0.513, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.660, 10.298], loss: 0.002809, mae: 0.051988, mean_q: 1.174215
 12700/100000: episode: 127, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.236, mean reward: 0.592 [0.502, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.627, 10.285], loss: 0.002658, mae: 0.049340, mean_q: 1.174918
 12800/100000: episode: 128, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.034, mean reward: 0.600 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.528, 10.098], loss: 0.002751, mae: 0.052421, mean_q: 1.175118
 12900/100000: episode: 129, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.753, mean reward: 0.598 [0.504, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.238, 10.356], loss: 0.002319, mae: 0.049402, mean_q: 1.174881
 13000/100000: episode: 130, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 59.048, mean reward: 0.590 [0.499, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.509, 10.098], loss: 0.002303, mae: 0.049307, mean_q: 1.170689
 13100/100000: episode: 131, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 59.461, mean reward: 0.595 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.539, 10.098], loss: 0.002184, mae: 0.048491, mean_q: 1.174999
 13200/100000: episode: 132, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.916, mean reward: 0.589 [0.510, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.993, 10.098], loss: 0.002305, mae: 0.050292, mean_q: 1.171907
 13300/100000: episode: 133, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 59.197, mean reward: 0.592 [0.510, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.545, 10.098], loss: 0.002159, mae: 0.048487, mean_q: 1.170729
 13400/100000: episode: 134, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.696, mean reward: 0.577 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.729, 10.276], loss: 0.002161, mae: 0.048907, mean_q: 1.173715
 13500/100000: episode: 135, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.689, mean reward: 0.597 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.346, 10.305], loss: 0.002107, mae: 0.048501, mean_q: 1.170640
 13600/100000: episode: 136, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.966, mean reward: 0.600 [0.507, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.425, 10.241], loss: 0.001910, mae: 0.046230, mean_q: 1.172122
 13700/100000: episode: 137, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 56.434, mean reward: 0.564 [0.498, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.669, 10.109], loss: 0.002127, mae: 0.048928, mean_q: 1.172230
 13800/100000: episode: 138, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.509, mean reward: 0.585 [0.505, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.326, 10.104], loss: 0.001887, mae: 0.045878, mean_q: 1.169465
 13900/100000: episode: 139, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 62.264, mean reward: 0.623 [0.513, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.290, 10.501], loss: 0.001977, mae: 0.047349, mean_q: 1.168660
 14000/100000: episode: 140, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.909, mean reward: 0.589 [0.511, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.059, 10.242], loss: 0.001897, mae: 0.046202, mean_q: 1.172281
 14100/100000: episode: 141, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.449, mean reward: 0.574 [0.500, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.668, 10.099], loss: 0.002171, mae: 0.047307, mean_q: 1.168882
 14200/100000: episode: 142, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 56.897, mean reward: 0.569 [0.500, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.884, 10.098], loss: 0.002534, mae: 0.049531, mean_q: 1.168259
 14300/100000: episode: 143, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.674, mean reward: 0.587 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.420, 10.329], loss: 0.002741, mae: 0.050132, mean_q: 1.164375
 14400/100000: episode: 144, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.882, mean reward: 0.599 [0.510, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.560, 10.098], loss: 0.002385, mae: 0.048178, mean_q: 1.164865
 14500/100000: episode: 145, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 60.147, mean reward: 0.601 [0.518, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.655, 10.098], loss: 0.002486, mae: 0.048061, mean_q: 1.166962
 14600/100000: episode: 146, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.523, mean reward: 0.575 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.125], loss: 0.002628, mae: 0.049719, mean_q: 1.164586
 14700/100000: episode: 147, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 61.901, mean reward: 0.619 [0.519, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.142, 10.098], loss: 0.002355, mae: 0.049041, mean_q: 1.164612
 14800/100000: episode: 148, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.585, mean reward: 0.586 [0.499, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.835, 10.098], loss: 0.002176, mae: 0.047151, mean_q: 1.165221
 14900/100000: episode: 149, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.054, mean reward: 0.571 [0.503, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.274, 10.166], loss: 0.002180, mae: 0.048086, mean_q: 1.163030
[Info] 1-TH LEVEL FOUND: 1.2120271921157837, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.242s, episode steps: 100, steps per second: 19, episode reward: 57.699, mean reward: 0.577 [0.509, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.504, 10.202], loss: 0.001933, mae: 0.046318, mean_q: 1.160837
 15012/100000: episode: 151, duration: 0.091s, episode steps: 12, steps per second: 133, episode reward: 8.624, mean reward: 0.719 [0.674, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.251, 10.471], loss: 0.002103, mae: 0.045646, mean_q: 1.159143
 15026/100000: episode: 152, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 10.606, mean reward: 0.758 [0.721, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.125, 10.412], loss: 0.002092, mae: 0.045640, mean_q: 1.165423
 15036/100000: episode: 153, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 6.545, mean reward: 0.655 [0.579, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-1.113, 10.377], loss: 0.002109, mae: 0.050427, mean_q: 1.175069
 15050/100000: episode: 154, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 9.794, mean reward: 0.700 [0.621, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.431], loss: 0.002448, mae: 0.049286, mean_q: 1.154461
 15105/100000: episode: 155, duration: 0.281s, episode steps: 55, steps per second: 196, episode reward: 37.728, mean reward: 0.686 [0.515, 0.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.743, 10.591], loss: 0.002108, mae: 0.048301, mean_q: 1.169858
 15199/100000: episode: 156, duration: 0.521s, episode steps: 94, steps per second: 181, episode reward: 55.175, mean reward: 0.587 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-1.093, 10.100], loss: 0.002270, mae: 0.050691, mean_q: 1.166491
 15293/100000: episode: 157, duration: 0.497s, episode steps: 94, steps per second: 189, episode reward: 58.388, mean reward: 0.621 [0.503, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.517 [-1.432, 10.270], loss: 0.002111, mae: 0.047578, mean_q: 1.170080
 15387/100000: episode: 158, duration: 0.520s, episode steps: 94, steps per second: 181, episode reward: 56.603, mean reward: 0.602 [0.508, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-1.167, 10.189], loss: 0.001901, mae: 0.046233, mean_q: 1.167605
 15396/100000: episode: 159, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 5.699, mean reward: 0.633 [0.583, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.246], loss: 0.002932, mae: 0.053912, mean_q: 1.157506
 15408/100000: episode: 160, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 8.093, mean reward: 0.674 [0.629, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.372], loss: 0.002064, mae: 0.049253, mean_q: 1.172708
 15463/100000: episode: 161, duration: 0.290s, episode steps: 55, steps per second: 189, episode reward: 31.936, mean reward: 0.581 [0.514, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-1.183, 10.100], loss: 0.001940, mae: 0.047434, mean_q: 1.169275
 15559/100000: episode: 162, duration: 0.514s, episode steps: 96, steps per second: 187, episode reward: 57.100, mean reward: 0.595 [0.498, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.806, 10.305], loss: 0.002185, mae: 0.049165, mean_q: 1.165884
 15655/100000: episode: 163, duration: 0.499s, episode steps: 96, steps per second: 193, episode reward: 55.939, mean reward: 0.583 [0.500, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-1.135, 10.146], loss: 0.002060, mae: 0.047867, mean_q: 1.164751
 15710/100000: episode: 164, duration: 0.320s, episode steps: 55, steps per second: 172, episode reward: 33.465, mean reward: 0.608 [0.511, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-1.179, 10.345], loss: 0.001746, mae: 0.045661, mean_q: 1.169730
 15806/100000: episode: 165, duration: 0.501s, episode steps: 96, steps per second: 192, episode reward: 58.784, mean reward: 0.612 [0.526, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.715, 10.100], loss: 0.002287, mae: 0.048686, mean_q: 1.165699
 15820/100000: episode: 166, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 9.527, mean reward: 0.680 [0.601, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.365], loss: 0.001636, mae: 0.044412, mean_q: 1.173995
 15857/100000: episode: 167, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 24.020, mean reward: 0.649 [0.576, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.063, 10.283], loss: 0.001753, mae: 0.045761, mean_q: 1.164365
 15869/100000: episode: 168, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 8.000, mean reward: 0.667 [0.628, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.378], loss: 0.001755, mae: 0.046625, mean_q: 1.171004
 15879/100000: episode: 169, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 5.886, mean reward: 0.589 [0.555, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.227], loss: 0.001720, mae: 0.044262, mean_q: 1.169393
 15891/100000: episode: 170, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 8.168, mean reward: 0.681 [0.595, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-1.182, 10.295], loss: 0.001609, mae: 0.042492, mean_q: 1.161361
 15946/100000: episode: 171, duration: 0.290s, episode steps: 55, steps per second: 190, episode reward: 33.015, mean reward: 0.600 [0.500, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.303, 10.123], loss: 0.001790, mae: 0.046268, mean_q: 1.169706
 15964/100000: episode: 172, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 13.407, mean reward: 0.745 [0.668, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.595, 10.555], loss: 0.002691, mae: 0.053226, mean_q: 1.168937
 16019/100000: episode: 173, duration: 0.294s, episode steps: 55, steps per second: 187, episode reward: 32.478, mean reward: 0.591 [0.501, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-1.009, 10.247], loss: 0.001984, mae: 0.047742, mean_q: 1.166981
 16037/100000: episode: 174, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 13.048, mean reward: 0.725 [0.676, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.105, 10.418], loss: 0.001983, mae: 0.045471, mean_q: 1.162378
 16047/100000: episode: 175, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 6.324, mean reward: 0.632 [0.596, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.493, 10.358], loss: 0.002184, mae: 0.049270, mean_q: 1.171338
 16084/100000: episode: 176, duration: 0.191s, episode steps: 37, steps per second: 193, episode reward: 24.369, mean reward: 0.659 [0.607, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.454, 10.357], loss: 0.001870, mae: 0.047286, mean_q: 1.172027
 16131/100000: episode: 177, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 28.376, mean reward: 0.604 [0.507, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.302, 10.100], loss: 0.002038, mae: 0.047205, mean_q: 1.169485
 16145/100000: episode: 178, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 10.396, mean reward: 0.743 [0.701, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.587, 10.453], loss: 0.001912, mae: 0.047105, mean_q: 1.169008
 16192/100000: episode: 179, duration: 0.241s, episode steps: 47, steps per second: 195, episode reward: 28.874, mean reward: 0.614 [0.539, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-1.113, 10.285], loss: 0.002359, mae: 0.051526, mean_q: 1.172160
 16202/100000: episode: 180, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 6.140, mean reward: 0.614 [0.586, 0.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.345], loss: 0.002414, mae: 0.051227, mean_q: 1.175028
 16239/100000: episode: 181, duration: 0.207s, episode steps: 37, steps per second: 179, episode reward: 27.116, mean reward: 0.733 [0.626, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.797, 10.556], loss: 0.002340, mae: 0.051089, mean_q: 1.168391
 16335/100000: episode: 182, duration: 0.509s, episode steps: 96, steps per second: 189, episode reward: 56.196, mean reward: 0.585 [0.508, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.280, 10.226], loss: 0.002503, mae: 0.052367, mean_q: 1.172608
 16372/100000: episode: 183, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: 25.430, mean reward: 0.687 [0.609, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.401, 10.475], loss: 0.002049, mae: 0.047243, mean_q: 1.176153
 16419/100000: episode: 184, duration: 0.238s, episode steps: 47, steps per second: 197, episode reward: 28.591, mean reward: 0.608 [0.515, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-1.013, 10.401], loss: 0.002098, mae: 0.048150, mean_q: 1.179733
 16466/100000: episode: 185, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 28.867, mean reward: 0.614 [0.511, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.470, 10.100], loss: 0.002005, mae: 0.048435, mean_q: 1.176241
 16475/100000: episode: 186, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 5.444, mean reward: 0.605 [0.554, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.292], loss: 0.001908, mae: 0.047983, mean_q: 1.181233
 16512/100000: episode: 187, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 25.520, mean reward: 0.690 [0.591, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.339, 10.379], loss: 0.002141, mae: 0.049567, mean_q: 1.181683
 16521/100000: episode: 188, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 6.041, mean reward: 0.671 [0.620, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.432], loss: 0.002528, mae: 0.052323, mean_q: 1.176215
 16576/100000: episode: 189, duration: 0.284s, episode steps: 55, steps per second: 194, episode reward: 31.182, mean reward: 0.567 [0.505, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.182, 10.168], loss: 0.002067, mae: 0.047966, mean_q: 1.179177
 16631/100000: episode: 190, duration: 0.298s, episode steps: 55, steps per second: 185, episode reward: 33.152, mean reward: 0.603 [0.504, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.138, 10.318], loss: 0.002204, mae: 0.049956, mean_q: 1.178939
 16641/100000: episode: 191, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 6.472, mean reward: 0.647 [0.609, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.064, 10.321], loss: 0.002070, mae: 0.048869, mean_q: 1.190366
 16659/100000: episode: 192, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 12.331, mean reward: 0.685 [0.614, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.364, 10.482], loss: 0.002021, mae: 0.047198, mean_q: 1.176114
 16706/100000: episode: 193, duration: 0.244s, episode steps: 47, steps per second: 193, episode reward: 28.225, mean reward: 0.601 [0.539, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-1.367, 10.100], loss: 0.002459, mae: 0.052020, mean_q: 1.181174
 16800/100000: episode: 194, duration: 0.510s, episode steps: 94, steps per second: 184, episode reward: 55.450, mean reward: 0.590 [0.508, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-1.600, 10.162], loss: 0.002215, mae: 0.049854, mean_q: 1.177591
 16814/100000: episode: 195, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 9.685, mean reward: 0.692 [0.592, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.418, 10.339], loss: 0.001852, mae: 0.047207, mean_q: 1.178555
 16832/100000: episode: 196, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 11.791, mean reward: 0.655 [0.562, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.712, 10.378], loss: 0.001988, mae: 0.044949, mean_q: 1.179132
 16842/100000: episode: 197, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 6.423, mean reward: 0.642 [0.586, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-2.148, 10.362], loss: 0.001676, mae: 0.045751, mean_q: 1.188722
 16852/100000: episode: 198, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 6.677, mean reward: 0.668 [0.650, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-1.243, 10.429], loss: 0.001444, mae: 0.040318, mean_q: 1.183800
 16862/100000: episode: 199, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 6.621, mean reward: 0.662 [0.633, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.371], loss: 0.002135, mae: 0.047096, mean_q: 1.188442
 16874/100000: episode: 200, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 9.424, mean reward: 0.785 [0.710, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.607], loss: 0.002442, mae: 0.050310, mean_q: 1.176452
 16892/100000: episode: 201, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 13.879, mean reward: 0.771 [0.683, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.640, 10.371], loss: 0.002354, mae: 0.048833, mean_q: 1.185548
 16906/100000: episode: 202, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 10.172, mean reward: 0.727 [0.635, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.441], loss: 0.002327, mae: 0.049917, mean_q: 1.176805
 16924/100000: episode: 203, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 12.865, mean reward: 0.715 [0.667, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.087, 10.379], loss: 0.002253, mae: 0.047766, mean_q: 1.177911
 16936/100000: episode: 204, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 8.129, mean reward: 0.677 [0.621, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.349], loss: 0.001788, mae: 0.043913, mean_q: 1.181357
 17030/100000: episode: 205, duration: 0.534s, episode steps: 94, steps per second: 176, episode reward: 57.517, mean reward: 0.612 [0.507, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-1.032, 10.119], loss: 0.002464, mae: 0.051803, mean_q: 1.182912
 17124/100000: episode: 206, duration: 0.506s, episode steps: 94, steps per second: 186, episode reward: 55.839, mean reward: 0.594 [0.504, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.519 [-0.593, 10.311], loss: 0.002437, mae: 0.051566, mean_q: 1.190116
 17161/100000: episode: 207, duration: 0.207s, episode steps: 37, steps per second: 178, episode reward: 24.468, mean reward: 0.661 [0.612, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.201, 10.494], loss: 0.002314, mae: 0.049020, mean_q: 1.184805
 17175/100000: episode: 208, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 10.684, mean reward: 0.763 [0.712, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.504], loss: 0.002458, mae: 0.053761, mean_q: 1.188572
 17271/100000: episode: 209, duration: 0.497s, episode steps: 96, steps per second: 193, episode reward: 62.930, mean reward: 0.656 [0.506, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-1.094, 10.100], loss: 0.002118, mae: 0.048509, mean_q: 1.182483
 17326/100000: episode: 210, duration: 0.309s, episode steps: 55, steps per second: 178, episode reward: 32.333, mean reward: 0.588 [0.510, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.698, 10.100], loss: 0.002027, mae: 0.048103, mean_q: 1.189023
 17373/100000: episode: 211, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 27.068, mean reward: 0.576 [0.505, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.645, 10.231], loss: 0.002225, mae: 0.050478, mean_q: 1.192766
 17385/100000: episode: 212, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 7.950, mean reward: 0.663 [0.581, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.342], loss: 0.002151, mae: 0.047327, mean_q: 1.188691
 17481/100000: episode: 213, duration: 0.510s, episode steps: 96, steps per second: 188, episode reward: 56.317, mean reward: 0.587 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.990, 10.106], loss: 0.002120, mae: 0.049395, mean_q: 1.188145
 17495/100000: episode: 214, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 9.732, mean reward: 0.695 [0.659, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.132, 10.407], loss: 0.002015, mae: 0.049161, mean_q: 1.193152
 17505/100000: episode: 215, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 7.030, mean reward: 0.703 [0.630, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.509], loss: 0.002316, mae: 0.050306, mean_q: 1.180285
 17515/100000: episode: 216, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 6.312, mean reward: 0.631 [0.590, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.206, 10.279], loss: 0.002079, mae: 0.050246, mean_q: 1.199314
 17562/100000: episode: 217, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 27.614, mean reward: 0.588 [0.519, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.185, 10.100], loss: 0.002118, mae: 0.049434, mean_q: 1.189091
 17609/100000: episode: 218, duration: 0.253s, episode steps: 47, steps per second: 185, episode reward: 27.852, mean reward: 0.593 [0.509, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.760, 10.163], loss: 0.002379, mae: 0.051299, mean_q: 1.187524
 17621/100000: episode: 219, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 9.157, mean reward: 0.763 [0.687, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.665], loss: 0.002063, mae: 0.047088, mean_q: 1.191831
 17715/100000: episode: 220, duration: 0.496s, episode steps: 94, steps per second: 190, episode reward: 56.038, mean reward: 0.596 [0.508, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.520 [-0.377, 10.100], loss: 0.002663, mae: 0.053013, mean_q: 1.190292
 17724/100000: episode: 221, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 5.641, mean reward: 0.627 [0.583, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.334], loss: 0.002325, mae: 0.049480, mean_q: 1.198050
 17779/100000: episode: 222, duration: 0.293s, episode steps: 55, steps per second: 188, episode reward: 33.711, mean reward: 0.613 [0.513, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-0.214, 10.223], loss: 0.002392, mae: 0.051888, mean_q: 1.191506
 17875/100000: episode: 223, duration: 0.506s, episode steps: 96, steps per second: 190, episode reward: 55.864, mean reward: 0.582 [0.511, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.504, 10.154], loss: 0.002625, mae: 0.053311, mean_q: 1.193749
 17884/100000: episode: 224, duration: 0.059s, episode steps: 9, steps per second: 151, episode reward: 5.364, mean reward: 0.596 [0.539, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.219], loss: 0.002786, mae: 0.056196, mean_q: 1.178423
 17896/100000: episode: 225, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 7.791, mean reward: 0.649 [0.591, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.281, 10.300], loss: 0.002455, mae: 0.052329, mean_q: 1.199011
 17933/100000: episode: 226, duration: 0.215s, episode steps: 37, steps per second: 172, episode reward: 24.846, mean reward: 0.672 [0.571, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.880, 10.269], loss: 0.002274, mae: 0.050496, mean_q: 1.190169
 17988/100000: episode: 227, duration: 0.300s, episode steps: 55, steps per second: 183, episode reward: 33.282, mean reward: 0.605 [0.521, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.661, 10.297], loss: 0.002361, mae: 0.050497, mean_q: 1.188388
 18006/100000: episode: 228, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 13.313, mean reward: 0.740 [0.695, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.851, 10.498], loss: 0.002598, mae: 0.053858, mean_q: 1.180802
 18018/100000: episode: 229, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 8.394, mean reward: 0.699 [0.646, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.452], loss: 0.002164, mae: 0.052192, mean_q: 1.200633
 18073/100000: episode: 230, duration: 0.294s, episode steps: 55, steps per second: 187, episode reward: 31.346, mean reward: 0.570 [0.511, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.941, 10.164], loss: 0.002185, mae: 0.049086, mean_q: 1.196806
 18091/100000: episode: 231, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 12.377, mean reward: 0.688 [0.614, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.591, 10.299], loss: 0.002011, mae: 0.046853, mean_q: 1.194958
 18146/100000: episode: 232, duration: 0.316s, episode steps: 55, steps per second: 174, episode reward: 32.404, mean reward: 0.589 [0.505, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.929, 10.240], loss: 0.002001, mae: 0.046339, mean_q: 1.193181
 18183/100000: episode: 233, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 24.168, mean reward: 0.653 [0.594, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.621, 10.303], loss: 0.001944, mae: 0.046650, mean_q: 1.193347
 18220/100000: episode: 234, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 26.740, mean reward: 0.723 [0.586, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.035, 10.520], loss: 0.002903, mae: 0.055325, mean_q: 1.199504
 18257/100000: episode: 235, duration: 0.211s, episode steps: 37, steps per second: 175, episode reward: 23.598, mean reward: 0.638 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.418, 10.145], loss: 0.002257, mae: 0.050146, mean_q: 1.194146
 18294/100000: episode: 236, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 26.259, mean reward: 0.710 [0.622, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.961, 10.383], loss: 0.002607, mae: 0.053920, mean_q: 1.192961
 18341/100000: episode: 237, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 29.644, mean reward: 0.631 [0.502, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.830, 10.105], loss: 0.002746, mae: 0.053891, mean_q: 1.191315
 18355/100000: episode: 238, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 9.255, mean reward: 0.661 [0.611, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.395], loss: 0.002200, mae: 0.048039, mean_q: 1.195401
 18367/100000: episode: 239, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 8.469, mean reward: 0.706 [0.667, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.257, 10.396], loss: 0.001966, mae: 0.048068, mean_q: 1.190516
[Info] 2-TH LEVEL FOUND: 1.4959455728530884, Considering 10/90 traces
 18461/100000: episode: 240, duration: 4.703s, episode steps: 94, steps per second: 20, episode reward: 54.470, mean reward: 0.579 [0.498, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.700, 10.100], loss: 0.002531, mae: 0.052237, mean_q: 1.197633
 18474/100000: episode: 241, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 10.246, mean reward: 0.788 [0.633, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.320, 10.460], loss: 0.002282, mae: 0.050620, mean_q: 1.194280
 18479/100000: episode: 242, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 3.752, mean reward: 0.750 [0.726, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.471, 10.505], loss: 0.001801, mae: 0.047211, mean_q: 1.193620
 18491/100000: episode: 243, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 9.176, mean reward: 0.765 [0.694, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.616], loss: 0.001877, mae: 0.045145, mean_q: 1.200078
 18503/100000: episode: 244, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 8.369, mean reward: 0.697 [0.672, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.420], loss: 0.002096, mae: 0.047273, mean_q: 1.189596
 18508/100000: episode: 245, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 3.666, mean reward: 0.733 [0.705, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.538], loss: 0.002144, mae: 0.049517, mean_q: 1.190907
 18519/100000: episode: 246, duration: 0.073s, episode steps: 11, steps per second: 152, episode reward: 7.679, mean reward: 0.698 [0.664, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.049, 10.433], loss: 0.002498, mae: 0.052912, mean_q: 1.200530
 18529/100000: episode: 247, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 7.925, mean reward: 0.793 [0.749, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.555], loss: 0.001842, mae: 0.046648, mean_q: 1.206571
 18542/100000: episode: 248, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 10.060, mean reward: 0.774 [0.727, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.304, 10.482], loss: 0.002104, mae: 0.047659, mean_q: 1.203907
 18555/100000: episode: 249, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 10.999, mean reward: 0.846 [0.774, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.563], loss: 0.001730, mae: 0.043570, mean_q: 1.195151
 18560/100000: episode: 250, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 3.830, mean reward: 0.766 [0.723, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.534], loss: 0.002020, mae: 0.048805, mean_q: 1.209906
 18565/100000: episode: 251, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 4.091, mean reward: 0.818 [0.782, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.500], loss: 0.002393, mae: 0.052077, mean_q: 1.190297
 18570/100000: episode: 252, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 3.946, mean reward: 0.789 [0.755, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.521], loss: 0.001989, mae: 0.049612, mean_q: 1.224360
 18583/100000: episode: 253, duration: 0.086s, episode steps: 13, steps per second: 152, episode reward: 10.346, mean reward: 0.796 [0.759, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.431], loss: 0.002200, mae: 0.050947, mean_q: 1.199124
 18593/100000: episode: 254, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 7.621, mean reward: 0.762 [0.685, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.482], loss: 0.002378, mae: 0.052302, mean_q: 1.205958
 18598/100000: episode: 255, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 4.023, mean reward: 0.805 [0.771, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.504], loss: 0.002344, mae: 0.050429, mean_q: 1.195260
 18603/100000: episode: 256, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 3.731, mean reward: 0.746 [0.730, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.475], loss: 0.001939, mae: 0.049843, mean_q: 1.220256
 18615/100000: episode: 257, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 9.102, mean reward: 0.759 [0.701, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.024, 10.486], loss: 0.002592, mae: 0.054318, mean_q: 1.194055
 18626/100000: episode: 258, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 8.601, mean reward: 0.782 [0.715, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.265, 10.460], loss: 0.002533, mae: 0.054721, mean_q: 1.205824
 18638/100000: episode: 259, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 8.914, mean reward: 0.743 [0.707, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.353, 10.507], loss: 0.001892, mae: 0.047387, mean_q: 1.197692
 18643/100000: episode: 260, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 3.646, mean reward: 0.729 [0.702, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.987, 10.455], loss: 0.001896, mae: 0.046007, mean_q: 1.226804
 18653/100000: episode: 261, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 8.020, mean reward: 0.802 [0.782, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.570], loss: 0.002020, mae: 0.046391, mean_q: 1.199657
 18658/100000: episode: 262, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 4.010, mean reward: 0.802 [0.788, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.580], loss: 0.002134, mae: 0.049243, mean_q: 1.212641
 18670/100000: episode: 263, duration: 0.065s, episode steps: 12, steps per second: 183, episode reward: 10.958, mean reward: 0.913 [0.836, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.580], loss: 0.002246, mae: 0.049407, mean_q: 1.208371
 18680/100000: episode: 264, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 7.568, mean reward: 0.757 [0.710, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.490], loss: 0.002586, mae: 0.053922, mean_q: 1.209115
 18692/100000: episode: 265, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 8.428, mean reward: 0.702 [0.670, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.484], loss: 0.002865, mae: 0.056377, mean_q: 1.207256
 18702/100000: episode: 266, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 8.770, mean reward: 0.877 [0.814, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.116, 10.665], loss: 0.003222, mae: 0.058212, mean_q: 1.216143
 18714/100000: episode: 267, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 8.823, mean reward: 0.735 [0.710, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.500], loss: 0.002836, mae: 0.055531, mean_q: 1.203301
 18719/100000: episode: 268, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 3.668, mean reward: 0.734 [0.700, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.425], loss: 0.003503, mae: 0.058264, mean_q: 1.199227
 18730/100000: episode: 269, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 7.951, mean reward: 0.723 [0.698, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.458], loss: 0.002449, mae: 0.052406, mean_q: 1.231082
 18744/100000: episode: 270, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 10.080, mean reward: 0.720 [0.658, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.447], loss: 0.002889, mae: 0.054778, mean_q: 1.202792
 18757/100000: episode: 271, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 10.877, mean reward: 0.837 [0.785, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.742, 10.588], loss: 0.002615, mae: 0.053438, mean_q: 1.200765
 18767/100000: episode: 272, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 8.565, mean reward: 0.857 [0.799, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.816, 10.657], loss: 0.002281, mae: 0.052967, mean_q: 1.212393
 18780/100000: episode: 273, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 10.352, mean reward: 0.796 [0.750, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.618], loss: 0.002209, mae: 0.049756, mean_q: 1.215782
 18790/100000: episode: 274, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 7.355, mean reward: 0.736 [0.705, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.475], loss: 0.002960, mae: 0.056354, mean_q: 1.213954
 18800/100000: episode: 275, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 7.753, mean reward: 0.775 [0.736, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.883, 10.518], loss: 0.002943, mae: 0.056421, mean_q: 1.234261
 18810/100000: episode: 276, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 7.906, mean reward: 0.791 [0.698, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.699], loss: 0.003874, mae: 0.061531, mean_q: 1.212993
 18822/100000: episode: 277, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 8.731, mean reward: 0.728 [0.627, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.447], loss: 0.003392, mae: 0.062386, mean_q: 1.204175
 18836/100000: episode: 278, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 11.324, mean reward: 0.809 [0.719, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-1.257, 10.570], loss: 0.002501, mae: 0.054205, mean_q: 1.211727
 18849/100000: episode: 279, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 10.116, mean reward: 0.778 [0.740, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-1.343, 10.549], loss: 0.002195, mae: 0.050840, mean_q: 1.214763
 18861/100000: episode: 280, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 8.836, mean reward: 0.736 [0.706, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.486], loss: 0.002099, mae: 0.049238, mean_q: 1.211070
 18871/100000: episode: 281, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 7.314, mean reward: 0.731 [0.708, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.507], loss: 0.001752, mae: 0.043973, mean_q: 1.225363
 18884/100000: episode: 282, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 9.337, mean reward: 0.718 [0.610, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.383, 10.473], loss: 0.002242, mae: 0.052125, mean_q: 1.226840
 18895/100000: episode: 283, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 8.141, mean reward: 0.740 [0.705, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-1.270, 10.447], loss: 0.002147, mae: 0.047656, mean_q: 1.230739
 18905/100000: episode: 284, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 8.103, mean reward: 0.810 [0.761, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.650], loss: 0.002769, mae: 0.055243, mean_q: 1.200848
 18917/100000: episode: 285, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 9.580, mean reward: 0.798 [0.712, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.621, 10.568], loss: 0.003549, mae: 0.060903, mean_q: 1.212661
 18931/100000: episode: 286, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 11.346, mean reward: 0.810 [0.767, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.616], loss: 0.002862, mae: 0.056353, mean_q: 1.221832
 18945/100000: episode: 287, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 11.992, mean reward: 0.857 [0.791, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.152, 10.635], loss: 0.003906, mae: 0.061955, mean_q: 1.221326
 18950/100000: episode: 288, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 3.833, mean reward: 0.767 [0.736, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.531], loss: 0.004023, mae: 0.064611, mean_q: 1.205784
 18964/100000: episode: 289, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 10.695, mean reward: 0.764 [0.686, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.084, 10.478], loss: 0.002962, mae: 0.056855, mean_q: 1.218339
 18976/100000: episode: 290, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 9.459, mean reward: 0.788 [0.703, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.535, 10.538], loss: 0.002682, mae: 0.055330, mean_q: 1.222122
 18981/100000: episode: 291, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 4.066, mean reward: 0.813 [0.787, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.548, 10.577], loss: 0.002618, mae: 0.052837, mean_q: 1.238981
 18995/100000: episode: 292, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 10.646, mean reward: 0.760 [0.724, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.763, 10.310], loss: 0.003022, mae: 0.056876, mean_q: 1.234829
 19008/100000: episode: 293, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 9.464, mean reward: 0.728 [0.676, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.609, 10.485], loss: 0.002375, mae: 0.052720, mean_q: 1.223061
 19018/100000: episode: 294, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 7.296, mean reward: 0.730 [0.651, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.628, 10.386], loss: 0.002085, mae: 0.050926, mean_q: 1.221192
 19031/100000: episode: 295, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 9.778, mean reward: 0.752 [0.710, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.796, 10.423], loss: 0.002229, mae: 0.050808, mean_q: 1.221726
 19043/100000: episode: 296, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 8.683, mean reward: 0.724 [0.679, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.315, 10.556], loss: 0.001985, mae: 0.047231, mean_q: 1.215564
 19053/100000: episode: 297, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 7.709, mean reward: 0.771 [0.735, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.503], loss: 0.002268, mae: 0.050009, mean_q: 1.247620
 19058/100000: episode: 298, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 3.990, mean reward: 0.798 [0.783, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.555], loss: 0.002628, mae: 0.051190, mean_q: 1.228847
 19068/100000: episode: 299, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 7.374, mean reward: 0.737 [0.702, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-1.022, 10.471], loss: 0.002771, mae: 0.054228, mean_q: 1.206326
 19082/100000: episode: 300, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 11.065, mean reward: 0.790 [0.684, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.487], loss: 0.002900, mae: 0.057358, mean_q: 1.233038
 19094/100000: episode: 301, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 9.768, mean reward: 0.814 [0.776, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.673, 10.535], loss: 0.003303, mae: 0.061139, mean_q: 1.216840
 19105/100000: episode: 302, duration: 0.071s, episode steps: 11, steps per second: 156, episode reward: 7.800, mean reward: 0.709 [0.672, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.453], loss: 0.002319, mae: 0.051600, mean_q: 1.221584
 19110/100000: episode: 303, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 3.830, mean reward: 0.766 [0.747, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.271, 10.487], loss: 0.002404, mae: 0.053147, mean_q: 1.224676
 19121/100000: episode: 304, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 8.010, mean reward: 0.728 [0.671, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.460], loss: 0.002322, mae: 0.050967, mean_q: 1.213507
 19132/100000: episode: 305, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 8.532, mean reward: 0.776 [0.740, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.628, 10.560], loss: 0.002791, mae: 0.056043, mean_q: 1.229441
 19137/100000: episode: 306, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 3.968, mean reward: 0.794 [0.758, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.571], loss: 0.003212, mae: 0.059518, mean_q: 1.212769
 19151/100000: episode: 307, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 10.787, mean reward: 0.771 [0.675, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.433], loss: 0.002272, mae: 0.050129, mean_q: 1.225856
 19163/100000: episode: 308, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 9.224, mean reward: 0.769 [0.734, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.093, 10.639], loss: 0.002663, mae: 0.055832, mean_q: 1.238803
 19175/100000: episode: 309, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 8.681, mean reward: 0.723 [0.670, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.575], loss: 0.002585, mae: 0.053928, mean_q: 1.232912
 19185/100000: episode: 310, duration: 0.068s, episode steps: 10, steps per second: 146, episode reward: 7.667, mean reward: 0.767 [0.736, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.488], loss: 0.003019, mae: 0.055160, mean_q: 1.219594
 19196/100000: episode: 311, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 7.685, mean reward: 0.699 [0.636, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-1.298, 10.414], loss: 0.002972, mae: 0.055618, mean_q: 1.239674
 19208/100000: episode: 312, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 9.910, mean reward: 0.826 [0.774, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.961, 10.464], loss: 0.003142, mae: 0.055472, mean_q: 1.228158
 19220/100000: episode: 313, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 9.984, mean reward: 0.832 [0.789, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.319, 10.550], loss: 0.002767, mae: 0.053499, mean_q: 1.232203
 19230/100000: episode: 314, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 7.852, mean reward: 0.785 [0.734, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.560], loss: 0.003183, mae: 0.057947, mean_q: 1.248740
 19242/100000: episode: 315, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 9.370, mean reward: 0.781 [0.699, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.255, 10.558], loss: 0.003081, mae: 0.056980, mean_q: 1.225289
 19253/100000: episode: 316, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 7.833, mean reward: 0.712 [0.582, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.552], loss: 0.002594, mae: 0.052132, mean_q: 1.231240
 19258/100000: episode: 317, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 3.875, mean reward: 0.775 [0.734, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.478], loss: 0.002917, mae: 0.057608, mean_q: 1.260522
 19268/100000: episode: 318, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 8.651, mean reward: 0.865 [0.786, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.493], loss: 0.002934, mae: 0.058810, mean_q: 1.253920
 19278/100000: episode: 319, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 8.501, mean reward: 0.850 [0.804, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-1.222, 10.645], loss: 0.002127, mae: 0.050227, mean_q: 1.231320
 19290/100000: episode: 320, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 8.760, mean reward: 0.730 [0.701, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.711, 10.377], loss: 0.002040, mae: 0.047355, mean_q: 1.245786
 19300/100000: episode: 321, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 8.745, mean reward: 0.874 [0.827, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.428, 10.599], loss: 0.002589, mae: 0.052994, mean_q: 1.232857
 19311/100000: episode: 322, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 7.842, mean reward: 0.713 [0.668, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-1.006, 10.452], loss: 0.002301, mae: 0.051294, mean_q: 1.246330
 19321/100000: episode: 323, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 8.165, mean reward: 0.817 [0.729, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.623], loss: 0.002285, mae: 0.051862, mean_q: 1.230686
 19333/100000: episode: 324, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 10.286, mean reward: 0.857 [0.809, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.560], loss: 0.002639, mae: 0.053784, mean_q: 1.235337
 19346/100000: episode: 325, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 11.055, mean reward: 0.850 [0.794, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.226, 10.603], loss: 0.002135, mae: 0.049546, mean_q: 1.233076
 19359/100000: episode: 326, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 10.050, mean reward: 0.773 [0.647, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.525], loss: 0.002888, mae: 0.056884, mean_q: 1.229373
 19373/100000: episode: 327, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 10.819, mean reward: 0.773 [0.705, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.578], loss: 0.002698, mae: 0.054965, mean_q: 1.238735
 19384/100000: episode: 328, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 8.027, mean reward: 0.730 [0.676, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.451], loss: 0.002891, mae: 0.053097, mean_q: 1.234188
 19395/100000: episode: 329, duration: 0.057s, episode steps: 11, steps per second: 191, episode reward: 8.085, mean reward: 0.735 [0.678, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.569], loss: 0.002223, mae: 0.050173, mean_q: 1.253811
[Info] 3-TH LEVEL FOUND: 1.5891838073730469, Considering 10/90 traces
 19407/100000: episode: 330, duration: 4.254s, episode steps: 12, steps per second: 3, episode reward: 10.099, mean reward: 0.842 [0.802, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.103, 10.532], loss: 0.002685, mae: 0.053664, mean_q: 1.244092
 19414/100000: episode: 331, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 5.842, mean reward: 0.835 [0.783, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.602], loss: 0.002643, mae: 0.053711, mean_q: 1.235957
 19424/100000: episode: 332, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 9.221, mean reward: 0.922 [0.843, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.550], loss: 0.002303, mae: 0.051335, mean_q: 1.252125
 19434/100000: episode: 333, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 8.074, mean reward: 0.807 [0.755, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.042, 10.440], loss: 0.001996, mae: 0.050380, mean_q: 1.263353
 19442/100000: episode: 334, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 7.002, mean reward: 0.875 [0.839, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.735], loss: 0.002489, mae: 0.052098, mean_q: 1.242120
 19448/100000: episode: 335, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 4.547, mean reward: 0.758 [0.739, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.557], loss: 0.003186, mae: 0.056309, mean_q: 1.250511
 19452/100000: episode: 336, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 3.491, mean reward: 0.873 [0.807, 0.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-1.089, 10.485], loss: 0.003197, mae: 0.056075, mean_q: 1.234873
[Info] FALSIFICATION!
 19457/100000: episode: 337, duration: 0.471s, episode steps: 5, steps per second: 11, episode reward: 4.872, mean reward: 0.974 [0.895, 1.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.015, 10.661], loss: 0.003617, mae: 0.059733, mean_q: 1.268659
 19461/100000: episode: 338, duration: 0.035s, episode steps: 4, steps per second: 115, episode reward: 3.468, mean reward: 0.867 [0.827, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.115, 10.604], loss: 0.003731, mae: 0.060710, mean_q: 1.244228
 19471/100000: episode: 339, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 8.852, mean reward: 0.885 [0.861, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.662], loss: 0.004507, mae: 0.066649, mean_q: 1.252745
 19480/100000: episode: 340, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 6.955, mean reward: 0.773 [0.726, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.770, 10.441], loss: 0.003195, mae: 0.059809, mean_q: 1.256259
 19490/100000: episode: 341, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 8.561, mean reward: 0.856 [0.815, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.539], loss: 0.002775, mae: 0.057087, mean_q: 1.251218
 19496/100000: episode: 342, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 5.044, mean reward: 0.841 [0.800, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.674], loss: 0.003185, mae: 0.059460, mean_q: 1.220773
 19503/100000: episode: 343, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 5.573, mean reward: 0.796 [0.758, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.524], loss: 0.005816, mae: 0.071178, mean_q: 1.214948
 19513/100000: episode: 344, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 8.863, mean reward: 0.886 [0.853, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.746], loss: 0.002969, mae: 0.060318, mean_q: 1.262527
 19523/100000: episode: 345, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 8.500, mean reward: 0.850 [0.805, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.291, 10.601], loss: 0.002763, mae: 0.054415, mean_q: 1.257712
 19530/100000: episode: 346, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 6.217, mean reward: 0.888 [0.846, 0.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.657, 10.713], loss: 0.003400, mae: 0.061166, mean_q: 1.239545
 19538/100000: episode: 347, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 6.775, mean reward: 0.847 [0.802, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.534], loss: 0.002944, mae: 0.055461, mean_q: 1.263671
 19548/100000: episode: 348, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 9.172, mean reward: 0.917 [0.871, 0.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.686], loss: 0.003110, mae: 0.057728, mean_q: 1.251659
 19558/100000: episode: 349, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 8.122, mean reward: 0.812 [0.755, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.589], loss: 0.003044, mae: 0.059325, mean_q: 1.261409
 19568/100000: episode: 350, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 8.545, mean reward: 0.854 [0.808, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.152, 10.603], loss: 0.003042, mae: 0.054125, mean_q: 1.245992
 19576/100000: episode: 351, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 6.591, mean reward: 0.824 [0.777, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.585], loss: 0.003089, mae: 0.056004, mean_q: 1.220044
 19582/100000: episode: 352, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 4.970, mean reward: 0.828 [0.810, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.629], loss: 0.002208, mae: 0.050292, mean_q: 1.264529
 19592/100000: episode: 353, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 8.420, mean reward: 0.842 [0.788, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.566], loss: 0.002815, mae: 0.052803, mean_q: 1.232583
 19599/100000: episode: 354, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 5.838, mean reward: 0.834 [0.805, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.056, 10.573], loss: 0.003253, mae: 0.057596, mean_q: 1.234776
 19605/100000: episode: 355, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 4.865, mean reward: 0.811 [0.762, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.585], loss: 0.004099, mae: 0.062874, mean_q: 1.261355
 19615/100000: episode: 356, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 8.707, mean reward: 0.871 [0.813, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.625], loss: 0.002587, mae: 0.053334, mean_q: 1.261095
 19623/100000: episode: 357, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 6.963, mean reward: 0.870 [0.819, 0.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.655], loss: 0.002779, mae: 0.053433, mean_q: 1.269196
 19630/100000: episode: 358, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 5.463, mean reward: 0.780 [0.714, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.496], loss: 0.003487, mae: 0.058158, mean_q: 1.263865
 19640/100000: episode: 359, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 9.197, mean reward: 0.920 [0.873, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.981, 10.579], loss: 0.003487, mae: 0.061771, mean_q: 1.255361
 19648/100000: episode: 360, duration: 0.056s, episode steps: 8, steps per second: 143, episode reward: 7.565, mean reward: 0.946 [0.852, 0.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.697], loss: 0.003576, mae: 0.063490, mean_q: 1.275046
 19658/100000: episode: 361, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 8.565, mean reward: 0.856 [0.839, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.588], loss: 0.003454, mae: 0.064785, mean_q: 1.243812
 19665/100000: episode: 362, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 5.676, mean reward: 0.811 [0.758, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.487, 10.519], loss: 0.002790, mae: 0.052804, mean_q: 1.221449
 19673/100000: episode: 363, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 6.534, mean reward: 0.817 [0.759, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.530], loss: 0.002543, mae: 0.054098, mean_q: 1.251822
[Info] FALSIFICATION!
 19680/100000: episode: 364, duration: 0.304s, episode steps: 7, steps per second: 23, episode reward: 6.586, mean reward: 0.941 [0.875, 1.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.822], loss: 0.003708, mae: 0.063731, mean_q: 1.255565
 19689/100000: episode: 365, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 7.638, mean reward: 0.849 [0.771, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.610], loss: 0.003109, mae: 0.058619, mean_q: 1.264861
 19699/100000: episode: 366, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 9.179, mean reward: 0.918 [0.878, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.656, 10.675], loss: 0.002455, mae: 0.051981, mean_q: 1.257462
 19709/100000: episode: 367, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 9.100, mean reward: 0.910 [0.845, 0.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.733, 10.570], loss: 0.002627, mae: 0.053041, mean_q: 1.258786
 19718/100000: episode: 368, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 7.455, mean reward: 0.828 [0.760, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.141, 10.569], loss: 0.003707, mae: 0.060252, mean_q: 1.262674
 19728/100000: episode: 369, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 8.205, mean reward: 0.821 [0.787, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.603], loss: 0.004575, mae: 0.067943, mean_q: 1.281744
 19738/100000: episode: 370, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 8.415, mean reward: 0.841 [0.800, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.072, 10.632], loss: 0.007361, mae: 0.087285, mean_q: 1.284045
[Info] FALSIFICATION!
 19744/100000: episode: 371, duration: 0.300s, episode steps: 6, steps per second: 20, episode reward: 5.470, mean reward: 0.912 [0.804, 1.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.011, 10.602], loss: 0.003790, mae: 0.066871, mean_q: 1.294487
 19750/100000: episode: 372, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 4.908, mean reward: 0.818 [0.806, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.457, 10.546], loss: 0.003089, mae: 0.059928, mean_q: 1.269769
 19758/100000: episode: 373, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 6.708, mean reward: 0.838 [0.794, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.486, 10.559], loss: 0.002715, mae: 0.056182, mean_q: 1.315132
 19768/100000: episode: 374, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 8.207, mean reward: 0.821 [0.781, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.490], loss: 0.002947, mae: 0.057332, mean_q: 1.267401
 19778/100000: episode: 375, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 8.023, mean reward: 0.802 [0.762, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.545], loss: 0.003198, mae: 0.057866, mean_q: 1.268790
[Info] FALSIFICATION!
 19785/100000: episode: 376, duration: 0.305s, episode steps: 7, steps per second: 23, episode reward: 6.582, mean reward: 0.940 [0.875, 1.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.808, 10.504], loss: 0.004224, mae: 0.071356, mean_q: 1.304282
 19793/100000: episode: 377, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 6.409, mean reward: 0.801 [0.737, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.561], loss: 0.003114, mae: 0.060389, mean_q: 1.269881
 19802/100000: episode: 378, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 7.595, mean reward: 0.844 [0.794, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.609], loss: 0.003168, mae: 0.056889, mean_q: 1.255726
 19810/100000: episode: 379, duration: 0.042s, episode steps: 8, steps per second: 188, episode reward: 6.542, mean reward: 0.818 [0.779, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.477], loss: 0.003568, mae: 0.059799, mean_q: 1.291178
 19818/100000: episode: 380, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 6.316, mean reward: 0.789 [0.774, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.565], loss: 0.005882, mae: 0.078404, mean_q: 1.231852
 19828/100000: episode: 381, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 8.277, mean reward: 0.828 [0.728, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.474], loss: 0.004222, mae: 0.066392, mean_q: 1.249144
 19838/100000: episode: 382, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 8.848, mean reward: 0.885 [0.845, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.651], loss: 0.003580, mae: 0.059724, mean_q: 1.257846
 19848/100000: episode: 383, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 7.896, mean reward: 0.790 [0.724, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.625, 10.492], loss: 0.003000, mae: 0.058679, mean_q: 1.288630
 19858/100000: episode: 384, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 8.488, mean reward: 0.849 [0.805, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.631], loss: 0.002731, mae: 0.055864, mean_q: 1.284394
 19867/100000: episode: 385, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 7.687, mean reward: 0.854 [0.786, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.448, 10.685], loss: 0.002898, mae: 0.056137, mean_q: 1.271425
 19877/100000: episode: 386, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 9.260, mean reward: 0.926 [0.839, 0.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.091, 10.629], loss: 0.002865, mae: 0.056795, mean_q: 1.270214
 19887/100000: episode: 387, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 8.186, mean reward: 0.819 [0.773, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.436, 10.536], loss: 0.001979, mae: 0.048207, mean_q: 1.288163
 19897/100000: episode: 388, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 8.719, mean reward: 0.872 [0.849, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.646], loss: 0.002667, mae: 0.055384, mean_q: 1.274918
 19907/100000: episode: 389, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 8.426, mean reward: 0.843 [0.791, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.611], loss: 0.002128, mae: 0.048419, mean_q: 1.277663
 19914/100000: episode: 390, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 5.808, mean reward: 0.830 [0.772, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.669], loss: 0.002379, mae: 0.051460, mean_q: 1.299182
 19924/100000: episode: 391, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 8.150, mean reward: 0.815 [0.698, 0.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.589], loss: 0.002774, mae: 0.053978, mean_q: 1.285688
 19934/100000: episode: 392, duration: 0.069s, episode steps: 10, steps per second: 146, episode reward: 8.367, mean reward: 0.837 [0.762, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-1.476, 10.470], loss: 0.002734, mae: 0.056103, mean_q: 1.268011
 19938/100000: episode: 393, duration: 0.024s, episode steps: 4, steps per second: 163, episode reward: 3.515, mean reward: 0.879 [0.844, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.355 [-0.035, 10.658], loss: 0.002795, mae: 0.057738, mean_q: 1.279515
 19947/100000: episode: 394, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 7.326, mean reward: 0.814 [0.758, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.588], loss: 0.003503, mae: 0.063184, mean_q: 1.280658
 19954/100000: episode: 395, duration: 0.039s, episode steps: 7, steps per second: 177, episode reward: 5.629, mean reward: 0.804 [0.747, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.211, 10.510], loss: 0.003613, mae: 0.062040, mean_q: 1.267273
 19964/100000: episode: 396, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 8.885, mean reward: 0.889 [0.813, 0.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.639], loss: 0.002955, mae: 0.055604, mean_q: 1.296333
 19974/100000: episode: 397, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 8.493, mean reward: 0.849 [0.779, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.315, 10.575], loss: 0.005298, mae: 0.070887, mean_q: 1.281792
 19984/100000: episode: 398, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 8.344, mean reward: 0.834 [0.773, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.567], loss: 0.003723, mae: 0.063388, mean_q: 1.283993
 19988/100000: episode: 399, duration: 0.024s, episode steps: 4, steps per second: 165, episode reward: 3.416, mean reward: 0.854 [0.840, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.580], loss: 0.004762, mae: 0.063929, mean_q: 1.281562
 19996/100000: episode: 400, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 6.806, mean reward: 0.851 [0.787, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.605], loss: 0.002247, mae: 0.048236, mean_q: 1.259836
 20006/100000: episode: 401, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 8.161, mean reward: 0.816 [0.747, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.528], loss: 0.002983, mae: 0.057407, mean_q: 1.300449
 20012/100000: episode: 402, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 4.692, mean reward: 0.782 [0.757, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.474], loss: 0.002459, mae: 0.054526, mean_q: 1.270306
 20022/100000: episode: 403, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 8.720, mean reward: 0.872 [0.795, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.602], loss: 0.002609, mae: 0.050211, mean_q: 1.293959
 20032/100000: episode: 404, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 8.884, mean reward: 0.888 [0.860, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.645, 10.637], loss: 0.003215, mae: 0.056376, mean_q: 1.289100
 20042/100000: episode: 405, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 8.751, mean reward: 0.875 [0.834, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.558], loss: 0.003029, mae: 0.056881, mean_q: 1.297607
 20052/100000: episode: 406, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 8.246, mean reward: 0.825 [0.765, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.530], loss: 0.003965, mae: 0.062596, mean_q: 1.266810
 20056/100000: episode: 407, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 3.545, mean reward: 0.886 [0.847, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.656], loss: 0.005734, mae: 0.071920, mean_q: 1.286712
 20066/100000: episode: 408, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 8.215, mean reward: 0.822 [0.746, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.497], loss: 0.005139, mae: 0.071299, mean_q: 1.299125
[Info] FALSIFICATION!
 20076/100000: episode: 409, duration: 0.283s, episode steps: 10, steps per second: 35, episode reward: 9.487, mean reward: 0.949 [0.879, 1.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.262, 10.662], loss: 0.003356, mae: 0.062710, mean_q: 1.290821
 20086/100000: episode: 410, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 8.977, mean reward: 0.898 [0.846, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.352, 10.616], loss: 0.003102, mae: 0.053001, mean_q: 1.291708
 20093/100000: episode: 411, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 5.629, mean reward: 0.804 [0.778, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.674, 10.552], loss: 0.002754, mae: 0.054644, mean_q: 1.309206
 20102/100000: episode: 412, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 7.792, mean reward: 0.866 [0.809, 0.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.607], loss: 0.002811, mae: 0.052371, mean_q: 1.293932
 20109/100000: episode: 413, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 6.251, mean reward: 0.893 [0.810, 0.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.698], loss: 0.003316, mae: 0.060034, mean_q: 1.261293
 20115/100000: episode: 414, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 5.188, mean reward: 0.865 [0.826, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.660], loss: 0.002235, mae: 0.051164, mean_q: 1.291605
 20123/100000: episode: 415, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 7.026, mean reward: 0.878 [0.807, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.709], loss: 0.003220, mae: 0.058364, mean_q: 1.308141
 20127/100000: episode: 416, duration: 0.029s, episode steps: 4, steps per second: 140, episode reward: 3.491, mean reward: 0.873 [0.826, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.711], loss: 0.003414, mae: 0.061567, mean_q: 1.280957
 20134/100000: episode: 417, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 5.552, mean reward: 0.793 [0.756, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.204, 10.525], loss: 0.004517, mae: 0.066677, mean_q: 1.271254
 20142/100000: episode: 418, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 7.096, mean reward: 0.887 [0.862, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.596], loss: 0.002745, mae: 0.057089, mean_q: 1.277683
 20152/100000: episode: 419, duration: 0.070s, episode steps: 10, steps per second: 142, episode reward: 8.171, mean reward: 0.817 [0.743, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.594], loss: 0.002602, mae: 0.054581, mean_q: 1.279415
[Info] FALSIFICATION!
[Info] Complete ISplit Iteration
[Info] Levels: [1.2120272, 1.4959456, 1.5891838, 1.6635126]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.93]
[Info] Error Prob: 0.0009300000000000003

 20158/100000: episode: 420, duration: 4.597s, episode steps: 6, steps per second: 1, episode reward: 5.500, mean reward: 0.917 [0.887, 1.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.015, 10.801], loss: 0.004488, mae: 0.063737, mean_q: 1.306673
 20258/100000: episode: 421, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.492, mean reward: 0.595 [0.507, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.660, 10.390], loss: 0.003049, mae: 0.056488, mean_q: 1.299585
 20358/100000: episode: 422, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.407, mean reward: 0.584 [0.506, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.886, 10.098], loss: 0.003379, mae: 0.060153, mean_q: 1.284593
 20458/100000: episode: 423, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.785, mean reward: 0.588 [0.501, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.355, 10.168], loss: 0.002929, mae: 0.057011, mean_q: 1.303294
 20558/100000: episode: 424, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.717, mean reward: 0.587 [0.500, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.829, 10.098], loss: 0.002848, mae: 0.054851, mean_q: 1.293476
 20658/100000: episode: 425, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.397, mean reward: 0.594 [0.505, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.564, 10.098], loss: 0.002694, mae: 0.055054, mean_q: 1.291209
 20758/100000: episode: 426, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 58.983, mean reward: 0.590 [0.508, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.912, 10.333], loss: 0.003277, mae: 0.059402, mean_q: 1.290457
 20858/100000: episode: 427, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 60.801, mean reward: 0.608 [0.501, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.617, 10.134], loss: 0.002875, mae: 0.056125, mean_q: 1.291667
 20958/100000: episode: 428, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.020, mean reward: 0.600 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.531, 10.279], loss: 0.002777, mae: 0.054043, mean_q: 1.296945
 21058/100000: episode: 429, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.680, mean reward: 0.577 [0.510, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.456, 10.098], loss: 0.002846, mae: 0.055226, mean_q: 1.290159
 21158/100000: episode: 430, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.633, mean reward: 0.586 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.429, 10.286], loss: 0.002667, mae: 0.054428, mean_q: 1.286774
 21258/100000: episode: 431, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.870, mean reward: 0.589 [0.513, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.602, 10.263], loss: 0.002612, mae: 0.053444, mean_q: 1.283887
 21358/100000: episode: 432, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 61.309, mean reward: 0.613 [0.504, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.869, 10.098], loss: 0.003251, mae: 0.057484, mean_q: 1.284634
 21458/100000: episode: 433, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.864, mean reward: 0.599 [0.505, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.169, 10.159], loss: 0.002810, mae: 0.054099, mean_q: 1.288751
 21558/100000: episode: 434, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.936, mean reward: 0.589 [0.499, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.251, 10.345], loss: 0.002992, mae: 0.057953, mean_q: 1.287037
 21658/100000: episode: 435, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 55.959, mean reward: 0.560 [0.504, 0.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.821, 10.098], loss: 0.002784, mae: 0.054540, mean_q: 1.283777
 21758/100000: episode: 436, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.150, mean reward: 0.592 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.079, 10.293], loss: 0.002628, mae: 0.052980, mean_q: 1.282040
 21858/100000: episode: 437, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 57.967, mean reward: 0.580 [0.504, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.136, 10.287], loss: 0.002801, mae: 0.054664, mean_q: 1.283793
 21958/100000: episode: 438, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.029, mean reward: 0.590 [0.514, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.251, 10.098], loss: 0.002804, mae: 0.055297, mean_q: 1.274750
 22058/100000: episode: 439, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.616, mean reward: 0.586 [0.507, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.485, 10.098], loss: 0.002753, mae: 0.053950, mean_q: 1.278483
 22158/100000: episode: 440, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 60.119, mean reward: 0.601 [0.505, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.034, 10.098], loss: 0.003060, mae: 0.055533, mean_q: 1.278704
 22258/100000: episode: 441, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.600, mean reward: 0.586 [0.505, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.049, 10.098], loss: 0.002511, mae: 0.051784, mean_q: 1.277666
 22358/100000: episode: 442, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.603, mean reward: 0.606 [0.513, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.684, 10.102], loss: 0.002925, mae: 0.055237, mean_q: 1.283823
 22458/100000: episode: 443, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 57.759, mean reward: 0.578 [0.514, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.226, 10.156], loss: 0.002553, mae: 0.052621, mean_q: 1.279605
 22558/100000: episode: 444, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 60.313, mean reward: 0.603 [0.504, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.982, 10.224], loss: 0.002768, mae: 0.053454, mean_q: 1.274888
 22658/100000: episode: 445, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.482, mean reward: 0.585 [0.500, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.948, 10.203], loss: 0.002372, mae: 0.052255, mean_q: 1.267378
 22758/100000: episode: 446, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.391, mean reward: 0.594 [0.507, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.952, 10.513], loss: 0.002785, mae: 0.052843, mean_q: 1.271439
 22858/100000: episode: 447, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 62.554, mean reward: 0.626 [0.509, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.512, 10.098], loss: 0.002518, mae: 0.052492, mean_q: 1.279423
 22958/100000: episode: 448, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.611, mean reward: 0.586 [0.502, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.507, 10.098], loss: 0.003024, mae: 0.056752, mean_q: 1.282722
 23058/100000: episode: 449, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.040, mean reward: 0.580 [0.513, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.365, 10.215], loss: 0.002864, mae: 0.054289, mean_q: 1.267765
 23158/100000: episode: 450, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.519, mean reward: 0.575 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.520, 10.212], loss: 0.002477, mae: 0.051346, mean_q: 1.264368
 23258/100000: episode: 451, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.916, mean reward: 0.599 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.291, 10.306], loss: 0.002587, mae: 0.052759, mean_q: 1.268231
 23358/100000: episode: 452, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.228, mean reward: 0.592 [0.509, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.720, 10.169], loss: 0.002951, mae: 0.054120, mean_q: 1.251745
 23458/100000: episode: 453, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 57.257, mean reward: 0.573 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.920, 10.104], loss: 0.002527, mae: 0.051502, mean_q: 1.256519
 23558/100000: episode: 454, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.227, mean reward: 0.582 [0.503, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.629, 10.193], loss: 0.002629, mae: 0.053061, mean_q: 1.249615
 23658/100000: episode: 455, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 61.485, mean reward: 0.615 [0.506, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.645, 10.098], loss: 0.002645, mae: 0.052850, mean_q: 1.245203
 23758/100000: episode: 456, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.943, mean reward: 0.589 [0.500, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.168, 10.098], loss: 0.002478, mae: 0.052419, mean_q: 1.243398
 23858/100000: episode: 457, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.344, mean reward: 0.583 [0.504, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.863, 10.098], loss: 0.002499, mae: 0.051447, mean_q: 1.237733
 23958/100000: episode: 458, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.272, mean reward: 0.583 [0.512, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.136, 10.098], loss: 0.002415, mae: 0.051385, mean_q: 1.236367
 24058/100000: episode: 459, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.693, mean reward: 0.577 [0.500, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.449, 10.098], loss: 0.002200, mae: 0.049909, mean_q: 1.231951
 24158/100000: episode: 460, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 61.258, mean reward: 0.613 [0.511, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.746, 10.098], loss: 0.002397, mae: 0.049585, mean_q: 1.222358
 24258/100000: episode: 461, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 59.509, mean reward: 0.595 [0.509, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.637, 10.098], loss: 0.002163, mae: 0.049127, mean_q: 1.224204
 24358/100000: episode: 462, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.486, mean reward: 0.575 [0.502, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.924, 10.098], loss: 0.002162, mae: 0.048758, mean_q: 1.210528
 24458/100000: episode: 463, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.710, mean reward: 0.577 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.782, 10.141], loss: 0.002108, mae: 0.048537, mean_q: 1.207514
 24558/100000: episode: 464, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.101, mean reward: 0.591 [0.515, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.376, 10.098], loss: 0.002143, mae: 0.048253, mean_q: 1.207483
 24658/100000: episode: 465, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.440, mean reward: 0.594 [0.505, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.202, 10.098], loss: 0.001853, mae: 0.046431, mean_q: 1.203996
 24758/100000: episode: 466, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 56.982, mean reward: 0.570 [0.508, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.397, 10.098], loss: 0.001751, mae: 0.044856, mean_q: 1.190578
 24858/100000: episode: 467, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 56.072, mean reward: 0.561 [0.504, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.175, 10.181], loss: 0.001678, mae: 0.044747, mean_q: 1.183285
 24958/100000: episode: 468, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.460, mean reward: 0.575 [0.503, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.326, 10.122], loss: 0.001922, mae: 0.046980, mean_q: 1.175182
 25058/100000: episode: 469, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 56.591, mean reward: 0.566 [0.503, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.216, 10.098], loss: 0.001696, mae: 0.044567, mean_q: 1.169458
 25158/100000: episode: 470, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 59.955, mean reward: 0.600 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.369, 10.166], loss: 0.001486, mae: 0.042702, mean_q: 1.162257
 25258/100000: episode: 471, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.182, mean reward: 0.582 [0.505, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.572, 10.098], loss: 0.001432, mae: 0.041608, mean_q: 1.160991
 25358/100000: episode: 472, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 57.799, mean reward: 0.578 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.098], loss: 0.001548, mae: 0.043024, mean_q: 1.159656
 25458/100000: episode: 473, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.145, mean reward: 0.591 [0.498, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.093, 10.100], loss: 0.001405, mae: 0.041506, mean_q: 1.163232
 25558/100000: episode: 474, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.330, mean reward: 0.583 [0.502, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.136, 10.098], loss: 0.001506, mae: 0.042739, mean_q: 1.165028
 25658/100000: episode: 475, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 63.547, mean reward: 0.635 [0.519, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.306, 10.098], loss: 0.001489, mae: 0.042621, mean_q: 1.163163
 25758/100000: episode: 476, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.601, mean reward: 0.586 [0.517, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.059, 10.098], loss: 0.001433, mae: 0.041715, mean_q: 1.163726
 25858/100000: episode: 477, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 60.665, mean reward: 0.607 [0.518, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.746, 10.275], loss: 0.001510, mae: 0.043186, mean_q: 1.164726
 25958/100000: episode: 478, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.092, mean reward: 0.581 [0.508, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.408, 10.357], loss: 0.001418, mae: 0.041507, mean_q: 1.162163
 26058/100000: episode: 479, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.230, mean reward: 0.592 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.469, 10.098], loss: 0.001389, mae: 0.041035, mean_q: 1.162552
 26158/100000: episode: 480, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.127, mean reward: 0.571 [0.503, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.308, 10.168], loss: 0.001483, mae: 0.042511, mean_q: 1.165682
 26258/100000: episode: 481, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.422, mean reward: 0.584 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.446, 10.205], loss: 0.001550, mae: 0.043028, mean_q: 1.163960
 26358/100000: episode: 482, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 59.199, mean reward: 0.592 [0.509, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.368, 10.098], loss: 0.001360, mae: 0.040929, mean_q: 1.164837
 26458/100000: episode: 483, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.797, mean reward: 0.598 [0.505, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.201, 10.098], loss: 0.001348, mae: 0.040395, mean_q: 1.161739
 26558/100000: episode: 484, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.212, mean reward: 0.602 [0.504, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.309], loss: 0.001378, mae: 0.040899, mean_q: 1.160967
 26658/100000: episode: 485, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 61.962, mean reward: 0.620 [0.505, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.076, 10.098], loss: 0.001427, mae: 0.041023, mean_q: 1.164845
 26758/100000: episode: 486, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.256, mean reward: 0.583 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.981, 10.098], loss: 0.001452, mae: 0.041806, mean_q: 1.165039
 26858/100000: episode: 487, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.608, mean reward: 0.586 [0.498, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.427, 10.319], loss: 0.001375, mae: 0.041325, mean_q: 1.166112
 26958/100000: episode: 488, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.744, mean reward: 0.567 [0.500, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.846, 10.158], loss: 0.001367, mae: 0.040433, mean_q: 1.161513
 27058/100000: episode: 489, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 60.156, mean reward: 0.602 [0.503, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.064, 10.098], loss: 0.001461, mae: 0.041844, mean_q: 1.163118
 27158/100000: episode: 490, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.375, mean reward: 0.604 [0.504, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.016, 10.098], loss: 0.001619, mae: 0.043784, mean_q: 1.164672
 27258/100000: episode: 491, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 57.470, mean reward: 0.575 [0.504, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.864, 10.098], loss: 0.001502, mae: 0.043006, mean_q: 1.163628
 27358/100000: episode: 492, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 62.377, mean reward: 0.624 [0.497, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.702, 10.406], loss: 0.001526, mae: 0.042792, mean_q: 1.164432
 27458/100000: episode: 493, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 66.123, mean reward: 0.661 [0.509, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.810, 10.405], loss: 0.001544, mae: 0.042921, mean_q: 1.167110
 27558/100000: episode: 494, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.914, mean reward: 0.579 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.976, 10.153], loss: 0.001528, mae: 0.042767, mean_q: 1.170562
 27658/100000: episode: 495, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.447, mean reward: 0.594 [0.511, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.588, 10.332], loss: 0.001600, mae: 0.043622, mean_q: 1.168773
 27758/100000: episode: 496, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 56.939, mean reward: 0.569 [0.510, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.945, 10.211], loss: 0.001735, mae: 0.045209, mean_q: 1.165228
 27858/100000: episode: 497, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 56.931, mean reward: 0.569 [0.504, 0.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.675, 10.147], loss: 0.001705, mae: 0.045634, mean_q: 1.167591
 27958/100000: episode: 498, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.606, mean reward: 0.576 [0.502, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.478, 10.098], loss: 0.001520, mae: 0.042538, mean_q: 1.166785
 28058/100000: episode: 499, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 56.792, mean reward: 0.568 [0.509, 0.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.440, 10.098], loss: 0.001627, mae: 0.043679, mean_q: 1.164965
 28158/100000: episode: 500, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.085, mean reward: 0.591 [0.512, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.525, 10.199], loss: 0.001612, mae: 0.043096, mean_q: 1.165207
 28258/100000: episode: 501, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 57.566, mean reward: 0.576 [0.501, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.748, 10.261], loss: 0.001582, mae: 0.043669, mean_q: 1.164275
 28358/100000: episode: 502, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 59.433, mean reward: 0.594 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.195, 10.098], loss: 0.001476, mae: 0.042136, mean_q: 1.163600
 28458/100000: episode: 503, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.873, mean reward: 0.589 [0.501, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.327, 10.098], loss: 0.001426, mae: 0.040777, mean_q: 1.162848
 28558/100000: episode: 504, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 60.477, mean reward: 0.605 [0.498, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.238, 10.098], loss: 0.001520, mae: 0.042573, mean_q: 1.165399
 28658/100000: episode: 505, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.873, mean reward: 0.589 [0.506, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.877, 10.209], loss: 0.001358, mae: 0.040474, mean_q: 1.164569
 28758/100000: episode: 506, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.505, mean reward: 0.595 [0.508, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.974, 10.098], loss: 0.001523, mae: 0.042543, mean_q: 1.164920
 28858/100000: episode: 507, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.344, mean reward: 0.583 [0.505, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.136, 10.173], loss: 0.001559, mae: 0.042923, mean_q: 1.169602
 28958/100000: episode: 508, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.620, mean reward: 0.586 [0.504, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.562, 10.212], loss: 0.001516, mae: 0.042924, mean_q: 1.163304
 29058/100000: episode: 509, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.688, mean reward: 0.607 [0.508, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.594, 10.098], loss: 0.001448, mae: 0.041667, mean_q: 1.164194
 29158/100000: episode: 510, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.371, mean reward: 0.574 [0.499, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.535, 10.189], loss: 0.001603, mae: 0.043633, mean_q: 1.162546
 29258/100000: episode: 511, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.874, mean reward: 0.589 [0.498, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.328, 10.211], loss: 0.001452, mae: 0.041382, mean_q: 1.163983
 29358/100000: episode: 512, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 64.030, mean reward: 0.640 [0.506, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.870, 10.098], loss: 0.001335, mae: 0.040360, mean_q: 1.166082
 29458/100000: episode: 513, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 62.048, mean reward: 0.620 [0.497, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.889, 10.098], loss: 0.001582, mae: 0.042702, mean_q: 1.167791
 29558/100000: episode: 514, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.423, mean reward: 0.574 [0.503, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.036, 10.106], loss: 0.001606, mae: 0.044386, mean_q: 1.170896
 29658/100000: episode: 515, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 60.338, mean reward: 0.603 [0.501, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.417, 10.453], loss: 0.001522, mae: 0.042277, mean_q: 1.165718
 29758/100000: episode: 516, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.771, mean reward: 0.598 [0.506, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.663, 10.124], loss: 0.001665, mae: 0.044747, mean_q: 1.169527
 29858/100000: episode: 517, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.741, mean reward: 0.597 [0.507, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.604, 10.098], loss: 0.001529, mae: 0.042775, mean_q: 1.169473
 29958/100000: episode: 518, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.037, mean reward: 0.580 [0.504, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.759, 10.098], loss: 0.001657, mae: 0.044176, mean_q: 1.168084
 30058/100000: episode: 519, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 62.147, mean reward: 0.621 [0.511, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.112, 10.364], loss: 0.001607, mae: 0.043614, mean_q: 1.170086
[Info] 1-TH LEVEL FOUND: 1.3410192728042603, Considering 10/90 traces
 30158/100000: episode: 520, duration: 4.806s, episode steps: 100, steps per second: 21, episode reward: 58.609, mean reward: 0.586 [0.498, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.815, 10.346], loss: 0.001498, mae: 0.041975, mean_q: 1.174899
 30171/100000: episode: 521, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 9.660, mean reward: 0.743 [0.657, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.526], loss: 0.001509, mae: 0.042507, mean_q: 1.177761
 30192/100000: episode: 522, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 16.632, mean reward: 0.792 [0.705, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.403, 10.100], loss: 0.001670, mae: 0.044089, mean_q: 1.176357
 30244/100000: episode: 523, duration: 0.279s, episode steps: 52, steps per second: 186, episode reward: 35.390, mean reward: 0.681 [0.526, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.131, 10.100], loss: 0.001516, mae: 0.042080, mean_q: 1.177450
 30255/100000: episode: 524, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 7.705, mean reward: 0.700 [0.621, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.567], loss: 0.001640, mae: 0.043476, mean_q: 1.179172
 30266/100000: episode: 525, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 7.083, mean reward: 0.644 [0.598, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.342], loss: 0.001363, mae: 0.039851, mean_q: 1.180017
 30277/100000: episode: 526, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 7.892, mean reward: 0.717 [0.701, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.463], loss: 0.001438, mae: 0.042099, mean_q: 1.173604
 30297/100000: episode: 527, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 14.478, mean reward: 0.724 [0.623, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.340, 10.100], loss: 0.001556, mae: 0.043178, mean_q: 1.179236
 30319/100000: episode: 528, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 15.548, mean reward: 0.707 [0.562, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.663, 10.100], loss: 0.001546, mae: 0.043047, mean_q: 1.182227
 30339/100000: episode: 529, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 13.927, mean reward: 0.696 [0.640, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.119, 10.100], loss: 0.001537, mae: 0.041667, mean_q: 1.170621
 30361/100000: episode: 530, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 15.613, mean reward: 0.710 [0.647, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.574, 10.100], loss: 0.001719, mae: 0.043919, mean_q: 1.180248
 30388/100000: episode: 531, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 18.838, mean reward: 0.698 [0.629, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.856, 10.405], loss: 0.001821, mae: 0.045853, mean_q: 1.178222
 30410/100000: episode: 532, duration: 0.130s, episode steps: 22, steps per second: 169, episode reward: 14.270, mean reward: 0.649 [0.556, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.632, 10.100], loss: 0.001673, mae: 0.044048, mean_q: 1.179692
 30447/100000: episode: 533, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 27.404, mean reward: 0.741 [0.600, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.035, 10.374], loss: 0.001745, mae: 0.045090, mean_q: 1.182042
 30468/100000: episode: 534, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 16.048, mean reward: 0.764 [0.696, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.521, 10.100], loss: 0.001887, mae: 0.045135, mean_q: 1.192620
 30505/100000: episode: 535, duration: 0.198s, episode steps: 37, steps per second: 186, episode reward: 26.665, mean reward: 0.721 [0.607, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.598, 10.296], loss: 0.001794, mae: 0.045175, mean_q: 1.185605
 30527/100000: episode: 536, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 14.385, mean reward: 0.654 [0.588, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.498, 10.100], loss: 0.001558, mae: 0.043690, mean_q: 1.187838
 30549/100000: episode: 537, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 14.467, mean reward: 0.658 [0.552, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.148, 10.100], loss: 0.001492, mae: 0.042396, mean_q: 1.197920
 30576/100000: episode: 538, duration: 0.164s, episode steps: 27, steps per second: 165, episode reward: 17.530, mean reward: 0.649 [0.573, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.964, 10.197], loss: 0.002195, mae: 0.048570, mean_q: 1.182918
 30597/100000: episode: 539, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 15.787, mean reward: 0.752 [0.604, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.193, 10.100], loss: 0.001955, mae: 0.046495, mean_q: 1.184715
 30608/100000: episode: 540, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 7.294, mean reward: 0.663 [0.618, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.483], loss: 0.001859, mae: 0.046518, mean_q: 1.192813
 30629/100000: episode: 541, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 14.599, mean reward: 0.695 [0.627, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.252, 10.100], loss: 0.002653, mae: 0.052065, mean_q: 1.189901
 30642/100000: episode: 542, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 9.234, mean reward: 0.710 [0.612, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.397, 10.473], loss: 0.002082, mae: 0.048311, mean_q: 1.184356
 30655/100000: episode: 543, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 9.344, mean reward: 0.719 [0.662, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.524], loss: 0.001490, mae: 0.042619, mean_q: 1.180101
 30692/100000: episode: 544, duration: 0.211s, episode steps: 37, steps per second: 176, episode reward: 24.173, mean reward: 0.653 [0.536, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.048, 10.266], loss: 0.001978, mae: 0.047595, mean_q: 1.186568
 30703/100000: episode: 545, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 7.011, mean reward: 0.637 [0.598, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.645, 10.331], loss: 0.002026, mae: 0.048867, mean_q: 1.192603
 30714/100000: episode: 546, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 7.721, mean reward: 0.702 [0.654, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.484], loss: 0.001697, mae: 0.045007, mean_q: 1.178332
 30741/100000: episode: 547, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 18.242, mean reward: 0.676 [0.554, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.583, 10.309], loss: 0.001744, mae: 0.044665, mean_q: 1.190998
 30768/100000: episode: 548, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 18.969, mean reward: 0.703 [0.648, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.334], loss: 0.001875, mae: 0.046497, mean_q: 1.190707
 30779/100000: episode: 549, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 7.851, mean reward: 0.714 [0.678, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.475, 10.447], loss: 0.001814, mae: 0.044945, mean_q: 1.190022
 30787/100000: episode: 550, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 5.958, mean reward: 0.745 [0.723, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.405, 10.100], loss: 0.002043, mae: 0.044745, mean_q: 1.195557
 30824/100000: episode: 551, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 26.692, mean reward: 0.721 [0.620, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.352, 10.430], loss: 0.001852, mae: 0.046482, mean_q: 1.199886
 30835/100000: episode: 552, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 7.531, mean reward: 0.685 [0.622, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.424], loss: 0.002084, mae: 0.048360, mean_q: 1.183692
 30848/100000: episode: 553, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 8.728, mean reward: 0.671 [0.625, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.222, 10.351], loss: 0.001790, mae: 0.046134, mean_q: 1.194604
 30859/100000: episode: 554, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 7.349, mean reward: 0.668 [0.597, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.245, 10.341], loss: 0.001456, mae: 0.042387, mean_q: 1.203935
 30896/100000: episode: 555, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 26.559, mean reward: 0.718 [0.588, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.931, 10.343], loss: 0.001882, mae: 0.046824, mean_q: 1.201421
 30948/100000: episode: 556, duration: 0.273s, episode steps: 52, steps per second: 190, episode reward: 31.181, mean reward: 0.600 [0.502, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.114, 10.100], loss: 0.002120, mae: 0.049442, mean_q: 1.194756
 30975/100000: episode: 557, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 18.823, mean reward: 0.697 [0.618, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.279, 10.458], loss: 0.001977, mae: 0.047090, mean_q: 1.191889
 31002/100000: episode: 558, duration: 0.153s, episode steps: 27, steps per second: 177, episode reward: 19.496, mean reward: 0.722 [0.654, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.270, 10.465], loss: 0.002214, mae: 0.048389, mean_q: 1.201301
 31013/100000: episode: 559, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 6.928, mean reward: 0.630 [0.578, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.621, 10.354], loss: 0.002131, mae: 0.047472, mean_q: 1.190630
 31024/100000: episode: 560, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 6.654, mean reward: 0.605 [0.566, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.307], loss: 0.002327, mae: 0.053964, mean_q: 1.204534
 31045/100000: episode: 561, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 15.522, mean reward: 0.739 [0.679, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.376, 10.100], loss: 0.002375, mae: 0.051110, mean_q: 1.204533
 31067/100000: episode: 562, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 14.517, mean reward: 0.660 [0.605, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.335, 10.100], loss: 0.002104, mae: 0.049332, mean_q: 1.204503
 31119/100000: episode: 563, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 33.860, mean reward: 0.651 [0.513, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.781, 10.100], loss: 0.001956, mae: 0.045834, mean_q: 1.198429
 31139/100000: episode: 564, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 13.739, mean reward: 0.687 [0.640, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.349, 10.100], loss: 0.002195, mae: 0.049228, mean_q: 1.204803
 31150/100000: episode: 565, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 7.754, mean reward: 0.705 [0.670, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.530, 10.390], loss: 0.001845, mae: 0.046668, mean_q: 1.192562
 31170/100000: episode: 566, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 14.612, mean reward: 0.731 [0.630, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.601, 10.100], loss: 0.002172, mae: 0.048940, mean_q: 1.194318
 31181/100000: episode: 567, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 7.410, mean reward: 0.674 [0.655, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.405], loss: 0.002778, mae: 0.053074, mean_q: 1.189765
 31194/100000: episode: 568, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 9.616, mean reward: 0.740 [0.686, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-1.504, 10.455], loss: 0.002253, mae: 0.048809, mean_q: 1.197042
 31205/100000: episode: 569, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 7.951, mean reward: 0.723 [0.663, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.605], loss: 0.001925, mae: 0.047038, mean_q: 1.196560
 31257/100000: episode: 570, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 33.428, mean reward: 0.643 [0.557, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.542, 10.100], loss: 0.001794, mae: 0.044941, mean_q: 1.204649
 31270/100000: episode: 571, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 9.308, mean reward: 0.716 [0.665, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.440], loss: 0.002228, mae: 0.051383, mean_q: 1.208007
 31292/100000: episode: 572, duration: 0.142s, episode steps: 22, steps per second: 155, episode reward: 16.080, mean reward: 0.731 [0.610, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.503, 10.100], loss: 0.002021, mae: 0.048338, mean_q: 1.210587
 31303/100000: episode: 573, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 7.038, mean reward: 0.640 [0.551, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-1.069, 10.195], loss: 0.001964, mae: 0.047840, mean_q: 1.206297
 31314/100000: episode: 574, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 6.447, mean reward: 0.586 [0.531, 0.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.180], loss: 0.001837, mae: 0.046464, mean_q: 1.207232
 31325/100000: episode: 575, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 7.476, mean reward: 0.680 [0.636, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.443], loss: 0.001870, mae: 0.045370, mean_q: 1.196690
 31352/100000: episode: 576, duration: 0.156s, episode steps: 27, steps per second: 174, episode reward: 18.226, mean reward: 0.675 [0.561, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.096, 10.305], loss: 0.001681, mae: 0.044761, mean_q: 1.215180
 31365/100000: episode: 577, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 9.742, mean reward: 0.749 [0.687, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.446], loss: 0.001672, mae: 0.044018, mean_q: 1.208568
 31376/100000: episode: 578, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 7.287, mean reward: 0.662 [0.598, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.501, 10.273], loss: 0.001838, mae: 0.043726, mean_q: 1.194589
 31384/100000: episode: 579, duration: 0.043s, episode steps: 8, steps per second: 186, episode reward: 6.258, mean reward: 0.782 [0.707, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.417, 10.100], loss: 0.002297, mae: 0.047268, mean_q: 1.207886
 31404/100000: episode: 580, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 13.445, mean reward: 0.672 [0.631, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.174, 10.100], loss: 0.002120, mae: 0.050666, mean_q: 1.212583
 31424/100000: episode: 581, duration: 0.104s, episode steps: 20, steps per second: 191, episode reward: 13.640, mean reward: 0.682 [0.630, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.471, 10.100], loss: 0.002218, mae: 0.050265, mean_q: 1.201743
 31476/100000: episode: 582, duration: 0.271s, episode steps: 52, steps per second: 192, episode reward: 34.323, mean reward: 0.660 [0.502, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.704, 10.127], loss: 0.001983, mae: 0.047036, mean_q: 1.215883
 31487/100000: episode: 583, duration: 0.076s, episode steps: 11, steps per second: 144, episode reward: 7.351, mean reward: 0.668 [0.624, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.275, 10.350], loss: 0.001910, mae: 0.047585, mean_q: 1.218714
 31539/100000: episode: 584, duration: 0.274s, episode steps: 52, steps per second: 189, episode reward: 36.853, mean reward: 0.709 [0.601, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.448, 10.100], loss: 0.001863, mae: 0.046666, mean_q: 1.212927
 31591/100000: episode: 585, duration: 0.291s, episode steps: 52, steps per second: 179, episode reward: 32.871, mean reward: 0.632 [0.519, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.041, 10.104], loss: 0.002248, mae: 0.050557, mean_q: 1.211166
 31604/100000: episode: 586, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 9.083, mean reward: 0.699 [0.634, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.140, 10.546], loss: 0.001995, mae: 0.047739, mean_q: 1.203159
 31656/100000: episode: 587, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 37.141, mean reward: 0.714 [0.612, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.679, 10.100], loss: 0.001831, mae: 0.046647, mean_q: 1.214339
 31676/100000: episode: 588, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 13.492, mean reward: 0.675 [0.614, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.478, 10.100], loss: 0.001867, mae: 0.046466, mean_q: 1.209983
 31684/100000: episode: 589, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 5.800, mean reward: 0.725 [0.694, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.418, 10.100], loss: 0.001918, mae: 0.046179, mean_q: 1.201630
 31692/100000: episode: 590, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 6.218, mean reward: 0.777 [0.718, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.618, 10.100], loss: 0.001918, mae: 0.046222, mean_q: 1.219080
 31713/100000: episode: 591, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 14.454, mean reward: 0.688 [0.586, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.165, 10.100], loss: 0.001748, mae: 0.044890, mean_q: 1.205461
 31726/100000: episode: 592, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 8.231, mean reward: 0.633 [0.572, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.098, 10.308], loss: 0.001748, mae: 0.046044, mean_q: 1.220264
 31737/100000: episode: 593, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 7.586, mean reward: 0.690 [0.642, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.439], loss: 0.001713, mae: 0.044964, mean_q: 1.210191
 31759/100000: episode: 594, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 16.951, mean reward: 0.771 [0.639, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.838, 10.100], loss: 0.002151, mae: 0.049089, mean_q: 1.220218
 31786/100000: episode: 595, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 19.955, mean reward: 0.739 [0.637, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.317, 10.392], loss: 0.002081, mae: 0.048289, mean_q: 1.226607
 31808/100000: episode: 596, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 15.338, mean reward: 0.697 [0.630, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.127, 10.100], loss: 0.002000, mae: 0.048406, mean_q: 1.228357
 31835/100000: episode: 597, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 19.319, mean reward: 0.716 [0.591, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.533, 10.296], loss: 0.002162, mae: 0.051033, mean_q: 1.229730
 31848/100000: episode: 598, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 7.627, mean reward: 0.587 [0.510, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.469, 10.152], loss: 0.001754, mae: 0.043149, mean_q: 1.207849
 31900/100000: episode: 599, duration: 0.295s, episode steps: 52, steps per second: 176, episode reward: 31.583, mean reward: 0.607 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.120, 10.140], loss: 0.002564, mae: 0.052828, mean_q: 1.216869
 31920/100000: episode: 600, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 13.980, mean reward: 0.699 [0.570, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.517, 10.100], loss: 0.001693, mae: 0.044091, mean_q: 1.207944
 31931/100000: episode: 601, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 8.015, mean reward: 0.729 [0.633, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.100, 10.462], loss: 0.002337, mae: 0.051850, mean_q: 1.225687
 31953/100000: episode: 602, duration: 0.136s, episode steps: 22, steps per second: 162, episode reward: 14.158, mean reward: 0.644 [0.550, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.183, 10.100], loss: 0.002194, mae: 0.050172, mean_q: 1.233579
 31961/100000: episode: 603, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 6.359, mean reward: 0.795 [0.689, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.502, 10.100], loss: 0.002027, mae: 0.049098, mean_q: 1.213413
 31988/100000: episode: 604, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 18.113, mean reward: 0.671 [0.554, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.264, 10.240], loss: 0.001873, mae: 0.046961, mean_q: 1.226626
 31999/100000: episode: 605, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 7.404, mean reward: 0.673 [0.607, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.169, 10.379], loss: 0.001883, mae: 0.044506, mean_q: 1.203753
 32012/100000: episode: 606, duration: 0.089s, episode steps: 13, steps per second: 145, episode reward: 9.520, mean reward: 0.732 [0.674, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.422], loss: 0.001850, mae: 0.046857, mean_q: 1.239136
 32064/100000: episode: 607, duration: 0.291s, episode steps: 52, steps per second: 178, episode reward: 31.005, mean reward: 0.596 [0.504, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.336, 10.100], loss: 0.001857, mae: 0.045503, mean_q: 1.220916
 32077/100000: episode: 608, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 9.409, mean reward: 0.724 [0.692, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.539], loss: 0.001741, mae: 0.046149, mean_q: 1.227448
 32088/100000: episode: 609, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 7.518, mean reward: 0.683 [0.633, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.128, 10.374], loss: 0.001847, mae: 0.046119, mean_q: 1.221187
[Info] 2-TH LEVEL FOUND: 1.530808925628662, Considering 10/90 traces
 32099/100000: episode: 610, duration: 4.340s, episode steps: 11, steps per second: 3, episode reward: 7.133, mean reward: 0.648 [0.574, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.306], loss: 0.002255, mae: 0.050404, mean_q: 1.223414
 32129/100000: episode: 611, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 22.563, mean reward: 0.752 [0.669, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.237, 10.593], loss: 0.001894, mae: 0.046653, mean_q: 1.226686
 32134/100000: episode: 612, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 4.198, mean reward: 0.840 [0.794, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.990, 10.527], loss: 0.002223, mae: 0.049098, mean_q: 1.205123
 32143/100000: episode: 613, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 6.830, mean reward: 0.759 [0.658, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.477, 10.420], loss: 0.002081, mae: 0.048273, mean_q: 1.232146
 32157/100000: episode: 614, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 11.409, mean reward: 0.815 [0.768, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.438, 10.100], loss: 0.001987, mae: 0.049419, mean_q: 1.241861
 32187/100000: episode: 615, duration: 0.187s, episode steps: 30, steps per second: 160, episode reward: 24.227, mean reward: 0.808 [0.722, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.035, 10.546], loss: 0.001684, mae: 0.044963, mean_q: 1.230366
 32204/100000: episode: 616, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 11.245, mean reward: 0.661 [0.584, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.314, 10.100], loss: 0.001645, mae: 0.044239, mean_q: 1.237375
 32218/100000: episode: 617, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 10.668, mean reward: 0.762 [0.710, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.400, 10.100], loss: 0.001795, mae: 0.044299, mean_q: 1.240618
 32232/100000: episode: 618, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 10.806, mean reward: 0.772 [0.680, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.201, 10.100], loss: 0.002041, mae: 0.047768, mean_q: 1.234861
 32257/100000: episode: 619, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 18.421, mean reward: 0.737 [0.676, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.599, 10.472], loss: 0.001900, mae: 0.045160, mean_q: 1.234511
 32274/100000: episode: 620, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 12.202, mean reward: 0.718 [0.667, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.724, 10.100], loss: 0.002084, mae: 0.049668, mean_q: 1.223714
 32281/100000: episode: 621, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 5.035, mean reward: 0.719 [0.679, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.394], loss: 0.001808, mae: 0.043794, mean_q: 1.242419
 32290/100000: episode: 622, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 6.246, mean reward: 0.694 [0.654, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.318], loss: 0.002153, mae: 0.050211, mean_q: 1.252013
 32297/100000: episode: 623, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.661, mean reward: 0.809 [0.741, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.633], loss: 0.003117, mae: 0.058378, mean_q: 1.224439
 32311/100000: episode: 624, duration: 0.086s, episode steps: 14, steps per second: 164, episode reward: 10.446, mean reward: 0.746 [0.645, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.883, 10.100], loss: 0.002316, mae: 0.050455, mean_q: 1.231116
 32328/100000: episode: 625, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 13.077, mean reward: 0.769 [0.733, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.463, 10.100], loss: 0.002310, mae: 0.051149, mean_q: 1.235385
 32352/100000: episode: 626, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 16.831, mean reward: 0.701 [0.553, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.242], loss: 0.002262, mae: 0.049402, mean_q: 1.239786
 32359/100000: episode: 627, duration: 0.054s, episode steps: 7, steps per second: 131, episode reward: 5.554, mean reward: 0.793 [0.737, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.632], loss: 0.002239, mae: 0.048745, mean_q: 1.229143
 32373/100000: episode: 628, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 10.253, mean reward: 0.732 [0.661, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.930, 10.100], loss: 0.001917, mae: 0.045789, mean_q: 1.234595
 32380/100000: episode: 629, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 5.664, mean reward: 0.809 [0.773, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.386, 10.589], loss: 0.002150, mae: 0.048983, mean_q: 1.235742
 32410/100000: episode: 630, duration: 0.167s, episode steps: 30, steps per second: 179, episode reward: 23.544, mean reward: 0.785 [0.707, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.497], loss: 0.001878, mae: 0.046537, mean_q: 1.233267
 32424/100000: episode: 631, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 11.300, mean reward: 0.807 [0.698, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.311, 10.100], loss: 0.002638, mae: 0.053631, mean_q: 1.238223
 32448/100000: episode: 632, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 17.893, mean reward: 0.746 [0.694, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.497], loss: 0.002334, mae: 0.051920, mean_q: 1.227564
 32472/100000: episode: 633, duration: 0.150s, episode steps: 24, steps per second: 160, episode reward: 17.742, mean reward: 0.739 [0.592, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.335, 10.355], loss: 0.002002, mae: 0.048049, mean_q: 1.244884
 32496/100000: episode: 634, duration: 0.145s, episode steps: 24, steps per second: 166, episode reward: 18.956, mean reward: 0.790 [0.701, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.646], loss: 0.001887, mae: 0.046492, mean_q: 1.246032
 32505/100000: episode: 635, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 5.881, mean reward: 0.653 [0.621, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.423], loss: 0.001947, mae: 0.047595, mean_q: 1.235257
 32514/100000: episode: 636, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 6.387, mean reward: 0.710 [0.686, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.462], loss: 0.002077, mae: 0.048742, mean_q: 1.229554
 32531/100000: episode: 637, duration: 0.109s, episode steps: 17, steps per second: 157, episode reward: 13.141, mean reward: 0.773 [0.703, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.918, 10.100], loss: 0.002161, mae: 0.049836, mean_q: 1.234009
 32548/100000: episode: 638, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 12.463, mean reward: 0.733 [0.641, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.943, 10.100], loss: 0.002084, mae: 0.047108, mean_q: 1.243452
 32555/100000: episode: 639, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 5.583, mean reward: 0.798 [0.770, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.622], loss: 0.002228, mae: 0.049002, mean_q: 1.233230
 32579/100000: episode: 640, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 16.371, mean reward: 0.682 [0.546, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.363, 10.237], loss: 0.001960, mae: 0.048372, mean_q: 1.250965
 32604/100000: episode: 641, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 17.306, mean reward: 0.692 [0.606, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.648, 10.296], loss: 0.002165, mae: 0.049172, mean_q: 1.253701
 32609/100000: episode: 642, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 3.935, mean reward: 0.787 [0.774, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.348 [-0.035, 10.578], loss: 0.002044, mae: 0.050941, mean_q: 1.253924
 32626/100000: episode: 643, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 12.844, mean reward: 0.756 [0.676, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.504, 10.100], loss: 0.002105, mae: 0.049185, mean_q: 1.252743
 32651/100000: episode: 644, duration: 0.149s, episode steps: 25, steps per second: 168, episode reward: 20.317, mean reward: 0.813 [0.745, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.345, 10.562], loss: 0.002131, mae: 0.048578, mean_q: 1.248580
 32675/100000: episode: 645, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 15.630, mean reward: 0.651 [0.506, 0.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.752, 10.210], loss: 0.002277, mae: 0.051094, mean_q: 1.256100
 32705/100000: episode: 646, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 21.119, mean reward: 0.704 [0.555, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.219, 10.223], loss: 0.001962, mae: 0.046989, mean_q: 1.253526
 32730/100000: episode: 647, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 19.504, mean reward: 0.780 [0.693, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.402, 10.480], loss: 0.002250, mae: 0.048708, mean_q: 1.262884
 32760/100000: episode: 648, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 23.441, mean reward: 0.781 [0.697, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.035, 10.531], loss: 0.002317, mae: 0.051386, mean_q: 1.255393
 32765/100000: episode: 649, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 3.754, mean reward: 0.751 [0.712, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.486], loss: 0.002171, mae: 0.051414, mean_q: 1.273623
 32789/100000: episode: 650, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 17.939, mean reward: 0.747 [0.674, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.226, 10.422], loss: 0.002506, mae: 0.052950, mean_q: 1.256325
 32803/100000: episode: 651, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 10.422, mean reward: 0.744 [0.638, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.279, 10.100], loss: 0.002687, mae: 0.052767, mean_q: 1.251490
 32812/100000: episode: 652, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 6.604, mean reward: 0.734 [0.691, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.524], loss: 0.001702, mae: 0.043440, mean_q: 1.260916
 32819/100000: episode: 653, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 5.272, mean reward: 0.753 [0.674, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.562], loss: 0.002096, mae: 0.051288, mean_q: 1.266340
 32824/100000: episode: 654, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 3.903, mean reward: 0.781 [0.750, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.557], loss: 0.002163, mae: 0.045991, mean_q: 1.253175
 32838/100000: episode: 655, duration: 0.084s, episode steps: 14, steps per second: 168, episode reward: 10.743, mean reward: 0.767 [0.697, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.363, 10.100], loss: 0.002149, mae: 0.048579, mean_q: 1.259885
 32862/100000: episode: 656, duration: 0.138s, episode steps: 24, steps per second: 175, episode reward: 16.729, mean reward: 0.697 [0.589, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.083, 10.318], loss: 0.002352, mae: 0.049450, mean_q: 1.264294
 32887/100000: episode: 657, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 18.951, mean reward: 0.758 [0.657, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.981, 10.421], loss: 0.001674, mae: 0.044689, mean_q: 1.273774
 32896/100000: episode: 658, duration: 0.067s, episode steps: 9, steps per second: 135, episode reward: 6.856, mean reward: 0.762 [0.668, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-1.473, 10.465], loss: 0.002031, mae: 0.048436, mean_q: 1.258444
 32905/100000: episode: 659, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 6.357, mean reward: 0.706 [0.642, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.502], loss: 0.002206, mae: 0.049828, mean_q: 1.270654
 32919/100000: episode: 660, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 10.611, mean reward: 0.758 [0.697, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.249, 10.100], loss: 0.001752, mae: 0.046520, mean_q: 1.271691
 32928/100000: episode: 661, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 6.444, mean reward: 0.716 [0.673, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.526], loss: 0.001614, mae: 0.044539, mean_q: 1.263266
 32958/100000: episode: 662, duration: 0.181s, episode steps: 30, steps per second: 165, episode reward: 20.614, mean reward: 0.687 [0.567, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.470, 10.238], loss: 0.001888, mae: 0.047179, mean_q: 1.273316
 32983/100000: episode: 663, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 18.163, mean reward: 0.727 [0.638, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.888, 10.535], loss: 0.002277, mae: 0.050886, mean_q: 1.272189
 32997/100000: episode: 664, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 11.234, mean reward: 0.802 [0.748, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.455, 10.100], loss: 0.001859, mae: 0.047047, mean_q: 1.261080
 33006/100000: episode: 665, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 6.374, mean reward: 0.708 [0.680, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.413], loss: 0.001952, mae: 0.047606, mean_q: 1.274530
 33011/100000: episode: 666, duration: 0.038s, episode steps: 5, steps per second: 133, episode reward: 3.852, mean reward: 0.770 [0.744, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.497], loss: 0.002195, mae: 0.050523, mean_q: 1.285255
 33016/100000: episode: 667, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 3.830, mean reward: 0.766 [0.718, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.488], loss: 0.002170, mae: 0.051526, mean_q: 1.270574
 33023/100000: episode: 668, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 5.171, mean reward: 0.739 [0.706, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.518], loss: 0.001903, mae: 0.047028, mean_q: 1.264294
 33037/100000: episode: 669, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 12.082, mean reward: 0.863 [0.750, 0.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.870, 10.100], loss: 0.002247, mae: 0.051591, mean_q: 1.265252
 33061/100000: episode: 670, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 17.894, mean reward: 0.746 [0.696, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.473, 10.444], loss: 0.002403, mae: 0.051192, mean_q: 1.268342
 33086/100000: episode: 671, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 17.259, mean reward: 0.690 [0.578, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.548, 10.359], loss: 0.002595, mae: 0.053074, mean_q: 1.283188
 33103/100000: episode: 672, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 12.176, mean reward: 0.716 [0.673, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.675, 10.100], loss: 0.002391, mae: 0.050346, mean_q: 1.261621
 33112/100000: episode: 673, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 6.430, mean reward: 0.714 [0.640, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.473, 10.501], loss: 0.002680, mae: 0.054970, mean_q: 1.303177
 33126/100000: episode: 674, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 10.713, mean reward: 0.765 [0.743, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.350, 10.100], loss: 0.001700, mae: 0.044812, mean_q: 1.270136
 33156/100000: episode: 675, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 21.252, mean reward: 0.708 [0.630, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.319, 10.390], loss: 0.002299, mae: 0.050356, mean_q: 1.262612
 33165/100000: episode: 676, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 6.742, mean reward: 0.749 [0.674, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.544], loss: 0.002103, mae: 0.049200, mean_q: 1.290302
 33189/100000: episode: 677, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 17.265, mean reward: 0.719 [0.643, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.093, 10.408], loss: 0.002123, mae: 0.048940, mean_q: 1.278290
 33219/100000: episode: 678, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 22.107, mean reward: 0.737 [0.580, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.482, 10.370], loss: 0.002570, mae: 0.054278, mean_q: 1.286007
 33226/100000: episode: 679, duration: 0.052s, episode steps: 7, steps per second: 134, episode reward: 5.612, mean reward: 0.802 [0.726, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.512], loss: 0.002544, mae: 0.050393, mean_q: 1.259478
 33240/100000: episode: 680, duration: 0.088s, episode steps: 14, steps per second: 160, episode reward: 11.438, mean reward: 0.817 [0.769, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.358, 10.100], loss: 0.002616, mae: 0.055972, mean_q: 1.285124
 33257/100000: episode: 681, duration: 0.110s, episode steps: 17, steps per second: 155, episode reward: 13.156, mean reward: 0.774 [0.675, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.791, 10.100], loss: 0.002569, mae: 0.054156, mean_q: 1.273407
 33266/100000: episode: 682, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 6.406, mean reward: 0.712 [0.657, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.231, 10.496], loss: 0.002226, mae: 0.052174, mean_q: 1.284019
 33291/100000: episode: 683, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 17.823, mean reward: 0.713 [0.621, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.084, 10.460], loss: 0.001958, mae: 0.048196, mean_q: 1.281561
 33296/100000: episode: 684, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 3.856, mean reward: 0.771 [0.755, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.520], loss: 0.002067, mae: 0.048022, mean_q: 1.275911
 33310/100000: episode: 685, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 10.752, mean reward: 0.768 [0.710, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.702, 10.100], loss: 0.001848, mae: 0.047280, mean_q: 1.270323
 33324/100000: episode: 686, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 10.813, mean reward: 0.772 [0.720, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.458, 10.100], loss: 0.001847, mae: 0.046417, mean_q: 1.280922
 33338/100000: episode: 687, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 10.324, mean reward: 0.737 [0.661, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.210, 10.100], loss: 0.002089, mae: 0.050059, mean_q: 1.286147
 33343/100000: episode: 688, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 3.859, mean reward: 0.772 [0.746, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.493], loss: 0.002485, mae: 0.049855, mean_q: 1.297375
 33367/100000: episode: 689, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 17.801, mean reward: 0.742 [0.609, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.521, 10.304], loss: 0.002172, mae: 0.049477, mean_q: 1.288073
 33372/100000: episode: 690, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 4.017, mean reward: 0.803 [0.791, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.352 [-0.035, 10.586], loss: 0.002034, mae: 0.049326, mean_q: 1.285388
 33397/100000: episode: 691, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 19.088, mean reward: 0.764 [0.652, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.942, 10.475], loss: 0.001922, mae: 0.047665, mean_q: 1.292747
 33406/100000: episode: 692, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 6.423, mean reward: 0.714 [0.638, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.408], loss: 0.002531, mae: 0.053618, mean_q: 1.277077
 33436/100000: episode: 693, duration: 0.175s, episode steps: 30, steps per second: 172, episode reward: 23.171, mean reward: 0.772 [0.675, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.879, 10.401], loss: 0.002197, mae: 0.051138, mean_q: 1.295188
 33445/100000: episode: 694, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 6.722, mean reward: 0.747 [0.683, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.524], loss: 0.002454, mae: 0.052612, mean_q: 1.289606
 33459/100000: episode: 695, duration: 0.089s, episode steps: 14, steps per second: 158, episode reward: 10.612, mean reward: 0.758 [0.718, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.498, 10.100], loss: 0.001858, mae: 0.047535, mean_q: 1.281273
 33464/100000: episode: 696, duration: 0.040s, episode steps: 5, steps per second: 124, episode reward: 3.921, mean reward: 0.784 [0.778, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.568], loss: 0.002119, mae: 0.050821, mean_q: 1.301541
 33488/100000: episode: 697, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 19.073, mean reward: 0.795 [0.739, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.640], loss: 0.002074, mae: 0.047269, mean_q: 1.294719
[Info] FALSIFICATION!
 33517/100000: episode: 698, duration: 0.418s, episode steps: 29, steps per second: 69, episode reward: 22.965, mean reward: 0.792 [0.712, 1.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.515, 10.465], loss: 0.002543, mae: 0.054350, mean_q: 1.283818
 33547/100000: episode: 699, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 22.389, mean reward: 0.746 [0.603, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.433, 10.389], loss: 0.001978, mae: 0.047183, mean_q: 1.305493
[Info] Complete ISplit Iteration
[Info] Levels: [1.3410193, 1.5308089, 1.5992345]
[Info] Cond. Prob: [0.1, 0.1, 0.16]
[Info] Error Prob: 0.0016000000000000003

 33564/100000: episode: 700, duration: 4.453s, episode steps: 17, steps per second: 4, episode reward: 13.642, mean reward: 0.802 [0.676, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.438, 10.100], loss: 0.002359, mae: 0.051350, mean_q: 1.293926
 33664/100000: episode: 701, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 58.225, mean reward: 0.582 [0.508, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.485, 10.098], loss: 0.002646, mae: 0.053664, mean_q: 1.298959
 33764/100000: episode: 702, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 62.925, mean reward: 0.629 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.413, 10.280], loss: 0.002105, mae: 0.049726, mean_q: 1.294631
 33864/100000: episode: 703, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.084, mean reward: 0.601 [0.510, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.769, 10.174], loss: 0.002053, mae: 0.049260, mean_q: 1.299967
 33964/100000: episode: 704, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 56.934, mean reward: 0.569 [0.505, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.513, 10.098], loss: 0.002024, mae: 0.048597, mean_q: 1.299281
 34064/100000: episode: 705, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.549, mean reward: 0.575 [0.507, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.533, 10.172], loss: 0.001899, mae: 0.047568, mean_q: 1.303161
 34164/100000: episode: 706, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 57.996, mean reward: 0.580 [0.506, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.841, 10.098], loss: 0.002152, mae: 0.049432, mean_q: 1.296823
 34264/100000: episode: 707, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.147, mean reward: 0.571 [0.499, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.056, 10.189], loss: 0.001959, mae: 0.047865, mean_q: 1.297520
 34364/100000: episode: 708, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.860, mean reward: 0.589 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.373, 10.098], loss: 0.002125, mae: 0.049766, mean_q: 1.296811
 34464/100000: episode: 709, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.475, mean reward: 0.585 [0.507, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.539, 10.339], loss: 0.001955, mae: 0.047850, mean_q: 1.298680
 34564/100000: episode: 710, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 59.460, mean reward: 0.595 [0.508, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.111, 10.098], loss: 0.002439, mae: 0.052765, mean_q: 1.295542
 34664/100000: episode: 711, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 61.528, mean reward: 0.615 [0.502, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.855, 10.098], loss: 0.002010, mae: 0.048101, mean_q: 1.293032
 34764/100000: episode: 712, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.602, mean reward: 0.586 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.815, 10.207], loss: 0.001876, mae: 0.047508, mean_q: 1.296075
 34864/100000: episode: 713, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.157, mean reward: 0.582 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.579, 10.128], loss: 0.002072, mae: 0.048982, mean_q: 1.291337
 34964/100000: episode: 714, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 58.556, mean reward: 0.586 [0.512, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.547, 10.220], loss: 0.001971, mae: 0.047859, mean_q: 1.290455
 35064/100000: episode: 715, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.643, mean reward: 0.596 [0.513, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.067, 10.098], loss: 0.002022, mae: 0.047770, mean_q: 1.292293
 35164/100000: episode: 716, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.943, mean reward: 0.589 [0.504, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.112, 10.168], loss: 0.001857, mae: 0.046999, mean_q: 1.287759
 35264/100000: episode: 717, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.341, mean reward: 0.583 [0.512, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.536, 10.306], loss: 0.002305, mae: 0.050961, mean_q: 1.284085
 35364/100000: episode: 718, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 59.422, mean reward: 0.594 [0.514, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.551, 10.467], loss: 0.002194, mae: 0.049648, mean_q: 1.276656
 35464/100000: episode: 719, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 57.981, mean reward: 0.580 [0.499, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.927, 10.098], loss: 0.002150, mae: 0.048778, mean_q: 1.273292
 35564/100000: episode: 720, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 58.896, mean reward: 0.589 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.050, 10.475], loss: 0.001937, mae: 0.046664, mean_q: 1.272247
 35664/100000: episode: 721, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 62.930, mean reward: 0.629 [0.504, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.421, 10.098], loss: 0.001817, mae: 0.045736, mean_q: 1.269820
 35764/100000: episode: 722, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.563, mean reward: 0.576 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.140, 10.098], loss: 0.001947, mae: 0.047101, mean_q: 1.269673
 35864/100000: episode: 723, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.086, mean reward: 0.591 [0.498, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.976, 10.173], loss: 0.001880, mae: 0.046122, mean_q: 1.268535
 35964/100000: episode: 724, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.761, mean reward: 0.588 [0.499, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.002, 10.172], loss: 0.001985, mae: 0.047801, mean_q: 1.262286
 36064/100000: episode: 725, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.622, mean reward: 0.586 [0.503, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.190, 10.177], loss: 0.001865, mae: 0.046242, mean_q: 1.267128
 36164/100000: episode: 726, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.199, mean reward: 0.582 [0.501, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.159, 10.216], loss: 0.002005, mae: 0.047541, mean_q: 1.259181
 36264/100000: episode: 727, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.390, mean reward: 0.594 [0.499, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.102, 10.098], loss: 0.002064, mae: 0.047919, mean_q: 1.257768
 36364/100000: episode: 728, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.538, mean reward: 0.585 [0.498, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.404, 10.098], loss: 0.002164, mae: 0.048019, mean_q: 1.252015
 36464/100000: episode: 729, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 60.734, mean reward: 0.607 [0.510, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.385, 10.397], loss: 0.002113, mae: 0.048322, mean_q: 1.248789
 36564/100000: episode: 730, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.592, mean reward: 0.596 [0.507, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.398, 10.098], loss: 0.001977, mae: 0.047160, mean_q: 1.241722
 36664/100000: episode: 731, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.572, mean reward: 0.576 [0.508, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.742, 10.098], loss: 0.002110, mae: 0.048497, mean_q: 1.248374
 36764/100000: episode: 732, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.897, mean reward: 0.589 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.446, 10.098], loss: 0.002004, mae: 0.048082, mean_q: 1.243178
 36864/100000: episode: 733, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.696, mean reward: 0.587 [0.500, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.242, 10.098], loss: 0.002026, mae: 0.048087, mean_q: 1.241683
 36964/100000: episode: 734, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 59.163, mean reward: 0.592 [0.507, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.099, 10.098], loss: 0.001854, mae: 0.046780, mean_q: 1.244006
 37064/100000: episode: 735, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.758, mean reward: 0.588 [0.504, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.138, 10.195], loss: 0.001901, mae: 0.046532, mean_q: 1.238327
 37164/100000: episode: 736, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 59.154, mean reward: 0.592 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.913, 10.321], loss: 0.001923, mae: 0.046682, mean_q: 1.226053
 37264/100000: episode: 737, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 59.740, mean reward: 0.597 [0.511, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.512, 10.098], loss: 0.001872, mae: 0.045383, mean_q: 1.222856
 37364/100000: episode: 738, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 60.056, mean reward: 0.601 [0.511, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.573, 10.098], loss: 0.001930, mae: 0.046797, mean_q: 1.222802
 37464/100000: episode: 739, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 56.802, mean reward: 0.568 [0.500, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.590, 10.133], loss: 0.001627, mae: 0.043799, mean_q: 1.220023
 37564/100000: episode: 740, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 60.328, mean reward: 0.603 [0.506, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.637, 10.098], loss: 0.001785, mae: 0.044916, mean_q: 1.210198
 37664/100000: episode: 741, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 60.677, mean reward: 0.607 [0.503, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.494, 10.293], loss: 0.001813, mae: 0.045279, mean_q: 1.208830
 37764/100000: episode: 742, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.976, mean reward: 0.590 [0.502, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.677, 10.430], loss: 0.001701, mae: 0.044334, mean_q: 1.207391
 37864/100000: episode: 743, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 61.318, mean reward: 0.613 [0.513, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.751, 10.098], loss: 0.001938, mae: 0.046493, mean_q: 1.193957
 37964/100000: episode: 744, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.227, mean reward: 0.592 [0.510, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.986, 10.175], loss: 0.001706, mae: 0.045079, mean_q: 1.200347
 38064/100000: episode: 745, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 61.752, mean reward: 0.618 [0.502, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.257, 10.424], loss: 0.001728, mae: 0.045021, mean_q: 1.195320
 38164/100000: episode: 746, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.943, mean reward: 0.579 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.684, 10.253], loss: 0.001770, mae: 0.044925, mean_q: 1.189050
 38264/100000: episode: 747, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 58.937, mean reward: 0.589 [0.504, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.459, 10.214], loss: 0.001682, mae: 0.044037, mean_q: 1.184143
 38364/100000: episode: 748, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.628, mean reward: 0.586 [0.500, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.530, 10.362], loss: 0.001628, mae: 0.043219, mean_q: 1.182000
 38464/100000: episode: 749, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 59.541, mean reward: 0.595 [0.507, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.749, 10.098], loss: 0.001589, mae: 0.043367, mean_q: 1.178717
 38564/100000: episode: 750, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 58.648, mean reward: 0.586 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.117, 10.098], loss: 0.001499, mae: 0.042389, mean_q: 1.170946
 38664/100000: episode: 751, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 59.291, mean reward: 0.593 [0.500, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.562, 10.367], loss: 0.001569, mae: 0.043022, mean_q: 1.169018
 38764/100000: episode: 752, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.537, mean reward: 0.585 [0.507, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.649, 10.230], loss: 0.001558, mae: 0.042603, mean_q: 1.168232
 38864/100000: episode: 753, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 59.164, mean reward: 0.592 [0.512, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.962, 10.240], loss: 0.001628, mae: 0.043754, mean_q: 1.170473
 38964/100000: episode: 754, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.991, mean reward: 0.580 [0.506, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.537, 10.098], loss: 0.001400, mae: 0.040688, mean_q: 1.166828
 39064/100000: episode: 755, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 58.803, mean reward: 0.588 [0.505, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.552, 10.112], loss: 0.001496, mae: 0.041669, mean_q: 1.170308
 39164/100000: episode: 756, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.259, mean reward: 0.593 [0.506, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.364, 10.098], loss: 0.001548, mae: 0.042060, mean_q: 1.165827
 39264/100000: episode: 757, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 61.631, mean reward: 0.616 [0.498, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.415, 10.391], loss: 0.001469, mae: 0.041751, mean_q: 1.167481
 39364/100000: episode: 758, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.464, mean reward: 0.595 [0.511, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.721, 10.245], loss: 0.001534, mae: 0.042566, mean_q: 1.171240
 39464/100000: episode: 759, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.620, mean reward: 0.596 [0.516, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.195, 10.366], loss: 0.001500, mae: 0.042042, mean_q: 1.169677
 39564/100000: episode: 760, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 57.953, mean reward: 0.580 [0.502, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.239, 10.135], loss: 0.001583, mae: 0.043246, mean_q: 1.171484
 39664/100000: episode: 761, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.348, mean reward: 0.603 [0.507, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.237, 10.098], loss: 0.001413, mae: 0.040703, mean_q: 1.169445
 39764/100000: episode: 762, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.801, mean reward: 0.588 [0.505, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.960, 10.098], loss: 0.001443, mae: 0.041445, mean_q: 1.171491
 39864/100000: episode: 763, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.849, mean reward: 0.578 [0.501, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.407, 10.141], loss: 0.001437, mae: 0.041267, mean_q: 1.172508
 39964/100000: episode: 764, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.527, mean reward: 0.585 [0.499, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.278, 10.098], loss: 0.001421, mae: 0.041044, mean_q: 1.168668
 40064/100000: episode: 765, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.119, mean reward: 0.591 [0.513, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.018, 10.338], loss: 0.001495, mae: 0.041911, mean_q: 1.169728
 40164/100000: episode: 766, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.227, mean reward: 0.592 [0.505, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.904, 10.411], loss: 0.001476, mae: 0.041618, mean_q: 1.172976
 40264/100000: episode: 767, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.816, mean reward: 0.578 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.215, 10.098], loss: 0.001718, mae: 0.043841, mean_q: 1.173037
 40364/100000: episode: 768, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.445, mean reward: 0.594 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.725, 10.132], loss: 0.001526, mae: 0.041841, mean_q: 1.169526
 40464/100000: episode: 769, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 63.218, mean reward: 0.632 [0.508, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.702, 10.395], loss: 0.001477, mae: 0.041500, mean_q: 1.169343
 40564/100000: episode: 770, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 60.213, mean reward: 0.602 [0.508, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.489, 10.098], loss: 0.001397, mae: 0.039924, mean_q: 1.168592
 40664/100000: episode: 771, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.821, mean reward: 0.578 [0.505, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.595, 10.125], loss: 0.001343, mae: 0.039828, mean_q: 1.169133
 40764/100000: episode: 772, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 60.641, mean reward: 0.606 [0.503, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.115, 10.098], loss: 0.001336, mae: 0.039973, mean_q: 1.169699
 40864/100000: episode: 773, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.796, mean reward: 0.588 [0.504, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.792, 10.282], loss: 0.001362, mae: 0.039850, mean_q: 1.169891
 40964/100000: episode: 774, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 62.747, mean reward: 0.627 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.758, 10.098], loss: 0.001355, mae: 0.040283, mean_q: 1.170334
 41064/100000: episode: 775, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.173, mean reward: 0.582 [0.503, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.367, 10.132], loss: 0.001796, mae: 0.045167, mean_q: 1.177507
 41164/100000: episode: 776, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 56.544, mean reward: 0.565 [0.497, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.258, 10.261], loss: 0.001419, mae: 0.040441, mean_q: 1.174943
 41264/100000: episode: 777, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 65.727, mean reward: 0.657 [0.510, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.934, 10.365], loss: 0.001564, mae: 0.041975, mean_q: 1.171924
 41364/100000: episode: 778, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 60.036, mean reward: 0.600 [0.499, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.380, 10.441], loss: 0.001597, mae: 0.043147, mean_q: 1.174370
 41464/100000: episode: 779, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 65.643, mean reward: 0.656 [0.527, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.491, 10.381], loss: 0.001494, mae: 0.041471, mean_q: 1.178741
 41564/100000: episode: 780, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 59.008, mean reward: 0.590 [0.504, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.427, 10.179], loss: 0.001502, mae: 0.041861, mean_q: 1.173691
 41664/100000: episode: 781, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.186, mean reward: 0.602 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.987, 10.098], loss: 0.001458, mae: 0.041824, mean_q: 1.180738
 41764/100000: episode: 782, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 59.616, mean reward: 0.596 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.363, 10.440], loss: 0.001632, mae: 0.043276, mean_q: 1.174728
 41864/100000: episode: 783, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.195, mean reward: 0.572 [0.504, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.285, 10.145], loss: 0.001362, mae: 0.040008, mean_q: 1.176216
 41964/100000: episode: 784, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 57.296, mean reward: 0.573 [0.504, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.511, 10.123], loss: 0.001540, mae: 0.042897, mean_q: 1.177232
 42064/100000: episode: 785, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 59.134, mean reward: 0.591 [0.513, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.620, 10.098], loss: 0.001467, mae: 0.040983, mean_q: 1.176348
 42164/100000: episode: 786, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.771, mean reward: 0.578 [0.499, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.665, 10.098], loss: 0.001591, mae: 0.043285, mean_q: 1.176475
 42264/100000: episode: 787, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.179, mean reward: 0.582 [0.498, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.866, 10.109], loss: 0.001465, mae: 0.041208, mean_q: 1.176238
 42364/100000: episode: 788, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 59.109, mean reward: 0.591 [0.506, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.725, 10.225], loss: 0.001503, mae: 0.042359, mean_q: 1.176203
 42464/100000: episode: 789, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 56.881, mean reward: 0.569 [0.498, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.052, 10.133], loss: 0.001519, mae: 0.042029, mean_q: 1.178279
 42564/100000: episode: 790, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.045, mean reward: 0.580 [0.497, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.494, 10.098], loss: 0.001461, mae: 0.041568, mean_q: 1.173161
 42664/100000: episode: 791, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 61.617, mean reward: 0.616 [0.526, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.300, 10.333], loss: 0.001397, mae: 0.040822, mean_q: 1.175236
 42764/100000: episode: 792, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.758, mean reward: 0.588 [0.510, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.537, 10.305], loss: 0.001375, mae: 0.040461, mean_q: 1.176661
 42864/100000: episode: 793, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.482, mean reward: 0.575 [0.498, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.918, 10.115], loss: 0.001361, mae: 0.039791, mean_q: 1.172517
 42964/100000: episode: 794, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 60.729, mean reward: 0.607 [0.509, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.672, 10.227], loss: 0.001367, mae: 0.040920, mean_q: 1.174298
 43064/100000: episode: 795, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.456, mean reward: 0.575 [0.510, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.491, 10.212], loss: 0.001417, mae: 0.041599, mean_q: 1.172493
 43164/100000: episode: 796, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 56.922, mean reward: 0.569 [0.508, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.061, 10.098], loss: 0.001423, mae: 0.041108, mean_q: 1.174598
 43264/100000: episode: 797, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.268, mean reward: 0.593 [0.508, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.838, 10.098], loss: 0.001513, mae: 0.042068, mean_q: 1.170505
 43364/100000: episode: 798, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.335, mean reward: 0.583 [0.501, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.690, 10.098], loss: 0.001420, mae: 0.041455, mean_q: 1.173296
 43464/100000: episode: 799, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 59.682, mean reward: 0.597 [0.502, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.473, 10.179], loss: 0.001503, mae: 0.042328, mean_q: 1.171444
[Info] 1-TH LEVEL FOUND: 1.3839390277862549, Considering 10/90 traces
 43564/100000: episode: 800, duration: 4.827s, episode steps: 100, steps per second: 21, episode reward: 59.678, mean reward: 0.597 [0.508, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.941, 10.098], loss: 0.001536, mae: 0.042550, mean_q: 1.172935
 43598/100000: episode: 801, duration: 0.209s, episode steps: 34, steps per second: 163, episode reward: 22.409, mean reward: 0.659 [0.511, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.810, 10.409], loss: 0.001607, mae: 0.043419, mean_q: 1.180804
 43632/100000: episode: 802, duration: 0.194s, episode steps: 34, steps per second: 175, episode reward: 22.745, mean reward: 0.669 [0.564, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.366, 10.288], loss: 0.001738, mae: 0.044569, mean_q: 1.168527
 43676/100000: episode: 803, duration: 0.240s, episode steps: 44, steps per second: 183, episode reward: 28.673, mean reward: 0.652 [0.569, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.901, 10.324], loss: 0.001416, mae: 0.040800, mean_q: 1.173672
 43697/100000: episode: 804, duration: 0.131s, episode steps: 21, steps per second: 160, episode reward: 13.939, mean reward: 0.664 [0.602, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.347], loss: 0.001400, mae: 0.040521, mean_q: 1.169391
 43737/100000: episode: 805, duration: 0.235s, episode steps: 40, steps per second: 170, episode reward: 23.841, mean reward: 0.596 [0.512, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.048, 10.100], loss: 0.001540, mae: 0.041941, mean_q: 1.177176
 43757/100000: episode: 806, duration: 0.121s, episode steps: 20, steps per second: 166, episode reward: 13.756, mean reward: 0.688 [0.632, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.076, 10.448], loss: 0.001822, mae: 0.045416, mean_q: 1.177056
 43801/100000: episode: 807, duration: 0.257s, episode steps: 44, steps per second: 171, episode reward: 29.655, mean reward: 0.674 [0.569, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.712, 10.265], loss: 0.001506, mae: 0.040853, mean_q: 1.180228
 43822/100000: episode: 808, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 14.327, mean reward: 0.682 [0.633, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.044, 10.450], loss: 0.001444, mae: 0.041198, mean_q: 1.174138
 43852/100000: episode: 809, duration: 0.179s, episode steps: 30, steps per second: 167, episode reward: 20.918, mean reward: 0.697 [0.636, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.475], loss: 0.001694, mae: 0.043800, mean_q: 1.175761
 43893/100000: episode: 810, duration: 0.235s, episode steps: 41, steps per second: 175, episode reward: 30.301, mean reward: 0.739 [0.596, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.560, 10.516], loss: 0.001744, mae: 0.045001, mean_q: 1.185225
 43913/100000: episode: 811, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 13.166, mean reward: 0.658 [0.613, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.126, 10.371], loss: 0.001309, mae: 0.039397, mean_q: 1.185404
 43933/100000: episode: 812, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 13.781, mean reward: 0.689 [0.625, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.468], loss: 0.001235, mae: 0.039674, mean_q: 1.190311
 43974/100000: episode: 813, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 28.031, mean reward: 0.684 [0.619, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.331, 10.426], loss: 0.001632, mae: 0.043119, mean_q: 1.178470
 43995/100000: episode: 814, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 15.532, mean reward: 0.740 [0.674, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.456], loss: 0.001301, mae: 0.039250, mean_q: 1.179621
 44036/100000: episode: 815, duration: 0.235s, episode steps: 41, steps per second: 175, episode reward: 27.208, mean reward: 0.664 [0.577, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.309, 10.463], loss: 0.001553, mae: 0.042666, mean_q: 1.188028
 44077/100000: episode: 816, duration: 0.223s, episode steps: 41, steps per second: 184, episode reward: 26.640, mean reward: 0.650 [0.581, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.625, 10.309], loss: 0.001567, mae: 0.042694, mean_q: 1.188279
 44121/100000: episode: 817, duration: 0.256s, episode steps: 44, steps per second: 172, episode reward: 29.737, mean reward: 0.676 [0.590, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.640, 10.332], loss: 0.001625, mae: 0.043038, mean_q: 1.192579
 44136/100000: episode: 818, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 10.220, mean reward: 0.681 [0.628, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.476], loss: 0.001589, mae: 0.044091, mean_q: 1.191851
 44142/100000: episode: 819, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 4.273, mean reward: 0.712 [0.669, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.464], loss: 0.001561, mae: 0.044002, mean_q: 1.181066
 44163/100000: episode: 820, duration: 0.106s, episode steps: 21, steps per second: 199, episode reward: 14.558, mean reward: 0.693 [0.645, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.488], loss: 0.001398, mae: 0.040785, mean_q: 1.187213
 44203/100000: episode: 821, duration: 0.223s, episode steps: 40, steps per second: 179, episode reward: 29.294, mean reward: 0.732 [0.626, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.962, 10.523], loss: 0.001464, mae: 0.042353, mean_q: 1.195768
 44224/100000: episode: 822, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 15.168, mean reward: 0.722 [0.636, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.298, 10.419], loss: 0.001390, mae: 0.040134, mean_q: 1.193825
 44264/100000: episode: 823, duration: 0.219s, episode steps: 40, steps per second: 183, episode reward: 23.741, mean reward: 0.594 [0.519, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.270, 10.100], loss: 0.001525, mae: 0.042729, mean_q: 1.192098
 44279/100000: episode: 824, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 9.792, mean reward: 0.653 [0.609, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.431], loss: 0.001318, mae: 0.039953, mean_q: 1.197254
 44305/100000: episode: 825, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 17.995, mean reward: 0.692 [0.632, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.929, 10.402], loss: 0.001659, mae: 0.042892, mean_q: 1.187531
 44346/100000: episode: 826, duration: 0.245s, episode steps: 41, steps per second: 167, episode reward: 26.343, mean reward: 0.643 [0.509, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-1.639, 10.163], loss: 0.001538, mae: 0.043313, mean_q: 1.197638
 44361/100000: episode: 827, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 10.644, mean reward: 0.710 [0.639, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.074, 10.396], loss: 0.001600, mae: 0.044141, mean_q: 1.188154
 44381/100000: episode: 828, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 13.847, mean reward: 0.692 [0.641, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.450], loss: 0.001502, mae: 0.041050, mean_q: 1.193215
 44415/100000: episode: 829, duration: 0.199s, episode steps: 34, steps per second: 171, episode reward: 23.781, mean reward: 0.699 [0.639, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-1.477, 10.393], loss: 0.001534, mae: 0.041461, mean_q: 1.197282
 44435/100000: episode: 830, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 13.520, mean reward: 0.676 [0.608, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.338, 10.353], loss: 0.001732, mae: 0.044613, mean_q: 1.190109
 44455/100000: episode: 831, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 13.715, mean reward: 0.686 [0.630, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.571, 10.416], loss: 0.001555, mae: 0.042955, mean_q: 1.201342
 44470/100000: episode: 832, duration: 0.093s, episode steps: 15, steps per second: 162, episode reward: 10.343, mean reward: 0.690 [0.652, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.329, 10.429], loss: 0.001669, mae: 0.042910, mean_q: 1.205462
 44514/100000: episode: 833, duration: 0.254s, episode steps: 44, steps per second: 173, episode reward: 29.761, mean reward: 0.676 [0.576, 0.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.864, 10.272], loss: 0.001930, mae: 0.046517, mean_q: 1.194677
 44544/100000: episode: 834, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 19.254, mean reward: 0.642 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.065, 10.100], loss: 0.001784, mae: 0.045161, mean_q: 1.193937
 44550/100000: episode: 835, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 4.782, mean reward: 0.797 [0.720, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.402, 10.398], loss: 0.001408, mae: 0.041610, mean_q: 1.195448
 44584/100000: episode: 836, duration: 0.203s, episode steps: 34, steps per second: 167, episode reward: 23.701, mean reward: 0.697 [0.616, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.103, 10.538], loss: 0.001723, mae: 0.043140, mean_q: 1.193091
 44618/100000: episode: 837, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 25.666, mean reward: 0.755 [0.605, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.152, 10.480], loss: 0.001868, mae: 0.045779, mean_q: 1.201443
 44644/100000: episode: 838, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 17.169, mean reward: 0.660 [0.583, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.054, 10.512], loss: 0.001782, mae: 0.045119, mean_q: 1.215789
 44650/100000: episode: 839, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 4.176, mean reward: 0.696 [0.662, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.424], loss: 0.001309, mae: 0.036329, mean_q: 1.195471
 44665/100000: episode: 840, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 10.617, mean reward: 0.708 [0.643, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.393], loss: 0.001477, mae: 0.042366, mean_q: 1.202347
 44705/100000: episode: 841, duration: 0.248s, episode steps: 40, steps per second: 162, episode reward: 25.596, mean reward: 0.640 [0.510, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.595, 10.175], loss: 0.001997, mae: 0.046161, mean_q: 1.198754
 44745/100000: episode: 842, duration: 0.236s, episode steps: 40, steps per second: 169, episode reward: 25.703, mean reward: 0.643 [0.530, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.996, 10.317], loss: 0.001832, mae: 0.047813, mean_q: 1.210057
 44766/100000: episode: 843, duration: 0.120s, episode steps: 21, steps per second: 174, episode reward: 14.925, mean reward: 0.711 [0.654, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.655, 10.450], loss: 0.001965, mae: 0.044796, mean_q: 1.198637
 44781/100000: episode: 844, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 11.016, mean reward: 0.734 [0.653, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.862, 10.529], loss: 0.001686, mae: 0.044721, mean_q: 1.208552
 44807/100000: episode: 845, duration: 0.160s, episode steps: 26, steps per second: 163, episode reward: 16.436, mean reward: 0.632 [0.556, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.153, 10.278], loss: 0.001509, mae: 0.041772, mean_q: 1.206631
 44833/100000: episode: 846, duration: 0.158s, episode steps: 26, steps per second: 164, episode reward: 16.910, mean reward: 0.650 [0.547, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.211, 10.232], loss: 0.001394, mae: 0.041258, mean_q: 1.208499
 44854/100000: episode: 847, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 14.743, mean reward: 0.702 [0.635, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.246, 10.563], loss: 0.001469, mae: 0.042161, mean_q: 1.209113
 44874/100000: episode: 848, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 12.715, mean reward: 0.636 [0.551, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.131, 10.258], loss: 0.001712, mae: 0.044369, mean_q: 1.210924
 44914/100000: episode: 849, duration: 0.234s, episode steps: 40, steps per second: 171, episode reward: 26.200, mean reward: 0.655 [0.552, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.641, 10.158], loss: 0.001787, mae: 0.045321, mean_q: 1.210665
 44929/100000: episode: 850, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 10.522, mean reward: 0.701 [0.649, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.454], loss: 0.001578, mae: 0.042223, mean_q: 1.210833
 44970/100000: episode: 851, duration: 0.239s, episode steps: 41, steps per second: 172, episode reward: 26.306, mean reward: 0.642 [0.563, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.662, 10.332], loss: 0.001723, mae: 0.043684, mean_q: 1.205775
 45004/100000: episode: 852, duration: 0.205s, episode steps: 34, steps per second: 166, episode reward: 24.777, mean reward: 0.729 [0.618, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.293, 10.323], loss: 0.001699, mae: 0.043662, mean_q: 1.210917
 45044/100000: episode: 853, duration: 0.241s, episode steps: 40, steps per second: 166, episode reward: 27.658, mean reward: 0.691 [0.633, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.831, 10.454], loss: 0.001899, mae: 0.044902, mean_q: 1.212472
 45085/100000: episode: 854, duration: 0.234s, episode steps: 41, steps per second: 175, episode reward: 25.925, mean reward: 0.632 [0.537, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.393, 10.210], loss: 0.001701, mae: 0.044119, mean_q: 1.214126
 45100/100000: episode: 855, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 9.939, mean reward: 0.663 [0.631, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.730, 10.462], loss: 0.001727, mae: 0.042960, mean_q: 1.196579
 45121/100000: episode: 856, duration: 0.130s, episode steps: 21, steps per second: 161, episode reward: 14.448, mean reward: 0.688 [0.635, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.316, 10.448], loss: 0.001670, mae: 0.043932, mean_q: 1.211926
 45147/100000: episode: 857, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 15.603, mean reward: 0.600 [0.519, 0.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.185], loss: 0.001926, mae: 0.047441, mean_q: 1.227376
 45177/100000: episode: 858, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 19.309, mean reward: 0.644 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.435, 10.239], loss: 0.001740, mae: 0.044295, mean_q: 1.222088
 45217/100000: episode: 859, duration: 0.233s, episode steps: 40, steps per second: 172, episode reward: 30.088, mean reward: 0.752 [0.636, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.552, 10.381], loss: 0.001710, mae: 0.044107, mean_q: 1.219330
 45251/100000: episode: 860, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 22.031, mean reward: 0.648 [0.559, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.435, 10.353], loss: 0.001537, mae: 0.041741, mean_q: 1.209640
 45295/100000: episode: 861, duration: 0.255s, episode steps: 44, steps per second: 173, episode reward: 29.675, mean reward: 0.674 [0.546, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.897, 10.236], loss: 0.001687, mae: 0.043742, mean_q: 1.221916
 45301/100000: episode: 862, duration: 0.047s, episode steps: 6, steps per second: 129, episode reward: 4.537, mean reward: 0.756 [0.692, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.572], loss: 0.001974, mae: 0.049283, mean_q: 1.238934
 45307/100000: episode: 863, duration: 0.036s, episode steps: 6, steps per second: 164, episode reward: 4.759, mean reward: 0.793 [0.743, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.570], loss: 0.001745, mae: 0.044204, mean_q: 1.231203
 45328/100000: episode: 864, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 13.119, mean reward: 0.625 [0.577, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.797, 10.270], loss: 0.001722, mae: 0.045198, mean_q: 1.225739
 45349/100000: episode: 865, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 14.784, mean reward: 0.704 [0.605, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.348], loss: 0.001614, mae: 0.042579, mean_q: 1.224160
 45393/100000: episode: 866, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 28.741, mean reward: 0.653 [0.544, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.621, 10.292], loss: 0.001643, mae: 0.043152, mean_q: 1.217546
 45433/100000: episode: 867, duration: 0.230s, episode steps: 40, steps per second: 174, episode reward: 25.027, mean reward: 0.626 [0.531, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.302, 10.155], loss: 0.001606, mae: 0.042593, mean_q: 1.220762
 45467/100000: episode: 868, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 26.028, mean reward: 0.766 [0.647, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.548, 10.506], loss: 0.001574, mae: 0.042001, mean_q: 1.222553
 45508/100000: episode: 869, duration: 0.248s, episode steps: 41, steps per second: 165, episode reward: 27.019, mean reward: 0.659 [0.556, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.221, 10.287], loss: 0.001630, mae: 0.043999, mean_q: 1.229274
 45542/100000: episode: 870, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 23.771, mean reward: 0.699 [0.626, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.087, 10.546], loss: 0.001865, mae: 0.046986, mean_q: 1.229646
 45563/100000: episode: 871, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 13.564, mean reward: 0.646 [0.518, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.080, 10.252], loss: 0.001627, mae: 0.043744, mean_q: 1.223922
 45583/100000: episode: 872, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 12.529, mean reward: 0.626 [0.521, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.053, 10.166], loss: 0.001504, mae: 0.041769, mean_q: 1.230212
 45603/100000: episode: 873, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 13.875, mean reward: 0.694 [0.631, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.451], loss: 0.001371, mae: 0.040728, mean_q: 1.214095
 45609/100000: episode: 874, duration: 0.045s, episode steps: 6, steps per second: 134, episode reward: 4.331, mean reward: 0.722 [0.682, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.384], loss: 0.001506, mae: 0.043227, mean_q: 1.241078
 45643/100000: episode: 875, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 21.794, mean reward: 0.641 [0.575, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.876, 10.259], loss: 0.001511, mae: 0.042241, mean_q: 1.230240
 45669/100000: episode: 876, duration: 0.156s, episode steps: 26, steps per second: 167, episode reward: 18.002, mean reward: 0.692 [0.578, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.238, 10.603], loss: 0.001578, mae: 0.043222, mean_q: 1.235061
 45703/100000: episode: 877, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 22.328, mean reward: 0.657 [0.604, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.460, 10.428], loss: 0.001858, mae: 0.046087, mean_q: 1.223231
 45723/100000: episode: 878, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 12.459, mean reward: 0.623 [0.539, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.742, 10.223], loss: 0.001651, mae: 0.043667, mean_q: 1.230919
 45744/100000: episode: 879, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 15.619, mean reward: 0.744 [0.667, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.677, 10.557], loss: 0.001553, mae: 0.040862, mean_q: 1.232494
 45764/100000: episode: 880, duration: 0.125s, episode steps: 20, steps per second: 160, episode reward: 14.120, mean reward: 0.706 [0.635, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.335, 10.505], loss: 0.001984, mae: 0.045885, mean_q: 1.225904
 45790/100000: episode: 881, duration: 0.159s, episode steps: 26, steps per second: 164, episode reward: 16.426, mean reward: 0.632 [0.542, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.177, 10.177], loss: 0.001764, mae: 0.045324, mean_q: 1.237783
 45810/100000: episode: 882, duration: 0.110s, episode steps: 20, steps per second: 183, episode reward: 13.020, mean reward: 0.651 [0.590, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.301], loss: 0.001508, mae: 0.041645, mean_q: 1.231982
 45850/100000: episode: 883, duration: 0.231s, episode steps: 40, steps per second: 173, episode reward: 26.708, mean reward: 0.668 [0.565, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.369, 10.301], loss: 0.001484, mae: 0.041757, mean_q: 1.229671
 45876/100000: episode: 884, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 18.596, mean reward: 0.715 [0.639, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.612, 10.499], loss: 0.002414, mae: 0.050135, mean_q: 1.230734
 45920/100000: episode: 885, duration: 0.245s, episode steps: 44, steps per second: 180, episode reward: 29.573, mean reward: 0.672 [0.548, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.270, 10.294], loss: 0.001701, mae: 0.044319, mean_q: 1.232762
 45935/100000: episode: 886, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 11.683, mean reward: 0.779 [0.643, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.373, 10.704], loss: 0.001614, mae: 0.043284, mean_q: 1.234318
 45961/100000: episode: 887, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 17.208, mean reward: 0.662 [0.599, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.434], loss: 0.001829, mae: 0.045506, mean_q: 1.226955
 45981/100000: episode: 888, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 14.257, mean reward: 0.713 [0.647, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.858, 10.453], loss: 0.001463, mae: 0.041865, mean_q: 1.238565
 46002/100000: episode: 889, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 13.907, mean reward: 0.662 [0.587, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.323], loss: 0.001669, mae: 0.043606, mean_q: 1.234617
[Info] 2-TH LEVEL FOUND: 1.489328145980835, Considering 10/90 traces
 46022/100000: episode: 890, duration: 4.520s, episode steps: 20, steps per second: 4, episode reward: 14.872, mean reward: 0.744 [0.695, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-1.722, 10.416], loss: 0.001950, mae: 0.048095, mean_q: 1.238079
 46036/100000: episode: 891, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 10.852, mean reward: 0.775 [0.679, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.367, 10.421], loss: 0.001758, mae: 0.043715, mean_q: 1.225276
 46044/100000: episode: 892, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 6.824, mean reward: 0.853 [0.834, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.558], loss: 0.001650, mae: 0.043178, mean_q: 1.245532
 46075/100000: episode: 893, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 20.364, mean reward: 0.657 [0.537, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.329, 10.233], loss: 0.001741, mae: 0.044810, mean_q: 1.231918
 46105/100000: episode: 894, duration: 0.186s, episode steps: 30, steps per second: 161, episode reward: 22.828, mean reward: 0.761 [0.699, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.207, 10.491], loss: 0.001672, mae: 0.044142, mean_q: 1.246259
 46119/100000: episode: 895, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 10.184, mean reward: 0.727 [0.640, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.350], loss: 0.001459, mae: 0.042047, mean_q: 1.244285
 46145/100000: episode: 896, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 18.458, mean reward: 0.710 [0.668, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.467, 10.460], loss: 0.001677, mae: 0.043451, mean_q: 1.236360
 46176/100000: episode: 897, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 24.879, mean reward: 0.803 [0.661, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.504], loss: 0.001810, mae: 0.044479, mean_q: 1.235942
 46193/100000: episode: 898, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 12.777, mean reward: 0.752 [0.718, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.426, 10.578], loss: 0.001701, mae: 0.043380, mean_q: 1.239616
 46217/100000: episode: 899, duration: 0.150s, episode steps: 24, steps per second: 160, episode reward: 19.272, mean reward: 0.803 [0.732, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.710, 10.668], loss: 0.001618, mae: 0.044111, mean_q: 1.237653
 46248/100000: episode: 900, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 21.473, mean reward: 0.693 [0.642, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.738, 10.373], loss: 0.001847, mae: 0.045315, mean_q: 1.240139
 46289/100000: episode: 901, duration: 0.232s, episode steps: 41, steps per second: 177, episode reward: 28.792, mean reward: 0.702 [0.577, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.851, 10.282], loss: 0.001823, mae: 0.044756, mean_q: 1.239742
 46313/100000: episode: 902, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 18.454, mean reward: 0.769 [0.685, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.614], loss: 0.001620, mae: 0.043432, mean_q: 1.243592
 46327/100000: episode: 903, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 10.184, mean reward: 0.727 [0.686, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.697, 10.506], loss: 0.002007, mae: 0.045820, mean_q: 1.243034
 46341/100000: episode: 904, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 10.838, mean reward: 0.774 [0.687, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.502], loss: 0.001867, mae: 0.045332, mean_q: 1.243619
 46370/100000: episode: 905, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 20.511, mean reward: 0.707 [0.594, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.447, 10.232], loss: 0.001586, mae: 0.043273, mean_q: 1.243991
 46411/100000: episode: 906, duration: 0.222s, episode steps: 41, steps per second: 184, episode reward: 29.553, mean reward: 0.721 [0.601, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.514, 10.357], loss: 0.001547, mae: 0.042078, mean_q: 1.243323
 46442/100000: episode: 907, duration: 0.186s, episode steps: 31, steps per second: 167, episode reward: 24.956, mean reward: 0.805 [0.713, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.401, 10.579], loss: 0.001482, mae: 0.041842, mean_q: 1.261039
 46450/100000: episode: 908, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 6.217, mean reward: 0.777 [0.704, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.441], loss: 0.001575, mae: 0.042692, mean_q: 1.248971
 46481/100000: episode: 909, duration: 0.187s, episode steps: 31, steps per second: 166, episode reward: 24.302, mean reward: 0.784 [0.707, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.436], loss: 0.001569, mae: 0.042503, mean_q: 1.248189
 46489/100000: episode: 910, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 5.659, mean reward: 0.707 [0.676, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.430], loss: 0.001812, mae: 0.048246, mean_q: 1.264504
 46530/100000: episode: 911, duration: 0.258s, episode steps: 41, steps per second: 159, episode reward: 27.124, mean reward: 0.662 [0.538, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-1.443, 10.190], loss: 0.001625, mae: 0.043805, mean_q: 1.261134
 46556/100000: episode: 912, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 17.608, mean reward: 0.677 [0.575, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.673, 10.269], loss: 0.001813, mae: 0.043543, mean_q: 1.253819
 46564/100000: episode: 913, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 6.790, mean reward: 0.849 [0.742, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.734], loss: 0.001621, mae: 0.045062, mean_q: 1.245105
 46581/100000: episode: 914, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 12.642, mean reward: 0.744 [0.686, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.566, 10.482], loss: 0.002003, mae: 0.045647, mean_q: 1.263664
 46605/100000: episode: 915, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 15.742, mean reward: 0.656 [0.583, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.223, 10.298], loss: 0.001744, mae: 0.043886, mean_q: 1.255270
 46630/100000: episode: 916, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 18.069, mean reward: 0.723 [0.658, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.417], loss: 0.001615, mae: 0.042705, mean_q: 1.264053
 46661/100000: episode: 917, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 23.324, mean reward: 0.752 [0.676, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.035, 10.498], loss: 0.001621, mae: 0.042219, mean_q: 1.265304
 46692/100000: episode: 918, duration: 0.181s, episode steps: 31, steps per second: 171, episode reward: 19.949, mean reward: 0.644 [0.515, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.164, 10.174], loss: 0.001987, mae: 0.046964, mean_q: 1.261592
 46717/100000: episode: 919, duration: 0.151s, episode steps: 25, steps per second: 166, episode reward: 17.204, mean reward: 0.688 [0.583, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.401, 10.263], loss: 0.001576, mae: 0.043112, mean_q: 1.271905
 46731/100000: episode: 920, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 11.217, mean reward: 0.801 [0.765, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.545], loss: 0.002058, mae: 0.047385, mean_q: 1.252696
 46772/100000: episode: 921, duration: 0.236s, episode steps: 41, steps per second: 174, episode reward: 27.961, mean reward: 0.682 [0.552, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.162, 10.192], loss: 0.001795, mae: 0.043724, mean_q: 1.266214
 46789/100000: episode: 922, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 13.425, mean reward: 0.790 [0.720, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.520], loss: 0.001762, mae: 0.043852, mean_q: 1.259486
 46819/100000: episode: 923, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 21.622, mean reward: 0.721 [0.587, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.545, 10.286], loss: 0.001687, mae: 0.043426, mean_q: 1.265623
 46836/100000: episode: 924, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 13.108, mean reward: 0.771 [0.731, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.277, 10.511], loss: 0.001585, mae: 0.043121, mean_q: 1.258008
 46853/100000: episode: 925, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 13.948, mean reward: 0.820 [0.746, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.960, 10.575], loss: 0.001752, mae: 0.044795, mean_q: 1.268543
 46861/100000: episode: 926, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 6.007, mean reward: 0.751 [0.731, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.501], loss: 0.001568, mae: 0.042790, mean_q: 1.256240
 46902/100000: episode: 927, duration: 0.260s, episode steps: 41, steps per second: 158, episode reward: 29.601, mean reward: 0.722 [0.551, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.929, 10.242], loss: 0.001735, mae: 0.045681, mean_q: 1.276153
 46943/100000: episode: 928, duration: 0.236s, episode steps: 41, steps per second: 174, episode reward: 27.272, mean reward: 0.665 [0.558, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-1.132, 10.276], loss: 0.001670, mae: 0.044504, mean_q: 1.278185
 46969/100000: episode: 929, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 18.346, mean reward: 0.706 [0.660, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.422], loss: 0.001625, mae: 0.042973, mean_q: 1.269072
 46995/100000: episode: 930, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 18.148, mean reward: 0.698 [0.642, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.752, 10.488], loss: 0.002097, mae: 0.049319, mean_q: 1.270134
 47019/100000: episode: 931, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 17.749, mean reward: 0.740 [0.670, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.446], loss: 0.001599, mae: 0.043383, mean_q: 1.283367
 47049/100000: episode: 932, duration: 0.177s, episode steps: 30, steps per second: 170, episode reward: 19.833, mean reward: 0.661 [0.582, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.616, 10.322], loss: 0.001645, mae: 0.043180, mean_q: 1.263320
 47078/100000: episode: 933, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 21.563, mean reward: 0.744 [0.689, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.545], loss: 0.001636, mae: 0.043283, mean_q: 1.273369
 47103/100000: episode: 934, duration: 0.154s, episode steps: 25, steps per second: 163, episode reward: 20.582, mean reward: 0.823 [0.692, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.502], loss: 0.001611, mae: 0.042444, mean_q: 1.280883
 47134/100000: episode: 935, duration: 0.186s, episode steps: 31, steps per second: 166, episode reward: 23.544, mean reward: 0.759 [0.680, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.991, 10.416], loss: 0.001516, mae: 0.042889, mean_q: 1.276808
 47175/100000: episode: 936, duration: 0.225s, episode steps: 41, steps per second: 183, episode reward: 27.409, mean reward: 0.669 [0.504, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.156, 10.100], loss: 0.001771, mae: 0.045580, mean_q: 1.279103
 47201/100000: episode: 937, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 19.362, mean reward: 0.745 [0.690, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.656, 10.436], loss: 0.001620, mae: 0.044013, mean_q: 1.281919
 47227/100000: episode: 938, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 19.239, mean reward: 0.740 [0.612, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.213, 10.286], loss: 0.001569, mae: 0.041452, mean_q: 1.286083
 47268/100000: episode: 939, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 30.989, mean reward: 0.756 [0.667, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-1.370, 10.502], loss: 0.001545, mae: 0.041789, mean_q: 1.282245
 47297/100000: episode: 940, duration: 0.174s, episode steps: 29, steps per second: 166, episode reward: 21.993, mean reward: 0.758 [0.699, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.482, 10.505], loss: 0.001838, mae: 0.045062, mean_q: 1.291086
 47326/100000: episode: 941, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 19.883, mean reward: 0.686 [0.620, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.330, 10.328], loss: 0.001669, mae: 0.044754, mean_q: 1.284997
 47367/100000: episode: 942, duration: 0.233s, episode steps: 41, steps per second: 176, episode reward: 29.381, mean reward: 0.717 [0.614, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.871, 10.339], loss: 0.001681, mae: 0.044030, mean_q: 1.281048
 47396/100000: episode: 943, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 21.715, mean reward: 0.749 [0.631, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.411, 10.525], loss: 0.001630, mae: 0.044970, mean_q: 1.284815
 47425/100000: episode: 944, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 20.516, mean reward: 0.707 [0.627, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.325, 10.504], loss: 0.001606, mae: 0.043166, mean_q: 1.307394
 47449/100000: episode: 945, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 19.463, mean reward: 0.811 [0.728, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.382, 10.512], loss: 0.001700, mae: 0.043568, mean_q: 1.287664
 47466/100000: episode: 946, duration: 0.114s, episode steps: 17, steps per second: 149, episode reward: 11.998, mean reward: 0.706 [0.648, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.658, 10.364], loss: 0.001960, mae: 0.046451, mean_q: 1.286929
 47480/100000: episode: 947, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 10.835, mean reward: 0.774 [0.706, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.667, 10.527], loss: 0.001718, mae: 0.045637, mean_q: 1.296144
 47511/100000: episode: 948, duration: 0.191s, episode steps: 31, steps per second: 162, episode reward: 20.720, mean reward: 0.668 [0.582, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-1.084, 10.286], loss: 0.001732, mae: 0.045060, mean_q: 1.303971
 47536/100000: episode: 949, duration: 0.157s, episode steps: 25, steps per second: 159, episode reward: 19.364, mean reward: 0.775 [0.683, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.626, 10.477], loss: 0.001874, mae: 0.047296, mean_q: 1.306204
 47550/100000: episode: 950, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 10.817, mean reward: 0.773 [0.733, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.396, 10.522], loss: 0.001806, mae: 0.047051, mean_q: 1.312454
 47581/100000: episode: 951, duration: 0.184s, episode steps: 31, steps per second: 168, episode reward: 21.587, mean reward: 0.696 [0.646, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.525], loss: 0.001685, mae: 0.043957, mean_q: 1.305059
 47612/100000: episode: 952, duration: 0.181s, episode steps: 31, steps per second: 172, episode reward: 21.250, mean reward: 0.685 [0.580, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.627, 10.337], loss: 0.001715, mae: 0.044618, mean_q: 1.308278
 47637/100000: episode: 953, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 17.347, mean reward: 0.694 [0.574, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.475, 10.325], loss: 0.001631, mae: 0.044842, mean_q: 1.296930
 47651/100000: episode: 954, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 11.283, mean reward: 0.806 [0.759, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.586], loss: 0.001627, mae: 0.043285, mean_q: 1.309302
 47682/100000: episode: 955, duration: 0.183s, episode steps: 31, steps per second: 169, episode reward: 22.797, mean reward: 0.735 [0.651, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.340, 10.532], loss: 0.001588, mae: 0.043834, mean_q: 1.315329
 47723/100000: episode: 956, duration: 0.238s, episode steps: 41, steps per second: 173, episode reward: 32.215, mean reward: 0.786 [0.603, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.047, 10.429], loss: 0.001941, mae: 0.047796, mean_q: 1.316040
[Info] FALSIFICATION!
 47740/100000: episode: 957, duration: 0.365s, episode steps: 17, steps per second: 47, episode reward: 12.960, mean reward: 0.762 [0.639, 1.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.043, 10.444], loss: 0.001364, mae: 0.039675, mean_q: 1.319422
 47757/100000: episode: 958, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 12.186, mean reward: 0.717 [0.634, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-1.448, 10.357], loss: 0.001343, mae: 0.039682, mean_q: 1.310470
 47787/100000: episode: 959, duration: 0.174s, episode steps: 30, steps per second: 173, episode reward: 21.989, mean reward: 0.733 [0.661, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.468], loss: 0.001898, mae: 0.043825, mean_q: 1.317932
 47811/100000: episode: 960, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 19.192, mean reward: 0.800 [0.717, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.665], loss: 0.002047, mae: 0.045510, mean_q: 1.308054
 47819/100000: episode: 961, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 6.795, mean reward: 0.849 [0.798, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-1.114, 10.560], loss: 0.001693, mae: 0.044111, mean_q: 1.332254
 47850/100000: episode: 962, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 27.650, mean reward: 0.892 [0.742, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.776, 10.588], loss: 0.001711, mae: 0.045217, mean_q: 1.329916
 47875/100000: episode: 963, duration: 0.139s, episode steps: 25, steps per second: 180, episode reward: 20.509, mean reward: 0.820 [0.703, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.623, 10.611], loss: 0.001744, mae: 0.045321, mean_q: 1.330683
 47901/100000: episode: 964, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 19.019, mean reward: 0.731 [0.645, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.072, 10.401], loss: 0.001453, mae: 0.041244, mean_q: 1.328796
 47931/100000: episode: 965, duration: 0.166s, episode steps: 30, steps per second: 180, episode reward: 19.918, mean reward: 0.664 [0.499, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.084, 10.148], loss: 0.001658, mae: 0.044296, mean_q: 1.331316
 47957/100000: episode: 966, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 18.644, mean reward: 0.717 [0.659, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.296, 10.405], loss: 0.001602, mae: 0.043327, mean_q: 1.327993
 47965/100000: episode: 967, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 6.080, mean reward: 0.760 [0.703, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.563], loss: 0.001422, mae: 0.040231, mean_q: 1.315215
 47979/100000: episode: 968, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 11.103, mean reward: 0.793 [0.736, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-1.189, 10.525], loss: 0.001560, mae: 0.043935, mean_q: 1.336471
 48004/100000: episode: 969, duration: 0.154s, episode steps: 25, steps per second: 162, episode reward: 16.974, mean reward: 0.679 [0.556, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.816, 10.236], loss: 0.001604, mae: 0.041864, mean_q: 1.331011
 48035/100000: episode: 970, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 24.320, mean reward: 0.785 [0.720, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.436, 10.545], loss: 0.001549, mae: 0.043335, mean_q: 1.339110
 48059/100000: episode: 971, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 17.996, mean reward: 0.750 [0.704, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.708, 10.427], loss: 0.001274, mae: 0.039142, mean_q: 1.332639
 48090/100000: episode: 972, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 22.105, mean reward: 0.713 [0.625, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.402, 10.403], loss: 0.001538, mae: 0.042571, mean_q: 1.343068
 48120/100000: episode: 973, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 22.542, mean reward: 0.751 [0.603, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.627, 10.337], loss: 0.001643, mae: 0.040921, mean_q: 1.343568
 48161/100000: episode: 974, duration: 0.245s, episode steps: 41, steps per second: 167, episode reward: 28.080, mean reward: 0.685 [0.583, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.847, 10.330], loss: 0.001539, mae: 0.043019, mean_q: 1.345070
 48175/100000: episode: 975, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 12.957, mean reward: 0.926 [0.845, 0.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.117, 10.729], loss: 0.001425, mae: 0.040454, mean_q: 1.356866
 48200/100000: episode: 976, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 17.561, mean reward: 0.702 [0.633, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.424], loss: 0.001764, mae: 0.045196, mean_q: 1.349949
 48224/100000: episode: 977, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 16.847, mean reward: 0.702 [0.653, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.790, 10.395], loss: 0.001655, mae: 0.044467, mean_q: 1.347265
 48238/100000: episode: 978, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 11.487, mean reward: 0.821 [0.734, 0.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.309, 10.524], loss: 0.002245, mae: 0.047167, mean_q: 1.353472
 48246/100000: episode: 979, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 6.213, mean reward: 0.777 [0.745, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.546], loss: 0.001202, mae: 0.038452, mean_q: 1.357424
[Info] Complete ISplit Iteration
[Info] Levels: [1.383939, 1.4893281, 1.6858264]
[Info] Cond. Prob: [0.1, 0.1, 0.02]
[Info] Error Prob: 0.00020000000000000004

 48254/100000: episode: 980, duration: 4.559s, episode steps: 8, steps per second: 2, episode reward: 6.299, mean reward: 0.787 [0.753, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.309, 10.560], loss: 0.001307, mae: 0.039965, mean_q: 1.365016
 48354/100000: episode: 981, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.491, mean reward: 0.575 [0.509, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.666, 10.221], loss: 0.001531, mae: 0.042191, mean_q: 1.344027
 48454/100000: episode: 982, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 59.951, mean reward: 0.600 [0.512, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.495, 10.229], loss: 0.001758, mae: 0.045200, mean_q: 1.342335
 48554/100000: episode: 983, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.108, mean reward: 0.571 [0.503, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.645, 10.098], loss: 0.001418, mae: 0.041090, mean_q: 1.341226
 48654/100000: episode: 984, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 63.079, mean reward: 0.631 [0.517, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.808, 10.276], loss: 0.001550, mae: 0.042271, mean_q: 1.340868
 48754/100000: episode: 985, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 63.427, mean reward: 0.634 [0.535, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.852, 10.332], loss: 0.001408, mae: 0.041288, mean_q: 1.339707
 48854/100000: episode: 986, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.278, mean reward: 0.593 [0.514, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.933, 10.221], loss: 0.001572, mae: 0.042409, mean_q: 1.338555
 48954/100000: episode: 987, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.353, mean reward: 0.594 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.871, 10.204], loss: 0.001600, mae: 0.043384, mean_q: 1.333217
 49054/100000: episode: 988, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.402, mean reward: 0.584 [0.508, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.535, 10.098], loss: 0.001654, mae: 0.042862, mean_q: 1.331515
 49154/100000: episode: 989, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 59.777, mean reward: 0.598 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.606, 10.113], loss: 0.001771, mae: 0.043715, mean_q: 1.316900
 49254/100000: episode: 990, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.365, mean reward: 0.574 [0.501, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.367, 10.141], loss: 0.001706, mae: 0.043936, mean_q: 1.319783
 49354/100000: episode: 991, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.567, mean reward: 0.586 [0.505, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.621, 10.098], loss: 0.001535, mae: 0.042420, mean_q: 1.315444
 49454/100000: episode: 992, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.198, mean reward: 0.582 [0.510, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.392, 10.161], loss: 0.001678, mae: 0.043505, mean_q: 1.317919
 49554/100000: episode: 993, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.892, mean reward: 0.579 [0.499, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.083, 10.223], loss: 0.001622, mae: 0.042884, mean_q: 1.319751
 49654/100000: episode: 994, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 59.681, mean reward: 0.597 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.161, 10.318], loss: 0.001992, mae: 0.047083, mean_q: 1.314373
 49754/100000: episode: 995, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 59.417, mean reward: 0.594 [0.518, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.549, 10.098], loss: 0.001856, mae: 0.045298, mean_q: 1.312521
 49854/100000: episode: 996, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 57.487, mean reward: 0.575 [0.505, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.821, 10.195], loss: 0.001670, mae: 0.043542, mean_q: 1.311973
 49954/100000: episode: 997, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.852, mean reward: 0.579 [0.500, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.857, 10.351], loss: 0.001642, mae: 0.044402, mean_q: 1.299788
 50054/100000: episode: 998, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.221, mean reward: 0.582 [0.509, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.152, 10.267], loss: 0.001507, mae: 0.042226, mean_q: 1.308031
 50154/100000: episode: 999, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.064, mean reward: 0.581 [0.506, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.668, 10.155], loss: 0.001807, mae: 0.045527, mean_q: 1.300372
 50254/100000: episode: 1000, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.649, mean reward: 0.576 [0.501, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.289, 10.308], loss: 0.001574, mae: 0.042644, mean_q: 1.300677
 50354/100000: episode: 1001, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.716, mean reward: 0.607 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.175, 10.332], loss: 0.001739, mae: 0.043900, mean_q: 1.289694
 50454/100000: episode: 1002, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 59.106, mean reward: 0.591 [0.507, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.156, 10.175], loss: 0.001677, mae: 0.043139, mean_q: 1.295378
 50554/100000: episode: 1003, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.474, mean reward: 0.595 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.091, 10.098], loss: 0.001554, mae: 0.042511, mean_q: 1.287608
 50654/100000: episode: 1004, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.564, mean reward: 0.586 [0.510, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.054, 10.137], loss: 0.001900, mae: 0.045869, mean_q: 1.285062
 50754/100000: episode: 1005, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.020, mean reward: 0.570 [0.512, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.431, 10.098], loss: 0.001762, mae: 0.044286, mean_q: 1.286326
 50854/100000: episode: 1006, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 59.363, mean reward: 0.594 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.126, 10.098], loss: 0.001550, mae: 0.041817, mean_q: 1.280165
 50954/100000: episode: 1007, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 59.753, mean reward: 0.598 [0.507, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.408, 10.098], loss: 0.001599, mae: 0.042513, mean_q: 1.280055
 51054/100000: episode: 1008, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.605, mean reward: 0.586 [0.509, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.768, 10.098], loss: 0.001676, mae: 0.044279, mean_q: 1.265039
 51154/100000: episode: 1009, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.794, mean reward: 0.588 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.379, 10.425], loss: 0.001810, mae: 0.044540, mean_q: 1.262549
 51254/100000: episode: 1010, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 58.391, mean reward: 0.584 [0.501, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.463, 10.305], loss: 0.001803, mae: 0.044818, mean_q: 1.259449
 51354/100000: episode: 1011, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.148, mean reward: 0.581 [0.503, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.065, 10.098], loss: 0.001663, mae: 0.043711, mean_q: 1.256375
 51454/100000: episode: 1012, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.002, mean reward: 0.580 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.568, 10.098], loss: 0.001583, mae: 0.043144, mean_q: 1.246889
 51554/100000: episode: 1013, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.923, mean reward: 0.589 [0.508, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.246, 10.098], loss: 0.001814, mae: 0.043800, mean_q: 1.246902
 51654/100000: episode: 1014, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.690, mean reward: 0.597 [0.501, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.563, 10.331], loss: 0.001542, mae: 0.041569, mean_q: 1.234645
 51754/100000: episode: 1015, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 58.032, mean reward: 0.580 [0.500, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.946, 10.185], loss: 0.001912, mae: 0.047024, mean_q: 1.242933
 51854/100000: episode: 1016, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.278, mean reward: 0.583 [0.506, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.262, 10.108], loss: 0.001673, mae: 0.043146, mean_q: 1.231475
 51954/100000: episode: 1017, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 60.245, mean reward: 0.602 [0.499, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.211, 10.098], loss: 0.001661, mae: 0.044082, mean_q: 1.233397
 52054/100000: episode: 1018, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 57.036, mean reward: 0.570 [0.507, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.300, 10.098], loss: 0.001803, mae: 0.043942, mean_q: 1.221494
 52154/100000: episode: 1019, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 61.413, mean reward: 0.614 [0.504, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.326, 10.098], loss: 0.001712, mae: 0.044238, mean_q: 1.225418
 52254/100000: episode: 1020, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.116, mean reward: 0.581 [0.509, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.801, 10.098], loss: 0.001536, mae: 0.041849, mean_q: 1.217651
 52354/100000: episode: 1021, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 59.895, mean reward: 0.599 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.732, 10.098], loss: 0.001711, mae: 0.044484, mean_q: 1.210981
 52454/100000: episode: 1022, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.627, mean reward: 0.576 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.287, 10.098], loss: 0.001667, mae: 0.043390, mean_q: 1.207088
 52554/100000: episode: 1023, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.087, mean reward: 0.581 [0.503, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.477, 10.173], loss: 0.001647, mae: 0.043026, mean_q: 1.206755
 52654/100000: episode: 1024, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.733, mean reward: 0.577 [0.501, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.954, 10.098], loss: 0.001498, mae: 0.042067, mean_q: 1.195821
 52754/100000: episode: 1025, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 60.858, mean reward: 0.609 [0.508, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.402, 10.305], loss: 0.001371, mae: 0.040468, mean_q: 1.188601
 52854/100000: episode: 1026, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.425, mean reward: 0.604 [0.509, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.655, 10.150], loss: 0.001426, mae: 0.040574, mean_q: 1.182971
 52954/100000: episode: 1027, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 59.442, mean reward: 0.594 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.919, 10.098], loss: 0.001596, mae: 0.042614, mean_q: 1.178430
 53054/100000: episode: 1028, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.349, mean reward: 0.583 [0.505, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.505, 10.139], loss: 0.001519, mae: 0.041985, mean_q: 1.172846
 53154/100000: episode: 1029, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 58.976, mean reward: 0.590 [0.509, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.787, 10.193], loss: 0.001481, mae: 0.042095, mean_q: 1.169190
 53254/100000: episode: 1030, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 59.726, mean reward: 0.597 [0.507, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.567, 10.129], loss: 0.001367, mae: 0.040229, mean_q: 1.158727
 53354/100000: episode: 1031, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 60.104, mean reward: 0.601 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.448, 10.114], loss: 0.001366, mae: 0.040070, mean_q: 1.166595
 53454/100000: episode: 1032, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 58.041, mean reward: 0.580 [0.500, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.907, 10.172], loss: 0.001420, mae: 0.041172, mean_q: 1.167371
 53554/100000: episode: 1033, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 62.172, mean reward: 0.622 [0.516, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.777, 10.098], loss: 0.001410, mae: 0.041219, mean_q: 1.167304
 53654/100000: episode: 1034, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 57.542, mean reward: 0.575 [0.503, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.755, 10.213], loss: 0.001457, mae: 0.041069, mean_q: 1.169242
 53754/100000: episode: 1035, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.610, mean reward: 0.576 [0.503, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.972, 10.098], loss: 0.001496, mae: 0.042456, mean_q: 1.165773
 53854/100000: episode: 1036, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.754, mean reward: 0.608 [0.509, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.523, 10.128], loss: 0.001334, mae: 0.039908, mean_q: 1.166945
 53954/100000: episode: 1037, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.895, mean reward: 0.589 [0.506, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.588, 10.322], loss: 0.001452, mae: 0.041311, mean_q: 1.165545
 54054/100000: episode: 1038, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.624, mean reward: 0.586 [0.501, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.934, 10.223], loss: 0.001583, mae: 0.042692, mean_q: 1.163012
 54154/100000: episode: 1039, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 56.185, mean reward: 0.562 [0.505, 0.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.781, 10.196], loss: 0.001325, mae: 0.039364, mean_q: 1.162496
 54254/100000: episode: 1040, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.663, mean reward: 0.587 [0.508, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.267, 10.098], loss: 0.001542, mae: 0.043116, mean_q: 1.164819
 54354/100000: episode: 1041, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 62.176, mean reward: 0.622 [0.504, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.016, 10.225], loss: 0.001377, mae: 0.040412, mean_q: 1.163162
 54454/100000: episode: 1042, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 60.422, mean reward: 0.604 [0.522, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.069, 10.136], loss: 0.001411, mae: 0.041382, mean_q: 1.162549
 54554/100000: episode: 1043, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.656, mean reward: 0.577 [0.499, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.341, 10.178], loss: 0.001345, mae: 0.039997, mean_q: 1.165597
 54654/100000: episode: 1044, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.706, mean reward: 0.577 [0.502, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.731, 10.098], loss: 0.001447, mae: 0.041386, mean_q: 1.167802
 54754/100000: episode: 1045, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.163, mean reward: 0.592 [0.501, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.516, 10.098], loss: 0.001482, mae: 0.041787, mean_q: 1.162870
 54854/100000: episode: 1046, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 59.826, mean reward: 0.598 [0.511, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.294, 10.408], loss: 0.001488, mae: 0.042225, mean_q: 1.165467
 54954/100000: episode: 1047, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.662, mean reward: 0.577 [0.511, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.198, 10.102], loss: 0.001536, mae: 0.042289, mean_q: 1.168141
 55054/100000: episode: 1048, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 57.970, mean reward: 0.580 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.567, 10.360], loss: 0.001509, mae: 0.042057, mean_q: 1.166673
 55154/100000: episode: 1049, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 58.497, mean reward: 0.585 [0.511, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.905, 10.190], loss: 0.001387, mae: 0.039775, mean_q: 1.166276
 55254/100000: episode: 1050, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.150, mean reward: 0.571 [0.500, 0.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.422, 10.222], loss: 0.001487, mae: 0.041571, mean_q: 1.168504
 55354/100000: episode: 1051, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: 58.506, mean reward: 0.585 [0.500, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.449, 10.098], loss: 0.001463, mae: 0.042012, mean_q: 1.163346
 55454/100000: episode: 1052, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.525, mean reward: 0.585 [0.509, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.677, 10.213], loss: 0.001434, mae: 0.040946, mean_q: 1.165427
 55554/100000: episode: 1053, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.709, mean reward: 0.587 [0.499, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.420, 10.098], loss: 0.001449, mae: 0.041196, mean_q: 1.166632
 55654/100000: episode: 1054, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 60.749, mean reward: 0.607 [0.517, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.240, 10.098], loss: 0.001456, mae: 0.041289, mean_q: 1.158884
 55754/100000: episode: 1055, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 60.027, mean reward: 0.600 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.349, 10.098], loss: 0.001386, mae: 0.040331, mean_q: 1.161964
 55854/100000: episode: 1056, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 57.893, mean reward: 0.579 [0.508, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.083, 10.232], loss: 0.001360, mae: 0.040047, mean_q: 1.165448
 55954/100000: episode: 1057, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 62.186, mean reward: 0.622 [0.503, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.518, 10.098], loss: 0.001519, mae: 0.041802, mean_q: 1.164375
 56054/100000: episode: 1058, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 59.200, mean reward: 0.592 [0.514, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.829, 10.261], loss: 0.001422, mae: 0.040483, mean_q: 1.163337
 56154/100000: episode: 1059, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 58.661, mean reward: 0.587 [0.509, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.945, 10.098], loss: 0.001441, mae: 0.041313, mean_q: 1.165162
 56254/100000: episode: 1060, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 60.924, mean reward: 0.609 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.494, 10.098], loss: 0.001479, mae: 0.041147, mean_q: 1.166566
 56354/100000: episode: 1061, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 60.086, mean reward: 0.601 [0.513, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.456, 10.098], loss: 0.001502, mae: 0.041996, mean_q: 1.165798
 56454/100000: episode: 1062, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.677, mean reward: 0.587 [0.498, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.837, 10.098], loss: 0.001277, mae: 0.039253, mean_q: 1.170146
 56554/100000: episode: 1063, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 59.390, mean reward: 0.594 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.261, 10.098], loss: 0.001493, mae: 0.041854, mean_q: 1.168172
 56654/100000: episode: 1064, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 57.729, mean reward: 0.577 [0.499, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.719, 10.116], loss: 0.001421, mae: 0.041115, mean_q: 1.171268
 56754/100000: episode: 1065, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 62.118, mean reward: 0.621 [0.503, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.064, 10.264], loss: 0.001470, mae: 0.041102, mean_q: 1.167940
 56854/100000: episode: 1066, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.639, mean reward: 0.576 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.188, 10.098], loss: 0.001498, mae: 0.041585, mean_q: 1.169892
 56954/100000: episode: 1067, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.814, mean reward: 0.598 [0.506, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.342, 10.250], loss: 0.001441, mae: 0.041150, mean_q: 1.169072
 57054/100000: episode: 1068, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.947, mean reward: 0.599 [0.501, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.556, 10.098], loss: 0.001415, mae: 0.040937, mean_q: 1.169438
 57154/100000: episode: 1069, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 58.321, mean reward: 0.583 [0.504, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.450, 10.343], loss: 0.001242, mae: 0.038655, mean_q: 1.167356
 57254/100000: episode: 1070, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.826, mean reward: 0.588 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.697, 10.098], loss: 0.001526, mae: 0.042098, mean_q: 1.168323
 57354/100000: episode: 1071, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 58.724, mean reward: 0.587 [0.506, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.936, 10.098], loss: 0.001411, mae: 0.041270, mean_q: 1.168388
 57454/100000: episode: 1072, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.872, mean reward: 0.579 [0.510, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.170, 10.239], loss: 0.001399, mae: 0.040923, mean_q: 1.167151
 57554/100000: episode: 1073, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 57.283, mean reward: 0.573 [0.501, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.512, 10.156], loss: 0.001451, mae: 0.041614, mean_q: 1.171156
 57654/100000: episode: 1074, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.264, mean reward: 0.573 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.891, 10.191], loss: 0.001398, mae: 0.040974, mean_q: 1.173538
 57754/100000: episode: 1075, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.673, mean reward: 0.587 [0.501, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.299, 10.268], loss: 0.001388, mae: 0.040541, mean_q: 1.170306
 57854/100000: episode: 1076, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 58.464, mean reward: 0.585 [0.506, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.450, 10.240], loss: 0.001483, mae: 0.042277, mean_q: 1.168749
 57954/100000: episode: 1077, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 63.971, mean reward: 0.640 [0.508, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.370, 10.514], loss: 0.001419, mae: 0.040566, mean_q: 1.167392
 58054/100000: episode: 1078, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 58.624, mean reward: 0.586 [0.511, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.596, 10.445], loss: 0.001404, mae: 0.041065, mean_q: 1.168231
 58154/100000: episode: 1079, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.967, mean reward: 0.590 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.492, 10.190], loss: 0.001485, mae: 0.041517, mean_q: 1.166945
[Info] 1-TH LEVEL FOUND: 1.337319254875183, Considering 10/90 traces
 58254/100000: episode: 1080, duration: 4.799s, episode steps: 100, steps per second: 21, episode reward: 58.970, mean reward: 0.590 [0.513, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.701, 10.117], loss: 0.001433, mae: 0.040698, mean_q: 1.166377
 58281/100000: episode: 1081, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 17.279, mean reward: 0.640 [0.552, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.382, 10.184], loss: 0.001196, mae: 0.038098, mean_q: 1.158933
 58314/100000: episode: 1082, duration: 0.193s, episode steps: 33, steps per second: 171, episode reward: 23.489, mean reward: 0.712 [0.661, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.679, 10.100], loss: 0.001448, mae: 0.040474, mean_q: 1.169480
 58322/100000: episode: 1083, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 5.375, mean reward: 0.672 [0.616, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.477, 10.100], loss: 0.001111, mae: 0.036291, mean_q: 1.171185
 58362/100000: episode: 1084, duration: 0.244s, episode steps: 40, steps per second: 164, episode reward: 27.059, mean reward: 0.676 [0.579, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.625, 10.316], loss: 0.001385, mae: 0.039675, mean_q: 1.171854
 58403/100000: episode: 1085, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 27.099, mean reward: 0.661 [0.515, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.795, 10.258], loss: 0.001141, mae: 0.036380, mean_q: 1.169822
 58450/100000: episode: 1086, duration: 0.269s, episode steps: 47, steps per second: 175, episode reward: 31.476, mean reward: 0.670 [0.508, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.325, 10.201], loss: 0.001294, mae: 0.038896, mean_q: 1.170712
 58497/100000: episode: 1087, duration: 0.264s, episode steps: 47, steps per second: 178, episode reward: 30.064, mean reward: 0.640 [0.564, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.320, 10.339], loss: 0.001463, mae: 0.041477, mean_q: 1.179245
 58537/100000: episode: 1088, duration: 0.233s, episode steps: 40, steps per second: 171, episode reward: 31.601, mean reward: 0.790 [0.683, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.172, 10.474], loss: 0.001455, mae: 0.040695, mean_q: 1.178701
 58584/100000: episode: 1089, duration: 0.260s, episode steps: 47, steps per second: 181, episode reward: 34.720, mean reward: 0.739 [0.647, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.317, 10.494], loss: 0.001461, mae: 0.041022, mean_q: 1.173309
 58597/100000: episode: 1090, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 9.144, mean reward: 0.703 [0.616, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.723, 10.100], loss: 0.001475, mae: 0.042877, mean_q: 1.174675
 58642/100000: episode: 1091, duration: 0.263s, episode steps: 45, steps per second: 171, episode reward: 29.989, mean reward: 0.666 [0.526, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.231, 10.281], loss: 0.001397, mae: 0.040512, mean_q: 1.176244
 58675/100000: episode: 1092, duration: 0.202s, episode steps: 33, steps per second: 164, episode reward: 21.822, mean reward: 0.661 [0.579, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.255, 10.100], loss: 0.001412, mae: 0.041012, mean_q: 1.178992
 58688/100000: episode: 1093, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 8.253, mean reward: 0.635 [0.588, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.210, 10.100], loss: 0.001316, mae: 0.039697, mean_q: 1.180752
 58696/100000: episode: 1094, duration: 0.056s, episode steps: 8, steps per second: 143, episode reward: 5.208, mean reward: 0.651 [0.606, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.422, 10.100], loss: 0.001707, mae: 0.042495, mean_q: 1.181797
 58709/100000: episode: 1095, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 9.458, mean reward: 0.728 [0.615, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.336, 10.100], loss: 0.001754, mae: 0.043912, mean_q: 1.167246
 58723/100000: episode: 1096, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 10.099, mean reward: 0.721 [0.647, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.227, 10.100], loss: 0.001607, mae: 0.044241, mean_q: 1.188952
 58764/100000: episode: 1097, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 24.807, mean reward: 0.605 [0.502, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.778, 10.221], loss: 0.001637, mae: 0.042596, mean_q: 1.177725
 58791/100000: episode: 1098, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 17.291, mean reward: 0.640 [0.569, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.234, 10.302], loss: 0.001614, mae: 0.041803, mean_q: 1.184245
 58804/100000: episode: 1099, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 8.628, mean reward: 0.664 [0.621, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.418, 10.100], loss: 0.001731, mae: 0.042475, mean_q: 1.181649
 58833/100000: episode: 1100, duration: 0.177s, episode steps: 29, steps per second: 163, episode reward: 18.706, mean reward: 0.645 [0.589, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.191, 10.100], loss: 0.002240, mae: 0.048173, mean_q: 1.188774
 58841/100000: episode: 1101, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 5.654, mean reward: 0.707 [0.632, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.369, 10.100], loss: 0.002092, mae: 0.045828, mean_q: 1.174397
 58868/100000: episode: 1102, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 18.228, mean reward: 0.675 [0.588, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.440, 10.392], loss: 0.001702, mae: 0.042660, mean_q: 1.177921
 58876/100000: episode: 1103, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 5.524, mean reward: 0.691 [0.643, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.727, 10.100], loss: 0.001502, mae: 0.040833, mean_q: 1.188444
 58909/100000: episode: 1104, duration: 0.188s, episode steps: 33, steps per second: 176, episode reward: 23.758, mean reward: 0.720 [0.623, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.365, 10.100], loss: 0.001555, mae: 0.041045, mean_q: 1.176664
 58938/100000: episode: 1105, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 18.949, mean reward: 0.653 [0.608, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.324, 10.100], loss: 0.001878, mae: 0.044239, mean_q: 1.176253
 58985/100000: episode: 1106, duration: 0.252s, episode steps: 47, steps per second: 187, episode reward: 32.996, mean reward: 0.702 [0.617, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.333, 10.459], loss: 0.002322, mae: 0.049961, mean_q: 1.183483
 58999/100000: episode: 1107, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 10.420, mean reward: 0.744 [0.696, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.065, 10.100], loss: 0.001492, mae: 0.041026, mean_q: 1.196287
 59032/100000: episode: 1108, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 24.785, mean reward: 0.751 [0.655, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.214, 10.100], loss: 0.001692, mae: 0.043630, mean_q: 1.186189
 59072/100000: episode: 1109, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 26.972, mean reward: 0.674 [0.538, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.557, 10.186], loss: 0.001694, mae: 0.043184, mean_q: 1.190894
 59119/100000: episode: 1110, duration: 0.266s, episode steps: 47, steps per second: 177, episode reward: 32.184, mean reward: 0.685 [0.582, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.336, 10.280], loss: 0.001621, mae: 0.043648, mean_q: 1.195353
 59152/100000: episode: 1111, duration: 0.200s, episode steps: 33, steps per second: 165, episode reward: 21.571, mean reward: 0.654 [0.532, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.338, 10.164], loss: 0.001651, mae: 0.042832, mean_q: 1.204652
 59197/100000: episode: 1112, duration: 0.256s, episode steps: 45, steps per second: 176, episode reward: 31.784, mean reward: 0.706 [0.656, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.938 [-0.674, 10.353], loss: 0.002115, mae: 0.046534, mean_q: 1.191985
 59238/100000: episode: 1113, duration: 0.242s, episode steps: 41, steps per second: 169, episode reward: 25.015, mean reward: 0.610 [0.515, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-1.187, 10.215], loss: 0.001879, mae: 0.043801, mean_q: 1.196377
 59271/100000: episode: 1114, duration: 0.186s, episode steps: 33, steps per second: 178, episode reward: 20.330, mean reward: 0.616 [0.510, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.558, 10.227], loss: 0.001361, mae: 0.038882, mean_q: 1.196336
 59284/100000: episode: 1115, duration: 0.089s, episode steps: 13, steps per second: 146, episode reward: 8.856, mean reward: 0.681 [0.648, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.479, 10.100], loss: 0.001409, mae: 0.039510, mean_q: 1.201235
 59324/100000: episode: 1116, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 27.839, mean reward: 0.696 [0.626, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.551, 10.352], loss: 0.001609, mae: 0.043003, mean_q: 1.198556
 59371/100000: episode: 1117, duration: 0.266s, episode steps: 47, steps per second: 176, episode reward: 28.630, mean reward: 0.609 [0.500, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.473, 10.100], loss: 0.001929, mae: 0.044914, mean_q: 1.191130
 59379/100000: episode: 1118, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 4.879, mean reward: 0.610 [0.573, 0.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.426, 10.100], loss: 0.001514, mae: 0.041325, mean_q: 1.181899
 59393/100000: episode: 1119, duration: 0.097s, episode steps: 14, steps per second: 145, episode reward: 10.144, mean reward: 0.725 [0.657, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.461, 10.100], loss: 0.001818, mae: 0.045539, mean_q: 1.197129
 59438/100000: episode: 1120, duration: 0.255s, episode steps: 45, steps per second: 177, episode reward: 30.130, mean reward: 0.670 [0.556, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.238, 10.232], loss: 0.001623, mae: 0.042436, mean_q: 1.197950
 59451/100000: episode: 1121, duration: 0.086s, episode steps: 13, steps per second: 152, episode reward: 8.880, mean reward: 0.683 [0.622, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.389, 10.100], loss: 0.001735, mae: 0.043752, mean_q: 1.192555
 59496/100000: episode: 1122, duration: 0.252s, episode steps: 45, steps per second: 179, episode reward: 27.847, mean reward: 0.619 [0.573, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-1.054, 10.349], loss: 0.001578, mae: 0.042250, mean_q: 1.207266
 59529/100000: episode: 1123, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 22.516, mean reward: 0.682 [0.614, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.398, 10.100], loss: 0.001698, mae: 0.042568, mean_q: 1.193534
 59562/100000: episode: 1124, duration: 0.200s, episode steps: 33, steps per second: 165, episode reward: 23.009, mean reward: 0.697 [0.630, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.881, 10.100], loss: 0.001599, mae: 0.042998, mean_q: 1.209898
 59609/100000: episode: 1125, duration: 0.257s, episode steps: 47, steps per second: 183, episode reward: 29.151, mean reward: 0.620 [0.508, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.322, 10.188], loss: 0.001660, mae: 0.043139, mean_q: 1.205646
 59622/100000: episode: 1126, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 9.272, mean reward: 0.713 [0.643, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.291, 10.100], loss: 0.001599, mae: 0.043022, mean_q: 1.195172
 59651/100000: episode: 1127, duration: 0.175s, episode steps: 29, steps per second: 166, episode reward: 20.919, mean reward: 0.721 [0.600, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.968, 10.100], loss: 0.001723, mae: 0.042487, mean_q: 1.202307
 59696/100000: episode: 1128, duration: 0.265s, episode steps: 45, steps per second: 170, episode reward: 28.362, mean reward: 0.630 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.227, 10.167], loss: 0.001776, mae: 0.045142, mean_q: 1.203337
 59729/100000: episode: 1129, duration: 0.198s, episode steps: 33, steps per second: 167, episode reward: 22.773, mean reward: 0.690 [0.538, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.232, 10.100], loss: 0.001451, mae: 0.040487, mean_q: 1.204628
 59737/100000: episode: 1130, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 4.923, mean reward: 0.615 [0.590, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.503, 10.100], loss: 0.001656, mae: 0.043989, mean_q: 1.216750
 59782/100000: episode: 1131, duration: 0.263s, episode steps: 45, steps per second: 171, episode reward: 28.031, mean reward: 0.623 [0.524, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-1.668, 10.100], loss: 0.001720, mae: 0.042788, mean_q: 1.209842
 59827/100000: episode: 1132, duration: 0.251s, episode steps: 45, steps per second: 179, episode reward: 30.538, mean reward: 0.679 [0.536, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.245, 10.220], loss: 0.001636, mae: 0.042618, mean_q: 1.202388
 59868/100000: episode: 1133, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 27.406, mean reward: 0.668 [0.508, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.569, 10.132], loss: 0.001834, mae: 0.044898, mean_q: 1.210249
 59908/100000: episode: 1134, duration: 0.242s, episode steps: 40, steps per second: 165, episode reward: 27.320, mean reward: 0.683 [0.576, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.815, 10.276], loss: 0.001748, mae: 0.043564, mean_q: 1.212164
 59948/100000: episode: 1135, duration: 0.239s, episode steps: 40, steps per second: 167, episode reward: 28.020, mean reward: 0.700 [0.615, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.509, 10.433], loss: 0.001476, mae: 0.040716, mean_q: 1.207333
 59977/100000: episode: 1136, duration: 0.167s, episode steps: 29, steps per second: 173, episode reward: 19.881, mean reward: 0.686 [0.576, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.257, 10.100], loss: 0.001608, mae: 0.043382, mean_q: 1.220230
 60004/100000: episode: 1137, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 17.385, mean reward: 0.644 [0.513, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.956, 10.123], loss: 0.001643, mae: 0.042754, mean_q: 1.210686
 60012/100000: episode: 1138, duration: 0.054s, episode steps: 8, steps per second: 147, episode reward: 5.331, mean reward: 0.666 [0.624, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-1.175, 10.100], loss: 0.001388, mae: 0.040538, mean_q: 1.212680
 60025/100000: episode: 1139, duration: 0.085s, episode steps: 13, steps per second: 152, episode reward: 8.699, mean reward: 0.669 [0.617, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.247, 10.100], loss: 0.001679, mae: 0.044128, mean_q: 1.218820
 60033/100000: episode: 1140, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 5.278, mean reward: 0.660 [0.608, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.423, 10.100], loss: 0.001667, mae: 0.042436, mean_q: 1.212158
 60047/100000: episode: 1141, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 10.465, mean reward: 0.747 [0.702, 0.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.692, 10.100], loss: 0.001310, mae: 0.038880, mean_q: 1.215073
 60092/100000: episode: 1142, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 32.141, mean reward: 0.714 [0.622, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-1.506, 10.527], loss: 0.001564, mae: 0.041575, mean_q: 1.223293
 60105/100000: episode: 1143, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 9.532, mean reward: 0.733 [0.650, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.341, 10.100], loss: 0.001573, mae: 0.040951, mean_q: 1.215282
 60138/100000: episode: 1144, duration: 0.203s, episode steps: 33, steps per second: 162, episode reward: 21.524, mean reward: 0.652 [0.544, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.658, 10.100], loss: 0.001831, mae: 0.044327, mean_q: 1.213685
 60171/100000: episode: 1145, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 21.266, mean reward: 0.644 [0.552, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.362, 10.100], loss: 0.001867, mae: 0.045021, mean_q: 1.216673
 60179/100000: episode: 1146, duration: 0.058s, episode steps: 8, steps per second: 139, episode reward: 5.572, mean reward: 0.697 [0.607, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.655, 10.100], loss: 0.002107, mae: 0.047748, mean_q: 1.211004
 60220/100000: episode: 1147, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 26.318, mean reward: 0.642 [0.589, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.130, 10.429], loss: 0.001783, mae: 0.044763, mean_q: 1.224423
 60267/100000: episode: 1148, duration: 0.259s, episode steps: 47, steps per second: 181, episode reward: 29.396, mean reward: 0.625 [0.520, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-1.150, 10.100], loss: 0.001572, mae: 0.042631, mean_q: 1.225460
 60294/100000: episode: 1149, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 16.428, mean reward: 0.608 [0.504, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.404, 10.102], loss: 0.001549, mae: 0.041240, mean_q: 1.222137
 60334/100000: episode: 1150, duration: 0.236s, episode steps: 40, steps per second: 170, episode reward: 23.915, mean reward: 0.598 [0.500, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.718, 10.224], loss: 0.001622, mae: 0.042866, mean_q: 1.229757
 60381/100000: episode: 1151, duration: 0.277s, episode steps: 47, steps per second: 170, episode reward: 37.838, mean reward: 0.805 [0.678, 0.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.880, 10.519], loss: 0.001706, mae: 0.044003, mean_q: 1.220146
 60394/100000: episode: 1152, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 8.330, mean reward: 0.641 [0.613, 0.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-1.001, 10.100], loss: 0.001583, mae: 0.042367, mean_q: 1.217133
 60407/100000: episode: 1153, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 9.761, mean reward: 0.751 [0.683, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.643, 10.100], loss: 0.001367, mae: 0.040002, mean_q: 1.221436
 60440/100000: episode: 1154, duration: 0.196s, episode steps: 33, steps per second: 168, episode reward: 23.574, mean reward: 0.714 [0.606, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.338, 10.100], loss: 0.002016, mae: 0.047916, mean_q: 1.223529
 60481/100000: episode: 1155, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 29.401, mean reward: 0.717 [0.654, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.673, 10.440], loss: 0.001654, mae: 0.043876, mean_q: 1.226176
 60514/100000: episode: 1156, duration: 0.197s, episode steps: 33, steps per second: 167, episode reward: 21.310, mean reward: 0.646 [0.562, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.887, 10.100], loss: 0.001783, mae: 0.044308, mean_q: 1.233888
 60527/100000: episode: 1157, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 8.942, mean reward: 0.688 [0.653, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.253, 10.100], loss: 0.002708, mae: 0.050440, mean_q: 1.222276
 60554/100000: episode: 1158, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 17.281, mean reward: 0.640 [0.598, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.364, 10.329], loss: 0.001934, mae: 0.046406, mean_q: 1.236161
 60601/100000: episode: 1159, duration: 0.255s, episode steps: 47, steps per second: 184, episode reward: 32.025, mean reward: 0.681 [0.568, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.996, 10.464], loss: 0.001809, mae: 0.044767, mean_q: 1.223279
 60628/100000: episode: 1160, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 18.652, mean reward: 0.691 [0.642, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.492, 10.511], loss: 0.001726, mae: 0.044978, mean_q: 1.235850
 60657/100000: episode: 1161, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 22.370, mean reward: 0.771 [0.604, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.365, 10.100], loss: 0.001428, mae: 0.041382, mean_q: 1.233634
 60690/100000: episode: 1162, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 21.655, mean reward: 0.656 [0.598, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.593, 10.100], loss: 0.001703, mae: 0.043889, mean_q: 1.230651
 60704/100000: episode: 1163, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 9.171, mean reward: 0.655 [0.623, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.315, 10.100], loss: 0.002086, mae: 0.048225, mean_q: 1.229752
 60749/100000: episode: 1164, duration: 0.246s, episode steps: 45, steps per second: 183, episode reward: 26.962, mean reward: 0.599 [0.509, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.403, 10.266], loss: 0.001733, mae: 0.044704, mean_q: 1.242260
 60762/100000: episode: 1165, duration: 0.088s, episode steps: 13, steps per second: 147, episode reward: 9.399, mean reward: 0.723 [0.641, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.384, 10.100], loss: 0.001567, mae: 0.042044, mean_q: 1.229496
 60807/100000: episode: 1166, duration: 0.264s, episode steps: 45, steps per second: 171, episode reward: 26.935, mean reward: 0.599 [0.500, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.690, 10.121], loss: 0.001867, mae: 0.045452, mean_q: 1.234021
 60815/100000: episode: 1167, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 5.581, mean reward: 0.698 [0.649, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.411, 10.100], loss: 0.001297, mae: 0.039665, mean_q: 1.241279
 60829/100000: episode: 1168, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 10.118, mean reward: 0.723 [0.696, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.560, 10.100], loss: 0.001638, mae: 0.044253, mean_q: 1.227796
 60837/100000: episode: 1169, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 5.187, mean reward: 0.648 [0.583, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.719, 10.100], loss: 0.001861, mae: 0.047352, mean_q: 1.233968
[Info] 2-TH LEVEL FOUND: 1.5286166667938232, Considering 10/90 traces
 60882/100000: episode: 1170, duration: 4.539s, episode steps: 45, steps per second: 10, episode reward: 29.211, mean reward: 0.649 [0.581, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.392, 10.297], loss: 0.001597, mae: 0.043009, mean_q: 1.240116
 60920/100000: episode: 1171, duration: 0.230s, episode steps: 38, steps per second: 165, episode reward: 26.201, mean reward: 0.690 [0.611, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.751, 10.300], loss: 0.001639, mae: 0.043030, mean_q: 1.247265
 60951/100000: episode: 1172, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 21.839, mean reward: 0.704 [0.578, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.510, 10.273], loss: 0.001335, mae: 0.039545, mean_q: 1.232070
 60989/100000: episode: 1173, duration: 0.220s, episode steps: 38, steps per second: 173, episode reward: 26.964, mean reward: 0.710 [0.588, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.200, 10.365], loss: 0.001652, mae: 0.043194, mean_q: 1.252464
 61020/100000: episode: 1174, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 22.338, mean reward: 0.721 [0.622, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.035, 10.408], loss: 0.001740, mae: 0.044245, mean_q: 1.244591
 61058/100000: episode: 1175, duration: 0.233s, episode steps: 38, steps per second: 163, episode reward: 24.804, mean reward: 0.653 [0.563, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.438, 10.329], loss: 0.001466, mae: 0.041832, mean_q: 1.248219
 61090/100000: episode: 1176, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 24.120, mean reward: 0.754 [0.632, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.660, 10.399], loss: 0.001725, mae: 0.043447, mean_q: 1.252494
 61123/100000: episode: 1177, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: 24.699, mean reward: 0.748 [0.627, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.478, 10.430], loss: 0.001763, mae: 0.044425, mean_q: 1.253761
 61154/100000: episode: 1178, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 24.081, mean reward: 0.777 [0.671, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.591, 10.472], loss: 0.001923, mae: 0.047012, mean_q: 1.239488
 61187/100000: episode: 1179, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 22.831, mean reward: 0.692 [0.567, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.600, 10.409], loss: 0.002003, mae: 0.046711, mean_q: 1.244817
 61222/100000: episode: 1180, duration: 0.202s, episode steps: 35, steps per second: 173, episode reward: 25.257, mean reward: 0.722 [0.618, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.059, 10.342], loss: 0.001587, mae: 0.043791, mean_q: 1.247915
 61260/100000: episode: 1181, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 26.587, mean reward: 0.700 [0.637, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.160, 10.382], loss: 0.001669, mae: 0.042979, mean_q: 1.252621
 61277/100000: episode: 1182, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 12.355, mean reward: 0.727 [0.661, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.157, 10.100], loss: 0.001588, mae: 0.042629, mean_q: 1.268963
 61315/100000: episode: 1183, duration: 0.209s, episode steps: 38, steps per second: 182, episode reward: 25.119, mean reward: 0.661 [0.563, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-1.700, 10.338], loss: 0.001693, mae: 0.044502, mean_q: 1.254024
 61337/100000: episode: 1184, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 18.109, mean reward: 0.823 [0.739, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.735, 10.100], loss: 0.001708, mae: 0.044205, mean_q: 1.232043
 61369/100000: episode: 1185, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 24.088, mean reward: 0.753 [0.686, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.721, 10.530], loss: 0.001884, mae: 0.045882, mean_q: 1.253509
 61404/100000: episode: 1186, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 27.636, mean reward: 0.790 [0.603, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.342, 10.485], loss: 0.001641, mae: 0.042451, mean_q: 1.250881
 61431/100000: episode: 1187, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 20.453, mean reward: 0.758 [0.602, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.779, 10.433], loss: 0.001495, mae: 0.041556, mean_q: 1.266380
 61453/100000: episode: 1188, duration: 0.136s, episode steps: 22, steps per second: 161, episode reward: 16.982, mean reward: 0.772 [0.714, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.276, 10.100], loss: 0.001444, mae: 0.041345, mean_q: 1.253363
 61485/100000: episode: 1189, duration: 0.178s, episode steps: 32, steps per second: 179, episode reward: 23.398, mean reward: 0.731 [0.541, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.644, 10.230], loss: 0.001753, mae: 0.046778, mean_q: 1.265921
 61520/100000: episode: 1190, duration: 0.218s, episode steps: 35, steps per second: 160, episode reward: 25.424, mean reward: 0.726 [0.650, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.038, 10.507], loss: 0.001729, mae: 0.046192, mean_q: 1.263900
 61547/100000: episode: 1191, duration: 0.159s, episode steps: 27, steps per second: 169, episode reward: 20.004, mean reward: 0.741 [0.631, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.237, 10.500], loss: 0.001722, mae: 0.044232, mean_q: 1.259919
 61582/100000: episode: 1192, duration: 0.203s, episode steps: 35, steps per second: 172, episode reward: 28.211, mean reward: 0.806 [0.705, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.854, 10.433], loss: 0.001958, mae: 0.046661, mean_q: 1.260944
 61613/100000: episode: 1193, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 24.296, mean reward: 0.784 [0.720, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.437, 10.450], loss: 0.001556, mae: 0.042850, mean_q: 1.272282
 61651/100000: episode: 1194, duration: 0.220s, episode steps: 38, steps per second: 173, episode reward: 25.405, mean reward: 0.669 [0.587, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-1.370, 10.296], loss: 0.001545, mae: 0.043060, mean_q: 1.266333
 61668/100000: episode: 1195, duration: 0.106s, episode steps: 17, steps per second: 161, episode reward: 12.593, mean reward: 0.741 [0.647, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.584, 10.100], loss: 0.001487, mae: 0.041623, mean_q: 1.284878
 61685/100000: episode: 1196, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 14.050, mean reward: 0.826 [0.690, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.960, 10.100], loss: 0.001694, mae: 0.043676, mean_q: 1.268556
 61712/100000: episode: 1197, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 18.037, mean reward: 0.668 [0.536, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.783, 10.103], loss: 0.001641, mae: 0.042607, mean_q: 1.285514
 61744/100000: episode: 1198, duration: 0.199s, episode steps: 32, steps per second: 160, episode reward: 23.193, mean reward: 0.725 [0.603, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.424, 10.235], loss: 0.001639, mae: 0.044573, mean_q: 1.281653
 61776/100000: episode: 1199, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 22.344, mean reward: 0.698 [0.593, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.384], loss: 0.001757, mae: 0.045844, mean_q: 1.266538
 61811/100000: episode: 1200, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 24.098, mean reward: 0.689 [0.557, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.175, 10.267], loss: 0.001888, mae: 0.045807, mean_q: 1.271132
 61828/100000: episode: 1201, duration: 0.111s, episode steps: 17, steps per second: 153, episode reward: 13.075, mean reward: 0.769 [0.696, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.704, 10.100], loss: 0.001613, mae: 0.044228, mean_q: 1.281249
 61863/100000: episode: 1202, duration: 0.197s, episode steps: 35, steps per second: 177, episode reward: 28.113, mean reward: 0.803 [0.708, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.010, 10.634], loss: 0.001567, mae: 0.042127, mean_q: 1.286768
 61898/100000: episode: 1203, duration: 0.212s, episode steps: 35, steps per second: 165, episode reward: 25.306, mean reward: 0.723 [0.590, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.331, 10.320], loss: 0.001501, mae: 0.042396, mean_q: 1.280629
 61933/100000: episode: 1204, duration: 0.207s, episode steps: 35, steps per second: 169, episode reward: 27.255, mean reward: 0.779 [0.691, 0.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.540], loss: 0.001613, mae: 0.041395, mean_q: 1.291893
 61960/100000: episode: 1205, duration: 0.176s, episode steps: 27, steps per second: 154, episode reward: 19.876, mean reward: 0.736 [0.591, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.792, 10.507], loss: 0.001618, mae: 0.044066, mean_q: 1.291061
 61982/100000: episode: 1206, duration: 0.138s, episode steps: 22, steps per second: 160, episode reward: 16.676, mean reward: 0.758 [0.661, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.026, 10.100], loss: 0.001627, mae: 0.044467, mean_q: 1.282787
 62015/100000: episode: 1207, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 23.794, mean reward: 0.721 [0.584, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.220, 10.322], loss: 0.001589, mae: 0.043864, mean_q: 1.280888
 62032/100000: episode: 1208, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 12.749, mean reward: 0.750 [0.697, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.342, 10.100], loss: 0.001614, mae: 0.041413, mean_q: 1.315776
 62065/100000: episode: 1209, duration: 0.190s, episode steps: 33, steps per second: 173, episode reward: 22.322, mean reward: 0.676 [0.519, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.106, 10.218], loss: 0.001723, mae: 0.044451, mean_q: 1.299463
 62098/100000: episode: 1210, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 24.935, mean reward: 0.756 [0.652, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.035, 10.398], loss: 0.001773, mae: 0.045254, mean_q: 1.293695
 62127/100000: episode: 1211, duration: 0.175s, episode steps: 29, steps per second: 166, episode reward: 17.820, mean reward: 0.614 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.360, 10.173], loss: 0.001503, mae: 0.041579, mean_q: 1.290307
 62162/100000: episode: 1212, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 23.710, mean reward: 0.677 [0.581, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.143, 10.304], loss: 0.001495, mae: 0.042304, mean_q: 1.308177
 62179/100000: episode: 1213, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 12.201, mean reward: 0.718 [0.644, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.440, 10.100], loss: 0.001563, mae: 0.042772, mean_q: 1.306794
 62206/100000: episode: 1214, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 21.432, mean reward: 0.794 [0.681, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.375, 10.397], loss: 0.001549, mae: 0.042274, mean_q: 1.288468
 62241/100000: episode: 1215, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 22.840, mean reward: 0.653 [0.503, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.217, 10.161], loss: 0.001515, mae: 0.041921, mean_q: 1.297751
 62268/100000: episode: 1216, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 19.014, mean reward: 0.704 [0.615, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.518, 10.370], loss: 0.001616, mae: 0.043875, mean_q: 1.297032
 62300/100000: episode: 1217, duration: 0.186s, episode steps: 32, steps per second: 172, episode reward: 20.671, mean reward: 0.646 [0.515, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.145, 10.115], loss: 0.001777, mae: 0.046418, mean_q: 1.318032
 62317/100000: episode: 1218, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 12.814, mean reward: 0.754 [0.683, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.248, 10.100], loss: 0.001413, mae: 0.040639, mean_q: 1.301390
 62346/100000: episode: 1219, duration: 0.166s, episode steps: 29, steps per second: 174, episode reward: 19.965, mean reward: 0.688 [0.510, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.364, 10.143], loss: 0.001593, mae: 0.043219, mean_q: 1.311486
[Info] FALSIFICATION!
 62353/100000: episode: 1220, duration: 0.290s, episode steps: 7, steps per second: 24, episode reward: 6.263, mean reward: 0.895 [0.809, 1.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-0.019, 9.186], loss: 0.001451, mae: 0.041666, mean_q: 1.328378
 62388/100000: episode: 1221, duration: 0.206s, episode steps: 35, steps per second: 170, episode reward: 23.370, mean reward: 0.668 [0.539, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.404, 10.234], loss: 0.001830, mae: 0.043974, mean_q: 1.301912
 62423/100000: episode: 1222, duration: 0.202s, episode steps: 35, steps per second: 173, episode reward: 25.240, mean reward: 0.721 [0.512, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.377, 10.178], loss: 0.001993, mae: 0.044824, mean_q: 1.314726
 62454/100000: episode: 1223, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 26.477, mean reward: 0.854 [0.785, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.961, 10.527], loss: 0.001746, mae: 0.045142, mean_q: 1.297578
 62476/100000: episode: 1224, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 16.652, mean reward: 0.757 [0.693, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.280, 10.100], loss: 0.001671, mae: 0.046181, mean_q: 1.310912
 62511/100000: episode: 1225, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 27.256, mean reward: 0.779 [0.700, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.865, 10.453], loss: 0.001737, mae: 0.043165, mean_q: 1.312505
 62542/100000: episode: 1226, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 23.610, mean reward: 0.762 [0.622, 0.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.286, 10.394], loss: 0.001528, mae: 0.042889, mean_q: 1.317372
 62577/100000: episode: 1227, duration: 0.196s, episode steps: 35, steps per second: 178, episode reward: 27.459, mean reward: 0.785 [0.700, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.292, 10.505], loss: 0.001804, mae: 0.044822, mean_q: 1.310251
 62599/100000: episode: 1228, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 17.121, mean reward: 0.778 [0.702, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.299, 10.100], loss: 0.001829, mae: 0.046095, mean_q: 1.325580
 62631/100000: episode: 1229, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 24.291, mean reward: 0.759 [0.663, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.202, 10.436], loss: 0.001532, mae: 0.042917, mean_q: 1.315721
 62648/100000: episode: 1230, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 12.638, mean reward: 0.743 [0.701, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.366, 10.100], loss: 0.001553, mae: 0.043195, mean_q: 1.325884
 62681/100000: episode: 1231, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 23.607, mean reward: 0.715 [0.565, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.105, 10.222], loss: 0.001733, mae: 0.045873, mean_q: 1.323929
 62716/100000: episode: 1232, duration: 0.196s, episode steps: 35, steps per second: 178, episode reward: 22.884, mean reward: 0.654 [0.509, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.340, 10.109], loss: 0.001462, mae: 0.042311, mean_q: 1.327699
 62743/100000: episode: 1233, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 19.572, mean reward: 0.725 [0.655, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.568, 10.533], loss: 0.001496, mae: 0.042307, mean_q: 1.320172
 62770/100000: episode: 1234, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 19.677, mean reward: 0.729 [0.563, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.753, 10.346], loss: 0.001460, mae: 0.041066, mean_q: 1.336328
 62792/100000: episode: 1235, duration: 0.137s, episode steps: 22, steps per second: 160, episode reward: 16.198, mean reward: 0.736 [0.669, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.603, 10.100], loss: 0.001274, mae: 0.039157, mean_q: 1.321748
 62825/100000: episode: 1236, duration: 0.202s, episode steps: 33, steps per second: 164, episode reward: 23.619, mean reward: 0.716 [0.621, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.400, 10.390], loss: 0.001678, mae: 0.043912, mean_q: 1.330008
 62858/100000: episode: 1237, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 26.322, mean reward: 0.798 [0.723, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.550, 10.598], loss: 0.001552, mae: 0.043074, mean_q: 1.331545
 62893/100000: episode: 1238, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 25.449, mean reward: 0.727 [0.546, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.228, 10.248], loss: 0.001500, mae: 0.043380, mean_q: 1.339380
 62926/100000: episode: 1239, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 22.305, mean reward: 0.676 [0.520, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.050, 10.179], loss: 0.001626, mae: 0.044320, mean_q: 1.339172
 62959/100000: episode: 1240, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 24.027, mean reward: 0.728 [0.672, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.681, 10.450], loss: 0.001301, mae: 0.039682, mean_q: 1.333307
 62981/100000: episode: 1241, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 17.289, mean reward: 0.786 [0.675, 0.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.473, 10.100], loss: 0.001415, mae: 0.041270, mean_q: 1.329418
 63003/100000: episode: 1242, duration: 0.125s, episode steps: 22, steps per second: 177, episode reward: 17.677, mean reward: 0.803 [0.752, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.956, 10.100], loss: 0.001465, mae: 0.042161, mean_q: 1.348548
 63025/100000: episode: 1243, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 16.863, mean reward: 0.766 [0.688, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.263, 10.100], loss: 0.001646, mae: 0.044560, mean_q: 1.332336
 63058/100000: episode: 1244, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 25.229, mean reward: 0.765 [0.674, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.349, 10.499], loss: 0.001370, mae: 0.040648, mean_q: 1.349288
 63093/100000: episode: 1245, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 25.043, mean reward: 0.716 [0.565, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.082, 10.293], loss: 0.001823, mae: 0.047529, mean_q: 1.350058
 63110/100000: episode: 1246, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 12.077, mean reward: 0.710 [0.635, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.944, 10.100], loss: 0.001520, mae: 0.043317, mean_q: 1.356579
 63139/100000: episode: 1247, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 20.447, mean reward: 0.705 [0.590, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.250, 10.293], loss: 0.001331, mae: 0.040256, mean_q: 1.355560
 63177/100000: episode: 1248, duration: 0.214s, episode steps: 38, steps per second: 177, episode reward: 25.444, mean reward: 0.670 [0.522, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-1.385, 10.214], loss: 0.001359, mae: 0.040861, mean_q: 1.345692
 63194/100000: episode: 1249, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 11.883, mean reward: 0.699 [0.640, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.329, 10.100], loss: 0.001302, mae: 0.040084, mean_q: 1.349519
 63229/100000: episode: 1250, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 26.244, mean reward: 0.750 [0.616, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.109, 10.437], loss: 0.001777, mae: 0.043798, mean_q: 1.333947
 63260/100000: episode: 1251, duration: 0.183s, episode steps: 31, steps per second: 169, episode reward: 23.185, mean reward: 0.748 [0.667, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.491, 10.448], loss: 0.001507, mae: 0.042948, mean_q: 1.351022
 63293/100000: episode: 1252, duration: 0.163s, episode steps: 33, steps per second: 203, episode reward: 25.169, mean reward: 0.763 [0.631, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.041, 10.355], loss: 0.001595, mae: 0.042896, mean_q: 1.355934
 63331/100000: episode: 1253, duration: 0.220s, episode steps: 38, steps per second: 173, episode reward: 28.520, mean reward: 0.751 [0.660, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.453, 10.433], loss: 0.001397, mae: 0.042479, mean_q: 1.352535
 63362/100000: episode: 1254, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 22.146, mean reward: 0.714 [0.528, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.035, 10.160], loss: 0.001534, mae: 0.043478, mean_q: 1.346787
 63394/100000: episode: 1255, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 25.326, mean reward: 0.791 [0.729, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.276, 10.552], loss: 0.001400, mae: 0.041313, mean_q: 1.344869
 63432/100000: episode: 1256, duration: 0.238s, episode steps: 38, steps per second: 159, episode reward: 27.511, mean reward: 0.724 [0.626, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.161, 10.448], loss: 0.001477, mae: 0.043643, mean_q: 1.356027
 63464/100000: episode: 1257, duration: 0.178s, episode steps: 32, steps per second: 180, episode reward: 23.752, mean reward: 0.742 [0.586, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.055, 10.275], loss: 0.001366, mae: 0.040598, mean_q: 1.358478
 63496/100000: episode: 1258, duration: 0.191s, episode steps: 32, steps per second: 168, episode reward: 22.533, mean reward: 0.704 [0.584, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.657, 10.321], loss: 0.001470, mae: 0.042924, mean_q: 1.353845
 63528/100000: episode: 1259, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 23.810, mean reward: 0.744 [0.632, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.040, 10.372], loss: 0.001443, mae: 0.043032, mean_q: 1.350438
[Info] Complete ISplit Iteration
[Info] Levels: [1.3373193, 1.5286167, 1.7493721]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 63555/100000: episode: 1260, duration: 7.074s, episode steps: 27, steps per second: 4, episode reward: 18.740, mean reward: 0.694 [0.605, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.155, 10.494], loss: 0.001357, mae: 0.040666, mean_q: 1.340270
 63655/100000: episode: 1261, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 56.468, mean reward: 0.565 [0.502, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.265, 10.189], loss: 0.001398, mae: 0.041393, mean_q: 1.357463
 63755/100000: episode: 1262, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: 58.647, mean reward: 0.586 [0.504, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.667, 10.214], loss: 0.001416, mae: 0.041967, mean_q: 1.343759
 63855/100000: episode: 1263, duration: 1.120s, episode steps: 100, steps per second: 89, episode reward: 57.973, mean reward: 0.580 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.531, 10.178], loss: 0.001580, mae: 0.043444, mean_q: 1.350965
 63955/100000: episode: 1264, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: 57.755, mean reward: 0.578 [0.501, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.040, 10.098], loss: 0.001549, mae: 0.043191, mean_q: 1.338066
 64055/100000: episode: 1265, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 65.760, mean reward: 0.658 [0.510, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.018, 10.098], loss: 0.001736, mae: 0.044025, mean_q: 1.336807
 64155/100000: episode: 1266, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 58.382, mean reward: 0.584 [0.503, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.499, 10.098], loss: 0.001493, mae: 0.041566, mean_q: 1.341081
 64255/100000: episode: 1267, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 57.831, mean reward: 0.578 [0.502, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.930, 10.307], loss: 0.001342, mae: 0.040264, mean_q: 1.335986
 64355/100000: episode: 1268, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 57.093, mean reward: 0.571 [0.506, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.841, 10.098], loss: 0.001473, mae: 0.041667, mean_q: 1.331547
 64455/100000: episode: 1269, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 57.058, mean reward: 0.571 [0.507, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.727, 10.158], loss: 0.001624, mae: 0.044141, mean_q: 1.324306
 64555/100000: episode: 1270, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: 60.346, mean reward: 0.603 [0.509, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.430, 10.260], loss: 0.001367, mae: 0.040493, mean_q: 1.329332
 64655/100000: episode: 1271, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: 63.168, mean reward: 0.632 [0.508, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.461, 10.098], loss: 0.001651, mae: 0.043821, mean_q: 1.330703
 64755/100000: episode: 1272, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 57.134, mean reward: 0.571 [0.500, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.545, 10.098], loss: 0.001534, mae: 0.042323, mean_q: 1.335886
 64855/100000: episode: 1273, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 59.019, mean reward: 0.590 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.992, 10.237], loss: 0.001545, mae: 0.043169, mean_q: 1.322420
 64955/100000: episode: 1274, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 58.827, mean reward: 0.588 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.033, 10.139], loss: 0.001759, mae: 0.044414, mean_q: 1.320953
 65055/100000: episode: 1275, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 61.165, mean reward: 0.612 [0.506, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.702, 10.098], loss: 0.001832, mae: 0.043562, mean_q: 1.309313
 65155/100000: episode: 1276, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 59.260, mean reward: 0.593 [0.515, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.026, 10.215], loss: 0.001840, mae: 0.043816, mean_q: 1.322659
 65255/100000: episode: 1277, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 57.852, mean reward: 0.579 [0.500, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.463, 10.327], loss: 0.001661, mae: 0.042687, mean_q: 1.312750
 65355/100000: episode: 1278, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: 58.210, mean reward: 0.582 [0.511, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.267, 10.310], loss: 0.001841, mae: 0.044739, mean_q: 1.301121
 65455/100000: episode: 1279, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 59.043, mean reward: 0.590 [0.502, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.992, 10.238], loss: 0.001756, mae: 0.043791, mean_q: 1.304318
 65555/100000: episode: 1280, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 58.944, mean reward: 0.589 [0.508, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.927, 10.194], loss: 0.001624, mae: 0.042954, mean_q: 1.299511
 65655/100000: episode: 1281, duration: 0.979s, episode steps: 100, steps per second: 102, episode reward: 59.129, mean reward: 0.591 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.054, 10.179], loss: 0.001520, mae: 0.041987, mean_q: 1.298779
 65755/100000: episode: 1282, duration: 0.857s, episode steps: 100, steps per second: 117, episode reward: 58.057, mean reward: 0.581 [0.499, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.357, 10.152], loss: 0.001810, mae: 0.044613, mean_q: 1.296535
 65855/100000: episode: 1283, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: 58.888, mean reward: 0.589 [0.506, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.200, 10.305], loss: 0.001667, mae: 0.043390, mean_q: 1.290835
 65955/100000: episode: 1284, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 58.304, mean reward: 0.583 [0.497, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.652, 10.128], loss: 0.001680, mae: 0.042173, mean_q: 1.288188
 66055/100000: episode: 1285, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: 58.000, mean reward: 0.580 [0.506, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.206, 10.125], loss: 0.001882, mae: 0.045037, mean_q: 1.285051
 66155/100000: episode: 1286, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 57.936, mean reward: 0.579 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.160, 10.177], loss: 0.001906, mae: 0.044754, mean_q: 1.275809
 66255/100000: episode: 1287, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.528, mean reward: 0.595 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.642, 10.098], loss: 0.001825, mae: 0.043680, mean_q: 1.279507
 66355/100000: episode: 1288, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: 58.785, mean reward: 0.588 [0.497, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.071, 10.314], loss: 0.001790, mae: 0.044393, mean_q: 1.273822
 66455/100000: episode: 1289, duration: 0.707s, episode steps: 100, steps per second: 141, episode reward: 59.569, mean reward: 0.596 [0.505, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.002, 10.137], loss: 0.001593, mae: 0.042800, mean_q: 1.270691
 66555/100000: episode: 1290, duration: 0.743s, episode steps: 100, steps per second: 135, episode reward: 57.072, mean reward: 0.571 [0.498, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.034, 10.201], loss: 0.001877, mae: 0.045224, mean_q: 1.261722
 66655/100000: episode: 1291, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 57.421, mean reward: 0.574 [0.506, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.076, 10.098], loss: 0.001572, mae: 0.042454, mean_q: 1.256949
 66755/100000: episode: 1292, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: 59.655, mean reward: 0.597 [0.500, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.265, 10.098], loss: 0.001533, mae: 0.041659, mean_q: 1.249312
 66855/100000: episode: 1293, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: 60.242, mean reward: 0.602 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.655, 10.191], loss: 0.001658, mae: 0.042695, mean_q: 1.244877
 66955/100000: episode: 1294, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: 58.335, mean reward: 0.583 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.811, 10.098], loss: 0.001633, mae: 0.043310, mean_q: 1.240896
 67055/100000: episode: 1295, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 62.014, mean reward: 0.620 [0.498, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.793, 10.098], loss: 0.001659, mae: 0.043419, mean_q: 1.239226
 67155/100000: episode: 1296, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: 58.895, mean reward: 0.589 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.780, 10.098], loss: 0.001549, mae: 0.041836, mean_q: 1.237218
 67255/100000: episode: 1297, duration: 0.917s, episode steps: 100, steps per second: 109, episode reward: 60.950, mean reward: 0.609 [0.534, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.773, 10.098], loss: 0.001527, mae: 0.042122, mean_q: 1.236292
 67355/100000: episode: 1298, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: 57.776, mean reward: 0.578 [0.502, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.906, 10.098], loss: 0.001630, mae: 0.043014, mean_q: 1.231843
 67455/100000: episode: 1299, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: 58.482, mean reward: 0.585 [0.509, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.861, 10.098], loss: 0.001564, mae: 0.042468, mean_q: 1.225057
 67555/100000: episode: 1300, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 61.119, mean reward: 0.611 [0.505, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.588, 10.245], loss: 0.001517, mae: 0.041593, mean_q: 1.216916
 67655/100000: episode: 1301, duration: 0.609s, episode steps: 100, steps per second: 164, episode reward: 60.445, mean reward: 0.604 [0.507, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.767, 10.098], loss: 0.001547, mae: 0.041766, mean_q: 1.209664
 67755/100000: episode: 1302, duration: 0.989s, episode steps: 100, steps per second: 101, episode reward: 57.154, mean reward: 0.572 [0.498, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.476, 10.231], loss: 0.001521, mae: 0.041782, mean_q: 1.204831
 67855/100000: episode: 1303, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: 59.451, mean reward: 0.595 [0.505, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.711, 10.149], loss: 0.001638, mae: 0.042978, mean_q: 1.198793
 67955/100000: episode: 1304, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 58.015, mean reward: 0.580 [0.499, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.897, 10.098], loss: 0.001556, mae: 0.042662, mean_q: 1.195547
 68055/100000: episode: 1305, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 59.556, mean reward: 0.596 [0.510, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.279, 10.124], loss: 0.001503, mae: 0.042169, mean_q: 1.196800
 68155/100000: episode: 1306, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.911, mean reward: 0.579 [0.505, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.896, 10.098], loss: 0.001346, mae: 0.040212, mean_q: 1.189770
 68255/100000: episode: 1307, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 57.806, mean reward: 0.578 [0.505, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.106, 10.098], loss: 0.001516, mae: 0.042188, mean_q: 1.182294
 68355/100000: episode: 1308, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 57.057, mean reward: 0.571 [0.505, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.674, 10.098], loss: 0.001386, mae: 0.040938, mean_q: 1.177382
 68455/100000: episode: 1309, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: 59.359, mean reward: 0.594 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.844, 10.378], loss: 0.001546, mae: 0.042272, mean_q: 1.171137
 68555/100000: episode: 1310, duration: 0.604s, episode steps: 100, steps per second: 165, episode reward: 59.118, mean reward: 0.591 [0.499, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.819, 10.098], loss: 0.001434, mae: 0.040852, mean_q: 1.165284
 68655/100000: episode: 1311, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 56.746, mean reward: 0.567 [0.509, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.111, 10.200], loss: 0.001414, mae: 0.040788, mean_q: 1.165176
 68755/100000: episode: 1312, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 59.370, mean reward: 0.594 [0.505, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.778, 10.180], loss: 0.001325, mae: 0.039588, mean_q: 1.165266
 68855/100000: episode: 1313, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 62.747, mean reward: 0.627 [0.524, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.083, 10.098], loss: 0.001387, mae: 0.040443, mean_q: 1.170286
 68955/100000: episode: 1314, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 57.505, mean reward: 0.575 [0.505, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.135, 10.151], loss: 0.001388, mae: 0.041036, mean_q: 1.168770
 69055/100000: episode: 1315, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: 59.819, mean reward: 0.598 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.357, 10.423], loss: 0.001328, mae: 0.039740, mean_q: 1.164356
 69155/100000: episode: 1316, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 59.812, mean reward: 0.598 [0.508, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.698, 10.098], loss: 0.001332, mae: 0.039552, mean_q: 1.161123
 69255/100000: episode: 1317, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: 57.115, mean reward: 0.571 [0.505, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.251, 10.098], loss: 0.001378, mae: 0.040156, mean_q: 1.164007
 69355/100000: episode: 1318, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: 57.120, mean reward: 0.571 [0.502, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.549, 10.174], loss: 0.001351, mae: 0.040220, mean_q: 1.164581
 69455/100000: episode: 1319, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 58.025, mean reward: 0.580 [0.500, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.462, 10.098], loss: 0.001323, mae: 0.039877, mean_q: 1.162835
 69555/100000: episode: 1320, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 58.855, mean reward: 0.589 [0.502, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.927, 10.098], loss: 0.001379, mae: 0.039869, mean_q: 1.169993
 69655/100000: episode: 1321, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: 57.596, mean reward: 0.576 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.626, 10.114], loss: 0.001355, mae: 0.040569, mean_q: 1.164103
 69755/100000: episode: 1322, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: 61.217, mean reward: 0.612 [0.504, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.008, 10.098], loss: 0.001353, mae: 0.039738, mean_q: 1.165924
 69855/100000: episode: 1323, duration: 1.207s, episode steps: 100, steps per second: 83, episode reward: 57.242, mean reward: 0.572 [0.500, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.771, 10.098], loss: 0.001256, mae: 0.038870, mean_q: 1.165972
 69955/100000: episode: 1324, duration: 1.054s, episode steps: 100, steps per second: 95, episode reward: 58.699, mean reward: 0.587 [0.500, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.458, 10.098], loss: 0.001397, mae: 0.040991, mean_q: 1.165678
 70055/100000: episode: 1325, duration: 1.059s, episode steps: 100, steps per second: 94, episode reward: 59.271, mean reward: 0.593 [0.504, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.009, 10.098], loss: 0.001367, mae: 0.040366, mean_q: 1.161568
 70155/100000: episode: 1326, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: 61.550, mean reward: 0.615 [0.515, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.590, 10.439], loss: 0.001348, mae: 0.040346, mean_q: 1.164550
 70255/100000: episode: 1327, duration: 1.132s, episode steps: 100, steps per second: 88, episode reward: 58.333, mean reward: 0.583 [0.508, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.958, 10.238], loss: 0.001321, mae: 0.039984, mean_q: 1.163957
 70355/100000: episode: 1328, duration: 0.870s, episode steps: 100, steps per second: 115, episode reward: 58.200, mean reward: 0.582 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.363, 10.232], loss: 0.001467, mae: 0.041780, mean_q: 1.162212
 70455/100000: episode: 1329, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: 58.616, mean reward: 0.586 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.470, 10.098], loss: 0.001352, mae: 0.040505, mean_q: 1.162312
 70555/100000: episode: 1330, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: 57.822, mean reward: 0.578 [0.500, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.955, 10.206], loss: 0.001233, mae: 0.038534, mean_q: 1.160648
 70655/100000: episode: 1331, duration: 0.648s, episode steps: 100, steps per second: 154, episode reward: 63.609, mean reward: 0.636 [0.517, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.776, 10.098], loss: 0.001444, mae: 0.041743, mean_q: 1.167531
 70755/100000: episode: 1332, duration: 0.802s, episode steps: 100, steps per second: 125, episode reward: 60.140, mean reward: 0.601 [0.501, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.569, 10.436], loss: 0.001299, mae: 0.039528, mean_q: 1.167250
 70855/100000: episode: 1333, duration: 1.135s, episode steps: 100, steps per second: 88, episode reward: 56.462, mean reward: 0.565 [0.499, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.418, 10.112], loss: 0.001252, mae: 0.038787, mean_q: 1.166651
 70955/100000: episode: 1334, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: 57.432, mean reward: 0.574 [0.506, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.206, 10.098], loss: 0.001318, mae: 0.039764, mean_q: 1.164219
 71055/100000: episode: 1335, duration: 0.627s, episode steps: 100, steps per second: 160, episode reward: 57.655, mean reward: 0.577 [0.506, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.946, 10.260], loss: 0.001367, mae: 0.040716, mean_q: 1.165899
 71155/100000: episode: 1336, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 58.138, mean reward: 0.581 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.479, 10.165], loss: 0.001390, mae: 0.041195, mean_q: 1.163429
 71255/100000: episode: 1337, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 61.143, mean reward: 0.611 [0.531, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.592, 10.098], loss: 0.001455, mae: 0.041729, mean_q: 1.165765
 71355/100000: episode: 1338, duration: 0.798s, episode steps: 100, steps per second: 125, episode reward: 59.364, mean reward: 0.594 [0.513, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.790, 10.140], loss: 0.001366, mae: 0.040519, mean_q: 1.167940
 71455/100000: episode: 1339, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: 59.311, mean reward: 0.593 [0.502, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.090, 10.098], loss: 0.001493, mae: 0.042565, mean_q: 1.164204
 71555/100000: episode: 1340, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.196, mean reward: 0.582 [0.516, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.678, 10.192], loss: 0.001374, mae: 0.040559, mean_q: 1.166465
 71655/100000: episode: 1341, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.675, mean reward: 0.577 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.486, 10.141], loss: 0.001466, mae: 0.041754, mean_q: 1.167516
 71755/100000: episode: 1342, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 57.597, mean reward: 0.576 [0.507, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.432, 10.276], loss: 0.001282, mae: 0.039367, mean_q: 1.164378
 71855/100000: episode: 1343, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: 59.153, mean reward: 0.592 [0.515, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.501, 10.098], loss: 0.001392, mae: 0.040626, mean_q: 1.167058
 71955/100000: episode: 1344, duration: 1.088s, episode steps: 100, steps per second: 92, episode reward: 57.274, mean reward: 0.573 [0.505, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.664, 10.191], loss: 0.001340, mae: 0.040040, mean_q: 1.161620
 72055/100000: episode: 1345, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: 61.814, mean reward: 0.618 [0.518, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.697, 10.098], loss: 0.001345, mae: 0.040404, mean_q: 1.162647
 72155/100000: episode: 1346, duration: 0.647s, episode steps: 100, steps per second: 155, episode reward: 59.902, mean reward: 0.599 [0.512, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.627, 10.288], loss: 0.001482, mae: 0.042069, mean_q: 1.163102
 72255/100000: episode: 1347, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 58.535, mean reward: 0.585 [0.504, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.786, 10.098], loss: 0.001363, mae: 0.040021, mean_q: 1.159375
 72355/100000: episode: 1348, duration: 0.633s, episode steps: 100, steps per second: 158, episode reward: 59.027, mean reward: 0.590 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.108, 10.098], loss: 0.001222, mae: 0.038637, mean_q: 1.162889
 72455/100000: episode: 1349, duration: 1.168s, episode steps: 100, steps per second: 86, episode reward: 57.617, mean reward: 0.576 [0.504, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.580, 10.137], loss: 0.001366, mae: 0.040449, mean_q: 1.163709
 72555/100000: episode: 1350, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: 61.671, mean reward: 0.617 [0.518, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.305, 10.098], loss: 0.001692, mae: 0.043577, mean_q: 1.162827
 72655/100000: episode: 1351, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: 57.814, mean reward: 0.578 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.441, 10.098], loss: 0.001484, mae: 0.041769, mean_q: 1.166450
 72755/100000: episode: 1352, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 56.772, mean reward: 0.568 [0.504, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.756, 10.118], loss: 0.001292, mae: 0.039222, mean_q: 1.165303
 72855/100000: episode: 1353, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: 59.021, mean reward: 0.590 [0.500, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.646, 10.176], loss: 0.001385, mae: 0.040224, mean_q: 1.163708
 72955/100000: episode: 1354, duration: 0.881s, episode steps: 100, steps per second: 114, episode reward: 57.643, mean reward: 0.576 [0.500, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.514, 10.171], loss: 0.001305, mae: 0.039258, mean_q: 1.159765
 73055/100000: episode: 1355, duration: 1.115s, episode steps: 100, steps per second: 90, episode reward: 57.843, mean reward: 0.578 [0.508, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.312, 10.211], loss: 0.001262, mae: 0.039084, mean_q: 1.159863
 73155/100000: episode: 1356, duration: 0.930s, episode steps: 100, steps per second: 107, episode reward: 60.157, mean reward: 0.602 [0.507, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.035, 10.098], loss: 0.001393, mae: 0.040768, mean_q: 1.161901
 73255/100000: episode: 1357, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 57.540, mean reward: 0.575 [0.502, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.032, 10.122], loss: 0.001344, mae: 0.040267, mean_q: 1.166776
 73355/100000: episode: 1358, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: 58.136, mean reward: 0.581 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.536, 10.098], loss: 0.001295, mae: 0.039762, mean_q: 1.160522
 73455/100000: episode: 1359, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 58.653, mean reward: 0.587 [0.507, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.429, 10.198], loss: 0.001337, mae: 0.039732, mean_q: 1.162616
[Info] 1-TH LEVEL FOUND: 1.3750851154327393, Considering 10/90 traces
 73555/100000: episode: 1360, duration: 6.807s, episode steps: 100, steps per second: 15, episode reward: 57.904, mean reward: 0.579 [0.499, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.961, 10.098], loss: 0.001358, mae: 0.039978, mean_q: 1.161035
 73630/100000: episode: 1361, duration: 0.696s, episode steps: 75, steps per second: 108, episode reward: 45.874, mean reward: 0.612 [0.502, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.681 [-1.175, 10.100], loss: 0.001279, mae: 0.039266, mean_q: 1.162921
 73656/100000: episode: 1362, duration: 0.178s, episode steps: 26, steps per second: 146, episode reward: 16.726, mean reward: 0.643 [0.531, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.172, 10.100], loss: 0.001314, mae: 0.039016, mean_q: 1.162979
 73754/100000: episode: 1363, duration: 0.630s, episode steps: 98, steps per second: 156, episode reward: 58.996, mean reward: 0.602 [0.526, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-0.998, 10.100], loss: 0.001408, mae: 0.041149, mean_q: 1.164117
 73794/100000: episode: 1364, duration: 0.220s, episode steps: 40, steps per second: 181, episode reward: 28.116, mean reward: 0.703 [0.596, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-1.001, 10.100], loss: 0.001452, mae: 0.040837, mean_q: 1.160632
 73869/100000: episode: 1365, duration: 0.602s, episode steps: 75, steps per second: 124, episode reward: 46.073, mean reward: 0.614 [0.500, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.694 [-0.554, 10.486], loss: 0.001278, mae: 0.039442, mean_q: 1.161349
 73944/100000: episode: 1366, duration: 0.478s, episode steps: 75, steps per second: 157, episode reward: 51.813, mean reward: 0.691 [0.573, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-0.651, 10.467], loss: 0.001479, mae: 0.041922, mean_q: 1.166523
 73972/100000: episode: 1367, duration: 0.214s, episode steps: 28, steps per second: 131, episode reward: 19.247, mean reward: 0.687 [0.629, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.718, 10.100], loss: 0.001521, mae: 0.042479, mean_q: 1.166908
 74067/100000: episode: 1368, duration: 0.606s, episode steps: 95, steps per second: 157, episode reward: 56.861, mean reward: 0.599 [0.513, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-0.728, 10.219], loss: 0.001448, mae: 0.041612, mean_q: 1.167715
 74095/100000: episode: 1369, duration: 0.189s, episode steps: 28, steps per second: 148, episode reward: 19.048, mean reward: 0.680 [0.602, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.464, 10.100], loss: 0.001602, mae: 0.043402, mean_q: 1.172313
 74117/100000: episode: 1370, duration: 0.132s, episode steps: 22, steps per second: 166, episode reward: 14.749, mean reward: 0.670 [0.547, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.764, 10.100], loss: 0.001209, mae: 0.037954, mean_q: 1.168332
 74212/100000: episode: 1371, duration: 0.965s, episode steps: 95, steps per second: 98, episode reward: 55.172, mean reward: 0.581 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.894, 10.366], loss: 0.001542, mae: 0.042592, mean_q: 1.165954
 74287/100000: episode: 1372, duration: 0.627s, episode steps: 75, steps per second: 120, episode reward: 48.912, mean reward: 0.652 [0.523, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.687 [-0.837, 10.319], loss: 0.001585, mae: 0.042184, mean_q: 1.167358
 74362/100000: episode: 1373, duration: 0.772s, episode steps: 75, steps per second: 97, episode reward: 44.967, mean reward: 0.600 [0.511, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.685 [-0.435, 10.100], loss: 0.001399, mae: 0.040348, mean_q: 1.170316
 74381/100000: episode: 1374, duration: 0.216s, episode steps: 19, steps per second: 88, episode reward: 12.370, mean reward: 0.651 [0.604, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.212, 10.100], loss: 0.001397, mae: 0.039878, mean_q: 1.177310
 74408/100000: episode: 1375, duration: 0.220s, episode steps: 27, steps per second: 123, episode reward: 19.748, mean reward: 0.731 [0.656, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.535, 10.100], loss: 0.001770, mae: 0.044264, mean_q: 1.174621
 74434/100000: episode: 1376, duration: 0.191s, episode steps: 26, steps per second: 136, episode reward: 17.512, mean reward: 0.674 [0.583, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.480, 10.100], loss: 0.001587, mae: 0.042460, mean_q: 1.169864
 74456/100000: episode: 1377, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 16.811, mean reward: 0.764 [0.689, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.370, 10.100], loss: 0.001509, mae: 0.041811, mean_q: 1.181360
 74475/100000: episode: 1378, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 13.049, mean reward: 0.687 [0.622, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.651, 10.100], loss: 0.001500, mae: 0.041415, mean_q: 1.177898
 74490/100000: episode: 1379, duration: 0.105s, episode steps: 15, steps per second: 142, episode reward: 10.603, mean reward: 0.707 [0.657, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.171, 10.100], loss: 0.001332, mae: 0.039258, mean_q: 1.171803
 74517/100000: episode: 1380, duration: 0.181s, episode steps: 27, steps per second: 150, episode reward: 16.503, mean reward: 0.611 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.100], loss: 0.001795, mae: 0.044651, mean_q: 1.171296
 74539/100000: episode: 1381, duration: 0.152s, episode steps: 22, steps per second: 145, episode reward: 16.006, mean reward: 0.728 [0.654, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.527, 10.100], loss: 0.001591, mae: 0.043457, mean_q: 1.182348
 74634/100000: episode: 1382, duration: 0.635s, episode steps: 95, steps per second: 150, episode reward: 56.887, mean reward: 0.599 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.995, 10.318], loss: 0.001816, mae: 0.044253, mean_q: 1.176792
 74729/100000: episode: 1383, duration: 0.702s, episode steps: 95, steps per second: 135, episode reward: 54.915, mean reward: 0.578 [0.506, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-1.209, 10.100], loss: 0.001795, mae: 0.044050, mean_q: 1.175963
 74827/100000: episode: 1384, duration: 0.560s, episode steps: 98, steps per second: 175, episode reward: 59.485, mean reward: 0.607 [0.503, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.478 [-0.530, 10.171], loss: 0.001686, mae: 0.043661, mean_q: 1.181785
 74849/100000: episode: 1385, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 15.530, mean reward: 0.706 [0.613, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.783, 10.100], loss: 0.002151, mae: 0.046283, mean_q: 1.172541
 74868/100000: episode: 1386, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 12.632, mean reward: 0.665 [0.556, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.662, 10.100], loss: 0.001759, mae: 0.043164, mean_q: 1.177949
 74895/100000: episode: 1387, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 17.685, mean reward: 0.655 [0.574, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.815, 10.100], loss: 0.001854, mae: 0.045276, mean_q: 1.178472
 74917/100000: episode: 1388, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 13.528, mean reward: 0.615 [0.521, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.430, 10.310], loss: 0.001876, mae: 0.045735, mean_q: 1.174930
 75012/100000: episode: 1389, duration: 0.584s, episode steps: 95, steps per second: 163, episode reward: 59.812, mean reward: 0.630 [0.503, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.446, 10.100], loss: 0.001705, mae: 0.042925, mean_q: 1.179589
 75110/100000: episode: 1390, duration: 0.609s, episode steps: 98, steps per second: 161, episode reward: 57.462, mean reward: 0.586 [0.502, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.731, 10.241], loss: 0.001666, mae: 0.043850, mean_q: 1.182154
 75125/100000: episode: 1391, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 11.035, mean reward: 0.736 [0.650, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.194, 10.100], loss: 0.001407, mae: 0.039695, mean_q: 1.179085
 75153/100000: episode: 1392, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 20.210, mean reward: 0.722 [0.612, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.581, 10.100], loss: 0.001739, mae: 0.044258, mean_q: 1.182562
 75181/100000: episode: 1393, duration: 0.170s, episode steps: 28, steps per second: 164, episode reward: 20.610, mean reward: 0.736 [0.600, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.676, 10.100], loss: 0.001574, mae: 0.041532, mean_q: 1.187638
 75256/100000: episode: 1394, duration: 0.492s, episode steps: 75, steps per second: 152, episode reward: 45.051, mean reward: 0.601 [0.511, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.684 [-1.476, 10.245], loss: 0.001662, mae: 0.042220, mean_q: 1.188635
 75296/100000: episode: 1395, duration: 0.288s, episode steps: 40, steps per second: 139, episode reward: 27.636, mean reward: 0.691 [0.536, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.926, 10.100], loss: 0.001397, mae: 0.039782, mean_q: 1.189266
 75322/100000: episode: 1396, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 16.844, mean reward: 0.648 [0.578, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.448, 10.100], loss: 0.001638, mae: 0.039785, mean_q: 1.182249
 75344/100000: episode: 1397, duration: 0.136s, episode steps: 22, steps per second: 161, episode reward: 14.428, mean reward: 0.656 [0.527, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.269, 10.100], loss: 0.001431, mae: 0.040372, mean_q: 1.191544
 75366/100000: episode: 1398, duration: 0.134s, episode steps: 22, steps per second: 165, episode reward: 15.359, mean reward: 0.698 [0.554, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.040, 10.100], loss: 0.001685, mae: 0.041399, mean_q: 1.183980
 75441/100000: episode: 1399, duration: 0.482s, episode steps: 75, steps per second: 156, episode reward: 45.070, mean reward: 0.601 [0.498, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.684 [-0.885, 10.100], loss: 0.001485, mae: 0.040830, mean_q: 1.189190
 75539/100000: episode: 1400, duration: 0.566s, episode steps: 98, steps per second: 173, episode reward: 56.762, mean reward: 0.579 [0.514, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.800, 10.100], loss: 0.001641, mae: 0.042491, mean_q: 1.189831
 75634/100000: episode: 1401, duration: 0.546s, episode steps: 95, steps per second: 174, episode reward: 57.299, mean reward: 0.603 [0.503, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-1.555, 10.100], loss: 0.001449, mae: 0.041271, mean_q: 1.192875
 75661/100000: episode: 1402, duration: 0.246s, episode steps: 27, steps per second: 110, episode reward: 18.941, mean reward: 0.702 [0.584, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.831, 10.100], loss: 0.001430, mae: 0.039817, mean_q: 1.187811
 75701/100000: episode: 1403, duration: 0.314s, episode steps: 40, steps per second: 127, episode reward: 26.645, mean reward: 0.666 [0.536, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.491, 10.100], loss: 0.001447, mae: 0.040709, mean_q: 1.190297
 75716/100000: episode: 1404, duration: 0.095s, episode steps: 15, steps per second: 157, episode reward: 10.352, mean reward: 0.690 [0.643, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.425, 10.100], loss: 0.001422, mae: 0.040632, mean_q: 1.195190
 75735/100000: episode: 1405, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 12.717, mean reward: 0.669 [0.593, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.022, 10.100], loss: 0.001617, mae: 0.041777, mean_q: 1.188666
 75754/100000: episode: 1406, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 13.114, mean reward: 0.690 [0.619, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.335, 10.100], loss: 0.001637, mae: 0.041467, mean_q: 1.193198
 75794/100000: episode: 1407, duration: 0.228s, episode steps: 40, steps per second: 175, episode reward: 30.234, mean reward: 0.756 [0.665, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.706, 10.100], loss: 0.001427, mae: 0.040738, mean_q: 1.195705
 75820/100000: episode: 1408, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 17.338, mean reward: 0.667 [0.588, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.637, 10.100], loss: 0.001625, mae: 0.043007, mean_q: 1.195513
 75895/100000: episode: 1409, duration: 0.423s, episode steps: 75, steps per second: 177, episode reward: 49.644, mean reward: 0.662 [0.571, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.681 [-0.335, 10.388], loss: 0.001658, mae: 0.042440, mean_q: 1.194686
 75923/100000: episode: 1410, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 18.066, mean reward: 0.645 [0.539, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.270, 10.100], loss: 0.001496, mae: 0.041145, mean_q: 1.206977
 75949/100000: episode: 1411, duration: 0.176s, episode steps: 26, steps per second: 148, episode reward: 16.565, mean reward: 0.637 [0.561, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.204, 10.100], loss: 0.001832, mae: 0.043550, mean_q: 1.189132
 75989/100000: episode: 1412, duration: 0.318s, episode steps: 40, steps per second: 126, episode reward: 26.386, mean reward: 0.660 [0.577, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.612, 10.100], loss: 0.001383, mae: 0.040473, mean_q: 1.205734
 76011/100000: episode: 1413, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 16.117, mean reward: 0.733 [0.664, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.369, 10.100], loss: 0.001420, mae: 0.041471, mean_q: 1.208727
 76026/100000: episode: 1414, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 10.101, mean reward: 0.673 [0.610, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.161, 10.100], loss: 0.001569, mae: 0.042068, mean_q: 1.195422
 76052/100000: episode: 1415, duration: 0.155s, episode steps: 26, steps per second: 167, episode reward: 17.934, mean reward: 0.690 [0.619, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.041, 10.100], loss: 0.001639, mae: 0.043979, mean_q: 1.204531
 76150/100000: episode: 1416, duration: 0.648s, episode steps: 98, steps per second: 151, episode reward: 57.594, mean reward: 0.588 [0.508, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-1.235, 10.375], loss: 0.001509, mae: 0.041361, mean_q: 1.199648
 76165/100000: episode: 1417, duration: 0.127s, episode steps: 15, steps per second: 118, episode reward: 11.147, mean reward: 0.743 [0.679, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.373, 10.100], loss: 0.001605, mae: 0.043371, mean_q: 1.196188
 76193/100000: episode: 1418, duration: 0.219s, episode steps: 28, steps per second: 128, episode reward: 20.399, mean reward: 0.729 [0.580, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.836, 10.100], loss: 0.001506, mae: 0.041928, mean_q: 1.200824
 76215/100000: episode: 1419, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 14.577, mean reward: 0.663 [0.565, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.973, 10.100], loss: 0.001478, mae: 0.039101, mean_q: 1.200455
 76237/100000: episode: 1420, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 14.631, mean reward: 0.665 [0.577, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.667, 10.100], loss: 0.001642, mae: 0.043142, mean_q: 1.204904
 76252/100000: episode: 1421, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 9.784, mean reward: 0.652 [0.600, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.275, 10.100], loss: 0.001713, mae: 0.045140, mean_q: 1.203306
 76327/100000: episode: 1422, duration: 0.422s, episode steps: 75, steps per second: 178, episode reward: 49.356, mean reward: 0.658 [0.549, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.673 [-1.114, 10.291], loss: 0.001540, mae: 0.042224, mean_q: 1.202956
 76355/100000: episode: 1423, duration: 0.174s, episode steps: 28, steps per second: 161, episode reward: 18.876, mean reward: 0.674 [0.613, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.596, 10.100], loss: 0.001834, mae: 0.045871, mean_q: 1.208251
 76374/100000: episode: 1424, duration: 0.125s, episode steps: 19, steps per second: 152, episode reward: 12.780, mean reward: 0.673 [0.612, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.537, 10.100], loss: 0.001873, mae: 0.045812, mean_q: 1.209920
 76472/100000: episode: 1425, duration: 0.541s, episode steps: 98, steps per second: 181, episode reward: 57.233, mean reward: 0.584 [0.511, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-1.225, 10.100], loss: 0.001449, mae: 0.039748, mean_q: 1.210545
 76499/100000: episode: 1426, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 19.052, mean reward: 0.706 [0.636, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.084, 10.100], loss: 0.001615, mae: 0.042060, mean_q: 1.217278
 76525/100000: episode: 1427, duration: 0.190s, episode steps: 26, steps per second: 137, episode reward: 16.664, mean reward: 0.641 [0.530, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.843, 10.100], loss: 0.001466, mae: 0.041588, mean_q: 1.208798
 76552/100000: episode: 1428, duration: 0.171s, episode steps: 27, steps per second: 158, episode reward: 19.511, mean reward: 0.723 [0.619, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.453, 10.100], loss: 0.001647, mae: 0.043632, mean_q: 1.208590
 76627/100000: episode: 1429, duration: 0.442s, episode steps: 75, steps per second: 170, episode reward: 52.429, mean reward: 0.699 [0.610, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.689 [-1.422, 10.440], loss: 0.001516, mae: 0.041392, mean_q: 1.213376
 76649/100000: episode: 1430, duration: 0.143s, episode steps: 22, steps per second: 153, episode reward: 14.858, mean reward: 0.675 [0.629, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.104, 10.100], loss: 0.001573, mae: 0.043500, mean_q: 1.215697
 76676/100000: episode: 1431, duration: 0.161s, episode steps: 27, steps per second: 168, episode reward: 18.557, mean reward: 0.687 [0.597, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.259, 10.100], loss: 0.001499, mae: 0.043315, mean_q: 1.226670
 76771/100000: episode: 1432, duration: 0.675s, episode steps: 95, steps per second: 141, episode reward: 57.841, mean reward: 0.609 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.242, 10.100], loss: 0.001599, mae: 0.042840, mean_q: 1.217784
 76869/100000: episode: 1433, duration: 0.608s, episode steps: 98, steps per second: 161, episode reward: 59.572, mean reward: 0.608 [0.511, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.800, 10.292], loss: 0.001505, mae: 0.041300, mean_q: 1.209523
 76967/100000: episode: 1434, duration: 0.692s, episode steps: 98, steps per second: 142, episode reward: 59.124, mean reward: 0.603 [0.502, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.568, 10.426], loss: 0.001405, mae: 0.040405, mean_q: 1.216418
 77042/100000: episode: 1435, duration: 0.510s, episode steps: 75, steps per second: 147, episode reward: 44.592, mean reward: 0.595 [0.512, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.698 [-1.322, 10.153], loss: 0.001526, mae: 0.042032, mean_q: 1.213147
 77070/100000: episode: 1436, duration: 0.159s, episode steps: 28, steps per second: 177, episode reward: 17.247, mean reward: 0.616 [0.549, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.284, 10.100], loss: 0.001497, mae: 0.041583, mean_q: 1.220218
 77089/100000: episode: 1437, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 13.230, mean reward: 0.696 [0.607, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.473, 10.100], loss: 0.001594, mae: 0.043345, mean_q: 1.222505
 77115/100000: episode: 1438, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 16.422, mean reward: 0.632 [0.534, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.225, 10.100], loss: 0.001254, mae: 0.039249, mean_q: 1.212914
 77155/100000: episode: 1439, duration: 0.244s, episode steps: 40, steps per second: 164, episode reward: 28.614, mean reward: 0.715 [0.575, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.994, 10.100], loss: 0.001327, mae: 0.039306, mean_q: 1.219127
 77174/100000: episode: 1440, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 12.736, mean reward: 0.670 [0.621, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.313, 10.100], loss: 0.001682, mae: 0.044277, mean_q: 1.214328
 77196/100000: episode: 1441, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 14.674, mean reward: 0.667 [0.588, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.223, 10.100], loss: 0.001516, mae: 0.042423, mean_q: 1.225278
 77291/100000: episode: 1442, duration: 0.562s, episode steps: 95, steps per second: 169, episode reward: 55.632, mean reward: 0.586 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-1.168, 10.100], loss: 0.001486, mae: 0.041456, mean_q: 1.220596
 77319/100000: episode: 1443, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 19.176, mean reward: 0.685 [0.630, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.213, 10.100], loss: 0.001306, mae: 0.040024, mean_q: 1.223787
 77417/100000: episode: 1444, duration: 0.602s, episode steps: 98, steps per second: 163, episode reward: 59.145, mean reward: 0.604 [0.506, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-0.954, 10.290], loss: 0.001534, mae: 0.042136, mean_q: 1.221828
 77515/100000: episode: 1445, duration: 0.548s, episode steps: 98, steps per second: 179, episode reward: 59.423, mean reward: 0.606 [0.501, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.449, 10.100], loss: 0.001554, mae: 0.042249, mean_q: 1.226134
 77610/100000: episode: 1446, duration: 0.639s, episode steps: 95, steps per second: 149, episode reward: 55.666, mean reward: 0.586 [0.511, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.760, 10.163], loss: 0.001393, mae: 0.040597, mean_q: 1.226766
 77632/100000: episode: 1447, duration: 0.148s, episode steps: 22, steps per second: 148, episode reward: 14.518, mean reward: 0.660 [0.614, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.219, 10.100], loss: 0.001459, mae: 0.040998, mean_q: 1.234072
 77730/100000: episode: 1448, duration: 0.553s, episode steps: 98, steps per second: 177, episode reward: 59.240, mean reward: 0.604 [0.519, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-1.440, 10.100], loss: 0.001494, mae: 0.042081, mean_q: 1.228858
 77756/100000: episode: 1449, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 15.964, mean reward: 0.614 [0.501, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.380, 10.100], loss: 0.001597, mae: 0.043642, mean_q: 1.226009
[Info] 2-TH LEVEL FOUND: 1.5007684230804443, Considering 10/90 traces
 77796/100000: episode: 1450, duration: 4.675s, episode steps: 40, steps per second: 9, episode reward: 24.971, mean reward: 0.624 [0.529, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.447, 10.115], loss: 0.001481, mae: 0.042764, mean_q: 1.229337
 77806/100000: episode: 1451, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 7.694, mean reward: 0.769 [0.659, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.494], loss: 0.001399, mae: 0.041184, mean_q: 1.218441
 77816/100000: episode: 1452, duration: 0.071s, episode steps: 10, steps per second: 142, episode reward: 8.403, mean reward: 0.840 [0.801, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.603], loss: 0.001444, mae: 0.042004, mean_q: 1.222396
 77822/100000: episode: 1453, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 4.746, mean reward: 0.791 [0.774, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.405, 10.100], loss: 0.001547, mae: 0.043009, mean_q: 1.218712
 77836/100000: episode: 1454, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 9.878, mean reward: 0.706 [0.628, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.225, 10.100], loss: 0.001702, mae: 0.044411, mean_q: 1.231234
 77849/100000: episode: 1455, duration: 0.100s, episode steps: 13, steps per second: 130, episode reward: 9.989, mean reward: 0.768 [0.694, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.383, 10.100], loss: 0.001606, mae: 0.044534, mean_q: 1.226317
 77863/100000: episode: 1456, duration: 0.112s, episode steps: 14, steps per second: 125, episode reward: 11.081, mean reward: 0.792 [0.722, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.252, 10.100], loss: 0.001409, mae: 0.040647, mean_q: 1.223299
 77876/100000: episode: 1457, duration: 0.108s, episode steps: 13, steps per second: 120, episode reward: 9.014, mean reward: 0.693 [0.650, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.314, 10.100], loss: 0.001484, mae: 0.042726, mean_q: 1.219827
 77889/100000: episode: 1458, duration: 0.108s, episode steps: 13, steps per second: 120, episode reward: 9.993, mean reward: 0.769 [0.716, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.386, 10.100], loss: 0.001545, mae: 0.041767, mean_q: 1.217417
 77900/100000: episode: 1459, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 9.053, mean reward: 0.823 [0.779, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.351, 10.100], loss: 0.001461, mae: 0.042861, mean_q: 1.236156
 77911/100000: episode: 1460, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 8.322, mean reward: 0.757 [0.696, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.249, 10.100], loss: 0.001477, mae: 0.042434, mean_q: 1.233659
 77922/100000: episode: 1461, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 8.410, mean reward: 0.765 [0.725, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.506, 10.100], loss: 0.001378, mae: 0.041475, mean_q: 1.232555
 77937/100000: episode: 1462, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 11.473, mean reward: 0.765 [0.722, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.266, 10.100], loss: 0.001280, mae: 0.037714, mean_q: 1.231035
 77950/100000: episode: 1463, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 10.692, mean reward: 0.822 [0.800, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.549, 10.100], loss: 0.001417, mae: 0.040254, mean_q: 1.224223
 77967/100000: episode: 1464, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 13.637, mean reward: 0.802 [0.740, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.432, 10.100], loss: 0.001299, mae: 0.038439, mean_q: 1.229089
 77984/100000: episode: 1465, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 12.569, mean reward: 0.739 [0.704, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.697, 10.100], loss: 0.001358, mae: 0.040902, mean_q: 1.239891
 77995/100000: episode: 1466, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 8.346, mean reward: 0.759 [0.720, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.539, 10.100], loss: 0.001416, mae: 0.041257, mean_q: 1.231112
 78012/100000: episode: 1467, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 13.511, mean reward: 0.795 [0.718, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.425, 10.100], loss: 0.001508, mae: 0.041874, mean_q: 1.251861
 78025/100000: episode: 1468, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 10.166, mean reward: 0.782 [0.712, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.492, 10.100], loss: 0.001219, mae: 0.039192, mean_q: 1.233801
 78031/100000: episode: 1469, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 5.080, mean reward: 0.847 [0.832, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.319, 10.100], loss: 0.001429, mae: 0.040888, mean_q: 1.238060
 78048/100000: episode: 1470, duration: 0.122s, episode steps: 17, steps per second: 139, episode reward: 12.422, mean reward: 0.731 [0.667, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.399, 10.100], loss: 0.001430, mae: 0.041084, mean_q: 1.240481
 78054/100000: episode: 1471, duration: 0.052s, episode steps: 6, steps per second: 116, episode reward: 4.988, mean reward: 0.831 [0.797, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.036, 10.100], loss: 0.001937, mae: 0.048131, mean_q: 1.240155
 78067/100000: episode: 1472, duration: 0.108s, episode steps: 13, steps per second: 120, episode reward: 10.140, mean reward: 0.780 [0.688, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.359, 10.100], loss: 0.001346, mae: 0.039614, mean_q: 1.235943
 78080/100000: episode: 1473, duration: 0.118s, episode steps: 13, steps per second: 110, episode reward: 9.470, mean reward: 0.728 [0.620, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.371, 10.100], loss: 0.001241, mae: 0.038964, mean_q: 1.247650
 78094/100000: episode: 1474, duration: 0.097s, episode steps: 14, steps per second: 145, episode reward: 11.649, mean reward: 0.832 [0.696, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.340, 10.100], loss: 0.001666, mae: 0.045597, mean_q: 1.252532
 78108/100000: episode: 1475, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 11.184, mean reward: 0.799 [0.746, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.542, 10.100], loss: 0.001515, mae: 0.043539, mean_q: 1.222987
 78114/100000: episode: 1476, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 4.810, mean reward: 0.802 [0.766, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.676, 10.100], loss: 0.001548, mae: 0.040570, mean_q: 1.244398
 78127/100000: episode: 1477, duration: 0.085s, episode steps: 13, steps per second: 152, episode reward: 10.729, mean reward: 0.825 [0.776, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.688, 10.100], loss: 0.001309, mae: 0.040995, mean_q: 1.245306
 78140/100000: episode: 1478, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 9.107, mean reward: 0.701 [0.642, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.333, 10.100], loss: 0.001587, mae: 0.043612, mean_q: 1.246204
 78153/100000: episode: 1479, duration: 0.083s, episode steps: 13, steps per second: 158, episode reward: 9.646, mean reward: 0.742 [0.692, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.426, 10.100], loss: 0.001609, mae: 0.045110, mean_q: 1.247288
 78166/100000: episode: 1480, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 9.387, mean reward: 0.722 [0.651, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.647, 10.100], loss: 0.001617, mae: 0.043068, mean_q: 1.242222
 78179/100000: episode: 1481, duration: 0.089s, episode steps: 13, steps per second: 147, episode reward: 10.303, mean reward: 0.793 [0.752, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.261, 10.100], loss: 0.001588, mae: 0.044493, mean_q: 1.252253
 78211/100000: episode: 1482, duration: 0.184s, episode steps: 32, steps per second: 174, episode reward: 22.476, mean reward: 0.702 [0.609, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.688, 10.100], loss: 0.001278, mae: 0.039881, mean_q: 1.240817
 78243/100000: episode: 1483, duration: 0.200s, episode steps: 32, steps per second: 160, episode reward: 22.967, mean reward: 0.718 [0.552, 0.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.461, 10.100], loss: 0.001485, mae: 0.041870, mean_q: 1.253526
 78256/100000: episode: 1484, duration: 0.106s, episode steps: 13, steps per second: 122, episode reward: 10.480, mean reward: 0.806 [0.787, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.415, 10.100], loss: 0.001313, mae: 0.040444, mean_q: 1.251075
 78269/100000: episode: 1485, duration: 0.110s, episode steps: 13, steps per second: 118, episode reward: 9.867, mean reward: 0.759 [0.680, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.489, 10.100], loss: 0.001240, mae: 0.038989, mean_q: 1.249029
 78286/100000: episode: 1486, duration: 0.162s, episode steps: 17, steps per second: 105, episode reward: 13.507, mean reward: 0.795 [0.724, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.527, 10.100], loss: 0.001265, mae: 0.038941, mean_q: 1.260500
 78303/100000: episode: 1487, duration: 0.133s, episode steps: 17, steps per second: 128, episode reward: 13.920, mean reward: 0.819 [0.700, 0.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.559, 10.100], loss: 0.001515, mae: 0.043135, mean_q: 1.254412
 78318/100000: episode: 1488, duration: 0.106s, episode steps: 15, steps per second: 142, episode reward: 11.353, mean reward: 0.757 [0.686, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.638, 10.100], loss: 0.001514, mae: 0.043555, mean_q: 1.266884
 78350/100000: episode: 1489, duration: 0.205s, episode steps: 32, steps per second: 156, episode reward: 23.320, mean reward: 0.729 [0.638, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.424, 10.100], loss: 0.001347, mae: 0.040587, mean_q: 1.258749
 78367/100000: episode: 1490, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 14.792, mean reward: 0.870 [0.738, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.307, 10.100], loss: 0.001166, mae: 0.037679, mean_q: 1.252506
 78378/100000: episode: 1491, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 9.057, mean reward: 0.823 [0.782, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.444, 10.100], loss: 0.001290, mae: 0.040043, mean_q: 1.266750
 78388/100000: episode: 1492, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 7.628, mean reward: 0.763 [0.706, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.467], loss: 0.001420, mae: 0.041024, mean_q: 1.259176
 78401/100000: episode: 1493, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 9.865, mean reward: 0.759 [0.670, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.092, 10.100], loss: 0.001522, mae: 0.043589, mean_q: 1.264377
 78416/100000: episode: 1494, duration: 0.101s, episode steps: 15, steps per second: 149, episode reward: 11.350, mean reward: 0.757 [0.687, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.441, 10.100], loss: 0.001632, mae: 0.043021, mean_q: 1.258431
 78448/100000: episode: 1495, duration: 0.216s, episode steps: 32, steps per second: 148, episode reward: 26.108, mean reward: 0.816 [0.701, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.929, 10.100], loss: 0.001509, mae: 0.043181, mean_q: 1.266740
 78480/100000: episode: 1496, duration: 0.204s, episode steps: 32, steps per second: 157, episode reward: 22.753, mean reward: 0.711 [0.623, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.290, 10.100], loss: 0.001285, mae: 0.039929, mean_q: 1.271000
 78491/100000: episode: 1497, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 8.157, mean reward: 0.742 [0.704, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.370, 10.100], loss: 0.001410, mae: 0.042825, mean_q: 1.276809
 78502/100000: episode: 1498, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 8.306, mean reward: 0.755 [0.706, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.270, 10.100], loss: 0.001493, mae: 0.043017, mean_q: 1.289941
 78515/100000: episode: 1499, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 10.334, mean reward: 0.795 [0.737, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.448, 10.100], loss: 0.001266, mae: 0.040004, mean_q: 1.249923
 78526/100000: episode: 1500, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 8.710, mean reward: 0.792 [0.734, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.391, 10.100], loss: 0.001353, mae: 0.040899, mean_q: 1.278029
 78543/100000: episode: 1501, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 11.720, mean reward: 0.689 [0.640, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.331, 10.100], loss: 0.001264, mae: 0.039303, mean_q: 1.270152
 78554/100000: episode: 1502, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 8.083, mean reward: 0.735 [0.704, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.460, 10.100], loss: 0.001496, mae: 0.043190, mean_q: 1.262569
 78567/100000: episode: 1503, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 10.147, mean reward: 0.781 [0.712, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.275, 10.100], loss: 0.001450, mae: 0.042540, mean_q: 1.251424
 78580/100000: episode: 1504, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 9.763, mean reward: 0.751 [0.655, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.393, 10.100], loss: 0.001387, mae: 0.041093, mean_q: 1.280172
 78586/100000: episode: 1505, duration: 0.043s, episode steps: 6, steps per second: 138, episode reward: 4.890, mean reward: 0.815 [0.778, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.388, 10.100], loss: 0.001685, mae: 0.043682, mean_q: 1.285140
 78599/100000: episode: 1506, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 10.689, mean reward: 0.822 [0.751, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.521, 10.100], loss: 0.001385, mae: 0.041837, mean_q: 1.278335
 78614/100000: episode: 1507, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 10.921, mean reward: 0.728 [0.701, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.373, 10.100], loss: 0.001318, mae: 0.039939, mean_q: 1.271004
 78631/100000: episode: 1508, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 12.588, mean reward: 0.740 [0.686, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.384, 10.100], loss: 0.001424, mae: 0.041545, mean_q: 1.267386
 78644/100000: episode: 1509, duration: 0.083s, episode steps: 13, steps per second: 156, episode reward: 9.921, mean reward: 0.763 [0.726, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.488, 10.100], loss: 0.001416, mae: 0.043217, mean_q: 1.272738
 78657/100000: episode: 1510, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 9.215, mean reward: 0.709 [0.664, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.420, 10.100], loss: 0.001437, mae: 0.042153, mean_q: 1.277515
 78668/100000: episode: 1511, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 8.168, mean reward: 0.743 [0.688, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.787, 10.100], loss: 0.001498, mae: 0.043220, mean_q: 1.272867
 78700/100000: episode: 1512, duration: 0.210s, episode steps: 32, steps per second: 152, episode reward: 22.255, mean reward: 0.695 [0.574, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.237, 10.100], loss: 0.001181, mae: 0.037712, mean_q: 1.265248
 78713/100000: episode: 1513, duration: 0.101s, episode steps: 13, steps per second: 128, episode reward: 9.409, mean reward: 0.724 [0.672, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.312, 10.100], loss: 0.001277, mae: 0.039512, mean_q: 1.265219
 78719/100000: episode: 1514, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 4.851, mean reward: 0.808 [0.795, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.503, 10.100], loss: 0.001150, mae: 0.039089, mean_q: 1.259913
 78734/100000: episode: 1515, duration: 0.100s, episode steps: 15, steps per second: 151, episode reward: 11.512, mean reward: 0.767 [0.695, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.322, 10.100], loss: 0.001409, mae: 0.041699, mean_q: 1.281580
 78766/100000: episode: 1516, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 21.659, mean reward: 0.677 [0.581, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.482, 10.100], loss: 0.001339, mae: 0.040363, mean_q: 1.271991
 78776/100000: episode: 1517, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 7.804, mean reward: 0.780 [0.718, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.510], loss: 0.001103, mae: 0.036740, mean_q: 1.262295
 78791/100000: episode: 1518, duration: 0.106s, episode steps: 15, steps per second: 141, episode reward: 11.125, mean reward: 0.742 [0.657, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.261, 10.100], loss: 0.001435, mae: 0.042449, mean_q: 1.280512
 78823/100000: episode: 1519, duration: 0.211s, episode steps: 32, steps per second: 152, episode reward: 23.958, mean reward: 0.749 [0.666, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.658, 10.100], loss: 0.001241, mae: 0.038996, mean_q: 1.278610
 78837/100000: episode: 1520, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 10.899, mean reward: 0.779 [0.737, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.292, 10.100], loss: 0.001352, mae: 0.040113, mean_q: 1.268781
 78869/100000: episode: 1521, duration: 0.193s, episode steps: 32, steps per second: 166, episode reward: 24.178, mean reward: 0.756 [0.676, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.111, 10.100], loss: 0.001474, mae: 0.041535, mean_q: 1.275922
 78875/100000: episode: 1522, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 4.835, mean reward: 0.806 [0.738, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-1.401, 10.100], loss: 0.001104, mae: 0.037750, mean_q: 1.258328
 78885/100000: episode: 1523, duration: 0.068s, episode steps: 10, steps per second: 146, episode reward: 7.918, mean reward: 0.792 [0.740, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.563], loss: 0.001243, mae: 0.040357, mean_q: 1.276530
 78917/100000: episode: 1524, duration: 0.212s, episode steps: 32, steps per second: 151, episode reward: 23.494, mean reward: 0.734 [0.580, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.691, 10.100], loss: 0.001185, mae: 0.038762, mean_q: 1.275578
 78934/100000: episode: 1525, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 12.499, mean reward: 0.735 [0.613, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.389, 10.100], loss: 0.001325, mae: 0.040420, mean_q: 1.273171
 78944/100000: episode: 1526, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 8.191, mean reward: 0.819 [0.750, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.557], loss: 0.001649, mae: 0.044848, mean_q: 1.288526
 78959/100000: episode: 1527, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 10.884, mean reward: 0.726 [0.683, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.264, 10.100], loss: 0.001433, mae: 0.042878, mean_q: 1.276954
 78991/100000: episode: 1528, duration: 0.203s, episode steps: 32, steps per second: 158, episode reward: 21.842, mean reward: 0.683 [0.601, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.299, 10.100], loss: 0.001393, mae: 0.041249, mean_q: 1.277365
 79004/100000: episode: 1529, duration: 0.106s, episode steps: 13, steps per second: 123, episode reward: 10.803, mean reward: 0.831 [0.780, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.188, 10.100], loss: 0.001358, mae: 0.041070, mean_q: 1.265536
[Info] FALSIFICATION!
 79013/100000: episode: 1530, duration: 0.244s, episode steps: 9, steps per second: 37, episode reward: 7.826, mean reward: 0.870 [0.787, 1.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.858, 10.093], loss: 0.001628, mae: 0.043854, mean_q: 1.273767
 79024/100000: episode: 1531, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 8.973, mean reward: 0.816 [0.753, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.414, 10.100], loss: 0.002279, mae: 0.047803, mean_q: 1.281097
 79034/100000: episode: 1532, duration: 0.069s, episode steps: 10, steps per second: 144, episode reward: 8.084, mean reward: 0.808 [0.693, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.152, 10.623], loss: 0.001534, mae: 0.041626, mean_q: 1.279186
 79045/100000: episode: 1533, duration: 0.090s, episode steps: 11, steps per second: 122, episode reward: 8.888, mean reward: 0.808 [0.771, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.390, 10.100], loss: 0.001344, mae: 0.040450, mean_q: 1.265106
 79058/100000: episode: 1534, duration: 0.101s, episode steps: 13, steps per second: 129, episode reward: 10.826, mean reward: 0.833 [0.727, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.967, 10.100], loss: 0.001366, mae: 0.041710, mean_q: 1.273924
 79072/100000: episode: 1535, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 10.369, mean reward: 0.741 [0.637, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.777, 10.100], loss: 0.001691, mae: 0.044165, mean_q: 1.285752
 79083/100000: episode: 1536, duration: 0.073s, episode steps: 11, steps per second: 151, episode reward: 8.706, mean reward: 0.791 [0.736, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.399, 10.100], loss: 0.001936, mae: 0.043606, mean_q: 1.281303
 79096/100000: episode: 1537, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 9.762, mean reward: 0.751 [0.712, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.455, 10.100], loss: 0.001549, mae: 0.042262, mean_q: 1.276150
 79111/100000: episode: 1538, duration: 0.130s, episode steps: 15, steps per second: 116, episode reward: 10.964, mean reward: 0.731 [0.623, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.870, 10.100], loss: 0.001390, mae: 0.041139, mean_q: 1.289693
 79124/100000: episode: 1539, duration: 0.079s, episode steps: 13, steps per second: 166, episode reward: 10.147, mean reward: 0.781 [0.723, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.253, 10.100], loss: 0.001309, mae: 0.040804, mean_q: 1.285224
[Info] Complete ISplit Iteration
[Info] Levels: [1.3750851, 1.5007684, 1.6024166]
[Info] Cond. Prob: [0.1, 0.1, 0.41]
[Info] Error Prob: 0.0041

 79134/100000: episode: 1540, duration: 5.498s, episode steps: 10, steps per second: 2, episode reward: 6.929, mean reward: 0.693 [0.575, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.429], loss: 0.001244, mae: 0.039177, mean_q: 1.275574
 79234/100000: episode: 1541, duration: 0.969s, episode steps: 100, steps per second: 103, episode reward: 59.363, mean reward: 0.594 [0.505, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.599, 10.179], loss: 0.001524, mae: 0.043607, mean_q: 1.284715
 79334/100000: episode: 1542, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: 58.626, mean reward: 0.586 [0.511, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.281, 10.098], loss: 0.001477, mae: 0.042332, mean_q: 1.285164
 79434/100000: episode: 1543, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: 57.151, mean reward: 0.572 [0.502, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.630, 10.098], loss: 0.001362, mae: 0.041139, mean_q: 1.280823
 79534/100000: episode: 1544, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 59.315, mean reward: 0.593 [0.507, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.727, 10.098], loss: 0.001432, mae: 0.040931, mean_q: 1.279305
 79634/100000: episode: 1545, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: 57.920, mean reward: 0.579 [0.501, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.135, 10.098], loss: 0.001494, mae: 0.042282, mean_q: 1.284656
 79734/100000: episode: 1546, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 57.398, mean reward: 0.574 [0.499, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.579, 10.098], loss: 0.001695, mae: 0.043893, mean_q: 1.275409
 79834/100000: episode: 1547, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: 59.263, mean reward: 0.593 [0.506, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.581, 10.323], loss: 0.001648, mae: 0.044259, mean_q: 1.275086
 79934/100000: episode: 1548, duration: 0.663s, episode steps: 100, steps per second: 151, episode reward: 57.921, mean reward: 0.579 [0.507, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.473, 10.170], loss: 0.001394, mae: 0.040844, mean_q: 1.277358
 80034/100000: episode: 1549, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 60.319, mean reward: 0.603 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.632, 10.098], loss: 0.001619, mae: 0.043414, mean_q: 1.270781
 80134/100000: episode: 1550, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 59.360, mean reward: 0.594 [0.507, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.352, 10.098], loss: 0.001401, mae: 0.040798, mean_q: 1.276008
 80234/100000: episode: 1551, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 57.395, mean reward: 0.574 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.960, 10.098], loss: 0.001469, mae: 0.040988, mean_q: 1.269333
 80334/100000: episode: 1552, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 58.282, mean reward: 0.583 [0.505, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.478, 10.098], loss: 0.001503, mae: 0.041855, mean_q: 1.266677
 80434/100000: episode: 1553, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 61.536, mean reward: 0.615 [0.504, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.749, 10.098], loss: 0.001717, mae: 0.043952, mean_q: 1.266497
 80534/100000: episode: 1554, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: 59.821, mean reward: 0.598 [0.509, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.965, 10.098], loss: 0.001806, mae: 0.045074, mean_q: 1.273718
 80634/100000: episode: 1555, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: 59.520, mean reward: 0.595 [0.509, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.908, 10.098], loss: 0.001460, mae: 0.041062, mean_q: 1.266485
 80734/100000: episode: 1556, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: 58.115, mean reward: 0.581 [0.500, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.691, 10.382], loss: 0.001632, mae: 0.042366, mean_q: 1.265068
 80834/100000: episode: 1557, duration: 0.643s, episode steps: 100, steps per second: 156, episode reward: 61.152, mean reward: 0.612 [0.522, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.820, 10.183], loss: 0.001558, mae: 0.042380, mean_q: 1.258288
 80934/100000: episode: 1558, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 57.089, mean reward: 0.571 [0.503, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.600, 10.131], loss: 0.001551, mae: 0.042445, mean_q: 1.256892
 81034/100000: episode: 1559, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: 59.777, mean reward: 0.598 [0.501, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.551, 10.317], loss: 0.001641, mae: 0.043070, mean_q: 1.250218
 81134/100000: episode: 1560, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 61.729, mean reward: 0.617 [0.516, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.177, 10.358], loss: 0.001483, mae: 0.041510, mean_q: 1.259337
 81234/100000: episode: 1561, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 57.804, mean reward: 0.578 [0.499, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.629, 10.098], loss: 0.001523, mae: 0.042379, mean_q: 1.247039
 81334/100000: episode: 1562, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: 60.763, mean reward: 0.608 [0.508, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.302, 10.098], loss: 0.001471, mae: 0.041678, mean_q: 1.258661
 81434/100000: episode: 1563, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.651, mean reward: 0.587 [0.503, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.590, 10.116], loss: 0.001513, mae: 0.041461, mean_q: 1.246699
 81534/100000: episode: 1564, duration: 0.636s, episode steps: 100, steps per second: 157, episode reward: 61.027, mean reward: 0.610 [0.508, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.372, 10.098], loss: 0.001686, mae: 0.043766, mean_q: 1.246415
 81634/100000: episode: 1565, duration: 0.645s, episode steps: 100, steps per second: 155, episode reward: 60.080, mean reward: 0.601 [0.509, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.154, 10.163], loss: 0.001897, mae: 0.046759, mean_q: 1.252779
 81734/100000: episode: 1566, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: 60.619, mean reward: 0.606 [0.501, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.027, 10.388], loss: 0.001586, mae: 0.042181, mean_q: 1.243320
 81834/100000: episode: 1567, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 60.702, mean reward: 0.607 [0.514, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.880, 10.261], loss: 0.001600, mae: 0.042963, mean_q: 1.247642
 81934/100000: episode: 1568, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 59.802, mean reward: 0.598 [0.509, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.280, 10.148], loss: 0.001706, mae: 0.044216, mean_q: 1.242717
 82034/100000: episode: 1569, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 64.319, mean reward: 0.643 [0.522, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.582, 10.098], loss: 0.001715, mae: 0.044037, mean_q: 1.248910
 82134/100000: episode: 1570, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 58.019, mean reward: 0.580 [0.501, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.549, 10.098], loss: 0.001901, mae: 0.046656, mean_q: 1.246359
 82234/100000: episode: 1571, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 60.485, mean reward: 0.605 [0.508, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.931, 10.173], loss: 0.001708, mae: 0.044606, mean_q: 1.247474
 82334/100000: episode: 1572, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 58.981, mean reward: 0.590 [0.504, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.817, 10.098], loss: 0.001711, mae: 0.044893, mean_q: 1.243120
 82434/100000: episode: 1573, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 59.669, mean reward: 0.597 [0.501, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.172, 10.172], loss: 0.001632, mae: 0.043522, mean_q: 1.237433
 82534/100000: episode: 1574, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: 65.181, mean reward: 0.652 [0.524, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.222, 10.314], loss: 0.001733, mae: 0.043128, mean_q: 1.244182
 82634/100000: episode: 1575, duration: 0.629s, episode steps: 100, steps per second: 159, episode reward: 58.658, mean reward: 0.587 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.814, 10.153], loss: 0.001494, mae: 0.040942, mean_q: 1.243890
 82734/100000: episode: 1576, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 59.626, mean reward: 0.596 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.613, 10.098], loss: 0.001785, mae: 0.045313, mean_q: 1.239819
 82834/100000: episode: 1577, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 63.040, mean reward: 0.630 [0.514, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.915, 10.319], loss: 0.001435, mae: 0.041327, mean_q: 1.243810
 82934/100000: episode: 1578, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 60.343, mean reward: 0.603 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.252, 10.146], loss: 0.001903, mae: 0.046314, mean_q: 1.235927
 83034/100000: episode: 1579, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.299, mean reward: 0.583 [0.508, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.807, 10.098], loss: 0.001679, mae: 0.043777, mean_q: 1.234591
 83134/100000: episode: 1580, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 65.117, mean reward: 0.651 [0.511, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.755, 10.098], loss: 0.001553, mae: 0.042353, mean_q: 1.227801
 83234/100000: episode: 1581, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 58.233, mean reward: 0.582 [0.509, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.343, 10.098], loss: 0.001522, mae: 0.041833, mean_q: 1.218219
 83334/100000: episode: 1582, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.160, mean reward: 0.582 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.202, 10.098], loss: 0.001722, mae: 0.044264, mean_q: 1.211402
 83434/100000: episode: 1583, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 59.389, mean reward: 0.594 [0.514, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.550, 10.130], loss: 0.001613, mae: 0.043145, mean_q: 1.209545
 83534/100000: episode: 1584, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.799, mean reward: 0.588 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.546, 10.163], loss: 0.001673, mae: 0.043557, mean_q: 1.208835
 83634/100000: episode: 1585, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 62.734, mean reward: 0.627 [0.503, 0.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.078, 10.431], loss: 0.001520, mae: 0.041693, mean_q: 1.203520
 83734/100000: episode: 1586, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 57.543, mean reward: 0.575 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.762, 10.098], loss: 0.001820, mae: 0.044218, mean_q: 1.193562
 83834/100000: episode: 1587, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 58.002, mean reward: 0.580 [0.508, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.204, 10.179], loss: 0.001797, mae: 0.045494, mean_q: 1.196718
 83934/100000: episode: 1588, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 56.959, mean reward: 0.570 [0.498, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.495, 10.224], loss: 0.001586, mae: 0.042062, mean_q: 1.187237
 84034/100000: episode: 1589, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 59.202, mean reward: 0.592 [0.509, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.941, 10.289], loss: 0.001454, mae: 0.041736, mean_q: 1.182623
 84134/100000: episode: 1590, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.092, mean reward: 0.601 [0.512, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.463, 10.098], loss: 0.001360, mae: 0.040558, mean_q: 1.178879
 84234/100000: episode: 1591, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.926, mean reward: 0.589 [0.503, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.860, 10.215], loss: 0.001525, mae: 0.042004, mean_q: 1.181819
 84334/100000: episode: 1592, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.924, mean reward: 0.579 [0.502, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.734, 10.098], loss: 0.001510, mae: 0.042041, mean_q: 1.182688
 84434/100000: episode: 1593, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 56.562, mean reward: 0.566 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.968, 10.180], loss: 0.001447, mae: 0.041489, mean_q: 1.182110
 84534/100000: episode: 1594, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 58.245, mean reward: 0.582 [0.499, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.437, 10.098], loss: 0.001554, mae: 0.042781, mean_q: 1.178666
 84634/100000: episode: 1595, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 57.430, mean reward: 0.574 [0.500, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.670, 10.098], loss: 0.001342, mae: 0.040424, mean_q: 1.180280
 84734/100000: episode: 1596, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 57.944, mean reward: 0.579 [0.500, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.190, 10.181], loss: 0.001392, mae: 0.040919, mean_q: 1.180516
 84834/100000: episode: 1597, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 57.246, mean reward: 0.572 [0.500, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.335, 10.164], loss: 0.001433, mae: 0.041578, mean_q: 1.179352
 84934/100000: episode: 1598, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.764, mean reward: 0.588 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.026, 10.098], loss: 0.001430, mae: 0.041350, mean_q: 1.176906
 85034/100000: episode: 1599, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 56.843, mean reward: 0.568 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.468, 10.122], loss: 0.001558, mae: 0.042559, mean_q: 1.178779
 85134/100000: episode: 1600, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.081, mean reward: 0.591 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.436, 10.098], loss: 0.001456, mae: 0.041249, mean_q: 1.177828
 85234/100000: episode: 1601, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 56.793, mean reward: 0.568 [0.507, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.562, 10.098], loss: 0.001435, mae: 0.041342, mean_q: 1.174137
 85334/100000: episode: 1602, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.371, mean reward: 0.594 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.463, 10.098], loss: 0.001462, mae: 0.041401, mean_q: 1.176674
 85434/100000: episode: 1603, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.542, mean reward: 0.595 [0.504, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.468, 10.382], loss: 0.001431, mae: 0.041381, mean_q: 1.179744
 85534/100000: episode: 1604, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 61.697, mean reward: 0.617 [0.527, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.706, 10.098], loss: 0.001466, mae: 0.041359, mean_q: 1.177816
 85634/100000: episode: 1605, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 57.714, mean reward: 0.577 [0.504, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.053, 10.337], loss: 0.001418, mae: 0.041562, mean_q: 1.178832
 85734/100000: episode: 1606, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 59.521, mean reward: 0.595 [0.506, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.416, 10.098], loss: 0.001347, mae: 0.040378, mean_q: 1.177603
 85834/100000: episode: 1607, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 60.334, mean reward: 0.603 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.971, 10.398], loss: 0.001364, mae: 0.040402, mean_q: 1.174137
 85934/100000: episode: 1608, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.180, mean reward: 0.582 [0.499, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.500, 10.141], loss: 0.001467, mae: 0.041866, mean_q: 1.176315
 86034/100000: episode: 1609, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 64.377, mean reward: 0.644 [0.521, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.632, 10.256], loss: 0.001331, mae: 0.040315, mean_q: 1.176480
 86134/100000: episode: 1610, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 57.636, mean reward: 0.576 [0.506, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.620, 10.318], loss: 0.001287, mae: 0.039310, mean_q: 1.171353
 86234/100000: episode: 1611, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.823, mean reward: 0.578 [0.506, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.381, 10.098], loss: 0.001402, mae: 0.041594, mean_q: 1.176286
 86334/100000: episode: 1612, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 58.965, mean reward: 0.590 [0.507, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.907, 10.183], loss: 0.001421, mae: 0.040851, mean_q: 1.175613
 86434/100000: episode: 1613, duration: 0.601s, episode steps: 100, steps per second: 166, episode reward: 59.592, mean reward: 0.596 [0.499, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.850, 10.098], loss: 0.001403, mae: 0.040715, mean_q: 1.177145
 86534/100000: episode: 1614, duration: 0.791s, episode steps: 100, steps per second: 126, episode reward: 57.079, mean reward: 0.571 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.232, 10.134], loss: 0.001449, mae: 0.041783, mean_q: 1.177495
 86634/100000: episode: 1615, duration: 1.195s, episode steps: 100, steps per second: 84, episode reward: 59.844, mean reward: 0.598 [0.508, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.598, 10.098], loss: 0.001388, mae: 0.041055, mean_q: 1.173461
 86734/100000: episode: 1616, duration: 1.105s, episode steps: 100, steps per second: 90, episode reward: 59.622, mean reward: 0.596 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.577, 10.098], loss: 0.001479, mae: 0.041892, mean_q: 1.177561
 86834/100000: episode: 1617, duration: 1.146s, episode steps: 100, steps per second: 87, episode reward: 59.830, mean reward: 0.598 [0.505, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.838, 10.098], loss: 0.001311, mae: 0.039702, mean_q: 1.173335
 86934/100000: episode: 1618, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: 61.303, mean reward: 0.613 [0.510, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.219, 10.457], loss: 0.001446, mae: 0.041502, mean_q: 1.172318
 87034/100000: episode: 1619, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: 58.570, mean reward: 0.586 [0.502, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.382, 10.395], loss: 0.001470, mae: 0.041985, mean_q: 1.173224
 87134/100000: episode: 1620, duration: 1.027s, episode steps: 100, steps per second: 97, episode reward: 60.261, mean reward: 0.603 [0.511, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.806, 10.098], loss: 0.001541, mae: 0.042784, mean_q: 1.173272
 87234/100000: episode: 1621, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: 57.484, mean reward: 0.575 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.817, 10.128], loss: 0.001427, mae: 0.040629, mean_q: 1.170887
 87334/100000: episode: 1622, duration: 0.836s, episode steps: 100, steps per second: 120, episode reward: 56.708, mean reward: 0.567 [0.501, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.861, 10.234], loss: 0.001393, mae: 0.041118, mean_q: 1.171533
 87434/100000: episode: 1623, duration: 1.239s, episode steps: 100, steps per second: 81, episode reward: 57.424, mean reward: 0.574 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.211, 10.156], loss: 0.001382, mae: 0.040933, mean_q: 1.174663
 87534/100000: episode: 1624, duration: 0.866s, episode steps: 100, steps per second: 115, episode reward: 59.691, mean reward: 0.597 [0.506, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.394, 10.098], loss: 0.001396, mae: 0.040898, mean_q: 1.169109
 87634/100000: episode: 1625, duration: 1.023s, episode steps: 100, steps per second: 98, episode reward: 58.044, mean reward: 0.580 [0.508, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.865, 10.179], loss: 0.001388, mae: 0.040857, mean_q: 1.169800
 87734/100000: episode: 1626, duration: 0.662s, episode steps: 100, steps per second: 151, episode reward: 65.182, mean reward: 0.652 [0.511, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.430, 10.098], loss: 0.001554, mae: 0.042839, mean_q: 1.164572
 87834/100000: episode: 1627, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: 60.722, mean reward: 0.607 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.331, 10.193], loss: 0.001534, mae: 0.042237, mean_q: 1.168940
 87934/100000: episode: 1628, duration: 0.916s, episode steps: 100, steps per second: 109, episode reward: 57.467, mean reward: 0.575 [0.503, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.097, 10.098], loss: 0.001422, mae: 0.041368, mean_q: 1.168285
 88034/100000: episode: 1629, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: 56.911, mean reward: 0.569 [0.506, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.483, 10.098], loss: 0.001441, mae: 0.041784, mean_q: 1.168210
 88134/100000: episode: 1630, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: 57.201, mean reward: 0.572 [0.505, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.897, 10.165], loss: 0.001316, mae: 0.039780, mean_q: 1.162817
 88234/100000: episode: 1631, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: 57.348, mean reward: 0.573 [0.500, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.788, 10.229], loss: 0.001396, mae: 0.040640, mean_q: 1.164812
 88334/100000: episode: 1632, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: 60.013, mean reward: 0.600 [0.519, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.285, 10.203], loss: 0.001423, mae: 0.041186, mean_q: 1.163643
 88434/100000: episode: 1633, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 63.323, mean reward: 0.633 [0.503, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.850, 10.098], loss: 0.001292, mae: 0.039017, mean_q: 1.162169
 88534/100000: episode: 1634, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 59.468, mean reward: 0.595 [0.510, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.750, 10.098], loss: 0.001393, mae: 0.041019, mean_q: 1.163194
 88634/100000: episode: 1635, duration: 0.678s, episode steps: 100, steps per second: 148, episode reward: 58.424, mean reward: 0.584 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.901, 10.098], loss: 0.001423, mae: 0.041009, mean_q: 1.162295
 88734/100000: episode: 1636, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 55.785, mean reward: 0.558 [0.499, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.352, 10.208], loss: 0.001357, mae: 0.040752, mean_q: 1.164127
 88834/100000: episode: 1637, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 63.950, mean reward: 0.639 [0.522, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.705, 10.098], loss: 0.001380, mae: 0.040634, mean_q: 1.163709
 88934/100000: episode: 1638, duration: 0.607s, episode steps: 100, steps per second: 165, episode reward: 59.353, mean reward: 0.594 [0.517, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.190, 10.255], loss: 0.001498, mae: 0.041542, mean_q: 1.163196
 89034/100000: episode: 1639, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 60.356, mean reward: 0.604 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.549, 10.299], loss: 0.001495, mae: 0.042467, mean_q: 1.168710
[Info] 1-TH LEVEL FOUND: 1.4455885887145996, Considering 10/90 traces
 89134/100000: episode: 1640, duration: 5.128s, episode steps: 100, steps per second: 20, episode reward: 57.373, mean reward: 0.574 [0.504, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.774, 10.125], loss: 0.001435, mae: 0.041641, mean_q: 1.163304
 89154/100000: episode: 1641, duration: 0.125s, episode steps: 20, steps per second: 160, episode reward: 14.748, mean reward: 0.737 [0.673, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.643, 10.422], loss: 0.001250, mae: 0.038856, mean_q: 1.169340
 89174/100000: episode: 1642, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 14.244, mean reward: 0.712 [0.652, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.892, 10.100], loss: 0.001315, mae: 0.040412, mean_q: 1.176600
 89193/100000: episode: 1643, duration: 0.115s, episode steps: 19, steps per second: 166, episode reward: 14.310, mean reward: 0.753 [0.709, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.445, 10.100], loss: 0.001369, mae: 0.040343, mean_q: 1.163800
 89202/100000: episode: 1644, duration: 0.065s, episode steps: 9, steps per second: 138, episode reward: 6.591, mean reward: 0.732 [0.673, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.053, 10.405], loss: 0.001587, mae: 0.041020, mean_q: 1.171391
 89222/100000: episode: 1645, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 15.173, mean reward: 0.759 [0.661, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.252, 10.383], loss: 0.001501, mae: 0.042366, mean_q: 1.170980
 89256/100000: episode: 1646, duration: 0.208s, episode steps: 34, steps per second: 163, episode reward: 23.762, mean reward: 0.699 [0.585, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.307], loss: 0.001411, mae: 0.040707, mean_q: 1.169357
 89276/100000: episode: 1647, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 15.128, mean reward: 0.756 [0.669, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.179, 10.100], loss: 0.001328, mae: 0.040187, mean_q: 1.169515
 89295/100000: episode: 1648, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 12.688, mean reward: 0.668 [0.541, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.090, 10.100], loss: 0.001575, mae: 0.043086, mean_q: 1.180579
 89311/100000: episode: 1649, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 12.234, mean reward: 0.765 [0.691, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.100, 10.558], loss: 0.001512, mae: 0.042686, mean_q: 1.168424
[Info] FALSIFICATION!
 89330/100000: episode: 1650, duration: 0.256s, episode steps: 19, steps per second: 74, episode reward: 16.788, mean reward: 0.884 [0.776, 1.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.201, 10.578], loss: 0.001637, mae: 0.044021, mean_q: 1.175406
 89350/100000: episode: 1651, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 14.728, mean reward: 0.736 [0.679, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.982, 10.592], loss: 0.001301, mae: 0.039433, mean_q: 1.175802
 89370/100000: episode: 1652, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 13.889, mean reward: 0.694 [0.626, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.094, 10.100], loss: 0.001399, mae: 0.041594, mean_q: 1.169286
 89379/100000: episode: 1653, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 6.780, mean reward: 0.753 [0.693, 0.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.366, 10.422], loss: 0.001543, mae: 0.043400, mean_q: 1.177367
 89395/100000: episode: 1654, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 11.344, mean reward: 0.709 [0.645, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.339], loss: 0.001605, mae: 0.042779, mean_q: 1.166751
 89426/100000: episode: 1655, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 24.844, mean reward: 0.801 [0.676, 0.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.668, 10.100], loss: 0.001508, mae: 0.042177, mean_q: 1.177692
 89435/100000: episode: 1656, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 6.257, mean reward: 0.695 [0.640, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.370], loss: 0.001804, mae: 0.045619, mean_q: 1.185901
 89477/100000: episode: 1657, duration: 0.241s, episode steps: 42, steps per second: 175, episode reward: 26.048, mean reward: 0.620 [0.533, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.996, 10.313], loss: 0.001857, mae: 0.045021, mean_q: 1.179453
 89508/100000: episode: 1658, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 21.379, mean reward: 0.690 [0.639, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.029, 10.100], loss: 0.001423, mae: 0.042325, mean_q: 1.191014
 89517/100000: episode: 1659, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 7.065, mean reward: 0.785 [0.736, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.529], loss: 0.001665, mae: 0.044924, mean_q: 1.175014
 89537/100000: episode: 1660, duration: 0.117s, episode steps: 20, steps per second: 172, episode reward: 13.453, mean reward: 0.673 [0.613, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.442, 10.369], loss: 0.001358, mae: 0.039629, mean_q: 1.185578
 89551/100000: episode: 1661, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 9.820, mean reward: 0.701 [0.649, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.263, 10.100], loss: 0.001635, mae: 0.044243, mean_q: 1.185089
 89571/100000: episode: 1662, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 13.748, mean reward: 0.687 [0.595, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.300, 10.100], loss: 0.002009, mae: 0.046164, mean_q: 1.178058
 89600/100000: episode: 1663, duration: 0.186s, episode steps: 29, steps per second: 156, episode reward: 19.944, mean reward: 0.688 [0.572, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.235, 10.100], loss: 0.002069, mae: 0.049205, mean_q: 1.180792
 89620/100000: episode: 1664, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 15.763, mean reward: 0.788 [0.731, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.771, 10.543], loss: 0.001686, mae: 0.045031, mean_q: 1.191963
 89662/100000: episode: 1665, duration: 0.244s, episode steps: 42, steps per second: 172, episode reward: 28.482, mean reward: 0.678 [0.558, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-2.129, 10.319], loss: 0.001602, mae: 0.043281, mean_q: 1.191383
 89681/100000: episode: 1666, duration: 0.111s, episode steps: 19, steps per second: 172, episode reward: 13.513, mean reward: 0.711 [0.675, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.746, 10.100], loss: 0.001492, mae: 0.041258, mean_q: 1.195407
 89701/100000: episode: 1667, duration: 0.119s, episode steps: 20, steps per second: 169, episode reward: 13.701, mean reward: 0.685 [0.580, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.123, 10.288], loss: 0.001844, mae: 0.045611, mean_q: 1.192861
 89730/100000: episode: 1668, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 22.118, mean reward: 0.763 [0.643, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.505, 10.100], loss: 0.001660, mae: 0.043255, mean_q: 1.185035
 89759/100000: episode: 1669, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 19.186, mean reward: 0.662 [0.546, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.350, 10.100], loss: 0.001538, mae: 0.043197, mean_q: 1.197037
 89779/100000: episode: 1670, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 14.479, mean reward: 0.724 [0.685, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.604, 10.507], loss: 0.001927, mae: 0.045327, mean_q: 1.195354
 89810/100000: episode: 1671, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 21.835, mean reward: 0.704 [0.605, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.200, 10.100], loss: 0.001862, mae: 0.045572, mean_q: 1.193344
 89830/100000: episode: 1672, duration: 0.115s, episode steps: 20, steps per second: 173, episode reward: 15.206, mean reward: 0.760 [0.703, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.485], loss: 0.001694, mae: 0.044760, mean_q: 1.192467
 89844/100000: episode: 1673, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 9.598, mean reward: 0.686 [0.597, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.296, 10.100], loss: 0.001736, mae: 0.043393, mean_q: 1.191008
 89886/100000: episode: 1674, duration: 0.269s, episode steps: 42, steps per second: 156, episode reward: 28.315, mean reward: 0.674 [0.552, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.792, 10.307], loss: 0.001679, mae: 0.044063, mean_q: 1.200255
 89905/100000: episode: 1675, duration: 0.154s, episode steps: 19, steps per second: 124, episode reward: 13.078, mean reward: 0.688 [0.586, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.273, 10.100], loss: 0.001996, mae: 0.048398, mean_q: 1.197614
 89936/100000: episode: 1676, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 23.275, mean reward: 0.751 [0.674, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.848, 10.100], loss: 0.002064, mae: 0.048238, mean_q: 1.204922
 89956/100000: episode: 1677, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 14.220, mean reward: 0.711 [0.577, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.237, 10.100], loss: 0.001406, mae: 0.040846, mean_q: 1.196812
 89976/100000: episode: 1678, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 14.641, mean reward: 0.732 [0.634, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.208, 10.100], loss: 0.001547, mae: 0.042105, mean_q: 1.194407
 90018/100000: episode: 1679, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 26.434, mean reward: 0.629 [0.527, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.243, 10.205], loss: 0.001864, mae: 0.044194, mean_q: 1.206428
 90034/100000: episode: 1680, duration: 0.107s, episode steps: 16, steps per second: 150, episode reward: 11.919, mean reward: 0.745 [0.679, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.153, 10.562], loss: 0.002371, mae: 0.051343, mean_q: 1.198443
 90053/100000: episode: 1681, duration: 0.135s, episode steps: 19, steps per second: 140, episode reward: 14.201, mean reward: 0.747 [0.700, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.287, 10.100], loss: 0.001586, mae: 0.042797, mean_q: 1.209315
 90095/100000: episode: 1682, duration: 0.219s, episode steps: 42, steps per second: 192, episode reward: 26.217, mean reward: 0.624 [0.567, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-1.083, 10.279], loss: 0.001793, mae: 0.045037, mean_q: 1.205696
 90115/100000: episode: 1683, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 13.584, mean reward: 0.679 [0.603, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.303, 10.100], loss: 0.001985, mae: 0.047906, mean_q: 1.218742
 90149/100000: episode: 1684, duration: 0.203s, episode steps: 34, steps per second: 167, episode reward: 22.195, mean reward: 0.653 [0.594, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.227, 10.395], loss: 0.001682, mae: 0.044816, mean_q: 1.214648
 90169/100000: episode: 1685, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 13.561, mean reward: 0.678 [0.617, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.190, 10.100], loss: 0.001408, mae: 0.041751, mean_q: 1.218179
 90203/100000: episode: 1686, duration: 0.203s, episode steps: 34, steps per second: 168, episode reward: 22.293, mean reward: 0.656 [0.567, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.326], loss: 0.001571, mae: 0.042838, mean_q: 1.208825
 90223/100000: episode: 1687, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 13.253, mean reward: 0.663 [0.595, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.165, 10.100], loss: 0.001766, mae: 0.045110, mean_q: 1.217024
 90243/100000: episode: 1688, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 13.655, mean reward: 0.683 [0.617, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.989, 10.100], loss: 0.001816, mae: 0.044991, mean_q: 1.208358
 90263/100000: episode: 1689, duration: 0.130s, episode steps: 20, steps per second: 154, episode reward: 15.006, mean reward: 0.750 [0.658, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.190, 10.100], loss: 0.001779, mae: 0.044329, mean_q: 1.215006
 90305/100000: episode: 1690, duration: 0.248s, episode steps: 42, steps per second: 169, episode reward: 26.037, mean reward: 0.620 [0.518, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.291, 10.323], loss: 0.001517, mae: 0.042582, mean_q: 1.225692
 90324/100000: episode: 1691, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 12.884, mean reward: 0.678 [0.599, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.334, 10.100], loss: 0.001966, mae: 0.045357, mean_q: 1.215022
 90353/100000: episode: 1692, duration: 0.204s, episode steps: 29, steps per second: 142, episode reward: 18.573, mean reward: 0.640 [0.525, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.045, 10.100], loss: 0.001626, mae: 0.043663, mean_q: 1.211646
 90382/100000: episode: 1693, duration: 0.220s, episode steps: 29, steps per second: 132, episode reward: 21.672, mean reward: 0.747 [0.629, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.901, 10.100], loss: 0.001816, mae: 0.045462, mean_q: 1.215012
 90401/100000: episode: 1694, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 13.995, mean reward: 0.737 [0.688, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.323, 10.100], loss: 0.001701, mae: 0.045286, mean_q: 1.219798
 90421/100000: episode: 1695, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 15.205, mean reward: 0.760 [0.685, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.252, 10.454], loss: 0.001599, mae: 0.043532, mean_q: 1.216532
 90455/100000: episode: 1696, duration: 0.201s, episode steps: 34, steps per second: 170, episode reward: 23.669, mean reward: 0.696 [0.556, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.098, 10.261], loss: 0.001601, mae: 0.042925, mean_q: 1.215019
 90464/100000: episode: 1697, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 6.471, mean reward: 0.719 [0.666, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.469], loss: 0.001422, mae: 0.040752, mean_q: 1.245836
 90473/100000: episode: 1698, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 6.633, mean reward: 0.737 [0.648, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.411], loss: 0.001675, mae: 0.043130, mean_q: 1.208356
 90489/100000: episode: 1699, duration: 0.117s, episode steps: 16, steps per second: 136, episode reward: 12.776, mean reward: 0.798 [0.735, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.545], loss: 0.001362, mae: 0.040782, mean_q: 1.234184
 90531/100000: episode: 1700, duration: 0.243s, episode steps: 42, steps per second: 172, episode reward: 29.229, mean reward: 0.696 [0.616, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.534, 10.417], loss: 0.001582, mae: 0.042417, mean_q: 1.221954
 90540/100000: episode: 1701, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 6.897, mean reward: 0.766 [0.696, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.612, 10.437], loss: 0.001597, mae: 0.043860, mean_q: 1.206205
 90574/100000: episode: 1702, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 25.309, mean reward: 0.744 [0.606, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.542, 10.461], loss: 0.001565, mae: 0.042364, mean_q: 1.225446
 90593/100000: episode: 1703, duration: 0.121s, episode steps: 19, steps per second: 156, episode reward: 12.288, mean reward: 0.647 [0.506, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.329, 10.100], loss: 0.002026, mae: 0.047806, mean_q: 1.208216
 90607/100000: episode: 1704, duration: 0.105s, episode steps: 14, steps per second: 133, episode reward: 10.244, mean reward: 0.732 [0.703, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.769, 10.100], loss: 0.001494, mae: 0.043484, mean_q: 1.222924
 90626/100000: episode: 1705, duration: 0.142s, episode steps: 19, steps per second: 134, episode reward: 14.370, mean reward: 0.756 [0.711, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.965, 10.100], loss: 0.001618, mae: 0.042663, mean_q: 1.228297
 90646/100000: episode: 1706, duration: 0.139s, episode steps: 20, steps per second: 144, episode reward: 16.801, mean reward: 0.840 [0.736, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.530], loss: 0.001398, mae: 0.040031, mean_q: 1.222601
 90675/100000: episode: 1707, duration: 0.191s, episode steps: 29, steps per second: 152, episode reward: 19.769, mean reward: 0.682 [0.554, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.627, 10.100], loss: 0.001573, mae: 0.043820, mean_q: 1.234910
 90709/100000: episode: 1708, duration: 0.185s, episode steps: 34, steps per second: 183, episode reward: 23.400, mean reward: 0.688 [0.606, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.516, 10.324], loss: 0.001769, mae: 0.044740, mean_q: 1.216185
 90740/100000: episode: 1709, duration: 0.225s, episode steps: 31, steps per second: 138, episode reward: 21.882, mean reward: 0.706 [0.637, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.962, 10.100], loss: 0.001575, mae: 0.042689, mean_q: 1.227040
 90769/100000: episode: 1710, duration: 0.172s, episode steps: 29, steps per second: 168, episode reward: 19.437, mean reward: 0.670 [0.556, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.127], loss: 0.001788, mae: 0.043978, mean_q: 1.226828
 90788/100000: episode: 1711, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 13.666, mean reward: 0.719 [0.641, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.239, 10.100], loss: 0.001850, mae: 0.045304, mean_q: 1.235364
 90797/100000: episode: 1712, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 7.163, mean reward: 0.796 [0.754, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.447], loss: 0.001722, mae: 0.044997, mean_q: 1.240808
 90828/100000: episode: 1713, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 23.972, mean reward: 0.773 [0.697, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.381, 10.100], loss: 0.001621, mae: 0.043785, mean_q: 1.229697
 90848/100000: episode: 1714, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 14.370, mean reward: 0.718 [0.640, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.074, 10.100], loss: 0.001724, mae: 0.045464, mean_q: 1.233724
 90864/100000: episode: 1715, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 13.420, mean reward: 0.839 [0.721, 0.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.787], loss: 0.001572, mae: 0.040779, mean_q: 1.223443
 90898/100000: episode: 1716, duration: 0.233s, episode steps: 34, steps per second: 146, episode reward: 21.098, mean reward: 0.621 [0.503, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.035, 10.184], loss: 0.001629, mae: 0.042373, mean_q: 1.242517
 90907/100000: episode: 1717, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 6.756, mean reward: 0.751 [0.714, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.328, 10.554], loss: 0.001889, mae: 0.040542, mean_q: 1.233595
 90927/100000: episode: 1718, duration: 0.129s, episode steps: 20, steps per second: 156, episode reward: 14.525, mean reward: 0.726 [0.673, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.558], loss: 0.001713, mae: 0.042814, mean_q: 1.239751
 90958/100000: episode: 1719, duration: 0.228s, episode steps: 31, steps per second: 136, episode reward: 20.311, mean reward: 0.655 [0.551, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.776, 10.100], loss: 0.001660, mae: 0.041740, mean_q: 1.227885
 90987/100000: episode: 1720, duration: 0.200s, episode steps: 29, steps per second: 145, episode reward: 21.360, mean reward: 0.737 [0.632, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.907, 10.100], loss: 0.001735, mae: 0.043606, mean_q: 1.245179
 91016/100000: episode: 1721, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 20.691, mean reward: 0.713 [0.616, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.900, 10.100], loss: 0.002372, mae: 0.051642, mean_q: 1.231835
 91032/100000: episode: 1722, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 12.284, mean reward: 0.768 [0.697, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.568, 10.551], loss: 0.001613, mae: 0.041535, mean_q: 1.241361
 91063/100000: episode: 1723, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 22.057, mean reward: 0.712 [0.653, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.454, 10.100], loss: 0.001504, mae: 0.041659, mean_q: 1.238088
 91097/100000: episode: 1724, duration: 0.195s, episode steps: 34, steps per second: 174, episode reward: 22.137, mean reward: 0.651 [0.581, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.589, 10.339], loss: 0.001747, mae: 0.043636, mean_q: 1.245387
 91126/100000: episode: 1725, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 20.243, mean reward: 0.698 [0.623, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.903, 10.100], loss: 0.001393, mae: 0.039232, mean_q: 1.240149
 91145/100000: episode: 1726, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 12.507, mean reward: 0.658 [0.593, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.247, 10.100], loss: 0.001605, mae: 0.042445, mean_q: 1.248899
 91174/100000: episode: 1727, duration: 0.170s, episode steps: 29, steps per second: 171, episode reward: 20.823, mean reward: 0.718 [0.624, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.730, 10.100], loss: 0.001508, mae: 0.041275, mean_q: 1.251764
 91190/100000: episode: 1728, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 12.012, mean reward: 0.751 [0.693, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.436], loss: 0.001686, mae: 0.045626, mean_q: 1.238382
 91206/100000: episode: 1729, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 12.040, mean reward: 0.752 [0.675, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.881, 10.519], loss: 0.001353, mae: 0.040583, mean_q: 1.245745
[Info] Complete ISplit Iteration
[Info] Levels: [1.4455886, 1.7743471]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

 91220/100000: episode: 1730, duration: 4.473s, episode steps: 14, steps per second: 3, episode reward: 10.183, mean reward: 0.727 [0.693, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.297, 10.100], loss: 0.001281, mae: 0.038957, mean_q: 1.245598
 91320/100000: episode: 1731, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 59.903, mean reward: 0.599 [0.504, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.119, 10.098], loss: 0.001631, mae: 0.042899, mean_q: 1.249719
 91420/100000: episode: 1732, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.112, mean reward: 0.571 [0.509, 0.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.785, 10.332], loss: 0.001540, mae: 0.042125, mean_q: 1.248782
 91520/100000: episode: 1733, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.850, mean reward: 0.588 [0.500, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.603, 10.098], loss: 0.001631, mae: 0.042976, mean_q: 1.252329
 91620/100000: episode: 1734, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.630, mean reward: 0.596 [0.514, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.603, 10.098], loss: 0.001598, mae: 0.043019, mean_q: 1.248756
 91720/100000: episode: 1735, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.523, mean reward: 0.575 [0.497, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.908, 10.158], loss: 0.001512, mae: 0.042056, mean_q: 1.248985
 91820/100000: episode: 1736, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.757, mean reward: 0.608 [0.507, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.728, 10.253], loss: 0.001475, mae: 0.040856, mean_q: 1.247602
 91920/100000: episode: 1737, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.393, mean reward: 0.574 [0.503, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.021, 10.189], loss: 0.001455, mae: 0.041301, mean_q: 1.249438
 92020/100000: episode: 1738, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 59.178, mean reward: 0.592 [0.499, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.964, 10.098], loss: 0.001518, mae: 0.041956, mean_q: 1.246723
 92120/100000: episode: 1739, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 58.597, mean reward: 0.586 [0.499, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.183, 10.277], loss: 0.001541, mae: 0.041387, mean_q: 1.243577
 92220/100000: episode: 1740, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.260, mean reward: 0.593 [0.509, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.759, 10.098], loss: 0.001736, mae: 0.044691, mean_q: 1.252811
 92320/100000: episode: 1741, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 60.679, mean reward: 0.607 [0.504, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.768, 10.434], loss: 0.001456, mae: 0.040867, mean_q: 1.244347
 92420/100000: episode: 1742, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 57.628, mean reward: 0.576 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.542, 10.098], loss: 0.001805, mae: 0.044959, mean_q: 1.244535
 92520/100000: episode: 1743, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.905, mean reward: 0.599 [0.521, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.449, 10.174], loss: 0.001499, mae: 0.041538, mean_q: 1.247946
 92620/100000: episode: 1744, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 59.063, mean reward: 0.591 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.164, 10.098], loss: 0.001553, mae: 0.041659, mean_q: 1.240479
 92720/100000: episode: 1745, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.992, mean reward: 0.590 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.636, 10.289], loss: 0.001637, mae: 0.042964, mean_q: 1.240216
 92820/100000: episode: 1746, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 57.767, mean reward: 0.578 [0.504, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.037, 10.122], loss: 0.001626, mae: 0.043313, mean_q: 1.233975
 92920/100000: episode: 1747, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 58.018, mean reward: 0.580 [0.510, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.995, 10.098], loss: 0.001544, mae: 0.042429, mean_q: 1.241071
 93020/100000: episode: 1748, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 57.402, mean reward: 0.574 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.655, 10.151], loss: 0.001483, mae: 0.040383, mean_q: 1.242444
 93120/100000: episode: 1749, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 57.506, mean reward: 0.575 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.807, 10.098], loss: 0.001574, mae: 0.042099, mean_q: 1.244876
 93220/100000: episode: 1750, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 61.791, mean reward: 0.618 [0.501, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.176, 10.162], loss: 0.001554, mae: 0.042334, mean_q: 1.248534
 93320/100000: episode: 1751, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 60.084, mean reward: 0.601 [0.507, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.349, 10.141], loss: 0.001631, mae: 0.042813, mean_q: 1.241541
 93420/100000: episode: 1752, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 62.242, mean reward: 0.622 [0.506, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.173, 10.098], loss: 0.001594, mae: 0.042969, mean_q: 1.238640
 93520/100000: episode: 1753, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.500, mean reward: 0.595 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.866, 10.212], loss: 0.001539, mae: 0.042593, mean_q: 1.246161
 93620/100000: episode: 1754, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.120, mean reward: 0.581 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.904, 10.098], loss: 0.001731, mae: 0.044375, mean_q: 1.246049
 93720/100000: episode: 1755, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.250, mean reward: 0.582 [0.501, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.192, 10.155], loss: 0.001609, mae: 0.042830, mean_q: 1.247093
 93820/100000: episode: 1756, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 62.436, mean reward: 0.624 [0.511, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.858, 10.279], loss: 0.001516, mae: 0.041932, mean_q: 1.238642
 93920/100000: episode: 1757, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.093, mean reward: 0.571 [0.501, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.644, 10.221], loss: 0.001685, mae: 0.043224, mean_q: 1.241975
 94020/100000: episode: 1758, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 63.021, mean reward: 0.630 [0.523, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.326, 10.098], loss: 0.001554, mae: 0.042010, mean_q: 1.237726
 94120/100000: episode: 1759, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 58.347, mean reward: 0.583 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.768, 10.172], loss: 0.001517, mae: 0.041640, mean_q: 1.239093
 94220/100000: episode: 1760, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 60.242, mean reward: 0.602 [0.518, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.289, 10.240], loss: 0.001541, mae: 0.042289, mean_q: 1.239348
 94320/100000: episode: 1761, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 63.589, mean reward: 0.636 [0.510, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.824, 10.098], loss: 0.001450, mae: 0.041156, mean_q: 1.233986
 94420/100000: episode: 1762, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.293, mean reward: 0.583 [0.511, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.381, 10.098], loss: 0.001578, mae: 0.042600, mean_q: 1.231806
 94520/100000: episode: 1763, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.453, mean reward: 0.595 [0.512, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.662, 10.098], loss: 0.001613, mae: 0.042531, mean_q: 1.229771
 94620/100000: episode: 1764, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.395, mean reward: 0.574 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.995, 10.098], loss: 0.001614, mae: 0.042900, mean_q: 1.220647
 94720/100000: episode: 1765, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 57.562, mean reward: 0.576 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.803, 10.236], loss: 0.001528, mae: 0.041397, mean_q: 1.223894
 94820/100000: episode: 1766, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 58.307, mean reward: 0.583 [0.510, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.089, 10.098], loss: 0.001462, mae: 0.041355, mean_q: 1.218969
 94920/100000: episode: 1767, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 57.299, mean reward: 0.573 [0.507, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.298, 10.098], loss: 0.001644, mae: 0.042926, mean_q: 1.218324
 95020/100000: episode: 1768, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.172, mean reward: 0.582 [0.509, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.866, 10.364], loss: 0.001638, mae: 0.042706, mean_q: 1.208770
 95120/100000: episode: 1769, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 58.336, mean reward: 0.583 [0.504, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.913, 10.098], loss: 0.001502, mae: 0.041901, mean_q: 1.210845
 95220/100000: episode: 1770, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.978, mean reward: 0.590 [0.506, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.538, 10.170], loss: 0.001512, mae: 0.041602, mean_q: 1.208967
 95320/100000: episode: 1771, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 60.170, mean reward: 0.602 [0.499, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.234, 10.098], loss: 0.001695, mae: 0.043245, mean_q: 1.202411
 95420/100000: episode: 1772, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 57.737, mean reward: 0.577 [0.504, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.739, 10.130], loss: 0.001566, mae: 0.042231, mean_q: 1.200191
 95520/100000: episode: 1773, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 59.896, mean reward: 0.599 [0.503, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.019, 10.323], loss: 0.001713, mae: 0.043441, mean_q: 1.193609
 95620/100000: episode: 1774, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.458, mean reward: 0.595 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.879, 10.098], loss: 0.001674, mae: 0.042660, mean_q: 1.191634
 95720/100000: episode: 1775, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 59.417, mean reward: 0.594 [0.512, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.710, 10.098], loss: 0.001554, mae: 0.042602, mean_q: 1.188492
 95820/100000: episode: 1776, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.809, mean reward: 0.588 [0.502, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.267, 10.098], loss: 0.001561, mae: 0.042549, mean_q: 1.185605
 95920/100000: episode: 1777, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.007, mean reward: 0.570 [0.506, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.493, 10.104], loss: 0.001375, mae: 0.040075, mean_q: 1.181337
 96020/100000: episode: 1778, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 58.566, mean reward: 0.586 [0.502, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.810, 10.107], loss: 0.001322, mae: 0.039442, mean_q: 1.173478
 96120/100000: episode: 1779, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.129, mean reward: 0.571 [0.506, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.994, 10.119], loss: 0.001394, mae: 0.040765, mean_q: 1.168974
 96220/100000: episode: 1780, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 58.489, mean reward: 0.585 [0.511, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.347, 10.337], loss: 0.001327, mae: 0.039474, mean_q: 1.168550
 96320/100000: episode: 1781, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.579, mean reward: 0.586 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.710, 10.228], loss: 0.001288, mae: 0.039184, mean_q: 1.167618
 96420/100000: episode: 1782, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 59.678, mean reward: 0.597 [0.503, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.580, 10.098], loss: 0.001273, mae: 0.038889, mean_q: 1.166784
 96520/100000: episode: 1783, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 60.248, mean reward: 0.602 [0.511, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.468, 10.434], loss: 0.001361, mae: 0.040324, mean_q: 1.168818
 96620/100000: episode: 1784, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 57.601, mean reward: 0.576 [0.502, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.485, 10.098], loss: 0.001341, mae: 0.040194, mean_q: 1.164468
 96720/100000: episode: 1785, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 59.407, mean reward: 0.594 [0.507, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.369, 10.098], loss: 0.001277, mae: 0.038993, mean_q: 1.166642
 96820/100000: episode: 1786, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 58.549, mean reward: 0.585 [0.501, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.845, 10.147], loss: 0.001369, mae: 0.040500, mean_q: 1.166960
 96920/100000: episode: 1787, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.300, mean reward: 0.593 [0.510, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.388, 10.098], loss: 0.001336, mae: 0.039442, mean_q: 1.170869
 97020/100000: episode: 1788, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.344, mean reward: 0.603 [0.511, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.329, 10.235], loss: 0.001349, mae: 0.040289, mean_q: 1.168821
 97120/100000: episode: 1789, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 56.987, mean reward: 0.570 [0.506, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.986, 10.098], loss: 0.001320, mae: 0.039692, mean_q: 1.165397
 97220/100000: episode: 1790, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 57.414, mean reward: 0.574 [0.506, 0.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.627, 10.225], loss: 0.001328, mae: 0.039925, mean_q: 1.168227
 97320/100000: episode: 1791, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 60.824, mean reward: 0.608 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.961, 10.206], loss: 0.001402, mae: 0.040551, mean_q: 1.168867
 97420/100000: episode: 1792, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 56.963, mean reward: 0.570 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.272, 10.194], loss: 0.001426, mae: 0.041404, mean_q: 1.168968
 97520/100000: episode: 1793, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 59.262, mean reward: 0.593 [0.507, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.394, 10.273], loss: 0.001365, mae: 0.040457, mean_q: 1.164711
 97620/100000: episode: 1794, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.786, mean reward: 0.588 [0.501, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.426, 10.098], loss: 0.001331, mae: 0.039873, mean_q: 1.167062
 97720/100000: episode: 1795, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.765, mean reward: 0.588 [0.500, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.688, 10.186], loss: 0.001254, mae: 0.039010, mean_q: 1.168145
 97820/100000: episode: 1796, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 60.282, mean reward: 0.603 [0.501, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.812, 10.098], loss: 0.001561, mae: 0.043294, mean_q: 1.170566
 97920/100000: episode: 1797, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.949, mean reward: 0.579 [0.508, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.936, 10.189], loss: 0.001278, mae: 0.038989, mean_q: 1.168324
 98020/100000: episode: 1798, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.044, mean reward: 0.590 [0.503, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.873, 10.148], loss: 0.001234, mae: 0.038805, mean_q: 1.166896
 98120/100000: episode: 1799, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.046, mean reward: 0.590 [0.511, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.570, 10.098], loss: 0.001468, mae: 0.041531, mean_q: 1.167983
 98220/100000: episode: 1800, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 57.856, mean reward: 0.579 [0.503, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.592, 10.098], loss: 0.001469, mae: 0.041877, mean_q: 1.168579
 98320/100000: episode: 1801, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 59.643, mean reward: 0.596 [0.503, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.731, 10.098], loss: 0.001370, mae: 0.040561, mean_q: 1.164921
 98420/100000: episode: 1802, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 57.597, mean reward: 0.576 [0.499, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.639, 10.153], loss: 0.001324, mae: 0.039630, mean_q: 1.163711
 98520/100000: episode: 1803, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 58.845, mean reward: 0.588 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.547, 10.218], loss: 0.001448, mae: 0.041869, mean_q: 1.168076
 98620/100000: episode: 1804, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.577, mean reward: 0.606 [0.510, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.362, 10.098], loss: 0.001438, mae: 0.041024, mean_q: 1.167599
 98720/100000: episode: 1805, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 60.041, mean reward: 0.600 [0.515, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.714, 10.098], loss: 0.001406, mae: 0.041004, mean_q: 1.165602
 98820/100000: episode: 1806, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.297, mean reward: 0.573 [0.502, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.930, 10.157], loss: 0.001360, mae: 0.040393, mean_q: 1.166260
 98920/100000: episode: 1807, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 63.367, mean reward: 0.634 [0.512, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.680, 10.098], loss: 0.001349, mae: 0.040029, mean_q: 1.163075
 99020/100000: episode: 1808, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.200, mean reward: 0.582 [0.501, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.951, 10.098], loss: 0.001385, mae: 0.040246, mean_q: 1.165636
 99120/100000: episode: 1809, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 61.380, mean reward: 0.614 [0.504, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.000, 10.216], loss: 0.001512, mae: 0.042115, mean_q: 1.165182
 99220/100000: episode: 1810, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 57.528, mean reward: 0.575 [0.503, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.220, 10.248], loss: 0.001405, mae: 0.040496, mean_q: 1.165709
 99320/100000: episode: 1811, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 58.065, mean reward: 0.581 [0.516, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.187, 10.280], loss: 0.001407, mae: 0.040865, mean_q: 1.163616
 99420/100000: episode: 1812, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 58.354, mean reward: 0.584 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.897, 10.098], loss: 0.001363, mae: 0.040359, mean_q: 1.164630
 99520/100000: episode: 1813, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.543, mean reward: 0.565 [0.505, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.332, 10.182], loss: 0.001427, mae: 0.041129, mean_q: 1.164424
 99620/100000: episode: 1814, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.155, mean reward: 0.572 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.191, 10.098], loss: 0.001362, mae: 0.039736, mean_q: 1.160075
 99720/100000: episode: 1815, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 57.355, mean reward: 0.574 [0.504, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.211, 10.180], loss: 0.001364, mae: 0.040227, mean_q: 1.161514
 99820/100000: episode: 1816, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.698, mean reward: 0.577 [0.504, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.844, 10.098], loss: 0.001536, mae: 0.042390, mean_q: 1.161489
 99920/100000: episode: 1817, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 58.292, mean reward: 0.583 [0.506, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.325, 10.098], loss: 0.001399, mae: 0.040543, mean_q: 1.161306
done, took 654.714 seconds
[Info] End Importance Splitting. Falsification occurred 11 times.
