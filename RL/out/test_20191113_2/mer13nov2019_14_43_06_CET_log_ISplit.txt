Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.162s, episode steps: 100, steps per second: 617, episode reward: 57.911, mean reward: 0.579 [0.514, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.138, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.065s, episode steps: 100, steps per second: 1537, episode reward: 57.949, mean reward: 0.579 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.081, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.074s, episode steps: 100, steps per second: 1357, episode reward: 59.085, mean reward: 0.591 [0.500, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.516, 10.273], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.073s, episode steps: 100, steps per second: 1377, episode reward: 60.236, mean reward: 0.602 [0.506, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.232, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.065s, episode steps: 100, steps per second: 1547, episode reward: 59.104, mean reward: 0.591 [0.510, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.548, 10.279], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.067s, episode steps: 100, steps per second: 1494, episode reward: 57.157, mean reward: 0.572 [0.509, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.717, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 57.205, mean reward: 0.572 [0.504, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.625, 10.117], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.065s, episode steps: 100, steps per second: 1538, episode reward: 60.771, mean reward: 0.608 [0.508, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.987, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 57.785, mean reward: 0.578 [0.500, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.890, 10.151], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.071s, episode steps: 100, steps per second: 1402, episode reward: 56.945, mean reward: 0.569 [0.504, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.221, 10.138], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.069s, episode steps: 100, steps per second: 1447, episode reward: 56.399, mean reward: 0.564 [0.505, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.883, 10.165], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.065s, episode steps: 100, steps per second: 1540, episode reward: 57.120, mean reward: 0.571 [0.505, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.885, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.069s, episode steps: 100, steps per second: 1447, episode reward: 57.493, mean reward: 0.575 [0.504, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.993, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.065s, episode steps: 100, steps per second: 1543, episode reward: 58.644, mean reward: 0.586 [0.512, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.972, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 57.604, mean reward: 0.576 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.418, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 56.210, mean reward: 0.562 [0.503, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.858, 10.129], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.064s, episode steps: 100, steps per second: 1551, episode reward: 57.703, mean reward: 0.577 [0.505, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.204, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.065s, episode steps: 100, steps per second: 1529, episode reward: 56.675, mean reward: 0.567 [0.498, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.963, 10.180], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.065s, episode steps: 100, steps per second: 1541, episode reward: 57.211, mean reward: 0.572 [0.502, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.678, 10.342], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.070s, episode steps: 100, steps per second: 1423, episode reward: 58.709, mean reward: 0.587 [0.509, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.159, 10.160], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.077s, episode steps: 100, steps per second: 1307, episode reward: 59.022, mean reward: 0.590 [0.502, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.374, 10.350], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.071s, episode steps: 100, steps per second: 1401, episode reward: 57.489, mean reward: 0.575 [0.500, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.185, 10.147], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.074s, episode steps: 100, steps per second: 1346, episode reward: 58.824, mean reward: 0.588 [0.508, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.791, 10.268], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 62.039, mean reward: 0.620 [0.516, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.663, 10.316], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.066s, episode steps: 100, steps per second: 1525, episode reward: 57.993, mean reward: 0.580 [0.503, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.625, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.076s, episode steps: 100, steps per second: 1311, episode reward: 59.125, mean reward: 0.591 [0.516, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.392, 10.361], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.075s, episode steps: 100, steps per second: 1341, episode reward: 60.452, mean reward: 0.605 [0.507, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.873, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.067s, episode steps: 100, steps per second: 1481, episode reward: 57.854, mean reward: 0.579 [0.501, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.381, 10.168], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.075s, episode steps: 100, steps per second: 1329, episode reward: 60.790, mean reward: 0.608 [0.509, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.422, 10.098], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.064s, episode steps: 100, steps per second: 1554, episode reward: 61.927, mean reward: 0.619 [0.529, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.070, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.068s, episode steps: 100, steps per second: 1473, episode reward: 58.162, mean reward: 0.582 [0.512, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.892, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.079s, episode steps: 100, steps per second: 1258, episode reward: 59.415, mean reward: 0.594 [0.502, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.261, 10.361], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.070s, episode steps: 100, steps per second: 1429, episode reward: 58.851, mean reward: 0.589 [0.501, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.844, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.072s, episode steps: 100, steps per second: 1396, episode reward: 57.624, mean reward: 0.576 [0.498, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.821, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.065s, episode steps: 100, steps per second: 1541, episode reward: 57.327, mean reward: 0.573 [0.500, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.817, 10.139], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.064s, episode steps: 100, steps per second: 1551, episode reward: 57.350, mean reward: 0.574 [0.500, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.943, 10.124], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.073s, episode steps: 100, steps per second: 1377, episode reward: 58.750, mean reward: 0.587 [0.505, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.835, 10.195], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 57.386, mean reward: 0.574 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.050, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.065s, episode steps: 100, steps per second: 1530, episode reward: 59.529, mean reward: 0.595 [0.505, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.974, 10.337], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.065s, episode steps: 100, steps per second: 1541, episode reward: 62.225, mean reward: 0.622 [0.510, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.693, 10.164], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.078s, episode steps: 100, steps per second: 1285, episode reward: 62.050, mean reward: 0.621 [0.516, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.869, 10.487], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.065s, episode steps: 100, steps per second: 1527, episode reward: 56.498, mean reward: 0.565 [0.505, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.100, 10.201], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.064s, episode steps: 100, steps per second: 1555, episode reward: 61.401, mean reward: 0.614 [0.512, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.818, 10.259], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.064s, episode steps: 100, steps per second: 1561, episode reward: 60.211, mean reward: 0.602 [0.506, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.015, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 58.381, mean reward: 0.584 [0.506, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.510, 10.143], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.070s, episode steps: 100, steps per second: 1428, episode reward: 58.743, mean reward: 0.587 [0.499, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.753, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 62.450, mean reward: 0.624 [0.498, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.080, 10.100], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.065s, episode steps: 100, steps per second: 1545, episode reward: 58.320, mean reward: 0.583 [0.512, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.723, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.065s, episode steps: 100, steps per second: 1541, episode reward: 60.315, mean reward: 0.603 [0.499, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.966, 10.197], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.082s, episode steps: 100, steps per second: 1213, episode reward: 60.011, mean reward: 0.600 [0.508, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.258, 10.174], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.332s, episode steps: 100, steps per second: 75, episode reward: 58.684, mean reward: 0.587 [0.510, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.520, 10.150], loss: 0.012825, mae: 0.102820, mean_q: 0.863578
  5200/100000: episode: 52, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 60.440, mean reward: 0.604 [0.513, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.346, 10.291], loss: 0.004047, mae: 0.055545, mean_q: 1.007197
  5300/100000: episode: 53, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 61.195, mean reward: 0.612 [0.521, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.636, 10.098], loss: 0.003358, mae: 0.052015, mean_q: 1.068747
  5400/100000: episode: 54, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.922, mean reward: 0.599 [0.506, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.043, 10.213], loss: 0.003662, mae: 0.056214, mean_q: 1.105015
  5500/100000: episode: 55, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 58.487, mean reward: 0.585 [0.504, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.225, 10.098], loss: 0.003257, mae: 0.052248, mean_q: 1.127309
  5600/100000: episode: 56, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 63.124, mean reward: 0.631 [0.498, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.906, 10.198], loss: 0.003159, mae: 0.053479, mean_q: 1.143853
  5700/100000: episode: 57, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 57.379, mean reward: 0.574 [0.502, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.524, 10.121], loss: 0.002842, mae: 0.052203, mean_q: 1.156528
  5800/100000: episode: 58, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 58.110, mean reward: 0.581 [0.504, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.741, 10.098], loss: 0.003019, mae: 0.051475, mean_q: 1.159924
  5900/100000: episode: 59, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.010, mean reward: 0.590 [0.516, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.915, 10.210], loss: 0.002980, mae: 0.052427, mean_q: 1.162099
  6000/100000: episode: 60, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 60.241, mean reward: 0.602 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.800, 10.161], loss: 0.003294, mae: 0.055017, mean_q: 1.163068
  6100/100000: episode: 61, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.024, mean reward: 0.590 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.859, 10.098], loss: 0.002924, mae: 0.053370, mean_q: 1.165396
  6200/100000: episode: 62, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 56.500, mean reward: 0.565 [0.499, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.499, 10.124], loss: 0.002796, mae: 0.050459, mean_q: 1.167661
  6300/100000: episode: 63, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 57.670, mean reward: 0.577 [0.507, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.843, 10.204], loss: 0.002940, mae: 0.052485, mean_q: 1.168126
  6400/100000: episode: 64, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 58.635, mean reward: 0.586 [0.500, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.379, 10.098], loss: 0.002876, mae: 0.052064, mean_q: 1.169478
  6500/100000: episode: 65, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 61.685, mean reward: 0.617 [0.509, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.060, 10.098], loss: 0.003117, mae: 0.054980, mean_q: 1.168981
  6600/100000: episode: 66, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 60.730, mean reward: 0.607 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.036, 10.098], loss: 0.002875, mae: 0.052285, mean_q: 1.170791
  6700/100000: episode: 67, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 58.362, mean reward: 0.584 [0.503, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.604, 10.098], loss: 0.002591, mae: 0.051671, mean_q: 1.173478
  6800/100000: episode: 68, duration: 0.590s, episode steps: 100, steps per second: 169, episode reward: 57.007, mean reward: 0.570 [0.503, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.724, 10.098], loss: 0.002928, mae: 0.052889, mean_q: 1.172940
  6900/100000: episode: 69, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 58.334, mean reward: 0.583 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.324, 10.206], loss: 0.003337, mae: 0.055721, mean_q: 1.171213
  7000/100000: episode: 70, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 58.458, mean reward: 0.585 [0.499, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.017, 10.098], loss: 0.002889, mae: 0.052854, mean_q: 1.173648
  7100/100000: episode: 71, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.763, mean reward: 0.578 [0.499, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.714, 10.098], loss: 0.003410, mae: 0.056595, mean_q: 1.174607
  7200/100000: episode: 72, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 57.941, mean reward: 0.579 [0.511, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.524, 10.098], loss: 0.002908, mae: 0.053295, mean_q: 1.173533
  7300/100000: episode: 73, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 58.970, mean reward: 0.590 [0.498, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.631, 10.098], loss: 0.003051, mae: 0.054300, mean_q: 1.171654
  7400/100000: episode: 74, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 59.000, mean reward: 0.590 [0.518, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.447, 10.098], loss: 0.002957, mae: 0.053473, mean_q: 1.175001
  7500/100000: episode: 75, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 62.716, mean reward: 0.627 [0.501, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.399, 10.306], loss: 0.003182, mae: 0.054217, mean_q: 1.173600
  7600/100000: episode: 76, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 62.365, mean reward: 0.624 [0.507, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.454, 10.098], loss: 0.003179, mae: 0.055717, mean_q: 1.174507
  7700/100000: episode: 77, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: 58.393, mean reward: 0.584 [0.500, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.619, 10.207], loss: 0.003024, mae: 0.054207, mean_q: 1.174017
  7800/100000: episode: 78, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.742, mean reward: 0.587 [0.507, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.544, 10.130], loss: 0.002895, mae: 0.053498, mean_q: 1.172364
  7900/100000: episode: 79, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 57.076, mean reward: 0.571 [0.500, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.855, 10.098], loss: 0.003075, mae: 0.055387, mean_q: 1.175344
  8000/100000: episode: 80, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 58.701, mean reward: 0.587 [0.510, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.819, 10.098], loss: 0.003021, mae: 0.053657, mean_q: 1.170356
  8100/100000: episode: 81, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 57.297, mean reward: 0.573 [0.506, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.975, 10.155], loss: 0.003020, mae: 0.054569, mean_q: 1.171815
  8200/100000: episode: 82, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 58.715, mean reward: 0.587 [0.500, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.777, 10.098], loss: 0.003048, mae: 0.053806, mean_q: 1.170410
  8300/100000: episode: 83, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 57.982, mean reward: 0.580 [0.499, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.366, 10.098], loss: 0.002869, mae: 0.054101, mean_q: 1.171000
  8400/100000: episode: 84, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 61.385, mean reward: 0.614 [0.502, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.933, 10.098], loss: 0.003204, mae: 0.055358, mean_q: 1.172274
  8500/100000: episode: 85, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 57.868, mean reward: 0.579 [0.501, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.856, 10.326], loss: 0.002960, mae: 0.055367, mean_q: 1.174332
  8600/100000: episode: 86, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 59.830, mean reward: 0.598 [0.512, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.712, 10.232], loss: 0.003211, mae: 0.055499, mean_q: 1.172922
  8700/100000: episode: 87, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 57.215, mean reward: 0.572 [0.500, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.551, 10.098], loss: 0.002586, mae: 0.052701, mean_q: 1.175204
  8800/100000: episode: 88, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 60.138, mean reward: 0.601 [0.507, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.465, 10.098], loss: 0.003400, mae: 0.056271, mean_q: 1.169923
  8900/100000: episode: 89, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.927, mean reward: 0.589 [0.501, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.641, 10.098], loss: 0.003095, mae: 0.055181, mean_q: 1.169929
  9000/100000: episode: 90, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 59.104, mean reward: 0.591 [0.506, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.185, 10.287], loss: 0.003123, mae: 0.055232, mean_q: 1.169267
  9100/100000: episode: 91, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 59.303, mean reward: 0.593 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.217, 10.117], loss: 0.002749, mae: 0.052651, mean_q: 1.172028
  9200/100000: episode: 92, duration: 0.595s, episode steps: 100, steps per second: 168, episode reward: 58.099, mean reward: 0.581 [0.503, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.950, 10.169], loss: 0.003071, mae: 0.054342, mean_q: 1.172015
  9300/100000: episode: 93, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 56.890, mean reward: 0.569 [0.502, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.572, 10.157], loss: 0.002820, mae: 0.051838, mean_q: 1.169428
  9400/100000: episode: 94, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 62.026, mean reward: 0.620 [0.503, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.193, 10.416], loss: 0.003034, mae: 0.054331, mean_q: 1.167484
  9500/100000: episode: 95, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 57.813, mean reward: 0.578 [0.509, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.022, 10.105], loss: 0.002838, mae: 0.051765, mean_q: 1.169190
  9600/100000: episode: 96, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 57.840, mean reward: 0.578 [0.510, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.280, 10.255], loss: 0.002991, mae: 0.054160, mean_q: 1.171191
  9700/100000: episode: 97, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 57.331, mean reward: 0.573 [0.501, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.591, 10.098], loss: 0.002673, mae: 0.051390, mean_q: 1.170680
  9800/100000: episode: 98, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.566, mean reward: 0.586 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.812, 10.098], loss: 0.002763, mae: 0.051676, mean_q: 1.171859
  9900/100000: episode: 99, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.541, mean reward: 0.595 [0.507, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.530, 10.098], loss: 0.003085, mae: 0.052925, mean_q: 1.169550
 10000/100000: episode: 100, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 61.173, mean reward: 0.612 [0.506, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.538, 10.098], loss: 0.003158, mae: 0.054556, mean_q: 1.167491
 10100/100000: episode: 101, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 60.788, mean reward: 0.608 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.900, 10.098], loss: 0.002633, mae: 0.050915, mean_q: 1.167860
 10200/100000: episode: 102, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 56.994, mean reward: 0.570 [0.507, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.024, 10.157], loss: 0.002890, mae: 0.052257, mean_q: 1.166720
 10300/100000: episode: 103, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 58.812, mean reward: 0.588 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.074, 10.307], loss: 0.002822, mae: 0.052733, mean_q: 1.168946
 10400/100000: episode: 104, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 60.367, mean reward: 0.604 [0.510, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.384, 10.098], loss: 0.002752, mae: 0.051495, mean_q: 1.168101
 10500/100000: episode: 105, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 60.441, mean reward: 0.604 [0.502, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.348, 10.098], loss: 0.002698, mae: 0.050936, mean_q: 1.165732
 10600/100000: episode: 106, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 57.216, mean reward: 0.572 [0.506, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.880, 10.145], loss: 0.002708, mae: 0.050927, mean_q: 1.167429
 10700/100000: episode: 107, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 61.268, mean reward: 0.613 [0.504, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.988, 10.450], loss: 0.003150, mae: 0.054487, mean_q: 1.164693
 10800/100000: episode: 108, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: 62.359, mean reward: 0.624 [0.517, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.389, 10.098], loss: 0.002887, mae: 0.051765, mean_q: 1.165303
 10900/100000: episode: 109, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 59.285, mean reward: 0.593 [0.507, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.553, 10.098], loss: 0.003069, mae: 0.054819, mean_q: 1.168203
 11000/100000: episode: 110, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 58.110, mean reward: 0.581 [0.510, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.278, 10.098], loss: 0.002932, mae: 0.054001, mean_q: 1.168383
 11100/100000: episode: 111, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.764, mean reward: 0.578 [0.502, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.571, 10.163], loss: 0.002750, mae: 0.052146, mean_q: 1.170849
 11200/100000: episode: 112, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 60.699, mean reward: 0.607 [0.507, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.606, 10.416], loss: 0.002883, mae: 0.052606, mean_q: 1.166955
 11300/100000: episode: 113, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 57.890, mean reward: 0.579 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.520, 10.197], loss: 0.002906, mae: 0.053898, mean_q: 1.168009
 11400/100000: episode: 114, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 58.221, mean reward: 0.582 [0.503, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.077, 10.098], loss: 0.002882, mae: 0.052338, mean_q: 1.167835
 11500/100000: episode: 115, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 59.139, mean reward: 0.591 [0.511, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.416, 10.187], loss: 0.002668, mae: 0.050715, mean_q: 1.166814
 11600/100000: episode: 116, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 57.266, mean reward: 0.573 [0.499, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.533, 10.117], loss: 0.002728, mae: 0.051846, mean_q: 1.167047
 11700/100000: episode: 117, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 58.049, mean reward: 0.580 [0.502, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.518, 10.098], loss: 0.002402, mae: 0.049003, mean_q: 1.165098
 11800/100000: episode: 118, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 56.679, mean reward: 0.567 [0.500, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.927, 10.259], loss: 0.002798, mae: 0.051064, mean_q: 1.165391
 11900/100000: episode: 119, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 59.491, mean reward: 0.595 [0.500, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.599, 10.098], loss: 0.002640, mae: 0.050722, mean_q: 1.168614
 12000/100000: episode: 120, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 57.363, mean reward: 0.574 [0.501, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.581, 10.098], loss: 0.002618, mae: 0.050920, mean_q: 1.170314
 12100/100000: episode: 121, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 58.987, mean reward: 0.590 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.405, 10.098], loss: 0.002322, mae: 0.049241, mean_q: 1.171158
 12200/100000: episode: 122, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.219, mean reward: 0.572 [0.501, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.423, 10.133], loss: 0.002648, mae: 0.050664, mean_q: 1.171197
 12300/100000: episode: 123, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 60.026, mean reward: 0.600 [0.520, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.370, 10.233], loss: 0.003328, mae: 0.055731, mean_q: 1.169629
 12400/100000: episode: 124, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 59.602, mean reward: 0.596 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.270, 10.261], loss: 0.002655, mae: 0.050487, mean_q: 1.168897
 12500/100000: episode: 125, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.463, mean reward: 0.585 [0.502, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.596, 10.366], loss: 0.002361, mae: 0.048699, mean_q: 1.166223
 12600/100000: episode: 126, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 59.385, mean reward: 0.594 [0.532, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.743, 10.151], loss: 0.002512, mae: 0.049612, mean_q: 1.169021
 12700/100000: episode: 127, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 62.575, mean reward: 0.626 [0.521, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.764, 10.219], loss: 0.002749, mae: 0.050950, mean_q: 1.166454
 12800/100000: episode: 128, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 59.706, mean reward: 0.597 [0.512, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.956, 10.237], loss: 0.002929, mae: 0.052663, mean_q: 1.166033
 12900/100000: episode: 129, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 58.199, mean reward: 0.582 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.156, 10.098], loss: 0.002481, mae: 0.049327, mean_q: 1.167312
 13000/100000: episode: 130, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 59.355, mean reward: 0.594 [0.505, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.173, 10.098], loss: 0.002579, mae: 0.049075, mean_q: 1.166870
 13100/100000: episode: 131, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 57.854, mean reward: 0.579 [0.501, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.845, 10.201], loss: 0.002492, mae: 0.049232, mean_q: 1.167847
 13200/100000: episode: 132, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 62.053, mean reward: 0.621 [0.510, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.686, 10.492], loss: 0.002288, mae: 0.048231, mean_q: 1.168930
 13300/100000: episode: 133, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 61.877, mean reward: 0.619 [0.518, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.482, 10.098], loss: 0.002481, mae: 0.050177, mean_q: 1.172976
 13400/100000: episode: 134, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 59.137, mean reward: 0.591 [0.503, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.729, 10.098], loss: 0.002924, mae: 0.052023, mean_q: 1.171674
 13500/100000: episode: 135, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 58.881, mean reward: 0.589 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.772, 10.098], loss: 0.002530, mae: 0.049124, mean_q: 1.173750
 13600/100000: episode: 136, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 59.318, mean reward: 0.593 [0.512, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.315, 10.164], loss: 0.002771, mae: 0.050881, mean_q: 1.171113
 13700/100000: episode: 137, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 65.756, mean reward: 0.658 [0.520, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.355, 10.331], loss: 0.002996, mae: 0.053515, mean_q: 1.171964
 13800/100000: episode: 138, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 57.793, mean reward: 0.578 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.181, 10.098], loss: 0.002658, mae: 0.051573, mean_q: 1.175565
 13900/100000: episode: 139, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 58.985, mean reward: 0.590 [0.500, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.374, 10.098], loss: 0.002557, mae: 0.048772, mean_q: 1.172855
 14000/100000: episode: 140, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 58.987, mean reward: 0.590 [0.510, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.711, 10.098], loss: 0.002588, mae: 0.050438, mean_q: 1.173287
 14100/100000: episode: 141, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 58.160, mean reward: 0.582 [0.500, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.934, 10.209], loss: 0.002581, mae: 0.050734, mean_q: 1.174580
 14200/100000: episode: 142, duration: 0.590s, episode steps: 100, steps per second: 170, episode reward: 59.631, mean reward: 0.596 [0.507, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.246, 10.098], loss: 0.002811, mae: 0.052974, mean_q: 1.175384
 14300/100000: episode: 143, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward: 57.890, mean reward: 0.579 [0.514, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.231, 10.098], loss: 0.002596, mae: 0.050977, mean_q: 1.175464
 14400/100000: episode: 144, duration: 0.591s, episode steps: 100, steps per second: 169, episode reward: 57.481, mean reward: 0.575 [0.512, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.134, 10.143], loss: 0.002420, mae: 0.049676, mean_q: 1.175727
 14500/100000: episode: 145, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 58.172, mean reward: 0.582 [0.503, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.121, 10.098], loss: 0.002608, mae: 0.049781, mean_q: 1.173959
 14600/100000: episode: 146, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 57.910, mean reward: 0.579 [0.502, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.903, 10.358], loss: 0.002594, mae: 0.051148, mean_q: 1.175829
 14700/100000: episode: 147, duration: 0.597s, episode steps: 100, steps per second: 168, episode reward: 57.761, mean reward: 0.578 [0.508, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.975, 10.100], loss: 0.002964, mae: 0.052580, mean_q: 1.171943
 14800/100000: episode: 148, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 58.258, mean reward: 0.583 [0.514, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.014, 10.098], loss: 0.002177, mae: 0.048290, mean_q: 1.179049
 14900/100000: episode: 149, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 58.540, mean reward: 0.585 [0.499, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.747, 10.137], loss: 0.002236, mae: 0.048319, mean_q: 1.176331
[Info] 1-TH LEVEL FOUND: 1.278382658958435, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.187s, episode steps: 100, steps per second: 19, episode reward: 59.119, mean reward: 0.591 [0.507, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.716, 10.261], loss: 0.002923, mae: 0.052196, mean_q: 1.173502
 15026/100000: episode: 151, duration: 0.202s, episode steps: 26, steps per second: 129, episode reward: 17.222, mean reward: 0.662 [0.579, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.719, 10.100], loss: 0.001993, mae: 0.045904, mean_q: 1.176233
 15051/100000: episode: 152, duration: 0.177s, episode steps: 25, steps per second: 141, episode reward: 15.003, mean reward: 0.600 [0.522, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.718, 10.173], loss: 0.002861, mae: 0.051772, mean_q: 1.170773
 15076/100000: episode: 153, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 16.795, mean reward: 0.672 [0.622, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.194, 10.100], loss: 0.002217, mae: 0.047992, mean_q: 1.176771
 15100/100000: episode: 154, duration: 0.137s, episode steps: 24, steps per second: 176, episode reward: 14.265, mean reward: 0.594 [0.508, 0.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.470, 10.100], loss: 0.002298, mae: 0.049689, mean_q: 1.178265
 15125/100000: episode: 155, duration: 0.157s, episode steps: 25, steps per second: 159, episode reward: 16.423, mean reward: 0.657 [0.556, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.713, 10.100], loss: 0.002706, mae: 0.050344, mean_q: 1.172899
 15148/100000: episode: 156, duration: 0.162s, episode steps: 23, steps per second: 142, episode reward: 14.046, mean reward: 0.611 [0.572, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.139, 10.100], loss: 0.002680, mae: 0.052118, mean_q: 1.174179
 15174/100000: episode: 157, duration: 0.205s, episode steps: 26, steps per second: 127, episode reward: 16.895, mean reward: 0.650 [0.535, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.609, 10.100], loss: 0.002126, mae: 0.048686, mean_q: 1.175201
 15200/100000: episode: 158, duration: 0.151s, episode steps: 26, steps per second: 173, episode reward: 16.075, mean reward: 0.618 [0.530, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.122, 10.100], loss: 0.002895, mae: 0.051654, mean_q: 1.176619
 15223/100000: episode: 159, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 15.869, mean reward: 0.690 [0.608, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.330, 10.100], loss: 0.003123, mae: 0.051309, mean_q: 1.169261
 15248/100000: episode: 160, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 17.619, mean reward: 0.705 [0.654, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.395, 10.100], loss: 0.003339, mae: 0.052226, mean_q: 1.172347
 15273/100000: episode: 161, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 17.037, mean reward: 0.681 [0.634, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.798, 10.100], loss: 0.002983, mae: 0.054509, mean_q: 1.181408
 15299/100000: episode: 162, duration: 0.163s, episode steps: 26, steps per second: 160, episode reward: 17.278, mean reward: 0.665 [0.623, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.200, 10.100], loss: 0.003117, mae: 0.054333, mean_q: 1.169941
 15322/100000: episode: 163, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 15.944, mean reward: 0.693 [0.609, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.372, 10.100], loss: 0.002860, mae: 0.053729, mean_q: 1.173933
 15343/100000: episode: 164, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 13.832, mean reward: 0.659 [0.601, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.420, 10.100], loss: 0.003683, mae: 0.057824, mean_q: 1.165638
 15366/100000: episode: 165, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 17.344, mean reward: 0.754 [0.680, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.804, 10.100], loss: 0.003570, mae: 0.058537, mean_q: 1.181331
 15392/100000: episode: 166, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 18.035, mean reward: 0.694 [0.648, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.281, 10.100], loss: 0.003605, mae: 0.057825, mean_q: 1.171553
 15415/100000: episode: 167, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 15.433, mean reward: 0.671 [0.569, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.824, 10.100], loss: 0.002281, mae: 0.049592, mean_q: 1.174345
 15438/100000: episode: 168, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 13.883, mean reward: 0.604 [0.513, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.091, 10.100], loss: 0.004157, mae: 0.061030, mean_q: 1.171405
 15463/100000: episode: 169, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 16.226, mean reward: 0.649 [0.589, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.545, 10.100], loss: 0.003558, mae: 0.058896, mean_q: 1.175182
 15488/100000: episode: 170, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 16.725, mean reward: 0.669 [0.611, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-2.866, 10.100], loss: 0.003195, mae: 0.053597, mean_q: 1.181758
 15514/100000: episode: 171, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 17.275, mean reward: 0.664 [0.590, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.141, 10.100], loss: 0.002609, mae: 0.051922, mean_q: 1.185173
 15537/100000: episode: 172, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 14.167, mean reward: 0.616 [0.552, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.094, 10.100], loss: 0.003138, mae: 0.056268, mean_q: 1.175018
 15563/100000: episode: 173, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 18.002, mean reward: 0.692 [0.629, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.303, 10.100], loss: 0.003029, mae: 0.052900, mean_q: 1.182979
 15589/100000: episode: 174, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 19.001, mean reward: 0.731 [0.657, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.528, 10.100], loss: 0.002532, mae: 0.049720, mean_q: 1.188092
 15615/100000: episode: 175, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 18.105, mean reward: 0.696 [0.648, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.215, 10.100], loss: 0.003499, mae: 0.056434, mean_q: 1.179904
 15641/100000: episode: 176, duration: 0.158s, episode steps: 26, steps per second: 165, episode reward: 16.252, mean reward: 0.625 [0.542, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.202, 10.100], loss: 0.002457, mae: 0.049746, mean_q: 1.187675
 15662/100000: episode: 177, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 16.006, mean reward: 0.762 [0.687, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.423, 10.100], loss: 0.003756, mae: 0.057415, mean_q: 1.176193
 15687/100000: episode: 178, duration: 0.151s, episode steps: 25, steps per second: 165, episode reward: 15.185, mean reward: 0.607 [0.551, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.175, 10.100], loss: 0.002672, mae: 0.050396, mean_q: 1.191661
 15712/100000: episode: 179, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 17.058, mean reward: 0.682 [0.633, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.405, 10.100], loss: 0.003459, mae: 0.056692, mean_q: 1.181380
 15733/100000: episode: 180, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 15.711, mean reward: 0.748 [0.600, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.190, 10.100], loss: 0.003584, mae: 0.059116, mean_q: 1.180763
 15754/100000: episode: 181, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 14.207, mean reward: 0.677 [0.596, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.981, 10.100], loss: 0.004046, mae: 0.059965, mean_q: 1.181727
 15775/100000: episode: 182, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 14.925, mean reward: 0.711 [0.579, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.165, 10.100], loss: 0.002590, mae: 0.049902, mean_q: 1.185864
 15801/100000: episode: 183, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 17.671, mean reward: 0.680 [0.628, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.434, 10.100], loss: 0.003745, mae: 0.054125, mean_q: 1.175068
 15822/100000: episode: 184, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 15.378, mean reward: 0.732 [0.668, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.295, 10.100], loss: 0.003748, mae: 0.057698, mean_q: 1.181840
 15843/100000: episode: 185, duration: 0.124s, episode steps: 21, steps per second: 169, episode reward: 12.706, mean reward: 0.605 [0.575, 0.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.375, 10.100], loss: 0.003384, mae: 0.055589, mean_q: 1.189750
 15864/100000: episode: 186, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 14.409, mean reward: 0.686 [0.600, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.234, 10.100], loss: 0.002806, mae: 0.053414, mean_q: 1.184039
 15889/100000: episode: 187, duration: 0.148s, episode steps: 25, steps per second: 168, episode reward: 16.145, mean reward: 0.646 [0.566, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.508, 10.100], loss: 0.003863, mae: 0.057317, mean_q: 1.186306
 15910/100000: episode: 188, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 16.145, mean reward: 0.769 [0.698, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.435, 10.100], loss: 0.003666, mae: 0.055846, mean_q: 1.187201
 15931/100000: episode: 189, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 13.673, mean reward: 0.651 [0.568, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.091, 10.100], loss: 0.003319, mae: 0.054833, mean_q: 1.183598
 15957/100000: episode: 190, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 17.535, mean reward: 0.674 [0.602, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.171, 10.100], loss: 0.003599, mae: 0.056639, mean_q: 1.188165
 15978/100000: episode: 191, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 14.152, mean reward: 0.674 [0.569, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.188, 10.100], loss: 0.004312, mae: 0.060247, mean_q: 1.193176
 16004/100000: episode: 192, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 17.730, mean reward: 0.682 [0.614, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.514, 10.100], loss: 0.003275, mae: 0.054207, mean_q: 1.188084
 16029/100000: episode: 193, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 16.950, mean reward: 0.678 [0.563, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.455, 10.100], loss: 0.002815, mae: 0.050083, mean_q: 1.195454
 16055/100000: episode: 194, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 16.850, mean reward: 0.648 [0.528, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.317, 10.100], loss: 0.004075, mae: 0.058287, mean_q: 1.187356
 16078/100000: episode: 195, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 15.008, mean reward: 0.653 [0.563, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.877, 10.100], loss: 0.003449, mae: 0.058055, mean_q: 1.195804
 16104/100000: episode: 196, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 16.997, mean reward: 0.654 [0.597, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.097, 10.100], loss: 0.003106, mae: 0.053611, mean_q: 1.188464
 16125/100000: episode: 197, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 15.165, mean reward: 0.722 [0.637, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.244, 10.100], loss: 0.002579, mae: 0.054038, mean_q: 1.202014
 16149/100000: episode: 198, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 15.366, mean reward: 0.640 [0.594, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.228, 10.100], loss: 0.003547, mae: 0.055826, mean_q: 1.197214
 16172/100000: episode: 199, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 15.511, mean reward: 0.674 [0.601, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.307, 10.100], loss: 0.003366, mae: 0.056141, mean_q: 1.195627
 16198/100000: episode: 200, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 19.163, mean reward: 0.737 [0.665, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.398, 10.100], loss: 0.003940, mae: 0.059976, mean_q: 1.198000
 16224/100000: episode: 201, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 17.949, mean reward: 0.690 [0.620, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.739, 10.100], loss: 0.002565, mae: 0.052921, mean_q: 1.204419
 16247/100000: episode: 202, duration: 0.127s, episode steps: 23, steps per second: 180, episode reward: 14.334, mean reward: 0.623 [0.526, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.483, 10.193], loss: 0.004307, mae: 0.059053, mean_q: 1.190660
 16272/100000: episode: 203, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 15.904, mean reward: 0.636 [0.550, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.178, 10.100], loss: 0.003612, mae: 0.059470, mean_q: 1.199450
 16293/100000: episode: 204, duration: 0.120s, episode steps: 21, steps per second: 174, episode reward: 14.682, mean reward: 0.699 [0.606, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.704, 10.100], loss: 0.002865, mae: 0.054166, mean_q: 1.203393
 16314/100000: episode: 205, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 13.903, mean reward: 0.662 [0.611, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.215, 10.100], loss: 0.004270, mae: 0.061338, mean_q: 1.199748
 16339/100000: episode: 206, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 16.352, mean reward: 0.654 [0.605, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.246, 10.100], loss: 0.004006, mae: 0.061254, mean_q: 1.201850
 16364/100000: episode: 207, duration: 0.154s, episode steps: 25, steps per second: 162, episode reward: 17.726, mean reward: 0.709 [0.601, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.333, 10.100], loss: 0.003880, mae: 0.056997, mean_q: 1.192077
 16385/100000: episode: 208, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 13.614, mean reward: 0.648 [0.596, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.146, 10.100], loss: 0.003621, mae: 0.057354, mean_q: 1.201404
 16406/100000: episode: 209, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 12.984, mean reward: 0.618 [0.531, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.100], loss: 0.003357, mae: 0.051600, mean_q: 1.205009
 16429/100000: episode: 210, duration: 0.143s, episode steps: 23, steps per second: 160, episode reward: 14.552, mean reward: 0.633 [0.574, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.763, 10.100], loss: 0.003489, mae: 0.054368, mean_q: 1.200347
 16452/100000: episode: 211, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 13.514, mean reward: 0.588 [0.500, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.346, 10.191], loss: 0.002834, mae: 0.052388, mean_q: 1.202078
 16475/100000: episode: 212, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 14.392, mean reward: 0.626 [0.560, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.147, 10.100], loss: 0.003310, mae: 0.054974, mean_q: 1.202906
 16500/100000: episode: 213, duration: 0.133s, episode steps: 25, steps per second: 187, episode reward: 16.523, mean reward: 0.661 [0.607, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.206, 10.100], loss: 0.004107, mae: 0.059206, mean_q: 1.200535
 16525/100000: episode: 214, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 17.019, mean reward: 0.681 [0.582, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.717, 10.100], loss: 0.003435, mae: 0.058052, mean_q: 1.202092
 16550/100000: episode: 215, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 18.455, mean reward: 0.738 [0.650, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.530, 10.100], loss: 0.003634, mae: 0.061061, mean_q: 1.197407
 16575/100000: episode: 216, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 16.293, mean reward: 0.652 [0.574, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.291, 10.100], loss: 0.002391, mae: 0.048238, mean_q: 1.210469
 16596/100000: episode: 217, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 14.653, mean reward: 0.698 [0.647, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.242, 10.100], loss: 0.004053, mae: 0.062628, mean_q: 1.198148
 16617/100000: episode: 218, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 14.507, mean reward: 0.691 [0.587, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.700, 10.100], loss: 0.002836, mae: 0.052457, mean_q: 1.210732
 16643/100000: episode: 219, duration: 0.155s, episode steps: 26, steps per second: 167, episode reward: 17.093, mean reward: 0.657 [0.574, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.156, 10.100], loss: 0.002724, mae: 0.052190, mean_q: 1.204300
 16664/100000: episode: 220, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 15.213, mean reward: 0.724 [0.599, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.430, 10.100], loss: 0.003171, mae: 0.054334, mean_q: 1.206489
 16690/100000: episode: 221, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 17.718, mean reward: 0.681 [0.559, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.267, 10.100], loss: 0.002997, mae: 0.056166, mean_q: 1.212386
 16716/100000: episode: 222, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 16.356, mean reward: 0.629 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.426, 10.159], loss: 0.003782, mae: 0.060272, mean_q: 1.206974
 16737/100000: episode: 223, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 17.002, mean reward: 0.810 [0.747, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.388, 10.100], loss: 0.002776, mae: 0.052261, mean_q: 1.209685
 16758/100000: episode: 224, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 14.171, mean reward: 0.675 [0.588, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.776, 10.100], loss: 0.002952, mae: 0.055010, mean_q: 1.221505
 16784/100000: episode: 225, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 17.707, mean reward: 0.681 [0.602, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.747, 10.100], loss: 0.003663, mae: 0.057134, mean_q: 1.210831
 16810/100000: episode: 226, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 18.226, mean reward: 0.701 [0.649, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.202, 10.100], loss: 0.003867, mae: 0.062416, mean_q: 1.203721
 16835/100000: episode: 227, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 16.337, mean reward: 0.653 [0.559, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.322, 10.100], loss: 0.003077, mae: 0.054245, mean_q: 1.212408
 16860/100000: episode: 228, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 15.919, mean reward: 0.637 [0.547, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.559, 10.100], loss: 0.002857, mae: 0.053806, mean_q: 1.215338
 16883/100000: episode: 229, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 14.921, mean reward: 0.649 [0.577, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.202, 10.100], loss: 0.003862, mae: 0.059746, mean_q: 1.208068
 16904/100000: episode: 230, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 13.902, mean reward: 0.662 [0.604, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.136, 10.100], loss: 0.003709, mae: 0.060527, mean_q: 1.216832
 16927/100000: episode: 231, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 14.275, mean reward: 0.621 [0.549, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.269, 10.100], loss: 0.003224, mae: 0.056046, mean_q: 1.214952
 16952/100000: episode: 232, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 18.686, mean reward: 0.747 [0.607, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.453, 10.100], loss: 0.002643, mae: 0.051292, mean_q: 1.212794
 16976/100000: episode: 233, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 16.235, mean reward: 0.676 [0.606, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.694, 10.100], loss: 0.003224, mae: 0.056780, mean_q: 1.215822
 17001/100000: episode: 234, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 16.129, mean reward: 0.645 [0.594, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.348, 10.100], loss: 0.002945, mae: 0.053854, mean_q: 1.219168
 17024/100000: episode: 235, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 14.444, mean reward: 0.628 [0.568, 0.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.261, 10.100], loss: 0.002759, mae: 0.051873, mean_q: 1.216738
 17050/100000: episode: 236, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 16.064, mean reward: 0.618 [0.525, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.215, 10.100], loss: 0.002717, mae: 0.053061, mean_q: 1.221225
 17071/100000: episode: 237, duration: 0.120s, episode steps: 21, steps per second: 174, episode reward: 13.526, mean reward: 0.644 [0.548, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.372, 10.100], loss: 0.003128, mae: 0.057140, mean_q: 1.223067
 17097/100000: episode: 238, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 18.236, mean reward: 0.701 [0.636, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.239, 10.100], loss: 0.002915, mae: 0.053612, mean_q: 1.224028
 17118/100000: episode: 239, duration: 0.106s, episode steps: 21, steps per second: 199, episode reward: 15.256, mean reward: 0.726 [0.633, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.258, 10.100], loss: 0.003146, mae: 0.059044, mean_q: 1.213636
[Info] 2-TH LEVEL FOUND: 1.4105721712112427, Considering 10/90 traces
 17139/100000: episode: 240, duration: 4.341s, episode steps: 21, steps per second: 5, episode reward: 14.541, mean reward: 0.692 [0.612, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.810, 10.100], loss: 0.003022, mae: 0.056493, mean_q: 1.211261
 17158/100000: episode: 241, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 13.754, mean reward: 0.724 [0.625, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.311, 10.100], loss: 0.003587, mae: 0.056242, mean_q: 1.212173
 17177/100000: episode: 242, duration: 0.108s, episode steps: 19, steps per second: 177, episode reward: 13.235, mean reward: 0.697 [0.616, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.392, 10.100], loss: 0.002590, mae: 0.052636, mean_q: 1.222275
 17197/100000: episode: 243, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 13.406, mean reward: 0.670 [0.581, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.279, 10.100], loss: 0.002987, mae: 0.055090, mean_q: 1.220579
 17218/100000: episode: 244, duration: 0.125s, episode steps: 21, steps per second: 168, episode reward: 15.539, mean reward: 0.740 [0.693, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.375, 10.100], loss: 0.002376, mae: 0.050268, mean_q: 1.232375
 17238/100000: episode: 245, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 14.284, mean reward: 0.714 [0.667, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.207, 10.100], loss: 0.003762, mae: 0.064857, mean_q: 1.221208
 17255/100000: episode: 246, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 12.083, mean reward: 0.711 [0.666, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.210, 10.100], loss: 0.004059, mae: 0.064015, mean_q: 1.210912
 17273/100000: episode: 247, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 13.757, mean reward: 0.764 [0.719, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.291, 10.100], loss: 0.002687, mae: 0.053340, mean_q: 1.229655
 17290/100000: episode: 248, duration: 0.110s, episode steps: 17, steps per second: 155, episode reward: 11.894, mean reward: 0.700 [0.637, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.643, 10.100], loss: 0.004225, mae: 0.064343, mean_q: 1.223137
 17307/100000: episode: 249, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 12.307, mean reward: 0.724 [0.644, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.217, 10.100], loss: 0.003267, mae: 0.057847, mean_q: 1.231964
 17325/100000: episode: 250, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 12.247, mean reward: 0.680 [0.641, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.223, 10.100], loss: 0.003407, mae: 0.061156, mean_q: 1.223900
 17345/100000: episode: 251, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 14.744, mean reward: 0.737 [0.638, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.219, 10.100], loss: 0.002785, mae: 0.053571, mean_q: 1.228370
 17362/100000: episode: 252, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 12.680, mean reward: 0.746 [0.682, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.445, 10.100], loss: 0.002554, mae: 0.051021, mean_q: 1.234475
 17380/100000: episode: 253, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 13.409, mean reward: 0.745 [0.674, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.569, 10.100], loss: 0.002741, mae: 0.052618, mean_q: 1.223208
 17399/100000: episode: 254, duration: 0.102s, episode steps: 19, steps per second: 185, episode reward: 13.559, mean reward: 0.714 [0.634, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.664, 10.100], loss: 0.002712, mae: 0.054184, mean_q: 1.241563
 17419/100000: episode: 255, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 15.203, mean reward: 0.760 [0.695, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.689, 10.100], loss: 0.003013, mae: 0.052812, mean_q: 1.229388
 17436/100000: episode: 256, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 12.535, mean reward: 0.737 [0.657, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.332, 10.100], loss: 0.003133, mae: 0.056656, mean_q: 1.232374
 17454/100000: episode: 257, duration: 0.099s, episode steps: 18, steps per second: 183, episode reward: 13.243, mean reward: 0.736 [0.695, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.265, 10.100], loss: 0.002536, mae: 0.049567, mean_q: 1.235107
 17473/100000: episode: 258, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 13.576, mean reward: 0.715 [0.662, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.339, 10.100], loss: 0.002502, mae: 0.050903, mean_q: 1.232349
 17490/100000: episode: 259, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 12.802, mean reward: 0.753 [0.623, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.670, 10.100], loss: 0.002515, mae: 0.052187, mean_q: 1.232582
 17509/100000: episode: 260, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 14.098, mean reward: 0.742 [0.694, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.369, 10.100], loss: 0.002593, mae: 0.050273, mean_q: 1.242835
 17526/100000: episode: 261, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 12.124, mean reward: 0.713 [0.650, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.362, 10.100], loss: 0.002851, mae: 0.056086, mean_q: 1.240269
 17546/100000: episode: 262, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 15.035, mean reward: 0.752 [0.647, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.285, 10.100], loss: 0.004015, mae: 0.062274, mean_q: 1.229488
 17565/100000: episode: 263, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 14.472, mean reward: 0.762 [0.688, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.358, 10.100], loss: 0.004005, mae: 0.062684, mean_q: 1.242435
 17586/100000: episode: 264, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 15.362, mean reward: 0.732 [0.705, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.315, 10.100], loss: 0.002907, mae: 0.055359, mean_q: 1.242132
 17603/100000: episode: 265, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 11.384, mean reward: 0.670 [0.636, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.334, 10.100], loss: 0.002305, mae: 0.050934, mean_q: 1.240822
 17623/100000: episode: 266, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 13.899, mean reward: 0.695 [0.646, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.399, 10.100], loss: 0.002858, mae: 0.054639, mean_q: 1.239545
 17640/100000: episode: 267, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 11.737, mean reward: 0.690 [0.642, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.303, 10.100], loss: 0.002575, mae: 0.050941, mean_q: 1.250084
 17659/100000: episode: 268, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 14.784, mean reward: 0.778 [0.704, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.917, 10.100], loss: 0.002725, mae: 0.052099, mean_q: 1.235248
 17680/100000: episode: 269, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 15.998, mean reward: 0.762 [0.691, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.780, 10.100], loss: 0.002890, mae: 0.054879, mean_q: 1.245642
 17699/100000: episode: 270, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 13.066, mean reward: 0.688 [0.622, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.993, 10.100], loss: 0.002611, mae: 0.054877, mean_q: 1.251860
 17719/100000: episode: 271, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 14.427, mean reward: 0.721 [0.657, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.321, 10.100], loss: 0.002524, mae: 0.050098, mean_q: 1.237177
 17737/100000: episode: 272, duration: 0.109s, episode steps: 18, steps per second: 165, episode reward: 13.150, mean reward: 0.731 [0.648, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.290, 10.100], loss: 0.002891, mae: 0.053977, mean_q: 1.247542
 17756/100000: episode: 273, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 15.320, mean reward: 0.806 [0.713, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.789, 10.100], loss: 0.002536, mae: 0.050392, mean_q: 1.251678
 17775/100000: episode: 274, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 13.732, mean reward: 0.723 [0.689, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.459, 10.100], loss: 0.002364, mae: 0.051602, mean_q: 1.249313
 17792/100000: episode: 275, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 12.912, mean reward: 0.760 [0.656, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.688, 10.100], loss: 0.002582, mae: 0.053944, mean_q: 1.253091
 17809/100000: episode: 276, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 11.986, mean reward: 0.705 [0.657, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.298, 10.100], loss: 0.002850, mae: 0.053810, mean_q: 1.250252
 17828/100000: episode: 277, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 14.912, mean reward: 0.785 [0.682, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.487, 10.100], loss: 0.002815, mae: 0.054778, mean_q: 1.254020
 17849/100000: episode: 278, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 16.436, mean reward: 0.783 [0.708, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.381, 10.100], loss: 0.002674, mae: 0.052669, mean_q: 1.236328
 17869/100000: episode: 279, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 15.223, mean reward: 0.761 [0.692, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.343, 10.100], loss: 0.003184, mae: 0.058546, mean_q: 1.234812
 17889/100000: episode: 280, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 14.101, mean reward: 0.705 [0.585, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.198, 10.100], loss: 0.003809, mae: 0.063035, mean_q: 1.248954
 17908/100000: episode: 281, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 13.170, mean reward: 0.693 [0.615, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.408, 10.100], loss: 0.003239, mae: 0.057730, mean_q: 1.253301
 17927/100000: episode: 282, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 13.472, mean reward: 0.709 [0.633, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.444, 10.100], loss: 0.002975, mae: 0.057818, mean_q: 1.259166
 17944/100000: episode: 283, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 11.841, mean reward: 0.697 [0.628, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.565, 10.100], loss: 0.003077, mae: 0.057250, mean_q: 1.249698
 17964/100000: episode: 284, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 14.615, mean reward: 0.731 [0.689, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.838, 10.100], loss: 0.002559, mae: 0.053612, mean_q: 1.259882
 17982/100000: episode: 285, duration: 0.106s, episode steps: 18, steps per second: 170, episode reward: 13.144, mean reward: 0.730 [0.660, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.925, 10.100], loss: 0.002826, mae: 0.054233, mean_q: 1.239799
 17999/100000: episode: 286, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 12.658, mean reward: 0.745 [0.668, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.583, 10.100], loss: 0.003408, mae: 0.063712, mean_q: 1.269082
 18016/100000: episode: 287, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 12.939, mean reward: 0.761 [0.670, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.454, 10.100], loss: 0.002490, mae: 0.051776, mean_q: 1.265662
 18033/100000: episode: 288, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 11.998, mean reward: 0.706 [0.614, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.477, 10.100], loss: 0.002392, mae: 0.051839, mean_q: 1.260130
 18050/100000: episode: 289, duration: 0.109s, episode steps: 17, steps per second: 155, episode reward: 12.170, mean reward: 0.716 [0.634, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.272, 10.100], loss: 0.002542, mae: 0.054382, mean_q: 1.254370
 18067/100000: episode: 290, duration: 0.100s, episode steps: 17, steps per second: 169, episode reward: 11.877, mean reward: 0.699 [0.655, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.497, 10.100], loss: 0.002541, mae: 0.053196, mean_q: 1.264659
 18087/100000: episode: 291, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 15.509, mean reward: 0.775 [0.652, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.982, 10.100], loss: 0.002547, mae: 0.051815, mean_q: 1.265859
 18104/100000: episode: 292, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 12.002, mean reward: 0.706 [0.631, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.339, 10.100], loss: 0.002444, mae: 0.051572, mean_q: 1.266972
 18124/100000: episode: 293, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 15.070, mean reward: 0.754 [0.668, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.441, 10.100], loss: 0.002511, mae: 0.050964, mean_q: 1.264698
 18144/100000: episode: 294, duration: 0.117s, episode steps: 20, steps per second: 172, episode reward: 15.189, mean reward: 0.759 [0.666, 0.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.441, 10.100], loss: 0.002255, mae: 0.049893, mean_q: 1.248117
 18163/100000: episode: 295, duration: 0.113s, episode steps: 19, steps per second: 169, episode reward: 14.854, mean reward: 0.782 [0.710, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.260, 10.100], loss: 0.002727, mae: 0.055057, mean_q: 1.262982
 18180/100000: episode: 296, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 11.754, mean reward: 0.691 [0.625, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.255, 10.100], loss: 0.003053, mae: 0.057255, mean_q: 1.259777
 18198/100000: episode: 297, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 13.941, mean reward: 0.774 [0.696, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.543, 10.100], loss: 0.002612, mae: 0.053747, mean_q: 1.276773
 18215/100000: episode: 298, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 11.882, mean reward: 0.699 [0.628, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.231, 10.100], loss: 0.002521, mae: 0.052434, mean_q: 1.251863
 18233/100000: episode: 299, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 12.513, mean reward: 0.695 [0.623, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.546, 10.100], loss: 0.002344, mae: 0.051059, mean_q: 1.276625
 18250/100000: episode: 300, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 13.232, mean reward: 0.778 [0.683, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.375, 10.100], loss: 0.002414, mae: 0.051354, mean_q: 1.269760
 18269/100000: episode: 301, duration: 0.117s, episode steps: 19, steps per second: 163, episode reward: 13.027, mean reward: 0.686 [0.638, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.238, 10.100], loss: 0.002639, mae: 0.053302, mean_q: 1.263988
 18286/100000: episode: 302, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 11.288, mean reward: 0.664 [0.590, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.160, 10.100], loss: 0.002082, mae: 0.049728, mean_q: 1.269319
 18303/100000: episode: 303, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 13.344, mean reward: 0.785 [0.630, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.720, 10.100], loss: 0.002844, mae: 0.056907, mean_q: 1.272089
 18322/100000: episode: 304, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 14.771, mean reward: 0.777 [0.686, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.444, 10.100], loss: 0.002778, mae: 0.053969, mean_q: 1.274812
 18341/100000: episode: 305, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 13.939, mean reward: 0.734 [0.677, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.181, 10.100], loss: 0.003495, mae: 0.062855, mean_q: 1.285689
 18359/100000: episode: 306, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 14.104, mean reward: 0.784 [0.739, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.480, 10.100], loss: 0.002744, mae: 0.057207, mean_q: 1.278334
 18376/100000: episode: 307, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 12.327, mean reward: 0.725 [0.653, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.312, 10.100], loss: 0.003500, mae: 0.062452, mean_q: 1.258697
 18395/100000: episode: 308, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 13.683, mean reward: 0.720 [0.668, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.397, 10.100], loss: 0.002674, mae: 0.053668, mean_q: 1.274345
 18414/100000: episode: 309, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 14.079, mean reward: 0.741 [0.658, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.333, 10.100], loss: 0.002921, mae: 0.055144, mean_q: 1.275565
 18433/100000: episode: 310, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 13.807, mean reward: 0.727 [0.624, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.092, 10.100], loss: 0.002125, mae: 0.049755, mean_q: 1.273893
 18450/100000: episode: 311, duration: 0.097s, episode steps: 17, steps per second: 174, episode reward: 12.690, mean reward: 0.746 [0.688, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.422, 10.100], loss: 0.002262, mae: 0.050900, mean_q: 1.282722
 18469/100000: episode: 312, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 11.665, mean reward: 0.614 [0.550, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.846, 10.100], loss: 0.002327, mae: 0.051565, mean_q: 1.288915
 18488/100000: episode: 313, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 14.548, mean reward: 0.766 [0.686, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.318, 10.100], loss: 0.002487, mae: 0.051445, mean_q: 1.279820
 18509/100000: episode: 314, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 15.144, mean reward: 0.721 [0.642, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.286, 10.100], loss: 0.002381, mae: 0.051546, mean_q: 1.273734
 18526/100000: episode: 315, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 12.039, mean reward: 0.708 [0.642, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.356, 10.100], loss: 0.002433, mae: 0.051032, mean_q: 1.282059
 18545/100000: episode: 316, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 13.594, mean reward: 0.715 [0.644, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.477, 10.100], loss: 0.002116, mae: 0.048760, mean_q: 1.290244
 18566/100000: episode: 317, duration: 0.134s, episode steps: 21, steps per second: 157, episode reward: 15.531, mean reward: 0.740 [0.696, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.305, 10.100], loss: 0.002107, mae: 0.048667, mean_q: 1.286566
 18586/100000: episode: 318, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 12.990, mean reward: 0.650 [0.604, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.140, 10.100], loss: 0.002195, mae: 0.050225, mean_q: 1.287853
 18607/100000: episode: 319, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 14.941, mean reward: 0.711 [0.642, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.273, 10.100], loss: 0.002214, mae: 0.051395, mean_q: 1.290916
 18626/100000: episode: 320, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 13.237, mean reward: 0.697 [0.615, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.731, 10.100], loss: 0.002596, mae: 0.053123, mean_q: 1.285603
 18643/100000: episode: 321, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 11.547, mean reward: 0.679 [0.600, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.424, 10.100], loss: 0.003010, mae: 0.057568, mean_q: 1.278869
 18660/100000: episode: 322, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 11.224, mean reward: 0.660 [0.603, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.461, 10.100], loss: 0.002223, mae: 0.051052, mean_q: 1.280912
 18677/100000: episode: 323, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 12.358, mean reward: 0.727 [0.658, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.411, 10.100], loss: 0.001915, mae: 0.046260, mean_q: 1.284446
 18697/100000: episode: 324, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 13.372, mean reward: 0.669 [0.615, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.329, 10.100], loss: 0.002255, mae: 0.050869, mean_q: 1.283672
 18718/100000: episode: 325, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 15.194, mean reward: 0.724 [0.654, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.219, 10.100], loss: 0.002064, mae: 0.049250, mean_q: 1.283087
 18735/100000: episode: 326, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 12.189, mean reward: 0.717 [0.649, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.380, 10.100], loss: 0.002165, mae: 0.050175, mean_q: 1.293796
 18754/100000: episode: 327, duration: 0.113s, episode steps: 19, steps per second: 169, episode reward: 14.012, mean reward: 0.737 [0.698, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.242, 10.100], loss: 0.002591, mae: 0.054351, mean_q: 1.283665
 18773/100000: episode: 328, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 14.018, mean reward: 0.738 [0.685, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.331, 10.100], loss: 0.002622, mae: 0.055628, mean_q: 1.280075
 18790/100000: episode: 329, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 12.464, mean reward: 0.733 [0.653, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.288, 10.100], loss: 0.002271, mae: 0.050085, mean_q: 1.292686
[Info] 3-TH LEVEL FOUND: 1.501257061958313, Considering 10/90 traces
 18807/100000: episode: 330, duration: 4.274s, episode steps: 17, steps per second: 4, episode reward: 11.670, mean reward: 0.686 [0.641, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.361, 10.100], loss: 0.001735, mae: 0.045847, mean_q: 1.297398
 18819/100000: episode: 331, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 9.978, mean reward: 0.832 [0.703, 0.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.552, 10.100], loss: 0.002055, mae: 0.048593, mean_q: 1.277859
 18831/100000: episode: 332, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 8.958, mean reward: 0.747 [0.673, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.783, 10.100], loss: 0.001916, mae: 0.047311, mean_q: 1.293627
 18844/100000: episode: 333, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.318, mean reward: 0.717 [0.642, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.410, 10.100], loss: 0.002008, mae: 0.047834, mean_q: 1.296031
 18856/100000: episode: 334, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 9.249, mean reward: 0.771 [0.717, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.329, 10.100], loss: 0.002088, mae: 0.049004, mean_q: 1.291930
 18867/100000: episode: 335, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 8.154, mean reward: 0.741 [0.706, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.468, 10.100], loss: 0.002541, mae: 0.052108, mean_q: 1.273407
 18879/100000: episode: 336, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 8.677, mean reward: 0.723 [0.659, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.337, 10.100], loss: 0.002296, mae: 0.051463, mean_q: 1.310270
 18890/100000: episode: 337, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 8.884, mean reward: 0.808 [0.782, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.991, 10.100], loss: 0.003615, mae: 0.066922, mean_q: 1.294473
 18899/100000: episode: 338, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 7.839, mean reward: 0.871 [0.790, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.891, 10.100], loss: 0.002900, mae: 0.053580, mean_q: 1.286708
 18912/100000: episode: 339, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 9.757, mean reward: 0.751 [0.696, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.408, 10.100], loss: 0.002236, mae: 0.052056, mean_q: 1.289433
 18923/100000: episode: 340, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 8.414, mean reward: 0.765 [0.725, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.421, 10.100], loss: 0.003198, mae: 0.060182, mean_q: 1.282660
 18937/100000: episode: 341, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 10.682, mean reward: 0.763 [0.724, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.352, 10.100], loss: 0.002225, mae: 0.049271, mean_q: 1.308663
 18948/100000: episode: 342, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 8.265, mean reward: 0.751 [0.697, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.384, 10.100], loss: 0.002213, mae: 0.050912, mean_q: 1.304251
 18963/100000: episode: 343, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 12.571, mean reward: 0.838 [0.697, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.365, 10.100], loss: 0.002365, mae: 0.052198, mean_q: 1.281638
 18978/100000: episode: 344, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 11.851, mean reward: 0.790 [0.735, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.552, 10.100], loss: 0.002854, mae: 0.054267, mean_q: 1.288571
 18989/100000: episode: 345, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 9.135, mean reward: 0.830 [0.777, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.515, 10.100], loss: 0.002317, mae: 0.053022, mean_q: 1.300213
 19002/100000: episode: 346, duration: 0.072s, episode steps: 13, steps per second: 182, episode reward: 9.565, mean reward: 0.736 [0.662, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.361, 10.100], loss: 0.001927, mae: 0.045796, mean_q: 1.296999
 19013/100000: episode: 347, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 8.688, mean reward: 0.790 [0.731, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.529, 10.100], loss: 0.002478, mae: 0.052342, mean_q: 1.284289
 19026/100000: episode: 348, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 10.491, mean reward: 0.807 [0.740, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.557, 10.100], loss: 0.002137, mae: 0.048975, mean_q: 1.296406
 19035/100000: episode: 349, duration: 0.064s, episode steps: 9, steps per second: 140, episode reward: 7.016, mean reward: 0.780 [0.745, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.469, 10.100], loss: 0.002312, mae: 0.049252, mean_q: 1.309180
 19046/100000: episode: 350, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 8.102, mean reward: 0.737 [0.697, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.390, 10.100], loss: 0.002244, mae: 0.053064, mean_q: 1.303013
 19059/100000: episode: 351, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 9.567, mean reward: 0.736 [0.685, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.465, 10.100], loss: 0.002587, mae: 0.051996, mean_q: 1.300904
 19073/100000: episode: 352, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 9.884, mean reward: 0.706 [0.664, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.300, 10.100], loss: 0.002756, mae: 0.057013, mean_q: 1.306410
 19084/100000: episode: 353, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 8.819, mean reward: 0.802 [0.746, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.368, 10.100], loss: 0.002912, mae: 0.056068, mean_q: 1.295940
 19095/100000: episode: 354, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 9.214, mean reward: 0.838 [0.774, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.433, 10.100], loss: 0.002628, mae: 0.051187, mean_q: 1.292698
 19107/100000: episode: 355, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 9.096, mean reward: 0.758 [0.692, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.386, 10.100], loss: 0.002180, mae: 0.049515, mean_q: 1.299776
 19119/100000: episode: 356, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 8.556, mean reward: 0.713 [0.638, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.400, 10.100], loss: 0.001585, mae: 0.043808, mean_q: 1.310978
 19134/100000: episode: 357, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 10.685, mean reward: 0.712 [0.671, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.354, 10.100], loss: 0.002396, mae: 0.050378, mean_q: 1.314635
 19149/100000: episode: 358, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 11.550, mean reward: 0.770 [0.711, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.517, 10.100], loss: 0.002394, mae: 0.051480, mean_q: 1.310286
 19160/100000: episode: 359, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 8.961, mean reward: 0.815 [0.779, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.465, 10.100], loss: 0.002098, mae: 0.048694, mean_q: 1.309266
 19171/100000: episode: 360, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 8.997, mean reward: 0.818 [0.754, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.469, 10.100], loss: 0.003127, mae: 0.059540, mean_q: 1.307265
 19183/100000: episode: 361, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 8.474, mean reward: 0.706 [0.636, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.756, 10.100], loss: 0.001970, mae: 0.049740, mean_q: 1.303020
 19196/100000: episode: 362, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 9.787, mean reward: 0.753 [0.695, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.636, 10.100], loss: 0.001923, mae: 0.046911, mean_q: 1.314966
 19209/100000: episode: 363, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 10.292, mean reward: 0.792 [0.705, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.538, 10.100], loss: 0.002012, mae: 0.049207, mean_q: 1.317193
 19223/100000: episode: 364, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 11.166, mean reward: 0.798 [0.747, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.454, 10.100], loss: 0.001913, mae: 0.047337, mean_q: 1.311036
 19237/100000: episode: 365, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 11.368, mean reward: 0.812 [0.747, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.548, 10.100], loss: 0.002383, mae: 0.052218, mean_q: 1.302057
 19252/100000: episode: 366, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 12.005, mean reward: 0.800 [0.728, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.711, 10.100], loss: 0.002093, mae: 0.049150, mean_q: 1.320961
 19264/100000: episode: 367, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 8.208, mean reward: 0.684 [0.625, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.226, 10.100], loss: 0.001843, mae: 0.049447, mean_q: 1.332137
 19278/100000: episode: 368, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 9.960, mean reward: 0.711 [0.672, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.308, 10.100], loss: 0.002045, mae: 0.048769, mean_q: 1.301789
 19289/100000: episode: 369, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 9.277, mean reward: 0.843 [0.790, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.576, 10.100], loss: 0.002030, mae: 0.050104, mean_q: 1.313581
 19298/100000: episode: 370, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 6.771, mean reward: 0.752 [0.687, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.417, 10.100], loss: 0.001943, mae: 0.047791, mean_q: 1.315535
 19310/100000: episode: 371, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 8.600, mean reward: 0.717 [0.681, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.372, 10.100], loss: 0.002428, mae: 0.050605, mean_q: 1.310671
 19323/100000: episode: 372, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 9.547, mean reward: 0.734 [0.694, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.299, 10.100], loss: 0.002506, mae: 0.053219, mean_q: 1.305495
 19338/100000: episode: 373, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 11.723, mean reward: 0.782 [0.727, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.330, 10.100], loss: 0.002368, mae: 0.051749, mean_q: 1.315754
 19350/100000: episode: 374, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 10.219, mean reward: 0.852 [0.799, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.467, 10.100], loss: 0.002399, mae: 0.051721, mean_q: 1.307003
 19361/100000: episode: 375, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 8.535, mean reward: 0.776 [0.742, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.363, 10.100], loss: 0.003005, mae: 0.055964, mean_q: 1.335018
 19373/100000: episode: 376, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 9.222, mean reward: 0.769 [0.692, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-2.315, 10.100], loss: 0.003998, mae: 0.067563, mean_q: 1.329841
 19387/100000: episode: 377, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 10.967, mean reward: 0.783 [0.701, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.440, 10.100], loss: 0.003130, mae: 0.059891, mean_q: 1.314828
 19401/100000: episode: 378, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 9.991, mean reward: 0.714 [0.662, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.437, 10.100], loss: 0.001984, mae: 0.046673, mean_q: 1.336894
 19414/100000: episode: 379, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 10.011, mean reward: 0.770 [0.678, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.506, 10.100], loss: 0.002014, mae: 0.049001, mean_q: 1.315166
 19428/100000: episode: 380, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 11.023, mean reward: 0.787 [0.668, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.477, 10.100], loss: 0.002011, mae: 0.049033, mean_q: 1.328000
 19439/100000: episode: 381, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 8.282, mean reward: 0.753 [0.711, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.502, 10.100], loss: 0.002057, mae: 0.047878, mean_q: 1.332414
 19454/100000: episode: 382, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 11.157, mean reward: 0.744 [0.694, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.257, 10.100], loss: 0.002200, mae: 0.050112, mean_q: 1.322958
 19467/100000: episode: 383, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 10.596, mean reward: 0.815 [0.686, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.176, 10.100], loss: 0.002075, mae: 0.049644, mean_q: 1.317091
 19478/100000: episode: 384, duration: 0.071s, episode steps: 11, steps per second: 156, episode reward: 8.439, mean reward: 0.767 [0.691, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.528, 10.100], loss: 0.001831, mae: 0.046031, mean_q: 1.311123
 19487/100000: episode: 385, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 7.378, mean reward: 0.820 [0.758, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.567, 10.100], loss: 0.001940, mae: 0.047631, mean_q: 1.330582
 19501/100000: episode: 386, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 11.028, mean reward: 0.788 [0.734, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.473, 10.100], loss: 0.002572, mae: 0.055355, mean_q: 1.332408
 19515/100000: episode: 387, duration: 0.084s, episode steps: 14, steps per second: 168, episode reward: 11.060, mean reward: 0.790 [0.729, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.635, 10.100], loss: 0.002004, mae: 0.047712, mean_q: 1.326031
 19529/100000: episode: 388, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 11.097, mean reward: 0.793 [0.722, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.368, 10.100], loss: 0.002185, mae: 0.051010, mean_q: 1.329374
 19543/100000: episode: 389, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 9.782, mean reward: 0.699 [0.659, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.355, 10.100], loss: 0.001908, mae: 0.047009, mean_q: 1.325896
 19556/100000: episode: 390, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.920, mean reward: 0.763 [0.672, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.600, 10.100], loss: 0.001904, mae: 0.048045, mean_q: 1.341916
 19568/100000: episode: 391, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 8.868, mean reward: 0.739 [0.650, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.873, 10.100], loss: 0.002090, mae: 0.050004, mean_q: 1.336319
 19583/100000: episode: 392, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 10.889, mean reward: 0.726 [0.686, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.404, 10.100], loss: 0.001647, mae: 0.044890, mean_q: 1.323455
 19597/100000: episode: 393, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 11.060, mean reward: 0.790 [0.719, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.706, 10.100], loss: 0.001802, mae: 0.046551, mean_q: 1.332605
 19606/100000: episode: 394, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 6.985, mean reward: 0.776 [0.747, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.908, 10.100], loss: 0.001649, mae: 0.043200, mean_q: 1.343029
 19619/100000: episode: 395, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 9.587, mean reward: 0.737 [0.673, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.408, 10.100], loss: 0.001523, mae: 0.043198, mean_q: 1.340258
 19631/100000: episode: 396, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 9.632, mean reward: 0.803 [0.729, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.496, 10.100], loss: 0.001755, mae: 0.045580, mean_q: 1.353167
 19646/100000: episode: 397, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 11.318, mean reward: 0.755 [0.704, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.333, 10.100], loss: 0.002049, mae: 0.048966, mean_q: 1.337077
 19657/100000: episode: 398, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 8.666, mean reward: 0.788 [0.735, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.520, 10.100], loss: 0.002397, mae: 0.050333, mean_q: 1.334596
 19670/100000: episode: 399, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 10.414, mean reward: 0.801 [0.696, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.668, 10.100], loss: 0.001898, mae: 0.047538, mean_q: 1.347799
 19684/100000: episode: 400, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 10.667, mean reward: 0.762 [0.664, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.836, 10.100], loss: 0.001986, mae: 0.049413, mean_q: 1.342387
 19698/100000: episode: 401, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 11.106, mean reward: 0.793 [0.681, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.345, 10.100], loss: 0.002931, mae: 0.057154, mean_q: 1.327879
 19712/100000: episode: 402, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 11.059, mean reward: 0.790 [0.694, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.430, 10.100], loss: 0.001731, mae: 0.046883, mean_q: 1.349431
 19724/100000: episode: 403, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 8.871, mean reward: 0.739 [0.670, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.528, 10.100], loss: 0.001834, mae: 0.047727, mean_q: 1.342486
 19738/100000: episode: 404, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 11.248, mean reward: 0.803 [0.713, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.887, 10.100], loss: 0.001975, mae: 0.048489, mean_q: 1.351330
 19753/100000: episode: 405, duration: 0.095s, episode steps: 15, steps per second: 157, episode reward: 11.201, mean reward: 0.747 [0.662, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.517, 10.100], loss: 0.001988, mae: 0.047582, mean_q: 1.340749
 19764/100000: episode: 406, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 8.811, mean reward: 0.801 [0.765, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.875, 10.100], loss: 0.001806, mae: 0.045476, mean_q: 1.355273
 19778/100000: episode: 407, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 10.561, mean reward: 0.754 [0.683, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.595, 10.100], loss: 0.001941, mae: 0.050004, mean_q: 1.353993
 19787/100000: episode: 408, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 6.816, mean reward: 0.757 [0.705, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.445, 10.100], loss: 0.002713, mae: 0.056104, mean_q: 1.322760
 19800/100000: episode: 409, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 9.986, mean reward: 0.768 [0.701, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.544, 10.100], loss: 0.002356, mae: 0.051828, mean_q: 1.347564
 19815/100000: episode: 410, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 10.855, mean reward: 0.724 [0.686, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.288, 10.100], loss: 0.002734, mae: 0.057982, mean_q: 1.330523
 19824/100000: episode: 411, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 6.798, mean reward: 0.755 [0.712, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.465, 10.100], loss: 0.001626, mae: 0.043507, mean_q: 1.368826
 19837/100000: episode: 412, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 9.937, mean reward: 0.764 [0.654, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.886, 10.100], loss: 0.001869, mae: 0.048099, mean_q: 1.350814
 19846/100000: episode: 413, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 6.875, mean reward: 0.764 [0.712, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.560, 10.100], loss: 0.002061, mae: 0.048212, mean_q: 1.321788
 19858/100000: episode: 414, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 8.622, mean reward: 0.718 [0.651, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.358, 10.100], loss: 0.002007, mae: 0.049156, mean_q: 1.342961
 19867/100000: episode: 415, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 6.852, mean reward: 0.761 [0.733, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.443, 10.100], loss: 0.001994, mae: 0.049485, mean_q: 1.363442
 19882/100000: episode: 416, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 11.258, mean reward: 0.751 [0.694, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.424, 10.100], loss: 0.001910, mae: 0.048389, mean_q: 1.343040
 19893/100000: episode: 417, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 8.878, mean reward: 0.807 [0.711, 0.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.514, 10.100], loss: 0.002359, mae: 0.049478, mean_q: 1.342101
 19905/100000: episode: 418, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 9.694, mean reward: 0.808 [0.732, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.481, 10.100], loss: 0.002251, mae: 0.051181, mean_q: 1.355812
 19916/100000: episode: 419, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 8.396, mean reward: 0.763 [0.717, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.430, 10.100], loss: 0.002101, mae: 0.048742, mean_q: 1.344157
[Info] 4-TH LEVEL FOUND: 1.5618717670440674, Considering 10/90 traces
 19930/100000: episode: 420, duration: 4.282s, episode steps: 14, steps per second: 3, episode reward: 11.646, mean reward: 0.832 [0.732, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.617, 10.100], loss: 0.002248, mae: 0.052247, mean_q: 1.358812
 19936/100000: episode: 421, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 4.820, mean reward: 0.803 [0.781, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.439, 10.100], loss: 0.002580, mae: 0.055103, mean_q: 1.347260
 19942/100000: episode: 422, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 4.885, mean reward: 0.814 [0.788, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.525, 10.100], loss: 0.001754, mae: 0.046686, mean_q: 1.354521
 19951/100000: episode: 423, duration: 0.059s, episode steps: 9, steps per second: 151, episode reward: 7.356, mean reward: 0.817 [0.754, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.634, 10.100], loss: 0.002129, mae: 0.050639, mean_q: 1.333105
 19960/100000: episode: 424, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 7.575, mean reward: 0.842 [0.780, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.594, 10.100], loss: 0.002010, mae: 0.048539, mean_q: 1.356061
 19971/100000: episode: 425, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 9.400, mean reward: 0.855 [0.798, 0.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.538, 10.100], loss: 0.002636, mae: 0.056911, mean_q: 1.346378
 19981/100000: episode: 426, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 8.213, mean reward: 0.821 [0.738, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.495, 10.100], loss: 0.002792, mae: 0.054835, mean_q: 1.359975
 19991/100000: episode: 427, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 8.440, mean reward: 0.844 [0.803, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.970, 10.100], loss: 0.001585, mae: 0.045117, mean_q: 1.362620
 20001/100000: episode: 428, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 8.186, mean reward: 0.819 [0.781, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.527, 10.100], loss: 0.001864, mae: 0.046490, mean_q: 1.359689
 20011/100000: episode: 429, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 8.180, mean reward: 0.818 [0.767, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.369, 10.100], loss: 0.002150, mae: 0.048726, mean_q: 1.339242
 20020/100000: episode: 430, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 7.488, mean reward: 0.832 [0.765, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.405, 10.100], loss: 0.002380, mae: 0.052268, mean_q: 1.314713
 20029/100000: episode: 431, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 7.456, mean reward: 0.828 [0.765, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.524, 10.100], loss: 0.001716, mae: 0.043474, mean_q: 1.357940
 20036/100000: episode: 432, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 5.808, mean reward: 0.830 [0.803, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.470, 10.100], loss: 0.002084, mae: 0.048260, mean_q: 1.337667
 20045/100000: episode: 433, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 7.536, mean reward: 0.837 [0.785, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.556, 10.100], loss: 0.002116, mae: 0.050513, mean_q: 1.344810
 20054/100000: episode: 434, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 7.283, mean reward: 0.809 [0.778, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.608, 10.100], loss: 0.002099, mae: 0.049521, mean_q: 1.354716
 20065/100000: episode: 435, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 9.163, mean reward: 0.833 [0.781, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.489, 10.100], loss: 0.001843, mae: 0.047879, mean_q: 1.336475
 20074/100000: episode: 436, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 7.697, mean reward: 0.855 [0.756, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.562, 10.100], loss: 0.001733, mae: 0.045867, mean_q: 1.347360
[Info] FALSIFICATION!
 20078/100000: episode: 437, duration: 0.506s, episode steps: 4, steps per second: 8, episode reward: 3.671, mean reward: 0.918 [0.786, 1.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.483, 10.046], loss: 0.001758, mae: 0.046528, mean_q: 1.375851
 20086/100000: episode: 438, duration: 0.066s, episode steps: 8, steps per second: 121, episode reward: 6.422, mean reward: 0.803 [0.742, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.719, 10.100], loss: 0.002062, mae: 0.048135, mean_q: 1.369666
 20092/100000: episode: 439, duration: 0.047s, episode steps: 6, steps per second: 128, episode reward: 5.175, mean reward: 0.863 [0.808, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.475, 10.100], loss: 0.001967, mae: 0.047366, mean_q: 1.330399
 20102/100000: episode: 440, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 7.905, mean reward: 0.790 [0.708, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.534, 10.100], loss: 0.001856, mae: 0.048315, mean_q: 1.365488
 20112/100000: episode: 441, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 7.859, mean reward: 0.786 [0.736, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.533, 10.100], loss: 0.002044, mae: 0.048618, mean_q: 1.362008
 20121/100000: episode: 442, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 8.223, mean reward: 0.914 [0.844, 0.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.654, 10.100], loss: 0.001653, mae: 0.043688, mean_q: 1.360396
 20130/100000: episode: 443, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 7.819, mean reward: 0.869 [0.793, 0.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.582, 10.100], loss: 0.001882, mae: 0.047055, mean_q: 1.345928
 20137/100000: episode: 444, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 5.466, mean reward: 0.781 [0.728, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.467, 10.100], loss: 0.002763, mae: 0.050692, mean_q: 1.384974
 20147/100000: episode: 445, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 8.041, mean reward: 0.804 [0.712, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.571, 10.100], loss: 0.002283, mae: 0.048414, mean_q: 1.364002
 20157/100000: episode: 446, duration: 0.066s, episode steps: 10, steps per second: 153, episode reward: 8.483, mean reward: 0.848 [0.779, 0.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.416, 10.100], loss: 0.002593, mae: 0.052726, mean_q: 1.368722
 20163/100000: episode: 447, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 5.017, mean reward: 0.836 [0.820, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.572, 10.100], loss: 0.001932, mae: 0.048438, mean_q: 1.358752
 20171/100000: episode: 448, duration: 0.057s, episode steps: 8, steps per second: 140, episode reward: 6.637, mean reward: 0.830 [0.765, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.252, 10.100], loss: 0.001671, mae: 0.044332, mean_q: 1.369584
 20180/100000: episode: 449, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 7.466, mean reward: 0.830 [0.776, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.526, 10.100], loss: 0.001883, mae: 0.045767, mean_q: 1.375696
 20191/100000: episode: 450, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 9.338, mean reward: 0.849 [0.777, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.546, 10.100], loss: 0.002260, mae: 0.051360, mean_q: 1.381539
 20200/100000: episode: 451, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 7.526, mean reward: 0.836 [0.801, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.627, 10.100], loss: 0.002266, mae: 0.051862, mean_q: 1.370663
 20207/100000: episode: 452, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 5.241, mean reward: 0.749 [0.711, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.386, 10.100], loss: 0.003441, mae: 0.051954, mean_q: 1.375766
 20218/100000: episode: 453, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 9.706, mean reward: 0.882 [0.813, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.549, 10.100], loss: 0.001868, mae: 0.045769, mean_q: 1.355375
 20225/100000: episode: 454, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 5.485, mean reward: 0.784 [0.766, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.589, 10.100], loss: 0.001924, mae: 0.045447, mean_q: 1.348685
 20235/100000: episode: 455, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 8.014, mean reward: 0.801 [0.739, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.588, 10.100], loss: 0.001662, mae: 0.045978, mean_q: 1.355106
 20244/100000: episode: 456, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 7.126, mean reward: 0.792 [0.756, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.599, 10.100], loss: 0.001537, mae: 0.044319, mean_q: 1.368511
 20251/100000: episode: 457, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 6.384, mean reward: 0.912 [0.812, 0.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.661, 10.100], loss: 0.002744, mae: 0.047453, mean_q: 1.392766
 20261/100000: episode: 458, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 8.108, mean reward: 0.811 [0.720, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.943, 10.100], loss: 0.001900, mae: 0.046833, mean_q: 1.361280
 20270/100000: episode: 459, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 7.375, mean reward: 0.819 [0.769, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.611, 10.100], loss: 0.002303, mae: 0.053594, mean_q: 1.349306
 20277/100000: episode: 460, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 6.020, mean reward: 0.860 [0.770, 0.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.621, 10.100], loss: 0.001810, mae: 0.046829, mean_q: 1.350821
[Info] FALSIFICATION!
 20282/100000: episode: 461, duration: 0.312s, episode steps: 5, steps per second: 16, episode reward: 4.593, mean reward: 0.919 [0.870, 1.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.719, 10.098], loss: 0.001704, mae: 0.046564, mean_q: 1.360464
 20292/100000: episode: 462, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 8.513, mean reward: 0.851 [0.764, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.633, 10.100], loss: 0.002159, mae: 0.045505, mean_q: 1.375632
[Info] FALSIFICATION!
 20300/100000: episode: 463, duration: 0.246s, episode steps: 8, steps per second: 33, episode reward: 7.152, mean reward: 0.894 [0.804, 1.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.555, 10.098], loss: 0.002113, mae: 0.050336, mean_q: 1.355946
 20308/100000: episode: 464, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 6.446, mean reward: 0.806 [0.766, 0.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.511, 10.100], loss: 0.002211, mae: 0.049005, mean_q: 1.349752
 20315/100000: episode: 465, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 5.694, mean reward: 0.813 [0.700, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.626, 10.100], loss: 0.001771, mae: 0.045895, mean_q: 1.354721
 20322/100000: episode: 466, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 5.270, mean reward: 0.753 [0.706, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.514, 10.100], loss: 0.002229, mae: 0.049647, mean_q: 1.351486
 20332/100000: episode: 467, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 8.445, mean reward: 0.845 [0.744, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.401, 10.100], loss: 0.002098, mae: 0.052191, mean_q: 1.378252
 20342/100000: episode: 468, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 7.424, mean reward: 0.742 [0.706, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.549, 10.100], loss: 0.002026, mae: 0.048215, mean_q: 1.355494
 20352/100000: episode: 469, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 8.111, mean reward: 0.811 [0.702, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.584, 10.100], loss: 0.002278, mae: 0.050619, mean_q: 1.360959
 20359/100000: episode: 470, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 5.918, mean reward: 0.845 [0.755, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.640, 10.100], loss: 0.002235, mae: 0.053163, mean_q: 1.404005
 20369/100000: episode: 471, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 7.830, mean reward: 0.783 [0.732, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.449, 10.100], loss: 0.002575, mae: 0.053697, mean_q: 1.352359
 20376/100000: episode: 472, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 5.536, mean reward: 0.791 [0.737, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.518, 10.100], loss: 0.003434, mae: 0.064479, mean_q: 1.368383
 20382/100000: episode: 473, duration: 0.039s, episode steps: 6, steps per second: 156, episode reward: 4.892, mean reward: 0.815 [0.788, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.510, 10.100], loss: 0.002207, mae: 0.052223, mean_q: 1.385048
 20391/100000: episode: 474, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 7.583, mean reward: 0.843 [0.780, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.611, 10.100], loss: 0.002118, mae: 0.050668, mean_q: 1.363266
 20401/100000: episode: 475, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 7.707, mean reward: 0.771 [0.715, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.459, 10.100], loss: 0.002007, mae: 0.048996, mean_q: 1.393101
 20410/100000: episode: 476, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 7.766, mean reward: 0.863 [0.794, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.527, 10.100], loss: 0.002150, mae: 0.051041, mean_q: 1.362044
 20420/100000: episode: 477, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 7.944, mean reward: 0.794 [0.717, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.584, 10.100], loss: 0.002140, mae: 0.050005, mean_q: 1.366431
 20429/100000: episode: 478, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 7.247, mean reward: 0.805 [0.778, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.453, 10.100], loss: 0.002256, mae: 0.052633, mean_q: 1.356723
 20439/100000: episode: 479, duration: 0.069s, episode steps: 10, steps per second: 144, episode reward: 7.960, mean reward: 0.796 [0.712, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.457, 10.100], loss: 0.002154, mae: 0.050175, mean_q: 1.353171
 20449/100000: episode: 480, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 8.045, mean reward: 0.805 [0.781, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.504, 10.100], loss: 0.001956, mae: 0.049226, mean_q: 1.367150
 20459/100000: episode: 481, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 8.319, mean reward: 0.832 [0.762, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.621, 10.100], loss: 0.002277, mae: 0.050221, mean_q: 1.341437
 20466/100000: episode: 482, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 5.961, mean reward: 0.852 [0.814, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.543, 10.100], loss: 0.002559, mae: 0.055800, mean_q: 1.371793
 20475/100000: episode: 483, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 7.452, mean reward: 0.828 [0.785, 0.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.411, 10.100], loss: 0.001785, mae: 0.048525, mean_q: 1.403041
 20486/100000: episode: 484, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 8.666, mean reward: 0.788 [0.705, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.314, 10.100], loss: 0.003196, mae: 0.056699, mean_q: 1.352291
 20493/100000: episode: 485, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 5.857, mean reward: 0.837 [0.811, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.574, 10.100], loss: 0.002267, mae: 0.053813, mean_q: 1.363862
 20502/100000: episode: 486, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 7.509, mean reward: 0.834 [0.791, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.591, 10.100], loss: 0.002150, mae: 0.049634, mean_q: 1.370797
 20512/100000: episode: 487, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 7.760, mean reward: 0.776 [0.711, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.713, 10.100], loss: 0.001904, mae: 0.049204, mean_q: 1.368289
 20522/100000: episode: 488, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 8.250, mean reward: 0.825 [0.785, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.522, 10.100], loss: 0.002397, mae: 0.052809, mean_q: 1.374761
 20532/100000: episode: 489, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 8.043, mean reward: 0.804 [0.707, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.604, 10.100], loss: 0.001969, mae: 0.047957, mean_q: 1.375979
 20539/100000: episode: 490, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 5.697, mean reward: 0.814 [0.753, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.579, 10.100], loss: 0.001464, mae: 0.044202, mean_q: 1.387221
 20545/100000: episode: 491, duration: 0.043s, episode steps: 6, steps per second: 138, episode reward: 5.502, mean reward: 0.917 [0.867, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.538, 10.100], loss: 0.002074, mae: 0.049301, mean_q: 1.399148
 20552/100000: episode: 492, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 5.402, mean reward: 0.772 [0.711, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.648, 10.100], loss: 0.002669, mae: 0.053367, mean_q: 1.340191
 20558/100000: episode: 493, duration: 0.044s, episode steps: 6, steps per second: 137, episode reward: 5.236, mean reward: 0.873 [0.845, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.562, 10.100], loss: 0.001787, mae: 0.048565, mean_q: 1.372787
 20567/100000: episode: 494, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 7.180, mean reward: 0.798 [0.758, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.553, 10.100], loss: 0.001883, mae: 0.046608, mean_q: 1.363326
 20574/100000: episode: 495, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 5.817, mean reward: 0.831 [0.788, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.437, 10.100], loss: 0.001830, mae: 0.045540, mean_q: 1.371103
 20584/100000: episode: 496, duration: 0.064s, episode steps: 10, steps per second: 155, episode reward: 7.845, mean reward: 0.785 [0.742, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.531, 10.100], loss: 0.002004, mae: 0.049688, mean_q: 1.371741
 20593/100000: episode: 497, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 7.481, mean reward: 0.831 [0.747, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.564, 10.100], loss: 0.002537, mae: 0.052358, mean_q: 1.379674
 20603/100000: episode: 498, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 8.606, mean reward: 0.861 [0.820, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.657, 10.100], loss: 0.002035, mae: 0.049598, mean_q: 1.399079
 20610/100000: episode: 499, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 5.297, mean reward: 0.757 [0.729, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.549, 10.100], loss: 0.001742, mae: 0.045539, mean_q: 1.384821
 20617/100000: episode: 500, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 5.645, mean reward: 0.806 [0.742, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.486, 10.100], loss: 0.002215, mae: 0.048569, mean_q: 1.378645
[Info] FALSIFICATION!
 20623/100000: episode: 501, duration: 0.217s, episode steps: 6, steps per second: 28, episode reward: 5.231, mean reward: 0.872 [0.758, 1.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.599, 10.098], loss: 0.002293, mae: 0.051337, mean_q: 1.396695
[Info] FALSIFICATION!
 20630/100000: episode: 502, duration: 0.220s, episode steps: 7, steps per second: 32, episode reward: 6.388, mean reward: 0.913 [0.812, 1.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.288, 10.067], loss: 0.001950, mae: 0.047761, mean_q: 1.344131
 20641/100000: episode: 503, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 9.668, mean reward: 0.879 [0.760, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.555, 10.100], loss: 0.001579, mae: 0.042494, mean_q: 1.374666
 20651/100000: episode: 504, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 8.324, mean reward: 0.832 [0.790, 0.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.444, 10.100], loss: 0.001684, mae: 0.045352, mean_q: 1.364280
 20661/100000: episode: 505, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 8.038, mean reward: 0.804 [0.743, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.980, 10.100], loss: 0.001842, mae: 0.044710, mean_q: 1.393920
 20670/100000: episode: 506, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 7.448, mean reward: 0.828 [0.790, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.608, 10.100], loss: 0.002126, mae: 0.048588, mean_q: 1.384103
 20679/100000: episode: 507, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 7.956, mean reward: 0.884 [0.791, 0.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.650, 10.100], loss: 0.003751, mae: 0.068230, mean_q: 1.376625
 20686/100000: episode: 508, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 5.490, mean reward: 0.784 [0.714, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.515, 10.100], loss: 0.001869, mae: 0.044938, mean_q: 1.385913
 20696/100000: episode: 509, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 7.914, mean reward: 0.791 [0.735, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.483, 10.100], loss: 0.001751, mae: 0.045436, mean_q: 1.360166
[Info] Complete ISplit Iteration
[Info] Levels: [1.2783827, 1.4105722, 1.5012571, 1.5618718, 1.5771747]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.88]
[Info] Error Prob: 8.800000000000002e-05

 20702/100000: episode: 510, duration: 4.408s, episode steps: 6, steps per second: 1, episode reward: 5.292, mean reward: 0.882 [0.807, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.545, 10.100], loss: 0.002050, mae: 0.051548, mean_q: 1.372105
 20802/100000: episode: 511, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 58.143, mean reward: 0.581 [0.504, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.767, 10.142], loss: 0.002320, mae: 0.051020, mean_q: 1.378620
 20902/100000: episode: 512, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.313, mean reward: 0.583 [0.501, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.279, 10.217], loss: 0.001901, mae: 0.046672, mean_q: 1.372448
 21002/100000: episode: 513, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.811, mean reward: 0.578 [0.499, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.072, 10.270], loss: 0.001855, mae: 0.047357, mean_q: 1.373169
 21102/100000: episode: 514, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 60.543, mean reward: 0.605 [0.502, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.958, 10.203], loss: 0.001700, mae: 0.044691, mean_q: 1.368962
 21202/100000: episode: 515, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 61.744, mean reward: 0.617 [0.516, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.393, 10.343], loss: 0.002024, mae: 0.048046, mean_q: 1.369190
 21302/100000: episode: 516, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 57.785, mean reward: 0.578 [0.511, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.814, 10.098], loss: 0.002003, mae: 0.047803, mean_q: 1.364787
 21402/100000: episode: 517, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 57.705, mean reward: 0.577 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.059, 10.098], loss: 0.002094, mae: 0.048306, mean_q: 1.363107
 21502/100000: episode: 518, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.315, mean reward: 0.583 [0.514, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.671, 10.185], loss: 0.002073, mae: 0.049046, mean_q: 1.352160
 21602/100000: episode: 519, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 60.491, mean reward: 0.605 [0.497, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.826, 10.098], loss: 0.001852, mae: 0.046326, mean_q: 1.360529
 21702/100000: episode: 520, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 60.728, mean reward: 0.607 [0.516, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.954, 10.098], loss: 0.002024, mae: 0.048017, mean_q: 1.356548
 21802/100000: episode: 521, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.176, mean reward: 0.572 [0.509, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.781, 10.098], loss: 0.002135, mae: 0.049319, mean_q: 1.349566
 21902/100000: episode: 522, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 57.782, mean reward: 0.578 [0.501, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.839, 10.339], loss: 0.002044, mae: 0.048262, mean_q: 1.353124
 22002/100000: episode: 523, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 58.181, mean reward: 0.582 [0.502, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.744, 10.334], loss: 0.001998, mae: 0.047548, mean_q: 1.346999
 22102/100000: episode: 524, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.021, mean reward: 0.590 [0.516, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.753, 10.294], loss: 0.002139, mae: 0.049540, mean_q: 1.340310
 22202/100000: episode: 525, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 59.533, mean reward: 0.595 [0.513, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.639, 10.243], loss: 0.001922, mae: 0.047319, mean_q: 1.339385
 22302/100000: episode: 526, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.856, mean reward: 0.589 [0.508, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.726, 10.098], loss: 0.001872, mae: 0.046794, mean_q: 1.326114
 22402/100000: episode: 527, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.673, mean reward: 0.597 [0.502, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.944, 10.098], loss: 0.001899, mae: 0.047488, mean_q: 1.322822
 22502/100000: episode: 528, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 57.614, mean reward: 0.576 [0.507, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.340, 10.156], loss: 0.001986, mae: 0.047228, mean_q: 1.324532
 22602/100000: episode: 529, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 57.166, mean reward: 0.572 [0.499, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.090, 10.098], loss: 0.001851, mae: 0.045476, mean_q: 1.307947
 22702/100000: episode: 530, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 58.503, mean reward: 0.585 [0.523, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.996, 10.142], loss: 0.001938, mae: 0.046813, mean_q: 1.316280
 22802/100000: episode: 531, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 58.159, mean reward: 0.582 [0.501, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.297, 10.117], loss: 0.001881, mae: 0.045989, mean_q: 1.307724
 22902/100000: episode: 532, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.413, mean reward: 0.594 [0.510, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.343, 10.267], loss: 0.001710, mae: 0.045233, mean_q: 1.300844
 23002/100000: episode: 533, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 61.646, mean reward: 0.616 [0.505, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.929, 10.129], loss: 0.001819, mae: 0.046501, mean_q: 1.296800
 23102/100000: episode: 534, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.993, mean reward: 0.600 [0.501, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.171, 10.098], loss: 0.001849, mae: 0.046566, mean_q: 1.299007
 23202/100000: episode: 535, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 59.653, mean reward: 0.597 [0.499, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.396, 10.159], loss: 0.001815, mae: 0.046816, mean_q: 1.286661
 23302/100000: episode: 536, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.592, mean reward: 0.576 [0.507, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.810, 10.098], loss: 0.002264, mae: 0.050974, mean_q: 1.287423
 23402/100000: episode: 537, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 57.012, mean reward: 0.570 [0.502, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.198, 10.125], loss: 0.001866, mae: 0.045762, mean_q: 1.278122
 23502/100000: episode: 538, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 59.219, mean reward: 0.592 [0.512, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.927, 10.382], loss: 0.001710, mae: 0.045243, mean_q: 1.285275
 23602/100000: episode: 539, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 59.466, mean reward: 0.595 [0.502, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.646, 10.098], loss: 0.001781, mae: 0.045824, mean_q: 1.269331
 23702/100000: episode: 540, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.499, mean reward: 0.585 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.868, 10.178], loss: 0.001728, mae: 0.045148, mean_q: 1.270464
 23802/100000: episode: 541, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 58.732, mean reward: 0.587 [0.501, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.460, 10.182], loss: 0.001652, mae: 0.044512, mean_q: 1.261092
 23902/100000: episode: 542, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.997, mean reward: 0.590 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.811, 10.111], loss: 0.002022, mae: 0.047488, mean_q: 1.261689
 24002/100000: episode: 543, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 64.295, mean reward: 0.643 [0.509, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.605, 10.301], loss: 0.001957, mae: 0.047351, mean_q: 1.255878
 24102/100000: episode: 544, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.858, mean reward: 0.579 [0.500, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.714, 10.098], loss: 0.001937, mae: 0.046590, mean_q: 1.241645
 24202/100000: episode: 545, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 58.857, mean reward: 0.589 [0.502, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.726, 10.291], loss: 0.001579, mae: 0.043641, mean_q: 1.246435
 24302/100000: episode: 546, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 60.690, mean reward: 0.607 [0.510, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.183, 10.479], loss: 0.001763, mae: 0.045425, mean_q: 1.242920
 24402/100000: episode: 547, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 61.796, mean reward: 0.618 [0.514, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.745, 10.098], loss: 0.001919, mae: 0.046860, mean_q: 1.236147
 24502/100000: episode: 548, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 62.049, mean reward: 0.620 [0.515, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.811, 10.098], loss: 0.001850, mae: 0.045702, mean_q: 1.234510
 24602/100000: episode: 549, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 57.607, mean reward: 0.576 [0.510, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.328, 10.098], loss: 0.001766, mae: 0.045202, mean_q: 1.226949
 24702/100000: episode: 550, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 59.969, mean reward: 0.600 [0.510, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.858, 10.098], loss: 0.001881, mae: 0.046064, mean_q: 1.224665
 24802/100000: episode: 551, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.303, mean reward: 0.583 [0.498, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.800, 10.221], loss: 0.001719, mae: 0.045170, mean_q: 1.225865
 24902/100000: episode: 552, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 57.612, mean reward: 0.576 [0.501, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.347, 10.098], loss: 0.001720, mae: 0.045091, mean_q: 1.213970
 25002/100000: episode: 553, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 60.750, mean reward: 0.608 [0.505, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.555, 10.098], loss: 0.002083, mae: 0.048414, mean_q: 1.210435
 25102/100000: episode: 554, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.987, mean reward: 0.580 [0.505, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.842, 10.138], loss: 0.001777, mae: 0.045910, mean_q: 1.198288
 25202/100000: episode: 555, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 61.857, mean reward: 0.619 [0.506, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.834, 10.098], loss: 0.001633, mae: 0.044184, mean_q: 1.193861
 25302/100000: episode: 556, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 57.579, mean reward: 0.576 [0.503, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.669, 10.140], loss: 0.001666, mae: 0.044923, mean_q: 1.191807
 25402/100000: episode: 557, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 56.183, mean reward: 0.562 [0.508, 0.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.565, 10.257], loss: 0.001938, mae: 0.046572, mean_q: 1.184000
 25502/100000: episode: 558, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 58.085, mean reward: 0.581 [0.509, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.614, 10.176], loss: 0.001759, mae: 0.046210, mean_q: 1.181650
 25602/100000: episode: 559, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 60.672, mean reward: 0.607 [0.505, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.481, 10.098], loss: 0.001572, mae: 0.043683, mean_q: 1.177830
 25702/100000: episode: 560, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 58.573, mean reward: 0.586 [0.498, 0.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.958, 10.313], loss: 0.001532, mae: 0.043497, mean_q: 1.171509
 25802/100000: episode: 561, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 60.315, mean reward: 0.603 [0.511, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.952, 10.098], loss: 0.001488, mae: 0.042635, mean_q: 1.169202
 25902/100000: episode: 562, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 59.212, mean reward: 0.592 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.788, 10.382], loss: 0.001469, mae: 0.042496, mean_q: 1.172073
 26002/100000: episode: 563, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 58.024, mean reward: 0.580 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.148, 10.393], loss: 0.001601, mae: 0.043994, mean_q: 1.169818
 26102/100000: episode: 564, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.624, mean reward: 0.586 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.557, 10.289], loss: 0.001506, mae: 0.042789, mean_q: 1.171094
 26202/100000: episode: 565, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 60.538, mean reward: 0.605 [0.505, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.513, 10.098], loss: 0.001387, mae: 0.040861, mean_q: 1.173263
 26302/100000: episode: 566, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.920, mean reward: 0.599 [0.502, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.459, 10.098], loss: 0.001547, mae: 0.042759, mean_q: 1.165159
 26402/100000: episode: 567, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 63.200, mean reward: 0.632 [0.508, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.280, 10.646], loss: 0.001389, mae: 0.040671, mean_q: 1.173443
 26502/100000: episode: 568, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 59.013, mean reward: 0.590 [0.504, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.500, 10.316], loss: 0.001469, mae: 0.042202, mean_q: 1.172814
 26602/100000: episode: 569, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 64.980, mean reward: 0.650 [0.509, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.675, 10.098], loss: 0.001618, mae: 0.044108, mean_q: 1.173950
 26702/100000: episode: 570, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.169, mean reward: 0.592 [0.516, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.784, 10.300], loss: 0.001555, mae: 0.043168, mean_q: 1.176624
 26802/100000: episode: 571, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 57.779, mean reward: 0.578 [0.510, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.875, 10.098], loss: 0.001415, mae: 0.041318, mean_q: 1.173536
 26902/100000: episode: 572, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 58.077, mean reward: 0.581 [0.499, 0.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.878, 10.290], loss: 0.001491, mae: 0.042416, mean_q: 1.175058
 27002/100000: episode: 573, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 57.884, mean reward: 0.579 [0.504, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.872, 10.257], loss: 0.001561, mae: 0.043183, mean_q: 1.174440
 27102/100000: episode: 574, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.264, mean reward: 0.593 [0.498, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.409, 10.456], loss: 0.001547, mae: 0.042718, mean_q: 1.171604
 27202/100000: episode: 575, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 60.545, mean reward: 0.605 [0.503, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.531, 10.336], loss: 0.001409, mae: 0.041552, mean_q: 1.172135
 27302/100000: episode: 576, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.630, mean reward: 0.586 [0.508, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.409, 10.098], loss: 0.001447, mae: 0.042100, mean_q: 1.175529
 27402/100000: episode: 577, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.114, mean reward: 0.581 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.985, 10.116], loss: 0.001335, mae: 0.040324, mean_q: 1.171723
 27502/100000: episode: 578, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 58.763, mean reward: 0.588 [0.500, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.309, 10.098], loss: 0.001517, mae: 0.043084, mean_q: 1.173333
 27602/100000: episode: 579, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.095, mean reward: 0.581 [0.506, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.639, 10.102], loss: 0.001549, mae: 0.042738, mean_q: 1.175643
 27702/100000: episode: 580, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.634, mean reward: 0.586 [0.507, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.965, 10.103], loss: 0.001578, mae: 0.043230, mean_q: 1.173615
 27802/100000: episode: 581, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.801, mean reward: 0.588 [0.506, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.461, 10.098], loss: 0.001419, mae: 0.041327, mean_q: 1.174743
 27902/100000: episode: 582, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 57.989, mean reward: 0.580 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.843, 10.136], loss: 0.001433, mae: 0.041319, mean_q: 1.173341
 28002/100000: episode: 583, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.365, mean reward: 0.574 [0.504, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.741, 10.098], loss: 0.001476, mae: 0.042789, mean_q: 1.172712
 28102/100000: episode: 584, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 64.903, mean reward: 0.649 [0.512, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.465, 10.098], loss: 0.001515, mae: 0.042389, mean_q: 1.173612
 28202/100000: episode: 585, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 58.304, mean reward: 0.583 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.820, 10.159], loss: 0.001465, mae: 0.042255, mean_q: 1.174682
 28302/100000: episode: 586, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 61.725, mean reward: 0.617 [0.508, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.385, 10.358], loss: 0.001482, mae: 0.042365, mean_q: 1.171945
 28402/100000: episode: 587, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 60.444, mean reward: 0.604 [0.509, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.248, 10.211], loss: 0.001594, mae: 0.043178, mean_q: 1.176768
 28502/100000: episode: 588, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.477, mean reward: 0.585 [0.513, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.726, 10.098], loss: 0.001449, mae: 0.041881, mean_q: 1.180777
 28602/100000: episode: 589, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 58.760, mean reward: 0.588 [0.509, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.129, 10.098], loss: 0.001365, mae: 0.040236, mean_q: 1.177192
 28702/100000: episode: 590, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 59.973, mean reward: 0.600 [0.507, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.495, 10.164], loss: 0.001393, mae: 0.041337, mean_q: 1.175308
 28802/100000: episode: 591, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.098, mean reward: 0.571 [0.507, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.848, 10.098], loss: 0.001373, mae: 0.040772, mean_q: 1.173194
 28902/100000: episode: 592, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.644, mean reward: 0.586 [0.507, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.558, 10.222], loss: 0.001328, mae: 0.040111, mean_q: 1.176158
 29002/100000: episode: 593, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 60.343, mean reward: 0.603 [0.501, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.465, 10.098], loss: 0.001454, mae: 0.042304, mean_q: 1.174672
 29102/100000: episode: 594, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 57.832, mean reward: 0.578 [0.507, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.059, 10.258], loss: 0.001379, mae: 0.040760, mean_q: 1.171592
 29202/100000: episode: 595, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 58.311, mean reward: 0.583 [0.500, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.652, 10.098], loss: 0.001404, mae: 0.041077, mean_q: 1.175539
 29302/100000: episode: 596, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 60.141, mean reward: 0.601 [0.499, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.142, 10.098], loss: 0.001386, mae: 0.040977, mean_q: 1.176571
 29402/100000: episode: 597, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.572, mean reward: 0.576 [0.507, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.816, 10.224], loss: 0.001421, mae: 0.041085, mean_q: 1.172992
 29502/100000: episode: 598, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 59.643, mean reward: 0.596 [0.512, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.399, 10.098], loss: 0.001481, mae: 0.042363, mean_q: 1.171137
 29602/100000: episode: 599, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 57.006, mean reward: 0.570 [0.501, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.854, 10.098], loss: 0.001730, mae: 0.045098, mean_q: 1.170814
 29702/100000: episode: 600, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 56.796, mean reward: 0.568 [0.501, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.073, 10.160], loss: 0.001463, mae: 0.041821, mean_q: 1.170267
 29802/100000: episode: 601, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.088, mean reward: 0.591 [0.505, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.447, 10.374], loss: 0.001348, mae: 0.040218, mean_q: 1.171069
 29902/100000: episode: 602, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 62.885, mean reward: 0.629 [0.506, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.601, 10.098], loss: 0.001497, mae: 0.042542, mean_q: 1.169675
 30002/100000: episode: 603, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 59.141, mean reward: 0.591 [0.506, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.257, 10.511], loss: 0.001341, mae: 0.040186, mean_q: 1.172752
 30102/100000: episode: 604, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 59.271, mean reward: 0.593 [0.507, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.965, 10.222], loss: 0.001486, mae: 0.042071, mean_q: 1.167881
 30202/100000: episode: 605, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 57.120, mean reward: 0.571 [0.500, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.757, 10.230], loss: 0.001476, mae: 0.041674, mean_q: 1.172235
 30302/100000: episode: 606, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 56.701, mean reward: 0.567 [0.506, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.911, 10.098], loss: 0.001606, mae: 0.043049, mean_q: 1.167932
 30402/100000: episode: 607, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 61.520, mean reward: 0.615 [0.504, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.601, 10.178], loss: 0.001602, mae: 0.043466, mean_q: 1.173075
 30502/100000: episode: 608, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 64.065, mean reward: 0.641 [0.501, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.731, 10.098], loss: 0.001601, mae: 0.043289, mean_q: 1.172780
 30602/100000: episode: 609, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 58.359, mean reward: 0.584 [0.500, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.526, 10.098], loss: 0.001524, mae: 0.041607, mean_q: 1.170107
[Info] 1-TH LEVEL FOUND: 1.3643642663955688, Considering 10/90 traces
 30702/100000: episode: 610, duration: 4.695s, episode steps: 100, steps per second: 21, episode reward: 58.311, mean reward: 0.583 [0.518, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.418, 10.112], loss: 0.001488, mae: 0.041903, mean_q: 1.175700
 30727/100000: episode: 611, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 16.672, mean reward: 0.667 [0.575, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.357, 10.100], loss: 0.001519, mae: 0.042285, mean_q: 1.172999
 30768/100000: episode: 612, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 25.976, mean reward: 0.634 [0.512, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.439, 10.155], loss: 0.001488, mae: 0.041276, mean_q: 1.177079
 30822/100000: episode: 613, duration: 0.292s, episode steps: 54, steps per second: 185, episode reward: 37.789, mean reward: 0.700 [0.557, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.079, 10.100], loss: 0.001563, mae: 0.043030, mean_q: 1.178646
 30847/100000: episode: 614, duration: 0.133s, episode steps: 25, steps per second: 187, episode reward: 17.972, mean reward: 0.719 [0.613, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.087, 10.100], loss: 0.001438, mae: 0.040072, mean_q: 1.182438
 30872/100000: episode: 615, duration: 0.142s, episode steps: 25, steps per second: 177, episode reward: 17.607, mean reward: 0.704 [0.594, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.414, 10.100], loss: 0.001877, mae: 0.048250, mean_q: 1.175018
 30919/100000: episode: 616, duration: 0.256s, episode steps: 47, steps per second: 184, episode reward: 31.628, mean reward: 0.673 [0.510, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.838, 10.158], loss: 0.001780, mae: 0.045119, mean_q: 1.177809
 30966/100000: episode: 617, duration: 0.261s, episode steps: 47, steps per second: 180, episode reward: 30.606, mean reward: 0.651 [0.572, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.320, 10.262], loss: 0.001460, mae: 0.041920, mean_q: 1.180948
 31007/100000: episode: 618, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 24.985, mean reward: 0.609 [0.514, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.404, 10.100], loss: 0.001490, mae: 0.043054, mean_q: 1.186101
 31059/100000: episode: 619, duration: 0.290s, episode steps: 52, steps per second: 180, episode reward: 35.309, mean reward: 0.679 [0.561, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.507, 10.262], loss: 0.001688, mae: 0.044159, mean_q: 1.183791
 31084/100000: episode: 620, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 15.954, mean reward: 0.638 [0.583, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.270, 10.100], loss: 0.001720, mae: 0.044839, mean_q: 1.172579
 31109/100000: episode: 621, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 16.066, mean reward: 0.643 [0.555, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.480, 10.100], loss: 0.001765, mae: 0.045402, mean_q: 1.182699
 31163/100000: episode: 622, duration: 0.295s, episode steps: 54, steps per second: 183, episode reward: 35.001, mean reward: 0.648 [0.517, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.519, 10.100], loss: 0.001626, mae: 0.043471, mean_q: 1.184105
 31176/100000: episode: 623, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 8.178, mean reward: 0.629 [0.580, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.436], loss: 0.001463, mae: 0.041982, mean_q: 1.191060
 31222/100000: episode: 624, duration: 0.264s, episode steps: 46, steps per second: 174, episode reward: 30.463, mean reward: 0.662 [0.571, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.324, 10.100], loss: 0.001648, mae: 0.043553, mean_q: 1.183478
 31263/100000: episode: 625, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 26.862, mean reward: 0.655 [0.571, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.074, 10.100], loss: 0.001458, mae: 0.040979, mean_q: 1.181224
 31309/100000: episode: 626, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 30.504, mean reward: 0.663 [0.606, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.793, 10.100], loss: 0.001436, mae: 0.042058, mean_q: 1.188690
 31334/100000: episode: 627, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 16.668, mean reward: 0.667 [0.576, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.241, 10.100], loss: 0.001496, mae: 0.042085, mean_q: 1.185478
 31388/100000: episode: 628, duration: 0.279s, episode steps: 54, steps per second: 194, episode reward: 36.456, mean reward: 0.675 [0.598, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-1.012, 10.100], loss: 0.001549, mae: 0.041530, mean_q: 1.192006
 31429/100000: episode: 629, duration: 0.218s, episode steps: 41, steps per second: 189, episode reward: 26.257, mean reward: 0.640 [0.520, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.679, 10.146], loss: 0.001756, mae: 0.044171, mean_q: 1.189025
 31475/100000: episode: 630, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 26.916, mean reward: 0.585 [0.498, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.906, 10.128], loss: 0.001588, mae: 0.043166, mean_q: 1.190911
 31500/100000: episode: 631, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 15.367, mean reward: 0.615 [0.525, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.100], loss: 0.001807, mae: 0.045181, mean_q: 1.186547
 31547/100000: episode: 632, duration: 0.230s, episode steps: 47, steps per second: 205, episode reward: 32.996, mean reward: 0.702 [0.598, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.323, 10.526], loss: 0.001867, mae: 0.046157, mean_q: 1.188765
 31599/100000: episode: 633, duration: 0.269s, episode steps: 52, steps per second: 194, episode reward: 33.319, mean reward: 0.641 [0.545, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-0.684, 10.271], loss: 0.001846, mae: 0.045233, mean_q: 1.192789
 31645/100000: episode: 634, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 28.576, mean reward: 0.621 [0.499, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.876, 10.122], loss: 0.001695, mae: 0.044022, mean_q: 1.191428
 31692/100000: episode: 635, duration: 0.274s, episode steps: 47, steps per second: 171, episode reward: 29.611, mean reward: 0.630 [0.504, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.057, 10.100], loss: 0.001521, mae: 0.042225, mean_q: 1.189062
 31738/100000: episode: 636, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 29.195, mean reward: 0.635 [0.567, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.408, 10.100], loss: 0.001756, mae: 0.045592, mean_q: 1.188725
 31779/100000: episode: 637, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 24.602, mean reward: 0.600 [0.504, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.263, 10.157], loss: 0.001850, mae: 0.045824, mean_q: 1.192841
 31804/100000: episode: 638, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 17.592, mean reward: 0.704 [0.574, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.149, 10.100], loss: 0.001814, mae: 0.045328, mean_q: 1.184782
 31829/100000: episode: 639, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 17.255, mean reward: 0.690 [0.528, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.059, 10.100], loss: 0.002016, mae: 0.045807, mean_q: 1.195475
 31854/100000: episode: 640, duration: 0.145s, episode steps: 25, steps per second: 173, episode reward: 16.083, mean reward: 0.643 [0.554, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.297, 10.100], loss: 0.001976, mae: 0.046660, mean_q: 1.186415
 31900/100000: episode: 641, duration: 0.253s, episode steps: 46, steps per second: 182, episode reward: 27.073, mean reward: 0.589 [0.499, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.634, 10.100], loss: 0.001677, mae: 0.044449, mean_q: 1.197335
 31925/100000: episode: 642, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 20.100, mean reward: 0.804 [0.628, 0.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.997, 10.540], loss: 0.001658, mae: 0.044553, mean_q: 1.193858
 31971/100000: episode: 643, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 31.349, mean reward: 0.681 [0.622, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.490, 10.100], loss: 0.001809, mae: 0.045212, mean_q: 1.195443
 32025/100000: episode: 644, duration: 0.268s, episode steps: 54, steps per second: 202, episode reward: 33.398, mean reward: 0.618 [0.516, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.260, 10.167], loss: 0.002052, mae: 0.048569, mean_q: 1.201563
 32038/100000: episode: 645, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 8.846, mean reward: 0.680 [0.649, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.052, 10.444], loss: 0.001807, mae: 0.044892, mean_q: 1.195212
 32051/100000: episode: 646, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 7.934, mean reward: 0.610 [0.570, 0.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.926, 10.298], loss: 0.001882, mae: 0.046968, mean_q: 1.195271
 32076/100000: episode: 647, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 16.741, mean reward: 0.670 [0.614, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.616, 10.384], loss: 0.001585, mae: 0.042153, mean_q: 1.192632
 32117/100000: episode: 648, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 26.274, mean reward: 0.641 [0.521, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.545, 10.177], loss: 0.001667, mae: 0.043705, mean_q: 1.203728
 32130/100000: episode: 649, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 9.168, mean reward: 0.705 [0.617, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-1.330, 10.402], loss: 0.001802, mae: 0.045809, mean_q: 1.196381
 32155/100000: episode: 650, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 17.690, mean reward: 0.708 [0.602, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.492, 10.100], loss: 0.002208, mae: 0.050295, mean_q: 1.205312
 32180/100000: episode: 651, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 19.689, mean reward: 0.788 [0.632, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.061, 10.452], loss: 0.001889, mae: 0.046592, mean_q: 1.200092
 32193/100000: episode: 652, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 8.690, mean reward: 0.668 [0.593, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.199, 10.524], loss: 0.001646, mae: 0.043204, mean_q: 1.199383
 32218/100000: episode: 653, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 16.838, mean reward: 0.674 [0.609, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.310], loss: 0.001708, mae: 0.044112, mean_q: 1.201502
 32265/100000: episode: 654, duration: 0.246s, episode steps: 47, steps per second: 191, episode reward: 31.586, mean reward: 0.672 [0.599, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-1.433, 10.356], loss: 0.001659, mae: 0.044078, mean_q: 1.203677
 32311/100000: episode: 655, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 29.836, mean reward: 0.649 [0.523, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.520, 10.100], loss: 0.001553, mae: 0.042954, mean_q: 1.198137
 32336/100000: episode: 656, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 15.636, mean reward: 0.625 [0.579, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.339, 10.325], loss: 0.001657, mae: 0.042608, mean_q: 1.200970
 32382/100000: episode: 657, duration: 0.229s, episode steps: 46, steps per second: 201, episode reward: 30.899, mean reward: 0.672 [0.552, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.057, 10.100], loss: 0.001765, mae: 0.045369, mean_q: 1.198358
 32429/100000: episode: 658, duration: 0.254s, episode steps: 47, steps per second: 185, episode reward: 32.368, mean reward: 0.689 [0.531, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.610, 10.205], loss: 0.001719, mae: 0.044622, mean_q: 1.202016
 32470/100000: episode: 659, duration: 0.202s, episode steps: 41, steps per second: 203, episode reward: 26.924, mean reward: 0.657 [0.592, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.507, 10.100], loss: 0.001710, mae: 0.044023, mean_q: 1.207338
 32483/100000: episode: 660, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 8.715, mean reward: 0.670 [0.633, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.315, 10.402], loss: 0.001919, mae: 0.046704, mean_q: 1.208447
 32508/100000: episode: 661, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 17.285, mean reward: 0.691 [0.639, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.805, 10.360], loss: 0.001718, mae: 0.043996, mean_q: 1.203046
 32549/100000: episode: 662, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 27.632, mean reward: 0.674 [0.557, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.236, 10.100], loss: 0.001774, mae: 0.045811, mean_q: 1.213313
 32562/100000: episode: 663, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 8.741, mean reward: 0.672 [0.584, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.045, 10.402], loss: 0.001613, mae: 0.044037, mean_q: 1.220706
 32587/100000: episode: 664, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 17.478, mean reward: 0.699 [0.622, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.317, 10.100], loss: 0.001719, mae: 0.043791, mean_q: 1.220471
 32612/100000: episode: 665, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 17.117, mean reward: 0.685 [0.622, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.204, 10.100], loss: 0.001928, mae: 0.047024, mean_q: 1.218692
 32637/100000: episode: 666, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 16.148, mean reward: 0.646 [0.539, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.798, 10.525], loss: 0.001619, mae: 0.043417, mean_q: 1.210114
 32678/100000: episode: 667, duration: 0.236s, episode steps: 41, steps per second: 174, episode reward: 26.987, mean reward: 0.658 [0.561, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.652, 10.100], loss: 0.001856, mae: 0.045625, mean_q: 1.211314
 32730/100000: episode: 668, duration: 0.296s, episode steps: 52, steps per second: 175, episode reward: 33.784, mean reward: 0.650 [0.545, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.342, 10.284], loss: 0.001673, mae: 0.043782, mean_q: 1.213917
 32771/100000: episode: 669, duration: 0.209s, episode steps: 41, steps per second: 197, episode reward: 25.796, mean reward: 0.629 [0.525, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.336, 10.135], loss: 0.001875, mae: 0.045313, mean_q: 1.205496
 32796/100000: episode: 670, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 15.464, mean reward: 0.619 [0.533, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.152, 10.262], loss: 0.001678, mae: 0.043874, mean_q: 1.227416
 32843/100000: episode: 671, duration: 0.235s, episode steps: 47, steps per second: 200, episode reward: 33.418, mean reward: 0.711 [0.608, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-1.309, 10.327], loss: 0.001909, mae: 0.045956, mean_q: 1.222323
 32889/100000: episode: 672, duration: 0.248s, episode steps: 46, steps per second: 185, episode reward: 27.856, mean reward: 0.606 [0.502, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.496, 10.100], loss: 0.002015, mae: 0.046817, mean_q: 1.217155
 32914/100000: episode: 673, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 17.209, mean reward: 0.688 [0.607, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.421, 10.100], loss: 0.001641, mae: 0.044308, mean_q: 1.216512
 32960/100000: episode: 674, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 29.442, mean reward: 0.640 [0.561, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.155, 10.100], loss: 0.001639, mae: 0.043649, mean_q: 1.213895
 33012/100000: episode: 675, duration: 0.259s, episode steps: 52, steps per second: 200, episode reward: 33.393, mean reward: 0.642 [0.520, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.321, 10.208], loss: 0.001653, mae: 0.043345, mean_q: 1.224833
 33053/100000: episode: 676, duration: 0.210s, episode steps: 41, steps per second: 196, episode reward: 28.775, mean reward: 0.702 [0.594, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.740, 10.100], loss: 0.001831, mae: 0.045569, mean_q: 1.218417
 33078/100000: episode: 677, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 15.850, mean reward: 0.634 [0.518, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.494, 10.162], loss: 0.001647, mae: 0.044332, mean_q: 1.222388
 33124/100000: episode: 678, duration: 0.257s, episode steps: 46, steps per second: 179, episode reward: 27.034, mean reward: 0.588 [0.505, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.395, 10.147], loss: 0.001991, mae: 0.046341, mean_q: 1.219216
 33178/100000: episode: 679, duration: 0.278s, episode steps: 54, steps per second: 194, episode reward: 35.583, mean reward: 0.659 [0.521, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.458, 10.144], loss: 0.001596, mae: 0.042881, mean_q: 1.218616
 33203/100000: episode: 680, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 18.568, mean reward: 0.743 [0.684, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.418, 10.100], loss: 0.001680, mae: 0.045508, mean_q: 1.226285
 33228/100000: episode: 681, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 16.927, mean reward: 0.677 [0.597, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.450], loss: 0.001606, mae: 0.043750, mean_q: 1.216050
 33241/100000: episode: 682, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 8.862, mean reward: 0.682 [0.574, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.169, 10.485], loss: 0.001718, mae: 0.044562, mean_q: 1.213303
 33266/100000: episode: 683, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 18.688, mean reward: 0.748 [0.584, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.372, 10.501], loss: 0.001822, mae: 0.045748, mean_q: 1.220745
 33291/100000: episode: 684, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 16.436, mean reward: 0.657 [0.612, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.149, 10.307], loss: 0.001674, mae: 0.043927, mean_q: 1.227440
 33343/100000: episode: 685, duration: 0.273s, episode steps: 52, steps per second: 191, episode reward: 39.803, mean reward: 0.765 [0.661, 0.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.780, 10.376], loss: 0.002006, mae: 0.048259, mean_q: 1.227472
 33390/100000: episode: 686, duration: 0.255s, episode steps: 47, steps per second: 185, episode reward: 33.863, mean reward: 0.720 [0.650, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.644, 10.397], loss: 0.001915, mae: 0.046594, mean_q: 1.226380
 33403/100000: episode: 687, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 8.603, mean reward: 0.662 [0.610, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.363], loss: 0.001790, mae: 0.045595, mean_q: 1.227791
 33428/100000: episode: 688, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 17.594, mean reward: 0.704 [0.641, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.345, 10.100], loss: 0.001803, mae: 0.046077, mean_q: 1.228337
 33475/100000: episode: 689, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 27.562, mean reward: 0.586 [0.501, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.318, 10.100], loss: 0.001795, mae: 0.045941, mean_q: 1.230662
 33516/100000: episode: 690, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 26.000, mean reward: 0.634 [0.514, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.551, 10.224], loss: 0.001747, mae: 0.045410, mean_q: 1.243249
 33529/100000: episode: 691, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 8.992, mean reward: 0.692 [0.615, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.515], loss: 0.001807, mae: 0.046505, mean_q: 1.242296
 33554/100000: episode: 692, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 16.607, mean reward: 0.664 [0.610, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.348], loss: 0.001661, mae: 0.044364, mean_q: 1.230310
 33579/100000: episode: 693, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 18.899, mean reward: 0.756 [0.618, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.454, 10.434], loss: 0.001891, mae: 0.046753, mean_q: 1.239541
 33604/100000: episode: 694, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 16.786, mean reward: 0.671 [0.579, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.303], loss: 0.002199, mae: 0.049289, mean_q: 1.229941
 33651/100000: episode: 695, duration: 0.241s, episode steps: 47, steps per second: 195, episode reward: 27.953, mean reward: 0.595 [0.502, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.327, 10.181], loss: 0.002035, mae: 0.047403, mean_q: 1.240778
 33664/100000: episode: 696, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 8.362, mean reward: 0.643 [0.593, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.663, 10.396], loss: 0.002191, mae: 0.051379, mean_q: 1.224115
 33716/100000: episode: 697, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 34.623, mean reward: 0.666 [0.596, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.317, 10.436], loss: 0.001981, mae: 0.048186, mean_q: 1.234193
 33729/100000: episode: 698, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 7.861, mean reward: 0.605 [0.582, 0.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.069, 10.248], loss: 0.001878, mae: 0.045225, mean_q: 1.238328
 33754/100000: episode: 699, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 16.933, mean reward: 0.677 [0.596, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.378], loss: 0.001782, mae: 0.045945, mean_q: 1.234573
[Info] 2-TH LEVEL FOUND: 1.5039174556732178, Considering 10/90 traces
 33806/100000: episode: 700, duration: 4.439s, episode steps: 52, steps per second: 12, episode reward: 33.678, mean reward: 0.648 [0.516, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.624, 10.351], loss: 0.001648, mae: 0.043404, mean_q: 1.243967
 33827/100000: episode: 701, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 15.602, mean reward: 0.743 [0.642, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.507, 10.461], loss: 0.001942, mae: 0.045894, mean_q: 1.231717
 33876/100000: episode: 702, duration: 0.269s, episode steps: 49, steps per second: 182, episode reward: 36.291, mean reward: 0.741 [0.676, 0.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.477, 10.100], loss: 0.001978, mae: 0.047294, mean_q: 1.242157
 33896/100000: episode: 703, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 15.952, mean reward: 0.798 [0.715, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.304, 10.100], loss: 0.001788, mae: 0.046534, mean_q: 1.239604
 33939/100000: episode: 704, duration: 0.212s, episode steps: 43, steps per second: 203, episode reward: 28.462, mean reward: 0.662 [0.598, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.690, 10.364], loss: 0.001762, mae: 0.045416, mean_q: 1.246369
 33971/100000: episode: 705, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 23.281, mean reward: 0.728 [0.633, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.369, 10.100], loss: 0.001733, mae: 0.044962, mean_q: 1.240908
 33989/100000: episode: 706, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 14.048, mean reward: 0.780 [0.715, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.529, 10.496], loss: 0.001532, mae: 0.043190, mean_q: 1.243648
 34010/100000: episode: 707, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 16.300, mean reward: 0.776 [0.718, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.035, 10.555], loss: 0.001556, mae: 0.043015, mean_q: 1.254037
 34031/100000: episode: 708, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 17.857, mean reward: 0.850 [0.761, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.854, 10.553], loss: 0.001885, mae: 0.046888, mean_q: 1.252676
 34049/100000: episode: 709, duration: 0.092s, episode steps: 18, steps per second: 197, episode reward: 13.097, mean reward: 0.728 [0.634, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.928, 10.323], loss: 0.001840, mae: 0.045788, mean_q: 1.253082
 34064/100000: episode: 710, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 11.518, mean reward: 0.768 [0.713, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.045, 10.466], loss: 0.001660, mae: 0.043418, mean_q: 1.249560
 34107/100000: episode: 711, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 27.377, mean reward: 0.637 [0.502, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.354, 10.100], loss: 0.001608, mae: 0.044052, mean_q: 1.255775
 34135/100000: episode: 712, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 23.582, mean reward: 0.842 [0.632, 0.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.985, 10.406], loss: 0.001590, mae: 0.043631, mean_q: 1.248216
 34156/100000: episode: 713, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 17.858, mean reward: 0.850 [0.677, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.117, 10.682], loss: 0.001791, mae: 0.045398, mean_q: 1.251599
 34176/100000: episode: 714, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 14.354, mean reward: 0.718 [0.605, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.159, 10.100], loss: 0.001613, mae: 0.043616, mean_q: 1.250553
 34219/100000: episode: 715, duration: 0.246s, episode steps: 43, steps per second: 175, episode reward: 29.069, mean reward: 0.676 [0.540, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.906, 10.172], loss: 0.002011, mae: 0.048883, mean_q: 1.259924
[Info] FALSIFICATION!
 34230/100000: episode: 716, duration: 0.283s, episode steps: 11, steps per second: 39, episode reward: 10.209, mean reward: 0.928 [0.771, 1.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.018, 10.690], loss: 0.001728, mae: 0.046463, mean_q: 1.258747
 34245/100000: episode: 717, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 12.841, mean reward: 0.856 [0.808, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.047, 10.673], loss: 0.001878, mae: 0.047033, mean_q: 1.264327
 34288/100000: episode: 718, duration: 0.236s, episode steps: 43, steps per second: 182, episode reward: 28.644, mean reward: 0.666 [0.552, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-1.255, 10.347], loss: 0.002122, mae: 0.048484, mean_q: 1.267596
 34303/100000: episode: 719, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 12.524, mean reward: 0.835 [0.731, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.622, 10.730], loss: 0.001799, mae: 0.046300, mean_q: 1.265793
 34331/100000: episode: 720, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 23.730, mean reward: 0.848 [0.778, 0.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.198, 10.571], loss: 0.001815, mae: 0.045238, mean_q: 1.266177
 34380/100000: episode: 721, duration: 0.253s, episode steps: 49, steps per second: 193, episode reward: 35.511, mean reward: 0.725 [0.541, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.973, 10.100], loss: 0.002213, mae: 0.050060, mean_q: 1.276389
 34398/100000: episode: 722, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 13.438, mean reward: 0.747 [0.675, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.654, 10.541], loss: 0.002366, mae: 0.049828, mean_q: 1.278506
 34430/100000: episode: 723, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 23.816, mean reward: 0.744 [0.662, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.349, 10.100], loss: 0.001944, mae: 0.046648, mean_q: 1.276127
 34471/100000: episode: 724, duration: 0.200s, episode steps: 41, steps per second: 205, episode reward: 28.344, mean reward: 0.691 [0.540, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.074, 10.100], loss: 0.001748, mae: 0.045673, mean_q: 1.279312
 34491/100000: episode: 725, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 14.911, mean reward: 0.746 [0.665, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.479, 10.100], loss: 0.001905, mae: 0.048007, mean_q: 1.273046
[Info] FALSIFICATION!
 34497/100000: episode: 726, duration: 0.201s, episode steps: 6, steps per second: 30, episode reward: 5.250, mean reward: 0.875 [0.765, 1.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.205, 9.754], loss: 0.001505, mae: 0.043601, mean_q: 1.282669
 34538/100000: episode: 727, duration: 0.208s, episode steps: 41, steps per second: 197, episode reward: 29.568, mean reward: 0.721 [0.620, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.164, 10.100], loss: 0.001849, mae: 0.044950, mean_q: 1.275363
 34576/100000: episode: 728, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 30.986, mean reward: 0.815 [0.647, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.492, 10.100], loss: 0.002260, mae: 0.051526, mean_q: 1.282035
 34597/100000: episode: 729, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 18.054, mean reward: 0.860 [0.751, 0.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.379, 10.542], loss: 0.002540, mae: 0.054890, mean_q: 1.288935
 34638/100000: episode: 730, duration: 0.205s, episode steps: 41, steps per second: 200, episode reward: 28.136, mean reward: 0.686 [0.599, 0.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.244, 10.100], loss: 0.001957, mae: 0.046941, mean_q: 1.288918
 34681/100000: episode: 731, duration: 0.227s, episode steps: 43, steps per second: 190, episode reward: 27.937, mean reward: 0.650 [0.556, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.449, 10.256], loss: 0.002225, mae: 0.049409, mean_q: 1.291585
 34702/100000: episode: 732, duration: 0.103s, episode steps: 21, steps per second: 205, episode reward: 15.224, mean reward: 0.725 [0.690, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.559], loss: 0.002201, mae: 0.051747, mean_q: 1.285664
 34743/100000: episode: 733, duration: 0.214s, episode steps: 41, steps per second: 191, episode reward: 27.688, mean reward: 0.675 [0.585, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.678, 10.100], loss: 0.002118, mae: 0.050090, mean_q: 1.287418
 34792/100000: episode: 734, duration: 0.242s, episode steps: 49, steps per second: 203, episode reward: 37.129, mean reward: 0.758 [0.684, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.754, 10.100], loss: 0.001778, mae: 0.046183, mean_q: 1.293266
 34810/100000: episode: 735, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 14.581, mean reward: 0.810 [0.699, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.581], loss: 0.002006, mae: 0.048276, mean_q: 1.292723
 34853/100000: episode: 736, duration: 0.221s, episode steps: 43, steps per second: 195, episode reward: 29.016, mean reward: 0.675 [0.507, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.265, 10.244], loss: 0.002084, mae: 0.048441, mean_q: 1.301123
 34871/100000: episode: 737, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 13.903, mean reward: 0.772 [0.728, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.010, 10.447], loss: 0.002048, mae: 0.048642, mean_q: 1.303098
 34892/100000: episode: 738, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 16.534, mean reward: 0.787 [0.656, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.575, 10.450], loss: 0.002181, mae: 0.049396, mean_q: 1.282514
 34933/100000: episode: 739, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 28.600, mean reward: 0.698 [0.572, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.006, 10.100], loss: 0.001922, mae: 0.047204, mean_q: 1.293938
 34954/100000: episode: 740, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 16.768, mean reward: 0.798 [0.702, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.548], loss: 0.001933, mae: 0.047834, mean_q: 1.306512
 34986/100000: episode: 741, duration: 0.165s, episode steps: 32, steps per second: 193, episode reward: 23.122, mean reward: 0.723 [0.674, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.310, 10.100], loss: 0.002229, mae: 0.051185, mean_q: 1.306297
 35035/100000: episode: 742, duration: 0.254s, episode steps: 49, steps per second: 193, episode reward: 35.274, mean reward: 0.720 [0.619, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.895, 10.100], loss: 0.002045, mae: 0.048532, mean_q: 1.304329
 35076/100000: episode: 743, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 28.828, mean reward: 0.703 [0.542, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.285, 10.100], loss: 0.002171, mae: 0.047871, mean_q: 1.301181
 35108/100000: episode: 744, duration: 0.159s, episode steps: 32, steps per second: 202, episode reward: 24.015, mean reward: 0.750 [0.678, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.947, 10.100], loss: 0.001766, mae: 0.046707, mean_q: 1.316334
 35151/100000: episode: 745, duration: 0.214s, episode steps: 43, steps per second: 200, episode reward: 26.233, mean reward: 0.610 [0.536, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.744, 10.164], loss: 0.002003, mae: 0.048201, mean_q: 1.308460
 35192/100000: episode: 746, duration: 0.208s, episode steps: 41, steps per second: 197, episode reward: 31.618, mean reward: 0.771 [0.658, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.696, 10.100], loss: 0.002106, mae: 0.047881, mean_q: 1.305288
 35212/100000: episode: 747, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 13.179, mean reward: 0.659 [0.582, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.272, 10.100], loss: 0.002172, mae: 0.048766, mean_q: 1.303576
 35244/100000: episode: 748, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 26.462, mean reward: 0.827 [0.677, 0.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.471, 10.100], loss: 0.002062, mae: 0.049261, mean_q: 1.321038
 35293/100000: episode: 749, duration: 0.243s, episode steps: 49, steps per second: 202, episode reward: 36.384, mean reward: 0.743 [0.635, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.030, 10.100], loss: 0.001908, mae: 0.047873, mean_q: 1.328895
 35334/100000: episode: 750, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 27.243, mean reward: 0.664 [0.542, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-1.191, 10.100], loss: 0.002040, mae: 0.048140, mean_q: 1.319259
 35354/100000: episode: 751, duration: 0.107s, episode steps: 20, steps per second: 188, episode reward: 14.892, mean reward: 0.745 [0.705, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.203, 10.100], loss: 0.002005, mae: 0.049962, mean_q: 1.317014
 35372/100000: episode: 752, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 13.027, mean reward: 0.724 [0.675, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.492], loss: 0.002089, mae: 0.049318, mean_q: 1.301071
 35421/100000: episode: 753, duration: 0.269s, episode steps: 49, steps per second: 182, episode reward: 32.642, mean reward: 0.666 [0.508, 0.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.459, 10.116], loss: 0.002012, mae: 0.048010, mean_q: 1.322246
 35459/100000: episode: 754, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 27.650, mean reward: 0.728 [0.630, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.844, 10.100], loss: 0.001659, mae: 0.044676, mean_q: 1.320348
 35491/100000: episode: 755, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 25.386, mean reward: 0.793 [0.694, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.722, 10.100], loss: 0.001833, mae: 0.047152, mean_q: 1.328897
 35512/100000: episode: 756, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 18.203, mean reward: 0.867 [0.722, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.703], loss: 0.001805, mae: 0.047719, mean_q: 1.327976
 35527/100000: episode: 757, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 11.174, mean reward: 0.745 [0.642, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.423], loss: 0.001724, mae: 0.046538, mean_q: 1.332393
 35559/100000: episode: 758, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 24.763, mean reward: 0.774 [0.675, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.346, 10.100], loss: 0.001787, mae: 0.044883, mean_q: 1.320062
 35597/100000: episode: 759, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 26.696, mean reward: 0.703 [0.601, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.324, 10.100], loss: 0.002333, mae: 0.048492, mean_q: 1.319153
 35617/100000: episode: 760, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 14.805, mean reward: 0.740 [0.678, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.370, 10.100], loss: 0.001552, mae: 0.043614, mean_q: 1.345345
 35632/100000: episode: 761, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 12.710, mean reward: 0.847 [0.788, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.533], loss: 0.001433, mae: 0.042132, mean_q: 1.323644
 35670/100000: episode: 762, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 23.501, mean reward: 0.618 [0.510, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.035, 10.163], loss: 0.001816, mae: 0.045863, mean_q: 1.328719
 35713/100000: episode: 763, duration: 0.222s, episode steps: 43, steps per second: 193, episode reward: 30.191, mean reward: 0.702 [0.582, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.809, 10.310], loss: 0.001893, mae: 0.047983, mean_q: 1.336222
 35756/100000: episode: 764, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 29.614, mean reward: 0.689 [0.593, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.249, 10.430], loss: 0.001817, mae: 0.046895, mean_q: 1.338186
 35797/100000: episode: 765, duration: 0.233s, episode steps: 41, steps per second: 176, episode reward: 30.585, mean reward: 0.746 [0.586, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.269, 10.100], loss: 0.001814, mae: 0.046680, mean_q: 1.324597
 35829/100000: episode: 766, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 22.271, mean reward: 0.696 [0.542, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.035, 10.147], loss: 0.001670, mae: 0.045000, mean_q: 1.332471
 35878/100000: episode: 767, duration: 0.258s, episode steps: 49, steps per second: 190, episode reward: 36.677, mean reward: 0.749 [0.662, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.178, 10.100], loss: 0.001969, mae: 0.048774, mean_q: 1.339641
 35921/100000: episode: 768, duration: 0.214s, episode steps: 43, steps per second: 201, episode reward: 30.289, mean reward: 0.704 [0.532, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.247, 10.198], loss: 0.001761, mae: 0.046598, mean_q: 1.332420
 35964/100000: episode: 769, duration: 0.227s, episode steps: 43, steps per second: 190, episode reward: 32.209, mean reward: 0.749 [0.676, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.565, 10.458], loss: 0.001708, mae: 0.045997, mean_q: 1.338267
 36013/100000: episode: 770, duration: 0.282s, episode steps: 49, steps per second: 174, episode reward: 35.318, mean reward: 0.721 [0.507, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.089, 10.117], loss: 0.001800, mae: 0.045907, mean_q: 1.332351
 36034/100000: episode: 771, duration: 0.106s, episode steps: 21, steps per second: 197, episode reward: 18.064, mean reward: 0.860 [0.740, 0.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.928, 10.733], loss: 0.001620, mae: 0.044368, mean_q: 1.338711
 36054/100000: episode: 772, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 14.344, mean reward: 0.717 [0.584, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.360, 10.100], loss: 0.001867, mae: 0.047509, mean_q: 1.342183
 36095/100000: episode: 773, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 33.853, mean reward: 0.826 [0.645, 0.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.952, 10.100], loss: 0.001903, mae: 0.047388, mean_q: 1.342977
 36113/100000: episode: 774, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 13.793, mean reward: 0.766 [0.717, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.384, 10.534], loss: 0.002184, mae: 0.050368, mean_q: 1.348966
 36131/100000: episode: 775, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 13.391, mean reward: 0.744 [0.670, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.437], loss: 0.002135, mae: 0.050824, mean_q: 1.354089
 36151/100000: episode: 776, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 15.374, mean reward: 0.769 [0.673, 0.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.435, 10.100], loss: 0.002065, mae: 0.047979, mean_q: 1.346066
 36194/100000: episode: 777, duration: 0.223s, episode steps: 43, steps per second: 193, episode reward: 32.096, mean reward: 0.746 [0.619, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.258, 10.373], loss: 0.001897, mae: 0.046923, mean_q: 1.341073
 36222/100000: episode: 778, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 21.934, mean reward: 0.783 [0.663, 0.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.242, 10.391], loss: 0.002051, mae: 0.048465, mean_q: 1.343847
 36240/100000: episode: 779, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 15.482, mean reward: 0.860 [0.766, 0.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.630], loss: 0.001824, mae: 0.046458, mean_q: 1.341866
 36272/100000: episode: 780, duration: 0.194s, episode steps: 32, steps per second: 165, episode reward: 27.129, mean reward: 0.848 [0.719, 0.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.120, 10.100], loss: 0.001824, mae: 0.047560, mean_q: 1.342902
 36287/100000: episode: 781, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 11.847, mean reward: 0.790 [0.749, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.527], loss: 0.001661, mae: 0.045829, mean_q: 1.349719
 36319/100000: episode: 782, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 23.273, mean reward: 0.727 [0.596, 0.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.030, 10.100], loss: 0.001534, mae: 0.043518, mean_q: 1.345524
 36339/100000: episode: 783, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 14.211, mean reward: 0.711 [0.641, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.334, 10.100], loss: 0.002191, mae: 0.049193, mean_q: 1.359323
 36354/100000: episode: 784, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 12.844, mean reward: 0.856 [0.781, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.882, 10.534], loss: 0.001746, mae: 0.046789, mean_q: 1.362812
 36397/100000: episode: 785, duration: 0.221s, episode steps: 43, steps per second: 195, episode reward: 33.421, mean reward: 0.777 [0.633, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.600, 10.478], loss: 0.001727, mae: 0.045842, mean_q: 1.361168
[Info] FALSIFICATION!
 36410/100000: episode: 786, duration: 0.244s, episode steps: 13, steps per second: 53, episode reward: 11.583, mean reward: 0.891 [0.739, 1.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.297, 10.445], loss: 0.001660, mae: 0.043973, mean_q: 1.365885
 36431/100000: episode: 787, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 17.695, mean reward: 0.843 [0.767, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.570], loss: 0.002137, mae: 0.047697, mean_q: 1.358374
 36472/100000: episode: 788, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 31.010, mean reward: 0.756 [0.674, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-1.053, 10.100], loss: 0.001903, mae: 0.046195, mean_q: 1.359322
 36513/100000: episode: 789, duration: 0.232s, episode steps: 41, steps per second: 177, episode reward: 31.955, mean reward: 0.779 [0.626, 0.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.412, 10.100], loss: 0.002089, mae: 0.050309, mean_q: 1.359561
[Info] Complete ISplit Iteration
[Info] Levels: [1.3643643, 1.5039175, 1.6141533]
[Info] Cond. Prob: [0.1, 0.1, 0.24]
[Info] Error Prob: 0.0024000000000000002

 36528/100000: episode: 790, duration: 4.383s, episode steps: 15, steps per second: 3, episode reward: 11.474, mean reward: 0.765 [0.737, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.548], loss: 0.001661, mae: 0.045318, mean_q: 1.366381
 36628/100000: episode: 791, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 60.151, mean reward: 0.602 [0.503, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.914, 10.098], loss: 0.001729, mae: 0.045862, mean_q: 1.363687
 36728/100000: episode: 792, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 61.473, mean reward: 0.615 [0.509, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.119, 10.412], loss: 0.001816, mae: 0.045728, mean_q: 1.360880
 36828/100000: episode: 793, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 57.750, mean reward: 0.578 [0.507, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.323, 10.098], loss: 0.001896, mae: 0.046449, mean_q: 1.369303
 36928/100000: episode: 794, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 59.489, mean reward: 0.595 [0.501, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.598, 10.098], loss: 0.002118, mae: 0.048867, mean_q: 1.360446
 37028/100000: episode: 795, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 57.937, mean reward: 0.579 [0.512, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.852, 10.098], loss: 0.002131, mae: 0.049389, mean_q: 1.358375
 37128/100000: episode: 796, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.883, mean reward: 0.579 [0.502, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.738, 10.372], loss: 0.001874, mae: 0.045857, mean_q: 1.352950
 37228/100000: episode: 797, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 58.760, mean reward: 0.588 [0.506, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.650, 10.098], loss: 0.001774, mae: 0.045757, mean_q: 1.348893
 37328/100000: episode: 798, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.895, mean reward: 0.589 [0.497, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.841, 10.241], loss: 0.001691, mae: 0.044308, mean_q: 1.344958
 37428/100000: episode: 799, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.387, mean reward: 0.594 [0.501, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.829, 10.098], loss: 0.001850, mae: 0.046447, mean_q: 1.343983
 37528/100000: episode: 800, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 60.487, mean reward: 0.605 [0.503, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.128, 10.340], loss: 0.001578, mae: 0.043995, mean_q: 1.342493
 37628/100000: episode: 801, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 59.426, mean reward: 0.594 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.098], loss: 0.001683, mae: 0.044702, mean_q: 1.344112
 37728/100000: episode: 802, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.536, mean reward: 0.585 [0.499, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.511, 10.302], loss: 0.001888, mae: 0.047227, mean_q: 1.335731
 37828/100000: episode: 803, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 61.578, mean reward: 0.616 [0.506, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.642, 10.404], loss: 0.002130, mae: 0.048385, mean_q: 1.338616
 37928/100000: episode: 804, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.186, mean reward: 0.572 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.073, 10.178], loss: 0.002096, mae: 0.047156, mean_q: 1.338940
 38028/100000: episode: 805, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.752, mean reward: 0.588 [0.513, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.834, 10.528], loss: 0.002020, mae: 0.046204, mean_q: 1.331591
 38128/100000: episode: 806, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.217, mean reward: 0.582 [0.509, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.663, 10.098], loss: 0.001824, mae: 0.046534, mean_q: 1.333198
 38228/100000: episode: 807, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 56.863, mean reward: 0.569 [0.501, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.654, 10.184], loss: 0.001681, mae: 0.043854, mean_q: 1.335072
 38328/100000: episode: 808, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.610, mean reward: 0.596 [0.503, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.708, 10.158], loss: 0.002088, mae: 0.047643, mean_q: 1.322021
 38428/100000: episode: 809, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 56.675, mean reward: 0.567 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.706, 10.128], loss: 0.002185, mae: 0.048742, mean_q: 1.320920
 38528/100000: episode: 810, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.151, mean reward: 0.572 [0.499, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.941, 10.098], loss: 0.001908, mae: 0.046520, mean_q: 1.319961
 38628/100000: episode: 811, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 59.438, mean reward: 0.594 [0.502, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.977, 10.177], loss: 0.001822, mae: 0.045749, mean_q: 1.312300
 38728/100000: episode: 812, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.790, mean reward: 0.588 [0.499, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.812, 10.098], loss: 0.002083, mae: 0.046855, mean_q: 1.316033
 38828/100000: episode: 813, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 57.885, mean reward: 0.579 [0.502, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.098, 10.273], loss: 0.002176, mae: 0.048739, mean_q: 1.304658
 38928/100000: episode: 814, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.800, mean reward: 0.588 [0.507, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.585, 10.409], loss: 0.002089, mae: 0.047511, mean_q: 1.306558
 39028/100000: episode: 815, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 58.971, mean reward: 0.590 [0.505, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.509, 10.098], loss: 0.001892, mae: 0.046339, mean_q: 1.301139
 39128/100000: episode: 816, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 60.780, mean reward: 0.608 [0.503, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.550, 10.098], loss: 0.001727, mae: 0.044496, mean_q: 1.295189
 39228/100000: episode: 817, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.669, mean reward: 0.577 [0.499, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.969, 10.107], loss: 0.001747, mae: 0.044475, mean_q: 1.287128
 39328/100000: episode: 818, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.572, mean reward: 0.596 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.634, 10.098], loss: 0.001899, mae: 0.045365, mean_q: 1.285153
 39428/100000: episode: 819, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 56.881, mean reward: 0.569 [0.506, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.667, 10.155], loss: 0.001916, mae: 0.046038, mean_q: 1.268906
 39528/100000: episode: 820, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 60.790, mean reward: 0.608 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.858, 10.281], loss: 0.001936, mae: 0.046515, mean_q: 1.270792
 39628/100000: episode: 821, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 58.302, mean reward: 0.583 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.472, 10.216], loss: 0.001910, mae: 0.045261, mean_q: 1.272240
 39728/100000: episode: 822, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.224, mean reward: 0.582 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.479, 10.098], loss: 0.001697, mae: 0.043747, mean_q: 1.258641
 39828/100000: episode: 823, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.009, mean reward: 0.570 [0.502, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.820, 10.098], loss: 0.001718, mae: 0.043828, mean_q: 1.255014
 39928/100000: episode: 824, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 63.078, mean reward: 0.631 [0.511, 0.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.257, 10.468], loss: 0.001610, mae: 0.042109, mean_q: 1.256575
 40028/100000: episode: 825, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 58.507, mean reward: 0.585 [0.501, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.973, 10.137], loss: 0.001623, mae: 0.043535, mean_q: 1.246216
 40128/100000: episode: 826, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 59.982, mean reward: 0.600 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.084, 10.098], loss: 0.001723, mae: 0.044493, mean_q: 1.252644
 40228/100000: episode: 827, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.090, mean reward: 0.591 [0.503, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.689, 10.156], loss: 0.001575, mae: 0.042275, mean_q: 1.238832
 40328/100000: episode: 828, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.828, mean reward: 0.578 [0.499, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.178, 10.245], loss: 0.001687, mae: 0.043712, mean_q: 1.232304
 40428/100000: episode: 829, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 61.044, mean reward: 0.610 [0.504, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.339, 10.098], loss: 0.001701, mae: 0.043804, mean_q: 1.228442
 40528/100000: episode: 830, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.029, mean reward: 0.580 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.087, 10.136], loss: 0.001729, mae: 0.044051, mean_q: 1.225340
 40628/100000: episode: 831, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 58.056, mean reward: 0.581 [0.498, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.719, 10.309], loss: 0.001643, mae: 0.042891, mean_q: 1.216883
 40728/100000: episode: 832, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.749, mean reward: 0.587 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.699, 10.412], loss: 0.001809, mae: 0.044870, mean_q: 1.213960
 40828/100000: episode: 833, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 57.386, mean reward: 0.574 [0.497, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.517, 10.264], loss: 0.001674, mae: 0.043833, mean_q: 1.211353
 40928/100000: episode: 834, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.111, mean reward: 0.571 [0.502, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.451, 10.249], loss: 0.001493, mae: 0.041663, mean_q: 1.203063
 41028/100000: episode: 835, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 60.546, mean reward: 0.605 [0.512, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.761, 10.098], loss: 0.001641, mae: 0.044106, mean_q: 1.197465
 41128/100000: episode: 836, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.458, mean reward: 0.585 [0.504, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.030, 10.123], loss: 0.001835, mae: 0.045949, mean_q: 1.191653
 41228/100000: episode: 837, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 60.145, mean reward: 0.601 [0.513, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.690, 10.360], loss: 0.001569, mae: 0.042034, mean_q: 1.180111
 41328/100000: episode: 838, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.893, mean reward: 0.579 [0.497, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.141, 10.142], loss: 0.001501, mae: 0.041962, mean_q: 1.179576
 41428/100000: episode: 839, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.016, mean reward: 0.570 [0.506, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.237, 10.137], loss: 0.001545, mae: 0.042190, mean_q: 1.172695
 41528/100000: episode: 840, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 58.312, mean reward: 0.583 [0.507, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.293, 10.273], loss: 0.001328, mae: 0.039598, mean_q: 1.163831
 41628/100000: episode: 841, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 64.277, mean reward: 0.643 [0.511, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.170, 10.542], loss: 0.001385, mae: 0.040175, mean_q: 1.162401
 41728/100000: episode: 842, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.800, mean reward: 0.588 [0.504, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.848, 10.098], loss: 0.001487, mae: 0.041935, mean_q: 1.165324
 41828/100000: episode: 843, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.389, mean reward: 0.584 [0.508, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.054, 10.260], loss: 0.001374, mae: 0.040017, mean_q: 1.164785
 41928/100000: episode: 844, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 59.848, mean reward: 0.598 [0.502, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.207, 10.177], loss: 0.001321, mae: 0.039521, mean_q: 1.164348
 42028/100000: episode: 845, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 57.492, mean reward: 0.575 [0.502, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.149, 10.098], loss: 0.001413, mae: 0.040495, mean_q: 1.167057
 42128/100000: episode: 846, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 57.587, mean reward: 0.576 [0.499, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.755, 10.276], loss: 0.001334, mae: 0.040119, mean_q: 1.164236
 42228/100000: episode: 847, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 59.339, mean reward: 0.593 [0.499, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.593, 10.235], loss: 0.001293, mae: 0.039040, mean_q: 1.162657
 42328/100000: episode: 848, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.732, mean reward: 0.587 [0.503, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.622, 10.098], loss: 0.001456, mae: 0.041472, mean_q: 1.164168
 42428/100000: episode: 849, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 58.867, mean reward: 0.589 [0.508, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.535, 10.136], loss: 0.001453, mae: 0.041669, mean_q: 1.165094
 42528/100000: episode: 850, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 61.384, mean reward: 0.614 [0.506, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.168, 10.435], loss: 0.001209, mae: 0.038431, mean_q: 1.163934
 42628/100000: episode: 851, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.841, mean reward: 0.588 [0.503, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.899, 10.223], loss: 0.001455, mae: 0.041444, mean_q: 1.165833
 42728/100000: episode: 852, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 57.403, mean reward: 0.574 [0.508, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.900, 10.308], loss: 0.001453, mae: 0.041353, mean_q: 1.166792
 42828/100000: episode: 853, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 61.769, mean reward: 0.618 [0.528, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.775, 10.098], loss: 0.001476, mae: 0.042063, mean_q: 1.169491
 42928/100000: episode: 854, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.710, mean reward: 0.587 [0.499, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.354, 10.203], loss: 0.001355, mae: 0.040296, mean_q: 1.165240
 43028/100000: episode: 855, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.675, mean reward: 0.587 [0.510, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.658, 10.152], loss: 0.001373, mae: 0.040189, mean_q: 1.164362
 43128/100000: episode: 856, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.241, mean reward: 0.592 [0.509, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.511, 10.269], loss: 0.001315, mae: 0.039919, mean_q: 1.165844
 43228/100000: episode: 857, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 60.542, mean reward: 0.605 [0.499, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.323, 10.347], loss: 0.001354, mae: 0.040051, mean_q: 1.166132
 43328/100000: episode: 858, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 57.671, mean reward: 0.577 [0.511, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.849, 10.098], loss: 0.001414, mae: 0.040943, mean_q: 1.166818
 43428/100000: episode: 859, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.545, mean reward: 0.595 [0.510, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.917, 10.098], loss: 0.001427, mae: 0.041201, mean_q: 1.168996
 43528/100000: episode: 860, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 59.908, mean reward: 0.599 [0.506, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.960, 10.098], loss: 0.001354, mae: 0.040618, mean_q: 1.165505
 43628/100000: episode: 861, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.378, mean reward: 0.584 [0.509, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.635, 10.098], loss: 0.001344, mae: 0.040263, mean_q: 1.166242
 43728/100000: episode: 862, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 58.668, mean reward: 0.587 [0.505, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.555, 10.269], loss: 0.001319, mae: 0.040580, mean_q: 1.166024
 43828/100000: episode: 863, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 57.881, mean reward: 0.579 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.729, 10.170], loss: 0.001440, mae: 0.041300, mean_q: 1.166688
 43928/100000: episode: 864, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 62.412, mean reward: 0.624 [0.502, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.037, 10.441], loss: 0.001444, mae: 0.041197, mean_q: 1.171054
 44028/100000: episode: 865, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.626, mean reward: 0.586 [0.504, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.853, 10.413], loss: 0.001368, mae: 0.040751, mean_q: 1.169069
 44128/100000: episode: 866, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 60.213, mean reward: 0.602 [0.503, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.656, 10.098], loss: 0.001315, mae: 0.039741, mean_q: 1.166348
 44228/100000: episode: 867, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 57.220, mean reward: 0.572 [0.501, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.065, 10.098], loss: 0.001507, mae: 0.042107, mean_q: 1.166942
 44328/100000: episode: 868, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.163, mean reward: 0.572 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.460, 10.132], loss: 0.001426, mae: 0.041631, mean_q: 1.168323
 44428/100000: episode: 869, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.651, mean reward: 0.587 [0.511, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.268, 10.114], loss: 0.001352, mae: 0.040514, mean_q: 1.170339
 44528/100000: episode: 870, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.847, mean reward: 0.578 [0.505, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.506, 10.252], loss: 0.001382, mae: 0.040715, mean_q: 1.168504
 44628/100000: episode: 871, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.985, mean reward: 0.590 [0.498, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.877, 10.154], loss: 0.001384, mae: 0.040736, mean_q: 1.168283
 44728/100000: episode: 872, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.369, mean reward: 0.594 [0.509, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.563, 10.295], loss: 0.001435, mae: 0.041430, mean_q: 1.167003
 44828/100000: episode: 873, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 57.741, mean reward: 0.577 [0.501, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.928, 10.098], loss: 0.001554, mae: 0.042898, mean_q: 1.170643
 44928/100000: episode: 874, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 57.358, mean reward: 0.574 [0.502, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.867, 10.132], loss: 0.001427, mae: 0.041393, mean_q: 1.168788
 45028/100000: episode: 875, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 59.962, mean reward: 0.600 [0.516, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.280, 10.210], loss: 0.001424, mae: 0.040910, mean_q: 1.167140
 45128/100000: episode: 876, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.674, mean reward: 0.577 [0.502, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.686, 10.098], loss: 0.001450, mae: 0.041596, mean_q: 1.164020
 45228/100000: episode: 877, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 56.309, mean reward: 0.563 [0.501, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.181, 10.098], loss: 0.001451, mae: 0.041405, mean_q: 1.167753
 45328/100000: episode: 878, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 57.325, mean reward: 0.573 [0.500, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.793, 10.098], loss: 0.001652, mae: 0.043777, mean_q: 1.163960
 45428/100000: episode: 879, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 60.020, mean reward: 0.600 [0.505, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.470, 10.223], loss: 0.001370, mae: 0.040128, mean_q: 1.161573
 45528/100000: episode: 880, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 59.392, mean reward: 0.594 [0.499, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.075, 10.098], loss: 0.001295, mae: 0.038998, mean_q: 1.163838
 45628/100000: episode: 881, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 61.913, mean reward: 0.619 [0.508, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.396, 10.098], loss: 0.001372, mae: 0.040473, mean_q: 1.164531
 45728/100000: episode: 882, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 56.634, mean reward: 0.566 [0.498, 0.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.031, 10.098], loss: 0.001500, mae: 0.042302, mean_q: 1.167318
 45828/100000: episode: 883, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 60.059, mean reward: 0.601 [0.510, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.507, 10.167], loss: 0.001350, mae: 0.040774, mean_q: 1.162692
 45928/100000: episode: 884, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.102, mean reward: 0.581 [0.516, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.103, 10.098], loss: 0.001377, mae: 0.040122, mean_q: 1.165068
 46028/100000: episode: 885, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 57.721, mean reward: 0.577 [0.508, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.443, 10.098], loss: 0.001412, mae: 0.041232, mean_q: 1.165166
 46128/100000: episode: 886, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.371, mean reward: 0.574 [0.501, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.394, 10.132], loss: 0.001415, mae: 0.041204, mean_q: 1.165494
 46228/100000: episode: 887, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.820, mean reward: 0.588 [0.501, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.739, 10.335], loss: 0.001285, mae: 0.039087, mean_q: 1.164008
 46328/100000: episode: 888, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.532, mean reward: 0.585 [0.504, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.717, 10.160], loss: 0.001270, mae: 0.039332, mean_q: 1.161524
 46428/100000: episode: 889, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 62.205, mean reward: 0.622 [0.509, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.519, 10.098], loss: 0.001345, mae: 0.040183, mean_q: 1.163525
[Info] 1-TH LEVEL FOUND: 1.3617639541625977, Considering 10/90 traces
 46528/100000: episode: 890, duration: 4.764s, episode steps: 100, steps per second: 21, episode reward: 58.029, mean reward: 0.580 [0.508, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.134, 10.174], loss: 0.001381, mae: 0.040437, mean_q: 1.166656
 46547/100000: episode: 891, duration: 0.121s, episode steps: 19, steps per second: 158, episode reward: 11.996, mean reward: 0.631 [0.590, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.253, 10.100], loss: 0.001296, mae: 0.038147, mean_q: 1.157940
 46579/100000: episode: 892, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 20.084, mean reward: 0.628 [0.542, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.218, 10.269], loss: 0.001374, mae: 0.039817, mean_q: 1.160150
 46610/100000: episode: 893, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 21.208, mean reward: 0.684 [0.587, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.322, 10.324], loss: 0.001551, mae: 0.042789, mean_q: 1.166027
 46641/100000: episode: 894, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 21.140, mean reward: 0.682 [0.637, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.323, 10.378], loss: 0.001402, mae: 0.041320, mean_q: 1.162204
 46660/100000: episode: 895, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 12.378, mean reward: 0.651 [0.567, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.114, 10.100], loss: 0.001270, mae: 0.039018, mean_q: 1.162835
 46755/100000: episode: 896, duration: 0.490s, episode steps: 95, steps per second: 194, episode reward: 58.209, mean reward: 0.613 [0.510, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.838, 10.434], loss: 0.001471, mae: 0.041442, mean_q: 1.169379
 46850/100000: episode: 897, duration: 0.499s, episode steps: 95, steps per second: 190, episode reward: 58.068, mean reward: 0.611 [0.504, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-1.258, 10.100], loss: 0.001423, mae: 0.041044, mean_q: 1.165989
 46945/100000: episode: 898, duration: 0.499s, episode steps: 95, steps per second: 190, episode reward: 54.768, mean reward: 0.577 [0.509, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.733, 10.323], loss: 0.001633, mae: 0.043808, mean_q: 1.160567
 46964/100000: episode: 899, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 12.137, mean reward: 0.639 [0.580, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.256, 10.100], loss: 0.001648, mae: 0.043150, mean_q: 1.163158
 46998/100000: episode: 900, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 24.513, mean reward: 0.721 [0.619, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.623, 10.484], loss: 0.001483, mae: 0.041103, mean_q: 1.167668
 47093/100000: episode: 901, duration: 0.512s, episode steps: 95, steps per second: 186, episode reward: 54.484, mean reward: 0.574 [0.506, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-0.255, 10.272], loss: 0.001402, mae: 0.040508, mean_q: 1.168894
 47128/100000: episode: 902, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 23.675, mean reward: 0.676 [0.608, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.077, 10.516], loss: 0.001489, mae: 0.041904, mean_q: 1.170738
 47147/100000: episode: 903, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 11.728, mean reward: 0.617 [0.580, 0.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.168, 10.100], loss: 0.001723, mae: 0.042189, mean_q: 1.172337
 47181/100000: episode: 904, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 25.609, mean reward: 0.753 [0.632, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.512, 10.420], loss: 0.001473, mae: 0.042089, mean_q: 1.181038
 47206/100000: episode: 905, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 17.826, mean reward: 0.713 [0.654, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.420, 10.100], loss: 0.001671, mae: 0.042644, mean_q: 1.173009
 47238/100000: episode: 906, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 20.409, mean reward: 0.638 [0.542, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.230, 10.193], loss: 0.001672, mae: 0.043889, mean_q: 1.174599
 47267/100000: episode: 907, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 21.198, mean reward: 0.731 [0.676, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.638, 10.436], loss: 0.001610, mae: 0.043491, mean_q: 1.171343
 47275/100000: episode: 908, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 5.334, mean reward: 0.667 [0.625, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.363, 10.100], loss: 0.001445, mae: 0.040600, mean_q: 1.167316
 47300/100000: episode: 909, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 15.194, mean reward: 0.608 [0.508, 0.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.057, 10.100], loss: 0.001514, mae: 0.040854, mean_q: 1.172996
 47325/100000: episode: 910, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 16.663, mean reward: 0.667 [0.628, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.704, 10.100], loss: 0.001433, mae: 0.041423, mean_q: 1.170461
 47357/100000: episode: 911, duration: 0.154s, episode steps: 32, steps per second: 208, episode reward: 22.905, mean reward: 0.716 [0.652, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.585, 10.509], loss: 0.001282, mae: 0.039340, mean_q: 1.183850
 47386/100000: episode: 912, duration: 0.139s, episode steps: 29, steps per second: 208, episode reward: 20.296, mean reward: 0.700 [0.655, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.554, 10.556], loss: 0.001597, mae: 0.042325, mean_q: 1.181714
 47394/100000: episode: 913, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 5.524, mean reward: 0.691 [0.649, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.431, 10.100], loss: 0.001354, mae: 0.040356, mean_q: 1.176891
 47428/100000: episode: 914, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 25.439, mean reward: 0.748 [0.640, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.061, 10.505], loss: 0.001418, mae: 0.041247, mean_q: 1.188754
 47453/100000: episode: 915, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 15.559, mean reward: 0.622 [0.529, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.197, 10.100], loss: 0.001476, mae: 0.041186, mean_q: 1.177248
 47485/100000: episode: 916, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 21.286, mean reward: 0.665 [0.578, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.204, 10.293], loss: 0.001554, mae: 0.042962, mean_q: 1.178560
 47510/100000: episode: 917, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 17.519, mean reward: 0.701 [0.640, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.141, 10.100], loss: 0.001459, mae: 0.040731, mean_q: 1.182922
 47545/100000: episode: 918, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 24.839, mean reward: 0.710 [0.652, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-1.737, 10.545], loss: 0.001625, mae: 0.042304, mean_q: 1.182145
 47580/100000: episode: 919, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 24.425, mean reward: 0.698 [0.609, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.370, 10.391], loss: 0.001643, mae: 0.042509, mean_q: 1.188993
 47615/100000: episode: 920, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 24.239, mean reward: 0.693 [0.584, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.638, 10.323], loss: 0.001321, mae: 0.040212, mean_q: 1.185234
 47634/100000: episode: 921, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 12.867, mean reward: 0.677 [0.629, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.231, 10.100], loss: 0.001561, mae: 0.040995, mean_q: 1.185213
 47653/100000: episode: 922, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 12.523, mean reward: 0.659 [0.562, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.128, 10.100], loss: 0.001564, mae: 0.041460, mean_q: 1.180890
 47661/100000: episode: 923, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 5.749, mean reward: 0.719 [0.676, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.224, 10.100], loss: 0.001588, mae: 0.041924, mean_q: 1.196333
 47695/100000: episode: 924, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 25.244, mean reward: 0.742 [0.640, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.077, 10.476], loss: 0.001321, mae: 0.039130, mean_q: 1.193637
 47703/100000: episode: 925, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 5.566, mean reward: 0.696 [0.668, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.436, 10.100], loss: 0.001115, mae: 0.037528, mean_q: 1.187397
 47722/100000: episode: 926, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 12.926, mean reward: 0.680 [0.610, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.272, 10.100], loss: 0.001430, mae: 0.040248, mean_q: 1.179251
 47741/100000: episode: 927, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 12.040, mean reward: 0.634 [0.549, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.071, 10.100], loss: 0.001629, mae: 0.041062, mean_q: 1.192284
 47760/100000: episode: 928, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 12.282, mean reward: 0.646 [0.572, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.557, 10.100], loss: 0.001743, mae: 0.040879, mean_q: 1.184423
 47785/100000: episode: 929, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 16.484, mean reward: 0.659 [0.587, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.156, 10.100], loss: 0.001644, mae: 0.043219, mean_q: 1.190462
 47810/100000: episode: 930, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 16.201, mean reward: 0.648 [0.585, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.566, 10.100], loss: 0.001680, mae: 0.043889, mean_q: 1.193192
 47844/100000: episode: 931, duration: 0.179s, episode steps: 34, steps per second: 190, episode reward: 23.808, mean reward: 0.700 [0.639, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.542, 10.362], loss: 0.001743, mae: 0.045170, mean_q: 1.197618
 47869/100000: episode: 932, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 15.980, mean reward: 0.639 [0.536, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.451, 10.100], loss: 0.001430, mae: 0.039478, mean_q: 1.197055
 47898/100000: episode: 933, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 18.501, mean reward: 0.638 [0.511, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.169, 10.285], loss: 0.002052, mae: 0.045167, mean_q: 1.189875
 47917/100000: episode: 934, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 12.969, mean reward: 0.683 [0.605, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.994, 10.100], loss: 0.001252, mae: 0.037405, mean_q: 1.201139
 47925/100000: episode: 935, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 5.325, mean reward: 0.666 [0.631, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.530, 10.100], loss: 0.001232, mae: 0.037447, mean_q: 1.180426
 48020/100000: episode: 936, duration: 0.483s, episode steps: 95, steps per second: 197, episode reward: 54.414, mean reward: 0.573 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.882, 10.172], loss: 0.001549, mae: 0.041657, mean_q: 1.190040
 48045/100000: episode: 937, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 16.590, mean reward: 0.664 [0.608, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.931, 10.100], loss: 0.001677, mae: 0.043048, mean_q: 1.185351
 48077/100000: episode: 938, duration: 0.178s, episode steps: 32, steps per second: 179, episode reward: 19.525, mean reward: 0.610 [0.510, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.099, 10.157], loss: 0.001649, mae: 0.042555, mean_q: 1.185069
 48085/100000: episode: 939, duration: 0.055s, episode steps: 8, steps per second: 146, episode reward: 5.889, mean reward: 0.736 [0.648, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.258, 10.100], loss: 0.001984, mae: 0.046888, mean_q: 1.190661
 48180/100000: episode: 940, duration: 0.504s, episode steps: 95, steps per second: 188, episode reward: 56.094, mean reward: 0.590 [0.505, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.799, 10.100], loss: 0.001550, mae: 0.041615, mean_q: 1.188520
 48199/100000: episode: 941, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 11.728, mean reward: 0.617 [0.562, 0.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.189, 10.100], loss: 0.001480, mae: 0.041264, mean_q: 1.194099
 48294/100000: episode: 942, duration: 0.500s, episode steps: 95, steps per second: 190, episode reward: 57.191, mean reward: 0.602 [0.515, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.839, 10.197], loss: 0.001677, mae: 0.043576, mean_q: 1.190321
 48328/100000: episode: 943, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 23.685, mean reward: 0.697 [0.597, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.478, 10.361], loss: 0.001398, mae: 0.039835, mean_q: 1.196635
 48336/100000: episode: 944, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 5.209, mean reward: 0.651 [0.595, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.163, 10.100], loss: 0.001412, mae: 0.040956, mean_q: 1.192935
 48368/100000: episode: 945, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 22.990, mean reward: 0.718 [0.577, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.033, 10.302], loss: 0.001330, mae: 0.039378, mean_q: 1.194867
 48397/100000: episode: 946, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 21.361, mean reward: 0.737 [0.644, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.368], loss: 0.001318, mae: 0.037757, mean_q: 1.199304
 48431/100000: episode: 947, duration: 0.172s, episode steps: 34, steps per second: 197, episode reward: 22.163, mean reward: 0.652 [0.583, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.927, 10.307], loss: 0.001524, mae: 0.041964, mean_q: 1.195340
 48463/100000: episode: 948, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 20.593, mean reward: 0.644 [0.526, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.797, 10.123], loss: 0.001557, mae: 0.041548, mean_q: 1.192684
 48488/100000: episode: 949, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 16.960, mean reward: 0.678 [0.640, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.178, 10.100], loss: 0.001972, mae: 0.045940, mean_q: 1.202839
 48513/100000: episode: 950, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 15.837, mean reward: 0.633 [0.540, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.422, 10.100], loss: 0.001517, mae: 0.041971, mean_q: 1.194507
 48547/100000: episode: 951, duration: 0.176s, episode steps: 34, steps per second: 194, episode reward: 25.502, mean reward: 0.750 [0.626, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.336, 10.462], loss: 0.002056, mae: 0.046802, mean_q: 1.198418
 48581/100000: episode: 952, duration: 0.171s, episode steps: 34, steps per second: 198, episode reward: 24.618, mean reward: 0.724 [0.607, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.288, 10.464], loss: 0.001572, mae: 0.041479, mean_q: 1.204279
 48615/100000: episode: 953, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 28.208, mean reward: 0.830 [0.675, 0.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.058, 10.518], loss: 0.001353, mae: 0.039553, mean_q: 1.202055
 48634/100000: episode: 954, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 12.921, mean reward: 0.680 [0.608, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.627, 10.100], loss: 0.001531, mae: 0.041949, mean_q: 1.190707
 48666/100000: episode: 955, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 19.754, mean reward: 0.617 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.633, 10.219], loss: 0.001432, mae: 0.040736, mean_q: 1.201085
 48701/100000: episode: 956, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 23.830, mean reward: 0.681 [0.612, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.438, 10.426], loss: 0.001449, mae: 0.040098, mean_q: 1.203532
 48720/100000: episode: 957, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 13.222, mean reward: 0.696 [0.652, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.834, 10.100], loss: 0.001661, mae: 0.044419, mean_q: 1.209146
 48728/100000: episode: 958, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 5.348, mean reward: 0.669 [0.652, 0.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.155, 10.100], loss: 0.001586, mae: 0.041257, mean_q: 1.192495
 48759/100000: episode: 959, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 21.453, mean reward: 0.692 [0.618, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.251, 10.556], loss: 0.001452, mae: 0.040853, mean_q: 1.205860
 48794/100000: episode: 960, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 24.895, mean reward: 0.711 [0.592, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.999, 10.301], loss: 0.001369, mae: 0.039861, mean_q: 1.211259
 48826/100000: episode: 961, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 22.139, mean reward: 0.692 [0.608, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.443], loss: 0.001551, mae: 0.043383, mean_q: 1.213811
 48858/100000: episode: 962, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 21.959, mean reward: 0.686 [0.606, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.366], loss: 0.001682, mae: 0.043876, mean_q: 1.203321
 48892/100000: episode: 963, duration: 0.173s, episode steps: 34, steps per second: 196, episode reward: 21.634, mean reward: 0.636 [0.531, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.199, 10.100], loss: 0.001422, mae: 0.040494, mean_q: 1.218422
 48927/100000: episode: 964, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 23.097, mean reward: 0.660 [0.534, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.126, 10.173], loss: 0.001181, mae: 0.037667, mean_q: 1.213853
 49022/100000: episode: 965, duration: 0.509s, episode steps: 95, steps per second: 187, episode reward: 56.404, mean reward: 0.594 [0.516, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.501 [-0.751, 10.100], loss: 0.001556, mae: 0.042041, mean_q: 1.207512
 49047/100000: episode: 966, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 16.819, mean reward: 0.673 [0.621, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.149, 10.100], loss: 0.001459, mae: 0.041919, mean_q: 1.211368
 49142/100000: episode: 967, duration: 0.500s, episode steps: 95, steps per second: 190, episode reward: 59.698, mean reward: 0.628 [0.500, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-1.532, 10.100], loss: 0.001487, mae: 0.041107, mean_q: 1.216146
 49150/100000: episode: 968, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 5.701, mean reward: 0.713 [0.666, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.296, 10.100], loss: 0.001683, mae: 0.043865, mean_q: 1.203523
 49184/100000: episode: 969, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 23.887, mean reward: 0.703 [0.609, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.601, 10.374], loss: 0.001308, mae: 0.039135, mean_q: 1.219686
 49215/100000: episode: 970, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 21.026, mean reward: 0.678 [0.527, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.130, 10.199], loss: 0.001374, mae: 0.039878, mean_q: 1.219953
 49247/100000: episode: 971, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 21.415, mean reward: 0.669 [0.525, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-1.041, 10.200], loss: 0.001532, mae: 0.041552, mean_q: 1.220160
 49255/100000: episode: 972, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 5.851, mean reward: 0.731 [0.672, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.299, 10.100], loss: 0.001683, mae: 0.043729, mean_q: 1.213127
 49284/100000: episode: 973, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 18.609, mean reward: 0.642 [0.562, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.758, 10.284], loss: 0.001491, mae: 0.041686, mean_q: 1.222637
 49292/100000: episode: 974, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 5.286, mean reward: 0.661 [0.620, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.179, 10.100], loss: 0.001360, mae: 0.038777, mean_q: 1.229579
 49311/100000: episode: 975, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 12.616, mean reward: 0.664 [0.616, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.156, 10.100], loss: 0.001761, mae: 0.044384, mean_q: 1.211208
 49345/100000: episode: 976, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 21.912, mean reward: 0.644 [0.521, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.049, 10.244], loss: 0.001390, mae: 0.039837, mean_q: 1.222435
 49380/100000: episode: 977, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 24.240, mean reward: 0.693 [0.603, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.724, 10.333], loss: 0.001440, mae: 0.041233, mean_q: 1.221287
 49475/100000: episode: 978, duration: 0.480s, episode steps: 95, steps per second: 198, episode reward: 56.774, mean reward: 0.598 [0.507, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-1.136, 10.385], loss: 0.001399, mae: 0.039555, mean_q: 1.222178
 49494/100000: episode: 979, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 12.569, mean reward: 0.662 [0.564, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.064, 10.100], loss: 0.001308, mae: 0.038570, mean_q: 1.227046
[Info] 2-TH LEVEL FOUND: 1.548762321472168, Considering 10/90 traces
 49519/100000: episode: 980, duration: 4.324s, episode steps: 25, steps per second: 6, episode reward: 15.471, mean reward: 0.619 [0.521, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.548, 10.185], loss: 0.001399, mae: 0.040730, mean_q: 1.218467
 49536/100000: episode: 981, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 14.861, mean reward: 0.874 [0.797, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.196, 10.608], loss: 0.001424, mae: 0.041460, mean_q: 1.228184
 49553/100000: episode: 982, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 15.201, mean reward: 0.894 [0.825, 0.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.589], loss: 0.001602, mae: 0.042534, mean_q: 1.226245
 49575/100000: episode: 983, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 15.544, mean reward: 0.707 [0.592, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.728, 10.316], loss: 0.001666, mae: 0.043928, mean_q: 1.234627
 49601/100000: episode: 984, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 19.614, mean reward: 0.754 [0.651, 0.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.871, 10.412], loss: 0.001585, mae: 0.041183, mean_q: 1.231830
 49627/100000: episode: 985, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 19.720, mean reward: 0.758 [0.694, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.544], loss: 0.001432, mae: 0.040187, mean_q: 1.230497
 49637/100000: episode: 986, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 8.017, mean reward: 0.802 [0.748, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.596], loss: 0.001388, mae: 0.040287, mean_q: 1.221671
 49659/100000: episode: 987, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 18.386, mean reward: 0.836 [0.755, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.170, 10.551], loss: 0.001544, mae: 0.042233, mean_q: 1.245703
 49678/100000: episode: 988, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 13.369, mean reward: 0.704 [0.660, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.083, 10.412], loss: 0.001419, mae: 0.040199, mean_q: 1.234091
 49705/100000: episode: 989, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 20.770, mean reward: 0.769 [0.678, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.524], loss: 0.001402, mae: 0.040947, mean_q: 1.240339
 49731/100000: episode: 990, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 20.608, mean reward: 0.793 [0.719, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.495], loss: 0.001571, mae: 0.042584, mean_q: 1.239365
 49757/100000: episode: 991, duration: 0.134s, episode steps: 26, steps per second: 193, episode reward: 19.635, mean reward: 0.755 [0.693, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.611, 10.576], loss: 0.001507, mae: 0.041804, mean_q: 1.255011
 49767/100000: episode: 992, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 7.470, mean reward: 0.747 [0.715, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.968, 10.421], loss: 0.001396, mae: 0.040303, mean_q: 1.249468
 49790/100000: episode: 993, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 17.197, mean reward: 0.748 [0.676, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.388, 10.493], loss: 0.001378, mae: 0.039740, mean_q: 1.239409
 49815/100000: episode: 994, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 18.541, mean reward: 0.742 [0.588, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.069, 10.338], loss: 0.001439, mae: 0.040712, mean_q: 1.248547
 49842/100000: episode: 995, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 18.781, mean reward: 0.696 [0.638, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.364], loss: 0.001506, mae: 0.041142, mean_q: 1.249559
 49864/100000: episode: 996, duration: 0.139s, episode steps: 22, steps per second: 158, episode reward: 17.432, mean reward: 0.792 [0.748, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.392, 10.552], loss: 0.001618, mae: 0.042655, mean_q: 1.240693
 49890/100000: episode: 997, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 19.047, mean reward: 0.733 [0.614, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.287, 10.337], loss: 0.001602, mae: 0.043166, mean_q: 1.252153
 49907/100000: episode: 998, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 14.630, mean reward: 0.861 [0.809, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.547], loss: 0.001964, mae: 0.048682, mean_q: 1.252881
 49932/100000: episode: 999, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 18.405, mean reward: 0.736 [0.659, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.145, 10.270], loss: 0.001791, mae: 0.046170, mean_q: 1.257047
 49959/100000: episode: 1000, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 18.610, mean reward: 0.689 [0.608, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.360], loss: 0.001626, mae: 0.043253, mean_q: 1.260426
 49989/100000: episode: 1001, duration: 0.149s, episode steps: 30, steps per second: 202, episode reward: 22.636, mean reward: 0.755 [0.639, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.354], loss: 0.001629, mae: 0.043005, mean_q: 1.246017
 49999/100000: episode: 1002, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 7.651, mean reward: 0.765 [0.717, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.608, 10.548], loss: 0.001401, mae: 0.040021, mean_q: 1.262050
 50027/100000: episode: 1003, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 21.939, mean reward: 0.784 [0.670, 0.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.362, 10.432], loss: 0.001480, mae: 0.040364, mean_q: 1.249484
[Info] FALSIFICATION!
 50051/100000: episode: 1004, duration: 0.343s, episode steps: 24, steps per second: 70, episode reward: 20.660, mean reward: 0.861 [0.749, 1.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.226, 10.394], loss: 0.001311, mae: 0.038686, mean_q: 1.252339
 50079/100000: episode: 1005, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 22.289, mean reward: 0.796 [0.703, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.583], loss: 0.001497, mae: 0.040722, mean_q: 1.263712
 50102/100000: episode: 1006, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 19.757, mean reward: 0.859 [0.752, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.535], loss: 0.001449, mae: 0.040054, mean_q: 1.257999
 50124/100000: episode: 1007, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 18.208, mean reward: 0.828 [0.698, 0.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.195, 10.529], loss: 0.001409, mae: 0.039658, mean_q: 1.271816
 50151/100000: episode: 1008, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 18.963, mean reward: 0.702 [0.585, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.035, 10.372], loss: 0.001463, mae: 0.041814, mean_q: 1.264422
 50170/100000: episode: 1009, duration: 0.122s, episode steps: 19, steps per second: 155, episode reward: 14.977, mean reward: 0.788 [0.658, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.653, 10.638], loss: 0.002168, mae: 0.043935, mean_q: 1.248467
 50192/100000: episode: 1010, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 18.162, mean reward: 0.826 [0.729, 0.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.917, 10.465], loss: 0.001462, mae: 0.041070, mean_q: 1.276279
 50211/100000: episode: 1011, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 13.696, mean reward: 0.721 [0.615, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.324, 10.308], loss: 0.001451, mae: 0.039992, mean_q: 1.262674
 50234/100000: episode: 1012, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 17.095, mean reward: 0.743 [0.662, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.081, 10.388], loss: 0.001610, mae: 0.042442, mean_q: 1.261161
 50262/100000: episode: 1013, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 21.862, mean reward: 0.781 [0.641, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-1.008, 10.367], loss: 0.001402, mae: 0.040947, mean_q: 1.271313
 50279/100000: episode: 1014, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 12.508, mean reward: 0.736 [0.689, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.662, 10.543], loss: 0.002294, mae: 0.045924, mean_q: 1.279841
 50307/100000: episode: 1015, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 22.299, mean reward: 0.796 [0.695, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.849, 10.505], loss: 0.001763, mae: 0.044952, mean_q: 1.278521
 50317/100000: episode: 1016, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 7.675, mean reward: 0.768 [0.700, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.062, 10.456], loss: 0.002442, mae: 0.048671, mean_q: 1.288842
 50339/100000: episode: 1017, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 15.563, mean reward: 0.707 [0.557, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.276, 10.332], loss: 0.001944, mae: 0.046993, mean_q: 1.266288
 50367/100000: episode: 1018, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 22.740, mean reward: 0.812 [0.717, 0.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.554], loss: 0.001431, mae: 0.041532, mean_q: 1.268400
 50377/100000: episode: 1019, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 7.685, mean reward: 0.768 [0.714, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.097, 10.365], loss: 0.001620, mae: 0.044236, mean_q: 1.288284
 50403/100000: episode: 1020, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 21.219, mean reward: 0.816 [0.722, 0.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.599, 10.604], loss: 0.001615, mae: 0.042823, mean_q: 1.273397
 50429/100000: episode: 1021, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 20.625, mean reward: 0.793 [0.709, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.504], loss: 0.001335, mae: 0.040006, mean_q: 1.293491
 50456/100000: episode: 1022, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 20.757, mean reward: 0.769 [0.679, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.804, 10.473], loss: 0.001854, mae: 0.042757, mean_q: 1.279786
 50479/100000: episode: 1023, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 19.597, mean reward: 0.852 [0.772, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.288, 10.621], loss: 0.001559, mae: 0.040785, mean_q: 1.281764
 50489/100000: episode: 1024, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 7.350, mean reward: 0.735 [0.703, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.479], loss: 0.002060, mae: 0.040229, mean_q: 1.278405
 50519/100000: episode: 1025, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 25.178, mean reward: 0.839 [0.712, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.430, 10.482], loss: 0.001457, mae: 0.041282, mean_q: 1.284984
 50544/100000: episode: 1026, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 17.205, mean reward: 0.688 [0.644, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.311, 10.401], loss: 0.001683, mae: 0.042685, mean_q: 1.282030
 50574/100000: episode: 1027, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 23.210, mean reward: 0.774 [0.724, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.088, 10.584], loss: 0.001656, mae: 0.043495, mean_q: 1.288021
 50596/100000: episode: 1028, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 16.766, mean reward: 0.762 [0.704, 0.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.509, 10.429], loss: 0.001462, mae: 0.042291, mean_q: 1.285960
 50623/100000: episode: 1029, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 22.285, mean reward: 0.825 [0.754, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.315, 10.640], loss: 0.001340, mae: 0.040007, mean_q: 1.300255
 50633/100000: episode: 1030, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 7.457, mean reward: 0.746 [0.721, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.061, 10.526], loss: 0.001658, mae: 0.042772, mean_q: 1.303232
 50650/100000: episode: 1031, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 14.207, mean reward: 0.836 [0.788, 0.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.436, 10.583], loss: 0.001447, mae: 0.041214, mean_q: 1.318034
 50675/100000: episode: 1032, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 18.650, mean reward: 0.746 [0.635, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.728, 10.416], loss: 0.001434, mae: 0.042160, mean_q: 1.303111
 50703/100000: episode: 1033, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 21.642, mean reward: 0.773 [0.701, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.652, 10.485], loss: 0.001699, mae: 0.041600, mean_q: 1.303576
 50720/100000: episode: 1034, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 13.935, mean reward: 0.820 [0.737, 0.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-1.295, 10.568], loss: 0.001391, mae: 0.041143, mean_q: 1.296786
 50748/100000: episode: 1035, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 21.063, mean reward: 0.752 [0.675, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.068, 10.510], loss: 0.001350, mae: 0.039412, mean_q: 1.304982
 50765/100000: episode: 1036, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 12.975, mean reward: 0.763 [0.658, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.533, 10.592], loss: 0.001659, mae: 0.042688, mean_q: 1.308152
 50782/100000: episode: 1037, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 15.345, mean reward: 0.903 [0.808, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.637, 10.628], loss: 0.001580, mae: 0.043800, mean_q: 1.303224
 50807/100000: episode: 1038, duration: 0.125s, episode steps: 25, steps per second: 199, episode reward: 18.731, mean reward: 0.749 [0.676, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.957, 10.519], loss: 0.001273, mae: 0.039014, mean_q: 1.308328
 50826/100000: episode: 1039, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 13.368, mean reward: 0.704 [0.614, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.347], loss: 0.001440, mae: 0.041066, mean_q: 1.311339
 50853/100000: episode: 1040, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 21.568, mean reward: 0.799 [0.733, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.384, 10.460], loss: 0.001607, mae: 0.043183, mean_q: 1.312689
 50870/100000: episode: 1041, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 15.272, mean reward: 0.898 [0.843, 0.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.406, 10.542], loss: 0.001714, mae: 0.044801, mean_q: 1.317775
 50898/100000: episode: 1042, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 21.361, mean reward: 0.763 [0.572, 0.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.328], loss: 0.001574, mae: 0.043664, mean_q: 1.314752
 50924/100000: episode: 1043, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 20.531, mean reward: 0.790 [0.746, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.081, 10.512], loss: 0.001448, mae: 0.041022, mean_q: 1.310590
 50950/100000: episode: 1044, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 22.761, mean reward: 0.875 [0.777, 0.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.203, 10.664], loss: 0.001359, mae: 0.040243, mean_q: 1.341709
 50960/100000: episode: 1045, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 7.453, mean reward: 0.745 [0.661, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-1.087, 10.495], loss: 0.003365, mae: 0.047395, mean_q: 1.303845
 50979/100000: episode: 1046, duration: 0.095s, episode steps: 19, steps per second: 201, episode reward: 14.221, mean reward: 0.748 [0.645, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.109, 10.378], loss: 0.001467, mae: 0.042111, mean_q: 1.311729
 51006/100000: episode: 1047, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 21.044, mean reward: 0.779 [0.583, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.378], loss: 0.001473, mae: 0.042166, mean_q: 1.337000
 51032/100000: episode: 1048, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 21.559, mean reward: 0.829 [0.739, 0.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.820, 10.360], loss: 0.001599, mae: 0.043351, mean_q: 1.331466
 51057/100000: episode: 1049, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 19.550, mean reward: 0.782 [0.692, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.847, 10.472], loss: 0.002063, mae: 0.044666, mean_q: 1.325995
 51076/100000: episode: 1050, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 13.358, mean reward: 0.703 [0.630, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.050, 10.403], loss: 0.001855, mae: 0.045901, mean_q: 1.337152
 51102/100000: episode: 1051, duration: 0.143s, episode steps: 26, steps per second: 181, episode reward: 18.597, mean reward: 0.715 [0.604, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.334, 10.333], loss: 0.002214, mae: 0.044569, mean_q: 1.320037
 51130/100000: episode: 1052, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 20.985, mean reward: 0.749 [0.593, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.053, 10.244], loss: 0.001656, mae: 0.044069, mean_q: 1.331654
 51155/100000: episode: 1053, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 19.835, mean reward: 0.793 [0.714, 0.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.734, 10.494], loss: 0.001605, mae: 0.043216, mean_q: 1.334613
 51183/100000: episode: 1054, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 19.387, mean reward: 0.692 [0.565, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.425, 10.205], loss: 0.001511, mae: 0.042407, mean_q: 1.333353
 51206/100000: episode: 1055, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 17.281, mean reward: 0.751 [0.666, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.455, 10.472], loss: 0.001450, mae: 0.041404, mean_q: 1.332083
 51225/100000: episode: 1056, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 13.437, mean reward: 0.707 [0.632, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.614, 10.390], loss: 0.001331, mae: 0.039575, mean_q: 1.325324
 51248/100000: episode: 1057, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 18.957, mean reward: 0.824 [0.768, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.428, 10.581], loss: 0.001289, mae: 0.038830, mean_q: 1.344433
 51267/100000: episode: 1058, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 13.294, mean reward: 0.700 [0.583, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.334], loss: 0.001387, mae: 0.041989, mean_q: 1.338017
 51297/100000: episode: 1059, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 20.557, mean reward: 0.685 [0.586, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.712, 10.313], loss: 0.001248, mae: 0.039115, mean_q: 1.341069
 51307/100000: episode: 1060, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 7.103, mean reward: 0.710 [0.692, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.182, 10.445], loss: 0.001329, mae: 0.040053, mean_q: 1.353775
 51335/100000: episode: 1061, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 22.680, mean reward: 0.810 [0.666, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.480], loss: 0.001761, mae: 0.042277, mean_q: 1.348639
 51354/100000: episode: 1062, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 14.891, mean reward: 0.784 [0.717, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.599, 10.441], loss: 0.001229, mae: 0.037910, mean_q: 1.334424
 51379/100000: episode: 1063, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 19.631, mean reward: 0.785 [0.720, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.488], loss: 0.001483, mae: 0.042199, mean_q: 1.335458
 51396/100000: episode: 1064, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 13.384, mean reward: 0.787 [0.725, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.460], loss: 0.001822, mae: 0.041576, mean_q: 1.340282
 51426/100000: episode: 1065, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 24.351, mean reward: 0.812 [0.749, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.352, 10.529], loss: 0.001800, mae: 0.042583, mean_q: 1.362048
 51451/100000: episode: 1066, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 18.085, mean reward: 0.723 [0.598, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.381], loss: 0.001374, mae: 0.040560, mean_q: 1.363140
 51468/100000: episode: 1067, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 13.717, mean reward: 0.807 [0.705, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.202, 10.495], loss: 0.001491, mae: 0.042678, mean_q: 1.347531
[Info] FALSIFICATION!
 51478/100000: episode: 1068, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 8.213, mean reward: 0.821 [0.699, 1.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.604, 10.319], loss: 0.001067, mae: 0.035231, mean_q: 1.374408
 51500/100000: episode: 1069, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 16.790, mean reward: 0.763 [0.711, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.817, 10.616], loss: 0.001180, mae: 0.038672, mean_q: 1.356514
[Info] Complete ISplit Iteration
[Info] Levels: [1.361764, 1.5487623, 1.5729822]
[Info] Cond. Prob: [0.1, 0.1, 0.96]
[Info] Error Prob: 0.009600000000000001

 51526/100000: episode: 1070, duration: 4.498s, episode steps: 26, steps per second: 6, episode reward: 18.497, mean reward: 0.711 [0.613, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.048, 10.381], loss: 0.001341, mae: 0.040693, mean_q: 1.361451
 51626/100000: episode: 1071, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.490, mean reward: 0.575 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.741, 10.098], loss: 0.001381, mae: 0.039249, mean_q: 1.352269
 51726/100000: episode: 1072, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 59.339, mean reward: 0.593 [0.514, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.303, 10.098], loss: 0.001375, mae: 0.040368, mean_q: 1.354035
 51826/100000: episode: 1073, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.157, mean reward: 0.592 [0.512, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.847, 10.141], loss: 0.001454, mae: 0.041367, mean_q: 1.349677
 51926/100000: episode: 1074, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.637, mean reward: 0.596 [0.500, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.284, 10.098], loss: 0.001471, mae: 0.041835, mean_q: 1.359964
 52026/100000: episode: 1075, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 61.572, mean reward: 0.616 [0.526, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.479, 10.165], loss: 0.001497, mae: 0.042321, mean_q: 1.351753
 52126/100000: episode: 1076, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.741, mean reward: 0.577 [0.509, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.670, 10.150], loss: 0.001821, mae: 0.044113, mean_q: 1.345415
 52226/100000: episode: 1077, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.025, mean reward: 0.580 [0.511, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.019, 10.298], loss: 0.001471, mae: 0.040920, mean_q: 1.344770
 52326/100000: episode: 1078, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.841, mean reward: 0.578 [0.508, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.434, 10.098], loss: 0.001619, mae: 0.041827, mean_q: 1.342896
 52426/100000: episode: 1079, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.877, mean reward: 0.589 [0.499, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.616, 10.143], loss: 0.001742, mae: 0.044386, mean_q: 1.332694
 52526/100000: episode: 1080, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 59.446, mean reward: 0.594 [0.508, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.783, 10.116], loss: 0.001718, mae: 0.043230, mean_q: 1.338072
 52626/100000: episode: 1081, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.447, mean reward: 0.604 [0.511, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.769, 10.098], loss: 0.001653, mae: 0.043691, mean_q: 1.321446
 52726/100000: episode: 1082, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.867, mean reward: 0.579 [0.510, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.407, 10.098], loss: 0.001672, mae: 0.040954, mean_q: 1.329441
 52826/100000: episode: 1083, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 61.049, mean reward: 0.610 [0.508, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.920, 10.174], loss: 0.001649, mae: 0.041347, mean_q: 1.324825
 52926/100000: episode: 1084, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 61.660, mean reward: 0.617 [0.507, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.090, 10.098], loss: 0.001810, mae: 0.044388, mean_q: 1.319434
 53026/100000: episode: 1085, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.544, mean reward: 0.585 [0.507, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.467, 10.098], loss: 0.001522, mae: 0.042106, mean_q: 1.329454
 53126/100000: episode: 1086, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.281, mean reward: 0.583 [0.511, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.805, 10.098], loss: 0.001541, mae: 0.042344, mean_q: 1.324167
 53226/100000: episode: 1087, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.426, mean reward: 0.584 [0.511, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.020, 10.144], loss: 0.001583, mae: 0.041864, mean_q: 1.323291
 53326/100000: episode: 1088, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 57.797, mean reward: 0.578 [0.501, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.239, 10.315], loss: 0.001579, mae: 0.042675, mean_q: 1.326952
 53426/100000: episode: 1089, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.867, mean reward: 0.579 [0.512, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.562, 10.206], loss: 0.001623, mae: 0.043103, mean_q: 1.315332
 53526/100000: episode: 1090, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 57.686, mean reward: 0.577 [0.502, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.580, 10.179], loss: 0.001620, mae: 0.042816, mean_q: 1.320355
 53626/100000: episode: 1091, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.165, mean reward: 0.582 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.657, 10.138], loss: 0.001608, mae: 0.043159, mean_q: 1.308118
 53726/100000: episode: 1092, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.204, mean reward: 0.602 [0.513, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.788, 10.098], loss: 0.001789, mae: 0.044135, mean_q: 1.310402
 53826/100000: episode: 1093, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 59.039, mean reward: 0.590 [0.504, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.448, 10.270], loss: 0.001586, mae: 0.042724, mean_q: 1.311108
 53926/100000: episode: 1094, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.991, mean reward: 0.580 [0.513, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.056, 10.137], loss: 0.001628, mae: 0.043417, mean_q: 1.305329
 54026/100000: episode: 1095, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.066, mean reward: 0.591 [0.509, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.435, 10.179], loss: 0.001794, mae: 0.043355, mean_q: 1.297366
 54126/100000: episode: 1096, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 58.708, mean reward: 0.587 [0.504, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.080, 10.391], loss: 0.001545, mae: 0.041767, mean_q: 1.289970
 54226/100000: episode: 1097, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.258, mean reward: 0.573 [0.509, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.677, 10.220], loss: 0.001658, mae: 0.043438, mean_q: 1.295629
 54326/100000: episode: 1098, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 61.964, mean reward: 0.620 [0.511, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.724, 10.098], loss: 0.001725, mae: 0.043503, mean_q: 1.291248
 54426/100000: episode: 1099, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 57.550, mean reward: 0.576 [0.510, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.953, 10.098], loss: 0.001802, mae: 0.043272, mean_q: 1.290696
 54526/100000: episode: 1100, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 60.751, mean reward: 0.608 [0.514, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.993, 10.245], loss: 0.001483, mae: 0.041693, mean_q: 1.288696
 54626/100000: episode: 1101, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 58.703, mean reward: 0.587 [0.500, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.156, 10.337], loss: 0.001582, mae: 0.041810, mean_q: 1.281762
 54726/100000: episode: 1102, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 57.860, mean reward: 0.579 [0.504, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.715, 10.098], loss: 0.001705, mae: 0.043379, mean_q: 1.275398
 54826/100000: episode: 1103, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.394, mean reward: 0.574 [0.506, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.823, 10.154], loss: 0.001753, mae: 0.043636, mean_q: 1.270468
 54926/100000: episode: 1104, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.563, mean reward: 0.586 [0.503, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.081, 10.166], loss: 0.001705, mae: 0.042312, mean_q: 1.263102
 55026/100000: episode: 1105, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 57.488, mean reward: 0.575 [0.503, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.709, 10.098], loss: 0.001696, mae: 0.043806, mean_q: 1.254607
 55126/100000: episode: 1106, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 57.529, mean reward: 0.575 [0.510, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.791, 10.153], loss: 0.001613, mae: 0.043677, mean_q: 1.245015
 55226/100000: episode: 1107, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 57.245, mean reward: 0.572 [0.503, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.169, 10.098], loss: 0.001514, mae: 0.041065, mean_q: 1.245222
 55326/100000: episode: 1108, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 57.247, mean reward: 0.572 [0.499, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.794, 10.098], loss: 0.001580, mae: 0.042623, mean_q: 1.233753
 55426/100000: episode: 1109, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 60.011, mean reward: 0.600 [0.501, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.087, 10.247], loss: 0.001523, mae: 0.041513, mean_q: 1.231341
 55526/100000: episode: 1110, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.196, mean reward: 0.582 [0.504, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.637, 10.098], loss: 0.001556, mae: 0.042443, mean_q: 1.222381
 55626/100000: episode: 1111, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 57.542, mean reward: 0.575 [0.500, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.812, 10.180], loss: 0.001509, mae: 0.041948, mean_q: 1.218231
 55726/100000: episode: 1112, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.315, mean reward: 0.583 [0.499, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.517, 10.126], loss: 0.001508, mae: 0.042213, mean_q: 1.208910
 55826/100000: episode: 1113, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 60.015, mean reward: 0.600 [0.524, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.524, 10.098], loss: 0.001515, mae: 0.041792, mean_q: 1.202878
 55926/100000: episode: 1114, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 56.005, mean reward: 0.560 [0.501, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.274, 10.180], loss: 0.001527, mae: 0.042186, mean_q: 1.195166
 56026/100000: episode: 1115, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.705, mean reward: 0.597 [0.499, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.639, 10.098], loss: 0.001454, mae: 0.041293, mean_q: 1.191364
 56126/100000: episode: 1116, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.041, mean reward: 0.590 [0.500, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.955, 10.119], loss: 0.001542, mae: 0.042289, mean_q: 1.185333
 56226/100000: episode: 1117, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.900, mean reward: 0.599 [0.498, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.670, 10.098], loss: 0.001464, mae: 0.042006, mean_q: 1.176873
 56326/100000: episode: 1118, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.485, mean reward: 0.595 [0.508, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.625, 10.098], loss: 0.001498, mae: 0.041794, mean_q: 1.169770
 56426/100000: episode: 1119, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.254, mean reward: 0.583 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.638, 10.098], loss: 0.001436, mae: 0.041443, mean_q: 1.165150
 56526/100000: episode: 1120, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.664, mean reward: 0.597 [0.502, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.908, 10.167], loss: 0.001365, mae: 0.040019, mean_q: 1.163120
 56626/100000: episode: 1121, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.853, mean reward: 0.599 [0.512, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.294, 10.232], loss: 0.001301, mae: 0.039743, mean_q: 1.161590
 56726/100000: episode: 1122, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 60.459, mean reward: 0.605 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.738, 10.262], loss: 0.001443, mae: 0.040987, mean_q: 1.163477
 56826/100000: episode: 1123, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 57.340, mean reward: 0.573 [0.498, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.346, 10.098], loss: 0.001369, mae: 0.040863, mean_q: 1.164582
 56926/100000: episode: 1124, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.564, mean reward: 0.586 [0.500, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.665, 10.098], loss: 0.001328, mae: 0.039972, mean_q: 1.162896
 57026/100000: episode: 1125, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 59.751, mean reward: 0.598 [0.503, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.619, 10.098], loss: 0.001283, mae: 0.039202, mean_q: 1.164467
 57126/100000: episode: 1126, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 59.642, mean reward: 0.596 [0.505, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.299, 10.372], loss: 0.001357, mae: 0.040362, mean_q: 1.162580
 57226/100000: episode: 1127, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.329, mean reward: 0.603 [0.504, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.888, 10.098], loss: 0.001398, mae: 0.041017, mean_q: 1.164623
 57326/100000: episode: 1128, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 60.384, mean reward: 0.604 [0.504, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.988, 10.098], loss: 0.001342, mae: 0.040120, mean_q: 1.165673
 57426/100000: episode: 1129, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 57.798, mean reward: 0.578 [0.513, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.475, 10.297], loss: 0.001320, mae: 0.039968, mean_q: 1.161748
 57526/100000: episode: 1130, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.902, mean reward: 0.589 [0.500, 0.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.372, 10.315], loss: 0.001359, mae: 0.040761, mean_q: 1.164309
 57626/100000: episode: 1131, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 60.947, mean reward: 0.609 [0.509, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.771, 10.414], loss: 0.001326, mae: 0.039987, mean_q: 1.165390
 57726/100000: episode: 1132, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 60.587, mean reward: 0.606 [0.512, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.971, 10.384], loss: 0.001380, mae: 0.041095, mean_q: 1.163971
 57826/100000: episode: 1133, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 58.189, mean reward: 0.582 [0.497, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.159, 10.170], loss: 0.001374, mae: 0.041143, mean_q: 1.164223
 57926/100000: episode: 1134, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.254, mean reward: 0.583 [0.500, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.784, 10.178], loss: 0.001331, mae: 0.040146, mean_q: 1.163720
 58026/100000: episode: 1135, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 58.533, mean reward: 0.585 [0.500, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.021, 10.098], loss: 0.001246, mae: 0.039254, mean_q: 1.161175
 58126/100000: episode: 1136, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.461, mean reward: 0.585 [0.506, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.402, 10.204], loss: 0.001254, mae: 0.039115, mean_q: 1.164385
 58226/100000: episode: 1137, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.888, mean reward: 0.599 [0.517, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.799, 10.227], loss: 0.001348, mae: 0.040560, mean_q: 1.162426
 58326/100000: episode: 1138, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.154, mean reward: 0.572 [0.504, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.031, 10.098], loss: 0.001239, mae: 0.039237, mean_q: 1.162806
 58426/100000: episode: 1139, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 57.128, mean reward: 0.571 [0.504, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.000, 10.179], loss: 0.001255, mae: 0.039123, mean_q: 1.163045
 58526/100000: episode: 1140, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 59.083, mean reward: 0.591 [0.504, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.451, 10.296], loss: 0.001304, mae: 0.039982, mean_q: 1.163846
 58626/100000: episode: 1141, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.343, mean reward: 0.583 [0.502, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.543, 10.098], loss: 0.001394, mae: 0.041002, mean_q: 1.163327
 58726/100000: episode: 1142, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.472, mean reward: 0.605 [0.500, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.404, 10.098], loss: 0.001382, mae: 0.041180, mean_q: 1.162473
 58826/100000: episode: 1143, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.270, mean reward: 0.593 [0.506, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.218, 10.098], loss: 0.001398, mae: 0.041802, mean_q: 1.164159
 58926/100000: episode: 1144, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 58.449, mean reward: 0.584 [0.510, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.415, 10.119], loss: 0.001349, mae: 0.040735, mean_q: 1.161020
 59026/100000: episode: 1145, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.721, mean reward: 0.607 [0.503, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.449, 10.150], loss: 0.001292, mae: 0.039952, mean_q: 1.165067
 59126/100000: episode: 1146, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.650, mean reward: 0.577 [0.504, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.853, 10.160], loss: 0.001457, mae: 0.042112, mean_q: 1.164202
 59226/100000: episode: 1147, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.370, mean reward: 0.604 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.963, 10.346], loss: 0.001416, mae: 0.041455, mean_q: 1.161837
 59326/100000: episode: 1148, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.787, mean reward: 0.588 [0.498, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.807, 10.098], loss: 0.001534, mae: 0.042866, mean_q: 1.163134
 59426/100000: episode: 1149, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 59.213, mean reward: 0.592 [0.507, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.793, 10.112], loss: 0.001363, mae: 0.041024, mean_q: 1.165335
 59526/100000: episode: 1150, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 59.723, mean reward: 0.597 [0.505, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.041, 10.184], loss: 0.001435, mae: 0.042343, mean_q: 1.165139
 59626/100000: episode: 1151, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.205, mean reward: 0.582 [0.509, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.982, 10.155], loss: 0.001265, mae: 0.039415, mean_q: 1.164937
 59726/100000: episode: 1152, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.922, mean reward: 0.589 [0.507, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.108, 10.124], loss: 0.001365, mae: 0.040595, mean_q: 1.161812
 59826/100000: episode: 1153, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.024, mean reward: 0.590 [0.506, 0.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.802, 10.098], loss: 0.001550, mae: 0.043880, mean_q: 1.164773
 59926/100000: episode: 1154, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 57.011, mean reward: 0.570 [0.500, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.438, 10.098], loss: 0.001446, mae: 0.041600, mean_q: 1.165832
 60026/100000: episode: 1155, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 57.433, mean reward: 0.574 [0.502, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.926, 10.098], loss: 0.001420, mae: 0.041289, mean_q: 1.162557
 60126/100000: episode: 1156, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.696, mean reward: 0.587 [0.511, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.352, 10.098], loss: 0.001453, mae: 0.042190, mean_q: 1.162801
 60226/100000: episode: 1157, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 60.069, mean reward: 0.601 [0.502, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.734, 10.236], loss: 0.001402, mae: 0.041637, mean_q: 1.166816
 60326/100000: episode: 1158, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.982, mean reward: 0.590 [0.508, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.776, 10.098], loss: 0.001338, mae: 0.040444, mean_q: 1.167328
 60426/100000: episode: 1159, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.552, mean reward: 0.586 [0.503, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.797, 10.098], loss: 0.001392, mae: 0.041542, mean_q: 1.168783
 60526/100000: episode: 1160, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 60.570, mean reward: 0.606 [0.501, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.378, 10.098], loss: 0.001375, mae: 0.041088, mean_q: 1.169595
 60626/100000: episode: 1161, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.002, mean reward: 0.600 [0.502, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.481, 10.098], loss: 0.001444, mae: 0.041729, mean_q: 1.168566
 60726/100000: episode: 1162, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 59.440, mean reward: 0.594 [0.512, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.846, 10.104], loss: 0.001485, mae: 0.042167, mean_q: 1.167387
 60826/100000: episode: 1163, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.501, mean reward: 0.585 [0.511, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.956, 10.149], loss: 0.001390, mae: 0.041364, mean_q: 1.168440
 60926/100000: episode: 1164, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 57.417, mean reward: 0.574 [0.508, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.044, 10.152], loss: 0.001364, mae: 0.041200, mean_q: 1.166812
 61026/100000: episode: 1165, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 57.370, mean reward: 0.574 [0.501, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.919, 10.145], loss: 0.001447, mae: 0.041810, mean_q: 1.167753
 61126/100000: episode: 1166, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 58.110, mean reward: 0.581 [0.502, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.046, 10.250], loss: 0.001413, mae: 0.041334, mean_q: 1.167918
 61226/100000: episode: 1167, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 59.544, mean reward: 0.595 [0.506, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.505, 10.098], loss: 0.001470, mae: 0.042059, mean_q: 1.165960
 61326/100000: episode: 1168, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.751, mean reward: 0.588 [0.507, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.881, 10.098], loss: 0.001409, mae: 0.040941, mean_q: 1.165837
 61426/100000: episode: 1169, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 58.452, mean reward: 0.585 [0.510, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.437, 10.098], loss: 0.001450, mae: 0.041925, mean_q: 1.165774
[Info] 1-TH LEVEL FOUND: 1.3113863468170166, Considering 10/90 traces
 61526/100000: episode: 1170, duration: 4.719s, episode steps: 100, steps per second: 21, episode reward: 58.701, mean reward: 0.587 [0.504, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.229, 10.098], loss: 0.001430, mae: 0.041813, mean_q: 1.163564
 61554/100000: episode: 1171, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 19.292, mean reward: 0.689 [0.594, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.199, 10.100], loss: 0.001228, mae: 0.039060, mean_q: 1.161150
 61582/100000: episode: 1172, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 19.518, mean reward: 0.697 [0.643, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.189, 10.100], loss: 0.001446, mae: 0.042128, mean_q: 1.163311
 61589/100000: episode: 1173, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 4.789, mean reward: 0.684 [0.640, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.436], loss: 0.001951, mae: 0.048612, mean_q: 1.179868
 61608/100000: episode: 1174, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 12.298, mean reward: 0.647 [0.572, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.299], loss: 0.001561, mae: 0.043200, mean_q: 1.163754
 61631/100000: episode: 1175, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 15.551, mean reward: 0.676 [0.606, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.947, 10.429], loss: 0.001437, mae: 0.041170, mean_q: 1.171718
 61654/100000: episode: 1176, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 14.892, mean reward: 0.647 [0.546, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.083, 10.365], loss: 0.001443, mae: 0.040886, mean_q: 1.167512
 61673/100000: episode: 1177, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 13.177, mean reward: 0.694 [0.610, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.170, 10.442], loss: 0.001372, mae: 0.040338, mean_q: 1.171314
 61683/100000: episode: 1178, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 6.362, mean reward: 0.636 [0.586, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.225, 10.268], loss: 0.001322, mae: 0.039032, mean_q: 1.163704
 61706/100000: episode: 1179, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 16.323, mean reward: 0.710 [0.627, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.098, 10.473], loss: 0.001526, mae: 0.041850, mean_q: 1.169622
 61716/100000: episode: 1180, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 7.163, mean reward: 0.716 [0.645, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.525], loss: 0.001534, mae: 0.041409, mean_q: 1.172009
 61753/100000: episode: 1181, duration: 0.188s, episode steps: 37, steps per second: 196, episode reward: 23.386, mean reward: 0.632 [0.575, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.487, 10.362], loss: 0.001515, mae: 0.042952, mean_q: 1.166821
 61772/100000: episode: 1182, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 12.996, mean reward: 0.684 [0.615, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.253, 10.100], loss: 0.001483, mae: 0.041130, mean_q: 1.175108
 61795/100000: episode: 1183, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 14.215, mean reward: 0.618 [0.570, 0.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.709, 10.279], loss: 0.001529, mae: 0.040777, mean_q: 1.168353
 61802/100000: episode: 1184, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 4.721, mean reward: 0.674 [0.651, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.401], loss: 0.001393, mae: 0.043203, mean_q: 1.188257
 61812/100000: episode: 1185, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 6.297, mean reward: 0.630 [0.601, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.289], loss: 0.001456, mae: 0.040659, mean_q: 1.166288
 61831/100000: episode: 1186, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 12.470, mean reward: 0.656 [0.569, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.295], loss: 0.001352, mae: 0.040788, mean_q: 1.175880
 61854/100000: episode: 1187, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 15.918, mean reward: 0.692 [0.591, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.478, 10.356], loss: 0.001915, mae: 0.048286, mean_q: 1.176895
 61867/100000: episode: 1188, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 9.130, mean reward: 0.702 [0.641, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.398], loss: 0.001639, mae: 0.044732, mean_q: 1.169070
 61915/100000: episode: 1189, duration: 0.251s, episode steps: 48, steps per second: 192, episode reward: 30.449, mean reward: 0.634 [0.541, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.812, 10.334], loss: 0.001544, mae: 0.041382, mean_q: 1.175455
 61963/100000: episode: 1190, duration: 0.267s, episode steps: 48, steps per second: 180, episode reward: 38.865, mean reward: 0.810 [0.623, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-1.864, 10.516], loss: 0.001471, mae: 0.040977, mean_q: 1.175216
 61982/100000: episode: 1191, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 12.208, mean reward: 0.643 [0.575, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-1.055, 10.342], loss: 0.001348, mae: 0.040414, mean_q: 1.178058
 62010/100000: episode: 1192, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 18.619, mean reward: 0.665 [0.620, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.719, 10.100], loss: 0.001533, mae: 0.042050, mean_q: 1.175029
 62017/100000: episode: 1193, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 4.638, mean reward: 0.663 [0.632, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.352], loss: 0.001587, mae: 0.042853, mean_q: 1.190476
 62036/100000: episode: 1194, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 12.183, mean reward: 0.641 [0.607, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.283, 10.100], loss: 0.001559, mae: 0.043247, mean_q: 1.175896
 62084/100000: episode: 1195, duration: 0.256s, episode steps: 48, steps per second: 188, episode reward: 30.197, mean reward: 0.629 [0.540, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-1.297, 10.273], loss: 0.001567, mae: 0.042518, mean_q: 1.176006
 62103/100000: episode: 1196, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 13.986, mean reward: 0.736 [0.624, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.368, 10.100], loss: 0.001636, mae: 0.044631, mean_q: 1.183202
 62140/100000: episode: 1197, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 26.050, mean reward: 0.704 [0.581, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.583, 10.321], loss: 0.001553, mae: 0.043147, mean_q: 1.178213
 62147/100000: episode: 1198, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 4.804, mean reward: 0.686 [0.681, 0.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.418], loss: 0.001464, mae: 0.043340, mean_q: 1.176291
 62154/100000: episode: 1199, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.910, mean reward: 0.701 [0.672, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.447], loss: 0.001817, mae: 0.043421, mean_q: 1.161651
 62182/100000: episode: 1200, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 18.005, mean reward: 0.643 [0.514, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.151, 10.100], loss: 0.001475, mae: 0.040844, mean_q: 1.182681
 62189/100000: episode: 1201, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 4.566, mean reward: 0.652 [0.621, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.325], loss: 0.001676, mae: 0.042837, mean_q: 1.180514
 62196/100000: episode: 1202, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 5.294, mean reward: 0.756 [0.713, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.299, 10.542], loss: 0.001484, mae: 0.043831, mean_q: 1.183791
 62205/100000: episode: 1203, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 6.632, mean reward: 0.737 [0.665, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.503, 10.100], loss: 0.001738, mae: 0.045905, mean_q: 1.175586
 62214/100000: episode: 1204, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 6.849, mean reward: 0.761 [0.692, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.356, 10.100], loss: 0.001769, mae: 0.045451, mean_q: 1.163337
 62242/100000: episode: 1205, duration: 0.143s, episode steps: 28, steps per second: 195, episode reward: 18.388, mean reward: 0.657 [0.586, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.065, 10.100], loss: 0.001571, mae: 0.043095, mean_q: 1.189977
 62252/100000: episode: 1206, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 6.556, mean reward: 0.656 [0.592, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.433], loss: 0.001547, mae: 0.040936, mean_q: 1.182716
 62271/100000: episode: 1207, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 12.579, mean reward: 0.662 [0.591, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.530, 10.266], loss: 0.001661, mae: 0.043775, mean_q: 1.179383
 62319/100000: episode: 1208, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 31.445, mean reward: 0.655 [0.582, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.308, 10.390], loss: 0.001637, mae: 0.042612, mean_q: 1.182436
 62347/100000: episode: 1209, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 19.232, mean reward: 0.687 [0.621, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.315, 10.100], loss: 0.001617, mae: 0.043359, mean_q: 1.181494
 62395/100000: episode: 1210, duration: 0.266s, episode steps: 48, steps per second: 180, episode reward: 32.253, mean reward: 0.672 [0.573, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.306, 10.313], loss: 0.001655, mae: 0.044639, mean_q: 1.185286
 62423/100000: episode: 1211, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 18.728, mean reward: 0.669 [0.602, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.292, 10.100], loss: 0.001773, mae: 0.044860, mean_q: 1.182803
 62471/100000: episode: 1212, duration: 0.249s, episode steps: 48, steps per second: 193, episode reward: 33.505, mean reward: 0.698 [0.605, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.303, 10.328], loss: 0.001587, mae: 0.042996, mean_q: 1.188272
 62478/100000: episode: 1213, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 4.667, mean reward: 0.667 [0.630, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.334], loss: 0.001699, mae: 0.044666, mean_q: 1.189649
 62515/100000: episode: 1214, duration: 0.192s, episode steps: 37, steps per second: 192, episode reward: 23.503, mean reward: 0.635 [0.556, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.035, 10.460], loss: 0.001688, mae: 0.042984, mean_q: 1.182010
 62543/100000: episode: 1215, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 19.005, mean reward: 0.679 [0.626, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.610, 10.100], loss: 0.001527, mae: 0.041823, mean_q: 1.188295
 62550/100000: episode: 1216, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.119, mean reward: 0.731 [0.674, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.814, 10.482], loss: 0.001776, mae: 0.046313, mean_q: 1.185862
 62559/100000: episode: 1217, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 6.142, mean reward: 0.682 [0.644, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.421, 10.100], loss: 0.001925, mae: 0.047169, mean_q: 1.179924
 62572/100000: episode: 1218, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 9.090, mean reward: 0.699 [0.646, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.305], loss: 0.001723, mae: 0.044087, mean_q: 1.197516
 62591/100000: episode: 1219, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 11.849, mean reward: 0.624 [0.581, 0.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.080, 10.100], loss: 0.001659, mae: 0.044225, mean_q: 1.192484
 62600/100000: episode: 1220, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 6.219, mean reward: 0.691 [0.640, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.305, 10.100], loss: 0.001645, mae: 0.042996, mean_q: 1.206214
 62607/100000: episode: 1221, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 5.028, mean reward: 0.718 [0.687, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-1.279, 10.432], loss: 0.002298, mae: 0.048396, mean_q: 1.178303
 62626/100000: episode: 1222, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 12.420, mean reward: 0.654 [0.612, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.246, 10.100], loss: 0.001387, mae: 0.040056, mean_q: 1.188012
 62645/100000: episode: 1223, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 12.471, mean reward: 0.656 [0.599, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.901, 10.342], loss: 0.001539, mae: 0.041461, mean_q: 1.198459
 62652/100000: episode: 1224, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 4.559, mean reward: 0.651 [0.601, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.705, 10.368], loss: 0.001352, mae: 0.039333, mean_q: 1.168310
 62671/100000: episode: 1225, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 12.348, mean reward: 0.650 [0.577, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.638, 10.100], loss: 0.001635, mae: 0.043002, mean_q: 1.188172
 62681/100000: episode: 1226, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 6.410, mean reward: 0.641 [0.594, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.232], loss: 0.001497, mae: 0.043637, mean_q: 1.202287
 62729/100000: episode: 1227, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 32.771, mean reward: 0.683 [0.602, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.599, 10.553], loss: 0.001731, mae: 0.044873, mean_q: 1.192130
 62736/100000: episode: 1228, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 4.849, mean reward: 0.693 [0.658, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.421], loss: 0.001545, mae: 0.040812, mean_q: 1.193828
 62755/100000: episode: 1229, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 12.451, mean reward: 0.655 [0.620, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.156, 10.100], loss: 0.001539, mae: 0.043104, mean_q: 1.197655
 62764/100000: episode: 1230, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 6.313, mean reward: 0.701 [0.660, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.371, 10.100], loss: 0.001542, mae: 0.042927, mean_q: 1.192754
 62777/100000: episode: 1231, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 9.128, mean reward: 0.702 [0.621, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.052, 10.353], loss: 0.001789, mae: 0.044844, mean_q: 1.194337
 62786/100000: episode: 1232, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 6.194, mean reward: 0.688 [0.632, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.580, 10.100], loss: 0.001465, mae: 0.039979, mean_q: 1.200296
 62795/100000: episode: 1233, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 6.284, mean reward: 0.698 [0.648, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.351, 10.100], loss: 0.001754, mae: 0.045053, mean_q: 1.196578
 62805/100000: episode: 1234, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 6.384, mean reward: 0.638 [0.587, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.289, 10.302], loss: 0.001573, mae: 0.042733, mean_q: 1.204047
 62815/100000: episode: 1235, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 6.396, mean reward: 0.640 [0.593, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.127, 10.326], loss: 0.001376, mae: 0.040749, mean_q: 1.202998
 62838/100000: episode: 1236, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 15.648, mean reward: 0.680 [0.609, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.530, 10.397], loss: 0.001636, mae: 0.043438, mean_q: 1.198753
 62851/100000: episode: 1237, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 8.777, mean reward: 0.675 [0.631, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.341], loss: 0.001828, mae: 0.044872, mean_q: 1.196617
 62879/100000: episode: 1238, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 19.074, mean reward: 0.681 [0.602, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.164, 10.100], loss: 0.002023, mae: 0.047701, mean_q: 1.199134
 62898/100000: episode: 1239, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 13.023, mean reward: 0.685 [0.632, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.403, 10.100], loss: 0.001848, mae: 0.045373, mean_q: 1.189933
 62935/100000: episode: 1240, duration: 0.185s, episode steps: 37, steps per second: 200, episode reward: 24.868, mean reward: 0.672 [0.608, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.629, 10.378], loss: 0.001613, mae: 0.043750, mean_q: 1.198338
 62954/100000: episode: 1241, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 14.349, mean reward: 0.755 [0.654, 0.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.362, 10.432], loss: 0.001687, mae: 0.044114, mean_q: 1.188730
 62967/100000: episode: 1242, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 9.224, mean reward: 0.710 [0.633, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.492], loss: 0.001679, mae: 0.043603, mean_q: 1.204888
 62995/100000: episode: 1243, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 19.602, mean reward: 0.700 [0.633, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.836, 10.100], loss: 0.001623, mae: 0.042494, mean_q: 1.188301
 63032/100000: episode: 1244, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 21.169, mean reward: 0.572 [0.508, 0.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.459, 10.105], loss: 0.001713, mae: 0.044333, mean_q: 1.198016
 63045/100000: episode: 1245, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 8.910, mean reward: 0.685 [0.645, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.378], loss: 0.001996, mae: 0.046500, mean_q: 1.189190
 63064/100000: episode: 1246, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 13.173, mean reward: 0.693 [0.619, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.844, 10.100], loss: 0.001887, mae: 0.046737, mean_q: 1.202069
 63073/100000: episode: 1247, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 6.074, mean reward: 0.675 [0.634, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.350, 10.100], loss: 0.001906, mae: 0.046285, mean_q: 1.200625
 63086/100000: episode: 1248, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 8.468, mean reward: 0.651 [0.589, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.347], loss: 0.001894, mae: 0.044209, mean_q: 1.185795
 63123/100000: episode: 1249, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 21.260, mean reward: 0.575 [0.515, 0.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.088, 10.100], loss: 0.001813, mae: 0.044199, mean_q: 1.197210
 63171/100000: episode: 1250, duration: 0.241s, episode steps: 48, steps per second: 199, episode reward: 30.012, mean reward: 0.625 [0.535, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.301, 10.231], loss: 0.001614, mae: 0.043038, mean_q: 1.196337
 63190/100000: episode: 1251, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 12.481, mean reward: 0.657 [0.592, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.760, 10.100], loss: 0.001845, mae: 0.045279, mean_q: 1.193204
 63197/100000: episode: 1252, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 4.952, mean reward: 0.707 [0.674, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.501], loss: 0.001638, mae: 0.043676, mean_q: 1.205145
 63216/100000: episode: 1253, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 12.022, mean reward: 0.633 [0.586, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.282], loss: 0.001780, mae: 0.045882, mean_q: 1.191983
 63225/100000: episode: 1254, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 6.207, mean reward: 0.690 [0.669, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.232, 10.100], loss: 0.001428, mae: 0.040805, mean_q: 1.196979
 63244/100000: episode: 1255, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 13.567, mean reward: 0.714 [0.640, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.432], loss: 0.002129, mae: 0.049558, mean_q: 1.204183
 63253/100000: episode: 1256, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 6.471, mean reward: 0.719 [0.691, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.319, 10.100], loss: 0.001652, mae: 0.042836, mean_q: 1.207619
 63266/100000: episode: 1257, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 8.637, mean reward: 0.664 [0.589, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.299, 10.406], loss: 0.001530, mae: 0.041754, mean_q: 1.195980
 63273/100000: episode: 1258, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.818, mean reward: 0.688 [0.630, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.093, 10.418], loss: 0.001732, mae: 0.045569, mean_q: 1.195678
 63292/100000: episode: 1259, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 13.415, mean reward: 0.706 [0.624, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.437], loss: 0.001705, mae: 0.043175, mean_q: 1.203265
[Info] 2-TH LEVEL FOUND: 1.4911959171295166, Considering 10/90 traces
 63329/100000: episode: 1260, duration: 4.443s, episode steps: 37, steps per second: 8, episode reward: 23.139, mean reward: 0.625 [0.542, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-2.162, 10.303], loss: 0.001563, mae: 0.042555, mean_q: 1.197364
 63366/100000: episode: 1261, duration: 0.214s, episode steps: 37, steps per second: 173, episode reward: 28.846, mean reward: 0.780 [0.695, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.803, 10.477], loss: 0.002034, mae: 0.048249, mean_q: 1.202652
 63373/100000: episode: 1262, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 5.084, mean reward: 0.726 [0.705, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.196, 10.100], loss: 0.001792, mae: 0.045183, mean_q: 1.204664
 63380/100000: episode: 1263, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 5.321, mean reward: 0.760 [0.724, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.401, 10.100], loss: 0.001793, mae: 0.043584, mean_q: 1.181960
 63390/100000: episode: 1264, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 7.494, mean reward: 0.749 [0.704, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.154, 10.445], loss: 0.001897, mae: 0.046751, mean_q: 1.197495
 63397/100000: episode: 1265, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 5.222, mean reward: 0.746 [0.697, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.343, 10.100], loss: 0.001783, mae: 0.048449, mean_q: 1.225270
 63405/100000: episode: 1266, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 5.874, mean reward: 0.734 [0.699, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.420, 10.100], loss: 0.001787, mae: 0.043291, mean_q: 1.202235
 63442/100000: episode: 1267, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 24.919, mean reward: 0.673 [0.581, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.833, 10.396], loss: 0.001741, mae: 0.044588, mean_q: 1.199200
 63479/100000: episode: 1268, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 26.419, mean reward: 0.714 [0.631, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.230, 10.417], loss: 0.001849, mae: 0.047249, mean_q: 1.211260
 63516/100000: episode: 1269, duration: 0.190s, episode steps: 37, steps per second: 195, episode reward: 28.471, mean reward: 0.769 [0.710, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.924, 10.536], loss: 0.001509, mae: 0.041695, mean_q: 1.209989
 63532/100000: episode: 1270, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 12.271, mean reward: 0.767 [0.715, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-1.386, 10.554], loss: 0.001420, mae: 0.039979, mean_q: 1.216391
 63539/100000: episode: 1271, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 5.009, mean reward: 0.716 [0.680, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.334, 10.100], loss: 0.001678, mae: 0.041286, mean_q: 1.229845
 63546/100000: episode: 1272, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 5.309, mean reward: 0.758 [0.728, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.367, 10.100], loss: 0.001939, mae: 0.047611, mean_q: 1.213084
 63553/100000: episode: 1273, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.898, mean reward: 0.700 [0.663, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.376, 10.100], loss: 0.001505, mae: 0.043511, mean_q: 1.211793
 63560/100000: episode: 1274, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 4.944, mean reward: 0.706 [0.659, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.309, 10.100], loss: 0.001356, mae: 0.038979, mean_q: 1.210088
 63567/100000: episode: 1275, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.578, mean reward: 0.797 [0.749, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.348, 10.100], loss: 0.001711, mae: 0.044758, mean_q: 1.221090
 63575/100000: episode: 1276, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 5.903, mean reward: 0.738 [0.686, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.338, 10.100], loss: 0.001608, mae: 0.044450, mean_q: 1.215581
 63582/100000: episode: 1277, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 5.652, mean reward: 0.807 [0.763, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.362, 10.100], loss: 0.001709, mae: 0.044027, mean_q: 1.212920
 63592/100000: episode: 1278, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 7.615, mean reward: 0.762 [0.652, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.301, 10.516], loss: 0.001778, mae: 0.045089, mean_q: 1.223304
 63599/100000: episode: 1279, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 4.726, mean reward: 0.675 [0.625, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.194, 10.100], loss: 0.001706, mae: 0.046221, mean_q: 1.236219
 63636/100000: episode: 1280, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 29.605, mean reward: 0.800 [0.738, 0.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.142, 10.522], loss: 0.001909, mae: 0.046088, mean_q: 1.218694
 63643/100000: episode: 1281, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 5.082, mean reward: 0.726 [0.672, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.332, 10.100], loss: 0.001757, mae: 0.043172, mean_q: 1.218763
 63650/100000: episode: 1282, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 4.999, mean reward: 0.714 [0.689, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.349, 10.100], loss: 0.001510, mae: 0.043590, mean_q: 1.238320
 63657/100000: episode: 1283, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 5.500, mean reward: 0.786 [0.722, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.364, 10.100], loss: 0.001538, mae: 0.041873, mean_q: 1.209122
 63664/100000: episode: 1284, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 4.900, mean reward: 0.700 [0.640, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.338, 10.100], loss: 0.001626, mae: 0.045149, mean_q: 1.211469
 63671/100000: episode: 1285, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 4.875, mean reward: 0.696 [0.679, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.472, 10.100], loss: 0.001806, mae: 0.047780, mean_q: 1.225243
 63678/100000: episode: 1286, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 5.501, mean reward: 0.786 [0.709, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.431, 10.100], loss: 0.001481, mae: 0.040651, mean_q: 1.215303
 63685/100000: episode: 1287, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 5.386, mean reward: 0.769 [0.743, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.323, 10.100], loss: 0.001210, mae: 0.039101, mean_q: 1.231306
 63692/100000: episode: 1288, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 5.014, mean reward: 0.716 [0.686, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.437, 10.100], loss: 0.001707, mae: 0.043539, mean_q: 1.239061
 63704/100000: episode: 1289, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 9.621, mean reward: 0.802 [0.711, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.376, 10.100], loss: 0.001293, mae: 0.038993, mean_q: 1.227558
 63712/100000: episode: 1290, duration: 0.054s, episode steps: 8, steps per second: 147, episode reward: 6.415, mean reward: 0.802 [0.696, 0.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.368, 10.100], loss: 0.001604, mae: 0.043399, mean_q: 1.229980
 63749/100000: episode: 1291, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 28.567, mean reward: 0.772 [0.556, 0.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.481, 10.257], loss: 0.002107, mae: 0.049546, mean_q: 1.214510
 63757/100000: episode: 1292, duration: 0.042s, episode steps: 8, steps per second: 190, episode reward: 5.695, mean reward: 0.712 [0.683, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.283, 10.100], loss: 0.002059, mae: 0.048010, mean_q: 1.235936
 63764/100000: episode: 1293, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 5.158, mean reward: 0.737 [0.711, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.392, 10.100], loss: 0.001537, mae: 0.041419, mean_q: 1.198530
 63771/100000: episode: 1294, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 5.011, mean reward: 0.716 [0.665, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.290, 10.100], loss: 0.001557, mae: 0.041464, mean_q: 1.210738
 63783/100000: episode: 1295, duration: 0.082s, episode steps: 12, steps per second: 147, episode reward: 8.973, mean reward: 0.748 [0.691, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.687, 10.100], loss: 0.002056, mae: 0.046790, mean_q: 1.209196
 63790/100000: episode: 1296, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 5.123, mean reward: 0.732 [0.693, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.357, 10.100], loss: 0.001670, mae: 0.045386, mean_q: 1.226088
 63802/100000: episode: 1297, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 8.603, mean reward: 0.717 [0.653, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.525, 10.100], loss: 0.001691, mae: 0.043817, mean_q: 1.234483
 63809/100000: episode: 1298, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 5.521, mean reward: 0.789 [0.761, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.339, 10.100], loss: 0.001763, mae: 0.046088, mean_q: 1.235599
 63816/100000: episode: 1299, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 5.250, mean reward: 0.750 [0.714, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.895, 10.100], loss: 0.001764, mae: 0.045547, mean_q: 1.221828
 63826/100000: episode: 1300, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 8.802, mean reward: 0.880 [0.861, 0.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.648], loss: 0.001766, mae: 0.046452, mean_q: 1.217681
 63863/100000: episode: 1301, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 28.997, mean reward: 0.784 [0.683, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.301, 10.599], loss: 0.001685, mae: 0.043917, mean_q: 1.219571
 63873/100000: episode: 1302, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 8.501, mean reward: 0.850 [0.765, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.554, 10.618], loss: 0.001984, mae: 0.046843, mean_q: 1.208317
 63889/100000: episode: 1303, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 11.644, mean reward: 0.728 [0.607, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.383, 10.386], loss: 0.001586, mae: 0.043561, mean_q: 1.233292
 63901/100000: episode: 1304, duration: 0.073s, episode steps: 12, steps per second: 163, episode reward: 8.768, mean reward: 0.731 [0.649, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.241, 10.100], loss: 0.001420, mae: 0.040041, mean_q: 1.225432
[Info] FALSIFICATION!
 63933/100000: episode: 1305, duration: 0.323s, episode steps: 32, steps per second: 99, episode reward: 26.108, mean reward: 0.816 [0.700, 1.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.144, 10.565], loss: 0.001644, mae: 0.044088, mean_q: 1.240863
 63940/100000: episode: 1306, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 4.900, mean reward: 0.700 [0.664, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.238, 10.100], loss: 0.001742, mae: 0.043553, mean_q: 1.236233
 63947/100000: episode: 1307, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 5.207, mean reward: 0.744 [0.708, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.384, 10.100], loss: 0.002911, mae: 0.048655, mean_q: 1.217315
 63984/100000: episode: 1308, duration: 0.181s, episode steps: 37, steps per second: 205, episode reward: 31.012, mean reward: 0.838 [0.758, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.865, 10.602], loss: 0.001848, mae: 0.044638, mean_q: 1.229880
 63991/100000: episode: 1309, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 5.067, mean reward: 0.724 [0.664, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.264, 10.100], loss: 0.001355, mae: 0.039666, mean_q: 1.223241
 63998/100000: episode: 1310, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 5.028, mean reward: 0.718 [0.680, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-1.187, 10.100], loss: 0.002007, mae: 0.046861, mean_q: 1.229577
 64005/100000: episode: 1311, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 5.077, mean reward: 0.725 [0.688, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.334, 10.100], loss: 0.001562, mae: 0.041584, mean_q: 1.265495
 64042/100000: episode: 1312, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 26.650, mean reward: 0.720 [0.575, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.130, 10.279], loss: 0.001661, mae: 0.043307, mean_q: 1.241452
 64052/100000: episode: 1313, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 8.299, mean reward: 0.830 [0.749, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.674], loss: 0.001268, mae: 0.037707, mean_q: 1.217731
 64062/100000: episode: 1314, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 7.532, mean reward: 0.753 [0.685, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.459], loss: 0.002529, mae: 0.045473, mean_q: 1.239017
 64072/100000: episode: 1315, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 7.493, mean reward: 0.749 [0.708, 0.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.487], loss: 0.001488, mae: 0.040843, mean_q: 1.232560
 64109/100000: episode: 1316, duration: 0.190s, episode steps: 37, steps per second: 194, episode reward: 26.050, mean reward: 0.704 [0.565, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.154, 10.260], loss: 0.001602, mae: 0.043240, mean_q: 1.256806
 64116/100000: episode: 1317, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 5.036, mean reward: 0.719 [0.704, 0.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.379, 10.100], loss: 0.001309, mae: 0.040662, mean_q: 1.230832
 64124/100000: episode: 1318, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 5.724, mean reward: 0.715 [0.658, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.849, 10.100], loss: 0.001632, mae: 0.042972, mean_q: 1.246133
 64136/100000: episode: 1319, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 9.666, mean reward: 0.805 [0.680, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.492, 10.100], loss: 0.002239, mae: 0.045716, mean_q: 1.243881
 64143/100000: episode: 1320, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 4.961, mean reward: 0.709 [0.672, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.396, 10.100], loss: 0.001835, mae: 0.045257, mean_q: 1.222070
 64150/100000: episode: 1321, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 4.714, mean reward: 0.673 [0.633, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.464, 10.100], loss: 0.001865, mae: 0.043707, mean_q: 1.237294
 64157/100000: episode: 1322, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 5.099, mean reward: 0.728 [0.675, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.492, 10.100], loss: 0.001764, mae: 0.045013, mean_q: 1.229275
 64194/100000: episode: 1323, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 26.633, mean reward: 0.720 [0.583, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.762, 10.278], loss: 0.002007, mae: 0.045568, mean_q: 1.241292
 64201/100000: episode: 1324, duration: 0.048s, episode steps: 7, steps per second: 147, episode reward: 5.212, mean reward: 0.745 [0.711, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.364, 10.100], loss: 0.002343, mae: 0.050971, mean_q: 1.241004
 64209/100000: episode: 1325, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 5.409, mean reward: 0.676 [0.637, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.445, 10.100], loss: 0.001548, mae: 0.042755, mean_q: 1.250869
 64216/100000: episode: 1326, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 5.296, mean reward: 0.757 [0.692, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.513, 10.100], loss: 0.001945, mae: 0.046264, mean_q: 1.250485
 64223/100000: episode: 1327, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 4.818, mean reward: 0.688 [0.660, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.435, 10.100], loss: 0.001913, mae: 0.045956, mean_q: 1.262550
 64230/100000: episode: 1328, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 5.472, mean reward: 0.782 [0.696, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.581, 10.100], loss: 0.002842, mae: 0.057234, mean_q: 1.232980
 64238/100000: episode: 1329, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 6.145, mean reward: 0.768 [0.730, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.354, 10.100], loss: 0.001934, mae: 0.047902, mean_q: 1.258953
 64245/100000: episode: 1330, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 5.337, mean reward: 0.762 [0.706, 0.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-1.139, 10.100], loss: 0.001684, mae: 0.043271, mean_q: 1.239534
 64257/100000: episode: 1331, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 8.327, mean reward: 0.694 [0.659, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.317, 10.100], loss: 0.001660, mae: 0.044358, mean_q: 1.248397
 64269/100000: episode: 1332, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 8.455, mean reward: 0.705 [0.652, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.276, 10.100], loss: 0.001488, mae: 0.040206, mean_q: 1.250791
 64276/100000: episode: 1333, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 4.840, mean reward: 0.691 [0.671, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.322, 10.100], loss: 0.001811, mae: 0.044324, mean_q: 1.254333
 64313/100000: episode: 1334, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 29.070, mean reward: 0.786 [0.673, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.131, 10.636], loss: 0.001621, mae: 0.042838, mean_q: 1.236502
 64320/100000: episode: 1335, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 5.321, mean reward: 0.760 [0.690, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.362, 10.100], loss: 0.001270, mae: 0.038468, mean_q: 1.242308
 64332/100000: episode: 1336, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 9.648, mean reward: 0.804 [0.767, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.357, 10.100], loss: 0.001328, mae: 0.039240, mean_q: 1.253082
 64339/100000: episode: 1337, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 5.844, mean reward: 0.835 [0.738, 0.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.397, 10.100], loss: 0.001874, mae: 0.045211, mean_q: 1.256591
 64346/100000: episode: 1338, duration: 0.041s, episode steps: 7, steps per second: 171, episode reward: 5.007, mean reward: 0.715 [0.612, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.219, 10.100], loss: 0.001389, mae: 0.040503, mean_q: 1.235516
 64353/100000: episode: 1339, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 5.234, mean reward: 0.748 [0.716, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.475, 10.100], loss: 0.001681, mae: 0.044666, mean_q: 1.261478
 64390/100000: episode: 1340, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 28.354, mean reward: 0.766 [0.695, 0.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.458, 10.504], loss: 0.001709, mae: 0.045841, mean_q: 1.245942
 64427/100000: episode: 1341, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 27.755, mean reward: 0.750 [0.597, 0.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.139, 10.426], loss: 0.001676, mae: 0.044322, mean_q: 1.242596
 64434/100000: episode: 1342, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 4.893, mean reward: 0.699 [0.657, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.340, 10.100], loss: 0.001732, mae: 0.044444, mean_q: 1.253917
 64441/100000: episode: 1343, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 5.508, mean reward: 0.787 [0.736, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.474, 10.100], loss: 0.001779, mae: 0.045164, mean_q: 1.261876
 64478/100000: episode: 1344, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 27.361, mean reward: 0.739 [0.556, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.436, 10.276], loss: 0.001755, mae: 0.044699, mean_q: 1.267328
 64485/100000: episode: 1345, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 5.354, mean reward: 0.765 [0.721, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.483, 10.100], loss: 0.001941, mae: 0.046599, mean_q: 1.257780
 64492/100000: episode: 1346, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 4.662, mean reward: 0.666 [0.639, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.362, 10.100], loss: 0.001556, mae: 0.044929, mean_q: 1.270603
 64499/100000: episode: 1347, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 5.281, mean reward: 0.754 [0.715, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.303, 10.100], loss: 0.001795, mae: 0.045327, mean_q: 1.252649
 64507/100000: episode: 1348, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 6.039, mean reward: 0.755 [0.720, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.286, 10.100], loss: 0.001606, mae: 0.044883, mean_q: 1.259905
 64517/100000: episode: 1349, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 7.117, mean reward: 0.712 [0.630, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.365], loss: 0.001878, mae: 0.046994, mean_q: 1.225759
[Info] Complete ISplit Iteration
[Info] Levels: [1.3113863, 1.4911959, 1.7482575]
[Info] Cond. Prob: [0.1, 0.1, 0.03]
[Info] Error Prob: 0.00030000000000000003

 64525/100000: episode: 1350, duration: 4.374s, episode steps: 8, steps per second: 2, episode reward: 5.552, mean reward: 0.694 [0.619, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.622, 10.100], loss: 0.002581, mae: 0.051057, mean_q: 1.257180
 64625/100000: episode: 1351, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 56.917, mean reward: 0.569 [0.501, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.751, 10.098], loss: 0.001827, mae: 0.045206, mean_q: 1.266041
 64725/100000: episode: 1352, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 60.074, mean reward: 0.601 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.760, 10.325], loss: 0.001839, mae: 0.044284, mean_q: 1.264329
 64825/100000: episode: 1353, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 57.019, mean reward: 0.570 [0.499, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.260, 10.166], loss: 0.001585, mae: 0.042718, mean_q: 1.259940
 64925/100000: episode: 1354, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 58.924, mean reward: 0.589 [0.510, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.733, 10.145], loss: 0.001654, mae: 0.042765, mean_q: 1.256239
 65025/100000: episode: 1355, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.946, mean reward: 0.579 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.046, 10.123], loss: 0.001723, mae: 0.042600, mean_q: 1.258030
 65125/100000: episode: 1356, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 61.374, mean reward: 0.614 [0.513, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.157, 10.098], loss: 0.001909, mae: 0.044722, mean_q: 1.254303
 65225/100000: episode: 1357, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.340, mean reward: 0.583 [0.506, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.865, 10.142], loss: 0.001632, mae: 0.042590, mean_q: 1.256003
 65325/100000: episode: 1358, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 59.562, mean reward: 0.596 [0.506, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.890, 10.368], loss: 0.001520, mae: 0.041718, mean_q: 1.257128
 65425/100000: episode: 1359, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 60.077, mean reward: 0.601 [0.504, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.739, 10.098], loss: 0.001714, mae: 0.041694, mean_q: 1.262450
 65525/100000: episode: 1360, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 59.667, mean reward: 0.597 [0.504, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.908, 10.259], loss: 0.001381, mae: 0.039660, mean_q: 1.255200
 65625/100000: episode: 1361, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.791, mean reward: 0.588 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.606, 10.202], loss: 0.001564, mae: 0.042468, mean_q: 1.260843
 65725/100000: episode: 1362, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 56.844, mean reward: 0.568 [0.505, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.579, 10.098], loss: 0.001517, mae: 0.041609, mean_q: 1.258635
 65825/100000: episode: 1363, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 61.472, mean reward: 0.615 [0.509, 0.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.224, 10.144], loss: 0.001509, mae: 0.040985, mean_q: 1.253845
 65925/100000: episode: 1364, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 60.984, mean reward: 0.610 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.484, 10.347], loss: 0.001533, mae: 0.041379, mean_q: 1.258334
 66025/100000: episode: 1365, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.923, mean reward: 0.589 [0.504, 0.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.638, 10.223], loss: 0.001389, mae: 0.040751, mean_q: 1.260964
 66125/100000: episode: 1366, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 63.980, mean reward: 0.640 [0.508, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.523, 10.098], loss: 0.001672, mae: 0.042247, mean_q: 1.258422
 66225/100000: episode: 1367, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.038, mean reward: 0.600 [0.501, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.705, 10.266], loss: 0.001512, mae: 0.041644, mean_q: 1.268056
 66325/100000: episode: 1368, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 59.427, mean reward: 0.594 [0.500, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.870, 10.220], loss: 0.001392, mae: 0.040614, mean_q: 1.265398
 66425/100000: episode: 1369, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 58.991, mean reward: 0.590 [0.499, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.352, 10.126], loss: 0.001576, mae: 0.042665, mean_q: 1.268082
 66525/100000: episode: 1370, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.997, mean reward: 0.580 [0.514, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.645, 10.186], loss: 0.001626, mae: 0.043781, mean_q: 1.259645
 66625/100000: episode: 1371, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.860, mean reward: 0.579 [0.503, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.294, 10.201], loss: 0.001848, mae: 0.044737, mean_q: 1.255382
 66725/100000: episode: 1372, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 58.452, mean reward: 0.585 [0.508, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.757, 10.098], loss: 0.001758, mae: 0.043866, mean_q: 1.255408
 66825/100000: episode: 1373, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.837, mean reward: 0.598 [0.507, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.731, 10.098], loss: 0.001500, mae: 0.041537, mean_q: 1.253624
 66925/100000: episode: 1374, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.269, mean reward: 0.583 [0.501, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.688, 10.171], loss: 0.001671, mae: 0.042746, mean_q: 1.248481
 67025/100000: episode: 1375, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.523, mean reward: 0.585 [0.506, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.126, 10.176], loss: 0.001441, mae: 0.041376, mean_q: 1.247066
 67125/100000: episode: 1376, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 59.335, mean reward: 0.593 [0.511, 0.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.913, 10.217], loss: 0.001740, mae: 0.042622, mean_q: 1.245413
 67225/100000: episode: 1377, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 58.854, mean reward: 0.589 [0.522, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.406, 10.133], loss: 0.001542, mae: 0.042177, mean_q: 1.250350
 67325/100000: episode: 1378, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.744, mean reward: 0.587 [0.501, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.456, 10.281], loss: 0.001697, mae: 0.043029, mean_q: 1.239264
 67425/100000: episode: 1379, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 58.576, mean reward: 0.586 [0.506, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.467, 10.206], loss: 0.001805, mae: 0.043444, mean_q: 1.237265
 67525/100000: episode: 1380, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 57.200, mean reward: 0.572 [0.501, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.389, 10.098], loss: 0.001917, mae: 0.045429, mean_q: 1.232132
 67625/100000: episode: 1381, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 58.527, mean reward: 0.585 [0.498, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.360, 10.272], loss: 0.001692, mae: 0.043814, mean_q: 1.233455
 67725/100000: episode: 1382, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 57.828, mean reward: 0.578 [0.504, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.627, 10.114], loss: 0.001724, mae: 0.044117, mean_q: 1.229109
 67825/100000: episode: 1383, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 59.718, mean reward: 0.597 [0.506, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.509, 10.098], loss: 0.001866, mae: 0.045008, mean_q: 1.228370
 67925/100000: episode: 1384, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 60.138, mean reward: 0.601 [0.523, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.641, 10.098], loss: 0.001777, mae: 0.044503, mean_q: 1.226589
 68025/100000: episode: 1385, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 58.040, mean reward: 0.580 [0.515, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.042, 10.177], loss: 0.001594, mae: 0.042730, mean_q: 1.227387
 68125/100000: episode: 1386, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.641, mean reward: 0.576 [0.511, 0.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.585, 10.319], loss: 0.001695, mae: 0.043092, mean_q: 1.225568
 68225/100000: episode: 1387, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 59.262, mean reward: 0.593 [0.500, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.076, 10.199], loss: 0.001698, mae: 0.043752, mean_q: 1.219772
 68325/100000: episode: 1388, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.523, mean reward: 0.575 [0.501, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.889, 10.269], loss: 0.001604, mae: 0.043078, mean_q: 1.220635
 68425/100000: episode: 1389, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 59.276, mean reward: 0.593 [0.521, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.853, 10.224], loss: 0.001632, mae: 0.043227, mean_q: 1.219063
 68525/100000: episode: 1390, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 59.838, mean reward: 0.598 [0.510, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.064, 10.098], loss: 0.001652, mae: 0.043031, mean_q: 1.214319
 68625/100000: episode: 1391, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.418, mean reward: 0.604 [0.502, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.921, 10.098], loss: 0.001563, mae: 0.042678, mean_q: 1.205491
 68725/100000: episode: 1392, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 60.029, mean reward: 0.600 [0.503, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.581, 10.171], loss: 0.001867, mae: 0.045126, mean_q: 1.202982
 68825/100000: episode: 1393, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.070, mean reward: 0.591 [0.502, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.169, 10.098], loss: 0.001685, mae: 0.043649, mean_q: 1.195888
 68925/100000: episode: 1394, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 60.764, mean reward: 0.608 [0.503, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.367, 10.098], loss: 0.001599, mae: 0.042327, mean_q: 1.195966
 69025/100000: episode: 1395, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 57.725, mean reward: 0.577 [0.501, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.464, 10.098], loss: 0.001630, mae: 0.043471, mean_q: 1.186306
 69125/100000: episode: 1396, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.769, mean reward: 0.588 [0.501, 0.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.410, 10.382], loss: 0.001567, mae: 0.042397, mean_q: 1.185538
 69225/100000: episode: 1397, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.072, mean reward: 0.581 [0.512, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.399, 10.098], loss: 0.001551, mae: 0.042122, mean_q: 1.181699
 69325/100000: episode: 1398, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 60.317, mean reward: 0.603 [0.506, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.105, 10.154], loss: 0.001608, mae: 0.043656, mean_q: 1.179857
 69425/100000: episode: 1399, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.527, mean reward: 0.595 [0.512, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.605, 10.249], loss: 0.001469, mae: 0.041482, mean_q: 1.170825
 69525/100000: episode: 1400, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.388, mean reward: 0.584 [0.508, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.867, 10.098], loss: 0.001482, mae: 0.041462, mean_q: 1.167940
 69625/100000: episode: 1401, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.392, mean reward: 0.584 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.045, 10.105], loss: 0.001296, mae: 0.039297, mean_q: 1.170939
 69725/100000: episode: 1402, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.638, mean reward: 0.586 [0.508, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.091, 10.098], loss: 0.001363, mae: 0.040032, mean_q: 1.167774
 69825/100000: episode: 1403, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 58.288, mean reward: 0.583 [0.503, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.506, 10.098], loss: 0.001357, mae: 0.040321, mean_q: 1.168896
 69925/100000: episode: 1404, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.369, mean reward: 0.594 [0.509, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.084, 10.197], loss: 0.001417, mae: 0.041030, mean_q: 1.168269
 70025/100000: episode: 1405, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.616, mean reward: 0.596 [0.508, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.797, 10.340], loss: 0.001378, mae: 0.040666, mean_q: 1.169944
 70125/100000: episode: 1406, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 63.514, mean reward: 0.635 [0.515, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.098, 10.445], loss: 0.001366, mae: 0.040059, mean_q: 1.171007
 70225/100000: episode: 1407, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 57.667, mean reward: 0.577 [0.507, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.757, 10.148], loss: 0.001364, mae: 0.040259, mean_q: 1.168689
 70325/100000: episode: 1408, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 59.537, mean reward: 0.595 [0.504, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.604, 10.098], loss: 0.001448, mae: 0.041631, mean_q: 1.173376
 70425/100000: episode: 1409, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 56.635, mean reward: 0.566 [0.498, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.631, 10.188], loss: 0.001430, mae: 0.040883, mean_q: 1.170423
 70525/100000: episode: 1410, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 62.939, mean reward: 0.629 [0.514, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.667, 10.098], loss: 0.001492, mae: 0.042191, mean_q: 1.171342
 70625/100000: episode: 1411, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 58.136, mean reward: 0.581 [0.506, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.080, 10.132], loss: 0.001490, mae: 0.042039, mean_q: 1.167927
 70725/100000: episode: 1412, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.103, mean reward: 0.581 [0.507, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.593, 10.098], loss: 0.001421, mae: 0.041032, mean_q: 1.169416
 70825/100000: episode: 1413, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 58.381, mean reward: 0.584 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.345, 10.098], loss: 0.001528, mae: 0.042647, mean_q: 1.170389
 70925/100000: episode: 1414, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.699, mean reward: 0.587 [0.511, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.549, 10.098], loss: 0.001433, mae: 0.041773, mean_q: 1.173230
 71025/100000: episode: 1415, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 57.011, mean reward: 0.570 [0.511, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.434, 10.133], loss: 0.001453, mae: 0.041618, mean_q: 1.171962
 71125/100000: episode: 1416, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 57.918, mean reward: 0.579 [0.504, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.220, 10.098], loss: 0.001403, mae: 0.041004, mean_q: 1.168391
 71225/100000: episode: 1417, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 57.335, mean reward: 0.573 [0.498, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.294, 10.163], loss: 0.001405, mae: 0.041364, mean_q: 1.166635
 71325/100000: episode: 1418, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 57.612, mean reward: 0.576 [0.511, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.353, 10.098], loss: 0.001458, mae: 0.041954, mean_q: 1.171078
 71425/100000: episode: 1419, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 58.144, mean reward: 0.581 [0.499, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.173, 10.135], loss: 0.001442, mae: 0.041532, mean_q: 1.167091
 71525/100000: episode: 1420, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 57.653, mean reward: 0.577 [0.498, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.319, 10.098], loss: 0.001388, mae: 0.041388, mean_q: 1.164125
 71625/100000: episode: 1421, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 57.725, mean reward: 0.577 [0.502, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.013, 10.317], loss: 0.001335, mae: 0.040198, mean_q: 1.160783
 71725/100000: episode: 1422, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 61.391, mean reward: 0.614 [0.510, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.502, 10.320], loss: 0.001450, mae: 0.041440, mean_q: 1.163847
 71825/100000: episode: 1423, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.565, mean reward: 0.586 [0.515, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.467, 10.098], loss: 0.001467, mae: 0.041801, mean_q: 1.165164
 71925/100000: episode: 1424, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 56.028, mean reward: 0.560 [0.501, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.520, 10.107], loss: 0.001427, mae: 0.041276, mean_q: 1.163754
 72025/100000: episode: 1425, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 57.138, mean reward: 0.571 [0.508, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.367, 10.098], loss: 0.001411, mae: 0.040951, mean_q: 1.160799
 72125/100000: episode: 1426, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 60.551, mean reward: 0.606 [0.505, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.038, 10.208], loss: 0.001342, mae: 0.039749, mean_q: 1.161199
 72225/100000: episode: 1427, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.840, mean reward: 0.588 [0.517, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.533, 10.098], loss: 0.001288, mae: 0.039753, mean_q: 1.160813
 72325/100000: episode: 1428, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 58.699, mean reward: 0.587 [0.505, 0.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.930, 10.181], loss: 0.001345, mae: 0.040337, mean_q: 1.160489
 72425/100000: episode: 1429, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 60.196, mean reward: 0.602 [0.507, 0.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.316, 10.098], loss: 0.001502, mae: 0.042612, mean_q: 1.161821
 72525/100000: episode: 1430, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 60.622, mean reward: 0.606 [0.501, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.517, 10.210], loss: 0.001373, mae: 0.040628, mean_q: 1.165038
 72625/100000: episode: 1431, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 59.984, mean reward: 0.600 [0.513, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.914, 10.098], loss: 0.001296, mae: 0.039176, mean_q: 1.166103
 72725/100000: episode: 1432, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.803, mean reward: 0.588 [0.501, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.575, 10.315], loss: 0.001339, mae: 0.040112, mean_q: 1.165193
 72825/100000: episode: 1433, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 59.113, mean reward: 0.591 [0.504, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.861, 10.100], loss: 0.001404, mae: 0.040885, mean_q: 1.165574
 72925/100000: episode: 1434, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 59.120, mean reward: 0.591 [0.501, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.666, 10.335], loss: 0.001433, mae: 0.041268, mean_q: 1.164818
 73025/100000: episode: 1435, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 63.415, mean reward: 0.634 [0.511, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.202, 10.098], loss: 0.001400, mae: 0.041378, mean_q: 1.169040
 73125/100000: episode: 1436, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 59.202, mean reward: 0.592 [0.514, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.838, 10.109], loss: 0.001383, mae: 0.040804, mean_q: 1.167252
 73225/100000: episode: 1437, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 58.710, mean reward: 0.587 [0.504, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.831, 10.306], loss: 0.001320, mae: 0.039982, mean_q: 1.169464
 73325/100000: episode: 1438, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 56.357, mean reward: 0.564 [0.502, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.697, 10.102], loss: 0.001350, mae: 0.040032, mean_q: 1.167296
 73425/100000: episode: 1439, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 57.631, mean reward: 0.576 [0.504, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.930, 10.098], loss: 0.001335, mae: 0.040099, mean_q: 1.166290
 73525/100000: episode: 1440, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.278, mean reward: 0.573 [0.505, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.974, 10.103], loss: 0.001394, mae: 0.040689, mean_q: 1.166679
 73625/100000: episode: 1441, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 57.820, mean reward: 0.578 [0.507, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.611, 10.098], loss: 0.001326, mae: 0.039836, mean_q: 1.164259
 73725/100000: episode: 1442, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 59.394, mean reward: 0.594 [0.507, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.872, 10.098], loss: 0.001413, mae: 0.041238, mean_q: 1.166373
 73825/100000: episode: 1443, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.977, mean reward: 0.590 [0.502, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.768, 10.343], loss: 0.001436, mae: 0.041540, mean_q: 1.162029
 73925/100000: episode: 1444, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 61.994, mean reward: 0.620 [0.506, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.690, 10.434], loss: 0.001313, mae: 0.039482, mean_q: 1.163565
 74025/100000: episode: 1445, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 59.064, mean reward: 0.591 [0.503, 0.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.913, 10.098], loss: 0.001368, mae: 0.040326, mean_q: 1.165037
 74125/100000: episode: 1446, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 61.488, mean reward: 0.615 [0.499, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.717, 10.260], loss: 0.001561, mae: 0.042665, mean_q: 1.164551
 74225/100000: episode: 1447, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 61.696, mean reward: 0.617 [0.505, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.384, 10.339], loss: 0.001491, mae: 0.042248, mean_q: 1.165005
 74325/100000: episode: 1448, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 59.354, mean reward: 0.594 [0.505, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.286, 10.098], loss: 0.001423, mae: 0.040838, mean_q: 1.165886
 74425/100000: episode: 1449, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 57.977, mean reward: 0.580 [0.504, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.277, 10.202], loss: 0.001486, mae: 0.042241, mean_q: 1.165757
[Info] 1-TH LEVEL FOUND: 1.35548734664917, Considering 10/90 traces
 74525/100000: episode: 1450, duration: 4.753s, episode steps: 100, steps per second: 21, episode reward: 59.715, mean reward: 0.597 [0.509, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.204, 10.098], loss: 0.001510, mae: 0.041960, mean_q: 1.166045
 74551/100000: episode: 1451, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 16.674, mean reward: 0.641 [0.554, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.204, 10.279], loss: 0.001220, mae: 0.038888, mean_q: 1.171788
 74573/100000: episode: 1452, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 14.643, mean reward: 0.666 [0.586, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.092, 10.282], loss: 0.001283, mae: 0.039142, mean_q: 1.165325
 74668/100000: episode: 1453, duration: 0.536s, episode steps: 95, steps per second: 177, episode reward: 55.384, mean reward: 0.583 [0.508, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-1.594, 10.125], loss: 0.001500, mae: 0.042276, mean_q: 1.169653
 74762/100000: episode: 1454, duration: 0.478s, episode steps: 94, steps per second: 197, episode reward: 57.880, mean reward: 0.616 [0.505, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.513 [-0.763, 10.380], loss: 0.001477, mae: 0.041751, mean_q: 1.163882
 74856/100000: episode: 1455, duration: 0.477s, episode steps: 94, steps per second: 197, episode reward: 57.264, mean reward: 0.609 [0.509, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-0.549, 10.100], loss: 0.001592, mae: 0.042820, mean_q: 1.167168
 74867/100000: episode: 1456, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 7.586, mean reward: 0.690 [0.653, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.374], loss: 0.001471, mae: 0.042241, mean_q: 1.172216
 74889/100000: episode: 1457, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 13.891, mean reward: 0.631 [0.575, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.503, 10.279], loss: 0.001597, mae: 0.042913, mean_q: 1.164083
 74914/100000: episode: 1458, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 15.806, mean reward: 0.632 [0.571, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.267, 10.100], loss: 0.001727, mae: 0.044863, mean_q: 1.169842
 75009/100000: episode: 1459, duration: 0.488s, episode steps: 95, steps per second: 195, episode reward: 56.772, mean reward: 0.598 [0.509, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-1.671, 10.100], loss: 0.001777, mae: 0.045085, mean_q: 1.171556
 75034/100000: episode: 1460, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 15.940, mean reward: 0.638 [0.564, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.153, 10.100], loss: 0.001597, mae: 0.043380, mean_q: 1.164484
 75062/100000: episode: 1461, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 17.997, mean reward: 0.643 [0.573, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.165, 10.100], loss: 0.001736, mae: 0.043979, mean_q: 1.171839
 75090/100000: episode: 1462, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 19.494, mean reward: 0.696 [0.623, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.371, 10.100], loss: 0.001505, mae: 0.042456, mean_q: 1.170441
 75105/100000: episode: 1463, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 9.343, mean reward: 0.623 [0.574, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-1.528, 10.285], loss: 0.001723, mae: 0.043656, mean_q: 1.166162
 75116/100000: episode: 1464, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 7.312, mean reward: 0.665 [0.622, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.255, 10.442], loss: 0.001459, mae: 0.041490, mean_q: 1.167217
 75153/100000: episode: 1465, duration: 0.194s, episode steps: 37, steps per second: 190, episode reward: 23.680, mean reward: 0.640 [0.591, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.824, 10.100], loss: 0.001610, mae: 0.043188, mean_q: 1.170697
 75247/100000: episode: 1466, duration: 0.487s, episode steps: 94, steps per second: 193, episode reward: 55.723, mean reward: 0.593 [0.505, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-1.050, 10.100], loss: 0.001581, mae: 0.042510, mean_q: 1.170317
 75342/100000: episode: 1467, duration: 0.504s, episode steps: 95, steps per second: 189, episode reward: 55.566, mean reward: 0.585 [0.500, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.080, 10.100], loss: 0.001603, mae: 0.042734, mean_q: 1.170353
 75437/100000: episode: 1468, duration: 0.485s, episode steps: 95, steps per second: 196, episode reward: 55.803, mean reward: 0.587 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-0.571, 10.176], loss: 0.001529, mae: 0.043081, mean_q: 1.169507
 75462/100000: episode: 1469, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 15.801, mean reward: 0.632 [0.502, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.056, 10.147], loss: 0.001614, mae: 0.043671, mean_q: 1.166227
 75556/100000: episode: 1470, duration: 0.475s, episode steps: 94, steps per second: 198, episode reward: 55.151, mean reward: 0.587 [0.499, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.507 [-0.769, 10.100], loss: 0.001488, mae: 0.041499, mean_q: 1.171273
 75582/100000: episode: 1471, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 21.165, mean reward: 0.814 [0.674, 0.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.113, 10.579], loss: 0.001520, mae: 0.041021, mean_q: 1.172051
 75677/100000: episode: 1472, duration: 0.492s, episode steps: 95, steps per second: 193, episode reward: 57.966, mean reward: 0.610 [0.514, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.226, 10.504], loss: 0.001565, mae: 0.042224, mean_q: 1.170152
 75688/100000: episode: 1473, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 7.591, mean reward: 0.690 [0.644, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.050, 10.458], loss: 0.001638, mae: 0.043727, mean_q: 1.177539
 75787/100000: episode: 1474, duration: 0.486s, episode steps: 99, steps per second: 204, episode reward: 56.508, mean reward: 0.571 [0.500, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-0.746, 10.100], loss: 0.001568, mae: 0.042795, mean_q: 1.172119
 75809/100000: episode: 1475, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 16.268, mean reward: 0.739 [0.669, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.366, 10.498], loss: 0.001562, mae: 0.043583, mean_q: 1.168779
 75908/100000: episode: 1476, duration: 0.528s, episode steps: 99, steps per second: 188, episode reward: 59.150, mean reward: 0.597 [0.507, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.618, 10.100], loss: 0.001541, mae: 0.042650, mean_q: 1.175108
 75945/100000: episode: 1477, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 23.281, mean reward: 0.629 [0.536, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.219, 10.100], loss: 0.001467, mae: 0.041059, mean_q: 1.171460
 76040/100000: episode: 1478, duration: 0.509s, episode steps: 95, steps per second: 187, episode reward: 54.326, mean reward: 0.572 [0.502, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.931, 10.174], loss: 0.001680, mae: 0.043901, mean_q: 1.176488
 76068/100000: episode: 1479, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 19.136, mean reward: 0.683 [0.600, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.245, 10.100], loss: 0.001647, mae: 0.043486, mean_q: 1.173755
 76162/100000: episode: 1480, duration: 0.496s, episode steps: 94, steps per second: 189, episode reward: 55.266, mean reward: 0.588 [0.508, 0.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.521 [-1.085, 10.234], loss: 0.001530, mae: 0.041826, mean_q: 1.176621
 76184/100000: episode: 1481, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 14.640, mean reward: 0.665 [0.614, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.393], loss: 0.001587, mae: 0.042390, mean_q: 1.177838
 76199/100000: episode: 1482, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 10.241, mean reward: 0.683 [0.633, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.386], loss: 0.001719, mae: 0.044416, mean_q: 1.165024
 76221/100000: episode: 1483, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 15.565, mean reward: 0.707 [0.554, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.243], loss: 0.001579, mae: 0.043579, mean_q: 1.177350
 76249/100000: episode: 1484, duration: 0.135s, episode steps: 28, steps per second: 207, episode reward: 17.412, mean reward: 0.622 [0.512, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.110, 10.100], loss: 0.001622, mae: 0.043587, mean_q: 1.171606
 76264/100000: episode: 1485, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 10.092, mean reward: 0.673 [0.623, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.083, 10.392], loss: 0.001310, mae: 0.038758, mean_q: 1.179799
 76358/100000: episode: 1486, duration: 0.487s, episode steps: 94, steps per second: 193, episode reward: 54.932, mean reward: 0.584 [0.506, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.318, 10.204], loss: 0.001506, mae: 0.041305, mean_q: 1.175733
 76383/100000: episode: 1487, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 15.846, mean reward: 0.634 [0.508, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.597, 10.118], loss: 0.001477, mae: 0.042085, mean_q: 1.177168
 76405/100000: episode: 1488, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 15.221, mean reward: 0.692 [0.642, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.421], loss: 0.001425, mae: 0.039699, mean_q: 1.182620
 76420/100000: episode: 1489, duration: 0.090s, episode steps: 15, steps per second: 168, episode reward: 9.384, mean reward: 0.626 [0.547, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.212], loss: 0.001673, mae: 0.044192, mean_q: 1.180217
 76442/100000: episode: 1490, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 15.650, mean reward: 0.711 [0.651, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.563, 10.434], loss: 0.001407, mae: 0.041379, mean_q: 1.179919
 76536/100000: episode: 1491, duration: 0.466s, episode steps: 94, steps per second: 202, episode reward: 55.981, mean reward: 0.596 [0.511, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.509 [-1.363, 10.164], loss: 0.001701, mae: 0.044201, mean_q: 1.180848
 76562/100000: episode: 1492, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 18.241, mean reward: 0.702 [0.608, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.378], loss: 0.001614, mae: 0.043437, mean_q: 1.182452
 76656/100000: episode: 1493, duration: 0.510s, episode steps: 94, steps per second: 184, episode reward: 57.847, mean reward: 0.615 [0.517, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-1.303, 10.100], loss: 0.001568, mae: 0.042511, mean_q: 1.184721
 76667/100000: episode: 1494, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 7.656, mean reward: 0.696 [0.634, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.495], loss: 0.001560, mae: 0.042517, mean_q: 1.172876
 76704/100000: episode: 1495, duration: 0.202s, episode steps: 37, steps per second: 184, episode reward: 24.637, mean reward: 0.666 [0.607, 0.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.312, 10.100], loss: 0.001671, mae: 0.043840, mean_q: 1.178840
 76729/100000: episode: 1496, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 14.984, mean reward: 0.599 [0.528, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.154, 10.100], loss: 0.001622, mae: 0.043310, mean_q: 1.179602
 76823/100000: episode: 1497, duration: 0.465s, episode steps: 94, steps per second: 202, episode reward: 54.923, mean reward: 0.584 [0.517, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.501, 10.200], loss: 0.001661, mae: 0.043751, mean_q: 1.181550
 76849/100000: episode: 1498, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 18.101, mean reward: 0.696 [0.590, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.492, 10.294], loss: 0.001587, mae: 0.042684, mean_q: 1.184322
 76875/100000: episode: 1499, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 19.515, mean reward: 0.751 [0.631, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.267, 10.441], loss: 0.001646, mae: 0.044136, mean_q: 1.193913
 76901/100000: episode: 1500, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 19.141, mean reward: 0.736 [0.651, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.597], loss: 0.001415, mae: 0.040880, mean_q: 1.188975
 77000/100000: episode: 1501, duration: 0.488s, episode steps: 99, steps per second: 203, episode reward: 59.469, mean reward: 0.601 [0.507, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-0.557, 10.100], loss: 0.001529, mae: 0.041996, mean_q: 1.188084
 77025/100000: episode: 1502, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 16.670, mean reward: 0.667 [0.608, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.200, 10.100], loss: 0.001486, mae: 0.042060, mean_q: 1.187435
 77050/100000: episode: 1503, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 16.499, mean reward: 0.660 [0.595, 0.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.608, 10.100], loss: 0.001719, mae: 0.044689, mean_q: 1.196475
 77061/100000: episode: 1504, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 7.686, mean reward: 0.699 [0.651, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.426], loss: 0.001533, mae: 0.042358, mean_q: 1.195963
 77155/100000: episode: 1505, duration: 0.470s, episode steps: 94, steps per second: 200, episode reward: 58.113, mean reward: 0.618 [0.504, 0.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.518 [-0.655, 10.351], loss: 0.001597, mae: 0.043308, mean_q: 1.191365
 77192/100000: episode: 1506, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 23.337, mean reward: 0.631 [0.532, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.593, 10.216], loss: 0.001580, mae: 0.042144, mean_q: 1.192258
 77203/100000: episode: 1507, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 7.796, mean reward: 0.709 [0.651, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.404, 10.407], loss: 0.001750, mae: 0.043273, mean_q: 1.181247
 77225/100000: episode: 1508, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 15.065, mean reward: 0.685 [0.584, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.584, 10.356], loss: 0.001523, mae: 0.041826, mean_q: 1.195814
 77253/100000: episode: 1509, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 19.435, mean reward: 0.694 [0.618, 0.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.381, 10.100], loss: 0.001560, mae: 0.041894, mean_q: 1.192912
 77264/100000: episode: 1510, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 6.860, mean reward: 0.624 [0.569, 0.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.370], loss: 0.002005, mae: 0.048678, mean_q: 1.193852
 77289/100000: episode: 1511, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 15.195, mean reward: 0.608 [0.523, 0.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.192, 10.100], loss: 0.001770, mae: 0.044154, mean_q: 1.192873
 77300/100000: episode: 1512, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 8.001, mean reward: 0.727 [0.684, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.530], loss: 0.001597, mae: 0.043059, mean_q: 1.189648
 77322/100000: episode: 1513, duration: 0.107s, episode steps: 22, steps per second: 207, episode reward: 14.585, mean reward: 0.663 [0.537, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.259], loss: 0.001630, mae: 0.042257, mean_q: 1.192337
 77359/100000: episode: 1514, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 24.493, mean reward: 0.662 [0.526, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.782, 10.100], loss: 0.001479, mae: 0.042305, mean_q: 1.196574
 77387/100000: episode: 1515, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 17.461, mean reward: 0.624 [0.547, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.266, 10.100], loss: 0.001472, mae: 0.042418, mean_q: 1.193700
 77415/100000: episode: 1516, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 17.802, mean reward: 0.636 [0.581, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.218, 10.100], loss: 0.001525, mae: 0.041308, mean_q: 1.190274
 77437/100000: episode: 1517, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 15.195, mean reward: 0.691 [0.612, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.903, 10.371], loss: 0.001474, mae: 0.041026, mean_q: 1.188284
 77463/100000: episode: 1518, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 18.709, mean reward: 0.720 [0.663, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.482], loss: 0.001707, mae: 0.044815, mean_q: 1.199217
 77500/100000: episode: 1519, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 26.165, mean reward: 0.707 [0.631, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.395, 10.100], loss: 0.001717, mae: 0.042868, mean_q: 1.192996
 77525/100000: episode: 1520, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 15.766, mean reward: 0.631 [0.564, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.201, 10.100], loss: 0.001733, mae: 0.045164, mean_q: 1.201705
 77547/100000: episode: 1521, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 15.500, mean reward: 0.705 [0.606, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.494, 10.416], loss: 0.001573, mae: 0.040848, mean_q: 1.194274
 77575/100000: episode: 1522, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 17.973, mean reward: 0.642 [0.581, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.249, 10.100], loss: 0.001664, mae: 0.043888, mean_q: 1.201780
 77612/100000: episode: 1523, duration: 0.195s, episode steps: 37, steps per second: 189, episode reward: 25.221, mean reward: 0.682 [0.614, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.899, 10.100], loss: 0.001474, mae: 0.041951, mean_q: 1.205553
 77637/100000: episode: 1524, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 16.669, mean reward: 0.667 [0.542, 0.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.730, 10.100], loss: 0.001636, mae: 0.043835, mean_q: 1.209768
 77663/100000: episode: 1525, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 16.783, mean reward: 0.646 [0.582, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.810, 10.283], loss: 0.001458, mae: 0.041360, mean_q: 1.199568
 77678/100000: episode: 1526, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 9.752, mean reward: 0.650 [0.602, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.364, 10.386], loss: 0.001608, mae: 0.043177, mean_q: 1.209058
 77706/100000: episode: 1527, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 18.921, mean reward: 0.676 [0.595, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.873, 10.100], loss: 0.001571, mae: 0.042081, mean_q: 1.204133
 77734/100000: episode: 1528, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 21.255, mean reward: 0.759 [0.611, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.495, 10.100], loss: 0.001691, mae: 0.044478, mean_q: 1.211328
 77745/100000: episode: 1529, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 7.230, mean reward: 0.657 [0.615, 0.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.304], loss: 0.001365, mae: 0.040419, mean_q: 1.199306
 77767/100000: episode: 1530, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 15.560, mean reward: 0.707 [0.642, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.071, 10.462], loss: 0.001613, mae: 0.043102, mean_q: 1.218917
 77866/100000: episode: 1531, duration: 0.510s, episode steps: 99, steps per second: 194, episode reward: 56.993, mean reward: 0.576 [0.506, 0.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.426, 10.100], loss: 0.001563, mae: 0.042922, mean_q: 1.201547
 77894/100000: episode: 1532, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 18.388, mean reward: 0.657 [0.583, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.160, 10.100], loss: 0.001842, mae: 0.045554, mean_q: 1.200775
 77905/100000: episode: 1533, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 7.112, mean reward: 0.647 [0.577, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.241], loss: 0.001501, mae: 0.042803, mean_q: 1.214349
 77933/100000: episode: 1534, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 19.100, mean reward: 0.682 [0.595, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.673, 10.100], loss: 0.001676, mae: 0.044100, mean_q: 1.210475
 78028/100000: episode: 1535, duration: 0.481s, episode steps: 95, steps per second: 197, episode reward: 54.397, mean reward: 0.573 [0.503, 0.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.105, 10.248], loss: 0.001501, mae: 0.042151, mean_q: 1.205444
 78054/100000: episode: 1536, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 18.732, mean reward: 0.720 [0.648, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.233, 10.443], loss: 0.001669, mae: 0.043741, mean_q: 1.197811
 78076/100000: episode: 1537, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 15.134, mean reward: 0.688 [0.562, 0.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.259], loss: 0.001566, mae: 0.044162, mean_q: 1.205436
 78098/100000: episode: 1538, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 14.050, mean reward: 0.639 [0.533, 0.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.338], loss: 0.001728, mae: 0.044396, mean_q: 1.212638
 78135/100000: episode: 1539, duration: 0.196s, episode steps: 37, steps per second: 188, episode reward: 23.412, mean reward: 0.633 [0.510, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.802, 10.148], loss: 0.001442, mae: 0.041537, mean_q: 1.217273
[Info] 2-TH LEVEL FOUND: 1.4761396646499634, Considering 10/90 traces
 78146/100000: episode: 1540, duration: 4.241s, episode steps: 11, steps per second: 3, episode reward: 8.106, mean reward: 0.737 [0.686, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.541], loss: 0.001540, mae: 0.041640, mean_q: 1.207647
 78163/100000: episode: 1541, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 12.895, mean reward: 0.759 [0.721, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.642, 10.470], loss: 0.001393, mae: 0.040913, mean_q: 1.212930
 78180/100000: episode: 1542, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 12.056, mean reward: 0.709 [0.616, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.290], loss: 0.001516, mae: 0.042966, mean_q: 1.208815
 78199/100000: episode: 1543, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 14.848, mean reward: 0.781 [0.708, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.481], loss: 0.001548, mae: 0.041419, mean_q: 1.218342
 78218/100000: episode: 1544, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 14.422, mean reward: 0.759 [0.693, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.004, 10.100], loss: 0.001402, mae: 0.040337, mean_q: 1.211658
 78239/100000: episode: 1545, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 14.436, mean reward: 0.687 [0.602, 0.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.320, 10.345], loss: 0.001493, mae: 0.040274, mean_q: 1.210999
 78257/100000: episode: 1546, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 13.784, mean reward: 0.766 [0.664, 0.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.548], loss: 0.001453, mae: 0.041057, mean_q: 1.198536
 78276/100000: episode: 1547, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 14.536, mean reward: 0.765 [0.679, 0.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-1.224, 10.563], loss: 0.001622, mae: 0.042872, mean_q: 1.209107
 78297/100000: episode: 1548, duration: 0.102s, episode steps: 21, steps per second: 205, episode reward: 15.160, mean reward: 0.722 [0.689, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.778, 10.440], loss: 0.001550, mae: 0.041885, mean_q: 1.218219
 78318/100000: episode: 1549, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 14.588, mean reward: 0.695 [0.636, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.432], loss: 0.001606, mae: 0.044276, mean_q: 1.215588
 78336/100000: episode: 1550, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 12.582, mean reward: 0.699 [0.656, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.341], loss: 0.001526, mae: 0.042945, mean_q: 1.213668
 78352/100000: episode: 1551, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 11.658, mean reward: 0.729 [0.618, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.293, 10.443], loss: 0.001307, mae: 0.039968, mean_q: 1.231665
 78368/100000: episode: 1552, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 11.447, mean reward: 0.715 [0.656, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.547], loss: 0.001640, mae: 0.043653, mean_q: 1.219628
 78387/100000: episode: 1553, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 13.887, mean reward: 0.731 [0.674, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-1.015, 10.507], loss: 0.001888, mae: 0.047028, mean_q: 1.221010
 78408/100000: episode: 1554, duration: 0.132s, episode steps: 21, steps per second: 159, episode reward: 16.278, mean reward: 0.775 [0.680, 0.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.473], loss: 0.001660, mae: 0.043955, mean_q: 1.209624
 78427/100000: episode: 1555, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 12.886, mean reward: 0.678 [0.613, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.683, 10.363], loss: 0.001670, mae: 0.043999, mean_q: 1.225505
 78445/100000: episode: 1556, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 12.577, mean reward: 0.699 [0.629, 0.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-1.046, 10.413], loss: 0.001669, mae: 0.044625, mean_q: 1.222865
 78464/100000: episode: 1557, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 14.037, mean reward: 0.739 [0.651, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.898, 10.423], loss: 0.001727, mae: 0.044449, mean_q: 1.228481
 78483/100000: episode: 1558, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 12.446, mean reward: 0.655 [0.572, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.071, 10.234], loss: 0.001320, mae: 0.039280, mean_q: 1.219874
 78502/100000: episode: 1559, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 12.573, mean reward: 0.662 [0.558, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.219, 10.272], loss: 0.001415, mae: 0.040679, mean_q: 1.219015
 78521/100000: episode: 1560, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 14.193, mean reward: 0.747 [0.675, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.417, 10.100], loss: 0.001480, mae: 0.041775, mean_q: 1.223178
 78542/100000: episode: 1561, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 16.469, mean reward: 0.784 [0.655, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.409], loss: 0.001512, mae: 0.042238, mean_q: 1.221256
 78561/100000: episode: 1562, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 13.176, mean reward: 0.693 [0.639, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.747, 10.418], loss: 0.001694, mae: 0.044090, mean_q: 1.220536
 78580/100000: episode: 1563, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 14.169, mean reward: 0.746 [0.684, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.561, 10.100], loss: 0.001634, mae: 0.043245, mean_q: 1.226943
 78599/100000: episode: 1564, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 13.479, mean reward: 0.709 [0.627, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.514, 10.499], loss: 0.001616, mae: 0.042447, mean_q: 1.228853
 78618/100000: episode: 1565, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 13.320, mean reward: 0.701 [0.613, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.400], loss: 0.001621, mae: 0.043894, mean_q: 1.240228
 78636/100000: episode: 1566, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 12.280, mean reward: 0.682 [0.635, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.394], loss: 0.001438, mae: 0.040895, mean_q: 1.221301
 78655/100000: episode: 1567, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 13.672, mean reward: 0.720 [0.623, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.199, 10.409], loss: 0.001709, mae: 0.043464, mean_q: 1.230883
 78671/100000: episode: 1568, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 11.861, mean reward: 0.741 [0.693, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.637, 10.443], loss: 0.001265, mae: 0.039137, mean_q: 1.222260
 78690/100000: episode: 1569, duration: 0.095s, episode steps: 19, steps per second: 201, episode reward: 14.494, mean reward: 0.763 [0.693, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.558], loss: 0.001332, mae: 0.038851, mean_q: 1.234404
 78709/100000: episode: 1570, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 14.625, mean reward: 0.770 [0.727, 0.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.328, 10.100], loss: 0.001727, mae: 0.043916, mean_q: 1.236513
 78730/100000: episode: 1571, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 14.410, mean reward: 0.686 [0.593, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.108, 10.317], loss: 0.001488, mae: 0.041944, mean_q: 1.230562
 78747/100000: episode: 1572, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 12.751, mean reward: 0.750 [0.645, 0.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.464], loss: 0.001295, mae: 0.040081, mean_q: 1.246808
 78768/100000: episode: 1573, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 15.602, mean reward: 0.743 [0.714, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.523], loss: 0.001469, mae: 0.041131, mean_q: 1.234599
 78787/100000: episode: 1574, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 14.072, mean reward: 0.741 [0.652, 0.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.403, 10.100], loss: 0.001549, mae: 0.042997, mean_q: 1.235208
 78808/100000: episode: 1575, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 17.535, mean reward: 0.835 [0.670, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-1.505, 10.607], loss: 0.001427, mae: 0.041254, mean_q: 1.234103
 78829/100000: episode: 1576, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 14.373, mean reward: 0.684 [0.593, 0.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.330], loss: 0.001642, mae: 0.043443, mean_q: 1.217241
 78847/100000: episode: 1577, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 13.020, mean reward: 0.723 [0.650, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.511], loss: 0.001585, mae: 0.043009, mean_q: 1.224667
 78868/100000: episode: 1578, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 15.766, mean reward: 0.751 [0.699, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.105, 10.513], loss: 0.001324, mae: 0.040350, mean_q: 1.224212
 78887/100000: episode: 1579, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 12.744, mean reward: 0.671 [0.578, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.092, 10.320], loss: 0.001685, mae: 0.044246, mean_q: 1.241720
 78906/100000: episode: 1580, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 14.359, mean reward: 0.756 [0.704, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.278, 10.525], loss: 0.001442, mae: 0.042034, mean_q: 1.246670
 78927/100000: episode: 1581, duration: 0.119s, episode steps: 21, steps per second: 176, episode reward: 15.180, mean reward: 0.723 [0.673, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.037, 10.478], loss: 0.001388, mae: 0.041228, mean_q: 1.232494
 78939/100000: episode: 1582, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 9.746, mean reward: 0.812 [0.764, 0.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.437, 10.100], loss: 0.001715, mae: 0.046525, mean_q: 1.241488
 78960/100000: episode: 1583, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 16.268, mean reward: 0.775 [0.717, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.208, 10.539], loss: 0.001327, mae: 0.039911, mean_q: 1.229872
 78976/100000: episode: 1584, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 11.852, mean reward: 0.741 [0.693, 0.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.729, 10.498], loss: 0.001562, mae: 0.042343, mean_q: 1.246178
 78997/100000: episode: 1585, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 15.041, mean reward: 0.716 [0.682, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.175, 10.387], loss: 0.001315, mae: 0.041082, mean_q: 1.241673
 79016/100000: episode: 1586, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 14.520, mean reward: 0.764 [0.686, 0.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-1.621, 10.100], loss: 0.001451, mae: 0.039777, mean_q: 1.238868
 79037/100000: episode: 1587, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 14.058, mean reward: 0.669 [0.578, 0.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.254, 10.234], loss: 0.001226, mae: 0.038605, mean_q: 1.243963
 79053/100000: episode: 1588, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 12.668, mean reward: 0.792 [0.738, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.434, 10.575], loss: 0.001415, mae: 0.041490, mean_q: 1.238689
 79071/100000: episode: 1589, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 11.682, mean reward: 0.649 [0.549, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.342], loss: 0.001478, mae: 0.041048, mean_q: 1.242246
 79090/100000: episode: 1590, duration: 0.092s, episode steps: 19, steps per second: 206, episode reward: 14.435, mean reward: 0.760 [0.687, 0.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.451, 10.100], loss: 0.001344, mae: 0.040204, mean_q: 1.237552
 79109/100000: episode: 1591, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 13.249, mean reward: 0.697 [0.645, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.505], loss: 0.001374, mae: 0.040569, mean_q: 1.252299
 79125/100000: episode: 1592, duration: 0.097s, episode steps: 16, steps per second: 166, episode reward: 11.568, mean reward: 0.723 [0.667, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.471], loss: 0.001328, mae: 0.040150, mean_q: 1.257596
 79144/100000: episode: 1593, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 13.800, mean reward: 0.726 [0.650, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.444, 10.100], loss: 0.001239, mae: 0.039386, mean_q: 1.257263
 79163/100000: episode: 1594, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 13.173, mean reward: 0.693 [0.642, 0.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.573, 10.433], loss: 0.001300, mae: 0.039382, mean_q: 1.247170
 79179/100000: episode: 1595, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 11.396, mean reward: 0.712 [0.588, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.050, 10.317], loss: 0.001530, mae: 0.042742, mean_q: 1.256973
 79197/100000: episode: 1596, duration: 0.099s, episode steps: 18, steps per second: 183, episode reward: 11.935, mean reward: 0.663 [0.580, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.358], loss: 0.001363, mae: 0.040393, mean_q: 1.260065
 79216/100000: episode: 1597, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 14.301, mean reward: 0.753 [0.680, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.208, 10.499], loss: 0.001283, mae: 0.038624, mean_q: 1.239547
 79237/100000: episode: 1598, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 15.173, mean reward: 0.723 [0.540, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.818, 10.277], loss: 0.001239, mae: 0.039158, mean_q: 1.256235
 79256/100000: episode: 1599, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 14.667, mean reward: 0.772 [0.687, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.229, 10.518], loss: 0.001252, mae: 0.038460, mean_q: 1.250886
 79268/100000: episode: 1600, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 9.517, mean reward: 0.793 [0.745, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.545, 10.100], loss: 0.001675, mae: 0.041839, mean_q: 1.260073
 79289/100000: episode: 1601, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 14.470, mean reward: 0.689 [0.604, 0.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.967, 10.341], loss: 0.001482, mae: 0.040621, mean_q: 1.252883
 79301/100000: episode: 1602, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 9.840, mean reward: 0.820 [0.772, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.458, 10.100], loss: 0.001401, mae: 0.040893, mean_q: 1.244078
 79322/100000: episode: 1603, duration: 0.125s, episode steps: 21, steps per second: 168, episode reward: 14.824, mean reward: 0.706 [0.616, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.345], loss: 0.001206, mae: 0.037805, mean_q: 1.245953
 79338/100000: episode: 1604, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 10.801, mean reward: 0.675 [0.610, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.445, 10.310], loss: 0.001358, mae: 0.040914, mean_q: 1.257300
 79357/100000: episode: 1605, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 12.734, mean reward: 0.670 [0.554, 0.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.281], loss: 0.001488, mae: 0.041925, mean_q: 1.260987
 79376/100000: episode: 1606, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 14.871, mean reward: 0.783 [0.732, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.165, 10.100], loss: 0.001251, mae: 0.038428, mean_q: 1.253968
 79397/100000: episode: 1607, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 17.087, mean reward: 0.814 [0.738, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.110, 10.517], loss: 0.001506, mae: 0.042840, mean_q: 1.272362
 79416/100000: episode: 1608, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 13.283, mean reward: 0.699 [0.655, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.387], loss: 0.001304, mae: 0.039765, mean_q: 1.261423
 79435/100000: episode: 1609, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 12.735, mean reward: 0.670 [0.609, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.234, 10.428], loss: 0.001205, mae: 0.037638, mean_q: 1.252739
 79454/100000: episode: 1610, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 14.161, mean reward: 0.745 [0.668, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.313, 10.100], loss: 0.001429, mae: 0.040898, mean_q: 1.260678
 79473/100000: episode: 1611, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 13.073, mean reward: 0.688 [0.595, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.362], loss: 0.001453, mae: 0.042587, mean_q: 1.270706
 79489/100000: episode: 1612, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 11.169, mean reward: 0.698 [0.635, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.258, 10.379], loss: 0.001343, mae: 0.041618, mean_q: 1.257332
 79508/100000: episode: 1613, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 14.393, mean reward: 0.758 [0.709, 0.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.201, 10.100], loss: 0.001427, mae: 0.041920, mean_q: 1.248063
 79529/100000: episode: 1614, duration: 0.128s, episode steps: 21, steps per second: 164, episode reward: 15.455, mean reward: 0.736 [0.683, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.472], loss: 0.001527, mae: 0.042225, mean_q: 1.257635
 79548/100000: episode: 1615, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 15.365, mean reward: 0.809 [0.681, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.229, 10.100], loss: 0.001217, mae: 0.039754, mean_q: 1.272332
 79569/100000: episode: 1616, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 16.108, mean reward: 0.767 [0.689, 0.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.834, 10.433], loss: 0.001140, mae: 0.036420, mean_q: 1.265112
 79587/100000: episode: 1617, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 13.287, mean reward: 0.738 [0.683, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-1.608, 10.473], loss: 0.001194, mae: 0.038508, mean_q: 1.274150
 79606/100000: episode: 1618, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 13.691, mean reward: 0.721 [0.638, 0.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.324, 10.100], loss: 0.001167, mae: 0.037987, mean_q: 1.266752
 79622/100000: episode: 1619, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 11.479, mean reward: 0.717 [0.684, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.465], loss: 0.001145, mae: 0.036465, mean_q: 1.256933
 79643/100000: episode: 1620, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 17.334, mean reward: 0.825 [0.733, 0.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.129, 10.529], loss: 0.001386, mae: 0.040030, mean_q: 1.264669
 79664/100000: episode: 1621, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 14.435, mean reward: 0.687 [0.564, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.715, 10.321], loss: 0.001359, mae: 0.041112, mean_q: 1.276331
 79683/100000: episode: 1622, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 13.448, mean reward: 0.708 [0.648, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.498, 10.100], loss: 0.001337, mae: 0.039579, mean_q: 1.261315
 79702/100000: episode: 1623, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 13.636, mean reward: 0.718 [0.625, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.355, 10.422], loss: 0.001309, mae: 0.039577, mean_q: 1.274455
 79721/100000: episode: 1624, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 14.731, mean reward: 0.775 [0.736, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.469, 10.551], loss: 0.001237, mae: 0.039100, mean_q: 1.266236
 79740/100000: episode: 1625, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 13.197, mean reward: 0.695 [0.631, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.231, 10.376], loss: 0.001176, mae: 0.037792, mean_q: 1.281117
 79761/100000: episode: 1626, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 14.147, mean reward: 0.674 [0.595, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.517, 10.340], loss: 0.001167, mae: 0.037825, mean_q: 1.271810
 79782/100000: episode: 1627, duration: 0.125s, episode steps: 21, steps per second: 169, episode reward: 16.950, mean reward: 0.807 [0.729, 0.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.586, 10.478], loss: 0.001349, mae: 0.041164, mean_q: 1.271194
 79803/100000: episode: 1628, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 15.026, mean reward: 0.716 [0.644, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.133, 10.470], loss: 0.001281, mae: 0.039618, mean_q: 1.280295
 79822/100000: episode: 1629, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 14.508, mean reward: 0.764 [0.711, 0.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.720, 10.100], loss: 0.001218, mae: 0.037073, mean_q: 1.278337
[Info] 3-TH LEVEL FOUND: 1.6194236278533936, Considering 10/90 traces
 79841/100000: episode: 1630, duration: 4.454s, episode steps: 19, steps per second: 4, episode reward: 12.910, mean reward: 0.679 [0.602, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.766, 10.320], loss: 0.001453, mae: 0.042350, mean_q: 1.298503
 79856/100000: episode: 1631, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 10.989, mean reward: 0.733 [0.597, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.123, 10.435], loss: 0.001572, mae: 0.043776, mean_q: 1.285715
 79870/100000: episode: 1632, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 11.314, mean reward: 0.808 [0.740, 0.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.594, 10.589], loss: 0.001307, mae: 0.039271, mean_q: 1.273947
 79881/100000: episode: 1633, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 8.993, mean reward: 0.818 [0.760, 0.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.359, 10.100], loss: 0.001332, mae: 0.039334, mean_q: 1.283426
 79887/100000: episode: 1634, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 5.036, mean reward: 0.839 [0.823, 0.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.429, 10.100], loss: 0.001172, mae: 0.037181, mean_q: 1.283765
 79902/100000: episode: 1635, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 13.019, mean reward: 0.868 [0.807, 0.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.761, 10.100], loss: 0.001166, mae: 0.037318, mean_q: 1.275112
 79915/100000: episode: 1636, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 10.718, mean reward: 0.824 [0.795, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.500], loss: 0.001322, mae: 0.038658, mean_q: 1.288410
 79921/100000: episode: 1637, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 4.674, mean reward: 0.779 [0.742, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.393, 10.100], loss: 0.001145, mae: 0.038861, mean_q: 1.295473
 79927/100000: episode: 1638, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 4.798, mean reward: 0.800 [0.769, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.452, 10.100], loss: 0.001254, mae: 0.039178, mean_q: 1.298066
 79937/100000: episode: 1639, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 8.509, mean reward: 0.851 [0.761, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.233, 10.100], loss: 0.001499, mae: 0.041538, mean_q: 1.263818
 79948/100000: episode: 1640, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 8.736, mean reward: 0.794 [0.756, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.432, 10.100], loss: 0.001298, mae: 0.039508, mean_q: 1.299153
 79953/100000: episode: 1641, duration: 0.037s, episode steps: 5, steps per second: 136, episode reward: 4.127, mean reward: 0.825 [0.808, 0.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.483, 10.100], loss: 0.001007, mae: 0.036050, mean_q: 1.266838
 79963/100000: episode: 1642, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 7.985, mean reward: 0.799 [0.746, 0.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.511, 10.100], loss: 0.001342, mae: 0.038398, mean_q: 1.279468
 79968/100000: episode: 1643, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 4.095, mean reward: 0.819 [0.724, 0.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.446, 10.100], loss: 0.001217, mae: 0.039388, mean_q: 1.300989
 79985/100000: episode: 1644, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 14.192, mean reward: 0.835 [0.763, 0.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.283, 10.533], loss: 0.001232, mae: 0.040012, mean_q: 1.272412
 80002/100000: episode: 1645, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 14.310, mean reward: 0.842 [0.758, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.639], loss: 0.001387, mae: 0.040062, mean_q: 1.298102
 80008/100000: episode: 1646, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 5.008, mean reward: 0.835 [0.800, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.395, 10.100], loss: 0.001398, mae: 0.038648, mean_q: 1.281541
 80021/100000: episode: 1647, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 10.888, mean reward: 0.838 [0.734, 0.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.512, 10.496], loss: 0.001256, mae: 0.039900, mean_q: 1.279420
 80027/100000: episode: 1648, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 4.631, mean reward: 0.772 [0.730, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.440, 10.100], loss: 0.001696, mae: 0.047010, mean_q: 1.268922
 80037/100000: episode: 1649, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 7.880, mean reward: 0.788 [0.754, 0.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.512, 10.100], loss: 0.001853, mae: 0.047615, mean_q: 1.280134
 80052/100000: episode: 1650, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 12.586, mean reward: 0.839 [0.789, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.414, 10.100], loss: 0.001297, mae: 0.039187, mean_q: 1.278076
 80067/100000: episode: 1651, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 12.634, mean reward: 0.842 [0.750, 0.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.403, 10.493], loss: 0.001347, mae: 0.040707, mean_q: 1.282373
 80078/100000: episode: 1652, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 8.991, mean reward: 0.817 [0.763, 0.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.830, 10.100], loss: 0.001360, mae: 0.041570, mean_q: 1.286816
 80084/100000: episode: 1653, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 4.615, mean reward: 0.769 [0.724, 0.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.463, 10.100], loss: 0.001325, mae: 0.040175, mean_q: 1.300589
 80099/100000: episode: 1654, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 12.685, mean reward: 0.846 [0.803, 0.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.625], loss: 0.001371, mae: 0.039505, mean_q: 1.275054
 80104/100000: episode: 1655, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 4.179, mean reward: 0.836 [0.783, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.440, 10.100], loss: 0.001403, mae: 0.042581, mean_q: 1.305689
 80114/100000: episode: 1656, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 8.426, mean reward: 0.843 [0.763, 0.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.577, 10.100], loss: 0.001504, mae: 0.042784, mean_q: 1.278962
 80124/100000: episode: 1657, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 8.043, mean reward: 0.804 [0.741, 0.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.556, 10.100], loss: 0.001482, mae: 0.042329, mean_q: 1.279308
 80134/100000: episode: 1658, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 8.185, mean reward: 0.818 [0.736, 0.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.483, 10.100], loss: 0.001334, mae: 0.039601, mean_q: 1.280282
 80145/100000: episode: 1659, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 9.125, mean reward: 0.830 [0.779, 0.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.429, 10.100], loss: 0.001068, mae: 0.036846, mean_q: 1.309875
 80162/100000: episode: 1660, duration: 0.097s, episode steps: 17, steps per second: 174, episode reward: 13.710, mean reward: 0.806 [0.755, 0.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.500], loss: 0.001095, mae: 0.036628, mean_q: 1.292017
 80172/100000: episode: 1661, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 8.676, mean reward: 0.868 [0.794, 0.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.465, 10.100], loss: 0.001419, mae: 0.042154, mean_q: 1.317412
 80178/100000: episode: 1662, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 4.786, mean reward: 0.798 [0.765, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.347, 10.100], loss: 0.001721, mae: 0.044592, mean_q: 1.311476
[Info] FALSIFICATION!
 80191/100000: episode: 1663, duration: 0.299s, episode steps: 13, steps per second: 43, episode reward: 11.308, mean reward: 0.870 [0.804, 1.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.018, 10.796], loss: 0.001239, mae: 0.039131, mean_q: 1.292322
[Info] FALSIFICATION!
 80200/100000: episode: 1664, duration: 0.221s, episode steps: 9, steps per second: 41, episode reward: 8.398, mean reward: 0.933 [0.840, 1.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.538, 10.098], loss: 0.001233, mae: 0.038983, mean_q: 1.298180
 80206/100000: episode: 1665, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 5.374, mean reward: 0.896 [0.835, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.446, 10.100], loss: 0.001192, mae: 0.036006, mean_q: 1.295918
 80216/100000: episode: 1666, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 8.270, mean reward: 0.827 [0.761, 0.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.529, 10.100], loss: 0.001279, mae: 0.038900, mean_q: 1.294087
 80230/100000: episode: 1667, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 10.881, mean reward: 0.777 [0.673, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.466], loss: 0.001486, mae: 0.041929, mean_q: 1.306347
[Info] FALSIFICATION!
 80239/100000: episode: 1668, duration: 0.310s, episode steps: 9, steps per second: 29, episode reward: 8.123, mean reward: 0.903 [0.806, 1.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.061, 10.628], loss: 0.001430, mae: 0.039788, mean_q: 1.310723
 80254/100000: episode: 1669, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 12.193, mean reward: 0.813 [0.763, 0.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.639], loss: 0.001385, mae: 0.041855, mean_q: 1.308657
 80259/100000: episode: 1670, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 4.083, mean reward: 0.817 [0.790, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.393, 10.100], loss: 0.001226, mae: 0.038855, mean_q: 1.310443
 80274/100000: episode: 1671, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 12.625, mean reward: 0.842 [0.770, 0.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.294, 10.100], loss: 0.001409, mae: 0.041107, mean_q: 1.313327
 80287/100000: episode: 1672, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 10.549, mean reward: 0.811 [0.724, 0.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.883, 10.503], loss: 0.001354, mae: 0.041209, mean_q: 1.302823
 80297/100000: episode: 1673, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 8.100, mean reward: 0.810 [0.758, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.468, 10.100], loss: 0.001143, mae: 0.036744, mean_q: 1.317010
 80303/100000: episode: 1674, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 4.619, mean reward: 0.770 [0.748, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.489, 10.100], loss: 0.001477, mae: 0.039871, mean_q: 1.270764
 80314/100000: episode: 1675, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 8.516, mean reward: 0.774 [0.718, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.517, 10.100], loss: 0.001123, mae: 0.037277, mean_q: 1.307021
 80319/100000: episode: 1676, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 3.990, mean reward: 0.798 [0.761, 0.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.457, 10.100], loss: 0.001008, mae: 0.035602, mean_q: 1.310668
 80334/100000: episode: 1677, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 11.490, mean reward: 0.766 [0.729, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.186, 10.453], loss: 0.001907, mae: 0.042264, mean_q: 1.310758
 80344/100000: episode: 1678, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 8.082, mean reward: 0.808 [0.766, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.523, 10.100], loss: 0.001479, mae: 0.042787, mean_q: 1.308032
 80355/100000: episode: 1679, duration: 0.057s, episode steps: 11, steps per second: 191, episode reward: 8.804, mean reward: 0.800 [0.726, 0.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.438, 10.100], loss: 0.001575, mae: 0.039169, mean_q: 1.322104
 80369/100000: episode: 1680, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 11.449, mean reward: 0.818 [0.715, 0.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.595, 10.477], loss: 0.001094, mae: 0.037422, mean_q: 1.304028
 80379/100000: episode: 1681, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 8.206, mean reward: 0.821 [0.783, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.614, 10.100], loss: 0.001871, mae: 0.048344, mean_q: 1.317701
 80396/100000: episode: 1682, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 14.495, mean reward: 0.853 [0.786, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.270, 10.559], loss: 0.001227, mae: 0.038859, mean_q: 1.303308
 80407/100000: episode: 1683, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 8.827, mean reward: 0.802 [0.781, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.332, 10.100], loss: 0.001461, mae: 0.038295, mean_q: 1.317372
 80413/100000: episode: 1684, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 4.482, mean reward: 0.747 [0.711, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.382, 10.100], loss: 0.001179, mae: 0.037097, mean_q: 1.310976
[Info] FALSIFICATION!
 80427/100000: episode: 1685, duration: 0.340s, episode steps: 14, steps per second: 41, episode reward: 12.054, mean reward: 0.861 [0.786, 1.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.947, 10.854], loss: 0.001444, mae: 0.040646, mean_q: 1.331663
 80432/100000: episode: 1686, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 4.006, mean reward: 0.801 [0.778, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.096, 10.100], loss: 0.001410, mae: 0.042570, mean_q: 1.313441
 80443/100000: episode: 1687, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 8.594, mean reward: 0.781 [0.733, 0.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.437, 10.100], loss: 0.001430, mae: 0.042169, mean_q: 1.313126
 80456/100000: episode: 1688, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 9.359, mean reward: 0.720 [0.639, 0.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.378], loss: 0.001424, mae: 0.042080, mean_q: 1.307742
 80466/100000: episode: 1689, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 7.298, mean reward: 0.730 [0.710, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.367, 10.100], loss: 0.001505, mae: 0.042207, mean_q: 1.314320
 80483/100000: episode: 1690, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 13.365, mean reward: 0.786 [0.719, 0.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.094, 10.588], loss: 0.001591, mae: 0.041443, mean_q: 1.310726
 80496/100000: episode: 1691, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 10.188, mean reward: 0.784 [0.733, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.590], loss: 0.001987, mae: 0.042227, mean_q: 1.337928
 80506/100000: episode: 1692, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 7.911, mean reward: 0.791 [0.743, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.933, 10.100], loss: 0.001455, mae: 0.042033, mean_q: 1.325745
 80523/100000: episode: 1693, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 13.939, mean reward: 0.820 [0.773, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.238, 10.577], loss: 0.001555, mae: 0.041510, mean_q: 1.320199
 80529/100000: episode: 1694, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 5.139, mean reward: 0.856 [0.791, 0.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.447, 10.100], loss: 0.001593, mae: 0.045249, mean_q: 1.308567
 80546/100000: episode: 1695, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 14.916, mean reward: 0.877 [0.788, 0.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.421, 10.615], loss: 0.001390, mae: 0.041227, mean_q: 1.318800
 80561/100000: episode: 1696, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 13.358, mean reward: 0.891 [0.822, 0.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.684], loss: 0.001636, mae: 0.045294, mean_q: 1.320203
 80567/100000: episode: 1697, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 5.217, mean reward: 0.870 [0.823, 0.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.524, 10.100], loss: 0.001566, mae: 0.044569, mean_q: 1.345963
 80573/100000: episode: 1698, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 5.024, mean reward: 0.837 [0.773, 0.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.620, 10.100], loss: 0.001277, mae: 0.040344, mean_q: 1.349140
 80583/100000: episode: 1699, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 8.556, mean reward: 0.856 [0.775, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.412, 10.100], loss: 0.001171, mae: 0.036938, mean_q: 1.312629
 80594/100000: episode: 1700, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 8.487, mean reward: 0.772 [0.722, 0.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.270, 10.100], loss: 0.001498, mae: 0.041212, mean_q: 1.345940
 80605/100000: episode: 1701, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 9.018, mean reward: 0.820 [0.762, 0.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.461, 10.100], loss: 0.001197, mae: 0.038961, mean_q: 1.314348
 80615/100000: episode: 1702, duration: 0.064s, episode steps: 10, steps per second: 155, episode reward: 8.400, mean reward: 0.840 [0.773, 0.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.342, 10.100], loss: 0.001304, mae: 0.039231, mean_q: 1.328716
 80620/100000: episode: 1703, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 4.093, mean reward: 0.819 [0.799, 0.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.440, 10.100], loss: 0.001412, mae: 0.041603, mean_q: 1.332085
[Info] FALSIFICATION!
 80625/100000: episode: 1704, duration: 0.292s, episode steps: 5, steps per second: 17, episode reward: 4.436, mean reward: 0.887 [0.792, 1.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.547, 9.841], loss: 0.001578, mae: 0.041997, mean_q: 1.331919
 80640/100000: episode: 1705, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 12.794, mean reward: 0.853 [0.796, 0.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.384, 10.100], loss: 0.001223, mae: 0.038358, mean_q: 1.340560
 80657/100000: episode: 1706, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 14.255, mean reward: 0.839 [0.772, 0.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.665, 10.489], loss: 0.001731, mae: 0.041591, mean_q: 1.348811
 80663/100000: episode: 1707, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 5.066, mean reward: 0.844 [0.812, 0.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.462, 10.100], loss: 0.001546, mae: 0.043880, mean_q: 1.319910
 80673/100000: episode: 1708, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 8.523, mean reward: 0.852 [0.789, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.551, 10.100], loss: 0.001868, mae: 0.044419, mean_q: 1.333556
 80679/100000: episode: 1709, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 4.943, mean reward: 0.824 [0.763, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.505, 10.100], loss: 0.001332, mae: 0.042132, mean_q: 1.308296
 80694/100000: episode: 1710, duration: 0.106s, episode steps: 15, steps per second: 141, episode reward: 12.879, mean reward: 0.859 [0.821, 0.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.540, 10.100], loss: 0.001481, mae: 0.040629, mean_q: 1.327808
 80704/100000: episode: 1711, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 8.537, mean reward: 0.854 [0.779, 0.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.544, 10.100], loss: 0.001462, mae: 0.042044, mean_q: 1.337177
 80719/100000: episode: 1712, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 11.678, mean reward: 0.779 [0.673, 0.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.377, 10.432], loss: 0.001526, mae: 0.040735, mean_q: 1.337798
 80732/100000: episode: 1713, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 10.351, mean reward: 0.796 [0.749, 0.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.538], loss: 0.001143, mae: 0.037903, mean_q: 1.329368
 80745/100000: episode: 1714, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 10.355, mean reward: 0.797 [0.765, 0.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.534], loss: 0.001316, mae: 0.039458, mean_q: 1.333009
 80751/100000: episode: 1715, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 4.790, mean reward: 0.798 [0.785, 0.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.351, 10.100], loss: 0.001196, mae: 0.037993, mean_q: 1.323552
 80761/100000: episode: 1716, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 8.412, mean reward: 0.841 [0.787, 0.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.191, 10.100], loss: 0.001417, mae: 0.041967, mean_q: 1.329141
 80776/100000: episode: 1717, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 12.766, mean reward: 0.851 [0.776, 0.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.495], loss: 0.001405, mae: 0.041915, mean_q: 1.342266
 80787/100000: episode: 1718, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 8.876, mean reward: 0.807 [0.759, 0.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.433, 10.100], loss: 0.001208, mae: 0.038296, mean_q: 1.341617
 80804/100000: episode: 1719, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 15.100, mean reward: 0.888 [0.841, 0.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.260, 10.656], loss: 0.001143, mae: 0.036428, mean_q: 1.331326
[Info] Complete ISplit Iteration
[Info] Levels: [1.3554873, 1.4761397, 1.6194236, 1.6479506]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.3]
[Info] Error Prob: 0.0003000000000000001

 80815/100000: episode: 1720, duration: 4.527s, episode steps: 11, steps per second: 2, episode reward: 8.361, mean reward: 0.760 [0.691, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.300, 10.100], loss: 0.001365, mae: 0.040661, mean_q: 1.331358
 80915/100000: episode: 1721, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 59.516, mean reward: 0.595 [0.501, 0.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.442, 10.098], loss: 0.001572, mae: 0.041874, mean_q: 1.345874
 81015/100000: episode: 1722, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.030, mean reward: 0.580 [0.506, 0.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.692, 10.098], loss: 0.001627, mae: 0.041881, mean_q: 1.337513
 81115/100000: episode: 1723, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 58.690, mean reward: 0.587 [0.502, 0.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.378, 10.395], loss: 0.001413, mae: 0.039322, mean_q: 1.337270
 81215/100000: episode: 1724, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.506, mean reward: 0.585 [0.507, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.862, 10.098], loss: 0.001409, mae: 0.040843, mean_q: 1.333104
 81315/100000: episode: 1725, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.794, mean reward: 0.588 [0.512, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.987, 10.098], loss: 0.001492, mae: 0.040867, mean_q: 1.336457
 81415/100000: episode: 1726, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 59.743, mean reward: 0.597 [0.501, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.102, 10.098], loss: 0.001416, mae: 0.039737, mean_q: 1.338486
 81515/100000: episode: 1727, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.703, mean reward: 0.587 [0.505, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.745, 10.254], loss: 0.001616, mae: 0.041342, mean_q: 1.332313
 81615/100000: episode: 1728, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.562, mean reward: 0.586 [0.504, 0.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.989, 10.169], loss: 0.001615, mae: 0.041455, mean_q: 1.329198
 81715/100000: episode: 1729, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 57.736, mean reward: 0.577 [0.511, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.660, 10.176], loss: 0.001493, mae: 0.040784, mean_q: 1.340259
 81815/100000: episode: 1730, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.580, mean reward: 0.576 [0.502, 0.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.644, 10.200], loss: 0.001499, mae: 0.041376, mean_q: 1.333963
 81915/100000: episode: 1731, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 60.810, mean reward: 0.608 [0.505, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.642, 10.098], loss: 0.001383, mae: 0.040038, mean_q: 1.329221
 82015/100000: episode: 1732, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 59.700, mean reward: 0.597 [0.506, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.750, 10.098], loss: 0.001639, mae: 0.041821, mean_q: 1.325689
 82115/100000: episode: 1733, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.028, mean reward: 0.590 [0.512, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.966, 10.118], loss: 0.001675, mae: 0.043128, mean_q: 1.319578
 82215/100000: episode: 1734, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 59.367, mean reward: 0.594 [0.503, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.231, 10.131], loss: 0.002007, mae: 0.043998, mean_q: 1.327892
 82315/100000: episode: 1735, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 60.874, mean reward: 0.609 [0.500, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.673, 10.098], loss: 0.001816, mae: 0.043658, mean_q: 1.320678
 82415/100000: episode: 1736, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 58.672, mean reward: 0.587 [0.499, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.836, 10.120], loss: 0.001443, mae: 0.041034, mean_q: 1.316914
 82515/100000: episode: 1737, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.801, mean reward: 0.578 [0.503, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.690, 10.098], loss: 0.001872, mae: 0.043833, mean_q: 1.303758
 82615/100000: episode: 1738, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 60.701, mean reward: 0.607 [0.509, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.788, 10.098], loss: 0.001517, mae: 0.040998, mean_q: 1.311145
 82715/100000: episode: 1739, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 57.442, mean reward: 0.574 [0.511, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.258, 10.098], loss: 0.001761, mae: 0.043763, mean_q: 1.315173
 82815/100000: episode: 1740, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 60.607, mean reward: 0.606 [0.504, 0.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.950, 10.163], loss: 0.001783, mae: 0.042886, mean_q: 1.310307
 82915/100000: episode: 1741, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 57.810, mean reward: 0.578 [0.497, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.901, 10.137], loss: 0.001969, mae: 0.046255, mean_q: 1.306597
 83015/100000: episode: 1742, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 59.500, mean reward: 0.595 [0.503, 0.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.971, 10.098], loss: 0.001861, mae: 0.043186, mean_q: 1.303863
 83115/100000: episode: 1743, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 60.866, mean reward: 0.609 [0.504, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.161, 10.098], loss: 0.001719, mae: 0.043343, mean_q: 1.300134
 83215/100000: episode: 1744, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.148, mean reward: 0.581 [0.502, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.508, 10.098], loss: 0.001772, mae: 0.043302, mean_q: 1.306565
 83315/100000: episode: 1745, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 56.859, mean reward: 0.569 [0.499, 0.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.739, 10.098], loss: 0.001688, mae: 0.042839, mean_q: 1.299187
 83415/100000: episode: 1746, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 60.859, mean reward: 0.609 [0.504, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.510, 10.098], loss: 0.001688, mae: 0.043233, mean_q: 1.292482
 83515/100000: episode: 1747, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.564, mean reward: 0.586 [0.502, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.915, 10.109], loss: 0.001799, mae: 0.043667, mean_q: 1.289254
 83615/100000: episode: 1748, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.527, mean reward: 0.585 [0.505, 0.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.020, 10.151], loss: 0.001586, mae: 0.042542, mean_q: 1.287144
 83715/100000: episode: 1749, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 58.815, mean reward: 0.588 [0.510, 0.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.899, 10.195], loss: 0.001831, mae: 0.044294, mean_q: 1.277854
 83815/100000: episode: 1750, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.944, mean reward: 0.589 [0.513, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.833, 10.265], loss: 0.002009, mae: 0.046176, mean_q: 1.271083
 83915/100000: episode: 1751, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 60.530, mean reward: 0.605 [0.514, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.373, 10.312], loss: 0.001784, mae: 0.043792, mean_q: 1.272571
 84015/100000: episode: 1752, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 60.974, mean reward: 0.610 [0.511, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.777, 10.098], loss: 0.001907, mae: 0.044998, mean_q: 1.261508
 84115/100000: episode: 1753, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 59.045, mean reward: 0.590 [0.503, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.863, 10.384], loss: 0.001794, mae: 0.043546, mean_q: 1.264980
 84215/100000: episode: 1754, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 59.501, mean reward: 0.595 [0.514, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.776, 10.098], loss: 0.001698, mae: 0.042959, mean_q: 1.254591
 84315/100000: episode: 1755, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 62.604, mean reward: 0.626 [0.499, 0.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.189, 10.410], loss: 0.001983, mae: 0.045665, mean_q: 1.258225
 84415/100000: episode: 1756, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.791, mean reward: 0.598 [0.512, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.912, 10.405], loss: 0.002156, mae: 0.046529, mean_q: 1.256352
 84515/100000: episode: 1757, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 58.337, mean reward: 0.583 [0.511, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.588, 10.098], loss: 0.001726, mae: 0.043627, mean_q: 1.249536
 84615/100000: episode: 1758, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.856, mean reward: 0.589 [0.511, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.281, 10.098], loss: 0.001918, mae: 0.046050, mean_q: 1.244772
 84715/100000: episode: 1759, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.503, mean reward: 0.585 [0.507, 0.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.869, 10.337], loss: 0.001626, mae: 0.043492, mean_q: 1.242682
 84815/100000: episode: 1760, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 59.865, mean reward: 0.599 [0.529, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.228, 10.322], loss: 0.001775, mae: 0.042974, mean_q: 1.228015
 84915/100000: episode: 1761, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 58.823, mean reward: 0.588 [0.503, 0.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.808, 10.100], loss: 0.001735, mae: 0.042980, mean_q: 1.223367
 85015/100000: episode: 1762, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 60.848, mean reward: 0.608 [0.506, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.640, 10.098], loss: 0.001700, mae: 0.042769, mean_q: 1.217559
 85115/100000: episode: 1763, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 58.399, mean reward: 0.584 [0.505, 0.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.198, 10.098], loss: 0.001608, mae: 0.041300, mean_q: 1.217806
 85215/100000: episode: 1764, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 58.499, mean reward: 0.585 [0.500, 0.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.041, 10.098], loss: 0.001456, mae: 0.041489, mean_q: 1.205797
 85315/100000: episode: 1765, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 58.957, mean reward: 0.590 [0.505, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.424, 10.233], loss: 0.001734, mae: 0.043712, mean_q: 1.201303
 85415/100000: episode: 1766, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 57.407, mean reward: 0.574 [0.508, 0.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.185, 10.098], loss: 0.001601, mae: 0.043122, mean_q: 1.194926
 85515/100000: episode: 1767, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 57.434, mean reward: 0.574 [0.504, 0.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.569, 10.268], loss: 0.001431, mae: 0.041087, mean_q: 1.189434
 85615/100000: episode: 1768, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 56.700, mean reward: 0.567 [0.501, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.032, 10.098], loss: 0.001538, mae: 0.041792, mean_q: 1.176332
 85715/100000: episode: 1769, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.212, mean reward: 0.582 [0.501, 0.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.403, 10.098], loss: 0.001426, mae: 0.040666, mean_q: 1.172948
 85815/100000: episode: 1770, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 60.184, mean reward: 0.602 [0.509, 0.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.451, 10.098], loss: 0.001480, mae: 0.042183, mean_q: 1.169260
 85915/100000: episode: 1771, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.925, mean reward: 0.599 [0.510, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.762, 10.098], loss: 0.001451, mae: 0.041464, mean_q: 1.168442
 86015/100000: episode: 1772, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 57.053, mean reward: 0.571 [0.512, 0.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.578, 10.098], loss: 0.001445, mae: 0.041345, mean_q: 1.166196
 86115/100000: episode: 1773, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 58.163, mean reward: 0.582 [0.515, 0.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.909, 10.098], loss: 0.001372, mae: 0.040242, mean_q: 1.165548
 86215/100000: episode: 1774, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 57.886, mean reward: 0.579 [0.503, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.623, 10.137], loss: 0.001444, mae: 0.041191, mean_q: 1.170497
 86315/100000: episode: 1775, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 59.949, mean reward: 0.599 [0.507, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.206, 10.303], loss: 0.001352, mae: 0.039988, mean_q: 1.168891
 86415/100000: episode: 1776, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.165, mean reward: 0.582 [0.500, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.750, 10.146], loss: 0.001382, mae: 0.040438, mean_q: 1.164664
 86515/100000: episode: 1777, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 62.788, mean reward: 0.628 [0.504, 0.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.917, 10.098], loss: 0.001415, mae: 0.041027, mean_q: 1.166581
 86615/100000: episode: 1778, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 59.134, mean reward: 0.591 [0.500, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.306, 10.202], loss: 0.001397, mae: 0.041033, mean_q: 1.166328
 86715/100000: episode: 1779, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 57.522, mean reward: 0.575 [0.503, 0.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.219, 10.192], loss: 0.001440, mae: 0.041184, mean_q: 1.170129
 86815/100000: episode: 1780, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 57.150, mean reward: 0.572 [0.512, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.485, 10.098], loss: 0.001393, mae: 0.040609, mean_q: 1.171503
 86915/100000: episode: 1781, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.294, mean reward: 0.583 [0.516, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.170, 10.098], loss: 0.001544, mae: 0.042330, mean_q: 1.169882
 87015/100000: episode: 1782, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.080, mean reward: 0.581 [0.502, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.591, 10.098], loss: 0.001378, mae: 0.040844, mean_q: 1.167100
 87115/100000: episode: 1783, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 62.688, mean reward: 0.627 [0.500, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-0.602, 10.098], loss: 0.001326, mae: 0.040088, mean_q: 1.166655
 87215/100000: episode: 1784, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 58.060, mean reward: 0.581 [0.502, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.383, 10.104], loss: 0.001432, mae: 0.041541, mean_q: 1.167665
 87315/100000: episode: 1785, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 60.251, mean reward: 0.603 [0.502, 0.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.409, 10.330], loss: 0.001446, mae: 0.041708, mean_q: 1.169529
 87415/100000: episode: 1786, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 59.243, mean reward: 0.592 [0.506, 0.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.163, 10.098], loss: 0.001405, mae: 0.041150, mean_q: 1.168022
 87515/100000: episode: 1787, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 58.658, mean reward: 0.587 [0.508, 0.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.007, 10.182], loss: 0.001476, mae: 0.042441, mean_q: 1.168354
 87615/100000: episode: 1788, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 58.046, mean reward: 0.580 [0.507, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.566, 10.098], loss: 0.001493, mae: 0.042065, mean_q: 1.167653
 87715/100000: episode: 1789, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 66.531, mean reward: 0.665 [0.507, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.218, 10.098], loss: 0.001383, mae: 0.040923, mean_q: 1.170892
 87815/100000: episode: 1790, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 59.225, mean reward: 0.592 [0.516, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.111, 10.098], loss: 0.001463, mae: 0.041591, mean_q: 1.171037
 87915/100000: episode: 1791, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 56.409, mean reward: 0.564 [0.500, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.555, 10.098], loss: 0.001417, mae: 0.041066, mean_q: 1.171896
 88015/100000: episode: 1792, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 58.492, mean reward: 0.585 [0.507, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.383, 10.128], loss: 0.001434, mae: 0.040659, mean_q: 1.167815
 88115/100000: episode: 1793, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 58.750, mean reward: 0.588 [0.497, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.610, 10.329], loss: 0.001391, mae: 0.041051, mean_q: 1.169133
 88215/100000: episode: 1794, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 59.827, mean reward: 0.598 [0.503, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.144, 10.226], loss: 0.001392, mae: 0.040641, mean_q: 1.166507
 88315/100000: episode: 1795, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.263, mean reward: 0.583 [0.512, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.097, 10.200], loss: 0.001284, mae: 0.039213, mean_q: 1.170723
 88415/100000: episode: 1796, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 56.797, mean reward: 0.568 [0.500, 0.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.165, 10.189], loss: 0.001406, mae: 0.040945, mean_q: 1.168929
 88515/100000: episode: 1797, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.464, mean reward: 0.595 [0.504, 0.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.670, 10.098], loss: 0.001380, mae: 0.040533, mean_q: 1.166773
 88615/100000: episode: 1798, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 61.717, mean reward: 0.617 [0.513, 0.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.518, 10.098], loss: 0.001485, mae: 0.042137, mean_q: 1.169493
 88715/100000: episode: 1799, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 58.158, mean reward: 0.582 [0.503, 0.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.869, 10.098], loss: 0.001426, mae: 0.041287, mean_q: 1.169468
 88815/100000: episode: 1800, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 62.181, mean reward: 0.622 [0.504, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.513, 10.235], loss: 0.001439, mae: 0.041551, mean_q: 1.167550
 88915/100000: episode: 1801, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 59.786, mean reward: 0.598 [0.507, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.062, 10.184], loss: 0.001350, mae: 0.039693, mean_q: 1.171275
 89015/100000: episode: 1802, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.483, mean reward: 0.575 [0.506, 0.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.906, 10.255], loss: 0.001430, mae: 0.041478, mean_q: 1.168554
 89115/100000: episode: 1803, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 57.940, mean reward: 0.579 [0.511, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.600, 10.225], loss: 0.001518, mae: 0.042347, mean_q: 1.169015
 89215/100000: episode: 1804, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 59.523, mean reward: 0.595 [0.506, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.380, 10.098], loss: 0.001332, mae: 0.039649, mean_q: 1.168389
 89315/100000: episode: 1805, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 58.990, mean reward: 0.590 [0.500, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.279, 10.130], loss: 0.001466, mae: 0.041890, mean_q: 1.166404
 89415/100000: episode: 1806, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 58.736, mean reward: 0.587 [0.518, 0.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.810, 10.098], loss: 0.001240, mae: 0.037981, mean_q: 1.166793
 89515/100000: episode: 1807, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 59.447, mean reward: 0.594 [0.499, 0.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.398, 10.179], loss: 0.001385, mae: 0.040892, mean_q: 1.167265
 89615/100000: episode: 1808, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.221, mean reward: 0.582 [0.503, 0.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.633, 10.284], loss: 0.001435, mae: 0.040998, mean_q: 1.168619
 89715/100000: episode: 1809, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.083, mean reward: 0.581 [0.503, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.879, 10.264], loss: 0.001302, mae: 0.039136, mean_q: 1.164659
 89815/100000: episode: 1810, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 60.223, mean reward: 0.602 [0.503, 0.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.173, 10.291], loss: 0.001362, mae: 0.039676, mean_q: 1.162374
 89915/100000: episode: 1811, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.067, mean reward: 0.581 [0.503, 0.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.596, 10.098], loss: 0.001294, mae: 0.039678, mean_q: 1.165125
 90015/100000: episode: 1812, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 55.993, mean reward: 0.560 [0.500, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.465, 10.115], loss: 0.001317, mae: 0.039105, mean_q: 1.164236
 90115/100000: episode: 1813, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 57.539, mean reward: 0.575 [0.499, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.977, 10.098], loss: 0.001351, mae: 0.040548, mean_q: 1.163661
 90215/100000: episode: 1814, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 58.540, mean reward: 0.585 [0.506, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.013, 10.197], loss: 0.001384, mae: 0.040602, mean_q: 1.162643
 90315/100000: episode: 1815, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 60.395, mean reward: 0.604 [0.501, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.625, 10.287], loss: 0.001368, mae: 0.040500, mean_q: 1.161090
 90415/100000: episode: 1816, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 61.529, mean reward: 0.615 [0.511, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.429, 10.101], loss: 0.001361, mae: 0.040418, mean_q: 1.167753
 90515/100000: episode: 1817, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.910, mean reward: 0.589 [0.507, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.742, 10.098], loss: 0.001396, mae: 0.040624, mean_q: 1.166825
 90615/100000: episode: 1818, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.922, mean reward: 0.589 [0.501, 0.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.676, 10.098], loss: 0.001324, mae: 0.039665, mean_q: 1.167182
 90715/100000: episode: 1819, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 58.684, mean reward: 0.587 [0.505, 0.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.465, 10.124], loss: 0.001394, mae: 0.040974, mean_q: 1.168002
[Info] 1-TH LEVEL FOUND: 1.3527276515960693, Considering 10/90 traces
 90815/100000: episode: 1820, duration: 4.803s, episode steps: 100, steps per second: 21, episode reward: 60.265, mean reward: 0.603 [0.503, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.862, 10.387], loss: 0.001401, mae: 0.040481, mean_q: 1.167272
 90850/100000: episode: 1821, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 26.122, mean reward: 0.746 [0.664, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-1.053, 10.100], loss: 0.001445, mae: 0.041113, mean_q: 1.169038
 90883/100000: episode: 1822, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 22.386, mean reward: 0.678 [0.609, 0.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.875, 10.100], loss: 0.001447, mae: 0.041759, mean_q: 1.171857
 90905/100000: episode: 1823, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 14.310, mean reward: 0.650 [0.607, 0.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.232, 10.100], loss: 0.001345, mae: 0.039755, mean_q: 1.168362
 90927/100000: episode: 1824, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 13.422, mean reward: 0.610 [0.508, 0.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.036, 10.139], loss: 0.001309, mae: 0.039629, mean_q: 1.165695
 90962/100000: episode: 1825, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 21.895, mean reward: 0.626 [0.573, 0.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.539, 10.100], loss: 0.001463, mae: 0.040500, mean_q: 1.170375
 91004/100000: episode: 1826, duration: 0.217s, episode steps: 42, steps per second: 194, episode reward: 27.233, mean reward: 0.648 [0.511, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.717, 10.100], loss: 0.001488, mae: 0.041498, mean_q: 1.171459
 91080/100000: episode: 1827, duration: 0.390s, episode steps: 76, steps per second: 195, episode reward: 47.749, mean reward: 0.628 [0.501, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.682 [-1.760, 10.112], loss: 0.001370, mae: 0.040395, mean_q: 1.177886
 91156/100000: episode: 1828, duration: 0.410s, episode steps: 76, steps per second: 185, episode reward: 45.642, mean reward: 0.601 [0.513, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.676 [-1.277, 10.100], loss: 0.001364, mae: 0.040353, mean_q: 1.176498
 91207/100000: episode: 1829, duration: 0.251s, episode steps: 51, steps per second: 203, episode reward: 35.455, mean reward: 0.695 [0.605, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.410, 10.339], loss: 0.001549, mae: 0.042061, mean_q: 1.174007
 91236/100000: episode: 1830, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 19.575, mean reward: 0.675 [0.613, 0.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.094, 10.100], loss: 0.001462, mae: 0.041571, mean_q: 1.177470
 91265/100000: episode: 1831, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 18.640, mean reward: 0.643 [0.553, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.560, 10.100], loss: 0.001572, mae: 0.042606, mean_q: 1.180999
 91316/100000: episode: 1832, duration: 0.267s, episode steps: 51, steps per second: 191, episode reward: 38.169, mean reward: 0.748 [0.664, 0.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.643, 10.445], loss: 0.001362, mae: 0.040534, mean_q: 1.181659
 91358/100000: episode: 1833, duration: 0.221s, episode steps: 42, steps per second: 190, episode reward: 33.144, mean reward: 0.789 [0.687, 0.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.046, 10.100], loss: 0.001463, mae: 0.041769, mean_q: 1.185163
 91391/100000: episode: 1834, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 20.278, mean reward: 0.614 [0.537, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.369, 10.100], loss: 0.001391, mae: 0.040562, mean_q: 1.187953
 91424/100000: episode: 1835, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 20.649, mean reward: 0.626 [0.514, 0.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.305, 10.100], loss: 0.001573, mae: 0.043297, mean_q: 1.188154
 91459/100000: episode: 1836, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 23.956, mean reward: 0.684 [0.618, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.446, 10.100], loss: 0.001642, mae: 0.043566, mean_q: 1.185386
 91510/100000: episode: 1837, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 33.983, mean reward: 0.666 [0.597, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.407, 10.320], loss: 0.001634, mae: 0.044161, mean_q: 1.185589
 91532/100000: episode: 1838, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 13.170, mean reward: 0.599 [0.508, 0.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.386, 10.163], loss: 0.001354, mae: 0.040565, mean_q: 1.186192
 91608/100000: episode: 1839, duration: 0.379s, episode steps: 76, steps per second: 200, episode reward: 46.390, mean reward: 0.610 [0.499, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.682 [-1.021, 10.416], loss: 0.001561, mae: 0.042888, mean_q: 1.182844
 91659/100000: episode: 1840, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 31.513, mean reward: 0.618 [0.510, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.967, 10.137], loss: 0.001639, mae: 0.043540, mean_q: 1.181408
 91692/100000: episode: 1841, duration: 0.195s, episode steps: 33, steps per second: 169, episode reward: 20.570, mean reward: 0.623 [0.561, 0.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.097, 10.100], loss: 0.001754, mae: 0.044561, mean_q: 1.184459
 91745/100000: episode: 1842, duration: 0.259s, episode steps: 53, steps per second: 204, episode reward: 35.813, mean reward: 0.676 [0.553, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.635, 10.100], loss: 0.001482, mae: 0.041788, mean_q: 1.187393
 91780/100000: episode: 1843, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 25.959, mean reward: 0.742 [0.644, 0.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.571, 10.100], loss: 0.001782, mae: 0.045969, mean_q: 1.194445
 91819/100000: episode: 1844, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 25.490, mean reward: 0.654 [0.556, 0.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-1.080, 10.417], loss: 0.001499, mae: 0.041382, mean_q: 1.192017
 91872/100000: episode: 1845, duration: 0.285s, episode steps: 53, steps per second: 186, episode reward: 43.419, mean reward: 0.819 [0.711, 0.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.833 [-1.679, 10.100], loss: 0.001683, mae: 0.042889, mean_q: 1.193925
 91948/100000: episode: 1846, duration: 0.390s, episode steps: 76, steps per second: 195, episode reward: 48.636, mean reward: 0.640 [0.505, 0.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.667 [-1.661, 10.100], loss: 0.001672, mae: 0.043554, mean_q: 1.197020
 91970/100000: episode: 1847, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 17.008, mean reward: 0.773 [0.699, 0.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.484, 10.100], loss: 0.001638, mae: 0.043597, mean_q: 1.199998
 92004/100000: episode: 1848, duration: 0.167s, episode steps: 34, steps per second: 203, episode reward: 23.775, mean reward: 0.699 [0.593, 0.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.035, 10.298], loss: 0.001421, mae: 0.041478, mean_q: 1.196916
 92037/100000: episode: 1849, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 20.377, mean reward: 0.617 [0.517, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.448, 10.100], loss: 0.001652, mae: 0.042318, mean_q: 1.201638
 92071/100000: episode: 1850, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 24.297, mean reward: 0.715 [0.637, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-1.075, 10.373], loss: 0.001719, mae: 0.044293, mean_q: 1.196304
 92104/100000: episode: 1851, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 22.953, mean reward: 0.696 [0.628, 0.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.165, 10.100], loss: 0.001506, mae: 0.042446, mean_q: 1.204291
 92180/100000: episode: 1852, duration: 0.391s, episode steps: 76, steps per second: 194, episode reward: 47.714, mean reward: 0.628 [0.508, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.686 [-0.820, 10.281], loss: 0.001673, mae: 0.042886, mean_q: 1.202847
 92256/100000: episode: 1853, duration: 0.374s, episode steps: 76, steps per second: 203, episode reward: 46.201, mean reward: 0.608 [0.508, 0.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.682 [-1.189, 10.100], loss: 0.001620, mae: 0.042941, mean_q: 1.205650
 92295/100000: episode: 1854, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 26.457, mean reward: 0.678 [0.611, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.812, 10.489], loss: 0.001715, mae: 0.043955, mean_q: 1.206384
 92324/100000: episode: 1855, duration: 0.143s, episode steps: 29, steps per second: 202, episode reward: 19.865, mean reward: 0.685 [0.524, 0.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.314, 10.100], loss: 0.001617, mae: 0.042896, mean_q: 1.207005
 92400/100000: episode: 1856, duration: 0.399s, episode steps: 76, steps per second: 190, episode reward: 49.048, mean reward: 0.645 [0.525, 0.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.687 [-0.759, 10.100], loss: 0.001487, mae: 0.041152, mean_q: 1.210920
 92442/100000: episode: 1857, duration: 0.228s, episode steps: 42, steps per second: 184, episode reward: 26.263, mean reward: 0.625 [0.509, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.534, 10.151], loss: 0.001584, mae: 0.042174, mean_q: 1.210154
 92464/100000: episode: 1858, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 12.926, mean reward: 0.588 [0.505, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.647, 10.100], loss: 0.001332, mae: 0.039657, mean_q: 1.210481
 92493/100000: episode: 1859, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 20.301, mean reward: 0.700 [0.633, 0.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.309, 10.100], loss: 0.001526, mae: 0.041110, mean_q: 1.212314
 92515/100000: episode: 1860, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 14.283, mean reward: 0.649 [0.537, 0.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.172, 10.100], loss: 0.001605, mae: 0.043105, mean_q: 1.211414
 92549/100000: episode: 1861, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 23.148, mean reward: 0.681 [0.583, 0.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.035, 10.354], loss: 0.001662, mae: 0.043195, mean_q: 1.207609
 92600/100000: episode: 1862, duration: 0.254s, episode steps: 51, steps per second: 201, episode reward: 34.259, mean reward: 0.672 [0.576, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.414, 10.320], loss: 0.001543, mae: 0.042424, mean_q: 1.214942
 92633/100000: episode: 1863, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 20.549, mean reward: 0.623 [0.517, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.852, 10.100], loss: 0.001583, mae: 0.042060, mean_q: 1.203310
 92672/100000: episode: 1864, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 25.447, mean reward: 0.652 [0.589, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.837, 10.395], loss: 0.001667, mae: 0.043442, mean_q: 1.210600
 92723/100000: episode: 1865, duration: 0.261s, episode steps: 51, steps per second: 195, episode reward: 34.819, mean reward: 0.683 [0.558, 0.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.410, 10.311], loss: 0.001795, mae: 0.044853, mean_q: 1.216281
 92758/100000: episode: 1866, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 24.151, mean reward: 0.690 [0.615, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.027, 10.100], loss: 0.001760, mae: 0.044573, mean_q: 1.209847
 92791/100000: episode: 1867, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 21.191, mean reward: 0.642 [0.589, 0.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.246, 10.100], loss: 0.001723, mae: 0.044766, mean_q: 1.218647
 92830/100000: episode: 1868, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 22.374, mean reward: 0.574 [0.505, 0.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.435, 10.100], loss: 0.001790, mae: 0.044826, mean_q: 1.219661
 92869/100000: episode: 1869, duration: 0.191s, episode steps: 39, steps per second: 204, episode reward: 26.709, mean reward: 0.685 [0.573, 0.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.524, 10.318], loss: 0.001517, mae: 0.041650, mean_q: 1.220171
 92903/100000: episode: 1870, duration: 0.181s, episode steps: 34, steps per second: 187, episode reward: 24.947, mean reward: 0.734 [0.693, 0.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.035, 10.511], loss: 0.001610, mae: 0.043242, mean_q: 1.222886
 92925/100000: episode: 1871, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 13.409, mean reward: 0.609 [0.542, 0.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.560, 10.100], loss: 0.001481, mae: 0.042577, mean_q: 1.225757
[Info] FALSIFICATION!
 92954/100000: episode: 1872, duration: 0.387s, episode steps: 29, steps per second: 75, episode reward: 24.881, mean reward: 0.858 [0.718, 1.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.672 [-0.764, 8.729], loss: 0.001545, mae: 0.041565, mean_q: 1.218789
 92989/100000: episode: 1873, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 21.440, mean reward: 0.613 [0.536, 0.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.757, 10.100], loss: 0.001525, mae: 0.041067, mean_q: 1.222184
 93011/100000: episode: 1874, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 16.258, mean reward: 0.739 [0.590, 0.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.474, 10.100], loss: 0.002004, mae: 0.043443, mean_q: 1.230335
 93046/100000: episode: 1875, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 20.262, mean reward: 0.579 [0.506, 0.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.035, 10.252], loss: 0.001858, mae: 0.045602, mean_q: 1.213148
 93085/100000: episode: 1876, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 24.961, mean reward: 0.640 [0.534, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.035, 10.397], loss: 0.001656, mae: 0.043469, mean_q: 1.223937
 93120/100000: episode: 1877, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 22.731, mean reward: 0.649 [0.540, 0.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.312, 10.100], loss: 0.001611, mae: 0.040318, mean_q: 1.215100
 93149/100000: episode: 1878, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 20.841, mean reward: 0.719 [0.623, 0.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.319, 10.100], loss: 0.001668, mae: 0.043617, mean_q: 1.230523
 93183/100000: episode: 1879, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 26.373, mean reward: 0.776 [0.702, 0.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.528, 10.524], loss: 0.001658, mae: 0.043741, mean_q: 1.228575
 93222/100000: episode: 1880, duration: 0.201s, episode steps: 39, steps per second: 194, episode reward: 24.560, mean reward: 0.630 [0.536, 0.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.699, 10.252], loss: 0.001436, mae: 0.040158, mean_q: 1.228020
 93261/100000: episode: 1881, duration: 0.214s, episode steps: 39, steps per second: 182, episode reward: 26.542, mean reward: 0.681 [0.595, 0.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.788, 10.443], loss: 0.002299, mae: 0.046515, mean_q: 1.233279
 93314/100000: episode: 1882, duration: 0.270s, episode steps: 53, steps per second: 196, episode reward: 35.003, mean reward: 0.660 [0.566, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.269, 10.100], loss: 0.001546, mae: 0.042381, mean_q: 1.229830
 93336/100000: episode: 1883, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 13.485, mean reward: 0.613 [0.531, 0.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.449, 10.100], loss: 0.001666, mae: 0.042248, mean_q: 1.216845
 93371/100000: episode: 1884, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 22.871, mean reward: 0.653 [0.533, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.127, 10.100], loss: 0.001639, mae: 0.043862, mean_q: 1.228279
 93413/100000: episode: 1885, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 30.509, mean reward: 0.726 [0.615, 0.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.376, 10.100], loss: 0.001778, mae: 0.044672, mean_q: 1.233584
 93435/100000: episode: 1886, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 13.476, mean reward: 0.613 [0.552, 0.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.721, 10.100], loss: 0.001520, mae: 0.042768, mean_q: 1.239445
 93468/100000: episode: 1887, duration: 0.168s, episode steps: 33, steps per second: 196, episode reward: 20.827, mean reward: 0.631 [0.513, 0.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.042, 10.271], loss: 0.001553, mae: 0.042575, mean_q: 1.240827
 93507/100000: episode: 1888, duration: 0.196s, episode steps: 39, steps per second: 199, episode reward: 24.597, mean reward: 0.631 [0.515, 0.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.099, 10.100], loss: 0.002034, mae: 0.045877, mean_q: 1.232165
 93536/100000: episode: 1889, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 17.951, mean reward: 0.619 [0.555, 0.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.343, 10.100], loss: 0.002037, mae: 0.044781, mean_q: 1.237975
 93565/100000: episode: 1890, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 20.327, mean reward: 0.701 [0.619, 0.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.337, 10.100], loss: 0.001611, mae: 0.043743, mean_q: 1.251924
 93618/100000: episode: 1891, duration: 0.264s, episode steps: 53, steps per second: 201, episode reward: 37.354, mean reward: 0.705 [0.606, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.859, 10.100], loss: 0.001593, mae: 0.041021, mean_q: 1.235373
 93671/100000: episode: 1892, duration: 0.274s, episode steps: 53, steps per second: 193, episode reward: 32.926, mean reward: 0.621 [0.505, 0.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.967, 10.150], loss: 0.001887, mae: 0.045310, mean_q: 1.247244
 93706/100000: episode: 1893, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 22.924, mean reward: 0.655 [0.549, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.145, 10.100], loss: 0.001970, mae: 0.044438, mean_q: 1.241989
 93741/100000: episode: 1894, duration: 0.180s, episode steps: 35, steps per second: 195, episode reward: 24.543, mean reward: 0.701 [0.603, 0.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.932, 10.100], loss: 0.001528, mae: 0.042171, mean_q: 1.234500
 93792/100000: episode: 1895, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 31.692, mean reward: 0.621 [0.550, 0.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.394, 10.291], loss: 0.001482, mae: 0.041639, mean_q: 1.239444
 93821/100000: episode: 1896, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 19.791, mean reward: 0.682 [0.644, 0.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.594, 10.100], loss: 0.001679, mae: 0.044624, mean_q: 1.240142
 93874/100000: episode: 1897, duration: 0.276s, episode steps: 53, steps per second: 192, episode reward: 39.607, mean reward: 0.747 [0.610, 0.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.388, 10.100], loss: 0.001565, mae: 0.042426, mean_q: 1.238875
 93913/100000: episode: 1898, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 25.043, mean reward: 0.642 [0.546, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.660, 10.454], loss: 0.001622, mae: 0.043421, mean_q: 1.246286
 93946/100000: episode: 1899, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 24.201, mean reward: 0.733 [0.621, 0.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.201, 10.100], loss: 0.001511, mae: 0.041647, mean_q: 1.235085
 93985/100000: episode: 1900, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 23.253, mean reward: 0.596 [0.503, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.970, 10.237], loss: 0.001812, mae: 0.044177, mean_q: 1.243098
 94061/100000: episode: 1901, duration: 0.397s, episode steps: 76, steps per second: 192, episode reward: 46.281, mean reward: 0.609 [0.520, 0.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.679 [-0.935, 10.100], loss: 0.001722, mae: 0.042299, mean_q: 1.246628
 94103/100000: episode: 1902, duration: 0.208s, episode steps: 42, steps per second: 202, episode reward: 29.717, mean reward: 0.708 [0.636, 0.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.444, 10.100], loss: 0.001279, mae: 0.038028, mean_q: 1.245450
 94156/100000: episode: 1903, duration: 0.278s, episode steps: 53, steps per second: 191, episode reward: 32.648, mean reward: 0.616 [0.515, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.638, 10.211], loss: 0.001550, mae: 0.042146, mean_q: 1.253436
 94209/100000: episode: 1904, duration: 0.262s, episode steps: 53, steps per second: 202, episode reward: 36.588, mean reward: 0.690 [0.533, 0.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.856, 10.131], loss: 0.001489, mae: 0.041399, mean_q: 1.245869
 94262/100000: episode: 1905, duration: 0.269s, episode steps: 53, steps per second: 197, episode reward: 40.913, mean reward: 0.772 [0.657, 0.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.369, 10.100], loss: 0.001603, mae: 0.041159, mean_q: 1.256616
 94296/100000: episode: 1906, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 25.865, mean reward: 0.761 [0.670, 0.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.140, 10.518], loss: 0.001524, mae: 0.042486, mean_q: 1.249593
 94335/100000: episode: 1907, duration: 0.192s, episode steps: 39, steps per second: 203, episode reward: 24.894, mean reward: 0.638 [0.570, 0.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-1.337, 10.315], loss: 0.001449, mae: 0.040196, mean_q: 1.262825
 94364/100000: episode: 1908, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 21.180, mean reward: 0.730 [0.659, 0.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.342, 10.100], loss: 0.001846, mae: 0.043736, mean_q: 1.251405
 94440/100000: episode: 1909, duration: 0.412s, episode steps: 76, steps per second: 184, episode reward: 49.631, mean reward: 0.653 [0.575, 0.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.678 [-1.538, 10.100], loss: 0.001545, mae: 0.042338, mean_q: 1.264350
[Info] Complete ISplit Iteration
[Info] Levels: [1.3527277, 1.8050432]
[Info] Cond. Prob: [0.1, 0.02]
[Info] Error Prob: 0.002

 94516/100000: episode: 1910, duration: 4.627s, episode steps: 76, steps per second: 16, episode reward: 48.152, mean reward: 0.634 [0.519, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.685 [-0.469, 10.100], loss: 0.001613, mae: 0.041818, mean_q: 1.263866
 94616/100000: episode: 1911, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.799, mean reward: 0.588 [0.498, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.966, 10.098], loss: 0.001579, mae: 0.041497, mean_q: 1.261563
 94716/100000: episode: 1912, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 58.938, mean reward: 0.589 [0.510, 0.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.283, 10.098], loss: 0.001656, mae: 0.043752, mean_q: 1.263539
 94816/100000: episode: 1913, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 58.092, mean reward: 0.581 [0.504, 0.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.304, 10.350], loss: 0.001722, mae: 0.043097, mean_q: 1.261405
 94916/100000: episode: 1914, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 57.652, mean reward: 0.577 [0.504, 0.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.399, 10.098], loss: 0.001542, mae: 0.042079, mean_q: 1.264034
 95016/100000: episode: 1915, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 62.988, mean reward: 0.630 [0.512, 0.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.445, 10.098], loss: 0.001593, mae: 0.041864, mean_q: 1.268556
 95116/100000: episode: 1916, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 61.396, mean reward: 0.614 [0.511, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.401, 10.242], loss: 0.001623, mae: 0.042270, mean_q: 1.260630
 95216/100000: episode: 1917, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.100, mean reward: 0.591 [0.506, 0.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.988, 10.098], loss: 0.001625, mae: 0.043708, mean_q: 1.261380
 95316/100000: episode: 1918, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 60.195, mean reward: 0.602 [0.507, 0.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.315, 10.114], loss: 0.001706, mae: 0.043010, mean_q: 1.269900
 95416/100000: episode: 1919, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 57.640, mean reward: 0.576 [0.505, 0.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.603, 10.098], loss: 0.001703, mae: 0.042015, mean_q: 1.265296
 95516/100000: episode: 1920, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 60.699, mean reward: 0.607 [0.499, 0.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.399, 10.098], loss: 0.001628, mae: 0.042427, mean_q: 1.264417
 95616/100000: episode: 1921, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 59.541, mean reward: 0.595 [0.499, 0.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.115, 10.098], loss: 0.001541, mae: 0.041052, mean_q: 1.266601
 95716/100000: episode: 1922, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 60.419, mean reward: 0.604 [0.514, 0.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.821, 10.342], loss: 0.001643, mae: 0.042293, mean_q: 1.264127
 95816/100000: episode: 1923, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 56.686, mean reward: 0.567 [0.498, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.271, 10.108], loss: 0.001680, mae: 0.044171, mean_q: 1.261304
 95916/100000: episode: 1924, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 59.644, mean reward: 0.596 [0.510, 0.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.804, 10.098], loss: 0.001495, mae: 0.041694, mean_q: 1.262701
 96016/100000: episode: 1925, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 57.324, mean reward: 0.573 [0.511, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.558, 10.197], loss: 0.001659, mae: 0.042141, mean_q: 1.263772
 96116/100000: episode: 1926, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 58.718, mean reward: 0.587 [0.500, 0.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.083, 10.098], loss: 0.001510, mae: 0.041602, mean_q: 1.258231
 96216/100000: episode: 1927, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 59.068, mean reward: 0.591 [0.500, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.978, 10.098], loss: 0.001584, mae: 0.041293, mean_q: 1.257799
 96316/100000: episode: 1928, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 61.083, mean reward: 0.611 [0.518, 0.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.633, 10.098], loss: 0.001542, mae: 0.041701, mean_q: 1.248303
 96416/100000: episode: 1929, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 61.469, mean reward: 0.615 [0.505, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.022, 10.098], loss: 0.001455, mae: 0.041096, mean_q: 1.244836
 96516/100000: episode: 1930, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 58.558, mean reward: 0.586 [0.500, 0.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.357, 10.250], loss: 0.001464, mae: 0.041651, mean_q: 1.248938
 96616/100000: episode: 1931, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 58.051, mean reward: 0.581 [0.503, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.793, 10.098], loss: 0.001605, mae: 0.042477, mean_q: 1.249928
 96716/100000: episode: 1932, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 60.191, mean reward: 0.602 [0.500, 0.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.325, 10.223], loss: 0.001549, mae: 0.042186, mean_q: 1.242128
 96816/100000: episode: 1933, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 59.591, mean reward: 0.596 [0.515, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.651, 10.199], loss: 0.001695, mae: 0.043529, mean_q: 1.241630
 96916/100000: episode: 1934, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 56.620, mean reward: 0.566 [0.508, 0.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.966, 10.098], loss: 0.001860, mae: 0.043020, mean_q: 1.242236
 97016/100000: episode: 1935, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 59.452, mean reward: 0.595 [0.500, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.979, 10.098], loss: 0.001491, mae: 0.041581, mean_q: 1.235894
 97116/100000: episode: 1936, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 67.151, mean reward: 0.672 [0.512, 0.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.229, 10.632], loss: 0.001705, mae: 0.043653, mean_q: 1.224770
 97216/100000: episode: 1937, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 59.929, mean reward: 0.599 [0.497, 0.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.199, 10.182], loss: 0.001619, mae: 0.042396, mean_q: 1.234032
 97316/100000: episode: 1938, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 59.472, mean reward: 0.595 [0.499, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.759, 10.389], loss: 0.001562, mae: 0.042518, mean_q: 1.231401
 97416/100000: episode: 1939, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 56.962, mean reward: 0.570 [0.504, 0.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.468, 10.098], loss: 0.001660, mae: 0.043358, mean_q: 1.225656
 97516/100000: episode: 1940, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 61.912, mean reward: 0.619 [0.500, 0.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.142, 10.098], loss: 0.001670, mae: 0.042349, mean_q: 1.227165
 97616/100000: episode: 1941, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 58.218, mean reward: 0.582 [0.509, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.954, 10.098], loss: 0.001662, mae: 0.043305, mean_q: 1.221035
 97716/100000: episode: 1942, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 58.977, mean reward: 0.590 [0.503, 0.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.811, 10.098], loss: 0.001867, mae: 0.043633, mean_q: 1.222108
 97816/100000: episode: 1943, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 57.663, mean reward: 0.577 [0.506, 0.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.909, 10.098], loss: 0.001822, mae: 0.044743, mean_q: 1.219117
 97916/100000: episode: 1944, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 59.310, mean reward: 0.593 [0.503, 0.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.668, 10.165], loss: 0.001731, mae: 0.044010, mean_q: 1.217491
 98016/100000: episode: 1945, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.209, mean reward: 0.582 [0.499, 0.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.842, 10.098], loss: 0.001640, mae: 0.042521, mean_q: 1.215946
 98116/100000: episode: 1946, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 56.838, mean reward: 0.568 [0.501, 0.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.979, 10.144], loss: 0.001637, mae: 0.043170, mean_q: 1.209194
 98216/100000: episode: 1947, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 58.363, mean reward: 0.584 [0.507, 0.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.579, 10.123], loss: 0.001580, mae: 0.042375, mean_q: 1.200660
 98316/100000: episode: 1948, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 59.393, mean reward: 0.594 [0.504, 0.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.410, 10.098], loss: 0.001622, mae: 0.043709, mean_q: 1.206944
 98416/100000: episode: 1949, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 59.072, mean reward: 0.591 [0.514, 0.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.657, 10.206], loss: 0.001530, mae: 0.041813, mean_q: 1.202708
 98516/100000: episode: 1950, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 58.153, mean reward: 0.582 [0.506, 0.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.503, 10.098], loss: 0.001632, mae: 0.042935, mean_q: 1.193793
 98616/100000: episode: 1951, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 55.863, mean reward: 0.559 [0.503, 0.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.256, 10.262], loss: 0.001550, mae: 0.042792, mean_q: 1.197308
 98716/100000: episode: 1952, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 57.188, mean reward: 0.572 [0.506, 0.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.598, 10.098], loss: 0.001596, mae: 0.043186, mean_q: 1.192917
 98816/100000: episode: 1953, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 58.436, mean reward: 0.584 [0.507, 0.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.698, 10.262], loss: 0.001494, mae: 0.042449, mean_q: 1.192203
 98916/100000: episode: 1954, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 59.458, mean reward: 0.595 [0.507, 0.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.052, 10.098], loss: 0.001452, mae: 0.041078, mean_q: 1.186793
 99016/100000: episode: 1955, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 59.487, mean reward: 0.595 [0.505, 0.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.639, 10.116], loss: 0.001520, mae: 0.042468, mean_q: 1.184149
 99116/100000: episode: 1956, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 58.255, mean reward: 0.583 [0.505, 0.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.724, 10.144], loss: 0.001539, mae: 0.042206, mean_q: 1.180016
 99216/100000: episode: 1957, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 58.528, mean reward: 0.585 [0.501, 0.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.755, 10.207], loss: 0.001499, mae: 0.042412, mean_q: 1.182119
 99316/100000: episode: 1958, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 58.597, mean reward: 0.586 [0.512, 0.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.254, 10.251], loss: 0.001464, mae: 0.041988, mean_q: 1.177123
 99416/100000: episode: 1959, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 59.812, mean reward: 0.598 [0.501, 0.826], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.488, 10.284], loss: 0.001442, mae: 0.041575, mean_q: 1.171633
 99516/100000: episode: 1960, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 59.270, mean reward: 0.593 [0.511, 0.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.281, 10.220], loss: 0.001420, mae: 0.041413, mean_q: 1.168745
 99616/100000: episode: 1961, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 62.250, mean reward: 0.622 [0.514, 0.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.857, 10.098], loss: 0.001358, mae: 0.040579, mean_q: 1.169151
 99716/100000: episode: 1962, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 58.600, mean reward: 0.586 [0.505, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.233, 10.434], loss: 0.001247, mae: 0.038865, mean_q: 1.171263
 99816/100000: episode: 1963, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 58.228, mean reward: 0.582 [0.505, 0.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.468, 10.098], loss: 0.001410, mae: 0.041284, mean_q: 1.170327
 99916/100000: episode: 1964, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 59.136, mean reward: 0.591 [0.504, 0.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.678, 10.145], loss: 0.001444, mae: 0.041956, mean_q: 1.171989
done, took 601.058 seconds
[Info] End Importance Splitting. Falsification occurred 17 times.
