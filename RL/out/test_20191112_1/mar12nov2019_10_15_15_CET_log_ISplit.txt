Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.170s, episode steps: 100, steps per second: 589, episode reward: 205.975, mean reward: 2.060 [1.486, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.134, 10.411], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 196.345, mean reward: 1.963 [1.512, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.662, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.061s, episode steps: 100, steps per second: 1643, episode reward: 221.644, mean reward: 2.216 [1.503, 4.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.451, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.061s, episode steps: 100, steps per second: 1647, episode reward: 187.009, mean reward: 1.870 [1.441, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.791, 10.218], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.072s, episode steps: 100, steps per second: 1395, episode reward: 184.438, mean reward: 1.844 [1.448, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.381, 10.353], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.066s, episode steps: 100, steps per second: 1522, episode reward: 207.577, mean reward: 2.076 [1.475, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.056, 10.112], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.063s, episode steps: 100, steps per second: 1584, episode reward: 176.553, mean reward: 1.766 [1.452, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.521, 10.236], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.061s, episode steps: 100, steps per second: 1630, episode reward: 190.390, mean reward: 1.904 [1.466, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.399, 10.297], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.061s, episode steps: 100, steps per second: 1635, episode reward: 191.391, mean reward: 1.914 [1.450, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.686, 10.334], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.061s, episode steps: 100, steps per second: 1634, episode reward: 188.696, mean reward: 1.887 [1.466, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.811, 10.386], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.071s, episode steps: 100, steps per second: 1412, episode reward: 202.433, mean reward: 2.024 [1.465, 4.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.128, 10.375], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.066s, episode steps: 100, steps per second: 1524, episode reward: 191.819, mean reward: 1.918 [1.431, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.066s, episode steps: 100, steps per second: 1507, episode reward: 183.284, mean reward: 1.833 [1.465, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.869, 10.163], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.061s, episode steps: 100, steps per second: 1649, episode reward: 208.360, mean reward: 2.084 [1.498, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.737, 10.124], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.061s, episode steps: 100, steps per second: 1632, episode reward: 183.374, mean reward: 1.834 [1.456, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.323, 10.106], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.074s, episode steps: 100, steps per second: 1354, episode reward: 204.273, mean reward: 2.043 [1.494, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.338, 10.190], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.061s, episode steps: 100, steps per second: 1638, episode reward: 182.976, mean reward: 1.830 [1.450, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.483, 10.155], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.061s, episode steps: 100, steps per second: 1641, episode reward: 210.625, mean reward: 2.106 [1.441, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.446, 10.320], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.066s, episode steps: 100, steps per second: 1515, episode reward: 181.764, mean reward: 1.818 [1.460, 2.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.017, 10.197], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.071s, episode steps: 100, steps per second: 1417, episode reward: 192.611, mean reward: 1.926 [1.462, 6.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.452, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.061s, episode steps: 100, steps per second: 1652, episode reward: 245.428, mean reward: 2.454 [1.467, 4.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.099, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.060s, episode steps: 100, steps per second: 1668, episode reward: 201.727, mean reward: 2.017 [1.499, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.649, 10.363], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.061s, episode steps: 100, steps per second: 1643, episode reward: 199.723, mean reward: 1.997 [1.437, 6.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.183, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.061s, episode steps: 100, steps per second: 1636, episode reward: 212.385, mean reward: 2.124 [1.438, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.014, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.070s, episode steps: 100, steps per second: 1426, episode reward: 186.370, mean reward: 1.864 [1.437, 2.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.213, 10.155], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.061s, episode steps: 100, steps per second: 1636, episode reward: 184.242, mean reward: 1.842 [1.477, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.622, 10.182], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.061s, episode steps: 100, steps per second: 1648, episode reward: 195.002, mean reward: 1.950 [1.476, 6.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.393, 10.175], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.061s, episode steps: 100, steps per second: 1641, episode reward: 180.423, mean reward: 1.804 [1.443, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.919, 10.192], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.067s, episode steps: 100, steps per second: 1489, episode reward: 188.200, mean reward: 1.882 [1.462, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.238, 10.130], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.060s, episode steps: 100, steps per second: 1661, episode reward: 204.242, mean reward: 2.042 [1.498, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.683, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.068s, episode steps: 100, steps per second: 1471, episode reward: 186.777, mean reward: 1.868 [1.474, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.654, 10.338], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.070s, episode steps: 100, steps per second: 1424, episode reward: 208.588, mean reward: 2.086 [1.486, 5.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.557, 10.102], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.069s, episode steps: 100, steps per second: 1454, episode reward: 216.969, mean reward: 2.170 [1.493, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.157, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.062s, episode steps: 100, steps per second: 1619, episode reward: 216.829, mean reward: 2.168 [1.484, 5.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.772, 10.245], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.067s, episode steps: 100, steps per second: 1482, episode reward: 213.148, mean reward: 2.131 [1.475, 6.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.360, 10.380], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.061s, episode steps: 100, steps per second: 1640, episode reward: 203.849, mean reward: 2.038 [1.447, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.813, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.071s, episode steps: 100, steps per second: 1406, episode reward: 211.929, mean reward: 2.119 [1.432, 4.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.482, 10.539], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.072s, episode steps: 100, steps per second: 1394, episode reward: 204.293, mean reward: 2.043 [1.480, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.057, 10.298], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.061s, episode steps: 100, steps per second: 1644, episode reward: 186.379, mean reward: 1.864 [1.457, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.407, 10.283], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.069s, episode steps: 100, steps per second: 1440, episode reward: 187.003, mean reward: 1.870 [1.454, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.460, 10.199], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.060s, episode steps: 100, steps per second: 1667, episode reward: 195.062, mean reward: 1.951 [1.452, 4.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.215, 10.188], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.061s, episode steps: 100, steps per second: 1636, episode reward: 186.358, mean reward: 1.864 [1.449, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.121, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.072s, episode steps: 100, steps per second: 1396, episode reward: 201.498, mean reward: 2.015 [1.474, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.080, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.065s, episode steps: 100, steps per second: 1532, episode reward: 195.559, mean reward: 1.956 [1.473, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.500, 10.251], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 198.812, mean reward: 1.988 [1.505, 5.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.843, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.061s, episode steps: 100, steps per second: 1634, episode reward: 202.472, mean reward: 2.025 [1.474, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.760, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.061s, episode steps: 100, steps per second: 1634, episode reward: 182.846, mean reward: 1.828 [1.436, 4.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.552, 10.164], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.061s, episode steps: 100, steps per second: 1632, episode reward: 187.991, mean reward: 1.880 [1.478, 4.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.496, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.071s, episode steps: 100, steps per second: 1402, episode reward: 211.248, mean reward: 2.112 [1.502, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.334, 10.129], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.067s, episode steps: 100, steps per second: 1492, episode reward: 194.546, mean reward: 1.945 [1.478, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.722, 10.411], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.265s, episode steps: 100, steps per second: 79, episode reward: 186.827, mean reward: 1.868 [1.446, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.962, 10.098], loss: 0.304085, mae: 0.553086, mean_q: 2.260621
  5200/100000: episode: 52, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 203.309, mean reward: 2.033 [1.448, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.957, 10.177], loss: 0.112954, mae: 0.334495, mean_q: 2.881046
  5300/100000: episode: 53, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 262.591, mean reward: 2.626 [1.469, 5.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.228, 10.098], loss: 0.120266, mae: 0.328660, mean_q: 3.231363
  5400/100000: episode: 54, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 185.662, mean reward: 1.857 [1.498, 2.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.633, 10.098], loss: 0.124431, mae: 0.336968, mean_q: 3.496164
  5500/100000: episode: 55, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 197.334, mean reward: 1.973 [1.458, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.662, 10.098], loss: 0.133997, mae: 0.343226, mean_q: 3.656760
  5600/100000: episode: 56, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.315, mean reward: 1.973 [1.511, 3.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.696, 10.098], loss: 0.111941, mae: 0.324188, mean_q: 3.750600
  5700/100000: episode: 57, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 189.791, mean reward: 1.898 [1.468, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.853, 10.098], loss: 0.132299, mae: 0.347137, mean_q: 3.831837
  5800/100000: episode: 58, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 207.793, mean reward: 2.078 [1.468, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.567, 10.326], loss: 0.133877, mae: 0.343111, mean_q: 3.878819
  5900/100000: episode: 59, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 202.432, mean reward: 2.024 [1.456, 4.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.068, 10.098], loss: 0.123216, mae: 0.337787, mean_q: 3.884537
  6000/100000: episode: 60, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 177.612, mean reward: 1.776 [1.448, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.864, 10.098], loss: 0.134143, mae: 0.347056, mean_q: 3.910115
  6100/100000: episode: 61, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 211.595, mean reward: 2.116 [1.458, 4.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.366, 10.210], loss: 0.129628, mae: 0.343994, mean_q: 3.919354
  6200/100000: episode: 62, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 185.488, mean reward: 1.855 [1.467, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.744, 10.098], loss: 0.141400, mae: 0.349065, mean_q: 3.936353
  6300/100000: episode: 63, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 204.356, mean reward: 2.044 [1.469, 4.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.047, 10.098], loss: 0.140346, mae: 0.356976, mean_q: 3.943724
  6400/100000: episode: 64, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 180.414, mean reward: 1.804 [1.443, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.012, 10.098], loss: 0.129127, mae: 0.338505, mean_q: 3.920487
  6500/100000: episode: 65, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 185.936, mean reward: 1.859 [1.444, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.260, 10.169], loss: 0.157367, mae: 0.357799, mean_q: 3.930029
  6600/100000: episode: 66, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.670, mean reward: 1.977 [1.463, 5.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.126, 10.204], loss: 0.143838, mae: 0.351258, mean_q: 3.933539
  6700/100000: episode: 67, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 196.243, mean reward: 1.962 [1.466, 4.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.685, 10.103], loss: 0.126223, mae: 0.336413, mean_q: 3.933234
  6800/100000: episode: 68, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 202.211, mean reward: 2.022 [1.471, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.853, 10.098], loss: 0.129942, mae: 0.343394, mean_q: 3.937343
  6900/100000: episode: 69, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 186.733, mean reward: 1.867 [1.466, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.636, 10.197], loss: 0.124982, mae: 0.332713, mean_q: 3.907150
  7000/100000: episode: 70, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 189.207, mean reward: 1.892 [1.443, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.883, 10.272], loss: 0.139953, mae: 0.350111, mean_q: 3.952832
  7100/100000: episode: 71, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 186.992, mean reward: 1.870 [1.439, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.500, 10.154], loss: 0.135515, mae: 0.343362, mean_q: 3.938168
  7200/100000: episode: 72, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 191.363, mean reward: 1.914 [1.483, 5.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.186, 10.136], loss: 0.116756, mae: 0.320473, mean_q: 3.931808
  7300/100000: episode: 73, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 188.983, mean reward: 1.890 [1.460, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.613, 10.098], loss: 0.120120, mae: 0.331683, mean_q: 3.923506
  7400/100000: episode: 74, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 198.596, mean reward: 1.986 [1.482, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.929, 10.140], loss: 0.110266, mae: 0.317996, mean_q: 3.910989
  7500/100000: episode: 75, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 229.912, mean reward: 2.299 [1.539, 6.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.750, 10.098], loss: 0.123502, mae: 0.335736, mean_q: 3.916614
  7600/100000: episode: 76, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 200.848, mean reward: 2.008 [1.539, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.747, 10.256], loss: 0.117317, mae: 0.327476, mean_q: 3.913886
  7700/100000: episode: 77, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 192.336, mean reward: 1.923 [1.542, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.428, 10.302], loss: 0.105109, mae: 0.317668, mean_q: 3.904654
  7800/100000: episode: 78, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 184.551, mean reward: 1.846 [1.466, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.185, 10.098], loss: 0.129709, mae: 0.338828, mean_q: 3.945729
  7900/100000: episode: 79, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 198.292, mean reward: 1.983 [1.486, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.252, 10.313], loss: 0.113406, mae: 0.327267, mean_q: 3.925681
  8000/100000: episode: 80, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 193.824, mean reward: 1.938 [1.500, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.744, 10.113], loss: 0.115466, mae: 0.322878, mean_q: 3.934399
  8100/100000: episode: 81, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 194.987, mean reward: 1.950 [1.448, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.401, 10.257], loss: 0.118548, mae: 0.326136, mean_q: 3.924215
  8200/100000: episode: 82, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 190.771, mean reward: 1.908 [1.453, 3.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.717, 10.098], loss: 0.113197, mae: 0.331108, mean_q: 3.927258
  8300/100000: episode: 83, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 198.566, mean reward: 1.986 [1.459, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.772, 10.325], loss: 0.122135, mae: 0.332352, mean_q: 3.914272
  8400/100000: episode: 84, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 191.260, mean reward: 1.913 [1.491, 5.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.304, 10.324], loss: 0.111374, mae: 0.318287, mean_q: 3.912446
  8500/100000: episode: 85, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 193.615, mean reward: 1.936 [1.440, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.230, 10.098], loss: 0.116452, mae: 0.321212, mean_q: 3.898366
  8600/100000: episode: 86, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 198.494, mean reward: 1.985 [1.495, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.206, 10.227], loss: 0.116960, mae: 0.325628, mean_q: 3.910835
  8700/100000: episode: 87, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 266.414, mean reward: 2.664 [1.493, 7.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.280, 10.098], loss: 0.124527, mae: 0.328432, mean_q: 3.915756
  8800/100000: episode: 88, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 202.247, mean reward: 2.022 [1.440, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.521, 10.176], loss: 0.152107, mae: 0.358073, mean_q: 3.924956
  8900/100000: episode: 89, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 227.710, mean reward: 2.277 [1.454, 5.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.489, 10.098], loss: 0.122936, mae: 0.330236, mean_q: 3.913120
  9000/100000: episode: 90, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.172, mean reward: 1.962 [1.448, 5.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.945, 10.196], loss: 0.128455, mae: 0.338882, mean_q: 3.920894
  9100/100000: episode: 91, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 190.543, mean reward: 1.905 [1.436, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.985, 10.166], loss: 0.131128, mae: 0.342727, mean_q: 3.922358
  9200/100000: episode: 92, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 193.574, mean reward: 1.936 [1.444, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.025, 10.201], loss: 0.133683, mae: 0.340287, mean_q: 3.933755
  9300/100000: episode: 93, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 191.562, mean reward: 1.916 [1.499, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.304, 10.098], loss: 0.125482, mae: 0.336589, mean_q: 3.930936
  9400/100000: episode: 94, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 187.168, mean reward: 1.872 [1.452, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.178, 10.098], loss: 0.129255, mae: 0.342973, mean_q: 3.933801
  9500/100000: episode: 95, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 209.023, mean reward: 2.090 [1.506, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.303, 10.098], loss: 0.146665, mae: 0.355997, mean_q: 3.941376
  9600/100000: episode: 96, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 186.645, mean reward: 1.866 [1.451, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.944, 10.098], loss: 0.146509, mae: 0.348436, mean_q: 3.942775
  9700/100000: episode: 97, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 182.016, mean reward: 1.820 [1.454, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.611, 10.098], loss: 0.124354, mae: 0.334727, mean_q: 3.947228
  9800/100000: episode: 98, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 190.233, mean reward: 1.902 [1.439, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.672, 10.111], loss: 0.143411, mae: 0.352587, mean_q: 3.945663
  9900/100000: episode: 99, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 207.308, mean reward: 2.073 [1.470, 6.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.047, 10.127], loss: 0.133915, mae: 0.340123, mean_q: 3.929519
 10000/100000: episode: 100, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 210.836, mean reward: 2.108 [1.451, 3.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.810, 10.098], loss: 0.136369, mae: 0.343041, mean_q: 3.924250
 10100/100000: episode: 101, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 198.520, mean reward: 1.985 [1.441, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.126, 10.240], loss: 0.148717, mae: 0.354516, mean_q: 3.952616
 10200/100000: episode: 102, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 181.812, mean reward: 1.818 [1.442, 2.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.389, 10.186], loss: 0.128725, mae: 0.341373, mean_q: 3.928691
 10300/100000: episode: 103, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 244.429, mean reward: 2.444 [1.551, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.258, 10.404], loss: 0.145984, mae: 0.352212, mean_q: 3.939734
 10400/100000: episode: 104, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 205.320, mean reward: 2.053 [1.443, 4.584], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.229, 10.098], loss: 0.149359, mae: 0.355013, mean_q: 3.939337
 10500/100000: episode: 105, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.097, mean reward: 1.851 [1.455, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.063, 10.168], loss: 0.145120, mae: 0.342943, mean_q: 3.930318
 10600/100000: episode: 106, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.336, mean reward: 1.853 [1.514, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.229, 10.098], loss: 0.130614, mae: 0.336040, mean_q: 3.923263
 10700/100000: episode: 107, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 179.020, mean reward: 1.790 [1.445, 2.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.823, 10.142], loss: 0.118744, mae: 0.333665, mean_q: 3.905852
 10800/100000: episode: 108, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 195.309, mean reward: 1.953 [1.464, 3.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.423, 10.098], loss: 0.131347, mae: 0.339531, mean_q: 3.921593
 10900/100000: episode: 109, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 187.439, mean reward: 1.874 [1.474, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.315, 10.098], loss: 0.112920, mae: 0.318275, mean_q: 3.906059
 11000/100000: episode: 110, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 200.910, mean reward: 2.009 [1.474, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.770, 10.172], loss: 0.124242, mae: 0.333820, mean_q: 3.911769
 11100/100000: episode: 111, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 192.344, mean reward: 1.923 [1.455, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.857, 10.462], loss: 0.124106, mae: 0.333072, mean_q: 3.904394
 11200/100000: episode: 112, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 188.093, mean reward: 1.881 [1.447, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.618, 10.098], loss: 0.128161, mae: 0.321872, mean_q: 3.910764
 11300/100000: episode: 113, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 199.905, mean reward: 1.999 [1.470, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.114, 10.098], loss: 0.113625, mae: 0.321395, mean_q: 3.890665
 11400/100000: episode: 114, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 206.405, mean reward: 2.064 [1.472, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.517, 10.328], loss: 0.137937, mae: 0.347204, mean_q: 3.922242
 11500/100000: episode: 115, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 200.491, mean reward: 2.005 [1.479, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.792, 10.224], loss: 0.128112, mae: 0.333752, mean_q: 3.924298
 11600/100000: episode: 116, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 190.100, mean reward: 1.901 [1.461, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.107], loss: 0.115404, mae: 0.325736, mean_q: 3.915538
 11700/100000: episode: 117, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 185.634, mean reward: 1.856 [1.457, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.404, 10.098], loss: 0.109928, mae: 0.312377, mean_q: 3.899832
 11800/100000: episode: 118, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 182.562, mean reward: 1.826 [1.445, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.621, 10.198], loss: 0.129499, mae: 0.338056, mean_q: 3.914645
 11900/100000: episode: 119, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 230.591, mean reward: 2.306 [1.565, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.086, 10.098], loss: 0.126026, mae: 0.335997, mean_q: 3.921193
 12000/100000: episode: 120, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 193.334, mean reward: 1.933 [1.456, 6.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.770, 10.174], loss: 0.122630, mae: 0.331810, mean_q: 3.936216
 12100/100000: episode: 121, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 184.073, mean reward: 1.841 [1.499, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.603, 10.098], loss: 0.134400, mae: 0.348168, mean_q: 3.946322
 12200/100000: episode: 122, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 197.146, mean reward: 1.971 [1.481, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.936, 10.263], loss: 0.135963, mae: 0.343341, mean_q: 3.924594
 12300/100000: episode: 123, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 179.673, mean reward: 1.797 [1.460, 2.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.446, 10.181], loss: 0.119086, mae: 0.330497, mean_q: 3.935310
 12400/100000: episode: 124, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 184.289, mean reward: 1.843 [1.480, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.053, 10.324], loss: 0.126514, mae: 0.345821, mean_q: 3.934939
 12500/100000: episode: 125, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 197.142, mean reward: 1.971 [1.457, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.962, 10.098], loss: 0.115282, mae: 0.335506, mean_q: 3.924406
 12600/100000: episode: 126, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 215.196, mean reward: 2.152 [1.482, 3.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.279, 10.418], loss: 0.117407, mae: 0.323785, mean_q: 3.905602
 12700/100000: episode: 127, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 189.674, mean reward: 1.897 [1.459, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.956, 10.098], loss: 0.126973, mae: 0.344544, mean_q: 3.921407
 12800/100000: episode: 128, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 210.241, mean reward: 2.102 [1.481, 5.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.885, 10.239], loss: 0.120436, mae: 0.337181, mean_q: 3.918615
 12900/100000: episode: 129, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 193.172, mean reward: 1.932 [1.466, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.994, 10.098], loss: 0.113044, mae: 0.327705, mean_q: 3.907472
 13000/100000: episode: 130, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 184.241, mean reward: 1.842 [1.475, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.527, 10.278], loss: 0.128111, mae: 0.338536, mean_q: 3.904357
 13100/100000: episode: 131, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 202.671, mean reward: 2.027 [1.443, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.065, 10.098], loss: 0.112253, mae: 0.322388, mean_q: 3.896803
 13200/100000: episode: 132, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.408, mean reward: 1.894 [1.450, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.385, 10.150], loss: 0.113425, mae: 0.331780, mean_q: 3.905515
 13300/100000: episode: 133, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 192.730, mean reward: 1.927 [1.472, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.460, 10.277], loss: 0.109144, mae: 0.319025, mean_q: 3.896636
 13400/100000: episode: 134, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 184.810, mean reward: 1.848 [1.468, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.439, 10.149], loss: 0.119326, mae: 0.333326, mean_q: 3.911614
 13500/100000: episode: 135, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 194.227, mean reward: 1.942 [1.496, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.819, 10.098], loss: 0.138560, mae: 0.343841, mean_q: 3.902030
 13600/100000: episode: 136, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 199.819, mean reward: 1.998 [1.556, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.231, 10.098], loss: 0.109825, mae: 0.328073, mean_q: 3.897024
 13700/100000: episode: 137, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 231.091, mean reward: 2.311 [1.478, 5.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-0.612, 10.098], loss: 0.108146, mae: 0.318600, mean_q: 3.895770
 13800/100000: episode: 138, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 181.056, mean reward: 1.811 [1.443, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.670, 10.251], loss: 0.091714, mae: 0.302293, mean_q: 3.890568
 13900/100000: episode: 139, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 187.334, mean reward: 1.873 [1.487, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.429, 10.257], loss: 0.111297, mae: 0.330536, mean_q: 3.883652
 14000/100000: episode: 140, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 203.696, mean reward: 2.037 [1.442, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.649, 10.098], loss: 0.098823, mae: 0.314383, mean_q: 3.869618
 14100/100000: episode: 141, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 176.893, mean reward: 1.769 [1.448, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.312, 10.194], loss: 0.093225, mae: 0.304688, mean_q: 3.870317
 14200/100000: episode: 142, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 186.999, mean reward: 1.870 [1.445, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.504, 10.294], loss: 0.092752, mae: 0.302737, mean_q: 3.845726
 14300/100000: episode: 143, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 184.593, mean reward: 1.846 [1.451, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.304, 10.259], loss: 0.103219, mae: 0.318861, mean_q: 3.865467
 14400/100000: episode: 144, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 181.860, mean reward: 1.819 [1.477, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.834, 10.132], loss: 0.095057, mae: 0.310413, mean_q: 3.869985
 14500/100000: episode: 145, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 206.612, mean reward: 2.066 [1.467, 4.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.561, 10.098], loss: 0.097253, mae: 0.303544, mean_q: 3.846085
 14600/100000: episode: 146, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 182.864, mean reward: 1.829 [1.434, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.311, 10.196], loss: 0.095235, mae: 0.311808, mean_q: 3.861306
 14700/100000: episode: 147, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 196.950, mean reward: 1.969 [1.494, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.846, 10.098], loss: 0.093361, mae: 0.302848, mean_q: 3.855489
 14800/100000: episode: 148, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 187.581, mean reward: 1.876 [1.456, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.023, 10.121], loss: 0.093518, mae: 0.304834, mean_q: 3.864814
 14900/100000: episode: 149, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 177.220, mean reward: 1.772 [1.460, 2.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.010, 10.144], loss: 0.090404, mae: 0.304413, mean_q: 3.842680
[Info] 1-TH LEVEL FOUND: 4.447652339935303, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.087s, episode steps: 100, steps per second: 20, episode reward: 183.768, mean reward: 1.838 [1.480, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.994, 10.098], loss: 0.090223, mae: 0.307473, mean_q: 3.850382
 15016/100000: episode: 151, duration: 0.113s, episode steps: 16, steps per second: 142, episode reward: 44.570, mean reward: 2.786 [2.184, 4.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.718, 10.100], loss: 0.093924, mae: 0.298581, mean_q: 3.853359
 15034/100000: episode: 152, duration: 0.086s, episode steps: 18, steps per second: 209, episode reward: 47.414, mean reward: 2.634 [2.182, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.174, 10.100], loss: 0.112202, mae: 0.327652, mean_q: 3.879534
 15052/100000: episode: 153, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 51.502, mean reward: 2.861 [2.265, 4.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.225, 10.100], loss: 0.083548, mae: 0.278467, mean_q: 3.791570
 15070/100000: episode: 154, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 45.669, mean reward: 2.537 [1.897, 4.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.220, 10.100], loss: 0.092697, mae: 0.304970, mean_q: 3.868096
 15087/100000: episode: 155, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 44.150, mean reward: 2.597 [2.140, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.478, 10.100], loss: 0.089217, mae: 0.292304, mean_q: 3.846675
 15186/100000: episode: 156, duration: 0.543s, episode steps: 99, steps per second: 182, episode reward: 178.059, mean reward: 1.799 [1.484, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-2.131, 10.127], loss: 0.081102, mae: 0.293981, mean_q: 3.845897
 15282/100000: episode: 157, duration: 0.473s, episode steps: 96, steps per second: 203, episode reward: 188.715, mean reward: 1.966 [1.452, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-1.053, 10.100], loss: 0.080376, mae: 0.288878, mean_q: 3.826679
 15300/100000: episode: 158, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 43.649, mean reward: 2.425 [2.024, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.328, 10.100], loss: 0.076042, mae: 0.278691, mean_q: 3.831889
 15396/100000: episode: 159, duration: 0.459s, episode steps: 96, steps per second: 209, episode reward: 197.870, mean reward: 2.061 [1.476, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.582, 10.100], loss: 0.077382, mae: 0.285668, mean_q: 3.833539
 15415/100000: episode: 160, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 42.241, mean reward: 2.223 [1.879, 2.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.330, 10.100], loss: 0.091052, mae: 0.306042, mean_q: 3.836590
 15511/100000: episode: 161, duration: 0.481s, episode steps: 96, steps per second: 200, episode reward: 191.651, mean reward: 1.996 [1.466, 4.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-1.016, 10.563], loss: 0.084050, mae: 0.298828, mean_q: 3.836679
 15529/100000: episode: 162, duration: 0.086s, episode steps: 18, steps per second: 209, episode reward: 44.197, mean reward: 2.455 [1.791, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.295, 10.100], loss: 0.071331, mae: 0.282608, mean_q: 3.813349
 15548/100000: episode: 163, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 47.173, mean reward: 2.483 [1.952, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.204, 10.100], loss: 0.087111, mae: 0.295591, mean_q: 3.833095
 15566/100000: episode: 164, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 47.243, mean reward: 2.625 [2.227, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.191, 10.100], loss: 0.111930, mae: 0.330588, mean_q: 3.862022
 15662/100000: episode: 165, duration: 0.469s, episode steps: 96, steps per second: 205, episode reward: 190.758, mean reward: 1.987 [1.477, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.485 [-0.680, 10.136], loss: 0.084280, mae: 0.295244, mean_q: 3.843505
 15678/100000: episode: 166, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 47.727, mean reward: 2.983 [2.366, 4.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.066, 10.100], loss: 0.095876, mae: 0.311979, mean_q: 3.840903
 15694/100000: episode: 167, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 36.714, mean reward: 2.295 [1.616, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.040, 10.100], loss: 0.114494, mae: 0.335292, mean_q: 3.906153
 15713/100000: episode: 168, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 38.908, mean reward: 2.048 [1.688, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.159, 10.100], loss: 0.078386, mae: 0.298155, mean_q: 3.886668
 15723/100000: episode: 169, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 22.876, mean reward: 2.288 [2.030, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.253, 10.100], loss: 0.079867, mae: 0.289732, mean_q: 3.849761
 15741/100000: episode: 170, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 42.246, mean reward: 2.347 [2.001, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.179, 10.100], loss: 0.077177, mae: 0.289622, mean_q: 3.861007
 15760/100000: episode: 171, duration: 0.118s, episode steps: 19, steps per second: 161, episode reward: 52.638, mean reward: 2.770 [2.112, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.437, 10.100], loss: 0.065772, mae: 0.269210, mean_q: 3.834442
 15781/100000: episode: 172, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 54.958, mean reward: 2.617 [1.996, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.669, 10.100], loss: 0.082627, mae: 0.296594, mean_q: 3.864709
 15799/100000: episode: 173, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 46.334, mean reward: 2.574 [2.246, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.961, 10.100], loss: 0.108875, mae: 0.326172, mean_q: 3.882997
 15826/100000: episode: 174, duration: 0.162s, episode steps: 27, steps per second: 166, episode reward: 98.752, mean reward: 3.657 [2.588, 6.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.362, 10.100], loss: 0.111879, mae: 0.323726, mean_q: 3.905516
 15922/100000: episode: 175, duration: 0.466s, episode steps: 96, steps per second: 206, episode reward: 171.445, mean reward: 1.786 [1.442, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.619, 10.121], loss: 0.094723, mae: 0.306719, mean_q: 3.900990
 15941/100000: episode: 176, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 44.239, mean reward: 2.328 [1.902, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.038, 10.100], loss: 0.102172, mae: 0.313307, mean_q: 3.888441
 15951/100000: episode: 177, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 30.118, mean reward: 3.012 [2.224, 4.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.410, 10.100], loss: 0.080226, mae: 0.300732, mean_q: 3.955369
 15967/100000: episode: 178, duration: 0.080s, episode steps: 16, steps per second: 199, episode reward: 52.572, mean reward: 3.286 [2.376, 4.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.920, 10.100], loss: 0.089570, mae: 0.295829, mean_q: 3.878783
 16066/100000: episode: 179, duration: 0.506s, episode steps: 99, steps per second: 195, episode reward: 185.966, mean reward: 1.878 [1.505, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.882, 10.100], loss: 0.108546, mae: 0.326123, mean_q: 3.912785
 16084/100000: episode: 180, duration: 0.090s, episode steps: 18, steps per second: 199, episode reward: 39.181, mean reward: 2.177 [1.891, 2.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.393, 10.100], loss: 0.098563, mae: 0.305999, mean_q: 3.912025
 16103/100000: episode: 181, duration: 0.091s, episode steps: 19, steps per second: 210, episode reward: 43.843, mean reward: 2.308 [1.909, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.055, 10.100], loss: 0.081634, mae: 0.302859, mean_q: 3.881202
 16130/100000: episode: 182, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 144.875, mean reward: 5.366 [2.185, 17.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.186, 10.100], loss: 0.114244, mae: 0.344114, mean_q: 3.976051
 16151/100000: episode: 183, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 54.356, mean reward: 2.588 [1.754, 4.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.552, 10.100], loss: 0.128918, mae: 0.334588, mean_q: 3.947829
 16250/100000: episode: 184, duration: 0.506s, episode steps: 99, steps per second: 196, episode reward: 192.943, mean reward: 1.949 [1.491, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-0.555, 10.137], loss: 0.133559, mae: 0.335964, mean_q: 3.941017
 16268/100000: episode: 185, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 55.819, mean reward: 3.101 [2.439, 4.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.507, 10.100], loss: 0.101418, mae: 0.307712, mean_q: 3.912058
 16285/100000: episode: 186, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 38.178, mean reward: 2.246 [1.621, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.256, 10.100], loss: 0.108492, mae: 0.327269, mean_q: 3.928959
 16303/100000: episode: 187, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 40.443, mean reward: 2.247 [1.635, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.039, 10.100], loss: 0.527005, mae: 0.444841, mean_q: 4.031326
 16324/100000: episode: 188, duration: 0.111s, episode steps: 21, steps per second: 188, episode reward: 60.297, mean reward: 2.871 [2.361, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.896, 10.100], loss: 0.216870, mae: 0.384132, mean_q: 3.947588
 16340/100000: episode: 189, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 42.706, mean reward: 2.669 [2.315, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.271, 10.100], loss: 0.082547, mae: 0.305932, mean_q: 3.953048
 16359/100000: episode: 190, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 46.716, mean reward: 2.459 [1.867, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.085, 10.100], loss: 0.303436, mae: 0.371081, mean_q: 3.991757
 16375/100000: episode: 191, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 40.778, mean reward: 2.549 [2.159, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.397, 10.100], loss: 0.142454, mae: 0.345853, mean_q: 3.943310
 16471/100000: episode: 192, duration: 0.494s, episode steps: 96, steps per second: 194, episode reward: 177.941, mean reward: 1.854 [1.443, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-1.032, 10.100], loss: 0.155350, mae: 0.348191, mean_q: 3.971451
 16570/100000: episode: 193, duration: 0.507s, episode steps: 99, steps per second: 195, episode reward: 191.743, mean reward: 1.937 [1.437, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-1.218, 10.178], loss: 0.137945, mae: 0.334188, mean_q: 3.982634
 16666/100000: episode: 194, duration: 0.501s, episode steps: 96, steps per second: 192, episode reward: 179.203, mean reward: 1.867 [1.440, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-0.712, 10.100], loss: 0.166481, mae: 0.340962, mean_q: 3.990956
 16676/100000: episode: 195, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 26.250, mean reward: 2.625 [2.106, 3.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.288, 10.100], loss: 0.124867, mae: 0.338379, mean_q: 3.984769
 16686/100000: episode: 196, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 26.356, mean reward: 2.636 [2.186, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.434, 10.100], loss: 0.197101, mae: 0.365345, mean_q: 4.115863
 16782/100000: episode: 197, duration: 0.499s, episode steps: 96, steps per second: 192, episode reward: 182.722, mean reward: 1.903 [1.480, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.746, 10.147], loss: 0.118238, mae: 0.315300, mean_q: 3.983819
 16792/100000: episode: 198, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 21.284, mean reward: 2.128 [1.943, 2.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.181, 10.100], loss: 0.296513, mae: 0.420945, mean_q: 4.069238
 16809/100000: episode: 199, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 50.362, mean reward: 2.962 [2.293, 5.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.301, 10.100], loss: 0.136903, mae: 0.323518, mean_q: 3.988325
 16828/100000: episode: 200, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 35.635, mean reward: 1.876 [1.470, 2.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.129, 10.100], loss: 0.145423, mae: 0.338772, mean_q: 4.006920
 16844/100000: episode: 201, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 39.460, mean reward: 2.466 [1.756, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.487, 10.100], loss: 0.083193, mae: 0.284941, mean_q: 3.933495
 16860/100000: episode: 202, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 51.421, mean reward: 3.214 [2.440, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.281, 10.100], loss: 0.106200, mae: 0.320006, mean_q: 4.003224
 16887/100000: episode: 203, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 115.084, mean reward: 4.262 [2.185, 9.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.201, 10.100], loss: 0.167357, mae: 0.360070, mean_q: 4.033893
 16986/100000: episode: 204, duration: 0.490s, episode steps: 99, steps per second: 202, episode reward: 192.540, mean reward: 1.945 [1.460, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.605, 10.150], loss: 0.154258, mae: 0.349831, mean_q: 4.021557
 17003/100000: episode: 205, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 41.782, mean reward: 2.458 [2.123, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.231, 10.100], loss: 0.174902, mae: 0.367067, mean_q: 4.075375
 17030/100000: episode: 206, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 123.151, mean reward: 4.561 [1.938, 7.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.755, 10.100], loss: 0.240163, mae: 0.364309, mean_q: 4.056417
 17057/100000: episode: 207, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 84.426, mean reward: 3.127 [2.422, 4.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.914, 10.100], loss: 0.173483, mae: 0.357926, mean_q: 4.028843
 17076/100000: episode: 208, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 77.757, mean reward: 4.092 [2.049, 6.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.615, 10.100], loss: 0.182663, mae: 0.365463, mean_q: 4.089898
 17175/100000: episode: 209, duration: 0.492s, episode steps: 99, steps per second: 201, episode reward: 193.989, mean reward: 1.959 [1.499, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.815, 10.163], loss: 0.144435, mae: 0.348010, mean_q: 4.059494
 17192/100000: episode: 210, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 58.945, mean reward: 3.467 [2.380, 4.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.511, 10.100], loss: 0.097826, mae: 0.314152, mean_q: 3.971265
 17202/100000: episode: 211, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 26.339, mean reward: 2.634 [2.081, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.290, 10.100], loss: 0.175925, mae: 0.386964, mean_q: 4.174311
 17301/100000: episode: 212, duration: 0.510s, episode steps: 99, steps per second: 194, episode reward: 185.097, mean reward: 1.870 [1.455, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-0.972, 10.100], loss: 0.174880, mae: 0.366658, mean_q: 4.076655
 17322/100000: episode: 213, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 58.617, mean reward: 2.791 [2.348, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.658, 10.100], loss: 0.116160, mae: 0.333560, mean_q: 4.020074
 17421/100000: episode: 214, duration: 0.488s, episode steps: 99, steps per second: 203, episode reward: 204.029, mean reward: 2.061 [1.450, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-0.961, 10.100], loss: 0.201155, mae: 0.375804, mean_q: 4.089692
 17517/100000: episode: 215, duration: 0.485s, episode steps: 96, steps per second: 198, episode reward: 179.950, mean reward: 1.874 [1.445, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.681, 10.256], loss: 0.174197, mae: 0.368294, mean_q: 4.073704
 17527/100000: episode: 216, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 27.146, mean reward: 2.715 [2.037, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.514, 10.100], loss: 0.278316, mae: 0.401845, mean_q: 4.083105
 17626/100000: episode: 217, duration: 0.531s, episode steps: 99, steps per second: 186, episode reward: 201.367, mean reward: 2.034 [1.466, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-1.125, 10.100], loss: 0.148769, mae: 0.362286, mean_q: 4.088148
 17636/100000: episode: 218, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 22.754, mean reward: 2.275 [1.994, 2.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.519, 10.100], loss: 0.183256, mae: 0.344088, mean_q: 4.073678
 17735/100000: episode: 219, duration: 0.493s, episode steps: 99, steps per second: 201, episode reward: 179.653, mean reward: 1.815 [1.472, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.779, 10.120], loss: 0.186077, mae: 0.366791, mean_q: 4.088688
 17834/100000: episode: 220, duration: 0.493s, episode steps: 99, steps per second: 201, episode reward: 188.250, mean reward: 1.902 [1.435, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.590, 10.258], loss: 0.168634, mae: 0.363600, mean_q: 4.085708
 17933/100000: episode: 221, duration: 0.518s, episode steps: 99, steps per second: 191, episode reward: 194.954, mean reward: 1.969 [1.448, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.459 [-1.099, 10.168], loss: 0.177892, mae: 0.361927, mean_q: 4.051735
 17943/100000: episode: 222, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 23.078, mean reward: 2.308 [2.104, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.262, 10.100], loss: 0.414938, mae: 0.351984, mean_q: 4.050117
 17970/100000: episode: 223, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 103.925, mean reward: 3.849 [2.826, 6.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.399, 10.100], loss: 0.182607, mae: 0.400512, mean_q: 4.135782
 17988/100000: episode: 224, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 66.103, mean reward: 3.672 [2.243, 6.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.929, 10.100], loss: 0.140294, mae: 0.381866, mean_q: 4.107599
 18006/100000: episode: 225, duration: 0.086s, episode steps: 18, steps per second: 208, episode reward: 43.911, mean reward: 2.440 [1.665, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.055, 10.100], loss: 0.341606, mae: 0.384520, mean_q: 4.124364
 18022/100000: episode: 226, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 45.038, mean reward: 2.815 [2.331, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.165, 10.100], loss: 0.140573, mae: 0.373666, mean_q: 4.165443
 18040/100000: episode: 227, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 82.670, mean reward: 4.593 [2.841, 8.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.430, 10.100], loss: 0.251934, mae: 0.404596, mean_q: 4.105873
 18136/100000: episode: 228, duration: 0.468s, episode steps: 96, steps per second: 205, episode reward: 181.591, mean reward: 1.892 [1.445, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-0.328, 10.154], loss: 0.194796, mae: 0.379668, mean_q: 4.133081
 18235/100000: episode: 229, duration: 0.536s, episode steps: 99, steps per second: 185, episode reward: 205.721, mean reward: 2.078 [1.553, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.813, 10.100], loss: 0.251946, mae: 0.386436, mean_q: 4.151591
 18251/100000: episode: 230, duration: 0.077s, episode steps: 16, steps per second: 207, episode reward: 38.513, mean reward: 2.407 [2.095, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.417, 10.100], loss: 0.221590, mae: 0.418788, mean_q: 4.126828
 18270/100000: episode: 231, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 45.561, mean reward: 2.398 [2.086, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.875, 10.100], loss: 0.133303, mae: 0.352575, mean_q: 4.115752
 18366/100000: episode: 232, duration: 0.471s, episode steps: 96, steps per second: 204, episode reward: 247.379, mean reward: 2.577 [1.478, 9.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.702, 10.100], loss: 0.210402, mae: 0.383658, mean_q: 4.139866
 18385/100000: episode: 233, duration: 0.119s, episode steps: 19, steps per second: 160, episode reward: 45.247, mean reward: 2.381 [1.849, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.224, 10.100], loss: 0.183463, mae: 0.396092, mean_q: 4.234810
 18401/100000: episode: 234, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 41.359, mean reward: 2.585 [2.086, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.280, 10.100], loss: 0.217569, mae: 0.400300, mean_q: 4.241236
 18500/100000: episode: 235, duration: 0.507s, episode steps: 99, steps per second: 195, episode reward: 193.165, mean reward: 1.951 [1.460, 9.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.839, 10.100], loss: 0.223553, mae: 0.396751, mean_q: 4.196551
 18510/100000: episode: 236, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 23.219, mean reward: 2.322 [2.134, 2.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.340, 10.100], loss: 0.192220, mae: 0.345355, mean_q: 4.130242
 18526/100000: episode: 237, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 42.214, mean reward: 2.638 [2.136, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.209, 10.100], loss: 0.194918, mae: 0.382003, mean_q: 4.173148
 18536/100000: episode: 238, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 25.748, mean reward: 2.575 [2.257, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.363, 10.100], loss: 0.268730, mae: 0.406111, mean_q: 4.191549
 18632/100000: episode: 239, duration: 0.477s, episode steps: 96, steps per second: 201, episode reward: 192.698, mean reward: 2.007 [1.451, 4.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.140, 10.395], loss: 0.143092, mae: 0.352206, mean_q: 4.129737
[Info] 2-TH LEVEL FOUND: 6.75429630279541, Considering 10/90 traces
 18728/100000: episode: 240, duration: 4.659s, episode steps: 96, steps per second: 21, episode reward: 189.042, mean reward: 1.969 [1.441, 6.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.154, 10.323], loss: 0.233396, mae: 0.400067, mean_q: 4.170066
 18749/100000: episode: 241, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 55.365, mean reward: 2.636 [2.132, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.611, 10.100], loss: 0.146025, mae: 0.355256, mean_q: 4.174459
 18772/100000: episode: 242, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 99.484, mean reward: 4.325 [3.331, 6.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.412, 10.100], loss: 0.259648, mae: 0.413523, mean_q: 4.298832
 18795/100000: episode: 243, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 141.186, mean reward: 6.139 [3.315, 9.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.366, 10.100], loss: 0.170364, mae: 0.375879, mean_q: 4.130228
 18818/100000: episode: 244, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 116.187, mean reward: 5.052 [3.631, 8.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.457, 10.100], loss: 0.213492, mae: 0.404393, mean_q: 4.224252
 18827/100000: episode: 245, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 64.023, mean reward: 7.114 [4.666, 12.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.596, 10.100], loss: 0.375755, mae: 0.463432, mean_q: 4.136141
 18850/100000: episode: 246, duration: 0.130s, episode steps: 23, steps per second: 177, episode reward: 78.950, mean reward: 3.433 [2.405, 5.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.128, 10.100], loss: 0.364463, mae: 0.448760, mean_q: 4.344987
 18871/100000: episode: 247, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 54.541, mean reward: 2.597 [2.118, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.371, 10.100], loss: 0.380625, mae: 0.430531, mean_q: 4.284486
 18880/100000: episode: 248, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 48.726, mean reward: 5.414 [3.983, 8.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.416, 10.100], loss: 0.340131, mae: 0.438777, mean_q: 4.288401
 18905/100000: episode: 249, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 95.128, mean reward: 3.805 [3.077, 6.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.265, 10.100], loss: 0.283990, mae: 0.473172, mean_q: 4.344681
 18926/100000: episode: 250, duration: 0.101s, episode steps: 21, steps per second: 209, episode reward: 70.013, mean reward: 3.334 [2.136, 5.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.362, 10.100], loss: 0.181098, mae: 0.371884, mean_q: 4.274334
 18949/100000: episode: 251, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 141.381, mean reward: 6.147 [4.169, 9.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.404, 10.100], loss: 0.354852, mae: 0.487155, mean_q: 4.453894
 18959/100000: episode: 252, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 119.035, mean reward: 11.904 [3.696, 34.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.553, 10.100], loss: 0.195354, mae: 0.409986, mean_q: 4.262390
 18985/100000: episode: 253, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 111.898, mean reward: 4.304 [3.006, 6.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.035, 10.100], loss: 0.331944, mae: 0.480324, mean_q: 4.447493
 19002/100000: episode: 254, duration: 0.086s, episode steps: 17, steps per second: 199, episode reward: 57.604, mean reward: 3.388 [2.729, 6.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.147, 10.100], loss: 0.344331, mae: 0.459310, mean_q: 4.454510
 19012/100000: episode: 255, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 31.318, mean reward: 3.132 [2.891, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.619, 10.100], loss: 0.298902, mae: 0.424921, mean_q: 4.314134
 19029/100000: episode: 256, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 56.673, mean reward: 3.334 [2.460, 4.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.279, 10.100], loss: 0.233252, mae: 0.430794, mean_q: 4.424574
 19038/100000: episode: 257, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 37.939, mean reward: 4.215 [3.370, 7.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.382, 10.100], loss: 0.249285, mae: 0.423248, mean_q: 4.347425
 19064/100000: episode: 258, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 115.257, mean reward: 4.433 [3.095, 7.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.424, 10.100], loss: 0.295523, mae: 0.460653, mean_q: 4.447500
 19089/100000: episode: 259, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 103.034, mean reward: 4.121 [2.956, 5.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.693, 10.100], loss: 0.471171, mae: 0.550321, mean_q: 4.437502
 19114/100000: episode: 260, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 69.830, mean reward: 2.793 [2.117, 5.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.568, 10.100], loss: 0.295815, mae: 0.487031, mean_q: 4.512393
 19140/100000: episode: 261, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 93.685, mean reward: 3.603 [1.963, 8.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.321, 10.100], loss: 0.255811, mae: 0.444301, mean_q: 4.497997
 19165/100000: episode: 262, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 132.914, mean reward: 5.317 [2.982, 11.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.295, 10.100], loss: 0.238458, mae: 0.462463, mean_q: 4.528447
 19190/100000: episode: 263, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 178.151, mean reward: 7.126 [3.662, 15.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.805, 10.100], loss: 1.035988, mae: 0.568704, mean_q: 4.506951
 19215/100000: episode: 264, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 74.814, mean reward: 2.993 [1.920, 5.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.754, 10.100], loss: 0.380477, mae: 0.568923, mean_q: 4.606117
 19240/100000: episode: 265, duration: 0.122s, episode steps: 25, steps per second: 206, episode reward: 99.651, mean reward: 3.986 [2.538, 7.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.352, 10.100], loss: 0.483591, mae: 0.574028, mean_q: 4.551397
 19261/100000: episode: 266, duration: 0.111s, episode steps: 21, steps per second: 190, episode reward: 51.402, mean reward: 2.448 [1.874, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.235, 10.100], loss: 0.419990, mae: 0.542310, mean_q: 4.669508
[Info] FALSIFICATION!
[Info] Levels: [4.4476523, 6.7542963, 8.195258]
[Info] Cond. Prob: [0.1, 0.1, 0.02]
[Info] Error Prob: 0.00020000000000000004

 19275/100000: episode: 267, duration: 4.710s, episode steps: 14, steps per second: 3, episode reward: 150.927, mean reward: 10.780 [2.403, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.175, 9.988], loss: 0.325918, mae: 0.503286, mean_q: 4.584261
 19375/100000: episode: 268, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.345, mean reward: 1.813 [1.462, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.912, 10.098], loss: 1.931227, mae: 0.590795, mean_q: 4.709774
 19475/100000: episode: 269, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 195.874, mean reward: 1.959 [1.451, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.844, 10.101], loss: 0.506632, mae: 0.552644, mean_q: 4.635861
 19575/100000: episode: 270, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 187.208, mean reward: 1.872 [1.447, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.781, 10.125], loss: 0.502715, mae: 0.526827, mean_q: 4.631752
 19675/100000: episode: 271, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 243.357, mean reward: 2.434 [1.603, 5.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.058, 10.098], loss: 0.597782, mae: 0.549319, mean_q: 4.723990
 19775/100000: episode: 272, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 185.410, mean reward: 1.854 [1.476, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.312, 10.098], loss: 0.364199, mae: 0.489707, mean_q: 4.673141
 19875/100000: episode: 273, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 192.727, mean reward: 1.927 [1.455, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.812, 10.098], loss: 1.733289, mae: 0.583189, mean_q: 4.682062
 19975/100000: episode: 274, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 195.172, mean reward: 1.952 [1.443, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.827, 10.263], loss: 0.413097, mae: 0.511298, mean_q: 4.630579
 20075/100000: episode: 275, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 185.852, mean reward: 1.859 [1.434, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.950, 10.284], loss: 1.739504, mae: 0.620723, mean_q: 4.718874
 20175/100000: episode: 276, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 181.107, mean reward: 1.811 [1.467, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.350, 10.098], loss: 0.462720, mae: 0.484835, mean_q: 4.661487
 20275/100000: episode: 277, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.303, mean reward: 1.883 [1.461, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.563, 10.232], loss: 3.229175, mae: 0.661815, mean_q: 4.743679
 20375/100000: episode: 278, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 195.015, mean reward: 1.950 [1.460, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.053, 10.159], loss: 0.423626, mae: 0.503438, mean_q: 4.678588
 20475/100000: episode: 279, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.456, mean reward: 1.935 [1.479, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.188, 10.098], loss: 1.917674, mae: 0.615198, mean_q: 4.732222
 20575/100000: episode: 280, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 207.520, mean reward: 2.075 [1.509, 4.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.768, 10.098], loss: 1.744123, mae: 0.559636, mean_q: 4.656551
 20675/100000: episode: 281, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.023, mean reward: 1.920 [1.495, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.837, 10.218], loss: 0.347867, mae: 0.484118, mean_q: 4.632783
 20775/100000: episode: 282, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 183.980, mean reward: 1.840 [1.457, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.239, 10.098], loss: 1.866994, mae: 0.585348, mean_q: 4.616451
 20875/100000: episode: 283, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 209.573, mean reward: 2.096 [1.476, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.880, 10.098], loss: 0.305157, mae: 0.461483, mean_q: 4.583251
 20975/100000: episode: 284, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 181.622, mean reward: 1.816 [1.440, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.464, 10.098], loss: 1.668420, mae: 0.513261, mean_q: 4.618492
 21075/100000: episode: 285, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 195.932, mean reward: 1.959 [1.457, 9.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.819, 10.098], loss: 0.497341, mae: 0.520591, mean_q: 4.616851
 21175/100000: episode: 286, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 181.125, mean reward: 1.811 [1.442, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.943, 10.098], loss: 1.681962, mae: 0.529883, mean_q: 4.512796
 21275/100000: episode: 287, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 210.891, mean reward: 2.109 [1.483, 4.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.268, 10.098], loss: 0.516166, mae: 0.504469, mean_q: 4.544661
 21375/100000: episode: 288, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 185.696, mean reward: 1.857 [1.461, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.301, 10.247], loss: 0.492994, mae: 0.484971, mean_q: 4.568129
 21475/100000: episode: 289, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 196.217, mean reward: 1.962 [1.452, 4.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.852, 10.098], loss: 0.517695, mae: 0.485902, mean_q: 4.529570
 21575/100000: episode: 290, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 215.357, mean reward: 2.154 [1.539, 5.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.736, 10.421], loss: 0.403362, mae: 0.491594, mean_q: 4.599544
 21675/100000: episode: 291, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 218.139, mean reward: 2.181 [1.471, 7.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.016, 10.419], loss: 0.328898, mae: 0.451782, mean_q: 4.541314
 21775/100000: episode: 292, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 196.869, mean reward: 1.969 [1.483, 8.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.965, 10.098], loss: 0.484963, mae: 0.494780, mean_q: 4.537631
 21875/100000: episode: 293, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 208.089, mean reward: 2.081 [1.521, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.769, 10.212], loss: 0.256985, mae: 0.426204, mean_q: 4.453259
 21975/100000: episode: 294, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 190.530, mean reward: 1.905 [1.462, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.756, 10.273], loss: 0.411860, mae: 0.457089, mean_q: 4.470705
 22075/100000: episode: 295, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 181.189, mean reward: 1.812 [1.444, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.846, 10.308], loss: 0.324872, mae: 0.447703, mean_q: 4.445633
 22175/100000: episode: 296, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 216.499, mean reward: 2.165 [1.492, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.960, 10.098], loss: 4.470709, mae: 0.712247, mean_q: 4.614355
 22275/100000: episode: 297, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 186.989, mean reward: 1.870 [1.440, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.083, 10.237], loss: 1.603146, mae: 0.510429, mean_q: 4.480701
 22375/100000: episode: 298, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 239.868, mean reward: 2.399 [1.438, 21.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.382, 10.098], loss: 2.976706, mae: 0.590165, mean_q: 4.514126
 22475/100000: episode: 299, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 185.909, mean reward: 1.859 [1.448, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.382, 10.108], loss: 0.578143, mae: 0.524255, mean_q: 4.511499
 22575/100000: episode: 300, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 177.985, mean reward: 1.780 [1.483, 2.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.191, 10.139], loss: 1.629483, mae: 0.509563, mean_q: 4.433459
 22675/100000: episode: 301, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 208.254, mean reward: 2.083 [1.489, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.517, 10.224], loss: 2.040704, mae: 0.555222, mean_q: 4.533422
 22775/100000: episode: 302, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 196.803, mean reward: 1.968 [1.441, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.575, 10.108], loss: 0.529247, mae: 0.509472, mean_q: 4.446167
 22875/100000: episode: 303, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 186.538, mean reward: 1.865 [1.439, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.868, 10.266], loss: 0.646466, mae: 0.502754, mean_q: 4.503868
 22975/100000: episode: 304, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 191.381, mean reward: 1.914 [1.436, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.595, 10.242], loss: 3.046861, mae: 0.546780, mean_q: 4.421938
 23075/100000: episode: 305, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 187.508, mean reward: 1.875 [1.433, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.887, 10.098], loss: 1.919400, mae: 0.619742, mean_q: 4.451118
 23175/100000: episode: 306, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 182.872, mean reward: 1.829 [1.470, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.754, 10.251], loss: 0.365908, mae: 0.461843, mean_q: 4.376222
 23275/100000: episode: 307, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 220.239, mean reward: 2.202 [1.458, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.095, 10.098], loss: 1.584581, mae: 0.506901, mean_q: 4.407131
 23375/100000: episode: 308, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 198.411, mean reward: 1.984 [1.438, 4.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.881, 10.308], loss: 0.401275, mae: 0.454109, mean_q: 4.353255
 23475/100000: episode: 309, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 189.177, mean reward: 1.892 [1.469, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.579, 10.098], loss: 0.559638, mae: 0.466832, mean_q: 4.351987
 23575/100000: episode: 310, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 198.418, mean reward: 1.984 [1.451, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.282, 10.203], loss: 0.498089, mae: 0.461724, mean_q: 4.435955
 23675/100000: episode: 311, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 198.312, mean reward: 1.983 [1.465, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.005, 10.098], loss: 2.072335, mae: 0.599780, mean_q: 4.449875
 23775/100000: episode: 312, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.492, mean reward: 1.895 [1.498, 2.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.790, 10.278], loss: 0.328275, mae: 0.425959, mean_q: 4.224524
 23875/100000: episode: 313, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 196.970, mean reward: 1.970 [1.452, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.187, 10.098], loss: 1.465587, mae: 0.425284, mean_q: 4.188978
 23975/100000: episode: 314, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 184.942, mean reward: 1.849 [1.450, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.818, 10.139], loss: 1.517596, mae: 0.471824, mean_q: 4.111101
 24075/100000: episode: 315, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 196.761, mean reward: 1.968 [1.483, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.811, 10.098], loss: 0.152324, mae: 0.352364, mean_q: 3.950460
 24175/100000: episode: 316, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 206.699, mean reward: 2.067 [1.466, 4.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.291, 10.098], loss: 1.491022, mae: 0.404943, mean_q: 3.978438
 24275/100000: episode: 317, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.106, mean reward: 1.921 [1.483, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.745, 10.098], loss: 0.097340, mae: 0.310286, mean_q: 3.882237
 24375/100000: episode: 318, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 182.932, mean reward: 1.829 [1.472, 2.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.652, 10.098], loss: 0.116112, mae: 0.318873, mean_q: 3.879167
 24475/100000: episode: 319, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 177.900, mean reward: 1.779 [1.474, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.765, 10.144], loss: 0.094460, mae: 0.306459, mean_q: 3.870843
 24575/100000: episode: 320, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 185.838, mean reward: 1.858 [1.464, 4.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.739, 10.140], loss: 0.111932, mae: 0.310214, mean_q: 3.880625
 24675/100000: episode: 321, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 201.617, mean reward: 2.016 [1.444, 4.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.841, 10.098], loss: 0.168464, mae: 0.316970, mean_q: 3.864488
 24775/100000: episode: 322, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 202.555, mean reward: 2.026 [1.454, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.265, 10.098], loss: 0.105878, mae: 0.307692, mean_q: 3.866965
 24875/100000: episode: 323, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 222.433, mean reward: 2.224 [1.492, 5.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.018, 10.432], loss: 0.109967, mae: 0.310038, mean_q: 3.868367
 24975/100000: episode: 324, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 189.537, mean reward: 1.895 [1.439, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.812, 10.098], loss: 0.170305, mae: 0.315890, mean_q: 3.873636
 25075/100000: episode: 325, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 202.506, mean reward: 2.025 [1.447, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.410, 10.098], loss: 0.183165, mae: 0.327495, mean_q: 3.881158
 25175/100000: episode: 326, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 199.423, mean reward: 1.994 [1.483, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.210, 10.098], loss: 0.098930, mae: 0.299978, mean_q: 3.867441
 25275/100000: episode: 327, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 218.626, mean reward: 2.186 [1.502, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.625, 10.098], loss: 0.178278, mae: 0.325467, mean_q: 3.884103
 25375/100000: episode: 328, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 182.881, mean reward: 1.829 [1.440, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.252, 10.098], loss: 0.102520, mae: 0.307010, mean_q: 3.904759
 25475/100000: episode: 329, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 216.474, mean reward: 2.165 [1.509, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.388, 10.098], loss: 0.169795, mae: 0.316490, mean_q: 3.874227
 25575/100000: episode: 330, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 196.700, mean reward: 1.967 [1.540, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.734, 10.280], loss: 0.176545, mae: 0.324697, mean_q: 3.898759
 25675/100000: episode: 331, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.633, mean reward: 1.896 [1.496, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.587, 10.290], loss: 0.096477, mae: 0.299196, mean_q: 3.890450
 25775/100000: episode: 332, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 176.596, mean reward: 1.766 [1.449, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.910, 10.177], loss: 0.092290, mae: 0.304195, mean_q: 3.884999
 25875/100000: episode: 333, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.842, mean reward: 1.958 [1.475, 6.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.919, 10.141], loss: 0.087915, mae: 0.300702, mean_q: 3.894710
 25975/100000: episode: 334, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 209.072, mean reward: 2.091 [1.539, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.942, 10.098], loss: 0.336617, mae: 0.359350, mean_q: 3.921266
 26075/100000: episode: 335, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 184.836, mean reward: 1.848 [1.459, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.813, 10.098], loss: 0.108240, mae: 0.310005, mean_q: 3.889656
 26175/100000: episode: 336, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 194.860, mean reward: 1.949 [1.472, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.605, 10.247], loss: 0.097813, mae: 0.312835, mean_q: 3.883410
 26275/100000: episode: 337, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.663, mean reward: 1.817 [1.479, 2.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.164, 10.202], loss: 0.113024, mae: 0.323810, mean_q: 3.893987
 26375/100000: episode: 338, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 206.216, mean reward: 2.062 [1.471, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.841, 10.098], loss: 0.236710, mae: 0.339575, mean_q: 3.910535
 26475/100000: episode: 339, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.026, mean reward: 1.900 [1.441, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.818, 10.481], loss: 0.113189, mae: 0.314164, mean_q: 3.881594
 26575/100000: episode: 340, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 189.984, mean reward: 1.900 [1.446, 4.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.063, 10.098], loss: 0.102598, mae: 0.307695, mean_q: 3.882061
 26675/100000: episode: 341, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 200.485, mean reward: 2.005 [1.438, 9.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.859, 10.488], loss: 0.097981, mae: 0.305873, mean_q: 3.857628
 26775/100000: episode: 342, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 191.038, mean reward: 1.910 [1.496, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.849, 10.179], loss: 0.103739, mae: 0.310892, mean_q: 3.902337
 26875/100000: episode: 343, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.251, mean reward: 1.913 [1.431, 4.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.680, 10.164], loss: 0.082869, mae: 0.288223, mean_q: 3.869618
 26975/100000: episode: 344, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 204.411, mean reward: 2.044 [1.446, 4.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.849, 10.171], loss: 0.103972, mae: 0.306208, mean_q: 3.878088
 27075/100000: episode: 345, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.434, mean reward: 1.834 [1.454, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.339, 10.206], loss: 0.112164, mae: 0.313709, mean_q: 3.865904
 27175/100000: episode: 346, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 205.405, mean reward: 2.054 [1.523, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.766, 10.239], loss: 0.170174, mae: 0.318628, mean_q: 3.885498
 27275/100000: episode: 347, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 209.013, mean reward: 2.090 [1.492, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.996, 10.246], loss: 0.096206, mae: 0.296362, mean_q: 3.861418
 27375/100000: episode: 348, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 187.666, mean reward: 1.877 [1.441, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.715, 10.100], loss: 0.084212, mae: 0.297515, mean_q: 3.859264
 27475/100000: episode: 349, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.712, mean reward: 1.817 [1.452, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.329, 10.249], loss: 0.102090, mae: 0.300368, mean_q: 3.868982
 27575/100000: episode: 350, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 205.823, mean reward: 2.058 [1.494, 3.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.746, 10.221], loss: 0.086188, mae: 0.293770, mean_q: 3.861965
 27675/100000: episode: 351, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 213.218, mean reward: 2.132 [1.563, 4.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.707, 10.257], loss: 0.092757, mae: 0.303545, mean_q: 3.860976
 27775/100000: episode: 352, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 179.598, mean reward: 1.796 [1.474, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.964, 10.098], loss: 0.103992, mae: 0.304248, mean_q: 3.864450
 27875/100000: episode: 353, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 185.511, mean reward: 1.855 [1.453, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.139, 10.098], loss: 0.105799, mae: 0.312025, mean_q: 3.872468
 27975/100000: episode: 354, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 185.714, mean reward: 1.857 [1.443, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.262, 10.152], loss: 0.113966, mae: 0.313180, mean_q: 3.891387
 28075/100000: episode: 355, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 207.579, mean reward: 2.076 [1.498, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.714, 10.098], loss: 0.098436, mae: 0.309417, mean_q: 3.899781
 28175/100000: episode: 356, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 196.294, mean reward: 1.963 [1.470, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.552, 10.098], loss: 0.100936, mae: 0.302471, mean_q: 3.877091
 28275/100000: episode: 357, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 198.516, mean reward: 1.985 [1.493, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.775, 10.293], loss: 0.107636, mae: 0.308269, mean_q: 3.883445
 28375/100000: episode: 358, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 199.802, mean reward: 1.998 [1.500, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.906, 10.098], loss: 0.097088, mae: 0.295793, mean_q: 3.867676
 28475/100000: episode: 359, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 236.199, mean reward: 2.362 [1.497, 29.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.200, 10.098], loss: 0.105876, mae: 0.302904, mean_q: 3.865901
 28575/100000: episode: 360, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 194.461, mean reward: 1.945 [1.487, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.968, 10.098], loss: 0.346308, mae: 0.328498, mean_q: 3.881210
 28675/100000: episode: 361, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 177.295, mean reward: 1.773 [1.442, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.458, 10.098], loss: 0.206277, mae: 0.310264, mean_q: 3.884640
 28775/100000: episode: 362, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 221.393, mean reward: 2.214 [1.464, 7.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.238, 10.098], loss: 0.330035, mae: 0.332469, mean_q: 3.878731
 28875/100000: episode: 363, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 195.780, mean reward: 1.958 [1.445, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.576, 10.254], loss: 0.231107, mae: 0.330821, mean_q: 3.890667
 28975/100000: episode: 364, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 198.981, mean reward: 1.990 [1.490, 2.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.936, 10.098], loss: 0.103106, mae: 0.302571, mean_q: 3.885670
 29075/100000: episode: 365, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 182.145, mean reward: 1.821 [1.456, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.864, 10.098], loss: 0.223668, mae: 0.319958, mean_q: 3.892908
 29175/100000: episode: 366, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 201.017, mean reward: 2.010 [1.441, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.608, 10.098], loss: 0.092057, mae: 0.301244, mean_q: 3.863599
[Info] 1-TH LEVEL FOUND: 5.196843147277832, Considering 10/90 traces
 29275/100000: episode: 367, duration: 4.696s, episode steps: 100, steps per second: 21, episode reward: 187.676, mean reward: 1.877 [1.497, 2.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.500, 10.174], loss: 0.220909, mae: 0.322632, mean_q: 3.863192
 29289/100000: episode: 368, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 30.476, mean reward: 2.177 [1.858, 2.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.197, 10.100], loss: 0.092188, mae: 0.317874, mean_q: 3.943610
 29303/100000: episode: 369, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 27.154, mean reward: 1.940 [1.515, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.641, 10.100], loss: 0.128629, mae: 0.332705, mean_q: 3.895499
 29386/100000: episode: 370, duration: 0.420s, episode steps: 83, steps per second: 198, episode reward: 158.097, mean reward: 1.905 [1.482, 2.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.620 [-0.833, 10.100], loss: 0.119528, mae: 0.320414, mean_q: 3.870502
 29409/100000: episode: 371, duration: 0.114s, episode steps: 23, steps per second: 201, episode reward: 55.297, mean reward: 2.404 [1.814, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.258, 10.100], loss: 0.110243, mae: 0.306476, mean_q: 3.849836
 29424/100000: episode: 372, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 43.329, mean reward: 2.889 [2.350, 4.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.938, 10.100], loss: 0.099993, mae: 0.315354, mean_q: 3.894986
 29439/100000: episode: 373, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 40.654, mean reward: 2.710 [2.192, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.253, 10.100], loss: 0.093959, mae: 0.300177, mean_q: 3.855956
 29462/100000: episode: 374, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 48.430, mean reward: 2.106 [1.725, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.830, 10.100], loss: 0.609126, mae: 0.332237, mean_q: 3.911577
 29487/100000: episode: 375, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 63.098, mean reward: 2.524 [1.966, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.174, 10.100], loss: 0.556234, mae: 0.323819, mean_q: 3.910827
 29502/100000: episode: 376, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 47.352, mean reward: 3.157 [2.543, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.395, 10.100], loss: 0.115070, mae: 0.318665, mean_q: 3.916281
 29517/100000: episode: 377, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 45.323, mean reward: 3.022 [2.358, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.362, 10.100], loss: 0.120710, mae: 0.316079, mean_q: 3.912781
 29597/100000: episode: 378, duration: 0.438s, episode steps: 80, steps per second: 183, episode reward: 151.910, mean reward: 1.899 [1.469, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.645 [-0.184, 10.100], loss: 0.251815, mae: 0.338304, mean_q: 3.932314
 29611/100000: episode: 379, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 34.032, mean reward: 2.431 [1.830, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.417, 10.100], loss: 0.131902, mae: 0.333828, mean_q: 3.933674
 29691/100000: episode: 380, duration: 0.418s, episode steps: 80, steps per second: 191, episode reward: 165.428, mean reward: 2.068 [1.478, 4.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.660 [-0.553, 10.290], loss: 0.271734, mae: 0.334271, mean_q: 3.927214
 29705/100000: episode: 381, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 33.614, mean reward: 2.401 [2.061, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.317, 10.100], loss: 0.102066, mae: 0.314535, mean_q: 3.902337
 29735/100000: episode: 382, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 102.081, mean reward: 3.403 [2.280, 4.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.399, 10.100], loss: 0.912225, mae: 0.426399, mean_q: 3.984516
 29749/100000: episode: 383, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 40.042, mean reward: 2.860 [2.150, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.465, 10.100], loss: 0.097155, mae: 0.322113, mean_q: 3.968530
 29772/100000: episode: 384, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 70.553, mean reward: 3.068 [1.872, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.659, 10.100], loss: 0.120496, mae: 0.335663, mean_q: 3.944587
 29797/100000: episode: 385, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 60.003, mean reward: 2.400 [1.554, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.418, 10.100], loss: 0.103047, mae: 0.317502, mean_q: 3.905641
 29877/100000: episode: 386, duration: 0.413s, episode steps: 80, steps per second: 193, episode reward: 155.083, mean reward: 1.939 [1.512, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.640 [-0.807, 10.100], loss: 0.099793, mae: 0.311500, mean_q: 3.920173
 29892/100000: episode: 387, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 37.249, mean reward: 2.483 [2.065, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.170, 10.100], loss: 0.145450, mae: 0.353126, mean_q: 3.994056
 29906/100000: episode: 388, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 48.294, mean reward: 3.450 [2.525, 4.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.281, 10.100], loss: 0.078842, mae: 0.296078, mean_q: 3.930797
 29989/100000: episode: 389, duration: 0.434s, episode steps: 83, steps per second: 191, episode reward: 163.030, mean reward: 1.964 [1.488, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.614 [-0.925, 10.100], loss: 0.099319, mae: 0.309142, mean_q: 3.923259
 30012/100000: episode: 390, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 61.546, mean reward: 2.676 [1.929, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.311, 10.100], loss: 0.625476, mae: 0.373797, mean_q: 3.938013
 30095/100000: episode: 391, duration: 0.428s, episode steps: 83, steps per second: 194, episode reward: 160.305, mean reward: 1.931 [1.484, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.622 [-0.813, 10.100], loss: 0.093646, mae: 0.306311, mean_q: 3.927395
 30109/100000: episode: 392, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 38.250, mean reward: 2.732 [1.802, 5.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.791, 10.100], loss: 0.109871, mae: 0.317803, mean_q: 3.941234
 30134/100000: episode: 393, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 78.914, mean reward: 3.157 [2.200, 5.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.422, 10.100], loss: 0.199587, mae: 0.343157, mean_q: 3.987082
 30149/100000: episode: 394, duration: 0.092s, episode steps: 15, steps per second: 164, episode reward: 55.436, mean reward: 3.696 [2.388, 5.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.486, 10.100], loss: 0.118786, mae: 0.327079, mean_q: 3.941557
 30229/100000: episode: 395, duration: 0.408s, episode steps: 80, steps per second: 196, episode reward: 152.538, mean reward: 1.907 [1.514, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.646 [-1.312, 10.196], loss: 0.102542, mae: 0.305827, mean_q: 3.935227
 30312/100000: episode: 396, duration: 0.413s, episode steps: 83, steps per second: 201, episode reward: 165.742, mean reward: 1.997 [1.448, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.613 [-0.767, 10.109], loss: 0.543082, mae: 0.362079, mean_q: 3.966726
 30326/100000: episode: 397, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 32.282, mean reward: 2.306 [1.854, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.067, 10.100], loss: 0.133682, mae: 0.371760, mean_q: 3.951548
 30340/100000: episode: 398, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 43.026, mean reward: 3.073 [1.847, 6.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.628, 10.100], loss: 0.098607, mae: 0.319448, mean_q: 4.031295
 30365/100000: episode: 399, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 62.245, mean reward: 2.490 [1.512, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.096, 10.104], loss: 0.123324, mae: 0.310233, mean_q: 3.962406
 30388/100000: episode: 400, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 53.080, mean reward: 2.308 [1.920, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.704, 10.100], loss: 0.115141, mae: 0.322181, mean_q: 3.948763
 30402/100000: episode: 401, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 34.594, mean reward: 2.471 [2.161, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.308, 10.100], loss: 0.154736, mae: 0.384687, mean_q: 4.066623
 30425/100000: episode: 402, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 56.953, mean reward: 2.476 [1.812, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.511, 10.100], loss: 0.096644, mae: 0.291412, mean_q: 3.923510
 30446/100000: episode: 403, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 44.506, mean reward: 2.119 [1.868, 2.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.186, 10.100], loss: 0.127545, mae: 0.332126, mean_q: 3.981444
 30460/100000: episode: 404, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 35.886, mean reward: 2.563 [2.004, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.355, 10.100], loss: 0.163117, mae: 0.363608, mean_q: 4.021886
 30540/100000: episode: 405, duration: 0.410s, episode steps: 80, steps per second: 195, episode reward: 153.537, mean reward: 1.919 [1.460, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.645 [-0.905, 10.216], loss: 0.111294, mae: 0.322186, mean_q: 3.979949
 30554/100000: episode: 406, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 30.722, mean reward: 2.194 [1.836, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.306, 10.100], loss: 0.127161, mae: 0.342115, mean_q: 3.999222
 30584/100000: episode: 407, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 125.271, mean reward: 4.176 [2.332, 6.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.546, 10.100], loss: 0.535294, mae: 0.375601, mean_q: 4.008125
 30605/100000: episode: 408, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 43.175, mean reward: 2.056 [1.762, 2.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.092, 10.100], loss: 0.114089, mae: 0.329144, mean_q: 3.997672
 30630/100000: episode: 409, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 70.503, mean reward: 2.820 [1.848, 4.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.429, 10.100], loss: 0.597255, mae: 0.387663, mean_q: 4.037429
 30644/100000: episode: 410, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 37.524, mean reward: 2.680 [2.116, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.329, 10.100], loss: 0.111465, mae: 0.338476, mean_q: 4.011615
 30674/100000: episode: 411, duration: 0.176s, episode steps: 30, steps per second: 170, episode reward: 74.606, mean reward: 2.487 [2.121, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.231, 10.100], loss: 0.134859, mae: 0.333591, mean_q: 4.063402
 30699/100000: episode: 412, duration: 0.118s, episode steps: 25, steps per second: 212, episode reward: 64.349, mean reward: 2.574 [1.939, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.162, 10.100], loss: 0.611799, mae: 0.375232, mean_q: 4.052511
 30724/100000: episode: 413, duration: 0.148s, episode steps: 25, steps per second: 169, episode reward: 72.755, mean reward: 2.910 [2.072, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.138, 10.100], loss: 0.110704, mae: 0.335053, mean_q: 4.005708
 30739/100000: episode: 414, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 33.512, mean reward: 2.234 [1.586, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.368, 10.100], loss: 0.117517, mae: 0.332825, mean_q: 4.072324
 30753/100000: episode: 415, duration: 0.068s, episode steps: 14, steps per second: 204, episode reward: 36.430, mean reward: 2.602 [2.088, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.157, 10.100], loss: 0.116014, mae: 0.336740, mean_q: 4.101654
 30778/100000: episode: 416, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 80.274, mean reward: 3.211 [2.490, 4.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.332, 10.100], loss: 0.127067, mae: 0.331711, mean_q: 4.067191
 30792/100000: episode: 417, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 49.819, mean reward: 3.559 [2.185, 7.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.359, 10.100], loss: 0.097821, mae: 0.314713, mean_q: 3.944656
 30807/100000: episode: 418, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 53.053, mean reward: 3.537 [2.246, 4.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.405, 10.100], loss: 0.152021, mae: 0.371036, mean_q: 4.073287
 30822/100000: episode: 419, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 34.358, mean reward: 2.291 [1.690, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.086, 10.100], loss: 0.132203, mae: 0.346270, mean_q: 4.076726
 30837/100000: episode: 420, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 36.540, mean reward: 2.436 [2.045, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.231, 10.100], loss: 0.124823, mae: 0.333906, mean_q: 4.045692
 30851/100000: episode: 421, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 30.483, mean reward: 2.177 [1.839, 2.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.087, 10.100], loss: 0.120584, mae: 0.348719, mean_q: 4.035840
 30874/100000: episode: 422, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 65.262, mean reward: 2.837 [2.054, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.455, 10.100], loss: 0.169041, mae: 0.378207, mean_q: 4.095547
 30954/100000: episode: 423, duration: 0.412s, episode steps: 80, steps per second: 194, episode reward: 154.681, mean reward: 1.934 [1.440, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.646 [-0.422, 10.100], loss: 0.146791, mae: 0.361591, mean_q: 4.119645
 30969/100000: episode: 424, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 37.174, mean reward: 2.478 [1.836, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.647, 10.100], loss: 0.149772, mae: 0.378046, mean_q: 4.081429
 31052/100000: episode: 425, duration: 0.415s, episode steps: 83, steps per second: 200, episode reward: 151.883, mean reward: 1.830 [1.478, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.612 [-1.581, 10.100], loss: 0.419481, mae: 0.370998, mean_q: 4.109151
 31132/100000: episode: 426, duration: 0.399s, episode steps: 80, steps per second: 201, episode reward: 158.729, mean reward: 1.984 [1.439, 4.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.644 [-0.764, 10.112], loss: 0.162426, mae: 0.361926, mean_q: 4.094547
 31153/100000: episode: 427, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 48.473, mean reward: 2.308 [1.806, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.324, 10.100], loss: 0.121893, mae: 0.343108, mean_q: 4.077349
 31168/100000: episode: 428, duration: 0.093s, episode steps: 15, steps per second: 162, episode reward: 46.483, mean reward: 3.099 [2.444, 4.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.336, 10.100], loss: 0.106101, mae: 0.324112, mean_q: 4.130098
 31248/100000: episode: 429, duration: 0.415s, episode steps: 80, steps per second: 193, episode reward: 167.653, mean reward: 2.096 [1.500, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.653 [-0.520, 10.487], loss: 0.273791, mae: 0.350266, mean_q: 4.116920
 31278/100000: episode: 430, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 105.006, mean reward: 3.500 [2.564, 6.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.179, 10.100], loss: 0.503329, mae: 0.366318, mean_q: 4.117157
 31293/100000: episode: 431, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 42.132, mean reward: 2.809 [2.236, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.354, 10.100], loss: 0.115440, mae: 0.332942, mean_q: 4.116045
 31308/100000: episode: 432, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 36.921, mean reward: 2.461 [1.733, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.178, 10.100], loss: 0.078310, mae: 0.291103, mean_q: 4.080374
 31322/100000: episode: 433, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 35.059, mean reward: 2.504 [2.011, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.247, 10.100], loss: 0.939182, mae: 0.368327, mean_q: 4.132535
 31345/100000: episode: 434, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 64.653, mean reward: 2.811 [2.199, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.500, 10.100], loss: 0.137109, mae: 0.356434, mean_q: 4.145027
 31368/100000: episode: 435, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 48.785, mean reward: 2.121 [1.645, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.229, 10.100], loss: 0.146538, mae: 0.349171, mean_q: 4.074084
 31451/100000: episode: 436, duration: 0.409s, episode steps: 83, steps per second: 203, episode reward: 162.689, mean reward: 1.960 [1.508, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.614 [-0.799, 10.100], loss: 0.144498, mae: 0.372112, mean_q: 4.146596
 31466/100000: episode: 437, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 44.203, mean reward: 2.947 [2.159, 5.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.670, 10.100], loss: 0.118145, mae: 0.324227, mean_q: 4.168791
 31481/100000: episode: 438, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 38.619, mean reward: 2.575 [1.938, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.543, 10.100], loss: 0.134326, mae: 0.343964, mean_q: 4.101020
 31496/100000: episode: 439, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 40.992, mean reward: 2.733 [2.279, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.225, 10.100], loss: 0.938686, mae: 0.447893, mean_q: 4.229342
 31510/100000: episode: 440, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 36.195, mean reward: 2.585 [2.210, 4.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.196, 10.100], loss: 0.137431, mae: 0.373873, mean_q: 4.143306
 31590/100000: episode: 441, duration: 0.427s, episode steps: 80, steps per second: 188, episode reward: 154.890, mean reward: 1.936 [1.457, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.648 [-1.000, 10.208], loss: 0.133889, mae: 0.350596, mean_q: 4.130972
 31613/100000: episode: 442, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 54.804, mean reward: 2.383 [1.706, 4.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.624, 10.100], loss: 0.131170, mae: 0.347171, mean_q: 4.144225
 31628/100000: episode: 443, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 38.883, mean reward: 2.592 [2.054, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.268, 10.100], loss: 0.136474, mae: 0.351424, mean_q: 4.115259
 31649/100000: episode: 444, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 54.865, mean reward: 2.613 [1.868, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.469, 10.100], loss: 0.135582, mae: 0.368227, mean_q: 4.212471
 31670/100000: episode: 445, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 48.083, mean reward: 2.290 [1.767, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.644, 10.100], loss: 0.127842, mae: 0.345457, mean_q: 4.155678
 31685/100000: episode: 446, duration: 0.072s, episode steps: 15, steps per second: 208, episode reward: 37.935, mean reward: 2.529 [2.199, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.172, 10.100], loss: 0.138341, mae: 0.351425, mean_q: 4.123893
 31768/100000: episode: 447, duration: 0.405s, episode steps: 83, steps per second: 205, episode reward: 149.312, mean reward: 1.799 [1.435, 4.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.617 [-1.408, 10.251], loss: 0.277953, mae: 0.364746, mean_q: 4.198471
 31782/100000: episode: 448, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 40.269, mean reward: 2.876 [2.228, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.346, 10.100], loss: 0.165358, mae: 0.386206, mean_q: 4.091002
 31865/100000: episode: 449, duration: 0.417s, episode steps: 83, steps per second: 199, episode reward: 183.357, mean reward: 2.209 [1.529, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.613 [-0.603, 10.100], loss: 0.138692, mae: 0.363090, mean_q: 4.180307
 31890/100000: episode: 450, duration: 0.125s, episode steps: 25, steps per second: 199, episode reward: 66.061, mean reward: 2.642 [1.772, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.801, 10.100], loss: 0.113064, mae: 0.335180, mean_q: 4.195570
 31973/100000: episode: 451, duration: 0.438s, episode steps: 83, steps per second: 189, episode reward: 158.339, mean reward: 1.908 [1.460, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.624 [-1.074, 10.267], loss: 0.138126, mae: 0.360128, mean_q: 4.201781
 31996/100000: episode: 452, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 71.106, mean reward: 3.092 [1.891, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.437, 10.100], loss: 0.164337, mae: 0.370487, mean_q: 4.214202
 32011/100000: episode: 453, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 36.612, mean reward: 2.441 [1.853, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.341, 10.100], loss: 0.903759, mae: 0.408067, mean_q: 4.280974
 32034/100000: episode: 454, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 62.236, mean reward: 2.706 [1.967, 7.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.798, 10.100], loss: 0.137497, mae: 0.370888, mean_q: 4.241591
 32048/100000: episode: 455, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 38.716, mean reward: 2.765 [2.001, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.565, 10.100], loss: 0.123457, mae: 0.349837, mean_q: 4.192454
 32078/100000: episode: 456, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 94.432, mean reward: 3.148 [1.963, 6.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.149, 10.100], loss: 0.119559, mae: 0.347193, mean_q: 4.255189
[Info] 2-TH LEVEL FOUND: 6.596182823181152, Considering 10/90 traces
 32108/100000: episode: 457, duration: 4.260s, episode steps: 30, steps per second: 7, episode reward: 81.471, mean reward: 2.716 [2.278, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.392, 10.100], loss: 0.150092, mae: 0.364506, mean_q: 4.203202
 32118/100000: episode: 458, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 29.330, mean reward: 2.933 [2.169, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.534, 10.100], loss: 0.139762, mae: 0.369882, mean_q: 4.293672
 32138/100000: episode: 459, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 65.102, mean reward: 3.255 [2.450, 4.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.249, 10.100], loss: 0.172880, mae: 0.389331, mean_q: 4.262225
 32153/100000: episode: 460, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 41.620, mean reward: 2.775 [2.075, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.311, 10.100], loss: 0.159613, mae: 0.384504, mean_q: 4.236392
 32163/100000: episode: 461, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 29.292, mean reward: 2.929 [2.574, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.285, 10.100], loss: 0.171881, mae: 0.392116, mean_q: 4.324862
 32173/100000: episode: 462, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 27.922, mean reward: 2.792 [2.235, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.564, 10.100], loss: 0.120323, mae: 0.327018, mean_q: 4.173110
 32183/100000: episode: 463, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 28.453, mean reward: 2.845 [2.352, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.350, 10.100], loss: 0.124796, mae: 0.351801, mean_q: 4.257932
 32193/100000: episode: 464, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 33.624, mean reward: 3.362 [2.342, 5.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.573, 10.100], loss: 0.124881, mae: 0.329328, mean_q: 4.154927
 32210/100000: episode: 465, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 60.194, mean reward: 3.541 [2.230, 6.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.657, 10.100], loss: 0.115997, mae: 0.347666, mean_q: 4.213927
 32227/100000: episode: 466, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 52.146, mean reward: 3.067 [2.173, 5.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.778, 10.100], loss: 0.142527, mae: 0.380271, mean_q: 4.259560
 32241/100000: episode: 467, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 87.777, mean reward: 6.270 [2.738, 14.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.553, 10.100], loss: 0.176931, mae: 0.390026, mean_q: 4.237958
 32257/100000: episode: 468, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 51.926, mean reward: 3.245 [2.610, 3.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.436, 10.100], loss: 0.124930, mae: 0.348622, mean_q: 4.224996
 32271/100000: episode: 469, duration: 0.068s, episode steps: 14, steps per second: 206, episode reward: 42.637, mean reward: 3.045 [2.279, 4.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.198, 10.100], loss: 0.328872, mae: 0.399900, mean_q: 4.283780
 32281/100000: episode: 470, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 27.071, mean reward: 2.707 [2.478, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.341, 10.100], loss: 0.212110, mae: 0.460862, mean_q: 4.333025
 32301/100000: episode: 471, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 53.498, mean reward: 2.675 [1.924, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.651, 10.100], loss: 0.175367, mae: 0.423101, mean_q: 4.252395
 32310/100000: episode: 472, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 29.087, mean reward: 3.232 [2.433, 5.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.355, 10.100], loss: 0.126899, mae: 0.363681, mean_q: 4.371458
 32320/100000: episode: 473, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 26.688, mean reward: 2.669 [2.315, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.407, 10.100], loss: 0.258540, mae: 0.451911, mean_q: 4.386650
 32330/100000: episode: 474, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 27.097, mean reward: 2.710 [2.477, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.363, 10.100], loss: 0.166489, mae: 0.388249, mean_q: 4.307575
 32350/100000: episode: 475, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 99.752, mean reward: 4.988 [2.651, 18.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.221, 10.100], loss: 0.194646, mae: 0.413312, mean_q: 4.279111
 32370/100000: episode: 476, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 91.445, mean reward: 4.572 [2.310, 8.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.669, 10.100], loss: 0.267796, mae: 0.410335, mean_q: 4.332954
 32379/100000: episode: 477, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 23.678, mean reward: 2.631 [2.305, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-1.615, 10.100], loss: 0.178033, mae: 0.413630, mean_q: 4.413774
 32388/100000: episode: 478, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 27.568, mean reward: 3.063 [2.362, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.442, 10.100], loss: 0.156707, mae: 0.371917, mean_q: 4.345674
 32404/100000: episode: 479, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 61.112, mean reward: 3.819 [2.343, 5.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.920, 10.100], loss: 0.184317, mae: 0.394782, mean_q: 4.389877
 32428/100000: episode: 480, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 109.162, mean reward: 4.548 [2.552, 9.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.194, 10.100], loss: 0.301113, mae: 0.391894, mean_q: 4.348260
 32443/100000: episode: 481, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 74.130, mean reward: 4.942 [2.455, 9.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.420, 10.100], loss: 0.150469, mae: 0.378009, mean_q: 4.422636
 32467/100000: episode: 482, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 92.743, mean reward: 3.864 [2.411, 6.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.295, 10.100], loss: 0.177294, mae: 0.404614, mean_q: 4.353306
 32481/100000: episode: 483, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 55.250, mean reward: 3.946 [2.877, 5.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.015, 10.100], loss: 0.213033, mae: 0.410245, mean_q: 4.369886
 32491/100000: episode: 484, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 29.475, mean reward: 2.947 [2.311, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.452, 10.100], loss: 0.232862, mae: 0.428720, mean_q: 4.513030
 32515/100000: episode: 485, duration: 0.117s, episode steps: 24, steps per second: 205, episode reward: 137.364, mean reward: 5.723 [3.087, 14.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.836, 10.100], loss: 0.188883, mae: 0.389287, mean_q: 4.400878
 32525/100000: episode: 486, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 26.491, mean reward: 2.649 [2.259, 2.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.335, 10.100], loss: 0.299625, mae: 0.455392, mean_q: 4.480689
 32535/100000: episode: 487, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 27.657, mean reward: 2.766 [2.282, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.586, 10.100], loss: 0.216465, mae: 0.437354, mean_q: 4.525613
 32559/100000: episode: 488, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 109.842, mean reward: 4.577 [2.741, 8.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.509, 10.100], loss: 0.326846, mae: 0.422736, mean_q: 4.481847
 32569/100000: episode: 489, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 22.397, mean reward: 2.240 [2.046, 2.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.825, 10.100], loss: 0.160139, mae: 0.384451, mean_q: 4.520842
 32579/100000: episode: 490, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 27.689, mean reward: 2.769 [2.147, 4.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.316, 10.100], loss: 0.162613, mae: 0.388999, mean_q: 4.390937
 32593/100000: episode: 491, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 53.402, mean reward: 3.814 [2.729, 4.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.494, 10.100], loss: 0.259695, mae: 0.459559, mean_q: 4.629253
 32603/100000: episode: 492, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 33.953, mean reward: 3.395 [2.844, 4.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.460, 10.100], loss: 0.235916, mae: 0.448699, mean_q: 4.646128
 32613/100000: episode: 493, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 34.584, mean reward: 3.458 [2.759, 4.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.378, 10.100], loss: 0.154791, mae: 0.388309, mean_q: 4.331000
 32628/100000: episode: 494, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 49.103, mean reward: 3.274 [2.625, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.200, 10.100], loss: 0.219496, mae: 0.415622, mean_q: 4.440079
 32638/100000: episode: 495, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 32.381, mean reward: 3.238 [2.614, 5.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.154, 10.100], loss: 0.350790, mae: 0.526987, mean_q: 4.595651
 32658/100000: episode: 496, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 104.075, mean reward: 5.204 [3.458, 11.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.321, 10.100], loss: 0.271022, mae: 0.475289, mean_q: 4.573432
 32674/100000: episode: 497, duration: 0.079s, episode steps: 16, steps per second: 201, episode reward: 73.110, mean reward: 4.569 [3.013, 7.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.386, 10.100], loss: 0.272728, mae: 0.469819, mean_q: 4.627985
 32690/100000: episode: 498, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 45.484, mean reward: 2.843 [1.828, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.238, 10.100], loss: 0.260331, mae: 0.441067, mean_q: 4.568758
 32707/100000: episode: 499, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 74.411, mean reward: 4.377 [2.118, 9.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.278, 10.100], loss: 0.375844, mae: 0.503634, mean_q: 4.682972
 32716/100000: episode: 500, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 24.582, mean reward: 2.731 [2.537, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.522, 10.100], loss: 0.327762, mae: 0.479649, mean_q: 4.504408
 32740/100000: episode: 501, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 83.483, mean reward: 3.478 [2.566, 6.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.575, 10.100], loss: 0.483528, mae: 0.539709, mean_q: 4.691860
 32750/100000: episode: 502, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 51.267, mean reward: 5.127 [2.885, 7.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.363, 10.100], loss: 0.252602, mae: 0.434766, mean_q: 4.539350
 32759/100000: episode: 503, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 25.514, mean reward: 2.835 [2.577, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.484, 10.100], loss: 0.291661, mae: 0.486128, mean_q: 4.756831
 32774/100000: episode: 504, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 58.554, mean reward: 3.904 [2.609, 5.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.408, 10.100], loss: 0.284452, mae: 0.464781, mean_q: 4.670509
 32791/100000: episode: 505, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 56.908, mean reward: 3.348 [2.825, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.556, 10.100], loss: 0.251477, mae: 0.458280, mean_q: 4.538837
 32815/100000: episode: 506, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 90.120, mean reward: 3.755 [2.442, 8.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.569, 10.100], loss: 0.753171, mae: 0.530504, mean_q: 4.709128
 32825/100000: episode: 507, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 29.247, mean reward: 2.925 [2.262, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.407, 10.100], loss: 0.250304, mae: 0.457994, mean_q: 4.565450
 32835/100000: episode: 508, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 32.849, mean reward: 3.285 [2.149, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.383, 10.100], loss: 0.281758, mae: 0.491759, mean_q: 4.634974
 32844/100000: episode: 509, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 21.058, mean reward: 2.340 [2.073, 2.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.417, 10.100], loss: 0.171604, mae: 0.392180, mean_q: 4.629412
 32854/100000: episode: 510, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 25.000, mean reward: 2.500 [2.294, 2.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.415, 10.100], loss: 0.565333, mae: 0.503267, mean_q: 4.752135
 32864/100000: episode: 511, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 31.099, mean reward: 3.110 [2.288, 5.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.450, 10.100], loss: 0.333172, mae: 0.473839, mean_q: 4.729077
 32873/100000: episode: 512, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 27.792, mean reward: 3.088 [2.619, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.458, 10.100], loss: 0.289142, mae: 0.473304, mean_q: 4.786148
 32887/100000: episode: 513, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 58.779, mean reward: 4.199 [2.834, 7.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.433, 10.100], loss: 0.202482, mae: 0.433823, mean_q: 4.755840
 32902/100000: episode: 514, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 55.356, mean reward: 3.690 [2.738, 5.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.423, 10.100], loss: 0.420409, mae: 0.522514, mean_q: 4.699359
 32912/100000: episode: 515, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 33.058, mean reward: 3.306 [2.474, 5.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.292, 10.100], loss: 1.932067, mae: 0.720614, mean_q: 5.124419
 32921/100000: episode: 516, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 26.367, mean reward: 2.930 [2.326, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.494, 10.100], loss: 0.424268, mae: 0.562336, mean_q: 4.711966
 32931/100000: episode: 517, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 27.530, mean reward: 2.753 [2.149, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.602, 10.100], loss: 0.452807, mae: 0.587398, mean_q: 4.710423
 32945/100000: episode: 518, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 55.264, mean reward: 3.947 [3.180, 5.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.416, 10.100], loss: 1.124039, mae: 0.564786, mean_q: 4.823234
 32961/100000: episode: 519, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 76.001, mean reward: 4.750 [2.421, 11.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.372, 10.100], loss: 0.336687, mae: 0.473766, mean_q: 4.894562
 32978/100000: episode: 520, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 77.988, mean reward: 4.588 [3.084, 7.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.381, 10.100], loss: 0.303560, mae: 0.468163, mean_q: 4.722922
 32998/100000: episode: 521, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 55.850, mean reward: 2.792 [1.940, 4.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.157, 10.100], loss: 0.198832, mae: 0.409433, mean_q: 4.667264
 33015/100000: episode: 522, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 73.246, mean reward: 4.309 [3.554, 5.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.222, 10.100], loss: 0.495955, mae: 0.495018, mean_q: 4.755873
 33025/100000: episode: 523, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 32.521, mean reward: 3.252 [2.485, 5.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.314, 10.100], loss: 0.247082, mae: 0.478289, mean_q: 4.737663
 33040/100000: episode: 524, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 47.361, mean reward: 3.157 [2.497, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.319, 10.100], loss: 0.525572, mae: 0.527660, mean_q: 4.845748
 33050/100000: episode: 525, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 26.188, mean reward: 2.619 [2.454, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.782, 10.100], loss: 1.862990, mae: 0.636806, mean_q: 4.859805
 33060/100000: episode: 526, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 29.460, mean reward: 2.946 [2.190, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.342, 10.100], loss: 0.266787, mae: 0.504054, mean_q: 4.799807
 33070/100000: episode: 527, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 26.176, mean reward: 2.618 [2.398, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.384, 10.100], loss: 0.380463, mae: 0.546573, mean_q: 4.884164
 33084/100000: episode: 528, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 123.396, mean reward: 8.814 [2.759, 64.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.467, 10.100], loss: 0.230152, mae: 0.461801, mean_q: 4.819996
 33104/100000: episode: 529, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 73.208, mean reward: 3.660 [2.798, 4.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.264, 10.100], loss: 0.324115, mae: 0.501499, mean_q: 4.797879
 33124/100000: episode: 530, duration: 0.100s, episode steps: 20, steps per second: 201, episode reward: 62.841, mean reward: 3.142 [1.977, 5.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.522, 10.100], loss: 3.618411, mae: 0.773492, mean_q: 4.770915
 33134/100000: episode: 531, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 25.205, mean reward: 2.520 [2.209, 2.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.339, 10.100], loss: 0.466281, mae: 0.592907, mean_q: 4.925282
 33143/100000: episode: 532, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 27.142, mean reward: 3.016 [2.442, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.310, 10.100], loss: 0.346046, mae: 0.561060, mean_q: 4.882601
 33158/100000: episode: 533, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 64.961, mean reward: 4.331 [2.611, 13.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.123, 10.100], loss: 0.411275, mae: 0.521586, mean_q: 5.059537
 33168/100000: episode: 534, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 31.762, mean reward: 3.176 [2.304, 6.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.310, 10.100], loss: 0.195158, mae: 0.432166, mean_q: 4.822119
 33177/100000: episode: 535, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 24.070, mean reward: 2.674 [2.506, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.279, 10.100], loss: 0.255879, mae: 0.437262, mean_q: 4.743958
 33201/100000: episode: 536, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 94.569, mean reward: 3.940 [2.896, 5.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.509, 10.100], loss: 0.309480, mae: 0.498446, mean_q: 4.856236
 33221/100000: episode: 537, duration: 0.096s, episode steps: 20, steps per second: 209, episode reward: 59.413, mean reward: 2.971 [1.821, 4.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.126, 10.100], loss: 3.331688, mae: 0.684273, mean_q: 5.042511
 33230/100000: episode: 538, duration: 0.046s, episode steps: 9, steps per second: 198, episode reward: 25.976, mean reward: 2.886 [2.374, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.391, 10.100], loss: 2.203796, mae: 0.741550, mean_q: 4.617524
 33240/100000: episode: 539, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 26.281, mean reward: 2.628 [2.396, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.354, 10.100], loss: 0.276816, mae: 0.548527, mean_q: 4.900352
 33250/100000: episode: 540, duration: 0.073s, episode steps: 10, steps per second: 138, episode reward: 26.551, mean reward: 2.655 [2.152, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.373, 10.100], loss: 0.380561, mae: 0.594589, mean_q: 5.073451
 33265/100000: episode: 541, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 39.804, mean reward: 2.654 [1.709, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.296, 10.100], loss: 0.287148, mae: 0.496259, mean_q: 4.819580
 33281/100000: episode: 542, duration: 0.077s, episode steps: 16, steps per second: 207, episode reward: 46.591, mean reward: 2.912 [2.490, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.262, 10.100], loss: 0.312176, mae: 0.496620, mean_q: 4.920461
 33298/100000: episode: 543, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 56.652, mean reward: 3.332 [2.714, 4.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.161, 10.100], loss: 0.545027, mae: 0.568452, mean_q: 4.968001
 33315/100000: episode: 544, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 84.741, mean reward: 4.985 [3.377, 7.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.403, 10.100], loss: 7.110384, mae: 0.841668, mean_q: 5.171445
 33332/100000: episode: 545, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 75.413, mean reward: 4.436 [2.564, 7.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.888, 10.100], loss: 0.337945, mae: 0.510077, mean_q: 4.942458
 33346/100000: episode: 546, duration: 0.097s, episode steps: 14, steps per second: 144, episode reward: 43.894, mean reward: 3.135 [2.619, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.228, 10.100], loss: 4.609381, mae: 0.750996, mean_q: 5.118754
[Info] 3-TH LEVEL FOUND: 8.844452857971191, Considering 10/90 traces
 33363/100000: episode: 547, duration: 4.239s, episode steps: 17, steps per second: 4, episode reward: 69.626, mean reward: 4.096 [2.607, 11.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.693, 10.100], loss: 0.594663, mae: 0.590056, mean_q: 4.790309
 33381/100000: episode: 548, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 90.150, mean reward: 5.008 [3.755, 7.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.409, 10.100], loss: 0.442866, mae: 0.575364, mean_q: 5.041129
 33388/100000: episode: 549, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 25.730, mean reward: 3.676 [2.950, 5.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.372, 10.100], loss: 0.261200, mae: 0.491946, mean_q: 5.000958
 33397/100000: episode: 550, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 28.542, mean reward: 3.171 [2.698, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.279, 10.100], loss: 0.413125, mae: 0.570280, mean_q: 5.219749
 33406/100000: episode: 551, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 78.949, mean reward: 8.772 [4.028, 15.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.439, 10.100], loss: 0.305524, mae: 0.495981, mean_q: 4.998751
 33415/100000: episode: 552, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 34.470, mean reward: 3.830 [3.191, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.416, 10.100], loss: 0.390841, mae: 0.586004, mean_q: 5.095691
 33427/100000: episode: 553, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 72.543, mean reward: 6.045 [4.838, 7.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.319, 10.100], loss: 0.415122, mae: 0.535591, mean_q: 5.075202
 33445/100000: episode: 554, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 89.265, mean reward: 4.959 [2.733, 8.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.347, 10.100], loss: 0.460920, mae: 0.542418, mean_q: 5.104996
 33463/100000: episode: 555, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 114.966, mean reward: 6.387 [4.241, 13.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.648, 10.100], loss: 3.653383, mae: 0.710128, mean_q: 5.238617
[Info] FALSIFICATION!
[Info] Levels: [5.196843, 6.596183, 8.844453, 9.814203]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.02]
[Info] Error Prob: 2.0000000000000005e-05

 33466/100000: episode: 556, duration: 4.321s, episode steps: 3, steps per second: 1, episode reward: 118.627, mean reward: 39.542 [8.492, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.142, 9.813], loss: 0.464476, mae: 0.547007, mean_q: 4.963911
 33566/100000: episode: 557, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 193.858, mean reward: 1.939 [1.466, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.538, 10.098], loss: 1.026875, mae: 0.612626, mean_q: 5.058133
 33666/100000: episode: 558, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 188.071, mean reward: 1.881 [1.451, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.411, 10.098], loss: 1.583805, mae: 0.645409, mean_q: 5.069995
 33766/100000: episode: 559, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.732, mean reward: 1.837 [1.445, 2.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.579, 10.098], loss: 0.495534, mae: 0.573016, mean_q: 5.060033
 33866/100000: episode: 560, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 185.666, mean reward: 1.857 [1.482, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.890, 10.098], loss: 0.506582, mae: 0.561602, mean_q: 5.042455
 33966/100000: episode: 561, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 183.882, mean reward: 1.839 [1.457, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.957, 10.219], loss: 1.057806, mae: 0.590985, mean_q: 5.084055
 34066/100000: episode: 562, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 186.755, mean reward: 1.868 [1.460, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.713, 10.156], loss: 0.470498, mae: 0.556079, mean_q: 5.029329
 34166/100000: episode: 563, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 185.939, mean reward: 1.859 [1.446, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.961, 10.098], loss: 2.908199, mae: 0.671525, mean_q: 5.104532
 34266/100000: episode: 564, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 208.116, mean reward: 2.081 [1.450, 17.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.593, 10.098], loss: 1.008856, mae: 0.582646, mean_q: 5.088974
 34366/100000: episode: 565, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 199.236, mean reward: 1.992 [1.490, 5.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.777, 10.098], loss: 1.797699, mae: 0.612577, mean_q: 5.068877
 34466/100000: episode: 566, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.414, mean reward: 1.914 [1.477, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.459, 10.098], loss: 1.627048, mae: 0.540346, mean_q: 4.993387
 34566/100000: episode: 567, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 192.291, mean reward: 1.923 [1.443, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.862, 10.098], loss: 2.327778, mae: 0.664059, mean_q: 5.135363
 34666/100000: episode: 568, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 188.309, mean reward: 1.883 [1.463, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.770, 10.333], loss: 4.212474, mae: 0.770406, mean_q: 5.137506
 34766/100000: episode: 569, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.251, mean reward: 1.893 [1.462, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.404, 10.256], loss: 1.055441, mae: 0.585929, mean_q: 5.087196
 34866/100000: episode: 570, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.333, mean reward: 1.853 [1.487, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.570, 10.098], loss: 3.054821, mae: 0.665276, mean_q: 5.088054
 34966/100000: episode: 571, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.585, mean reward: 1.896 [1.479, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.870, 10.229], loss: 1.095306, mae: 0.566954, mean_q: 4.976036
 35066/100000: episode: 572, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 188.957, mean reward: 1.890 [1.482, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.265, 10.099], loss: 1.648759, mae: 0.566565, mean_q: 5.007931
 35166/100000: episode: 573, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 186.481, mean reward: 1.865 [1.444, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.703, 10.098], loss: 0.477619, mae: 0.530141, mean_q: 4.960964
 35266/100000: episode: 574, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 178.370, mean reward: 1.784 [1.439, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.775, 10.127], loss: 1.671025, mae: 0.581248, mean_q: 4.969594
 35366/100000: episode: 575, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 196.429, mean reward: 1.964 [1.488, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.835, 10.098], loss: 2.329288, mae: 0.638988, mean_q: 4.998810
 35466/100000: episode: 576, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 221.297, mean reward: 2.213 [1.456, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.224, 10.098], loss: 0.978134, mae: 0.557952, mean_q: 4.915590
 35566/100000: episode: 577, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 177.709, mean reward: 1.777 [1.484, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.444, 10.100], loss: 3.545852, mae: 0.672815, mean_q: 4.941538
 35666/100000: episode: 578, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 221.745, mean reward: 2.217 [1.441, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.605, 10.578], loss: 0.415242, mae: 0.497330, mean_q: 4.849294
 35766/100000: episode: 579, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.117, mean reward: 1.951 [1.490, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.511, 10.098], loss: 3.070292, mae: 0.656082, mean_q: 4.952957
 35866/100000: episode: 580, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 198.052, mean reward: 1.981 [1.483, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.103, 10.098], loss: 0.430974, mae: 0.499135, mean_q: 4.796430
 35966/100000: episode: 581, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 187.434, mean reward: 1.874 [1.459, 2.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.743, 10.098], loss: 2.354278, mae: 0.605751, mean_q: 4.855912
 36066/100000: episode: 582, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 199.880, mean reward: 1.999 [1.441, 5.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.238, 10.098], loss: 1.015570, mae: 0.561297, mean_q: 4.894875
 36166/100000: episode: 583, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 188.334, mean reward: 1.883 [1.440, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.174, 10.098], loss: 3.516463, mae: 0.656252, mean_q: 4.841486
 36266/100000: episode: 584, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 192.300, mean reward: 1.923 [1.484, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.332, 10.098], loss: 1.662551, mae: 0.580192, mean_q: 4.867145
 36366/100000: episode: 585, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.981, mean reward: 1.880 [1.474, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.430, 10.098], loss: 1.643600, mae: 0.535841, mean_q: 4.790263
 36466/100000: episode: 586, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 191.593, mean reward: 1.916 [1.478, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.049, 10.124], loss: 3.047807, mae: 0.652594, mean_q: 4.884421
 36566/100000: episode: 587, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.099, mean reward: 1.891 [1.453, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.239, 10.098], loss: 0.418289, mae: 0.506464, mean_q: 4.839940
 36666/100000: episode: 588, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 188.052, mean reward: 1.881 [1.443, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.360, 10.110], loss: 1.605186, mae: 0.545213, mean_q: 4.760149
 36766/100000: episode: 589, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 187.543, mean reward: 1.875 [1.448, 5.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.532, 10.415], loss: 0.943806, mae: 0.503360, mean_q: 4.727166
 36866/100000: episode: 590, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 192.276, mean reward: 1.923 [1.510, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.408, 10.155], loss: 1.630657, mae: 0.574165, mean_q: 4.841335
 36966/100000: episode: 591, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 190.851, mean reward: 1.909 [1.447, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.363, 10.114], loss: 2.718406, mae: 0.572097, mean_q: 4.713614
 37066/100000: episode: 592, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 198.394, mean reward: 1.984 [1.452, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.405, 10.098], loss: 0.507477, mae: 0.537723, mean_q: 4.696010
 37166/100000: episode: 593, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 216.250, mean reward: 2.162 [1.488, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.599, 10.098], loss: 1.611173, mae: 0.517178, mean_q: 4.707051
 37266/100000: episode: 594, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 178.039, mean reward: 1.780 [1.444, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.973, 10.163], loss: 2.912438, mae: 0.588814, mean_q: 4.613861
 37366/100000: episode: 595, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.422, mean reward: 1.884 [1.462, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.048, 10.229], loss: 1.556391, mae: 0.488065, mean_q: 4.591059
 37466/100000: episode: 596, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 194.252, mean reward: 1.943 [1.481, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.722, 10.122], loss: 1.470966, mae: 0.518336, mean_q: 4.437341
 37566/100000: episode: 597, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 191.768, mean reward: 1.918 [1.489, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.799, 10.098], loss: 0.312804, mae: 0.413081, mean_q: 4.380407
 37666/100000: episode: 598, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 231.484, mean reward: 2.315 [1.460, 8.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.928, 10.098], loss: 0.883454, mae: 0.436609, mean_q: 4.272407
 37766/100000: episode: 599, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.702, mean reward: 1.967 [1.465, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.982, 10.098], loss: 2.080662, mae: 0.491205, mean_q: 4.287313
 37866/100000: episode: 600, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.533, mean reward: 1.845 [1.458, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.487, 10.098], loss: 3.240156, mae: 0.503020, mean_q: 4.293866
 37966/100000: episode: 601, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 182.982, mean reward: 1.830 [1.472, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.268, 10.124], loss: 1.566223, mae: 0.486706, mean_q: 4.208039
 38066/100000: episode: 602, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 182.206, mean reward: 1.822 [1.463, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.760, 10.098], loss: 0.203590, mae: 0.356270, mean_q: 4.110310
 38166/100000: episode: 603, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 185.252, mean reward: 1.853 [1.486, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.487, 10.098], loss: 1.479578, mae: 0.419380, mean_q: 4.090664
 38266/100000: episode: 604, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.522, mean reward: 1.845 [1.466, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.834, 10.171], loss: 0.173153, mae: 0.319096, mean_q: 3.987689
 38366/100000: episode: 605, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 181.132, mean reward: 1.811 [1.448, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.297, 10.098], loss: 1.414213, mae: 0.385515, mean_q: 3.919246
 38466/100000: episode: 606, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 190.917, mean reward: 1.909 [1.431, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.166, 10.308], loss: 0.172508, mae: 0.303332, mean_q: 3.848112
 38566/100000: episode: 607, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 188.061, mean reward: 1.881 [1.459, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.790, 10.249], loss: 0.074978, mae: 0.272095, mean_q: 3.782632
 38666/100000: episode: 608, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.610, mean reward: 1.886 [1.444, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.585, 10.098], loss: 0.083703, mae: 0.277317, mean_q: 3.789639
 38766/100000: episode: 609, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.580, mean reward: 1.956 [1.445, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.058, 10.261], loss: 0.152097, mae: 0.284755, mean_q: 3.809154
 38866/100000: episode: 610, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 226.484, mean reward: 2.265 [1.513, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.227, 10.098], loss: 0.083105, mae: 0.282527, mean_q: 3.805316
 38966/100000: episode: 611, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 193.040, mean reward: 1.930 [1.468, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.763, 10.098], loss: 0.078234, mae: 0.278383, mean_q: 3.812630
 39066/100000: episode: 612, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.743, mean reward: 1.857 [1.444, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.136, 10.156], loss: 0.116925, mae: 0.282092, mean_q: 3.821807
 39166/100000: episode: 613, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 185.794, mean reward: 1.858 [1.459, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.863, 10.223], loss: 0.172573, mae: 0.295744, mean_q: 3.815012
 39266/100000: episode: 614, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 184.091, mean reward: 1.841 [1.445, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.260, 10.275], loss: 0.079233, mae: 0.282316, mean_q: 3.839044
 39366/100000: episode: 615, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 203.597, mean reward: 2.036 [1.457, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.987, 10.100], loss: 0.076762, mae: 0.270826, mean_q: 3.803287
 39466/100000: episode: 616, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 187.657, mean reward: 1.877 [1.455, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.272, 10.098], loss: 0.082613, mae: 0.280272, mean_q: 3.817375
 39566/100000: episode: 617, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 211.811, mean reward: 2.118 [1.545, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.106, 10.275], loss: 0.077218, mae: 0.280399, mean_q: 3.821336
 39666/100000: episode: 618, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 198.119, mean reward: 1.981 [1.479, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.373, 10.098], loss: 0.077337, mae: 0.272378, mean_q: 3.812801
 39766/100000: episode: 619, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 198.422, mean reward: 1.984 [1.445, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.058, 10.098], loss: 0.084225, mae: 0.282414, mean_q: 3.842961
 39866/100000: episode: 620, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 193.427, mean reward: 1.934 [1.452, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.013, 10.098], loss: 0.077212, mae: 0.282649, mean_q: 3.835019
 39966/100000: episode: 621, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 197.230, mean reward: 1.972 [1.438, 5.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.819, 10.098], loss: 0.082020, mae: 0.285606, mean_q: 3.844677
 40066/100000: episode: 622, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 187.660, mean reward: 1.877 [1.497, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.026, 10.296], loss: 0.097805, mae: 0.285792, mean_q: 3.828774
 40166/100000: episode: 623, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 224.731, mean reward: 2.247 [1.472, 4.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.360, 10.237], loss: 0.077650, mae: 0.277501, mean_q: 3.813256
 40266/100000: episode: 624, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 211.635, mean reward: 2.116 [1.457, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.872, 10.098], loss: 0.090898, mae: 0.300962, mean_q: 3.852160
 40366/100000: episode: 625, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 176.775, mean reward: 1.768 [1.462, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.392, 10.179], loss: 0.094002, mae: 0.291281, mean_q: 3.842982
 40466/100000: episode: 626, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 223.384, mean reward: 2.234 [1.520, 4.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.720, 10.098], loss: 0.087531, mae: 0.296123, mean_q: 3.861392
 40566/100000: episode: 627, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 203.412, mean reward: 2.034 [1.453, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.168, 10.444], loss: 0.083034, mae: 0.288441, mean_q: 3.863040
 40666/100000: episode: 628, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 199.073, mean reward: 1.991 [1.434, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.591, 10.098], loss: 0.083566, mae: 0.290670, mean_q: 3.846833
 40766/100000: episode: 629, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 178.796, mean reward: 1.788 [1.466, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.053, 10.143], loss: 0.090608, mae: 0.289841, mean_q: 3.848851
 40866/100000: episode: 630, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 191.119, mean reward: 1.911 [1.488, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.282, 10.311], loss: 0.082509, mae: 0.286299, mean_q: 3.834428
 40966/100000: episode: 631, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 190.841, mean reward: 1.908 [1.451, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.662, 10.098], loss: 0.079254, mae: 0.287610, mean_q: 3.849866
 41066/100000: episode: 632, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 199.059, mean reward: 1.991 [1.461, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.762, 10.188], loss: 0.073110, mae: 0.274674, mean_q: 3.831938
 41166/100000: episode: 633, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 278.910, mean reward: 2.789 [1.452, 11.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.277, 10.098], loss: 0.093407, mae: 0.299616, mean_q: 3.857914
 41266/100000: episode: 634, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 195.323, mean reward: 1.953 [1.484, 4.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.456, 10.234], loss: 0.090856, mae: 0.290119, mean_q: 3.884929
 41366/100000: episode: 635, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 184.912, mean reward: 1.849 [1.440, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.161, 10.178], loss: 0.087403, mae: 0.287512, mean_q: 3.880701
 41466/100000: episode: 636, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 192.817, mean reward: 1.928 [1.456, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.237, 10.098], loss: 0.096766, mae: 0.300844, mean_q: 3.887718
 41566/100000: episode: 637, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 202.351, mean reward: 2.024 [1.482, 4.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.700, 10.176], loss: 0.082631, mae: 0.283128, mean_q: 3.882030
 41666/100000: episode: 638, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 200.678, mean reward: 2.007 [1.448, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.799, 10.352], loss: 0.092294, mae: 0.290133, mean_q: 3.899401
 41766/100000: episode: 639, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 205.866, mean reward: 2.059 [1.485, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.472, 10.098], loss: 0.102888, mae: 0.307682, mean_q: 3.908749
 41866/100000: episode: 640, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 210.923, mean reward: 2.109 [1.452, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.285, 10.180], loss: 0.085519, mae: 0.290410, mean_q: 3.880728
 41966/100000: episode: 641, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.062, mean reward: 1.921 [1.457, 7.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.020, 10.192], loss: 0.090623, mae: 0.298717, mean_q: 3.918818
 42066/100000: episode: 642, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 185.550, mean reward: 1.855 [1.450, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.177, 10.171], loss: 0.086600, mae: 0.297380, mean_q: 3.884197
 42166/100000: episode: 643, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.812, mean reward: 1.868 [1.469, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.160, 10.098], loss: 0.095430, mae: 0.298892, mean_q: 3.895155
 42266/100000: episode: 644, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 198.608, mean reward: 1.986 [1.456, 6.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.746, 10.098], loss: 0.093082, mae: 0.295372, mean_q: 3.906497
 42366/100000: episode: 645, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.107, mean reward: 1.861 [1.510, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.745, 10.295], loss: 0.095376, mae: 0.300233, mean_q: 3.896052
 42466/100000: episode: 646, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 180.007, mean reward: 1.800 [1.454, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.460, 10.098], loss: 0.106738, mae: 0.305156, mean_q: 3.906028
 42566/100000: episode: 647, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.224, mean reward: 1.932 [1.463, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.435, 10.188], loss: 0.075319, mae: 0.286857, mean_q: 3.891721
 42666/100000: episode: 648, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 179.405, mean reward: 1.794 [1.440, 2.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.632, 10.098], loss: 0.099285, mae: 0.295056, mean_q: 3.899697
 42766/100000: episode: 649, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 181.590, mean reward: 1.816 [1.446, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.878, 10.120], loss: 0.096151, mae: 0.298073, mean_q: 3.867501
 42866/100000: episode: 650, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 207.082, mean reward: 2.071 [1.504, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.087, 10.283], loss: 0.084464, mae: 0.283262, mean_q: 3.864811
 42966/100000: episode: 651, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 178.834, mean reward: 1.788 [1.446, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.081, 10.098], loss: 0.090026, mae: 0.294796, mean_q: 3.864566
 43066/100000: episode: 652, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 180.595, mean reward: 1.806 [1.468, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.233, 10.132], loss: 0.074658, mae: 0.280622, mean_q: 3.851955
 43166/100000: episode: 653, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 187.921, mean reward: 1.879 [1.456, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.842, 10.098], loss: 0.077490, mae: 0.285503, mean_q: 3.888478
 43266/100000: episode: 654, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 207.358, mean reward: 2.074 [1.450, 3.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.153, 10.105], loss: 0.096448, mae: 0.295225, mean_q: 3.878624
 43366/100000: episode: 655, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.822, mean reward: 1.928 [1.451, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.988, 10.320], loss: 0.104146, mae: 0.302534, mean_q: 3.894226
[Info] 1-TH LEVEL FOUND: 5.354841232299805, Considering 10/90 traces
 43466/100000: episode: 656, duration: 4.656s, episode steps: 100, steps per second: 21, episode reward: 179.878, mean reward: 1.799 [1.458, 5.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.903, 10.098], loss: 0.103635, mae: 0.302208, mean_q: 3.887598
 43515/100000: episode: 657, duration: 0.256s, episode steps: 49, steps per second: 192, episode reward: 124.604, mean reward: 2.543 [1.668, 6.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.223, 10.100], loss: 0.099487, mae: 0.304170, mean_q: 3.885751
 43532/100000: episode: 658, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 54.411, mean reward: 3.201 [2.625, 4.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.457, 10.100], loss: 0.099978, mae: 0.322957, mean_q: 3.916713
 43540/100000: episode: 659, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 15.084, mean reward: 1.885 [1.678, 2.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.518, 10.100], loss: 0.112384, mae: 0.344128, mean_q: 3.933470
 43588/100000: episode: 660, duration: 0.235s, episode steps: 48, steps per second: 204, episode reward: 157.169, mean reward: 3.274 [2.127, 6.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.534, 10.100], loss: 0.138324, mae: 0.349836, mean_q: 3.960041
 43602/100000: episode: 661, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 44.643, mean reward: 3.189 [2.055, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.619, 10.100], loss: 0.171284, mae: 0.349307, mean_q: 3.966930
 43650/100000: episode: 662, duration: 0.250s, episode steps: 48, steps per second: 192, episode reward: 136.713, mean reward: 2.848 [1.967, 5.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.781, 10.100], loss: 0.129730, mae: 0.325125, mean_q: 3.920269
 43658/100000: episode: 663, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 19.118, mean reward: 2.390 [2.209, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.276, 10.100], loss: 0.110432, mae: 0.361855, mean_q: 3.984711
 43748/100000: episode: 664, duration: 0.481s, episode steps: 90, steps per second: 187, episode reward: 175.174, mean reward: 1.946 [1.445, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.541 [-1.098, 10.178], loss: 0.101452, mae: 0.306157, mean_q: 3.947169
 43770/100000: episode: 665, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 56.834, mean reward: 2.583 [2.215, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.000, 10.100], loss: 0.096636, mae: 0.300962, mean_q: 3.966203
 43795/100000: episode: 666, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 83.663, mean reward: 3.347 [1.788, 8.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.691, 10.100], loss: 0.089236, mae: 0.303231, mean_q: 3.897921
 43820/100000: episode: 667, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 92.951, mean reward: 3.718 [1.794, 6.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.272, 10.100], loss: 0.148856, mae: 0.326823, mean_q: 3.951653
 43845/100000: episode: 668, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 73.899, mean reward: 2.956 [1.610, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.437, 10.100], loss: 0.125945, mae: 0.324209, mean_q: 3.990970
 43935/100000: episode: 669, duration: 0.461s, episode steps: 90, steps per second: 195, episode reward: 178.804, mean reward: 1.987 [1.564, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-0.810, 10.100], loss: 0.136234, mae: 0.331959, mean_q: 3.998274
 43943/100000: episode: 670, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 17.175, mean reward: 2.147 [2.000, 2.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.651, 10.100], loss: 0.187715, mae: 0.374973, mean_q: 4.137552
 43957/100000: episode: 671, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 36.496, mean reward: 2.607 [2.167, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.200, 10.100], loss: 0.206453, mae: 0.351815, mean_q: 3.984701
 44006/100000: episode: 672, duration: 0.257s, episode steps: 49, steps per second: 191, episode reward: 136.829, mean reward: 2.792 [1.883, 4.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.472, 10.100], loss: 0.115106, mae: 0.334501, mean_q: 4.005803
 44096/100000: episode: 673, duration: 0.456s, episode steps: 90, steps per second: 197, episode reward: 176.711, mean reward: 1.963 [1.461, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.555 [-0.563, 10.282], loss: 0.122000, mae: 0.325795, mean_q: 4.006831
 44118/100000: episode: 674, duration: 0.132s, episode steps: 22, steps per second: 166, episode reward: 47.262, mean reward: 2.148 [1.605, 2.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.049, 10.100], loss: 0.178238, mae: 0.363474, mean_q: 4.063407
 44208/100000: episode: 675, duration: 0.489s, episode steps: 90, steps per second: 184, episode reward: 169.247, mean reward: 1.881 [1.449, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-0.171, 10.100], loss: 0.118467, mae: 0.335048, mean_q: 4.024369
 44225/100000: episode: 676, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 58.644, mean reward: 3.450 [2.402, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.267, 10.100], loss: 0.156861, mae: 0.350146, mean_q: 3.952507
 44242/100000: episode: 677, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 43.954, mean reward: 2.586 [2.052, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.242, 10.100], loss: 0.155013, mae: 0.377283, mean_q: 4.128763
 44291/100000: episode: 678, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 110.398, mean reward: 2.253 [1.455, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.308, 10.100], loss: 0.124263, mae: 0.335081, mean_q: 4.068302
 44339/100000: episode: 679, duration: 0.233s, episode steps: 48, steps per second: 206, episode reward: 141.912, mean reward: 2.956 [1.780, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.425, 10.100], loss: 0.148950, mae: 0.345807, mean_q: 4.024529
 44364/100000: episode: 680, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 76.268, mean reward: 3.051 [1.740, 5.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-1.335, 10.100], loss: 0.136712, mae: 0.332518, mean_q: 4.102627
 44405/100000: episode: 681, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 119.117, mean reward: 2.905 [2.156, 5.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.592, 10.100], loss: 0.122136, mae: 0.330124, mean_q: 4.048703
 44413/100000: episode: 682, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 23.938, mean reward: 2.992 [2.370, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.283, 10.100], loss: 0.104483, mae: 0.333667, mean_q: 4.128284
 44435/100000: episode: 683, duration: 0.109s, episode steps: 22, steps per second: 203, episode reward: 52.068, mean reward: 2.367 [2.029, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.288, 10.100], loss: 0.086268, mae: 0.296854, mean_q: 4.086673
 44443/100000: episode: 684, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 18.350, mean reward: 2.294 [1.972, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.299, 10.100], loss: 0.082876, mae: 0.299478, mean_q: 4.080633
 44484/100000: episode: 685, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 102.993, mean reward: 2.512 [1.615, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.207, 10.100], loss: 0.129418, mae: 0.338012, mean_q: 4.084506
 44525/100000: episode: 686, duration: 0.234s, episode steps: 41, steps per second: 175, episode reward: 112.932, mean reward: 2.754 [2.226, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.242, 10.100], loss: 0.147720, mae: 0.345106, mean_q: 4.111859
 44615/100000: episode: 687, duration: 0.499s, episode steps: 90, steps per second: 180, episode reward: 180.319, mean reward: 2.004 [1.454, 5.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.544 [-0.966, 10.154], loss: 0.143566, mae: 0.350660, mean_q: 4.111505
 44705/100000: episode: 688, duration: 0.448s, episode steps: 90, steps per second: 201, episode reward: 171.410, mean reward: 1.905 [1.466, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-0.430, 10.156], loss: 0.158381, mae: 0.356719, mean_q: 4.098118
 44746/100000: episode: 689, duration: 0.223s, episode steps: 41, steps per second: 184, episode reward: 105.611, mean reward: 2.576 [1.460, 4.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.142, 10.100], loss: 0.153173, mae: 0.344018, mean_q: 4.091153
 44836/100000: episode: 690, duration: 0.481s, episode steps: 90, steps per second: 187, episode reward: 170.268, mean reward: 1.892 [1.469, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.552 [-1.098, 10.100], loss: 0.111806, mae: 0.331228, mean_q: 4.092051
 44885/100000: episode: 691, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 113.959, mean reward: 2.326 [1.477, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-0.938, 10.331], loss: 0.111268, mae: 0.315247, mean_q: 4.061203
 44893/100000: episode: 692, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 22.623, mean reward: 2.828 [2.396, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.414, 10.100], loss: 0.140764, mae: 0.348961, mean_q: 4.132899
 44983/100000: episode: 693, duration: 0.477s, episode steps: 90, steps per second: 189, episode reward: 193.284, mean reward: 2.148 [1.522, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-1.229, 10.160], loss: 0.123545, mae: 0.335311, mean_q: 4.146798
 45031/100000: episode: 694, duration: 0.243s, episode steps: 48, steps per second: 197, episode reward: 104.271, mean reward: 2.172 [1.470, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.976, 10.203], loss: 0.112218, mae: 0.335468, mean_q: 4.114709
 45080/100000: episode: 695, duration: 0.266s, episode steps: 49, steps per second: 184, episode reward: 123.798, mean reward: 2.526 [1.446, 6.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-1.487, 10.439], loss: 0.123180, mae: 0.322244, mean_q: 4.092263
 45121/100000: episode: 696, duration: 0.219s, episode steps: 41, steps per second: 188, episode reward: 141.398, mean reward: 3.449 [2.548, 5.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.566, 10.100], loss: 0.144253, mae: 0.337397, mean_q: 4.128578
 45162/100000: episode: 697, duration: 0.214s, episode steps: 41, steps per second: 192, episode reward: 115.339, mean reward: 2.813 [1.678, 6.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.523, 10.100], loss: 0.128119, mae: 0.346712, mean_q: 4.174847
 45176/100000: episode: 698, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 37.257, mean reward: 2.661 [2.260, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.162, 10.100], loss: 0.144028, mae: 0.368585, mean_q: 4.251202
 45184/100000: episode: 699, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 19.581, mean reward: 2.448 [2.082, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.365, 10.100], loss: 0.118836, mae: 0.320451, mean_q: 4.106834
 45225/100000: episode: 700, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 112.713, mean reward: 2.749 [1.932, 5.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.503, 10.100], loss: 0.139888, mae: 0.355228, mean_q: 4.186097
 45273/100000: episode: 701, duration: 0.246s, episode steps: 48, steps per second: 195, episode reward: 113.969, mean reward: 2.374 [1.497, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-1.442, 10.172], loss: 0.126774, mae: 0.340193, mean_q: 4.151626
 45363/100000: episode: 702, duration: 0.466s, episode steps: 90, steps per second: 193, episode reward: 166.664, mean reward: 1.852 [1.467, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.551 [-0.808, 10.100], loss: 0.133313, mae: 0.335472, mean_q: 4.162271
 45377/100000: episode: 703, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 34.919, mean reward: 2.494 [2.039, 3.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.816, 10.100], loss: 0.128168, mae: 0.333571, mean_q: 4.157237
 45385/100000: episode: 704, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 17.258, mean reward: 2.157 [1.827, 2.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.344, 10.100], loss: 0.114477, mae: 0.344841, mean_q: 4.256038
 45475/100000: episode: 705, duration: 0.512s, episode steps: 90, steps per second: 176, episode reward: 181.176, mean reward: 2.013 [1.466, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.602, 10.100], loss: 0.117845, mae: 0.330213, mean_q: 4.154949
 45516/100000: episode: 706, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 103.176, mean reward: 2.516 [1.884, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.562, 10.100], loss: 0.116607, mae: 0.336653, mean_q: 4.169700
 45565/100000: episode: 707, duration: 0.244s, episode steps: 49, steps per second: 200, episode reward: 215.339, mean reward: 4.395 [2.641, 8.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.289, 10.100], loss: 0.168569, mae: 0.371372, mean_q: 4.214985
 45613/100000: episode: 708, duration: 0.237s, episode steps: 48, steps per second: 203, episode reward: 155.202, mean reward: 3.233 [1.807, 6.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.985, 10.100], loss: 0.181955, mae: 0.369545, mean_q: 4.244399
 45635/100000: episode: 709, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 50.728, mean reward: 2.306 [1.594, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.157, 10.100], loss: 0.122049, mae: 0.351830, mean_q: 4.298078
 45660/100000: episode: 710, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 54.785, mean reward: 2.191 [1.752, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.560, 10.100], loss: 0.134331, mae: 0.363291, mean_q: 4.250800
 45674/100000: episode: 711, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 34.840, mean reward: 2.489 [1.838, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.267, 10.100], loss: 0.181793, mae: 0.396303, mean_q: 4.404078
 45688/100000: episode: 712, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 38.825, mean reward: 2.773 [2.252, 4.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.314, 10.100], loss: 0.179827, mae: 0.364799, mean_q: 4.278358
 45705/100000: episode: 713, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 51.336, mean reward: 3.020 [2.256, 4.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.230, 10.100], loss: 0.184559, mae: 0.367593, mean_q: 4.327193
 45713/100000: episode: 714, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 28.002, mean reward: 3.500 [2.404, 5.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.934, 10.100], loss: 0.107136, mae: 0.333060, mean_q: 4.200509
 45735/100000: episode: 715, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 57.771, mean reward: 2.626 [2.079, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.651, 10.100], loss: 0.153498, mae: 0.357963, mean_q: 4.178640
 45752/100000: episode: 716, duration: 0.109s, episode steps: 17, steps per second: 157, episode reward: 52.759, mean reward: 3.103 [2.422, 4.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.197, 10.100], loss: 0.152253, mae: 0.361199, mean_q: 4.254170
 45793/100000: episode: 717, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 131.969, mean reward: 3.219 [2.138, 5.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.340, 10.100], loss: 0.146910, mae: 0.369347, mean_q: 4.301994
 45841/100000: episode: 718, duration: 0.249s, episode steps: 48, steps per second: 193, episode reward: 111.999, mean reward: 2.333 [1.535, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.100, 10.150], loss: 0.143210, mae: 0.361645, mean_q: 4.304911
 45849/100000: episode: 719, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 20.046, mean reward: 2.506 [2.271, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.337, 10.100], loss: 0.207961, mae: 0.432963, mean_q: 4.401405
 45871/100000: episode: 720, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 66.986, mean reward: 3.045 [2.070, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.352, 10.100], loss: 0.148200, mae: 0.350708, mean_q: 4.244977
 45879/100000: episode: 721, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 27.657, mean reward: 3.457 [2.579, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.384, 10.100], loss: 0.195846, mae: 0.390297, mean_q: 4.424193
 45969/100000: episode: 722, duration: 0.460s, episode steps: 90, steps per second: 196, episode reward: 203.966, mean reward: 2.266 [1.472, 5.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.504, 10.100], loss: 0.153926, mae: 0.355364, mean_q: 4.326793
 45977/100000: episode: 723, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 18.702, mean reward: 2.338 [2.046, 2.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.338, 10.100], loss: 0.139912, mae: 0.374675, mean_q: 4.403367
 46018/100000: episode: 724, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 103.011, mean reward: 2.512 [1.554, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.429, 10.100], loss: 0.178893, mae: 0.394250, mean_q: 4.342741
 46108/100000: episode: 725, duration: 0.470s, episode steps: 90, steps per second: 192, episode reward: 163.525, mean reward: 1.817 [1.474, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.556 [-0.577, 10.275], loss: 0.138295, mae: 0.356231, mean_q: 4.318764
 46198/100000: episode: 726, duration: 0.480s, episode steps: 90, steps per second: 187, episode reward: 188.642, mean reward: 2.096 [1.469, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.551 [-1.047, 10.303], loss: 0.138753, mae: 0.351306, mean_q: 4.280396
 46220/100000: episode: 727, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 73.703, mean reward: 3.350 [2.642, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.051, 10.100], loss: 0.163400, mae: 0.368015, mean_q: 4.246171
 46228/100000: episode: 728, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 21.945, mean reward: 2.743 [2.116, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.270, 10.100], loss: 0.204042, mae: 0.459852, mean_q: 4.603388
 46236/100000: episode: 729, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 24.869, mean reward: 3.109 [2.371, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.192, 10.100], loss: 0.207392, mae: 0.366907, mean_q: 4.215618
 46244/100000: episode: 730, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 23.627, mean reward: 2.953 [2.298, 4.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-1.295, 10.100], loss: 0.195364, mae: 0.385417, mean_q: 4.357933
 46261/100000: episode: 731, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 52.982, mean reward: 3.117 [2.631, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.330, 10.100], loss: 0.161305, mae: 0.361003, mean_q: 4.257467
 46269/100000: episode: 732, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 25.266, mean reward: 3.158 [2.277, 5.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-1.105, 10.100], loss: 0.126218, mae: 0.345657, mean_q: 4.381868
 46277/100000: episode: 733, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 22.090, mean reward: 2.761 [2.076, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.276, 10.100], loss: 0.111339, mae: 0.339975, mean_q: 4.416574
 46294/100000: episode: 734, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 57.882, mean reward: 3.405 [2.300, 5.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.167, 10.100], loss: 0.174265, mae: 0.384592, mean_q: 4.334610
 46384/100000: episode: 735, duration: 0.478s, episode steps: 90, steps per second: 188, episode reward: 201.276, mean reward: 2.236 [1.466, 4.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.546 [-0.287, 10.360], loss: 0.138513, mae: 0.353377, mean_q: 4.321994
 46398/100000: episode: 736, duration: 0.094s, episode steps: 14, steps per second: 149, episode reward: 47.344, mean reward: 3.382 [2.457, 4.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.413, 10.100], loss: 0.153697, mae: 0.387263, mean_q: 4.405151
 46423/100000: episode: 737, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 70.695, mean reward: 2.828 [1.744, 4.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.535, 10.100], loss: 0.138559, mae: 0.378606, mean_q: 4.376202
 46431/100000: episode: 738, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 21.839, mean reward: 2.730 [2.185, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.241, 10.100], loss: 0.168292, mae: 0.399891, mean_q: 4.453180
 46479/100000: episode: 739, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 131.889, mean reward: 2.748 [1.932, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.457, 10.100], loss: 0.156332, mae: 0.372086, mean_q: 4.364291
 46520/100000: episode: 740, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 125.595, mean reward: 3.063 [2.326, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.599, 10.100], loss: 0.248590, mae: 0.446434, mean_q: 4.403424
 46534/100000: episode: 741, duration: 0.086s, episode steps: 14, steps per second: 164, episode reward: 44.568, mean reward: 3.183 [2.506, 6.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.832, 10.100], loss: 0.163520, mae: 0.380969, mean_q: 4.430493
 46559/100000: episode: 742, duration: 0.142s, episode steps: 25, steps per second: 177, episode reward: 90.985, mean reward: 3.639 [2.007, 5.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.500, 10.100], loss: 0.198528, mae: 0.414557, mean_q: 4.457362
 46608/100000: episode: 743, duration: 0.257s, episode steps: 49, steps per second: 191, episode reward: 115.353, mean reward: 2.354 [1.886, 4.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.455, 10.100], loss: 0.152826, mae: 0.379090, mean_q: 4.464248
 46630/100000: episode: 744, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 64.806, mean reward: 2.946 [2.260, 6.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.487, 10.100], loss: 0.190051, mae: 0.393835, mean_q: 4.537339
 46671/100000: episode: 745, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 112.438, mean reward: 2.742 [1.805, 5.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.501, 10.100], loss: 0.157917, mae: 0.383826, mean_q: 4.497361
[Info] 2-TH LEVEL FOUND: 7.637135028839111, Considering 10/90 traces
 46712/100000: episode: 746, duration: 4.426s, episode steps: 41, steps per second: 9, episode reward: 118.514, mean reward: 2.891 [2.143, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.376, 10.100], loss: 0.153820, mae: 0.372048, mean_q: 4.453341
 46742/100000: episode: 747, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 87.189, mean reward: 2.906 [2.043, 4.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.436, 10.100], loss: 0.135961, mae: 0.358394, mean_q: 4.523694
 46768/100000: episode: 748, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 88.415, mean reward: 3.401 [2.297, 10.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.654, 10.100], loss: 0.234954, mae: 0.431678, mean_q: 4.551378
 46798/100000: episode: 749, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 389.721, mean reward: 12.991 [3.930, 30.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.714, 10.100], loss: 0.412002, mae: 0.448999, mean_q: 4.510941
 46812/100000: episode: 750, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 52.811, mean reward: 3.772 [2.525, 4.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.550, 10.100], loss: 0.315264, mae: 0.472274, mean_q: 4.681688
 46841/100000: episode: 751, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 135.253, mean reward: 4.664 [2.531, 11.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.047, 10.100], loss: 0.534365, mae: 0.486053, mean_q: 4.621992
 46884/100000: episode: 752, duration: 0.233s, episode steps: 43, steps per second: 185, episode reward: 111.344, mean reward: 2.589 [1.495, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.844, 10.168], loss: 0.385520, mae: 0.504422, mean_q: 4.669982
 46910/100000: episode: 753, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 73.252, mean reward: 2.817 [1.909, 6.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.166, 10.100], loss: 0.292797, mae: 0.444585, mean_q: 4.618616
 46926/100000: episode: 754, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 58.316, mean reward: 3.645 [3.030, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.563, 10.100], loss: 0.546793, mae: 0.439610, mean_q: 4.665873
 46942/100000: episode: 755, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 58.191, mean reward: 3.637 [2.603, 5.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.307, 10.100], loss: 0.418604, mae: 0.406387, mean_q: 4.432325
 46985/100000: episode: 756, duration: 0.232s, episode steps: 43, steps per second: 186, episode reward: 157.882, mean reward: 3.672 [2.330, 6.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.340, 10.100], loss: 0.477817, mae: 0.454450, mean_q: 4.603570
 47014/100000: episode: 757, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 113.360, mean reward: 3.909 [2.660, 6.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.695, 10.100], loss: 0.460065, mae: 0.549866, mean_q: 4.780662
 47049/100000: episode: 758, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 124.957, mean reward: 3.570 [2.402, 5.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.692, 10.100], loss: 0.550605, mae: 0.542329, mean_q: 4.637838
 47063/100000: episode: 759, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 75.356, mean reward: 5.383 [2.748, 15.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.488, 10.100], loss: 0.238835, mae: 0.415914, mean_q: 4.666999
 47093/100000: episode: 760, duration: 0.162s, episode steps: 30, steps per second: 186, episode reward: 128.736, mean reward: 4.291 [3.183, 7.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.988, 10.100], loss: 0.625819, mae: 0.473246, mean_q: 4.765164
[Info] FALSIFICATION!
[Info] Levels: [5.354841, 7.637135, 12.481739]
[Info] Cond. Prob: [0.1, 0.1, 0.06]
[Info] Error Prob: 0.0006000000000000001

 47108/100000: episode: 761, duration: 4.588s, episode steps: 15, steps per second: 3, episode reward: 165.606, mean reward: 11.040 [2.971, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.002, 10.098], loss: 0.971577, mae: 0.535759, mean_q: 4.732905
 47208/100000: episode: 762, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 186.781, mean reward: 1.868 [1.474, 5.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.420, 10.138], loss: 3.318062, mae: 0.748921, mean_q: 4.897955
 47308/100000: episode: 763, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 200.741, mean reward: 2.007 [1.451, 5.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.596, 10.459], loss: 2.043863, mae: 0.688863, mean_q: 4.785638
 47408/100000: episode: 764, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.696, mean reward: 1.957 [1.464, 4.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.317, 10.098], loss: 0.573098, mae: 0.532701, mean_q: 4.830542
 47508/100000: episode: 765, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.627, mean reward: 1.936 [1.470, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.741, 10.157], loss: 4.206708, mae: 0.727878, mean_q: 4.939453
 47608/100000: episode: 766, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 181.660, mean reward: 1.817 [1.497, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.384, 10.139], loss: 2.978074, mae: 0.712854, mean_q: 4.865269
 47708/100000: episode: 767, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.018, mean reward: 1.890 [1.462, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.554, 10.098], loss: 1.638500, mae: 0.592735, mean_q: 4.924148
 47808/100000: episode: 768, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 201.177, mean reward: 2.012 [1.484, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.725, 10.098], loss: 1.831140, mae: 0.593930, mean_q: 4.872540
 47908/100000: episode: 769, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 198.420, mean reward: 1.984 [1.435, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.666, 10.282], loss: 0.509540, mae: 0.509398, mean_q: 4.838557
 48008/100000: episode: 770, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 186.033, mean reward: 1.860 [1.476, 2.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.074, 10.377], loss: 1.694148, mae: 0.589176, mean_q: 4.874387
 48108/100000: episode: 771, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 192.037, mean reward: 1.920 [1.455, 2.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.637, 10.141], loss: 0.411902, mae: 0.508790, mean_q: 4.796075
 48208/100000: episode: 772, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 181.872, mean reward: 1.819 [1.452, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.011, 10.098], loss: 1.679572, mae: 0.575828, mean_q: 4.864539
 48308/100000: episode: 773, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.902, mean reward: 1.859 [1.483, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.286, 10.134], loss: 1.687684, mae: 0.604908, mean_q: 4.875503
 48408/100000: episode: 774, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 202.633, mean reward: 2.026 [1.472, 4.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.457, 10.098], loss: 0.418952, mae: 0.484562, mean_q: 4.858937
 48508/100000: episode: 775, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 183.557, mean reward: 1.836 [1.446, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.988, 10.098], loss: 1.734203, mae: 0.573719, mean_q: 4.835024
 48608/100000: episode: 776, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 211.813, mean reward: 2.118 [1.478, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.168, 10.098], loss: 1.674218, mae: 0.557391, mean_q: 4.907930
 48708/100000: episode: 777, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 188.081, mean reward: 1.881 [1.435, 4.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.149, 10.179], loss: 3.060838, mae: 0.694639, mean_q: 4.802023
 48808/100000: episode: 778, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 185.197, mean reward: 1.852 [1.455, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.762, 10.225], loss: 1.659537, mae: 0.562023, mean_q: 4.762774
 48908/100000: episode: 779, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 199.824, mean reward: 1.998 [1.471, 6.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.828, 10.098], loss: 0.556735, mae: 0.514105, mean_q: 4.798265
 49008/100000: episode: 780, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 180.128, mean reward: 1.801 [1.437, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.325, 10.098], loss: 1.514535, mae: 0.530969, mean_q: 4.674085
 49108/100000: episode: 781, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 203.278, mean reward: 2.033 [1.457, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.503, 10.176], loss: 1.673180, mae: 0.573067, mean_q: 4.764753
 49208/100000: episode: 782, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 196.412, mean reward: 1.964 [1.454, 5.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.488, 10.287], loss: 0.728574, mae: 0.526652, mean_q: 4.697279
 49308/100000: episode: 783, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 200.205, mean reward: 2.002 [1.474, 4.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.478, 10.098], loss: 2.827671, mae: 0.645660, mean_q: 4.837256
 49408/100000: episode: 784, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 184.206, mean reward: 1.842 [1.453, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.372, 10.098], loss: 0.464940, mae: 0.497490, mean_q: 4.693838
 49508/100000: episode: 785, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 209.057, mean reward: 2.091 [1.457, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.052, 10.196], loss: 3.930226, mae: 0.671348, mean_q: 4.713400
 49608/100000: episode: 786, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 205.569, mean reward: 2.056 [1.490, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.668, 10.098], loss: 0.652190, mae: 0.532426, mean_q: 4.656274
 49708/100000: episode: 787, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 186.049, mean reward: 1.860 [1.475, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.664, 10.098], loss: 1.708166, mae: 0.536216, mean_q: 4.615431
 49808/100000: episode: 788, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 217.863, mean reward: 2.179 [1.515, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.714, 10.098], loss: 0.423340, mae: 0.460377, mean_q: 4.555934
 49908/100000: episode: 789, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 196.763, mean reward: 1.968 [1.533, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.593, 10.299], loss: 0.488914, mae: 0.485580, mean_q: 4.696238
 50008/100000: episode: 790, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 208.181, mean reward: 2.082 [1.437, 4.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.058, 10.110], loss: 0.458806, mae: 0.478488, mean_q: 4.685703
 50108/100000: episode: 791, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 186.744, mean reward: 1.867 [1.433, 2.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.410, 10.204], loss: 0.321566, mae: 0.413819, mean_q: 4.539201
 50208/100000: episode: 792, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 198.742, mean reward: 1.987 [1.466, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.031, 10.098], loss: 0.449763, mae: 0.452654, mean_q: 4.538255
 50308/100000: episode: 793, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 205.334, mean reward: 2.053 [1.436, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.892, 10.098], loss: 2.660331, mae: 0.568991, mean_q: 4.559482
 50408/100000: episode: 794, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 219.526, mean reward: 2.195 [1.492, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.970, 10.115], loss: 0.333175, mae: 0.440666, mean_q: 4.570853
 50508/100000: episode: 795, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.650, mean reward: 1.886 [1.436, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.607, 10.098], loss: 2.685795, mae: 0.574563, mean_q: 4.625649
 50608/100000: episode: 796, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 199.663, mean reward: 1.997 [1.454, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.645, 10.151], loss: 1.571154, mae: 0.490372, mean_q: 4.470622
 50708/100000: episode: 797, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 196.188, mean reward: 1.962 [1.451, 4.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.669, 10.351], loss: 1.625427, mae: 0.522340, mean_q: 4.498343
 50808/100000: episode: 798, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 211.151, mean reward: 2.112 [1.468, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.290, 10.364], loss: 2.946343, mae: 0.562703, mean_q: 4.461963
 50908/100000: episode: 799, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 179.745, mean reward: 1.797 [1.453, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.704, 10.098], loss: 0.506390, mae: 0.461483, mean_q: 4.451186
 51008/100000: episode: 800, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 194.273, mean reward: 1.943 [1.479, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.864, 10.167], loss: 0.423963, mae: 0.422750, mean_q: 4.455585
 51108/100000: episode: 801, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 188.158, mean reward: 1.882 [1.435, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.509, 10.098], loss: 0.578835, mae: 0.460262, mean_q: 4.424842
 51208/100000: episode: 802, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.531, mean reward: 1.875 [1.482, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.625, 10.173], loss: 0.520424, mae: 0.450139, mean_q: 4.476745
 51308/100000: episode: 803, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.255, mean reward: 1.903 [1.465, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.313, 10.098], loss: 0.445781, mae: 0.419346, mean_q: 4.352362
 51408/100000: episode: 804, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 196.634, mean reward: 1.966 [1.468, 4.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.312, 10.098], loss: 2.834829, mae: 0.586896, mean_q: 4.440522
 51508/100000: episode: 805, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 184.165, mean reward: 1.842 [1.463, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.159, 10.199], loss: 0.486789, mae: 0.445375, mean_q: 4.267205
 51608/100000: episode: 806, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 204.131, mean reward: 2.041 [1.465, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.585, 10.372], loss: 2.687166, mae: 0.547858, mean_q: 4.336169
 51708/100000: episode: 807, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 184.772, mean reward: 1.848 [1.455, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.343, 10.265], loss: 2.486289, mae: 0.500198, mean_q: 4.261219
 51808/100000: episode: 808, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 182.732, mean reward: 1.827 [1.457, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.167, 10.098], loss: 0.202544, mae: 0.354977, mean_q: 4.103583
 51908/100000: episode: 809, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 184.501, mean reward: 1.845 [1.480, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.564, 10.125], loss: 1.192469, mae: 0.357307, mean_q: 4.026403
 52008/100000: episode: 810, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 185.165, mean reward: 1.852 [1.475, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.052, 10.135], loss: 0.109128, mae: 0.307632, mean_q: 3.942177
 52108/100000: episode: 811, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.259, mean reward: 1.873 [1.476, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.821, 10.098], loss: 0.086247, mae: 0.286630, mean_q: 3.871845
 52208/100000: episode: 812, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 215.476, mean reward: 2.155 [1.674, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.334, 10.318], loss: 0.098731, mae: 0.290649, mean_q: 3.853427
 52308/100000: episode: 813, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 190.626, mean reward: 1.906 [1.495, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.492, 10.245], loss: 0.087711, mae: 0.284270, mean_q: 3.853311
 52408/100000: episode: 814, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 194.663, mean reward: 1.947 [1.465, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.661, 10.117], loss: 0.083257, mae: 0.282200, mean_q: 3.855224
 52508/100000: episode: 815, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 201.557, mean reward: 2.016 [1.454, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.303, 10.335], loss: 0.091860, mae: 0.288831, mean_q: 3.863602
 52608/100000: episode: 816, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.661, mean reward: 1.897 [1.458, 4.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.023, 10.177], loss: 0.086750, mae: 0.287869, mean_q: 3.860455
 52708/100000: episode: 817, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 181.644, mean reward: 1.816 [1.470, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.228, 10.098], loss: 0.082467, mae: 0.285588, mean_q: 3.872465
 52808/100000: episode: 818, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 191.425, mean reward: 1.914 [1.469, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.789, 10.150], loss: 0.091317, mae: 0.295331, mean_q: 3.881336
 52908/100000: episode: 819, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 205.127, mean reward: 2.051 [1.499, 4.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.063, 10.312], loss: 0.081233, mae: 0.285605, mean_q: 3.872719
 53008/100000: episode: 820, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 181.336, mean reward: 1.813 [1.455, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.698, 10.098], loss: 0.095674, mae: 0.293498, mean_q: 3.847600
 53108/100000: episode: 821, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 198.108, mean reward: 1.981 [1.444, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.330, 10.191], loss: 0.082006, mae: 0.285021, mean_q: 3.871066
 53208/100000: episode: 822, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 212.187, mean reward: 2.122 [1.501, 7.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.730, 10.098], loss: 0.082099, mae: 0.282691, mean_q: 3.840031
 53308/100000: episode: 823, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 186.274, mean reward: 1.863 [1.442, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.697, 10.119], loss: 0.093669, mae: 0.288286, mean_q: 3.846437
 53408/100000: episode: 824, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 195.932, mean reward: 1.959 [1.448, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.761, 10.120], loss: 0.095612, mae: 0.286436, mean_q: 3.862393
 53508/100000: episode: 825, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.211, mean reward: 1.862 [1.431, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.635, 10.148], loss: 0.091391, mae: 0.294150, mean_q: 3.880639
 53608/100000: episode: 826, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 201.416, mean reward: 2.014 [1.475, 4.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.821, 10.186], loss: 0.074134, mae: 0.269380, mean_q: 3.837409
 53708/100000: episode: 827, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 188.567, mean reward: 1.886 [1.436, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.843, 10.098], loss: 0.081640, mae: 0.280554, mean_q: 3.836819
 53808/100000: episode: 828, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 202.356, mean reward: 2.024 [1.467, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.033, 10.355], loss: 0.086194, mae: 0.281501, mean_q: 3.836083
 53908/100000: episode: 829, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 202.761, mean reward: 2.028 [1.440, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.597, 10.098], loss: 0.068919, mae: 0.266951, mean_q: 3.863334
 54008/100000: episode: 830, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 182.582, mean reward: 1.826 [1.472, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.252, 10.119], loss: 0.079682, mae: 0.283438, mean_q: 3.861751
 54108/100000: episode: 831, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.551, mean reward: 1.846 [1.457, 2.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.100, 10.098], loss: 0.084103, mae: 0.284989, mean_q: 3.848565
 54208/100000: episode: 832, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 179.770, mean reward: 1.798 [1.445, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.271, 10.098], loss: 0.085174, mae: 0.281054, mean_q: 3.835162
 54308/100000: episode: 833, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 181.362, mean reward: 1.814 [1.432, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.478, 10.098], loss: 0.083735, mae: 0.281667, mean_q: 3.859480
 54408/100000: episode: 834, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 212.371, mean reward: 2.124 [1.471, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.950, 10.400], loss: 0.082552, mae: 0.274798, mean_q: 3.850096
 54508/100000: episode: 835, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 188.613, mean reward: 1.886 [1.450, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.341, 10.140], loss: 0.084302, mae: 0.287419, mean_q: 3.847492
 54608/100000: episode: 836, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 204.000, mean reward: 2.040 [1.445, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.293, 10.098], loss: 0.076500, mae: 0.277843, mean_q: 3.833320
 54708/100000: episode: 837, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 196.850, mean reward: 1.968 [1.523, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.865, 10.098], loss: 0.073080, mae: 0.271440, mean_q: 3.831396
 54808/100000: episode: 838, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 238.190, mean reward: 2.382 [1.505, 3.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.968, 10.266], loss: 0.084168, mae: 0.286995, mean_q: 3.855461
 54908/100000: episode: 839, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 203.812, mean reward: 2.038 [1.463, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.900, 10.391], loss: 0.084979, mae: 0.292006, mean_q: 3.840514
 55008/100000: episode: 840, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 201.865, mean reward: 2.019 [1.450, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.181, 10.170], loss: 0.088238, mae: 0.291825, mean_q: 3.848107
 55108/100000: episode: 841, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 182.483, mean reward: 1.825 [1.431, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.934, 10.428], loss: 0.078329, mae: 0.286069, mean_q: 3.867398
 55208/100000: episode: 842, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 185.076, mean reward: 1.851 [1.483, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.549, 10.098], loss: 0.081554, mae: 0.284294, mean_q: 3.862958
 55308/100000: episode: 843, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 225.371, mean reward: 2.254 [1.484, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.909, 10.188], loss: 0.091133, mae: 0.295367, mean_q: 3.856839
 55408/100000: episode: 844, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 181.097, mean reward: 1.811 [1.456, 3.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.840, 10.136], loss: 0.081124, mae: 0.284637, mean_q: 3.854780
 55508/100000: episode: 845, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 195.532, mean reward: 1.955 [1.445, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.556, 10.098], loss: 0.074089, mae: 0.276748, mean_q: 3.831628
 55608/100000: episode: 846, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.598, mean reward: 1.936 [1.442, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.317, 10.098], loss: 0.075815, mae: 0.280885, mean_q: 3.847192
 55708/100000: episode: 847, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 219.666, mean reward: 2.197 [1.476, 4.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.925, 10.256], loss: 0.076607, mae: 0.279134, mean_q: 3.830524
 55808/100000: episode: 848, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 206.780, mean reward: 2.068 [1.433, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.172, 10.098], loss: 0.081064, mae: 0.283711, mean_q: 3.853896
 55908/100000: episode: 849, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 182.102, mean reward: 1.821 [1.449, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.044, 10.113], loss: 0.080620, mae: 0.282499, mean_q: 3.837246
 56008/100000: episode: 850, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 186.574, mean reward: 1.866 [1.448, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.038, 10.098], loss: 0.077172, mae: 0.277827, mean_q: 3.830606
 56108/100000: episode: 851, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 219.786, mean reward: 2.198 [1.505, 13.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.800, 10.098], loss: 0.079130, mae: 0.282846, mean_q: 3.845484
 56208/100000: episode: 852, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 183.351, mean reward: 1.834 [1.497, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.580, 10.158], loss: 0.124917, mae: 0.293857, mean_q: 3.864249
 56308/100000: episode: 853, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 187.246, mean reward: 1.872 [1.440, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.917, 10.192], loss: 0.087528, mae: 0.293079, mean_q: 3.863783
 56408/100000: episode: 854, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 198.088, mean reward: 1.981 [1.461, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.853, 10.448], loss: 0.100756, mae: 0.285482, mean_q: 3.861698
 56508/100000: episode: 855, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.563, mean reward: 1.866 [1.481, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.981, 10.155], loss: 0.092022, mae: 0.289601, mean_q: 3.844468
 56608/100000: episode: 856, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 185.898, mean reward: 1.859 [1.467, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.700, 10.193], loss: 0.108788, mae: 0.293179, mean_q: 3.853688
 56708/100000: episode: 857, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 176.296, mean reward: 1.763 [1.452, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.201, 10.152], loss: 0.086001, mae: 0.284133, mean_q: 3.834129
 56808/100000: episode: 858, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 192.400, mean reward: 1.924 [1.448, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.806, 10.364], loss: 0.095262, mae: 0.282755, mean_q: 3.852540
 56908/100000: episode: 859, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.144, mean reward: 1.951 [1.454, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.919, 10.152], loss: 0.071732, mae: 0.269651, mean_q: 3.838379
 57008/100000: episode: 860, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 197.833, mean reward: 1.978 [1.480, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.651, 10.223], loss: 0.123985, mae: 0.300048, mean_q: 3.870034
[Info] 1-TH LEVEL FOUND: 6.047840595245361, Considering 10/90 traces
 57108/100000: episode: 861, duration: 4.708s, episode steps: 100, steps per second: 21, episode reward: 182.653, mean reward: 1.827 [1.453, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.810, 10.160], loss: 0.107986, mae: 0.292665, mean_q: 3.851673
 57139/100000: episode: 862, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 80.399, mean reward: 2.594 [1.439, 4.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.133], loss: 0.117577, mae: 0.309384, mean_q: 3.893015
 57147/100000: episode: 863, duration: 0.055s, episode steps: 8, steps per second: 147, episode reward: 16.165, mean reward: 2.021 [1.578, 2.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.327, 10.100], loss: 0.100088, mae: 0.314925, mean_q: 3.932042
 57161/100000: episode: 864, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 55.888, mean reward: 3.992 [2.757, 6.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.267, 10.100], loss: 0.233734, mae: 0.320833, mean_q: 3.880539
 57173/100000: episode: 865, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 24.019, mean reward: 2.002 [1.696, 2.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.168, 10.100], loss: 0.088845, mae: 0.277128, mean_q: 3.793856
 57187/100000: episode: 866, duration: 0.087s, episode steps: 14, steps per second: 162, episode reward: 34.034, mean reward: 2.431 [1.905, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.263, 10.100], loss: 0.086080, mae: 0.287946, mean_q: 3.871683
 57201/100000: episode: 867, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 36.686, mean reward: 2.620 [2.159, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.375, 10.100], loss: 0.209457, mae: 0.300585, mean_q: 3.901310
 57211/100000: episode: 868, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 20.328, mean reward: 2.033 [1.706, 2.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.170, 10.100], loss: 0.089280, mae: 0.296707, mean_q: 3.854500
 57225/100000: episode: 869, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 31.226, mean reward: 2.230 [1.952, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.255, 10.100], loss: 0.107920, mae: 0.310004, mean_q: 3.911749
 57251/100000: episode: 870, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 97.135, mean reward: 3.736 [2.274, 6.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.349, 10.100], loss: 0.307970, mae: 0.342852, mean_q: 3.921262
 57276/100000: episode: 871, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 75.276, mean reward: 3.011 [1.769, 5.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.458, 10.100], loss: 0.092775, mae: 0.302908, mean_q: 3.899198
 57286/100000: episode: 872, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 22.778, mean reward: 2.278 [2.133, 2.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.264, 10.100], loss: 0.086318, mae: 0.291179, mean_q: 3.874313
 57311/100000: episode: 873, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 62.443, mean reward: 2.498 [1.839, 3.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.306, 10.100], loss: 0.172292, mae: 0.319112, mean_q: 3.925698
 57321/100000: episode: 874, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 30.103, mean reward: 3.010 [2.390, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.274, 10.100], loss: 0.084144, mae: 0.295444, mean_q: 3.913316
 57349/100000: episode: 875, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 67.910, mean reward: 2.425 [1.977, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.364, 10.100], loss: 0.090119, mae: 0.305978, mean_q: 3.936323
 57376/100000: episode: 876, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 75.445, mean reward: 2.794 [1.789, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.290, 10.100], loss: 0.098267, mae: 0.285825, mean_q: 3.855035
 57386/100000: episode: 877, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 27.529, mean reward: 2.753 [2.196, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.641, 10.100], loss: 0.078991, mae: 0.306991, mean_q: 3.984969
 57412/100000: episode: 878, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 74.701, mean reward: 2.873 [2.085, 5.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.102, 10.100], loss: 0.117600, mae: 0.324258, mean_q: 4.001828
 57426/100000: episode: 879, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 49.217, mean reward: 3.515 [2.496, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.137, 10.100], loss: 0.076063, mae: 0.278054, mean_q: 3.854973
 57434/100000: episode: 880, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 15.392, mean reward: 1.924 [1.762, 2.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.493, 10.100], loss: 0.060004, mae: 0.265791, mean_q: 3.935137
 57461/100000: episode: 881, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 87.748, mean reward: 3.250 [2.167, 5.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.347, 10.100], loss: 0.100444, mae: 0.309463, mean_q: 3.940466
 57488/100000: episode: 882, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 194.012, mean reward: 7.186 [2.516, 38.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.369, 10.100], loss: 0.103975, mae: 0.305302, mean_q: 3.900686
 57515/100000: episode: 883, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 80.672, mean reward: 2.988 [2.179, 4.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.652, 10.100], loss: 0.114665, mae: 0.321786, mean_q: 3.974273
 57542/100000: episode: 884, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 76.078, mean reward: 2.818 [2.103, 6.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.430, 10.100], loss: 0.349589, mae: 0.433689, mean_q: 4.027382
 57552/100000: episode: 885, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 26.136, mean reward: 2.614 [2.400, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.188, 10.100], loss: 0.200122, mae: 0.410427, mean_q: 4.101015
 57560/100000: episode: 886, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 17.002, mean reward: 2.125 [1.786, 2.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.316, 10.100], loss: 0.370501, mae: 0.426181, mean_q: 3.937766
 57585/100000: episode: 887, duration: 0.154s, episode steps: 25, steps per second: 163, episode reward: 70.933, mean reward: 2.837 [1.850, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.312, 10.100], loss: 0.147087, mae: 0.343339, mean_q: 3.969811
 57613/100000: episode: 888, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 65.261, mean reward: 2.331 [1.913, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-2.388, 10.100], loss: 0.100012, mae: 0.312042, mean_q: 3.962401
 57625/100000: episode: 889, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 26.753, mean reward: 2.229 [1.865, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.569, 10.100], loss: 0.104573, mae: 0.349231, mean_q: 3.973811
 57651/100000: episode: 890, duration: 0.132s, episode steps: 26, steps per second: 198, episode reward: 66.129, mean reward: 2.543 [2.023, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.370, 10.100], loss: 0.185686, mae: 0.338420, mean_q: 4.024302
 57682/100000: episode: 891, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 72.888, mean reward: 2.351 [1.564, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.271, 10.100], loss: 0.819954, mae: 0.546890, mean_q: 4.006412
 57708/100000: episode: 892, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 112.204, mean reward: 4.316 [2.283, 7.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.811, 10.100], loss: 0.437223, mae: 0.476229, mean_q: 4.029739
 57720/100000: episode: 893, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 22.248, mean reward: 1.854 [1.483, 2.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.140, 10.100], loss: 0.186171, mae: 0.385662, mean_q: 3.993138
 57728/100000: episode: 894, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 15.737, mean reward: 1.967 [1.485, 2.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.419, 10.100], loss: 0.117089, mae: 0.323691, mean_q: 3.903293
 57755/100000: episode: 895, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 66.317, mean reward: 2.456 [1.780, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.100], loss: 0.143252, mae: 0.336768, mean_q: 4.029950
 57780/100000: episode: 896, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 54.879, mean reward: 2.195 [1.715, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.169, 10.100], loss: 0.102950, mae: 0.319701, mean_q: 3.985976
 57807/100000: episode: 897, duration: 0.164s, episode steps: 27, steps per second: 165, episode reward: 56.750, mean reward: 2.102 [1.605, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.139, 10.100], loss: 0.251861, mae: 0.355298, mean_q: 4.061956
 57821/100000: episode: 898, duration: 0.094s, episode steps: 14, steps per second: 150, episode reward: 37.499, mean reward: 2.679 [2.163, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.291, 10.100], loss: 1.465598, mae: 0.525171, mean_q: 3.947603
 57835/100000: episode: 899, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 31.324, mean reward: 2.237 [1.788, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.892, 10.100], loss: 0.516928, mae: 0.530545, mean_q: 4.103872
 57860/100000: episode: 900, duration: 0.156s, episode steps: 25, steps per second: 160, episode reward: 67.741, mean reward: 2.710 [1.775, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.477, 10.100], loss: 0.194581, mae: 0.385973, mean_q: 4.043603
 57887/100000: episode: 901, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 69.337, mean reward: 2.568 [1.970, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-1.252, 10.100], loss: 0.280236, mae: 0.391804, mean_q: 4.135664
 57918/100000: episode: 902, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 77.818, mean reward: 2.510 [1.726, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.147, 10.100], loss: 0.146564, mae: 0.345648, mean_q: 3.991161
 57949/100000: episode: 903, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 99.873, mean reward: 3.222 [2.171, 5.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.408, 10.100], loss: 0.176503, mae: 0.344989, mean_q: 4.089072
 57974/100000: episode: 904, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 55.172, mean reward: 2.207 [1.790, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.235, 10.100], loss: 0.697543, mae: 0.429051, mean_q: 4.096929
 57984/100000: episode: 905, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 24.408, mean reward: 2.441 [2.094, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.232, 10.100], loss: 0.104867, mae: 0.351617, mean_q: 4.117310
 57996/100000: episode: 906, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 28.020, mean reward: 2.335 [2.094, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.330, 10.100], loss: 1.371464, mae: 0.498145, mean_q: 4.302440
 58021/100000: episode: 907, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 52.687, mean reward: 2.107 [1.581, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.359, 10.185], loss: 0.914004, mae: 0.508436, mean_q: 4.126538
 58031/100000: episode: 908, duration: 0.071s, episode steps: 10, steps per second: 142, episode reward: 18.998, mean reward: 1.900 [1.501, 2.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.124, 10.100], loss: 0.182801, mae: 0.391175, mean_q: 3.772569
 58045/100000: episode: 909, duration: 0.078s, episode steps: 14, steps per second: 178, episode reward: 28.491, mean reward: 2.035 [1.821, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.238, 10.100], loss: 0.198672, mae: 0.427730, mean_q: 4.102088
 58071/100000: episode: 910, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 77.618, mean reward: 2.985 [1.943, 4.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-1.102, 10.100], loss: 0.158388, mae: 0.368726, mean_q: 4.060102
 58098/100000: episode: 911, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 65.224, mean reward: 2.416 [1.807, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.523, 10.100], loss: 0.163352, mae: 0.371124, mean_q: 4.131223
 58110/100000: episode: 912, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 28.383, mean reward: 2.365 [1.917, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.323, 10.100], loss: 0.294714, mae: 0.367303, mean_q: 4.010619
 58136/100000: episode: 913, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 69.059, mean reward: 2.656 [2.116, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.261, 10.100], loss: 1.503904, mae: 0.523592, mean_q: 4.195838
 58162/100000: episode: 914, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 72.244, mean reward: 2.779 [2.004, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.398, 10.100], loss: 0.219118, mae: 0.421749, mean_q: 4.170133
 58187/100000: episode: 915, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 60.683, mean reward: 2.427 [2.029, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.349, 10.100], loss: 0.200505, mae: 0.350509, mean_q: 4.145137
 58212/100000: episode: 916, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 78.176, mean reward: 3.127 [1.963, 5.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.431, 10.100], loss: 0.135984, mae: 0.334829, mean_q: 4.106317
 58238/100000: episode: 917, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 57.702, mean reward: 2.219 [1.559, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.201, 10.100], loss: 0.497616, mae: 0.406585, mean_q: 4.129177
 58246/100000: episode: 918, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 15.537, mean reward: 1.942 [1.602, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.190, 10.100], loss: 0.162306, mae: 0.356700, mean_q: 4.039522
 58274/100000: episode: 919, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 208.526, mean reward: 7.447 [2.192, 86.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.240, 10.100], loss: 0.372994, mae: 0.388625, mean_q: 4.182756
 58282/100000: episode: 920, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 14.983, mean reward: 1.873 [1.479, 2.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.218, 10.100], loss: 0.725522, mae: 0.424018, mean_q: 4.124933
 58309/100000: episode: 921, duration: 0.162s, episode steps: 27, steps per second: 167, episode reward: 118.058, mean reward: 4.373 [2.103, 9.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.521, 10.100], loss: 0.206893, mae: 0.369852, mean_q: 4.147891
 58334/100000: episode: 922, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 69.239, mean reward: 2.770 [1.999, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.328, 10.100], loss: 0.240263, mae: 0.358582, mean_q: 4.256228
 58344/100000: episode: 923, duration: 0.064s, episode steps: 10, steps per second: 155, episode reward: 26.471, mean reward: 2.647 [2.447, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.279, 10.100], loss: 0.268213, mae: 0.428226, mean_q: 4.386868
 58352/100000: episode: 924, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 13.633, mean reward: 1.704 [1.592, 2.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.370, 10.100], loss: 0.139798, mae: 0.375899, mean_q: 4.205083
 58366/100000: episode: 925, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 29.661, mean reward: 2.119 [1.648, 2.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.504, 10.100], loss: 0.162080, mae: 0.366743, mean_q: 4.182922
 58394/100000: episode: 926, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 93.241, mean reward: 3.330 [2.512, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.338, 10.100], loss: 0.629100, mae: 0.455795, mean_q: 4.257167
 58425/100000: episode: 927, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 88.004, mean reward: 2.839 [1.849, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.197, 10.100], loss: 0.165407, mae: 0.372913, mean_q: 4.259502
 58433/100000: episode: 928, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 13.484, mean reward: 1.686 [1.511, 2.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.420, 10.100], loss: 0.139960, mae: 0.363641, mean_q: 4.262777
 58447/100000: episode: 929, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 39.152, mean reward: 2.797 [2.076, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.336, 10.100], loss: 0.460419, mae: 0.389737, mean_q: 4.228662
 58478/100000: episode: 930, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 81.292, mean reward: 2.622 [1.825, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.434, 10.100], loss: 0.331044, mae: 0.406674, mean_q: 4.251413
 58492/100000: episode: 931, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 28.315, mean reward: 2.022 [1.683, 2.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.139, 10.100], loss: 0.188251, mae: 0.376201, mean_q: 4.326454
 58500/100000: episode: 932, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 15.594, mean reward: 1.949 [1.589, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.603, 10.100], loss: 0.176505, mae: 0.403835, mean_q: 4.350867
 58514/100000: episode: 933, duration: 0.083s, episode steps: 14, steps per second: 170, episode reward: 34.881, mean reward: 2.491 [2.055, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.221, 10.100], loss: 0.135154, mae: 0.351155, mean_q: 4.144843
 58528/100000: episode: 934, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 41.253, mean reward: 2.947 [2.329, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.417, 10.100], loss: 0.110990, mae: 0.323150, mean_q: 4.216070
 58540/100000: episode: 935, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 28.399, mean reward: 2.367 [1.891, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.100], loss: 0.180120, mae: 0.390500, mean_q: 4.238166
 58565/100000: episode: 936, duration: 0.118s, episode steps: 25, steps per second: 212, episode reward: 55.303, mean reward: 2.212 [1.611, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.544, 10.100], loss: 0.264713, mae: 0.391912, mean_q: 4.234390
 58579/100000: episode: 937, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 32.086, mean reward: 2.292 [1.894, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.298, 10.100], loss: 0.820381, mae: 0.535090, mean_q: 4.366737
 58593/100000: episode: 938, duration: 0.072s, episode steps: 14, steps per second: 196, episode reward: 32.338, mean reward: 2.310 [1.569, 3.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.567, 10.100], loss: 0.227009, mae: 0.399059, mean_q: 4.119338
 58601/100000: episode: 939, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 18.531, mean reward: 2.316 [1.669, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.397, 10.100], loss: 0.456226, mae: 0.509134, mean_q: 4.453027
 58615/100000: episode: 940, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 29.805, mean reward: 2.129 [1.827, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.096, 10.100], loss: 0.282424, mae: 0.410159, mean_q: 4.249803
 58623/100000: episode: 941, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 14.395, mean reward: 1.799 [1.581, 1.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.789, 10.100], loss: 0.141256, mae: 0.361248, mean_q: 4.174533
 58654/100000: episode: 942, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 84.530, mean reward: 2.727 [1.899, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.302, 10.100], loss: 0.138907, mae: 0.363075, mean_q: 4.239035
 58666/100000: episode: 943, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 34.214, mean reward: 2.851 [2.451, 4.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.211, 10.100], loss: 0.334434, mae: 0.413421, mean_q: 4.255373
 58692/100000: episode: 944, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 61.796, mean reward: 2.377 [1.747, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.719, 10.100], loss: 0.867456, mae: 0.489966, mean_q: 4.357917
 58723/100000: episode: 945, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 72.905, mean reward: 2.352 [1.810, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.368, 10.100], loss: 0.271404, mae: 0.420914, mean_q: 4.263509
 58750/100000: episode: 946, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 68.705, mean reward: 2.545 [2.026, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.817, 10.100], loss: 0.755035, mae: 0.452480, mean_q: 4.289356
 58764/100000: episode: 947, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 31.787, mean reward: 2.270 [1.853, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.322, 10.100], loss: 7.770047, mae: 0.859513, mean_q: 4.673945
 58778/100000: episode: 948, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 45.481, mean reward: 3.249 [2.510, 4.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.246, 10.100], loss: 0.384335, mae: 0.635183, mean_q: 4.056026
 58803/100000: episode: 949, duration: 0.160s, episode steps: 25, steps per second: 157, episode reward: 64.382, mean reward: 2.575 [1.865, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.344, 10.100], loss: 0.246606, mae: 0.466168, mean_q: 4.285760
 58828/100000: episode: 950, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 61.187, mean reward: 2.447 [1.987, 2.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.213, 10.100], loss: 9.180388, mae: 0.931665, mean_q: 4.610431
[Info] NOT FOUND NEW LEVEL, Current Best Level is 6.047840595245361
 58859/100000: episode: 951, duration: 4.119s, episode steps: 31, steps per second: 8, episode reward: 111.573, mean reward: 3.599 [2.361, 5.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.068, 10.100], loss: 7.682549, mae: 0.779869, mean_q: 4.212326
 58959/100000: episode: 952, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 269.211, mean reward: 2.692 [1.486, 5.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.954, 10.436], loss: 1.364201, mae: 0.457041, mean_q: 4.279561
 59059/100000: episode: 953, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.650, mean reward: 1.957 [1.461, 3.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.861, 10.309], loss: 0.525677, mae: 0.501931, mean_q: 4.361726
 59159/100000: episode: 954, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 195.046, mean reward: 1.950 [1.438, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.807, 10.098], loss: 0.476466, mae: 0.451771, mean_q: 4.421503
 59259/100000: episode: 955, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 212.421, mean reward: 2.124 [1.441, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.853, 10.098], loss: 1.383527, mae: 0.495134, mean_q: 4.339997
 59359/100000: episode: 956, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 190.421, mean reward: 1.904 [1.437, 3.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.834, 10.218], loss: 3.897880, mae: 0.658446, mean_q: 4.438949
 59459/100000: episode: 957, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 185.736, mean reward: 1.857 [1.454, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.501, 10.098], loss: 0.357232, mae: 0.423985, mean_q: 4.340738
 59559/100000: episode: 958, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 180.531, mean reward: 1.805 [1.448, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.364, 10.098], loss: 1.539187, mae: 0.519845, mean_q: 4.418458
 59659/100000: episode: 959, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 200.350, mean reward: 2.003 [1.483, 4.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.819, 10.098], loss: 1.384440, mae: 0.483435, mean_q: 4.369987
 59759/100000: episode: 960, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 189.527, mean reward: 1.895 [1.464, 5.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.393, 10.098], loss: 0.423569, mae: 0.427641, mean_q: 4.393971
 59859/100000: episode: 961, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 193.617, mean reward: 1.936 [1.443, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.076, 10.245], loss: 0.189565, mae: 0.384600, mean_q: 4.332038
 59959/100000: episode: 962, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 202.331, mean reward: 2.023 [1.431, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.045, 10.098], loss: 0.569235, mae: 0.443065, mean_q: 4.354253
 60059/100000: episode: 963, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.236, mean reward: 1.882 [1.460, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.973, 10.415], loss: 0.379913, mae: 0.415706, mean_q: 4.327122
 60159/100000: episode: 964, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 178.322, mean reward: 1.783 [1.494, 2.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.660, 10.186], loss: 2.380087, mae: 0.554670, mean_q: 4.422336
 60259/100000: episode: 965, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 182.522, mean reward: 1.825 [1.440, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.506, 10.098], loss: 1.494476, mae: 0.512260, mean_q: 4.315973
 60359/100000: episode: 966, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 186.909, mean reward: 1.869 [1.475, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.229, 10.098], loss: 0.654832, mae: 0.473191, mean_q: 4.357690
 60459/100000: episode: 967, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 196.558, mean reward: 1.966 [1.504, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.957, 10.098], loss: 1.339544, mae: 0.473425, mean_q: 4.334244
 60559/100000: episode: 968, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 194.531, mean reward: 1.945 [1.485, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.172, 10.210], loss: 0.378656, mae: 0.426366, mean_q: 4.332925
 60659/100000: episode: 969, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 212.609, mean reward: 2.126 [1.483, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.713, 10.098], loss: 0.276729, mae: 0.407313, mean_q: 4.353513
 60759/100000: episode: 970, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 203.398, mean reward: 2.034 [1.456, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.092, 10.402], loss: 1.280491, mae: 0.428560, mean_q: 4.327033
 60859/100000: episode: 971, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 195.718, mean reward: 1.957 [1.473, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.844, 10.321], loss: 0.681739, mae: 0.504620, mean_q: 4.363505
 60959/100000: episode: 972, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 227.237, mean reward: 2.272 [1.528, 5.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.472, 10.254], loss: 0.228267, mae: 0.392489, mean_q: 4.298726
 61059/100000: episode: 973, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 187.084, mean reward: 1.871 [1.494, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.968, 10.236], loss: 0.358949, mae: 0.383371, mean_q: 4.308970
 61159/100000: episode: 974, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 190.579, mean reward: 1.906 [1.450, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.531, 10.231], loss: 0.421540, mae: 0.430013, mean_q: 4.399529
 61259/100000: episode: 975, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.132, mean reward: 1.871 [1.458, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.980, 10.255], loss: 0.443337, mae: 0.415886, mean_q: 4.382318
 61359/100000: episode: 976, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.056, mean reward: 1.911 [1.444, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.610, 10.184], loss: 0.412035, mae: 0.428761, mean_q: 4.344496
 61459/100000: episode: 977, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 188.025, mean reward: 1.880 [1.469, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.071, 10.189], loss: 0.434880, mae: 0.401097, mean_q: 4.375978
 61559/100000: episode: 978, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 195.029, mean reward: 1.950 [1.442, 4.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.546, 10.098], loss: 2.266190, mae: 0.511784, mean_q: 4.365805
 61659/100000: episode: 979, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 202.690, mean reward: 2.027 [1.451, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.808, 10.214], loss: 0.608707, mae: 0.531756, mean_q: 4.365621
 61759/100000: episode: 980, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 189.313, mean reward: 1.893 [1.451, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.425, 10.144], loss: 0.269531, mae: 0.396682, mean_q: 4.332527
 61859/100000: episode: 981, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 204.883, mean reward: 2.049 [1.495, 4.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.544, 10.225], loss: 0.692905, mae: 0.430804, mean_q: 4.391382
 61959/100000: episode: 982, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.148, mean reward: 1.941 [1.482, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.673, 10.098], loss: 1.554009, mae: 0.506305, mean_q: 4.359789
 62059/100000: episode: 983, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 187.094, mean reward: 1.871 [1.447, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.785, 10.098], loss: 0.554372, mae: 0.430742, mean_q: 4.396010
 62159/100000: episode: 984, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 190.208, mean reward: 1.902 [1.449, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.048, 10.098], loss: 1.583262, mae: 0.479554, mean_q: 4.411122
 62259/100000: episode: 985, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 208.289, mean reward: 2.083 [1.489, 6.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.002, 10.443], loss: 0.408988, mae: 0.422602, mean_q: 4.385174
 62359/100000: episode: 986, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 228.540, mean reward: 2.285 [1.682, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.077, 10.358], loss: 0.483615, mae: 0.420621, mean_q: 4.276504
 62459/100000: episode: 987, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 190.328, mean reward: 1.903 [1.495, 4.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.816, 10.295], loss: 1.294934, mae: 0.440097, mean_q: 4.282219
 62559/100000: episode: 988, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 205.003, mean reward: 2.050 [1.462, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.634, 10.433], loss: 1.345825, mae: 0.444722, mean_q: 4.187701
 62659/100000: episode: 989, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 192.140, mean reward: 1.921 [1.453, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.522, 10.106], loss: 0.229139, mae: 0.357316, mean_q: 4.178199
 62759/100000: episode: 990, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.718, mean reward: 1.937 [1.457, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.831, 10.098], loss: 0.234456, mae: 0.353730, mean_q: 4.154069
 62859/100000: episode: 991, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 213.403, mean reward: 2.134 [1.460, 5.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.590, 10.098], loss: 0.138818, mae: 0.335460, mean_q: 4.136088
 62959/100000: episode: 992, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 208.403, mean reward: 2.084 [1.446, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.425, 10.392], loss: 1.268084, mae: 0.434975, mean_q: 4.134984
 63059/100000: episode: 993, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 183.879, mean reward: 1.839 [1.450, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.117, 10.240], loss: 0.185184, mae: 0.357734, mean_q: 4.114626
 63159/100000: episode: 994, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 196.943, mean reward: 1.969 [1.459, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.983, 10.151], loss: 1.427968, mae: 0.491838, mean_q: 4.105521
 63259/100000: episode: 995, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 196.777, mean reward: 1.968 [1.459, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.659, 10.184], loss: 0.119883, mae: 0.331301, mean_q: 4.031130
 63359/100000: episode: 996, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.646, mean reward: 1.836 [1.446, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.059, 10.098], loss: 0.114681, mae: 0.315708, mean_q: 3.970588
 63459/100000: episode: 997, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 193.695, mean reward: 1.937 [1.465, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.201, 10.286], loss: 0.090515, mae: 0.295824, mean_q: 3.965675
 63559/100000: episode: 998, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.259, mean reward: 1.953 [1.433, 3.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.444, 10.201], loss: 0.090766, mae: 0.292079, mean_q: 3.959175
 63659/100000: episode: 999, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 181.064, mean reward: 1.811 [1.450, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.706, 10.290], loss: 0.098974, mae: 0.300950, mean_q: 3.965685
 63759/100000: episode: 1000, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 191.157, mean reward: 1.912 [1.521, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.438, 10.206], loss: 0.094893, mae: 0.292864, mean_q: 3.924680
 63859/100000: episode: 1001, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 191.436, mean reward: 1.914 [1.480, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.722, 10.331], loss: 0.089292, mae: 0.295023, mean_q: 3.919883
 63959/100000: episode: 1002, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 190.174, mean reward: 1.902 [1.461, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.493, 10.098], loss: 0.083409, mae: 0.280940, mean_q: 3.883290
 64059/100000: episode: 1003, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.248, mean reward: 1.882 [1.462, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.748, 10.326], loss: 0.075186, mae: 0.271229, mean_q: 3.870058
 64159/100000: episode: 1004, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 213.324, mean reward: 2.133 [1.471, 3.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.438, 10.388], loss: 0.080868, mae: 0.283435, mean_q: 3.874575
 64259/100000: episode: 1005, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 204.938, mean reward: 2.049 [1.458, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.958, 10.192], loss: 0.079990, mae: 0.283369, mean_q: 3.879720
 64359/100000: episode: 1006, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.776, mean reward: 1.898 [1.440, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.050, 10.125], loss: 0.079625, mae: 0.275555, mean_q: 3.852577
 64459/100000: episode: 1007, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 199.458, mean reward: 1.995 [1.438, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.952, 10.098], loss: 0.078828, mae: 0.273626, mean_q: 3.851840
 64559/100000: episode: 1008, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 193.878, mean reward: 1.939 [1.455, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.583, 10.098], loss: 0.083711, mae: 0.280686, mean_q: 3.852695
 64659/100000: episode: 1009, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 200.537, mean reward: 2.005 [1.455, 4.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.702, 10.098], loss: 0.078641, mae: 0.278508, mean_q: 3.867637
 64759/100000: episode: 1010, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 184.861, mean reward: 1.849 [1.467, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.034, 10.098], loss: 0.075031, mae: 0.275250, mean_q: 3.869604
 64859/100000: episode: 1011, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 181.909, mean reward: 1.819 [1.451, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.452, 10.098], loss: 0.073530, mae: 0.271851, mean_q: 3.845635
 64959/100000: episode: 1012, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 202.242, mean reward: 2.022 [1.473, 10.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.479, 10.098], loss: 0.083754, mae: 0.289867, mean_q: 3.882264
 65059/100000: episode: 1013, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 206.624, mean reward: 2.066 [1.488, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.634, 10.418], loss: 0.083713, mae: 0.288937, mean_q: 3.879056
 65159/100000: episode: 1014, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.435, mean reward: 1.844 [1.467, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.262, 10.136], loss: 0.090582, mae: 0.286884, mean_q: 3.879458
 65259/100000: episode: 1015, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.126, mean reward: 1.881 [1.454, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.613, 10.311], loss: 0.083246, mae: 0.284946, mean_q: 3.886036
 65359/100000: episode: 1016, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 186.678, mean reward: 1.867 [1.430, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.923, 10.117], loss: 0.096542, mae: 0.302893, mean_q: 3.893619
 65459/100000: episode: 1017, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 193.013, mean reward: 1.930 [1.461, 3.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.938, 10.375], loss: 0.100168, mae: 0.291596, mean_q: 3.901048
 65559/100000: episode: 1018, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 193.324, mean reward: 1.933 [1.474, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.018, 10.098], loss: 0.108321, mae: 0.298476, mean_q: 3.893879
 65659/100000: episode: 1019, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 201.291, mean reward: 2.013 [1.484, 5.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.687, 10.098], loss: 0.087644, mae: 0.291352, mean_q: 3.891197
 65759/100000: episode: 1020, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 185.871, mean reward: 1.859 [1.456, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.311, 10.245], loss: 0.086613, mae: 0.289226, mean_q: 3.878391
 65859/100000: episode: 1021, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.315, mean reward: 1.983 [1.443, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.719, 10.313], loss: 0.104739, mae: 0.291311, mean_q: 3.854917
 65959/100000: episode: 1022, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.317, mean reward: 1.883 [1.473, 4.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.480, 10.098], loss: 0.124719, mae: 0.304304, mean_q: 3.882422
 66059/100000: episode: 1023, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 185.479, mean reward: 1.855 [1.454, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.288, 10.098], loss: 0.084507, mae: 0.286089, mean_q: 3.853222
 66159/100000: episode: 1024, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 242.464, mean reward: 2.425 [1.496, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.761, 10.566], loss: 0.102762, mae: 0.299693, mean_q: 3.879892
 66259/100000: episode: 1025, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 202.261, mean reward: 2.023 [1.437, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.664, 10.303], loss: 0.085238, mae: 0.289208, mean_q: 3.883392
 66359/100000: episode: 1026, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 182.202, mean reward: 1.822 [1.442, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.729, 10.162], loss: 0.083552, mae: 0.290342, mean_q: 3.870439
 66459/100000: episode: 1027, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 225.597, mean reward: 2.256 [1.479, 5.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.683, 10.255], loss: 0.107605, mae: 0.292625, mean_q: 3.866029
 66559/100000: episode: 1028, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 179.080, mean reward: 1.791 [1.465, 2.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.308, 10.098], loss: 0.090133, mae: 0.285680, mean_q: 3.872919
 66659/100000: episode: 1029, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.445, mean reward: 1.934 [1.437, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.865, 10.098], loss: 0.092027, mae: 0.294276, mean_q: 3.894008
 66759/100000: episode: 1030, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 211.339, mean reward: 2.113 [1.450, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.704, 10.098], loss: 0.098676, mae: 0.289044, mean_q: 3.881077
 66859/100000: episode: 1031, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 214.100, mean reward: 2.141 [1.455, 4.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.024, 10.098], loss: 0.092882, mae: 0.289214, mean_q: 3.900283
 66959/100000: episode: 1032, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.816, mean reward: 1.918 [1.460, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.468, 10.244], loss: 0.095833, mae: 0.287351, mean_q: 3.911655
 67059/100000: episode: 1033, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 197.318, mean reward: 1.973 [1.455, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.124, 10.098], loss: 0.106449, mae: 0.292180, mean_q: 3.876848
 67159/100000: episode: 1034, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 203.335, mean reward: 2.033 [1.482, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.081, 10.098], loss: 0.087034, mae: 0.292498, mean_q: 3.913399
 67259/100000: episode: 1035, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 186.779, mean reward: 1.868 [1.466, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.574, 10.120], loss: 0.105692, mae: 0.287365, mean_q: 3.887300
 67359/100000: episode: 1036, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.467, mean reward: 1.955 [1.463, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.060, 10.098], loss: 0.072167, mae: 0.270871, mean_q: 3.880282
 67459/100000: episode: 1037, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 201.315, mean reward: 2.013 [1.465, 5.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.671, 10.171], loss: 0.095864, mae: 0.292080, mean_q: 3.872885
 67559/100000: episode: 1038, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 200.976, mean reward: 2.010 [1.470, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.966, 10.354], loss: 0.100784, mae: 0.287733, mean_q: 3.862210
 67659/100000: episode: 1039, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.943, mean reward: 1.839 [1.465, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.322, 10.154], loss: 0.109253, mae: 0.285896, mean_q: 3.864650
 67759/100000: episode: 1040, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 184.437, mean reward: 1.844 [1.434, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.475, 10.164], loss: 0.093031, mae: 0.292983, mean_q: 3.911200
 67859/100000: episode: 1041, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 186.268, mean reward: 1.863 [1.435, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.676, 10.202], loss: 0.082544, mae: 0.287740, mean_q: 3.850549
 67959/100000: episode: 1042, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 186.513, mean reward: 1.865 [1.505, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.859, 10.164], loss: 0.079998, mae: 0.281643, mean_q: 3.851394
 68059/100000: episode: 1043, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 186.698, mean reward: 1.867 [1.451, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.912, 10.117], loss: 0.092985, mae: 0.288000, mean_q: 3.876005
 68159/100000: episode: 1044, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 176.600, mean reward: 1.766 [1.450, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.831, 10.106], loss: 0.088031, mae: 0.288114, mean_q: 3.852176
 68259/100000: episode: 1045, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.662, mean reward: 1.937 [1.456, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.385, 10.193], loss: 0.107184, mae: 0.279795, mean_q: 3.844709
 68359/100000: episode: 1046, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 188.465, mean reward: 1.885 [1.441, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.175, 10.098], loss: 0.086903, mae: 0.278504, mean_q: 3.861840
 68459/100000: episode: 1047, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.992, mean reward: 1.970 [1.473, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.062, 10.153], loss: 0.085683, mae: 0.278444, mean_q: 3.834755
 68559/100000: episode: 1048, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 219.180, mean reward: 2.192 [1.483, 5.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.413, 10.374], loss: 0.094590, mae: 0.287606, mean_q: 3.842428
 68659/100000: episode: 1049, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 185.001, mean reward: 1.850 [1.460, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.667, 10.098], loss: 0.095012, mae: 0.287053, mean_q: 3.861602
 68759/100000: episode: 1050, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 212.214, mean reward: 2.122 [1.486, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.306, 10.340], loss: 0.081016, mae: 0.287555, mean_q: 3.856970
[Info] 1-TH LEVEL FOUND: 5.863770484924316, Considering 10/90 traces
 68859/100000: episode: 1051, duration: 4.646s, episode steps: 100, steps per second: 22, episode reward: 187.563, mean reward: 1.876 [1.498, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.897, 10.098], loss: 0.102694, mae: 0.288787, mean_q: 3.849980
 68895/100000: episode: 1052, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 82.094, mean reward: 2.280 [1.660, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.586, 10.261], loss: 0.086408, mae: 0.290381, mean_q: 3.895720
 68914/100000: episode: 1053, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 55.899, mean reward: 2.942 [2.154, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.396], loss: 0.112189, mae: 0.314823, mean_q: 3.892105
 68927/100000: episode: 1054, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 37.274, mean reward: 2.867 [2.093, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.487], loss: 0.078625, mae: 0.294788, mean_q: 3.821770
 68965/100000: episode: 1055, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 103.231, mean reward: 2.717 [1.802, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-1.019, 10.576], loss: 0.081019, mae: 0.288020, mean_q: 3.874079
 68999/100000: episode: 1056, duration: 0.204s, episode steps: 34, steps per second: 166, episode reward: 70.781, mean reward: 2.082 [1.505, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.013, 10.100], loss: 0.095309, mae: 0.294989, mean_q: 3.902318
 69012/100000: episode: 1057, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 35.509, mean reward: 2.731 [1.921, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.479], loss: 0.082977, mae: 0.294309, mean_q: 3.970278
 69049/100000: episode: 1058, duration: 0.219s, episode steps: 37, steps per second: 169, episode reward: 130.997, mean reward: 3.540 [1.947, 8.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.035, 10.414], loss: 0.086838, mae: 0.295025, mean_q: 3.887519
 69085/100000: episode: 1059, duration: 0.178s, episode steps: 36, steps per second: 203, episode reward: 104.288, mean reward: 2.897 [1.894, 7.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.945, 10.386], loss: 0.070965, mae: 0.271588, mean_q: 3.889794
 69123/100000: episode: 1060, duration: 0.204s, episode steps: 38, steps per second: 187, episode reward: 190.466, mean reward: 5.012 [1.802, 22.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.040, 10.690], loss: 0.104591, mae: 0.304472, mean_q: 3.890782
 69136/100000: episode: 1061, duration: 0.072s, episode steps: 13, steps per second: 182, episode reward: 30.490, mean reward: 2.345 [2.042, 2.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.089, 10.333], loss: 0.129834, mae: 0.326506, mean_q: 4.050820
 69170/100000: episode: 1062, duration: 0.166s, episode steps: 34, steps per second: 205, episode reward: 105.835, mean reward: 3.113 [1.794, 7.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.357, 10.434], loss: 0.103525, mae: 0.300137, mean_q: 3.927900
 69199/100000: episode: 1063, duration: 0.156s, episode steps: 29, steps per second: 185, episode reward: 59.874, mean reward: 2.065 [1.454, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.427, 10.141], loss: 0.121239, mae: 0.320033, mean_q: 3.943071
 69273/100000: episode: 1064, duration: 0.398s, episode steps: 74, steps per second: 186, episode reward: 151.930, mean reward: 2.053 [1.483, 5.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-0.651, 10.116], loss: 0.201845, mae: 0.338388, mean_q: 3.946705
 69302/100000: episode: 1065, duration: 0.160s, episode steps: 29, steps per second: 182, episode reward: 97.627, mean reward: 3.366 [2.255, 7.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.303, 10.458], loss: 0.091419, mae: 0.295408, mean_q: 3.989832
 69338/100000: episode: 1066, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 96.361, mean reward: 2.677 [1.571, 5.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.796, 10.100], loss: 0.122404, mae: 0.326518, mean_q: 3.992213
 69357/100000: episode: 1067, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 51.635, mean reward: 2.718 [2.140, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.741, 10.329], loss: 0.172867, mae: 0.334774, mean_q: 4.001051
 69394/100000: episode: 1068, duration: 0.225s, episode steps: 37, steps per second: 164, episode reward: 113.645, mean reward: 3.071 [1.990, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.305, 10.529], loss: 0.098924, mae: 0.305164, mean_q: 3.966882
 69407/100000: episode: 1069, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 30.349, mean reward: 2.335 [1.711, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.603, 10.478], loss: 0.172089, mae: 0.326335, mean_q: 3.990788
 69444/100000: episode: 1070, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 83.985, mean reward: 2.270 [1.690, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.035, 10.315], loss: 0.317649, mae: 0.381721, mean_q: 4.042942
 69481/100000: episode: 1071, duration: 0.187s, episode steps: 37, steps per second: 197, episode reward: 78.233, mean reward: 2.114 [1.536, 4.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.035, 10.273], loss: 0.501427, mae: 0.403643, mean_q: 4.044466
 69517/100000: episode: 1072, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 100.835, mean reward: 2.801 [1.907, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.386, 10.338], loss: 0.124141, mae: 0.325615, mean_q: 3.999742
 69551/100000: episode: 1073, duration: 0.181s, episode steps: 34, steps per second: 187, episode reward: 86.202, mean reward: 2.535 [1.744, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.077, 10.264], loss: 0.112342, mae: 0.315379, mean_q: 4.006864
 69570/100000: episode: 1074, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 40.782, mean reward: 2.146 [1.498, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.655, 10.145], loss: 0.126146, mae: 0.315975, mean_q: 4.025905
 69599/100000: episode: 1075, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 64.949, mean reward: 2.240 [1.779, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.716, 10.244], loss: 0.122069, mae: 0.316031, mean_q: 4.055106
 69612/100000: episode: 1076, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 31.071, mean reward: 2.390 [1.985, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.302], loss: 0.681985, mae: 0.453550, mean_q: 4.125369
 69631/100000: episode: 1077, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 53.564, mean reward: 2.819 [2.340, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.722, 10.345], loss: 0.155047, mae: 0.380119, mean_q: 4.065649
 69665/100000: episode: 1078, duration: 0.166s, episode steps: 34, steps per second: 205, episode reward: 71.009, mean reward: 2.088 [1.510, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.286, 10.100], loss: 0.130399, mae: 0.339657, mean_q: 4.057063
 69699/100000: episode: 1079, duration: 0.166s, episode steps: 34, steps per second: 204, episode reward: 87.568, mean reward: 2.576 [1.884, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.397], loss: 0.158181, mae: 0.339633, mean_q: 4.061526
 69735/100000: episode: 1080, duration: 0.177s, episode steps: 36, steps per second: 204, episode reward: 120.982, mean reward: 3.361 [2.426, 6.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.624, 10.372], loss: 0.145912, mae: 0.303885, mean_q: 4.084063
 69748/100000: episode: 1081, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 31.242, mean reward: 2.403 [2.050, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-1.442, 10.387], loss: 0.104848, mae: 0.305888, mean_q: 4.085988
 69782/100000: episode: 1082, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 85.397, mean reward: 2.512 [1.496, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.200, 10.283], loss: 0.116397, mae: 0.324165, mean_q: 4.078188
 69820/100000: episode: 1083, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 92.512, mean reward: 2.435 [1.579, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.059, 10.474], loss: 0.150038, mae: 0.333258, mean_q: 4.097966
 69854/100000: episode: 1084, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 91.172, mean reward: 2.682 [1.857, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.278, 10.404], loss: 0.375096, mae: 0.384517, mean_q: 4.078182
 69873/100000: episode: 1085, duration: 0.092s, episode steps: 19, steps per second: 206, episode reward: 70.169, mean reward: 3.693 [2.434, 6.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-1.073, 10.443], loss: 0.194108, mae: 0.410612, mean_q: 4.172506
 69892/100000: episode: 1086, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 46.752, mean reward: 2.461 [1.632, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.310], loss: 0.184522, mae: 0.377469, mean_q: 4.200453
 69930/100000: episode: 1087, duration: 0.186s, episode steps: 38, steps per second: 204, episode reward: 97.605, mean reward: 2.569 [1.661, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.819, 10.425], loss: 0.300754, mae: 0.371854, mean_q: 4.107295
 69949/100000: episode: 1088, duration: 0.108s, episode steps: 19, steps per second: 177, episode reward: 49.137, mean reward: 2.586 [1.964, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.142, 10.304], loss: 0.124894, mae: 0.364304, mean_q: 4.145325
 69985/100000: episode: 1089, duration: 0.209s, episode steps: 36, steps per second: 173, episode reward: 92.659, mean reward: 2.574 [1.822, 3.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.598, 10.367], loss: 0.112303, mae: 0.324839, mean_q: 4.144842
 70004/100000: episode: 1090, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 45.970, mean reward: 2.419 [1.962, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.032, 10.308], loss: 0.121439, mae: 0.325742, mean_q: 4.143505
 70042/100000: episode: 1091, duration: 0.185s, episode steps: 38, steps per second: 205, episode reward: 164.120, mean reward: 4.319 [1.793, 37.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.348, 10.474], loss: 0.163514, mae: 0.346740, mean_q: 4.192868
 70061/100000: episode: 1092, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 43.088, mean reward: 2.268 [1.566, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.241], loss: 0.092301, mae: 0.304741, mean_q: 4.164161
 70098/100000: episode: 1093, duration: 0.191s, episode steps: 37, steps per second: 194, episode reward: 85.673, mean reward: 2.315 [1.821, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.035, 10.332], loss: 0.143702, mae: 0.345594, mean_q: 4.131086
 70132/100000: episode: 1094, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 69.816, mean reward: 2.053 [1.646, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.738, 10.279], loss: 0.223310, mae: 0.379057, mean_q: 4.228625
 70166/100000: episode: 1095, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 114.612, mean reward: 3.371 [2.112, 8.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.950, 10.441], loss: 0.227634, mae: 0.357286, mean_q: 4.232844
 70240/100000: episode: 1096, duration: 0.365s, episode steps: 74, steps per second: 203, episode reward: 172.229, mean reward: 2.327 [1.543, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.704 [-0.296, 10.100], loss: 0.412650, mae: 0.398902, mean_q: 4.249375
 70274/100000: episode: 1097, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 77.038, mean reward: 2.266 [1.688, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.064, 10.451], loss: 0.648516, mae: 0.384779, mean_q: 4.213185
 70312/100000: episode: 1098, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 88.862, mean reward: 2.338 [1.704, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.057, 10.412], loss: 0.205056, mae: 0.425065, mean_q: 4.237471
 70350/100000: episode: 1099, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 125.377, mean reward: 3.299 [2.051, 4.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.368, 10.536], loss: 0.609937, mae: 0.376142, mean_q: 4.220095
 70363/100000: episode: 1100, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 43.277, mean reward: 3.329 [2.115, 4.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.968, 10.546], loss: 0.209903, mae: 0.414794, mean_q: 4.221292
 70401/100000: episode: 1101, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 111.878, mean reward: 2.944 [2.308, 4.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.054, 10.387], loss: 1.038269, mae: 0.416544, mean_q: 4.298008
 70435/100000: episode: 1102, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 82.487, mean reward: 2.426 [1.679, 4.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.798, 10.284], loss: 0.298818, mae: 0.527349, mean_q: 4.365877
 70464/100000: episode: 1103, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 86.416, mean reward: 2.980 [2.025, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.860, 10.484], loss: 0.385375, mae: 0.417995, mean_q: 4.327640
 70477/100000: episode: 1104, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 33.343, mean reward: 2.565 [2.142, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.437, 10.376], loss: 0.223576, mae: 0.457994, mean_q: 4.455610
 70515/100000: episode: 1105, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 119.199, mean reward: 3.137 [2.112, 6.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.381, 10.403], loss: 0.960477, mae: 0.472966, mean_q: 4.394624
 70544/100000: episode: 1106, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 61.053, mean reward: 2.105 [1.700, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.111, 10.225], loss: 0.154805, mae: 0.365763, mean_q: 4.180482
 70578/100000: episode: 1107, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 65.107, mean reward: 1.915 [1.446, 2.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.509, 10.241], loss: 0.249868, mae: 0.390539, mean_q: 4.325467
 70591/100000: episode: 1108, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 39.937, mean reward: 3.072 [2.478, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.521, 10.471], loss: 0.137867, mae: 0.357602, mean_q: 4.386260
 70629/100000: episode: 1109, duration: 0.213s, episode steps: 38, steps per second: 179, episode reward: 151.928, mean reward: 3.998 [2.286, 9.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.244, 10.450], loss: 0.265185, mae: 0.407380, mean_q: 4.407276
 70666/100000: episode: 1110, duration: 0.203s, episode steps: 37, steps per second: 182, episode reward: 73.152, mean reward: 1.977 [1.545, 2.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.285, 10.100], loss: 0.257112, mae: 0.404544, mean_q: 4.391965
 70702/100000: episode: 1111, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 91.822, mean reward: 2.551 [1.946, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.109, 10.413], loss: 0.831441, mae: 0.444730, mean_q: 4.400732
 70715/100000: episode: 1112, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 35.463, mean reward: 2.728 [2.227, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.405], loss: 0.217040, mae: 0.431828, mean_q: 4.479462
 70734/100000: episode: 1113, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 53.462, mean reward: 2.814 [2.025, 3.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.203, 10.363], loss: 0.132767, mae: 0.354499, mean_q: 4.355239
 70753/100000: episode: 1114, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 55.621, mean reward: 2.927 [1.929, 6.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-1.053, 10.298], loss: 0.371530, mae: 0.445949, mean_q: 4.450089
 70766/100000: episode: 1115, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 30.106, mean reward: 2.316 [1.798, 2.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.324], loss: 0.272354, mae: 0.388224, mean_q: 4.470679
 70800/100000: episode: 1116, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 70.513, mean reward: 2.074 [1.713, 2.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.706, 10.335], loss: 0.860510, mae: 0.416471, mean_q: 4.383712
 70829/100000: episode: 1117, duration: 0.169s, episode steps: 29, steps per second: 172, episode reward: 64.985, mean reward: 2.241 [1.642, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.778, 10.246], loss: 0.467843, mae: 0.464457, mean_q: 4.433110
 70903/100000: episode: 1118, duration: 0.390s, episode steps: 74, steps per second: 190, episode reward: 154.941, mean reward: 2.094 [1.449, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-1.010, 10.100], loss: 0.404403, mae: 0.395059, mean_q: 4.435852
 70922/100000: episode: 1119, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 51.138, mean reward: 2.691 [2.196, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.566, 10.419], loss: 0.288372, mae: 0.370983, mean_q: 4.392803
 70960/100000: episode: 1120, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 75.051, mean reward: 1.975 [1.533, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.092, 10.295], loss: 0.327014, mae: 0.397086, mean_q: 4.428617
 70973/100000: episode: 1121, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 35.730, mean reward: 2.748 [2.167, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.426], loss: 0.129760, mae: 0.365988, mean_q: 4.370968
 71009/100000: episode: 1122, duration: 0.178s, episode steps: 36, steps per second: 203, episode reward: 81.914, mean reward: 2.275 [1.515, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.107, 10.128], loss: 0.165622, mae: 0.384158, mean_q: 4.410879
 71046/100000: episode: 1123, duration: 0.178s, episode steps: 37, steps per second: 208, episode reward: 117.226, mean reward: 3.168 [1.955, 4.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.320, 10.494], loss: 0.137568, mae: 0.360856, mean_q: 4.325282
 71084/100000: episode: 1124, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 84.366, mean reward: 2.220 [1.639, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.182, 10.296], loss: 0.243595, mae: 0.388135, mean_q: 4.391186
 71118/100000: episode: 1125, duration: 0.176s, episode steps: 34, steps per second: 194, episode reward: 77.494, mean reward: 2.279 [1.474, 5.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.274, 10.160], loss: 0.221159, mae: 0.380552, mean_q: 4.374892
 71152/100000: episode: 1126, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 74.789, mean reward: 2.200 [1.640, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.947, 10.134], loss: 0.677083, mae: 0.420593, mean_q: 4.460121
 71190/100000: episode: 1127, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 111.441, mean reward: 2.933 [2.130, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.069, 10.320], loss: 0.345771, mae: 0.435101, mean_q: 4.425868
 71264/100000: episode: 1128, duration: 0.385s, episode steps: 74, steps per second: 192, episode reward: 155.469, mean reward: 2.101 [1.524, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.700 [-0.213, 10.100], loss: 0.815685, mae: 0.452988, mean_q: 4.458220
 71338/100000: episode: 1129, duration: 0.383s, episode steps: 74, steps per second: 193, episode reward: 204.669, mean reward: 2.766 [1.899, 5.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-0.572, 10.100], loss: 0.680296, mae: 0.455115, mean_q: 4.482583
 71372/100000: episode: 1130, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 74.636, mean reward: 2.195 [1.508, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.035, 10.230], loss: 0.355180, mae: 0.414382, mean_q: 4.476873
 71410/100000: episode: 1131, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 111.064, mean reward: 2.923 [1.951, 4.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.773, 10.416], loss: 0.277561, mae: 0.410390, mean_q: 4.468737
 71448/100000: episode: 1132, duration: 0.207s, episode steps: 38, steps per second: 183, episode reward: 124.757, mean reward: 3.283 [1.683, 7.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.522, 10.174], loss: 0.194180, mae: 0.398877, mean_q: 4.507962
 71486/100000: episode: 1133, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 89.442, mean reward: 2.354 [1.777, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.620, 10.300], loss: 0.159836, mae: 0.379698, mean_q: 4.496691
 71524/100000: episode: 1134, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 188.811, mean reward: 4.969 [2.016, 12.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.201, 10.286], loss: 0.742763, mae: 0.465747, mean_q: 4.573888
 71561/100000: episode: 1135, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 76.089, mean reward: 2.056 [1.548, 2.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.265, 10.165], loss: 0.842803, mae: 0.492686, mean_q: 4.590481
 71599/100000: episode: 1136, duration: 0.223s, episode steps: 38, steps per second: 171, episode reward: 85.131, mean reward: 2.240 [1.454, 4.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-1.424, 10.137], loss: 0.314064, mae: 0.438252, mean_q: 4.577307
 71637/100000: episode: 1137, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 85.395, mean reward: 2.247 [1.565, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.253, 10.110], loss: 0.244952, mae: 0.430114, mean_q: 4.613110
 71711/100000: episode: 1138, duration: 0.396s, episode steps: 74, steps per second: 187, episode reward: 141.203, mean reward: 1.908 [1.440, 2.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.700 [-0.970, 10.100], loss: 0.476043, mae: 0.450839, mean_q: 4.555692
 71785/100000: episode: 1139, duration: 0.374s, episode steps: 74, steps per second: 198, episode reward: 180.814, mean reward: 2.443 [1.697, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.695 [-0.988, 10.100], loss: 0.264191, mae: 0.417055, mean_q: 4.529912
 71859/100000: episode: 1140, duration: 0.395s, episode steps: 74, steps per second: 187, episode reward: 151.395, mean reward: 2.046 [1.584, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.697 [-1.253, 10.100], loss: 0.212081, mae: 0.389097, mean_q: 4.539551
[Info] 2-TH LEVEL FOUND: 7.795626163482666, Considering 10/90 traces
 71895/100000: episode: 1141, duration: 4.347s, episode steps: 36, steps per second: 8, episode reward: 110.240, mean reward: 3.062 [2.091, 5.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.332, 10.353], loss: 0.754694, mae: 0.448109, mean_q: 4.634177
 71920/100000: episode: 1142, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 68.517, mean reward: 2.741 [1.836, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.351, 10.244], loss: 0.363589, mae: 0.450034, mean_q: 4.550931
 71951/100000: episode: 1143, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 109.392, mean reward: 3.529 [2.300, 9.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.459, 10.487], loss: 0.233435, mae: 0.419868, mean_q: 4.671302
 71979/100000: episode: 1144, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 76.843, mean reward: 2.744 [1.640, 5.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.176, 10.312], loss: 0.227748, mae: 0.389202, mean_q: 4.579651
 72010/100000: episode: 1145, duration: 0.160s, episode steps: 31, steps per second: 193, episode reward: 100.147, mean reward: 3.231 [1.959, 4.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.026, 10.385], loss: 0.756344, mae: 0.454569, mean_q: 4.651033
 72042/100000: episode: 1146, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 121.684, mean reward: 3.803 [2.483, 7.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.314, 10.534], loss: 0.259293, mae: 0.464707, mean_q: 4.609045
 72074/100000: episode: 1147, duration: 0.181s, episode steps: 32, steps per second: 176, episode reward: 148.791, mean reward: 4.650 [2.694, 23.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.885, 10.484], loss: 0.423906, mae: 0.459556, mean_q: 4.662742
 72099/100000: episode: 1148, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 58.657, mean reward: 2.346 [1.938, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.323], loss: 0.496767, mae: 0.481637, mean_q: 4.810315
 72124/100000: episode: 1149, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 75.425, mean reward: 3.017 [1.967, 4.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.824, 10.409], loss: 0.910006, mae: 0.462371, mean_q: 4.660698
 72155/100000: episode: 1150, duration: 0.164s, episode steps: 31, steps per second: 188, episode reward: 94.986, mean reward: 3.064 [1.646, 6.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.477, 10.320], loss: 0.832926, mae: 0.514225, mean_q: 4.714152
 72180/100000: episode: 1151, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 119.772, mean reward: 4.791 [2.758, 9.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.520, 10.534], loss: 0.249021, mae: 0.427165, mean_q: 4.653869
 72211/100000: episode: 1152, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 88.572, mean reward: 2.857 [2.271, 5.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.104, 10.482], loss: 0.797512, mae: 0.489261, mean_q: 4.838581
 72243/100000: episode: 1153, duration: 0.164s, episode steps: 32, steps per second: 196, episode reward: 98.347, mean reward: 3.073 [1.720, 4.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.523, 10.209], loss: 0.407820, mae: 0.471491, mean_q: 4.747033
 72274/100000: episode: 1154, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 87.787, mean reward: 2.832 [1.675, 5.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.935, 10.295], loss: 0.212707, mae: 0.421881, mean_q: 4.754884
 72306/100000: episode: 1155, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 73.991, mean reward: 2.312 [1.490, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.193, 10.195], loss: 0.961465, mae: 0.528066, mean_q: 4.859080
 72331/100000: episode: 1156, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 68.388, mean reward: 2.736 [2.177, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.393], loss: 0.210690, mae: 0.435244, mean_q: 4.744512
 72355/100000: episode: 1157, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 79.170, mean reward: 3.299 [2.094, 5.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.056, 10.347], loss: 0.403311, mae: 0.474636, mean_q: 4.823966
 72379/100000: episode: 1158, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 68.923, mean reward: 2.872 [2.109, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.338], loss: 0.531993, mae: 0.496700, mean_q: 4.802752
 72410/100000: episode: 1159, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 130.947, mean reward: 4.224 [1.911, 27.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.725, 10.300], loss: 0.893399, mae: 0.550955, mean_q: 4.793985
 72434/100000: episode: 1160, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 71.740, mean reward: 2.989 [2.270, 6.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.884, 10.442], loss: 0.236018, mae: 0.446413, mean_q: 4.698422
 72464/100000: episode: 1161, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 95.735, mean reward: 3.191 [2.335, 5.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.073, 10.321], loss: 1.059155, mae: 0.574381, mean_q: 4.883719
 72489/100000: episode: 1162, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 77.573, mean reward: 3.103 [1.992, 5.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.708, 10.390], loss: 0.945925, mae: 0.532403, mean_q: 4.971848
 72514/100000: episode: 1163, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 122.443, mean reward: 4.898 [2.779, 8.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.718, 10.608], loss: 0.554506, mae: 0.515332, mean_q: 4.954457
 72539/100000: episode: 1164, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 72.517, mean reward: 2.901 [2.175, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.071, 10.417], loss: 0.517494, mae: 0.511996, mean_q: 5.006235
 72570/100000: episode: 1165, duration: 0.181s, episode steps: 31, steps per second: 172, episode reward: 192.506, mean reward: 6.210 [2.603, 17.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.589, 10.535], loss: 0.267973, mae: 0.458585, mean_q: 4.819877
 72595/100000: episode: 1166, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 69.988, mean reward: 2.800 [1.965, 6.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.435, 10.265], loss: 0.690005, mae: 0.540735, mean_q: 4.991039
 72627/100000: episode: 1167, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 78.645, mean reward: 2.458 [1.769, 4.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.235, 10.282], loss: 0.705208, mae: 0.557199, mean_q: 5.038226
 72652/100000: episode: 1168, duration: 0.143s, episode steps: 25, steps per second: 174, episode reward: 63.817, mean reward: 2.553 [1.800, 6.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.646, 10.295], loss: 0.311340, mae: 0.508428, mean_q: 4.958232
 72680/100000: episode: 1169, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 101.108, mean reward: 3.611 [2.637, 6.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.801, 10.511], loss: 0.286807, mae: 0.503945, mean_q: 5.051755
 72704/100000: episode: 1170, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 76.357, mean reward: 3.182 [2.324, 4.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.980, 10.435], loss: 0.291452, mae: 0.492745, mean_q: 4.964396
 72729/100000: episode: 1171, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 69.460, mean reward: 2.778 [1.948, 4.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.390], loss: 0.787228, mae: 0.575191, mean_q: 5.024050
 72758/100000: episode: 1172, duration: 0.158s, episode steps: 29, steps per second: 184, episode reward: 103.350, mean reward: 3.564 [2.594, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.367, 10.494], loss: 0.910227, mae: 0.584429, mean_q: 4.944674
 72783/100000: episode: 1173, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 85.057, mean reward: 3.402 [2.385, 5.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.561, 10.472], loss: 0.333934, mae: 0.530299, mean_q: 4.981828
 72808/100000: episode: 1174, duration: 0.157s, episode steps: 25, steps per second: 159, episode reward: 70.054, mean reward: 2.802 [2.231, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.176, 10.435], loss: 0.387165, mae: 0.480601, mean_q: 4.908563
 72840/100000: episode: 1175, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 97.969, mean reward: 3.062 [2.338, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.728, 10.432], loss: 0.536326, mae: 0.553561, mean_q: 5.108724
 72869/100000: episode: 1176, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 88.032, mean reward: 3.036 [2.358, 4.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.422, 10.481], loss: 0.266958, mae: 0.473411, mean_q: 5.059167
 72898/100000: episode: 1177, duration: 0.172s, episode steps: 29, steps per second: 169, episode reward: 133.236, mean reward: 4.594 [2.045, 12.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.139, 10.496], loss: 0.665322, mae: 0.557863, mean_q: 5.151346
 72923/100000: episode: 1178, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 83.314, mean reward: 3.333 [2.129, 9.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.867, 10.462], loss: 1.133157, mae: 0.639243, mean_q: 5.229700
 72951/100000: episode: 1179, duration: 0.140s, episode steps: 28, steps per second: 201, episode reward: 84.847, mean reward: 3.030 [1.990, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.296], loss: 0.681306, mae: 0.584206, mean_q: 5.155540
 72976/100000: episode: 1180, duration: 0.133s, episode steps: 25, steps per second: 189, episode reward: 67.642, mean reward: 2.706 [2.079, 4.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.617, 10.352], loss: 0.301451, mae: 0.478724, mean_q: 5.056069
 73005/100000: episode: 1181, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 87.464, mean reward: 3.016 [1.911, 4.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.339, 10.365], loss: 1.637853, mae: 0.642167, mean_q: 5.291613
 73036/100000: episode: 1182, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 180.766, mean reward: 5.831 [1.832, 21.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.035, 10.332], loss: 0.338349, mae: 0.532637, mean_q: 5.094927
 73067/100000: episode: 1183, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 113.861, mean reward: 3.673 [1.830, 6.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.035, 10.277], loss: 0.827815, mae: 0.597113, mean_q: 5.240378
 73092/100000: episode: 1184, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 62.431, mean reward: 2.497 [1.617, 4.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.329, 10.217], loss: 0.374358, mae: 0.534838, mean_q: 5.156959
 73123/100000: episode: 1185, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 145.005, mean reward: 4.678 [1.873, 18.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.347, 10.342], loss: 1.198805, mae: 0.692578, mean_q: 5.370800
 73154/100000: episode: 1186, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 393.799, mean reward: 12.703 [3.169, 98.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.504, 10.763], loss: 0.905134, mae: 0.631299, mean_q: 5.329782
 73186/100000: episode: 1187, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 101.941, mean reward: 3.186 [2.263, 6.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.507, 10.410], loss: 0.660573, mae: 0.589372, mean_q: 5.316511
 73211/100000: episode: 1188, duration: 0.120s, episode steps: 25, steps per second: 208, episode reward: 68.189, mean reward: 2.728 [2.208, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.619, 10.463], loss: 2.629152, mae: 0.793225, mean_q: 5.465604
 73243/100000: episode: 1189, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 114.808, mean reward: 3.588 [2.464, 5.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.778, 10.416], loss: 0.342370, mae: 0.586823, mean_q: 5.321811
 73272/100000: episode: 1190, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 88.288, mean reward: 3.044 [1.724, 7.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.252], loss: 0.553745, mae: 0.602751, mean_q: 5.288783
 73303/100000: episode: 1191, duration: 0.184s, episode steps: 31, steps per second: 168, episode reward: 155.027, mean reward: 5.001 [3.318, 8.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.317, 10.654], loss: 0.588802, mae: 0.600870, mean_q: 5.387129
 73328/100000: episode: 1192, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 78.994, mean reward: 3.160 [2.241, 4.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.549, 10.415], loss: 1.279070, mae: 0.700732, mean_q: 5.546731
 73358/100000: episode: 1193, duration: 0.168s, episode steps: 30, steps per second: 179, episode reward: 136.612, mean reward: 4.554 [3.028, 13.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.100, 10.541], loss: 0.903206, mae: 0.679375, mean_q: 5.480876
 73386/100000: episode: 1194, duration: 0.162s, episode steps: 28, steps per second: 172, episode reward: 105.919, mean reward: 3.783 [2.545, 8.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.738, 10.585], loss: 0.782628, mae: 0.606778, mean_q: 5.377674
 73411/100000: episode: 1195, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 68.781, mean reward: 2.751 [1.796, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.583, 10.319], loss: 0.770186, mae: 0.638019, mean_q: 5.518155
 73440/100000: episode: 1196, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 70.010, mean reward: 2.414 [1.571, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.109, 10.116], loss: 0.670563, mae: 0.617841, mean_q: 5.385810
 73464/100000: episode: 1197, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 74.937, mean reward: 3.122 [1.819, 4.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.352, 10.318], loss: 0.980556, mae: 0.722278, mean_q: 5.518177
 73495/100000: episode: 1198, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 105.932, mean reward: 3.417 [2.400, 6.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.136, 10.556], loss: 1.097752, mae: 0.697298, mean_q: 5.534018
 73525/100000: episode: 1199, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 119.086, mean reward: 3.970 [2.321, 7.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.035, 10.384], loss: 0.402776, mae: 0.600701, mean_q: 5.465807
 73549/100000: episode: 1200, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 96.073, mean reward: 4.003 [3.062, 4.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.211, 10.537], loss: 0.597929, mae: 0.615616, mean_q: 5.545660
 73580/100000: episode: 1201, duration: 0.155s, episode steps: 31, steps per second: 201, episode reward: 192.690, mean reward: 6.216 [2.202, 16.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.841, 10.429], loss: 5.041891, mae: 0.839093, mean_q: 5.619683
 73605/100000: episode: 1202, duration: 0.147s, episode steps: 25, steps per second: 171, episode reward: 72.413, mean reward: 2.897 [1.627, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.070, 10.234], loss: 0.876326, mae: 0.699446, mean_q: 5.708718
 73635/100000: episode: 1203, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 135.410, mean reward: 4.514 [3.195, 6.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.502], loss: 0.857912, mae: 0.637032, mean_q: 5.555587
 73659/100000: episode: 1204, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 59.203, mean reward: 2.467 [1.602, 4.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.488, 10.189], loss: 0.554288, mae: 0.637765, mean_q: 5.623302
 73687/100000: episode: 1205, duration: 0.136s, episode steps: 28, steps per second: 206, episode reward: 89.524, mean reward: 3.197 [2.449, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.178, 10.532], loss: 0.808006, mae: 0.593940, mean_q: 5.674409
 73717/100000: episode: 1206, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 168.669, mean reward: 5.622 [3.042, 14.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.307, 10.517], loss: 1.084172, mae: 0.653505, mean_q: 5.604782
 73741/100000: episode: 1207, duration: 0.151s, episode steps: 24, steps per second: 158, episode reward: 87.549, mean reward: 3.648 [2.588, 5.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.460], loss: 0.524189, mae: 0.634952, mean_q: 5.673189
 73773/100000: episode: 1208, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 142.048, mean reward: 4.439 [2.951, 8.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.479, 10.404], loss: 2.065667, mae: 0.728019, mean_q: 5.733031
 73802/100000: episode: 1209, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 87.885, mean reward: 3.031 [2.072, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.321, 10.478], loss: 5.435525, mae: 0.873066, mean_q: 5.836103
 73827/100000: episode: 1210, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 80.787, mean reward: 3.231 [2.303, 5.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.439, 10.397], loss: 6.352458, mae: 0.864210, mean_q: 5.880430
 73855/100000: episode: 1211, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 113.450, mean reward: 4.052 [2.310, 16.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.306, 10.419], loss: 0.512397, mae: 0.645089, mean_q: 5.733485
 73880/100000: episode: 1212, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 66.305, mean reward: 2.652 [1.679, 6.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.273], loss: 0.662253, mae: 0.646087, mean_q: 5.862266
 73904/100000: episode: 1213, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 92.130, mean reward: 3.839 [2.604, 8.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.035, 10.492], loss: 2.361896, mae: 0.730994, mean_q: 5.786566
 73935/100000: episode: 1214, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 193.834, mean reward: 6.253 [3.172, 12.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.569, 10.561], loss: 1.142854, mae: 0.718732, mean_q: 5.862486
 73960/100000: episode: 1215, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 75.436, mean reward: 3.017 [2.289, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.035, 10.367], loss: 1.990793, mae: 0.831615, mean_q: 5.791687
 73984/100000: episode: 1216, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 65.036, mean reward: 2.710 [1.726, 6.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.867, 10.261], loss: 6.608040, mae: 0.921007, mean_q: 5.879862
 74013/100000: episode: 1217, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 76.596, mean reward: 2.641 [1.936, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.299, 10.363], loss: 1.458002, mae: 0.766688, mean_q: 6.063005
 74043/100000: episode: 1218, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 90.022, mean reward: 3.001 [2.244, 6.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.117, 10.405], loss: 4.728646, mae: 0.935954, mean_q: 5.946068
 74068/100000: episode: 1219, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 67.279, mean reward: 2.691 [2.060, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.383], loss: 1.891930, mae: 0.769981, mean_q: 5.924739
 74100/100000: episode: 1220, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 84.207, mean reward: 2.631 [1.564, 4.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.631, 10.144], loss: 0.989002, mae: 0.671494, mean_q: 5.852623
 74130/100000: episode: 1221, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 104.692, mean reward: 3.490 [2.639, 4.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.751, 10.419], loss: 5.137576, mae: 0.796326, mean_q: 5.879397
 74161/100000: episode: 1222, duration: 0.151s, episode steps: 31, steps per second: 206, episode reward: 128.733, mean reward: 4.153 [2.175, 16.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.493, 10.408], loss: 5.231856, mae: 0.840027, mean_q: 5.879629
 74190/100000: episode: 1223, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 109.700, mean reward: 3.783 [2.419, 6.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.127, 10.411], loss: 0.461593, mae: 0.610820, mean_q: 5.876446
 74214/100000: episode: 1224, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 76.852, mean reward: 3.202 [1.806, 5.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.339], loss: 0.970191, mae: 0.704392, mean_q: 5.949275
 74239/100000: episode: 1225, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 84.173, mean reward: 3.367 [2.496, 4.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.300, 10.438], loss: 0.552659, mae: 0.604542, mean_q: 5.853177
 74270/100000: episode: 1226, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 94.774, mean reward: 3.057 [1.866, 5.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.366], loss: 0.633213, mae: 0.606398, mean_q: 5.785284
 74295/100000: episode: 1227, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 61.380, mean reward: 2.455 [1.767, 4.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.413, 10.285], loss: 0.382284, mae: 0.576411, mean_q: 5.837861
 74326/100000: episode: 1228, duration: 0.155s, episode steps: 31, steps per second: 199, episode reward: 139.202, mean reward: 4.490 [2.273, 7.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.651, 10.423], loss: 5.477726, mae: 0.973509, mean_q: 6.087411
 74357/100000: episode: 1229, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 142.228, mean reward: 4.588 [2.773, 7.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.360, 10.473], loss: 1.029932, mae: 0.735834, mean_q: 5.845695
 74382/100000: episode: 1230, duration: 0.139s, episode steps: 25, steps per second: 179, episode reward: 72.946, mean reward: 2.918 [1.506, 4.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.323, 10.165], loss: 1.058995, mae: 0.770341, mean_q: 6.088660
[Info] 3-TH LEVEL FOUND: 10.491764068603516, Considering 10/90 traces
 74411/100000: episode: 1231, duration: 4.309s, episode steps: 29, steps per second: 7, episode reward: 99.626, mean reward: 3.435 [2.573, 5.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.907, 10.604], loss: 1.248168, mae: 0.712893, mean_q: 5.932471
 74436/100000: episode: 1232, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 97.762, mean reward: 3.910 [1.859, 6.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.178, 10.295], loss: 2.225584, mae: 0.720526, mean_q: 5.896103
 74448/100000: episode: 1233, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 139.178, mean reward: 11.598 [5.122, 40.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.539], loss: 0.251106, mae: 0.515572, mean_q: 5.841511
 74473/100000: episode: 1234, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 186.717, mean reward: 7.469 [3.748, 25.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.657, 10.502], loss: 0.849495, mae: 0.693995, mean_q: 5.960739
 74485/100000: episode: 1235, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 304.483, mean reward: 25.374 [6.785, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.272, 10.789], loss: 1.223328, mae: 0.788074, mean_q: 6.234358
 74510/100000: episode: 1236, duration: 0.149s, episode steps: 25, steps per second: 167, episode reward: 97.958, mean reward: 3.918 [2.677, 7.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.177, 10.590], loss: 8.027089, mae: 0.997850, mean_q: 6.292610
[Info] FALSIFICATION!
[Info] Levels: [5.8637705, 7.795626, 10.491764, 11.881959]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.09]
[Info] Error Prob: 9.000000000000002e-05

 74511/100000: episode: 1237, duration: 4.338s, episode steps: 1, steps per second: 0, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.019, 9.546], loss: 0.978711, mae: 0.816549, mean_q: 6.075675
 74611/100000: episode: 1238, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 190.146, mean reward: 1.901 [1.476, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.886, 10.098], loss: 5.413002, mae: 0.992945, mean_q: 6.262761
 74711/100000: episode: 1239, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 195.323, mean reward: 1.953 [1.468, 4.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.159, 10.179], loss: 2.069828, mae: 0.902016, mean_q: 6.181515
 74811/100000: episode: 1240, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 217.148, mean reward: 2.171 [1.503, 4.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.992, 10.098], loss: 6.917685, mae: 1.064198, mean_q: 6.156997
 74911/100000: episode: 1241, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.312, mean reward: 1.883 [1.435, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.991, 10.098], loss: 2.776973, mae: 0.858983, mean_q: 6.067289
 75011/100000: episode: 1242, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 206.515, mean reward: 2.065 [1.490, 5.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.872, 10.098], loss: 5.094517, mae: 0.922411, mean_q: 6.038294
 75111/100000: episode: 1243, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 209.053, mean reward: 2.091 [1.449, 5.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.600, 10.376], loss: 4.496320, mae: 0.929720, mean_q: 6.071456
 75211/100000: episode: 1244, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 178.849, mean reward: 1.788 [1.437, 2.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.315, 10.263], loss: 3.951364, mae: 0.854212, mean_q: 6.058043
 75311/100000: episode: 1245, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 203.157, mean reward: 2.032 [1.491, 4.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.907, 10.148], loss: 1.259722, mae: 0.697661, mean_q: 5.921921
 75411/100000: episode: 1246, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 183.904, mean reward: 1.839 [1.454, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.786, 10.098], loss: 1.348791, mae: 0.727377, mean_q: 5.876905
 75511/100000: episode: 1247, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.550, mean reward: 1.915 [1.442, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.836, 10.098], loss: 1.935461, mae: 0.783248, mean_q: 5.932570
 75611/100000: episode: 1248, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.475, mean reward: 1.955 [1.448, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.087, 10.098], loss: 3.434582, mae: 0.809650, mean_q: 5.829934
 75711/100000: episode: 1249, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 209.545, mean reward: 2.095 [1.459, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.627, 10.425], loss: 2.175682, mae: 0.754167, mean_q: 5.894263
 75811/100000: episode: 1250, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 186.738, mean reward: 1.867 [1.457, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.278, 10.098], loss: 7.967558, mae: 1.041738, mean_q: 6.103025
 75911/100000: episode: 1251, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 187.856, mean reward: 1.879 [1.471, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.210, 10.099], loss: 1.825405, mae: 0.779542, mean_q: 5.796782
 76011/100000: episode: 1252, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.380, mean reward: 1.834 [1.434, 2.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.442, 10.098], loss: 4.762403, mae: 0.855253, mean_q: 5.830043
 76111/100000: episode: 1253, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 274.611, mean reward: 2.746 [1.483, 7.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.694, 10.454], loss: 7.401365, mae: 0.966650, mean_q: 5.974489
 76211/100000: episode: 1254, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 201.957, mean reward: 2.020 [1.493, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.034, 10.201], loss: 3.867605, mae: 0.928970, mean_q: 5.865732
 76311/100000: episode: 1255, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 215.841, mean reward: 2.158 [1.439, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.277, 10.162], loss: 7.765380, mae: 1.078447, mean_q: 5.918189
 76411/100000: episode: 1256, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 208.146, mean reward: 2.081 [1.471, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.079, 10.375], loss: 3.560320, mae: 0.763463, mean_q: 5.693996
 76511/100000: episode: 1257, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.037, mean reward: 1.950 [1.494, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.787, 10.098], loss: 1.266412, mae: 0.672857, mean_q: 5.660321
 76611/100000: episode: 1258, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 183.796, mean reward: 1.838 [1.470, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.472, 10.098], loss: 3.994773, mae: 0.792101, mean_q: 5.786110
 76711/100000: episode: 1259, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 190.783, mean reward: 1.908 [1.445, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.401, 10.098], loss: 5.728971, mae: 0.887183, mean_q: 5.858942
 76811/100000: episode: 1260, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.589, mean reward: 1.886 [1.447, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.210, 10.244], loss: 7.250650, mae: 0.928244, mean_q: 5.753035
 76911/100000: episode: 1261, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.803, mean reward: 1.888 [1.460, 3.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.552, 10.284], loss: 2.288689, mae: 0.694778, mean_q: 5.626493
 77011/100000: episode: 1262, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 178.772, mean reward: 1.788 [1.457, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.712, 10.098], loss: 1.975336, mae: 0.632363, mean_q: 5.510380
 77111/100000: episode: 1263, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 192.131, mean reward: 1.921 [1.452, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.551, 10.098], loss: 2.787321, mae: 0.664814, mean_q: 5.469255
 77211/100000: episode: 1264, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 184.919, mean reward: 1.849 [1.449, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.875, 10.296], loss: 6.902760, mae: 0.856269, mean_q: 5.479028
 77311/100000: episode: 1265, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 195.774, mean reward: 1.958 [1.456, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.927, 10.098], loss: 5.163157, mae: 0.767516, mean_q: 5.524628
 77411/100000: episode: 1266, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.154, mean reward: 1.892 [1.518, 2.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.003, 10.229], loss: 4.391602, mae: 0.834152, mean_q: 5.452060
 77511/100000: episode: 1267, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 180.954, mean reward: 1.810 [1.440, 2.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.260, 10.177], loss: 4.103751, mae: 0.707512, mean_q: 5.399788
 77611/100000: episode: 1268, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 191.277, mean reward: 1.913 [1.469, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.132], loss: 3.862462, mae: 0.768587, mean_q: 5.365840
 77711/100000: episode: 1269, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 230.279, mean reward: 2.303 [1.444, 4.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.613, 10.098], loss: 3.891424, mae: 0.730206, mean_q: 5.285707
 77811/100000: episode: 1270, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 190.542, mean reward: 1.905 [1.436, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.256, 10.251], loss: 5.880434, mae: 0.770188, mean_q: 5.252450
 77911/100000: episode: 1271, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 193.759, mean reward: 1.938 [1.473, 5.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.643, 10.098], loss: 3.470354, mae: 0.659753, mean_q: 5.104217
 78011/100000: episode: 1272, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 200.777, mean reward: 2.008 [1.493, 4.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.852, 10.125], loss: 4.491504, mae: 0.727497, mean_q: 5.103135
 78111/100000: episode: 1273, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 194.652, mean reward: 1.947 [1.495, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.846, 10.270], loss: 2.130311, mae: 0.583500, mean_q: 4.966657
 78211/100000: episode: 1274, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 184.580, mean reward: 1.846 [1.474, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.166, 10.101], loss: 1.161712, mae: 0.518373, mean_q: 4.881884
 78311/100000: episode: 1275, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 183.688, mean reward: 1.837 [1.439, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.244, 10.217], loss: 2.894561, mae: 0.537885, mean_q: 4.741862
 78411/100000: episode: 1276, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.384, mean reward: 1.814 [1.451, 2.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.814, 10.156], loss: 1.692236, mae: 0.483118, mean_q: 4.653788
 78511/100000: episode: 1277, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 197.405, mean reward: 1.974 [1.444, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.706, 10.098], loss: 2.333930, mae: 0.502773, mean_q: 4.670908
 78611/100000: episode: 1278, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 177.226, mean reward: 1.772 [1.443, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.030, 10.118], loss: 5.883803, mae: 0.748954, mean_q: 4.751576
 78711/100000: episode: 1279, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 195.596, mean reward: 1.956 [1.507, 4.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.938, 10.285], loss: 3.511642, mae: 0.580868, mean_q: 4.480534
 78811/100000: episode: 1280, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 238.767, mean reward: 2.388 [1.438, 5.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.033, 10.462], loss: 1.508828, mae: 0.447618, mean_q: 4.454484
 78911/100000: episode: 1281, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 189.133, mean reward: 1.891 [1.442, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.573, 10.098], loss: 1.693323, mae: 0.467913, mean_q: 4.358477
 79011/100000: episode: 1282, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.243, mean reward: 1.832 [1.456, 3.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.705, 10.098], loss: 3.426224, mae: 0.531995, mean_q: 4.390950
 79111/100000: episode: 1283, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.680, mean reward: 1.947 [1.513, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.284, 10.340], loss: 1.004706, mae: 0.430355, mean_q: 4.315834
 79211/100000: episode: 1284, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 194.998, mean reward: 1.950 [1.445, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.901, 10.182], loss: 2.774268, mae: 0.469259, mean_q: 4.228489
 79311/100000: episode: 1285, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 206.969, mean reward: 2.070 [1.508, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.246, 10.098], loss: 0.480048, mae: 0.381536, mean_q: 4.126609
 79411/100000: episode: 1286, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 194.707, mean reward: 1.947 [1.449, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.414, 10.098], loss: 1.607234, mae: 0.386732, mean_q: 4.073043
 79511/100000: episode: 1287, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 206.177, mean reward: 2.062 [1.509, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.215, 10.267], loss: 0.554708, mae: 0.325281, mean_q: 3.890427
 79611/100000: episode: 1288, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 235.285, mean reward: 2.353 [1.496, 7.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.969, 10.098], loss: 0.092503, mae: 0.297590, mean_q: 3.884468
 79711/100000: episode: 1289, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 207.630, mean reward: 2.076 [1.512, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.577, 10.098], loss: 0.089315, mae: 0.294080, mean_q: 3.924732
 79811/100000: episode: 1290, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 221.211, mean reward: 2.212 [1.463, 4.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.948, 10.207], loss: 0.093812, mae: 0.296892, mean_q: 3.915028
 79911/100000: episode: 1291, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 193.629, mean reward: 1.936 [1.477, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.161, 10.098], loss: 0.093205, mae: 0.291430, mean_q: 3.912582
 80011/100000: episode: 1292, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 197.558, mean reward: 1.976 [1.448, 5.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.177, 10.176], loss: 0.085884, mae: 0.285766, mean_q: 3.902797
 80111/100000: episode: 1293, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 204.280, mean reward: 2.043 [1.510, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.830, 10.200], loss: 0.083651, mae: 0.285169, mean_q: 3.900957
 80211/100000: episode: 1294, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 212.228, mean reward: 2.122 [1.558, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.787, 10.098], loss: 0.092081, mae: 0.293200, mean_q: 3.917581
 80311/100000: episode: 1295, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 222.384, mean reward: 2.224 [1.451, 5.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.499, 10.098], loss: 0.091047, mae: 0.293767, mean_q: 3.923460
 80411/100000: episode: 1296, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 203.062, mean reward: 2.031 [1.502, 3.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.363, 10.139], loss: 0.100299, mae: 0.306292, mean_q: 3.942448
 80511/100000: episode: 1297, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 179.738, mean reward: 1.797 [1.443, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.781, 10.098], loss: 0.096069, mae: 0.300933, mean_q: 3.943838
 80611/100000: episode: 1298, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 210.488, mean reward: 2.105 [1.455, 4.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.764, 10.218], loss: 0.090594, mae: 0.292063, mean_q: 3.932548
 80711/100000: episode: 1299, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 200.772, mean reward: 2.008 [1.482, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.451, 10.098], loss: 0.089539, mae: 0.287773, mean_q: 3.921654
 80811/100000: episode: 1300, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.784, mean reward: 1.898 [1.502, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.787, 10.277], loss: 0.088676, mae: 0.293980, mean_q: 3.963677
 80911/100000: episode: 1301, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 186.055, mean reward: 1.861 [1.465, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.093, 10.098], loss: 0.081727, mae: 0.285751, mean_q: 3.941231
 81011/100000: episode: 1302, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 187.830, mean reward: 1.878 [1.440, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.461, 10.098], loss: 0.087569, mae: 0.287357, mean_q: 3.930667
 81111/100000: episode: 1303, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 192.396, mean reward: 1.924 [1.469, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.309, 10.098], loss: 0.087081, mae: 0.292298, mean_q: 3.918309
 81211/100000: episode: 1304, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.190, mean reward: 1.842 [1.453, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.893, 10.150], loss: 0.089305, mae: 0.285436, mean_q: 3.921483
 81311/100000: episode: 1305, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 182.596, mean reward: 1.826 [1.446, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.972, 10.123], loss: 0.085212, mae: 0.287889, mean_q: 3.887715
 81411/100000: episode: 1306, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 178.565, mean reward: 1.786 [1.436, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.763, 10.098], loss: 0.081837, mae: 0.285827, mean_q: 3.875166
 81511/100000: episode: 1307, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 199.287, mean reward: 1.993 [1.451, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.291, 10.098], loss: 0.088349, mae: 0.287842, mean_q: 3.882682
 81611/100000: episode: 1308, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 192.652, mean reward: 1.927 [1.458, 4.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.972, 10.098], loss: 0.078794, mae: 0.285427, mean_q: 3.856327
 81711/100000: episode: 1309, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 182.270, mean reward: 1.823 [1.468, 2.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.725, 10.187], loss: 0.099040, mae: 0.291462, mean_q: 3.878042
 81811/100000: episode: 1310, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 182.996, mean reward: 1.830 [1.515, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.452, 10.098], loss: 0.080048, mae: 0.289279, mean_q: 3.908525
 81911/100000: episode: 1311, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 241.202, mean reward: 2.412 [1.515, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.635, 10.324], loss: 0.078118, mae: 0.272967, mean_q: 3.860940
 82011/100000: episode: 1312, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 192.938, mean reward: 1.929 [1.437, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.796, 10.145], loss: 0.088307, mae: 0.293625, mean_q: 3.897610
 82111/100000: episode: 1313, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 191.591, mean reward: 1.916 [1.452, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.462, 10.098], loss: 0.101081, mae: 0.290862, mean_q: 3.898676
 82211/100000: episode: 1314, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 209.403, mean reward: 2.094 [1.471, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.288, 10.098], loss: 0.088822, mae: 0.286793, mean_q: 3.892841
 82311/100000: episode: 1315, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 187.673, mean reward: 1.877 [1.445, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.345, 10.221], loss: 0.077368, mae: 0.278556, mean_q: 3.903091
 82411/100000: episode: 1316, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 222.314, mean reward: 2.223 [1.495, 11.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.666, 10.098], loss: 0.084357, mae: 0.282668, mean_q: 3.913362
 82511/100000: episode: 1317, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 191.115, mean reward: 1.911 [1.466, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.448, 10.098], loss: 0.094378, mae: 0.290698, mean_q: 3.913905
 82611/100000: episode: 1318, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 184.253, mean reward: 1.843 [1.496, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.439, 10.292], loss: 0.101685, mae: 0.295883, mean_q: 3.927058
 82711/100000: episode: 1319, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 187.120, mean reward: 1.871 [1.456, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.725, 10.170], loss: 0.086747, mae: 0.285708, mean_q: 3.889162
 82811/100000: episode: 1320, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 183.345, mean reward: 1.833 [1.459, 2.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.843, 10.193], loss: 0.094164, mae: 0.293509, mean_q: 3.904177
 82911/100000: episode: 1321, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 178.040, mean reward: 1.780 [1.453, 2.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.654, 10.199], loss: 0.080586, mae: 0.278506, mean_q: 3.884696
 83011/100000: episode: 1322, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 192.863, mean reward: 1.929 [1.452, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.619, 10.162], loss: 0.086099, mae: 0.286678, mean_q: 3.896276
 83111/100000: episode: 1323, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 177.165, mean reward: 1.772 [1.456, 2.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.686, 10.098], loss: 0.083222, mae: 0.278833, mean_q: 3.894570
 83211/100000: episode: 1324, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 191.023, mean reward: 1.910 [1.461, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.764, 10.098], loss: 0.083520, mae: 0.280592, mean_q: 3.905742
 83311/100000: episode: 1325, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 200.839, mean reward: 2.008 [1.473, 4.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.798, 10.098], loss: 0.079486, mae: 0.279311, mean_q: 3.882684
 83411/100000: episode: 1326, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 194.108, mean reward: 1.941 [1.478, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.526, 10.262], loss: 0.093488, mae: 0.290027, mean_q: 3.880156
 83511/100000: episode: 1327, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 192.822, mean reward: 1.928 [1.437, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.786, 10.098], loss: 0.091542, mae: 0.289893, mean_q: 3.919913
 83611/100000: episode: 1328, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 185.671, mean reward: 1.857 [1.488, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.588, 10.130], loss: 0.088388, mae: 0.286070, mean_q: 3.891919
 83711/100000: episode: 1329, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 267.881, mean reward: 2.679 [1.460, 5.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.582, 10.098], loss: 0.082375, mae: 0.280573, mean_q: 3.907243
 83811/100000: episode: 1330, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.595, mean reward: 1.916 [1.461, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.749, 10.098], loss: 0.090908, mae: 0.283333, mean_q: 3.910285
 83911/100000: episode: 1331, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 178.289, mean reward: 1.783 [1.447, 2.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.574, 10.098], loss: 0.082953, mae: 0.277815, mean_q: 3.893934
 84011/100000: episode: 1332, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.834, mean reward: 1.968 [1.457, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.260, 10.342], loss: 0.087317, mae: 0.284225, mean_q: 3.911916
 84111/100000: episode: 1333, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 206.399, mean reward: 2.064 [1.504, 5.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.994, 10.257], loss: 0.084485, mae: 0.283501, mean_q: 3.925692
 84211/100000: episode: 1334, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 192.774, mean reward: 1.928 [1.485, 4.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.232, 10.098], loss: 0.086601, mae: 0.290481, mean_q: 3.919222
 84311/100000: episode: 1335, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.195, mean reward: 1.932 [1.454, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.500, 10.098], loss: 0.094273, mae: 0.289866, mean_q: 3.927938
 84411/100000: episode: 1336, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.557, mean reward: 1.916 [1.449, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.440, 10.098], loss: 0.080064, mae: 0.276822, mean_q: 3.916465
[Info] 1-TH LEVEL FOUND: 5.753366470336914, Considering 10/90 traces
 84511/100000: episode: 1337, duration: 4.594s, episode steps: 100, steps per second: 22, episode reward: 191.974, mean reward: 1.920 [1.451, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.483, 10.098], loss: 0.090415, mae: 0.286388, mean_q: 3.907701
 84532/100000: episode: 1338, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 65.727, mean reward: 3.130 [2.455, 4.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.354, 10.406], loss: 0.073851, mae: 0.279434, mean_q: 3.901305
 84568/100000: episode: 1339, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 187.556, mean reward: 5.210 [2.362, 17.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.414, 10.100], loss: 0.088278, mae: 0.277209, mean_q: 3.891689
 84609/100000: episode: 1340, duration: 0.209s, episode steps: 41, steps per second: 196, episode reward: 140.134, mean reward: 3.418 [2.298, 7.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-1.032, 10.100], loss: 0.372047, mae: 0.380672, mean_q: 3.966541
 84637/100000: episode: 1341, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 68.055, mean reward: 2.431 [1.618, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.226, 10.256], loss: 0.139729, mae: 0.341798, mean_q: 3.857907
 84672/100000: episode: 1342, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 78.301, mean reward: 2.237 [1.447, 5.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.904, 10.145], loss: 0.138044, mae: 0.317942, mean_q: 3.961861
 84708/100000: episode: 1343, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 109.102, mean reward: 3.031 [2.014, 4.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.363, 10.100], loss: 0.086775, mae: 0.292623, mean_q: 3.932412
 84736/100000: episode: 1344, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 98.028, mean reward: 3.501 [2.562, 8.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.652, 10.483], loss: 0.124149, mae: 0.318145, mean_q: 3.969754
 84771/100000: episode: 1345, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 111.131, mean reward: 3.175 [1.893, 5.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.172, 10.383], loss: 0.118996, mae: 0.316031, mean_q: 3.991610
 84800/100000: episode: 1346, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 70.743, mean reward: 2.439 [1.956, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.315], loss: 0.099872, mae: 0.304660, mean_q: 3.932329
 84809/100000: episode: 1347, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 26.078, mean reward: 2.898 [2.390, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.400, 10.100], loss: 0.089801, mae: 0.284172, mean_q: 3.885791
 84845/100000: episode: 1348, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 107.256, mean reward: 2.979 [2.178, 5.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.540, 10.100], loss: 0.149224, mae: 0.316368, mean_q: 3.999919
 84886/100000: episode: 1349, duration: 0.212s, episode steps: 41, steps per second: 193, episode reward: 129.454, mean reward: 3.157 [1.760, 5.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.676, 10.100], loss: 0.171329, mae: 0.334976, mean_q: 4.055919
 84912/100000: episode: 1350, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 91.924, mean reward: 3.536 [2.113, 8.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.771, 10.488], loss: 0.097242, mae: 0.307499, mean_q: 4.061172
 84947/100000: episode: 1351, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 91.920, mean reward: 2.626 [1.544, 6.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.448, 10.250], loss: 0.111671, mae: 0.325800, mean_q: 4.077690
 84961/100000: episode: 1352, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 38.150, mean reward: 2.725 [2.318, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.432, 10.100], loss: 0.128684, mae: 0.297106, mean_q: 3.980943
 84989/100000: episode: 1353, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 74.171, mean reward: 2.649 [1.978, 5.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.284], loss: 0.128357, mae: 0.325475, mean_q: 4.057275
 85024/100000: episode: 1354, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 96.349, mean reward: 2.753 [1.919, 6.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.739, 10.261], loss: 0.112621, mae: 0.305905, mean_q: 4.070646
 85059/100000: episode: 1355, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 163.519, mean reward: 4.672 [2.618, 10.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-1.053, 10.652], loss: 0.151183, mae: 0.345279, mean_q: 4.119143
 85080/100000: episode: 1356, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 47.500, mean reward: 2.262 [1.635, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.486, 10.188], loss: 0.099984, mae: 0.313687, mean_q: 4.123449
 85101/100000: episode: 1357, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 46.305, mean reward: 2.205 [1.744, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.248, 10.249], loss: 0.162382, mae: 0.372919, mean_q: 4.153201
 85136/100000: episode: 1358, duration: 0.171s, episode steps: 35, steps per second: 204, episode reward: 97.334, mean reward: 2.781 [1.556, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.309, 10.100], loss: 0.158624, mae: 0.337029, mean_q: 4.112351
 85157/100000: episode: 1359, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 53.345, mean reward: 2.540 [1.718, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.236], loss: 0.170332, mae: 0.382506, mean_q: 4.034943
 85187/100000: episode: 1360, duration: 0.149s, episode steps: 30, steps per second: 202, episode reward: 75.424, mean reward: 2.514 [1.817, 4.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.570, 10.100], loss: 0.201996, mae: 0.368965, mean_q: 4.198472
 85217/100000: episode: 1361, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 80.178, mean reward: 2.673 [1.713, 5.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.896, 10.100], loss: 0.139738, mae: 0.327126, mean_q: 4.104296
 85245/100000: episode: 1362, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 141.551, mean reward: 5.055 [2.335, 11.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.089, 10.464], loss: 0.257706, mae: 0.358349, mean_q: 4.124276
 85281/100000: episode: 1363, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 88.792, mean reward: 2.466 [1.676, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.440, 10.100], loss: 0.128494, mae: 0.341061, mean_q: 4.184424
 85317/100000: episode: 1364, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 104.965, mean reward: 2.916 [1.580, 6.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.386, 10.100], loss: 0.135836, mae: 0.334327, mean_q: 4.172374
 85352/100000: episode: 1365, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 88.558, mean reward: 2.530 [1.872, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.455, 10.442], loss: 0.148092, mae: 0.341871, mean_q: 4.164281
 85366/100000: episode: 1366, duration: 0.078s, episode steps: 14, steps per second: 178, episode reward: 34.762, mean reward: 2.483 [1.985, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.359, 10.100], loss: 0.115761, mae: 0.337118, mean_q: 4.209898
 85396/100000: episode: 1367, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 69.006, mean reward: 2.300 [1.680, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.175, 10.100], loss: 0.125650, mae: 0.329225, mean_q: 4.170077
 85437/100000: episode: 1368, duration: 0.215s, episode steps: 41, steps per second: 191, episode reward: 122.081, mean reward: 2.978 [2.016, 4.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.244, 10.100], loss: 0.198132, mae: 0.371974, mean_q: 4.326147
 85465/100000: episode: 1369, duration: 0.145s, episode steps: 28, steps per second: 194, episode reward: 94.671, mean reward: 3.381 [2.357, 5.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.709, 10.463], loss: 0.144276, mae: 0.351397, mean_q: 4.148305
 85491/100000: episode: 1370, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 59.870, mean reward: 2.303 [1.551, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.506, 10.166], loss: 0.132631, mae: 0.343407, mean_q: 4.188609
 85505/100000: episode: 1371, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 43.670, mean reward: 3.119 [2.476, 4.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.381, 10.100], loss: 0.120245, mae: 0.330233, mean_q: 4.241856
 85540/100000: episode: 1372, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 94.081, mean reward: 2.688 [1.456, 12.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.452, 10.111], loss: 0.125448, mae: 0.327016, mean_q: 4.204347
 85566/100000: episode: 1373, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 65.514, mean reward: 2.520 [1.774, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.681, 10.253], loss: 0.183304, mae: 0.344459, mean_q: 4.237053
 85595/100000: episode: 1374, duration: 0.165s, episode steps: 29, steps per second: 176, episode reward: 62.441, mean reward: 2.153 [1.668, 2.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.522, 10.181], loss: 0.124234, mae: 0.342816, mean_q: 4.250928
 85630/100000: episode: 1375, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 116.345, mean reward: 3.324 [1.583, 5.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.520, 10.249], loss: 0.141465, mae: 0.338059, mean_q: 4.306868
 85671/100000: episode: 1376, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 118.851, mean reward: 2.899 [1.918, 4.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.348, 10.100], loss: 0.160526, mae: 0.359488, mean_q: 4.241104
 85685/100000: episode: 1377, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 43.209, mean reward: 3.086 [2.205, 4.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.322, 10.100], loss: 0.186469, mae: 0.382089, mean_q: 4.405957
 85706/100000: episode: 1378, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 50.455, mean reward: 2.403 [1.568, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.685, 10.240], loss: 0.196151, mae: 0.357987, mean_q: 4.223504
 85742/100000: episode: 1379, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 91.927, mean reward: 2.554 [1.882, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.017, 10.100], loss: 0.195611, mae: 0.358592, mean_q: 4.259975
 85751/100000: episode: 1380, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 25.156, mean reward: 2.795 [2.397, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.339, 10.100], loss: 0.174738, mae: 0.382656, mean_q: 4.207779
 85787/100000: episode: 1381, duration: 0.181s, episode steps: 36, steps per second: 198, episode reward: 90.780, mean reward: 2.522 [1.629, 4.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.836, 10.100], loss: 0.212129, mae: 0.386792, mean_q: 4.356237
 85813/100000: episode: 1382, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 62.496, mean reward: 2.404 [1.918, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.294, 10.193], loss: 0.145627, mae: 0.350864, mean_q: 4.315153
 85848/100000: episode: 1383, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 101.741, mean reward: 2.907 [2.020, 4.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.881, 10.392], loss: 0.137028, mae: 0.340465, mean_q: 4.259298
 85874/100000: episode: 1384, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 67.215, mean reward: 2.585 [2.105, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.524, 10.298], loss: 0.163163, mae: 0.379825, mean_q: 4.407735
 85888/100000: episode: 1385, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 42.176, mean reward: 3.013 [2.325, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.409, 10.100], loss: 0.223710, mae: 0.354277, mean_q: 4.248357
 85924/100000: episode: 1386, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 94.986, mean reward: 2.639 [2.173, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.072, 10.100], loss: 0.202236, mae: 0.376160, mean_q: 4.356995
 85965/100000: episode: 1387, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 92.501, mean reward: 2.256 [1.652, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.248, 10.100], loss: 0.168480, mae: 0.359767, mean_q: 4.369085
 85979/100000: episode: 1388, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 32.866, mean reward: 2.348 [1.924, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.605, 10.100], loss: 0.105041, mae: 0.351840, mean_q: 4.369277
 86008/100000: episode: 1389, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 70.644, mean reward: 2.436 [1.645, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.539, 10.346], loss: 0.135854, mae: 0.356735, mean_q: 4.298082
 86043/100000: episode: 1390, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 118.850, mean reward: 3.396 [2.330, 5.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.142, 10.366], loss: 0.287746, mae: 0.397405, mean_q: 4.401191
 86073/100000: episode: 1391, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 87.688, mean reward: 2.923 [2.368, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.298, 10.100], loss: 0.161892, mae: 0.382702, mean_q: 4.419416
 86094/100000: episode: 1392, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 41.921, mean reward: 1.996 [1.483, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.046, 10.116], loss: 0.251331, mae: 0.375087, mean_q: 4.597146
 86129/100000: episode: 1393, duration: 0.170s, episode steps: 35, steps per second: 205, episode reward: 119.332, mean reward: 3.409 [2.265, 6.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.845, 10.333], loss: 0.148700, mae: 0.356065, mean_q: 4.410787
 86143/100000: episode: 1394, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 39.613, mean reward: 2.829 [2.325, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.296, 10.100], loss: 0.223187, mae: 0.437944, mean_q: 4.347471
 86169/100000: episode: 1395, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 66.058, mean reward: 2.541 [1.470, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.454, 10.305], loss: 0.180277, mae: 0.375607, mean_q: 4.487762
 86205/100000: episode: 1396, duration: 0.203s, episode steps: 36, steps per second: 178, episode reward: 160.783, mean reward: 4.466 [2.590, 6.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.967, 10.100], loss: 0.157987, mae: 0.376682, mean_q: 4.412236
 86233/100000: episode: 1397, duration: 0.139s, episode steps: 28, steps per second: 202, episode reward: 96.040, mean reward: 3.430 [2.615, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.499], loss: 0.303787, mae: 0.467254, mean_q: 4.385867
 86268/100000: episode: 1398, duration: 0.167s, episode steps: 35, steps per second: 210, episode reward: 111.741, mean reward: 3.193 [2.601, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.184, 10.461], loss: 0.154215, mae: 0.372492, mean_q: 4.508955
 86309/100000: episode: 1399, duration: 0.220s, episode steps: 41, steps per second: 187, episode reward: 157.676, mean reward: 3.846 [2.850, 6.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.373, 10.100], loss: 0.155082, mae: 0.360360, mean_q: 4.461776
 86318/100000: episode: 1400, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 29.106, mean reward: 3.234 [2.646, 3.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.922, 10.100], loss: 0.177602, mae: 0.376233, mean_q: 4.521982
 86346/100000: episode: 1401, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 85.802, mean reward: 3.064 [1.810, 4.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.997, 10.401], loss: 0.139113, mae: 0.353818, mean_q: 4.455413
 86381/100000: episode: 1402, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 93.559, mean reward: 2.673 [1.781, 5.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.907, 10.285], loss: 0.138095, mae: 0.353713, mean_q: 4.494043
 86417/100000: episode: 1403, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 123.166, mean reward: 3.421 [2.105, 6.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.431, 10.100], loss: 0.147908, mae: 0.360471, mean_q: 4.552586
 86443/100000: episode: 1404, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 63.324, mean reward: 2.436 [1.857, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.316], loss: 0.173748, mae: 0.392002, mean_q: 4.513263
 86457/100000: episode: 1405, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 42.006, mean reward: 3.000 [2.061, 4.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.199, 10.100], loss: 0.241092, mae: 0.432120, mean_q: 4.625621
 86485/100000: episode: 1406, duration: 0.145s, episode steps: 28, steps per second: 192, episode reward: 69.668, mean reward: 2.488 [1.707, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.229, 10.398], loss: 0.248100, mae: 0.405992, mean_q: 4.576734
 86513/100000: episode: 1407, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 68.841, mean reward: 2.459 [1.774, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.424, 10.304], loss: 0.227983, mae: 0.403097, mean_q: 4.647338
 86527/100000: episode: 1408, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 42.969, mean reward: 3.069 [1.944, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.539, 10.100], loss: 0.156490, mae: 0.361733, mean_q: 4.591664
 86541/100000: episode: 1409, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 39.616, mean reward: 2.830 [2.247, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.320, 10.100], loss: 0.200684, mae: 0.397344, mean_q: 4.625323
 86576/100000: episode: 1410, duration: 0.205s, episode steps: 35, steps per second: 170, episode reward: 91.691, mean reward: 2.620 [1.965, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.134, 10.395], loss: 0.158754, mae: 0.384919, mean_q: 4.603335
 86597/100000: episode: 1411, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 53.627, mean reward: 2.554 [1.987, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.552, 10.398], loss: 0.175108, mae: 0.381987, mean_q: 4.660445
 86626/100000: episode: 1412, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 75.575, mean reward: 2.606 [2.050, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.475, 10.462], loss: 0.189753, mae: 0.405789, mean_q: 4.695775
 86652/100000: episode: 1413, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 55.222, mean reward: 2.124 [1.448, 2.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.060, 10.126], loss: 0.156216, mae: 0.382961, mean_q: 4.629105
 86681/100000: episode: 1414, duration: 0.155s, episode steps: 29, steps per second: 188, episode reward: 120.699, mean reward: 4.162 [2.190, 7.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.225, 10.626], loss: 0.183090, mae: 0.390604, mean_q: 4.655500
 86707/100000: episode: 1415, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 59.127, mean reward: 2.274 [1.729, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.198], loss: 0.203689, mae: 0.441089, mean_q: 4.584533
 86716/100000: episode: 1416, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 25.829, mean reward: 2.870 [2.621, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.353, 10.100], loss: 0.123247, mae: 0.331284, mean_q: 4.656020
 86746/100000: episode: 1417, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 86.359, mean reward: 2.879 [1.830, 4.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.312, 10.100], loss: 0.142511, mae: 0.364677, mean_q: 4.625943
 86775/100000: episode: 1418, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 86.830, mean reward: 2.994 [2.099, 7.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.524], loss: 0.170913, mae: 0.385983, mean_q: 4.700816
 86796/100000: episode: 1419, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 53.820, mean reward: 2.563 [2.243, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.358], loss: 0.191626, mae: 0.429391, mean_q: 4.602451
 86805/100000: episode: 1420, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 27.527, mean reward: 3.059 [2.635, 4.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.295, 10.100], loss: 0.144813, mae: 0.367394, mean_q: 4.696199
 86814/100000: episode: 1421, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 29.233, mean reward: 3.248 [2.567, 4.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.223, 10.100], loss: 0.144045, mae: 0.376320, mean_q: 4.572886
 86855/100000: episode: 1422, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 129.540, mean reward: 3.160 [2.113, 12.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.749, 10.100], loss: 0.227323, mae: 0.417587, mean_q: 4.669408
 86881/100000: episode: 1423, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 56.616, mean reward: 2.178 [1.742, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.349, 10.371], loss: 0.180475, mae: 0.400692, mean_q: 4.699752
 86907/100000: episode: 1424, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 53.508, mean reward: 2.058 [1.684, 2.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.650, 10.286], loss: 0.172810, mae: 0.394940, mean_q: 4.667804
 86933/100000: episode: 1425, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 59.573, mean reward: 2.291 [1.872, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.723, 10.338], loss: 0.251047, mae: 0.473328, mean_q: 4.825959
 86962/100000: episode: 1426, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 64.286, mean reward: 2.217 [1.543, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.239, 10.338], loss: 0.324946, mae: 0.516457, mean_q: 4.715200
[Info] 2-TH LEVEL FOUND: 8.154777526855469, Considering 10/90 traces
 86991/100000: episode: 1427, duration: 4.404s, episode steps: 29, steps per second: 7, episode reward: 61.570, mean reward: 2.123 [1.597, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.213, 10.138], loss: 0.150736, mae: 0.381836, mean_q: 4.642341
 87016/100000: episode: 1428, duration: 0.126s, episode steps: 25, steps per second: 198, episode reward: 85.314, mean reward: 3.413 [2.234, 7.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.774, 10.100], loss: 0.227838, mae: 0.382562, mean_q: 4.612201
 87036/100000: episode: 1429, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 75.592, mean reward: 3.780 [2.473, 7.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.689, 10.100], loss: 0.179433, mae: 0.394268, mean_q: 4.651255
 87054/100000: episode: 1430, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 77.005, mean reward: 4.278 [2.978, 6.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-1.115, 10.466], loss: 0.331350, mae: 0.487519, mean_q: 4.897159
 87089/100000: episode: 1431, duration: 0.203s, episode steps: 35, steps per second: 173, episode reward: 190.120, mean reward: 5.432 [3.071, 13.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-1.640, 10.100], loss: 0.344679, mae: 0.516478, mean_q: 4.769766
 87107/100000: episode: 1432, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 52.616, mean reward: 2.923 [2.266, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.400], loss: 0.209397, mae: 0.404818, mean_q: 4.637437
 87132/100000: episode: 1433, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 96.243, mean reward: 3.850 [2.675, 9.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.377, 10.100], loss: 0.278033, mae: 0.450518, mean_q: 4.862867
 87142/100000: episode: 1434, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 48.609, mean reward: 4.861 [3.675, 6.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.499, 10.100], loss: 0.288657, mae: 0.455664, mean_q: 4.759380
 87177/100000: episode: 1435, duration: 0.171s, episode steps: 35, steps per second: 204, episode reward: 127.501, mean reward: 3.643 [1.899, 8.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.745, 10.100], loss: 0.327622, mae: 0.476130, mean_q: 4.848833
 87202/100000: episode: 1436, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 73.388, mean reward: 2.936 [2.164, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.265, 10.100], loss: 0.268458, mae: 0.448743, mean_q: 4.867874
 87220/100000: episode: 1437, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 65.354, mean reward: 3.631 [2.592, 7.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.447, 10.325], loss: 0.285790, mae: 0.442063, mean_q: 4.881794
 87230/100000: episode: 1438, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 43.625, mean reward: 4.363 [2.914, 5.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.501, 10.100], loss: 0.253187, mae: 0.455679, mean_q: 4.874143
 87250/100000: episode: 1439, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 73.705, mean reward: 3.685 [2.758, 6.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.686, 10.100], loss: 0.320477, mae: 0.495686, mean_q: 4.863292
 87278/100000: episode: 1440, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 87.704, mean reward: 3.132 [1.890, 8.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.168, 10.100], loss: 0.284838, mae: 0.463340, mean_q: 4.875883
 87306/100000: episode: 1441, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 102.898, mean reward: 3.675 [2.482, 5.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.609, 10.100], loss: 0.299844, mae: 0.452263, mean_q: 4.900094
 87341/100000: episode: 1442, duration: 0.169s, episode steps: 35, steps per second: 208, episode reward: 352.619, mean reward: 10.075 [3.292, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.710, 10.100], loss: 0.444574, mae: 0.452631, mean_q: 4.936521
 87359/100000: episode: 1443, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 95.634, mean reward: 5.313 [3.524, 6.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.598], loss: 0.459085, mae: 0.550057, mean_q: 4.918879
 87371/100000: episode: 1444, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 52.679, mean reward: 4.390 [3.655, 5.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.285, 10.100], loss: 0.263445, mae: 0.510813, mean_q: 5.087960
 87400/100000: episode: 1445, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 100.727, mean reward: 3.473 [2.604, 5.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.381, 10.418], loss: 0.254799, mae: 0.455877, mean_q: 5.024336
 87420/100000: episode: 1446, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 216.033, mean reward: 10.802 [3.350, 29.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.617, 10.100], loss: 0.269193, mae: 0.462184, mean_q: 4.935775
 87424/100000: episode: 1447, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 15.997, mean reward: 3.999 [3.319, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.453, 10.100], loss: 0.603370, mae: 0.554861, mean_q: 5.359198
 87449/100000: episode: 1448, duration: 0.148s, episode steps: 25, steps per second: 168, episode reward: 109.134, mean reward: 4.365 [2.880, 6.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-1.446, 10.100], loss: 0.490997, mae: 0.529909, mean_q: 5.053333
 87477/100000: episode: 1449, duration: 0.140s, episode steps: 28, steps per second: 201, episode reward: 109.975, mean reward: 3.928 [2.371, 8.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.454], loss: 3.251011, mae: 0.786068, mean_q: 5.139555
 87481/100000: episode: 1450, duration: 0.023s, episode steps: 4, steps per second: 178, episode reward: 18.050, mean reward: 4.513 [4.131, 4.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.476, 10.100], loss: 0.259189, mae: 0.452768, mean_q: 4.775144
 87499/100000: episode: 1451, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 71.737, mean reward: 3.985 [2.201, 6.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.342, 10.427], loss: 0.403635, mae: 0.554218, mean_q: 5.210036
 87527/100000: episode: 1452, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 128.216, mean reward: 4.579 [2.791, 8.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.093, 10.568], loss: 0.397862, mae: 0.493582, mean_q: 5.027070
 87539/100000: episode: 1453, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 45.243, mean reward: 3.770 [2.756, 4.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.409, 10.100], loss: 0.355719, mae: 0.520429, mean_q: 5.055792
 87564/100000: episode: 1454, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 88.622, mean reward: 3.545 [2.620, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.461, 10.100], loss: 0.448633, mae: 0.519326, mean_q: 5.105103
 87589/100000: episode: 1455, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 121.599, mean reward: 4.864 [2.576, 7.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.429, 10.100], loss: 0.255397, mae: 0.461501, mean_q: 4.972804
 87601/100000: episode: 1456, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 91.851, mean reward: 7.654 [4.596, 21.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.331, 10.100], loss: 5.582780, mae: 0.867274, mean_q: 5.429179
 87636/100000: episode: 1457, duration: 0.174s, episode steps: 35, steps per second: 202, episode reward: 101.783, mean reward: 2.908 [1.796, 4.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.194, 10.100], loss: 5.206292, mae: 0.946120, mean_q: 5.318563
 87665/100000: episode: 1458, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 92.377, mean reward: 3.185 [2.067, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.165, 10.408], loss: 0.567644, mae: 0.570636, mean_q: 5.158558
 87677/100000: episode: 1459, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 38.982, mean reward: 3.248 [2.747, 3.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.375, 10.100], loss: 0.430918, mae: 0.582064, mean_q: 5.477448
 87687/100000: episode: 1460, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 48.480, mean reward: 4.848 [3.180, 7.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.497, 10.100], loss: 14.151411, mae: 0.901297, mean_q: 5.438586
[Info] FALSIFICATION!
[Info] Levels: [5.7533665, 8.154778, 11.313978]
[Info] Cond. Prob: [0.1, 0.1, 0.04]
[Info] Error Prob: 0.0004000000000000001

 87702/100000: episode: 1461, duration: 4.543s, episode steps: 15, steps per second: 3, episode reward: 177.378, mean reward: 11.825 [3.266, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-1.080, 9.864], loss: 0.660014, mae: 0.648798, mean_q: 5.237355
 87802/100000: episode: 1462, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 195.665, mean reward: 1.957 [1.463, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.899, 10.134], loss: 1.209062, mae: 0.630140, mean_q: 5.312601
 87902/100000: episode: 1463, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 184.300, mean reward: 1.843 [1.455, 2.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.775, 10.235], loss: 4.569295, mae: 0.769942, mean_q: 5.371642
 88002/100000: episode: 1464, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 177.852, mean reward: 1.779 [1.434, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.163, 10.216], loss: 3.621870, mae: 0.743749, mean_q: 5.498078
 88102/100000: episode: 1465, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 183.276, mean reward: 1.833 [1.461, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.318, 10.101], loss: 2.993563, mae: 0.697237, mean_q: 5.507101
 88202/100000: episode: 1466, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 197.623, mean reward: 1.976 [1.455, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.964, 10.098], loss: 3.981891, mae: 0.722410, mean_q: 5.328062
 88302/100000: episode: 1467, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 194.825, mean reward: 1.948 [1.478, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.882, 10.148], loss: 1.901961, mae: 0.631727, mean_q: 5.533306
 88402/100000: episode: 1468, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 182.749, mean reward: 1.827 [1.465, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.587, 10.138], loss: 2.357509, mae: 0.614564, mean_q: 5.408932
 88502/100000: episode: 1469, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 179.979, mean reward: 1.800 [1.481, 2.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.403, 10.165], loss: 1.311401, mae: 0.595659, mean_q: 5.350696
 88602/100000: episode: 1470, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.522, mean reward: 1.855 [1.491, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.009, 10.098], loss: 2.025559, mae: 0.621342, mean_q: 5.328721
 88702/100000: episode: 1471, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.044, mean reward: 1.960 [1.441, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.622, 10.337], loss: 3.175993, mae: 0.646991, mean_q: 5.384611
 88802/100000: episode: 1472, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 188.507, mean reward: 1.885 [1.449, 3.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.669, 10.098], loss: 1.258630, mae: 0.617446, mean_q: 5.419853
 88902/100000: episode: 1473, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 207.506, mean reward: 2.075 [1.476, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.003, 10.199], loss: 2.686951, mae: 0.669396, mean_q: 5.460032
 89002/100000: episode: 1474, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 184.673, mean reward: 1.847 [1.436, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.220, 10.209], loss: 4.847873, mae: 0.744292, mean_q: 5.422461
 89102/100000: episode: 1475, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 197.338, mean reward: 1.973 [1.445, 5.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.797, 10.098], loss: 0.597941, mae: 0.540893, mean_q: 5.327444
 89202/100000: episode: 1476, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 207.750, mean reward: 2.077 [1.463, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.005, 10.475], loss: 0.329854, mae: 0.502294, mean_q: 5.262222
 89302/100000: episode: 1477, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 188.001, mean reward: 1.880 [1.480, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.872, 10.225], loss: 2.907044, mae: 0.605400, mean_q: 5.274915
 89402/100000: episode: 1478, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 198.024, mean reward: 1.980 [1.486, 2.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.941, 10.098], loss: 7.855129, mae: 0.870065, mean_q: 5.435089
 89502/100000: episode: 1479, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 200.953, mean reward: 2.010 [1.475, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.379, 10.306], loss: 4.064301, mae: 0.710605, mean_q: 5.341581
 89602/100000: episode: 1480, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 188.379, mean reward: 1.884 [1.449, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.545, 10.242], loss: 1.719944, mae: 0.573782, mean_q: 5.239197
 89702/100000: episode: 1481, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.121, mean reward: 1.841 [1.440, 2.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.868, 10.135], loss: 2.641286, mae: 0.616446, mean_q: 5.249436
 89802/100000: episode: 1482, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 239.591, mean reward: 2.396 [1.497, 10.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.166, 10.467], loss: 4.365919, mae: 0.735047, mean_q: 5.178643
 89902/100000: episode: 1483, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 193.517, mean reward: 1.935 [1.471, 5.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.819, 10.268], loss: 2.467294, mae: 0.607191, mean_q: 5.200563
 90002/100000: episode: 1484, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.634, mean reward: 1.886 [1.469, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.112, 10.098], loss: 6.103276, mae: 0.823485, mean_q: 5.245239
 90102/100000: episode: 1485, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 193.289, mean reward: 1.933 [1.494, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.063, 10.098], loss: 7.149562, mae: 0.922749, mean_q: 5.176660
 90202/100000: episode: 1486, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 189.437, mean reward: 1.894 [1.477, 2.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.504, 10.098], loss: 3.996303, mae: 0.701658, mean_q: 5.084165
 90302/100000: episode: 1487, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 188.864, mean reward: 1.889 [1.445, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.805, 10.098], loss: 0.765280, mae: 0.563430, mean_q: 5.032549
 90402/100000: episode: 1488, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 197.234, mean reward: 1.972 [1.496, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.238, 10.156], loss: 0.363947, mae: 0.477961, mean_q: 4.902373
 90502/100000: episode: 1489, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 186.643, mean reward: 1.866 [1.457, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.436, 10.098], loss: 3.236645, mae: 0.652701, mean_q: 5.031968
 90602/100000: episode: 1490, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 187.048, mean reward: 1.870 [1.474, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.479, 10.269], loss: 3.179372, mae: 0.585092, mean_q: 4.931664
 90702/100000: episode: 1491, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 191.138, mean reward: 1.911 [1.442, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.191, 10.098], loss: 3.551839, mae: 0.688106, mean_q: 4.984476
 90802/100000: episode: 1492, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 198.279, mean reward: 1.983 [1.454, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.353, 10.149], loss: 2.440850, mae: 0.611204, mean_q: 4.889559
 90902/100000: episode: 1493, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 211.387, mean reward: 2.114 [1.467, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.268, 10.098], loss: 2.997547, mae: 0.588691, mean_q: 4.822202
 91002/100000: episode: 1494, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 205.010, mean reward: 2.050 [1.483, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.789, 10.098], loss: 3.838876, mae: 0.650061, mean_q: 4.843890
 91102/100000: episode: 1495, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 176.630, mean reward: 1.766 [1.448, 2.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.002, 10.270], loss: 6.896199, mae: 0.843520, mean_q: 4.960572
 91202/100000: episode: 1496, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 180.378, mean reward: 1.804 [1.449, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.929, 10.249], loss: 0.982118, mae: 0.499607, mean_q: 4.636699
 91302/100000: episode: 1497, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 197.598, mean reward: 1.976 [1.468, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.870, 10.162], loss: 3.726540, mae: 0.608925, mean_q: 4.760370
 91402/100000: episode: 1498, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 194.108, mean reward: 1.941 [1.484, 4.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.970, 10.330], loss: 1.631362, mae: 0.521392, mean_q: 4.630863
 91502/100000: episode: 1499, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 181.801, mean reward: 1.818 [1.446, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.861, 10.098], loss: 6.137685, mae: 0.672289, mean_q: 4.791965
 91602/100000: episode: 1500, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 185.718, mean reward: 1.857 [1.464, 4.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.337, 10.298], loss: 2.300567, mae: 0.522314, mean_q: 4.572233
 91702/100000: episode: 1501, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 199.689, mean reward: 1.997 [1.431, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.894, 10.124], loss: 1.703793, mae: 0.510480, mean_q: 4.592693
 91802/100000: episode: 1502, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.193, mean reward: 1.882 [1.434, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.474, 10.098], loss: 1.917219, mae: 0.497544, mean_q: 4.515914
 91902/100000: episode: 1503, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 223.442, mean reward: 2.234 [1.437, 4.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.106, 10.098], loss: 1.838146, mae: 0.515008, mean_q: 4.499477
 92002/100000: episode: 1504, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.341, mean reward: 1.883 [1.472, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.564, 10.098], loss: 3.251945, mae: 0.586404, mean_q: 4.562538
 92102/100000: episode: 1505, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 212.190, mean reward: 2.122 [1.439, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.507, 10.224], loss: 1.545901, mae: 0.451178, mean_q: 4.320610
 92202/100000: episode: 1506, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 183.450, mean reward: 1.835 [1.463, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.769, 10.098], loss: 2.716426, mae: 0.503316, mean_q: 4.381798
 92302/100000: episode: 1507, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 187.831, mean reward: 1.878 [1.448, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.315, 10.098], loss: 1.881528, mae: 0.479811, mean_q: 4.224104
 92402/100000: episode: 1508, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 184.460, mean reward: 1.845 [1.469, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.905, 10.098], loss: 1.438716, mae: 0.409134, mean_q: 4.131814
 92502/100000: episode: 1509, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 186.711, mean reward: 1.867 [1.461, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.314, 10.452], loss: 1.414999, mae: 0.376850, mean_q: 4.022356
 92602/100000: episode: 1510, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 251.598, mean reward: 2.516 [1.519, 4.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.632, 10.514], loss: 0.106857, mae: 0.298278, mean_q: 3.928730
 92702/100000: episode: 1511, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.494, mean reward: 1.885 [1.474, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.169, 10.162], loss: 2.339740, mae: 0.404187, mean_q: 3.910750
 92802/100000: episode: 1512, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.771, mean reward: 1.888 [1.468, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.521, 10.098], loss: 0.091912, mae: 0.294939, mean_q: 3.851463
 92902/100000: episode: 1513, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.358, mean reward: 1.894 [1.438, 4.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.862, 10.221], loss: 0.097646, mae: 0.293808, mean_q: 3.823096
 93002/100000: episode: 1514, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 205.185, mean reward: 2.052 [1.471, 4.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.503, 10.395], loss: 0.097112, mae: 0.283935, mean_q: 3.845763
 93102/100000: episode: 1515, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 186.251, mean reward: 1.863 [1.447, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.389, 10.133], loss: 0.127923, mae: 0.283956, mean_q: 3.850301
 93202/100000: episode: 1516, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.905, mean reward: 1.939 [1.461, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.035, 10.098], loss: 0.110034, mae: 0.288641, mean_q: 3.849417
 93302/100000: episode: 1517, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.379, mean reward: 1.824 [1.446, 2.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.164, 10.171], loss: 0.089415, mae: 0.290112, mean_q: 3.840097
 93402/100000: episode: 1518, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 181.783, mean reward: 1.818 [1.477, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.375, 10.111], loss: 0.086015, mae: 0.281164, mean_q: 3.867169
 93502/100000: episode: 1519, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 200.921, mean reward: 2.009 [1.489, 2.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.302, 10.098], loss: 0.086395, mae: 0.283436, mean_q: 3.839497
 93602/100000: episode: 1520, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 188.844, mean reward: 1.888 [1.494, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.764, 10.098], loss: 0.097056, mae: 0.287865, mean_q: 3.847684
 93702/100000: episode: 1521, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 190.291, mean reward: 1.903 [1.462, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.895, 10.274], loss: 0.099762, mae: 0.286944, mean_q: 3.850004
 93802/100000: episode: 1522, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 195.782, mean reward: 1.958 [1.480, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.464, 10.098], loss: 0.127980, mae: 0.298092, mean_q: 3.851550
 93902/100000: episode: 1523, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.453, mean reward: 1.875 [1.448, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.255, 10.119], loss: 0.080758, mae: 0.267621, mean_q: 3.827448
 94002/100000: episode: 1524, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 196.836, mean reward: 1.968 [1.479, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.064, 10.244], loss: 0.095782, mae: 0.290362, mean_q: 3.855125
 94102/100000: episode: 1525, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 216.414, mean reward: 2.164 [1.486, 5.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.040, 10.278], loss: 0.082631, mae: 0.277268, mean_q: 3.838681
 94202/100000: episode: 1526, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 176.412, mean reward: 1.764 [1.456, 2.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.904, 10.292], loss: 0.085079, mae: 0.281525, mean_q: 3.843419
 94302/100000: episode: 1527, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 207.928, mean reward: 2.079 [1.473, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.534, 10.098], loss: 0.096384, mae: 0.278065, mean_q: 3.830791
 94402/100000: episode: 1528, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 178.345, mean reward: 1.783 [1.460, 2.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.031, 10.098], loss: 0.099631, mae: 0.285788, mean_q: 3.857865
 94502/100000: episode: 1529, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 219.353, mean reward: 2.194 [1.458, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.453, 10.098], loss: 0.096521, mae: 0.281762, mean_q: 3.858879
 94602/100000: episode: 1530, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 185.746, mean reward: 1.857 [1.438, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.007, 10.098], loss: 0.100536, mae: 0.289300, mean_q: 3.868634
 94702/100000: episode: 1531, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 181.901, mean reward: 1.819 [1.438, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.061, 10.098], loss: 0.099635, mae: 0.288017, mean_q: 3.866236
 94802/100000: episode: 1532, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 191.643, mean reward: 1.916 [1.467, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.190, 10.234], loss: 0.080980, mae: 0.272980, mean_q: 3.845383
 94902/100000: episode: 1533, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.850, mean reward: 1.888 [1.478, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.301, 10.195], loss: 0.075657, mae: 0.270120, mean_q: 3.843199
 95002/100000: episode: 1534, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 181.393, mean reward: 1.814 [1.451, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.914, 10.110], loss: 0.075426, mae: 0.266392, mean_q: 3.842803
 95102/100000: episode: 1535, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 183.070, mean reward: 1.831 [1.459, 2.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.071, 10.098], loss: 0.079787, mae: 0.269181, mean_q: 3.820609
 95202/100000: episode: 1536, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 184.868, mean reward: 1.849 [1.458, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.986, 10.098], loss: 0.074028, mae: 0.270053, mean_q: 3.841523
 95302/100000: episode: 1537, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 186.370, mean reward: 1.864 [1.464, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.726, 10.098], loss: 0.076724, mae: 0.265679, mean_q: 3.814906
 95402/100000: episode: 1538, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 215.825, mean reward: 2.158 [1.539, 6.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.014, 10.098], loss: 0.067540, mae: 0.253385, mean_q: 3.835764
 95502/100000: episode: 1539, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 183.083, mean reward: 1.831 [1.438, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.874, 10.098], loss: 0.078149, mae: 0.260074, mean_q: 3.833869
 95602/100000: episode: 1540, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 200.344, mean reward: 2.003 [1.501, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.925, 10.098], loss: 0.080983, mae: 0.268766, mean_q: 3.839993
 95702/100000: episode: 1541, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.574, mean reward: 1.886 [1.468, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.600, 10.274], loss: 0.080299, mae: 0.272804, mean_q: 3.830054
 95802/100000: episode: 1542, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 183.822, mean reward: 1.838 [1.469, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.886, 10.098], loss: 0.074255, mae: 0.260389, mean_q: 3.833086
 95902/100000: episode: 1543, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.410, mean reward: 1.874 [1.442, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.491, 10.101], loss: 0.080617, mae: 0.272857, mean_q: 3.826648
 96002/100000: episode: 1544, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 193.720, mean reward: 1.937 [1.438, 4.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.888, 10.098], loss: 0.085856, mae: 0.277055, mean_q: 3.821610
 96102/100000: episode: 1545, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 215.639, mean reward: 2.156 [1.460, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.999, 10.384], loss: 0.073581, mae: 0.259531, mean_q: 3.798200
 96202/100000: episode: 1546, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.309, mean reward: 1.853 [1.468, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.537, 10.115], loss: 0.077979, mae: 0.271228, mean_q: 3.835099
 96302/100000: episode: 1547, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 190.445, mean reward: 1.904 [1.483, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.923, 10.098], loss: 0.077973, mae: 0.269473, mean_q: 3.830621
 96402/100000: episode: 1548, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 193.374, mean reward: 1.934 [1.473, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.778, 10.208], loss: 0.079430, mae: 0.272791, mean_q: 3.818586
 96502/100000: episode: 1549, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 194.977, mean reward: 1.950 [1.476, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.481, 10.098], loss: 0.072219, mae: 0.266229, mean_q: 3.807796
 96602/100000: episode: 1550, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 191.898, mean reward: 1.919 [1.469, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.723, 10.330], loss: 0.079862, mae: 0.267334, mean_q: 3.816529
 96702/100000: episode: 1551, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 211.984, mean reward: 2.120 [1.471, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.026, 10.098], loss: 0.079081, mae: 0.271376, mean_q: 3.820578
 96802/100000: episode: 1552, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 231.127, mean reward: 2.311 [1.446, 5.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.495, 10.170], loss: 0.081046, mae: 0.272523, mean_q: 3.847527
 96902/100000: episode: 1553, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 204.378, mean reward: 2.044 [1.486, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.820, 10.101], loss: 0.086598, mae: 0.283774, mean_q: 3.849655
 97002/100000: episode: 1554, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 200.067, mean reward: 2.001 [1.490, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.628, 10.098], loss: 0.079505, mae: 0.272849, mean_q: 3.840511
 97102/100000: episode: 1555, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 194.123, mean reward: 1.941 [1.438, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.806, 10.119], loss: 0.072682, mae: 0.266858, mean_q: 3.825676
 97202/100000: episode: 1556, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 182.852, mean reward: 1.829 [1.472, 2.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.772, 10.098], loss: 0.071229, mae: 0.262021, mean_q: 3.838328
 97302/100000: episode: 1557, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 182.216, mean reward: 1.822 [1.452, 4.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.814, 10.098], loss: 0.068737, mae: 0.267143, mean_q: 3.849613
 97402/100000: episode: 1558, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 195.112, mean reward: 1.951 [1.480, 4.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.999, 10.191], loss: 0.074365, mae: 0.264154, mean_q: 3.833805
 97502/100000: episode: 1559, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 188.043, mean reward: 1.880 [1.479, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.481, 10.230], loss: 0.083311, mae: 0.276561, mean_q: 3.852141
 97602/100000: episode: 1560, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 184.324, mean reward: 1.843 [1.457, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.356, 10.215], loss: 0.080478, mae: 0.269308, mean_q: 3.819935
[Info] 1-TH LEVEL FOUND: 5.400634765625, Considering 10/90 traces
 97702/100000: episode: 1561, duration: 4.683s, episode steps: 100, steps per second: 21, episode reward: 223.429, mean reward: 2.234 [1.517, 4.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.322, 10.098], loss: 0.077085, mae: 0.270669, mean_q: 3.821830
 97745/100000: episode: 1562, duration: 0.228s, episode steps: 43, steps per second: 188, episode reward: 91.146, mean reward: 2.120 [1.592, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.291, 10.100], loss: 0.078870, mae: 0.273777, mean_q: 3.822584
 97756/100000: episode: 1563, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 24.048, mean reward: 2.186 [1.913, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.631, 10.349], loss: 0.069222, mae: 0.269239, mean_q: 3.837691
 97799/100000: episode: 1564, duration: 0.218s, episode steps: 43, steps per second: 197, episode reward: 113.767, mean reward: 2.646 [1.770, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.399, 10.100], loss: 0.088503, mae: 0.286664, mean_q: 3.856091
 97833/100000: episode: 1565, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 77.544, mean reward: 2.281 [1.890, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.292, 10.345], loss: 0.078489, mae: 0.279377, mean_q: 3.876102
 97867/100000: episode: 1566, duration: 0.167s, episode steps: 34, steps per second: 203, episode reward: 65.317, mean reward: 1.921 [1.506, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.145, 10.145], loss: 0.070573, mae: 0.269862, mean_q: 3.868670
 97887/100000: episode: 1567, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 48.654, mean reward: 2.433 [1.974, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.954, 10.323], loss: 0.064425, mae: 0.258724, mean_q: 3.846070
 97898/100000: episode: 1568, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 27.443, mean reward: 2.495 [2.070, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.417], loss: 0.090972, mae: 0.278187, mean_q: 3.867741
 97917/100000: episode: 1569, duration: 0.104s, episode steps: 19, steps per second: 184, episode reward: 53.116, mean reward: 2.796 [2.412, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.208, 10.100], loss: 0.095412, mae: 0.297307, mean_q: 3.884871
 97960/100000: episode: 1570, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 127.948, mean reward: 2.976 [1.979, 4.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.956, 10.100], loss: 0.089800, mae: 0.283888, mean_q: 3.843990
 97995/100000: episode: 1571, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 87.967, mean reward: 2.513 [1.644, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.306, 10.215], loss: 0.071925, mae: 0.274395, mean_q: 3.890171
 98031/100000: episode: 1572, duration: 0.194s, episode steps: 36, steps per second: 185, episode reward: 90.437, mean reward: 2.512 [1.819, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.201, 10.422], loss: 0.074286, mae: 0.269966, mean_q: 3.897738
 98071/100000: episode: 1573, duration: 0.207s, episode steps: 40, steps per second: 193, episode reward: 137.823, mean reward: 3.446 [1.846, 8.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.076, 10.577], loss: 0.075643, mae: 0.274419, mean_q: 3.872287
 98107/100000: episode: 1574, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 99.383, mean reward: 2.761 [1.865, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-1.490, 10.500], loss: 0.083242, mae: 0.277050, mean_q: 3.914290
 98118/100000: episode: 1575, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 28.200, mean reward: 2.564 [2.096, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.387], loss: 0.067041, mae: 0.264579, mean_q: 3.898347
 98158/100000: episode: 1576, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 167.554, mean reward: 4.189 [1.829, 12.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.190, 10.299], loss: 0.091870, mae: 0.292722, mean_q: 3.953280
 98193/100000: episode: 1577, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 90.339, mean reward: 2.581 [2.008, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.242, 10.393], loss: 0.094629, mae: 0.283108, mean_q: 3.958863
 98236/100000: episode: 1578, duration: 0.228s, episode steps: 43, steps per second: 188, episode reward: 126.029, mean reward: 2.931 [1.927, 6.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.374, 10.100], loss: 0.121080, mae: 0.297371, mean_q: 3.980826
 98276/100000: episode: 1579, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 82.895, mean reward: 2.072 [1.476, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-1.175, 10.136], loss: 0.102328, mae: 0.307939, mean_q: 3.996642
 98287/100000: episode: 1580, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 35.124, mean reward: 3.193 [2.359, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.500], loss: 0.090943, mae: 0.310732, mean_q: 4.062449
 98327/100000: episode: 1581, duration: 0.228s, episode steps: 40, steps per second: 175, episode reward: 107.983, mean reward: 2.700 [1.943, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.851, 10.419], loss: 0.104780, mae: 0.302179, mean_q: 3.994133
 98367/100000: episode: 1582, duration: 0.218s, episode steps: 40, steps per second: 184, episode reward: 118.479, mean reward: 2.962 [2.182, 4.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.192, 10.465], loss: 0.163598, mae: 0.342814, mean_q: 4.092902
 98407/100000: episode: 1583, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 175.354, mean reward: 4.384 [2.648, 10.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.569, 10.564], loss: 0.117550, mae: 0.324900, mean_q: 4.078503
 98424/100000: episode: 1584, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 48.599, mean reward: 2.859 [1.789, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.395], loss: 0.096226, mae: 0.295241, mean_q: 4.094647
 98444/100000: episode: 1585, duration: 0.108s, episode steps: 20, steps per second: 184, episode reward: 51.148, mean reward: 2.557 [2.113, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.538, 10.473], loss: 0.116342, mae: 0.300197, mean_q: 4.111654
 98484/100000: episode: 1586, duration: 0.218s, episode steps: 40, steps per second: 184, episode reward: 98.316, mean reward: 2.458 [1.470, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.992, 10.100], loss: 0.178005, mae: 0.338275, mean_q: 4.075469
 98524/100000: episode: 1587, duration: 0.211s, episode steps: 40, steps per second: 190, episode reward: 96.739, mean reward: 2.418 [1.779, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.070, 10.379], loss: 0.112209, mae: 0.316837, mean_q: 4.082509
 98564/100000: episode: 1588, duration: 0.209s, episode steps: 40, steps per second: 191, episode reward: 112.893, mean reward: 2.822 [1.542, 6.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-2.146, 10.108], loss: 0.120352, mae: 0.321374, mean_q: 4.070598
 98599/100000: episode: 1589, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 110.657, mean reward: 3.162 [1.920, 4.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.481, 10.543], loss: 0.139479, mae: 0.325443, mean_q: 4.139261
 98642/100000: episode: 1590, duration: 0.229s, episode steps: 43, steps per second: 187, episode reward: 156.480, mean reward: 3.639 [2.323, 8.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.408, 10.100], loss: 0.154279, mae: 0.355484, mean_q: 4.135208
 98682/100000: episode: 1591, duration: 0.228s, episode steps: 40, steps per second: 175, episode reward: 91.078, mean reward: 2.277 [1.791, 2.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.192, 10.286], loss: 0.132640, mae: 0.348481, mean_q: 4.208788
 98716/100000: episode: 1592, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 85.716, mean reward: 2.521 [1.648, 4.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.655, 10.231], loss: 0.184734, mae: 0.366788, mean_q: 4.196989
 98736/100000: episode: 1593, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 58.521, mean reward: 2.926 [2.039, 5.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.358, 10.399], loss: 0.165324, mae: 0.381813, mean_q: 4.243381
 98776/100000: episode: 1594, duration: 0.230s, episode steps: 40, steps per second: 174, episode reward: 99.510, mean reward: 2.488 [1.797, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.371, 10.328], loss: 0.134782, mae: 0.337195, mean_q: 4.176246
 98816/100000: episode: 1595, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 111.758, mean reward: 2.794 [1.935, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.094, 10.431], loss: 0.129380, mae: 0.341579, mean_q: 4.193696
 98852/100000: episode: 1596, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 77.581, mean reward: 2.155 [1.582, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.921, 10.165], loss: 0.126441, mae: 0.330319, mean_q: 4.240453
 98869/100000: episode: 1597, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 37.574, mean reward: 2.210 [1.903, 2.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.309], loss: 0.109339, mae: 0.313570, mean_q: 4.097776
 98909/100000: episode: 1598, duration: 0.209s, episode steps: 40, steps per second: 191, episode reward: 124.045, mean reward: 3.101 [2.545, 5.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.593, 10.420], loss: 0.135189, mae: 0.352132, mean_q: 4.207900
 98920/100000: episode: 1599, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 29.027, mean reward: 2.639 [2.303, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.426], loss: 0.120275, mae: 0.337665, mean_q: 4.241329
 98931/100000: episode: 1600, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 25.926, mean reward: 2.357 [2.145, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.340], loss: 0.110579, mae: 0.322091, mean_q: 4.281216
 98971/100000: episode: 1601, duration: 0.236s, episode steps: 40, steps per second: 169, episode reward: 110.391, mean reward: 2.760 [2.155, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.366, 10.360], loss: 0.119162, mae: 0.340908, mean_q: 4.189523
 98990/100000: episode: 1602, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 72.702, mean reward: 3.826 [2.530, 5.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.516, 10.100], loss: 0.168763, mae: 0.353957, mean_q: 4.323073
 99024/100000: episode: 1603, duration: 0.184s, episode steps: 34, steps per second: 184, episode reward: 66.507, mean reward: 1.956 [1.480, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.143], loss: 0.136776, mae: 0.348159, mean_q: 4.292161
 99064/100000: episode: 1604, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 113.877, mean reward: 2.847 [1.663, 6.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.639, 10.227], loss: 0.139162, mae: 0.344242, mean_q: 4.240551
 99083/100000: episode: 1605, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 61.198, mean reward: 3.221 [2.205, 8.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.806, 10.100], loss: 0.208124, mae: 0.377711, mean_q: 4.202822
 99103/100000: episode: 1606, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 57.715, mean reward: 2.886 [2.224, 5.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.440, 10.422], loss: 0.149228, mae: 0.366974, mean_q: 4.324543
 99120/100000: episode: 1607, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 40.046, mean reward: 2.356 [1.787, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.127, 10.265], loss: 0.171067, mae: 0.403067, mean_q: 4.343230
 99140/100000: episode: 1608, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 42.156, mean reward: 2.108 [1.679, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.175, 10.301], loss: 0.115363, mae: 0.346627, mean_q: 4.281480
 99151/100000: episode: 1609, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 27.826, mean reward: 2.530 [2.137, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.854, 10.336], loss: 0.164435, mae: 0.350662, mean_q: 4.269548
 99191/100000: episode: 1610, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 113.664, mean reward: 2.842 [1.893, 4.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.361, 10.333], loss: 0.153515, mae: 0.367907, mean_q: 4.299725
 99211/100000: episode: 1611, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 42.834, mean reward: 2.142 [1.765, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.267], loss: 0.109753, mae: 0.321701, mean_q: 4.235379
 99228/100000: episode: 1612, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 37.278, mean reward: 2.193 [1.898, 2.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.085, 10.312], loss: 0.121937, mae: 0.336198, mean_q: 4.196310
 99264/100000: episode: 1613, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 83.875, mean reward: 2.330 [1.726, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-1.975, 10.229], loss: 0.240947, mae: 0.425023, mean_q: 4.356174
 99304/100000: episode: 1614, duration: 0.223s, episode steps: 40, steps per second: 179, episode reward: 96.080, mean reward: 2.402 [1.499, 6.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.818, 10.184], loss: 0.176203, mae: 0.374350, mean_q: 4.269643
 99344/100000: episode: 1615, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 116.956, mean reward: 2.924 [1.790, 5.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.370, 10.467], loss: 0.145783, mae: 0.357675, mean_q: 4.286159
 99384/100000: episode: 1616, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 94.042, mean reward: 2.351 [1.490, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-1.070, 10.169], loss: 0.173268, mae: 0.370420, mean_q: 4.374964
 99404/100000: episode: 1617, duration: 0.120s, episode steps: 20, steps per second: 166, episode reward: 56.186, mean reward: 2.809 [2.238, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.465], loss: 0.175117, mae: 0.379184, mean_q: 4.287805
 99444/100000: episode: 1618, duration: 0.218s, episode steps: 40, steps per second: 183, episode reward: 85.408, mean reward: 2.135 [1.437, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.293, 10.179], loss: 0.154304, mae: 0.366237, mean_q: 4.274705
 99480/100000: episode: 1619, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 119.372, mean reward: 3.316 [2.767, 5.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.574, 10.543], loss: 0.174355, mae: 0.388381, mean_q: 4.290077
 99500/100000: episode: 1620, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 48.043, mean reward: 2.402 [2.105, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.092, 10.398], loss: 0.202318, mae: 0.421380, mean_q: 4.361268
 99520/100000: episode: 1621, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 57.699, mean reward: 2.885 [2.016, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.011, 10.451], loss: 0.161966, mae: 0.368516, mean_q: 4.340463
 99560/100000: episode: 1622, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 110.759, mean reward: 2.769 [2.081, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.149, 10.342], loss: 0.145470, mae: 0.362981, mean_q: 4.319914
 99579/100000: episode: 1623, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 58.404, mean reward: 3.074 [2.294, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.572, 10.100], loss: 0.226909, mae: 0.398496, mean_q: 4.346272
 99613/100000: episode: 1624, duration: 0.161s, episode steps: 34, steps per second: 211, episode reward: 75.136, mean reward: 2.210 [1.763, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.394, 10.356], loss: 0.161515, mae: 0.377054, mean_q: 4.346321
 99624/100000: episode: 1625, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 33.664, mean reward: 3.060 [2.347, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.495], loss: 0.231058, mae: 0.418631, mean_q: 4.401154
 99641/100000: episode: 1626, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 34.619, mean reward: 2.036 [1.756, 2.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.304], loss: 0.151474, mae: 0.376163, mean_q: 4.431087
 99677/100000: episode: 1627, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 83.952, mean reward: 2.332 [1.658, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.406, 10.201], loss: 0.229331, mae: 0.417974, mean_q: 4.455674
 99717/100000: episode: 1628, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 101.670, mean reward: 2.542 [1.577, 4.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.742, 10.250], loss: 0.170888, mae: 0.384913, mean_q: 4.424592
 99728/100000: episode: 1629, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 23.808, mean reward: 2.164 [1.837, 2.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.908, 10.316], loss: 0.165140, mae: 0.375862, mean_q: 4.399839
 99747/100000: episode: 1630, duration: 0.091s, episode steps: 19, steps per second: 209, episode reward: 47.918, mean reward: 2.522 [2.200, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.521, 10.100], loss: 0.183195, mae: 0.398269, mean_q: 4.486194
 99781/100000: episode: 1631, duration: 0.165s, episode steps: 34, steps per second: 206, episode reward: 89.660, mean reward: 2.637 [1.790, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.140, 10.498], loss: 0.184781, mae: 0.379725, mean_q: 4.417281
 99817/100000: episode: 1632, duration: 0.176s, episode steps: 36, steps per second: 205, episode reward: 88.611, mean reward: 2.461 [1.832, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.464, 10.321], loss: 0.187426, mae: 0.417832, mean_q: 4.384508
 99853/100000: episode: 1633, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 89.990, mean reward: 2.500 [1.899, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.607, 10.323], loss: 0.143894, mae: 0.362207, mean_q: 4.391867
 99893/100000: episode: 1634, duration: 0.229s, episode steps: 40, steps per second: 175, episode reward: 117.458, mean reward: 2.936 [1.960, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.470, 10.391], loss: 0.161732, mae: 0.369771, mean_q: 4.424430
 99933/100000: episode: 1635, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 87.819, mean reward: 2.195 [1.593, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.489, 10.153], loss: 0.147858, mae: 0.365161, mean_q: 4.426986
 99953/100000: episode: 1636, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 43.379, mean reward: 2.169 [1.756, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.223, 10.247], loss: 0.192387, mae: 0.417547, mean_q: 4.514851
 99964/100000: episode: 1637, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 30.555, mean reward: 2.778 [2.195, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.491], loss: 0.349985, mae: 0.463230, mean_q: 4.662363
 100000/100000: episode: 1638, duration: 0.195s, episode steps: 36, steps per second: 184, episode reward: 80.486, mean reward: 2.236 [1.611, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.097, 10.201], loss: 0.180319, mae: 0.386853, mean_q: 4.452765
done, took 589.021 seconds
[Info] End Importance Splitting. Falsification occurred 5 times.
