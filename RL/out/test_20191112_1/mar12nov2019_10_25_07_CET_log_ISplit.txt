Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.173s, episode steps: 100, steps per second: 578, episode reward: 176.603, mean reward: 1.766 [1.453, 2.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.482, 10.189], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.062s, episode steps: 100, steps per second: 1615, episode reward: 192.665, mean reward: 1.927 [1.476, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.673, 10.310], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.063s, episode steps: 100, steps per second: 1598, episode reward: 207.064, mean reward: 2.071 [1.487, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.356, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.065s, episode steps: 100, steps per second: 1534, episode reward: 188.030, mean reward: 1.880 [1.450, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.668, 10.297], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 195.102, mean reward: 1.951 [1.445, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.948, 10.225], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.063s, episode steps: 100, steps per second: 1584, episode reward: 219.884, mean reward: 2.199 [1.439, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.836, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 183.409, mean reward: 1.834 [1.433, 2.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.689, 10.099], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.062s, episode steps: 100, steps per second: 1606, episode reward: 186.732, mean reward: 1.867 [1.453, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.422, 10.179], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.062s, episode steps: 100, steps per second: 1603, episode reward: 183.603, mean reward: 1.836 [1.458, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.738, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 189.502, mean reward: 1.895 [1.489, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.276, 10.288], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.062s, episode steps: 100, steps per second: 1603, episode reward: 176.513, mean reward: 1.765 [1.440, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.756, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.068s, episode steps: 100, steps per second: 1466, episode reward: 199.568, mean reward: 1.996 [1.504, 4.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.823, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 185.873, mean reward: 1.859 [1.447, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.453, 10.297], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.062s, episode steps: 100, steps per second: 1611, episode reward: 191.350, mean reward: 1.913 [1.463, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.579, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.062s, episode steps: 100, steps per second: 1605, episode reward: 200.247, mean reward: 2.002 [1.475, 4.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.648, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.065s, episode steps: 100, steps per second: 1549, episode reward: 219.366, mean reward: 2.194 [1.435, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.812, 10.371], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.062s, episode steps: 100, steps per second: 1617, episode reward: 179.377, mean reward: 1.794 [1.466, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.967, 10.201], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.062s, episode steps: 100, steps per second: 1601, episode reward: 214.121, mean reward: 2.141 [1.483, 10.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.863, 10.260], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.063s, episode steps: 100, steps per second: 1600, episode reward: 203.845, mean reward: 2.038 [1.463, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.800, 10.138], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.063s, episode steps: 100, steps per second: 1597, episode reward: 198.788, mean reward: 1.988 [1.536, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.462, 10.388], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.064s, episode steps: 100, steps per second: 1555, episode reward: 194.203, mean reward: 1.942 [1.478, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.210, 10.176], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.063s, episode steps: 100, steps per second: 1590, episode reward: 201.697, mean reward: 2.017 [1.503, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.388, 10.250], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.063s, episode steps: 100, steps per second: 1587, episode reward: 211.068, mean reward: 2.111 [1.495, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.662, 10.330], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.063s, episode steps: 100, steps per second: 1592, episode reward: 186.848, mean reward: 1.868 [1.527, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.829, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.065s, episode steps: 100, steps per second: 1539, episode reward: 182.153, mean reward: 1.822 [1.452, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.994, 10.244], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.063s, episode steps: 100, steps per second: 1585, episode reward: 184.711, mean reward: 1.847 [1.461, 2.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.161, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 187.309, mean reward: 1.873 [1.479, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.506, 10.230], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.064s, episode steps: 100, steps per second: 1563, episode reward: 187.262, mean reward: 1.873 [1.474, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.115, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.065s, episode steps: 100, steps per second: 1540, episode reward: 202.671, mean reward: 2.027 [1.453, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.250, 10.222], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.063s, episode steps: 100, steps per second: 1595, episode reward: 186.140, mean reward: 1.861 [1.432, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.988, 10.195], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.063s, episode steps: 100, steps per second: 1597, episode reward: 180.100, mean reward: 1.801 [1.445, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.980, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.063s, episode steps: 100, steps per second: 1594, episode reward: 181.359, mean reward: 1.814 [1.458, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.462, 10.118], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 200.137, mean reward: 2.001 [1.433, 5.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.995, 10.440], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.063s, episode steps: 100, steps per second: 1599, episode reward: 199.771, mean reward: 1.998 [1.492, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.598, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.063s, episode steps: 100, steps per second: 1589, episode reward: 180.574, mean reward: 1.806 [1.469, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.765, 10.101], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.062s, episode steps: 100, steps per second: 1605, episode reward: 212.247, mean reward: 2.122 [1.459, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.345, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.064s, episode steps: 100, steps per second: 1555, episode reward: 193.083, mean reward: 1.931 [1.449, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.968, 10.368], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.063s, episode steps: 100, steps per second: 1597, episode reward: 185.828, mean reward: 1.858 [1.456, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.668, 10.243], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.063s, episode steps: 100, steps per second: 1595, episode reward: 192.735, mean reward: 1.927 [1.449, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.080, 10.182], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.063s, episode steps: 100, steps per second: 1596, episode reward: 235.322, mean reward: 2.353 [1.551, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.556, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.064s, episode steps: 100, steps per second: 1575, episode reward: 303.401, mean reward: 3.034 [1.480, 7.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.433, 10.509], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.063s, episode steps: 100, steps per second: 1590, episode reward: 204.400, mean reward: 2.044 [1.473, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.558, 10.483], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.062s, episode steps: 100, steps per second: 1612, episode reward: 189.968, mean reward: 1.900 [1.447, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.879, 10.104], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.062s, episode steps: 100, steps per second: 1605, episode reward: 192.042, mean reward: 1.920 [1.452, 2.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.898, 10.368], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.063s, episode steps: 100, steps per second: 1590, episode reward: 186.745, mean reward: 1.867 [1.459, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.677, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.064s, episode steps: 100, steps per second: 1570, episode reward: 199.889, mean reward: 1.999 [1.455, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.540, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.062s, episode steps: 100, steps per second: 1609, episode reward: 209.302, mean reward: 2.093 [1.457, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.552, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.062s, episode steps: 100, steps per second: 1602, episode reward: 210.629, mean reward: 2.106 [1.464, 5.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.958, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.066s, episode steps: 100, steps per second: 1519, episode reward: 231.140, mean reward: 2.311 [1.554, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.222, 10.338], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.063s, episode steps: 100, steps per second: 1589, episode reward: 198.951, mean reward: 1.990 [1.465, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.201, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.162s, episode steps: 100, steps per second: 86, episode reward: 205.936, mean reward: 2.059 [1.450, 4.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.777, 10.098], loss: 0.219331, mae: 0.446873, mean_q: 3.252539
  5200/100000: episode: 52, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 194.012, mean reward: 1.940 [1.465, 4.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.386, 10.098], loss: 0.167165, mae: 0.363590, mean_q: 3.432074
  5300/100000: episode: 53, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 186.907, mean reward: 1.869 [1.474, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.701, 10.212], loss: 0.155130, mae: 0.359823, mean_q: 3.585272
  5400/100000: episode: 54, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 219.957, mean reward: 2.200 [1.531, 5.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.024, 10.098], loss: 0.142111, mae: 0.352849, mean_q: 3.705504
  5500/100000: episode: 55, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 198.374, mean reward: 1.984 [1.456, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.704, 10.119], loss: 0.163145, mae: 0.364436, mean_q: 3.807389
  5600/100000: episode: 56, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 190.542, mean reward: 1.905 [1.468, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.003, 10.098], loss: 0.139269, mae: 0.341155, mean_q: 3.836635
  5700/100000: episode: 57, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 193.700, mean reward: 1.937 [1.451, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.656, 10.360], loss: 0.146101, mae: 0.348664, mean_q: 3.872088
  5800/100000: episode: 58, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 191.649, mean reward: 1.916 [1.457, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.582, 10.098], loss: 0.146565, mae: 0.344003, mean_q: 3.898316
  5900/100000: episode: 59, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 228.589, mean reward: 2.286 [1.477, 4.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.304, 10.098], loss: 0.151278, mae: 0.352432, mean_q: 3.909313
  6000/100000: episode: 60, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 204.531, mean reward: 2.045 [1.561, 5.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.844, 10.098], loss: 0.163770, mae: 0.354711, mean_q: 3.944713
  6100/100000: episode: 61, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 189.418, mean reward: 1.894 [1.445, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.233, 10.098], loss: 0.138789, mae: 0.356899, mean_q: 3.939908
  6200/100000: episode: 62, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 190.495, mean reward: 1.905 [1.489, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.629, 10.098], loss: 0.130514, mae: 0.345796, mean_q: 3.946835
  6300/100000: episode: 63, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 198.853, mean reward: 1.989 [1.456, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.262, 10.333], loss: 0.134110, mae: 0.353881, mean_q: 3.951421
  6400/100000: episode: 64, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.022, mean reward: 1.950 [1.446, 6.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.906, 10.098], loss: 0.135881, mae: 0.344757, mean_q: 3.925162
  6500/100000: episode: 65, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 221.905, mean reward: 2.219 [1.433, 6.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.418, 10.098], loss: 0.142712, mae: 0.355714, mean_q: 3.953001
  6600/100000: episode: 66, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 200.841, mean reward: 2.008 [1.481, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.229, 10.098], loss: 0.171289, mae: 0.364352, mean_q: 3.949535
  6700/100000: episode: 67, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 190.071, mean reward: 1.901 [1.432, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.561, 10.098], loss: 0.158512, mae: 0.354717, mean_q: 3.959674
  6800/100000: episode: 68, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 198.296, mean reward: 1.983 [1.478, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.993, 10.275], loss: 0.140020, mae: 0.353001, mean_q: 3.952670
  6900/100000: episode: 69, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 217.976, mean reward: 2.180 [1.449, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.278, 10.315], loss: 0.133193, mae: 0.333654, mean_q: 3.961353
  7000/100000: episode: 70, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 207.732, mean reward: 2.077 [1.490, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.652, 10.380], loss: 0.155673, mae: 0.371765, mean_q: 3.986771
  7100/100000: episode: 71, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 190.967, mean reward: 1.910 [1.446, 3.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.855, 10.257], loss: 0.125605, mae: 0.334800, mean_q: 3.945693
  7200/100000: episode: 72, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 204.311, mean reward: 2.043 [1.447, 7.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.071, 10.098], loss: 0.141564, mae: 0.351942, mean_q: 3.980923
  7300/100000: episode: 73, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 185.942, mean reward: 1.859 [1.439, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.045, 10.190], loss: 0.141593, mae: 0.339670, mean_q: 3.949716
  7400/100000: episode: 74, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 195.035, mean reward: 1.950 [1.479, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.515, 10.320], loss: 0.145125, mae: 0.353526, mean_q: 3.961783
  7500/100000: episode: 75, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 190.367, mean reward: 1.904 [1.499, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.717, 10.179], loss: 0.158425, mae: 0.366488, mean_q: 3.973444
  7600/100000: episode: 76, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.500, mean reward: 1.875 [1.454, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.564, 10.122], loss: 0.163472, mae: 0.371678, mean_q: 3.973470
  7700/100000: episode: 77, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 201.617, mean reward: 2.016 [1.472, 6.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.520, 10.148], loss: 0.135985, mae: 0.343271, mean_q: 3.970973
  7800/100000: episode: 78, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 195.389, mean reward: 1.954 [1.455, 4.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.535, 10.304], loss: 0.146475, mae: 0.347677, mean_q: 3.976825
  7900/100000: episode: 79, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 200.897, mean reward: 2.009 [1.458, 3.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.791, 10.335], loss: 0.156439, mae: 0.350701, mean_q: 3.973409
  8000/100000: episode: 80, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 196.727, mean reward: 1.967 [1.494, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.279, 10.430], loss: 0.141411, mae: 0.352174, mean_q: 3.981696
  8100/100000: episode: 81, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 191.253, mean reward: 1.913 [1.481, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.506, 10.223], loss: 0.136793, mae: 0.346143, mean_q: 3.964605
  8200/100000: episode: 82, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 206.615, mean reward: 2.066 [1.503, 4.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.961, 10.098], loss: 0.152241, mae: 0.361296, mean_q: 3.996498
  8300/100000: episode: 83, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 205.840, mean reward: 2.058 [1.581, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.596, 10.366], loss: 0.149393, mae: 0.357572, mean_q: 4.002268
  8400/100000: episode: 84, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 192.988, mean reward: 1.930 [1.461, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.869, 10.162], loss: 0.144784, mae: 0.351409, mean_q: 3.993528
  8500/100000: episode: 85, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 286.785, mean reward: 2.868 [1.452, 12.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.252, 10.098], loss: 0.164245, mae: 0.369118, mean_q: 4.017219
  8600/100000: episode: 86, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 196.048, mean reward: 1.960 [1.444, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.842, 10.098], loss: 0.182810, mae: 0.369507, mean_q: 4.003576
  8700/100000: episode: 87, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 187.730, mean reward: 1.877 [1.493, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.778, 10.303], loss: 0.155595, mae: 0.360465, mean_q: 4.023859
  8800/100000: episode: 88, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 182.816, mean reward: 1.828 [1.441, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.029, 10.098], loss: 0.241367, mae: 0.410837, mean_q: 4.079823
  8900/100000: episode: 89, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.100, mean reward: 1.921 [1.451, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.788, 10.098], loss: 0.187290, mae: 0.384428, mean_q: 4.021358
  9000/100000: episode: 90, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 187.642, mean reward: 1.876 [1.486, 2.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.613, 10.098], loss: 0.151454, mae: 0.361758, mean_q: 3.994473
  9100/100000: episode: 91, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 224.542, mean reward: 2.245 [1.433, 11.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.318], loss: 0.173374, mae: 0.362556, mean_q: 3.989424
  9200/100000: episode: 92, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 216.171, mean reward: 2.162 [1.443, 6.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.723, 10.098], loss: 0.149749, mae: 0.347905, mean_q: 3.970964
  9300/100000: episode: 93, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 190.786, mean reward: 1.908 [1.441, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.252, 10.325], loss: 0.156957, mae: 0.343245, mean_q: 3.964690
  9400/100000: episode: 94, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 184.628, mean reward: 1.846 [1.461, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.707, 10.240], loss: 0.168633, mae: 0.358866, mean_q: 4.004758
  9500/100000: episode: 95, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 218.243, mean reward: 2.182 [1.516, 6.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.159, 10.098], loss: 0.136208, mae: 0.338629, mean_q: 3.989957
  9600/100000: episode: 96, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 192.744, mean reward: 1.927 [1.446, 6.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.045, 10.208], loss: 0.148421, mae: 0.341829, mean_q: 3.967008
  9700/100000: episode: 97, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 188.132, mean reward: 1.881 [1.485, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.875, 10.183], loss: 0.152841, mae: 0.346817, mean_q: 3.983251
  9800/100000: episode: 98, duration: 0.468s, episode steps: 100, steps per second: 214, episode reward: 192.493, mean reward: 1.925 [1.483, 6.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.814, 10.098], loss: 0.178482, mae: 0.373638, mean_q: 4.001342
  9900/100000: episode: 99, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 203.589, mean reward: 2.036 [1.465, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.914, 10.252], loss: 0.121595, mae: 0.325377, mean_q: 3.934891
 10000/100000: episode: 100, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 188.831, mean reward: 1.888 [1.469, 2.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.135, 10.262], loss: 0.134900, mae: 0.340021, mean_q: 3.946939
 10100/100000: episode: 101, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.370, mean reward: 1.914 [1.438, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.548, 10.098], loss: 0.151507, mae: 0.342389, mean_q: 3.946866
 10200/100000: episode: 102, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 193.378, mean reward: 1.934 [1.445, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.644, 10.098], loss: 0.178672, mae: 0.350846, mean_q: 3.954469
 10300/100000: episode: 103, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 202.879, mean reward: 2.029 [1.463, 3.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.409, 10.385], loss: 0.180825, mae: 0.345162, mean_q: 3.958369
 10400/100000: episode: 104, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 191.886, mean reward: 1.919 [1.460, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.625, 10.098], loss: 0.139668, mae: 0.342667, mean_q: 3.924554
 10500/100000: episode: 105, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 181.533, mean reward: 1.815 [1.480, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.653, 10.418], loss: 0.164467, mae: 0.346659, mean_q: 3.956151
 10600/100000: episode: 106, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.693, mean reward: 1.867 [1.461, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.579, 10.098], loss: 0.170168, mae: 0.350571, mean_q: 3.941692
 10700/100000: episode: 107, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 192.374, mean reward: 1.924 [1.467, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.896, 10.416], loss: 0.147876, mae: 0.342039, mean_q: 3.943059
 10800/100000: episode: 108, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 185.136, mean reward: 1.851 [1.449, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.907, 10.179], loss: 0.161121, mae: 0.342163, mean_q: 3.937814
 10900/100000: episode: 109, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 195.720, mean reward: 1.957 [1.460, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.878, 10.326], loss: 0.173818, mae: 0.351691, mean_q: 3.935474
 11000/100000: episode: 110, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 206.188, mean reward: 2.062 [1.466, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.334, 10.213], loss: 0.126711, mae: 0.331690, mean_q: 3.923816
 11100/100000: episode: 111, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 219.349, mean reward: 2.193 [1.454, 5.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.132, 10.098], loss: 0.159394, mae: 0.351646, mean_q: 3.918110
 11200/100000: episode: 112, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 194.282, mean reward: 1.943 [1.457, 3.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.598, 10.098], loss: 0.126087, mae: 0.323833, mean_q: 3.914933
 11300/100000: episode: 113, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 190.612, mean reward: 1.906 [1.482, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.071, 10.209], loss: 0.157727, mae: 0.333634, mean_q: 3.916394
 11400/100000: episode: 114, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 218.578, mean reward: 2.186 [1.469, 6.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.521, 10.098], loss: 0.149609, mae: 0.338761, mean_q: 3.930516
 11500/100000: episode: 115, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 197.164, mean reward: 1.972 [1.480, 3.819], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.210], loss: 0.126837, mae: 0.317243, mean_q: 3.895891
 11600/100000: episode: 116, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 183.518, mean reward: 1.835 [1.465, 2.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.265, 10.098], loss: 0.122711, mae: 0.330813, mean_q: 3.919807
 11700/100000: episode: 117, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 207.237, mean reward: 2.072 [1.434, 4.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.437, 10.375], loss: 0.138257, mae: 0.341903, mean_q: 3.922667
 11800/100000: episode: 118, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 212.128, mean reward: 2.121 [1.507, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.662, 10.098], loss: 0.146163, mae: 0.338174, mean_q: 3.918042
 11900/100000: episode: 119, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 185.713, mean reward: 1.857 [1.438, 2.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.088, 10.098], loss: 0.174644, mae: 0.357605, mean_q: 3.931417
 12000/100000: episode: 120, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 219.432, mean reward: 2.194 [1.512, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.112, 10.098], loss: 0.142806, mae: 0.339440, mean_q: 3.923600
 12100/100000: episode: 121, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 209.957, mean reward: 2.100 [1.491, 5.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.094, 10.098], loss: 0.123995, mae: 0.331360, mean_q: 3.917606
 12200/100000: episode: 122, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 193.501, mean reward: 1.935 [1.480, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.703, 10.137], loss: 0.136188, mae: 0.343133, mean_q: 3.935971
 12300/100000: episode: 123, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 191.878, mean reward: 1.919 [1.437, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.180], loss: 0.139459, mae: 0.338375, mean_q: 3.922498
 12400/100000: episode: 124, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 189.228, mean reward: 1.892 [1.445, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.375, 10.098], loss: 0.176542, mae: 0.363782, mean_q: 3.938999
 12500/100000: episode: 125, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 193.966, mean reward: 1.940 [1.452, 4.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.681, 10.098], loss: 0.148942, mae: 0.343647, mean_q: 3.931410
 12600/100000: episode: 126, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 196.384, mean reward: 1.964 [1.484, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.451, 10.098], loss: 0.171642, mae: 0.351774, mean_q: 3.936433
 12700/100000: episode: 127, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 189.196, mean reward: 1.892 [1.448, 5.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.556, 10.273], loss: 0.116931, mae: 0.327551, mean_q: 3.908469
 12800/100000: episode: 128, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 175.110, mean reward: 1.751 [1.472, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.567, 10.098], loss: 0.154501, mae: 0.339208, mean_q: 3.925653
 12900/100000: episode: 129, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 195.902, mean reward: 1.959 [1.451, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.637, 10.384], loss: 0.141336, mae: 0.344418, mean_q: 3.918056
 13000/100000: episode: 130, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 181.345, mean reward: 1.813 [1.474, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.766, 10.098], loss: 0.123952, mae: 0.337684, mean_q: 3.902844
 13100/100000: episode: 131, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 231.554, mean reward: 2.316 [1.466, 4.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.423, 10.398], loss: 0.190329, mae: 0.358498, mean_q: 3.935171
 13200/100000: episode: 132, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.969, mean reward: 1.900 [1.461, 4.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.061, 10.160], loss: 0.168205, mae: 0.352631, mean_q: 3.930995
 13300/100000: episode: 133, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 186.359, mean reward: 1.864 [1.459, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.006, 10.128], loss: 0.117597, mae: 0.322681, mean_q: 3.908526
 13400/100000: episode: 134, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 195.839, mean reward: 1.958 [1.506, 3.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.717, 10.115], loss: 0.158629, mae: 0.337814, mean_q: 3.922246
 13500/100000: episode: 135, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 197.653, mean reward: 1.977 [1.505, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.392, 10.116], loss: 0.126132, mae: 0.324664, mean_q: 3.906724
 13600/100000: episode: 136, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 207.476, mean reward: 2.075 [1.460, 5.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.671, 10.275], loss: 0.137580, mae: 0.335665, mean_q: 3.886093
 13700/100000: episode: 137, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 196.912, mean reward: 1.969 [1.435, 3.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.821, 10.293], loss: 0.142756, mae: 0.330165, mean_q: 3.892788
 13800/100000: episode: 138, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 201.876, mean reward: 2.019 [1.480, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.618, 10.432], loss: 0.114055, mae: 0.321402, mean_q: 3.897260
 13900/100000: episode: 139, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 197.109, mean reward: 1.971 [1.534, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.865, 10.098], loss: 0.119294, mae: 0.321797, mean_q: 3.898808
 14000/100000: episode: 140, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 182.294, mean reward: 1.823 [1.451, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.368, 10.187], loss: 0.108089, mae: 0.318046, mean_q: 3.903037
 14100/100000: episode: 141, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 190.534, mean reward: 1.905 [1.459, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.858, 10.098], loss: 0.114457, mae: 0.316174, mean_q: 3.897566
 14200/100000: episode: 142, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 193.144, mean reward: 1.931 [1.456, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.836, 10.098], loss: 0.100255, mae: 0.309603, mean_q: 3.877039
 14300/100000: episode: 143, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.240, mean reward: 1.952 [1.494, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.968, 10.183], loss: 0.101314, mae: 0.311651, mean_q: 3.885267
 14400/100000: episode: 144, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 184.263, mean reward: 1.843 [1.451, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.596, 10.098], loss: 0.108028, mae: 0.313781, mean_q: 3.878809
 14500/100000: episode: 145, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 204.160, mean reward: 2.042 [1.453, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.924, 10.098], loss: 0.121428, mae: 0.321786, mean_q: 3.884159
 14600/100000: episode: 146, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 183.715, mean reward: 1.837 [1.475, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.889, 10.262], loss: 0.109006, mae: 0.321619, mean_q: 3.871896
 14700/100000: episode: 147, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 200.670, mean reward: 2.007 [1.476, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.385, 10.098], loss: 0.111802, mae: 0.317346, mean_q: 3.885176
 14800/100000: episode: 148, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 204.629, mean reward: 2.046 [1.466, 4.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.833, 10.178], loss: 0.101203, mae: 0.314083, mean_q: 3.883407
 14900/100000: episode: 149, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 179.614, mean reward: 1.796 [1.441, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.073, 10.098], loss: 0.101583, mae: 0.308248, mean_q: 3.864865
[Info] 1-TH LEVEL FOUND: 4.704450607299805, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.052s, episode steps: 100, steps per second: 20, episode reward: 197.465, mean reward: 1.975 [1.485, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.877, 10.098], loss: 0.116394, mae: 0.325215, mean_q: 3.876687
 15014/100000: episode: 151, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 34.512, mean reward: 2.465 [1.850, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.303, 10.100], loss: 0.134592, mae: 0.353028, mean_q: 3.930415
 15032/100000: episode: 152, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 57.647, mean reward: 3.203 [2.745, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.237, 10.100], loss: 0.111436, mae: 0.321993, mean_q: 3.893890
 15044/100000: episode: 153, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 23.855, mean reward: 1.988 [1.742, 2.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.306, 10.100], loss: 0.125756, mae: 0.333290, mean_q: 3.865847
 15058/100000: episode: 154, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 41.548, mean reward: 2.968 [2.112, 5.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.411, 10.100], loss: 0.090474, mae: 0.293619, mean_q: 3.861957
 15084/100000: episode: 155, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 48.920, mean reward: 1.882 [1.435, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.104, 10.360], loss: 0.124466, mae: 0.333466, mean_q: 3.883946
 15100/100000: episode: 156, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 47.866, mean reward: 2.992 [2.049, 5.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.264, 10.100], loss: 0.087691, mae: 0.293217, mean_q: 3.884616
 15114/100000: episode: 157, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 31.067, mean reward: 2.219 [1.775, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.292, 10.100], loss: 0.112301, mae: 0.322920, mean_q: 3.817112
 15134/100000: episode: 158, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 73.241, mean reward: 3.662 [2.550, 5.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.904, 10.100], loss: 0.117495, mae: 0.328542, mean_q: 3.902572
 15146/100000: episode: 159, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 24.836, mean reward: 2.070 [1.663, 2.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.619, 10.100], loss: 0.123353, mae: 0.348789, mean_q: 3.908246
 15166/100000: episode: 160, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 63.059, mean reward: 3.153 [2.227, 3.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.252, 10.100], loss: 0.109404, mae: 0.321082, mean_q: 3.878628
 15180/100000: episode: 161, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 38.515, mean reward: 2.751 [2.367, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.459, 10.100], loss: 0.105200, mae: 0.318026, mean_q: 3.904282
 15206/100000: episode: 162, duration: 0.126s, episode steps: 26, steps per second: 207, episode reward: 56.132, mean reward: 2.159 [1.489, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.331, 10.326], loss: 0.145993, mae: 0.354404, mean_q: 3.914407
 15220/100000: episode: 163, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 48.268, mean reward: 3.448 [2.401, 4.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.300, 10.100], loss: 0.102010, mae: 0.316604, mean_q: 3.895845
 15255/100000: episode: 164, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 65.568, mean reward: 1.873 [1.523, 2.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.991, 10.100], loss: 0.136261, mae: 0.343864, mean_q: 3.910785
 15281/100000: episode: 165, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 66.360, mean reward: 2.552 [1.721, 4.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.431], loss: 0.104486, mae: 0.311589, mean_q: 3.871801
 15293/100000: episode: 166, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 29.557, mean reward: 2.463 [1.889, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-1.219, 10.100], loss: 0.102797, mae: 0.303286, mean_q: 3.861158
 15311/100000: episode: 167, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 38.294, mean reward: 2.127 [1.790, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.349, 10.100], loss: 0.105704, mae: 0.316292, mean_q: 3.881723
 15323/100000: episode: 168, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 24.907, mean reward: 2.076 [1.663, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.199, 10.100], loss: 0.133952, mae: 0.350295, mean_q: 3.973218
 15358/100000: episode: 169, duration: 0.169s, episode steps: 35, steps per second: 207, episode reward: 73.317, mean reward: 2.095 [1.712, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.142, 10.100], loss: 0.122859, mae: 0.333856, mean_q: 3.924164
 15426/100000: episode: 170, duration: 0.331s, episode steps: 68, steps per second: 206, episode reward: 129.099, mean reward: 1.899 [1.467, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.762 [-1.043, 10.176], loss: 0.144164, mae: 0.359619, mean_q: 3.927999
 15446/100000: episode: 171, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 68.352, mean reward: 3.418 [2.758, 4.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.324, 10.100], loss: 0.113602, mae: 0.329989, mean_q: 3.898767
 15458/100000: episode: 172, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 32.555, mean reward: 2.713 [2.216, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.300, 10.100], loss: 0.152517, mae: 0.348304, mean_q: 3.970529
 15484/100000: episode: 173, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 49.025, mean reward: 1.886 [1.490, 2.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.273, 10.251], loss: 0.143175, mae: 0.354529, mean_q: 3.929482
 15502/100000: episode: 174, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 52.723, mean reward: 2.929 [2.314, 5.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.462, 10.100], loss: 0.156042, mae: 0.362442, mean_q: 3.920930
 15514/100000: episode: 175, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 24.502, mean reward: 2.042 [1.746, 2.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.110, 10.100], loss: 0.115737, mae: 0.341613, mean_q: 3.950546
 15530/100000: episode: 176, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 33.745, mean reward: 2.109 [1.691, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.146, 10.100], loss: 0.128964, mae: 0.340347, mean_q: 3.939996
 15567/100000: episode: 177, duration: 0.183s, episode steps: 37, steps per second: 202, episode reward: 89.431, mean reward: 2.417 [1.706, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.035, 10.271], loss: 0.126488, mae: 0.333328, mean_q: 3.910127
 15579/100000: episode: 178, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 30.759, mean reward: 2.563 [2.038, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.679, 10.100], loss: 0.144084, mae: 0.371578, mean_q: 3.950914
 15599/100000: episode: 179, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 48.230, mean reward: 2.412 [1.891, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.414, 10.100], loss: 0.137134, mae: 0.354723, mean_q: 3.959747
 15636/100000: episode: 180, duration: 0.188s, episode steps: 37, steps per second: 196, episode reward: 86.530, mean reward: 2.339 [1.489, 5.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.809, 10.103], loss: 0.143989, mae: 0.365293, mean_q: 3.972907
 15736/100000: episode: 181, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 195.435, mean reward: 1.954 [1.466, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-0.198, 10.100], loss: 0.133033, mae: 0.348405, mean_q: 3.955240
 15752/100000: episode: 182, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 32.699, mean reward: 2.044 [1.610, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.388, 10.100], loss: 0.159053, mae: 0.389764, mean_q: 3.990079
 15820/100000: episode: 183, duration: 0.326s, episode steps: 68, steps per second: 208, episode reward: 131.912, mean reward: 1.940 [1.603, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.753 [-1.080, 10.100], loss: 0.136060, mae: 0.357077, mean_q: 3.959249
 15857/100000: episode: 184, duration: 0.182s, episode steps: 37, steps per second: 203, episode reward: 75.969, mean reward: 2.053 [1.501, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.692, 10.125], loss: 0.118736, mae: 0.342706, mean_q: 3.961480
 15892/100000: episode: 185, duration: 0.172s, episode steps: 35, steps per second: 204, episode reward: 81.131, mean reward: 2.318 [1.660, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.823, 10.100], loss: 0.134169, mae: 0.348798, mean_q: 3.965045
 15992/100000: episode: 186, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 197.530, mean reward: 1.975 [1.470, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-0.873, 10.253], loss: 0.141055, mae: 0.357289, mean_q: 3.969572
 16092/100000: episode: 187, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 180.057, mean reward: 1.801 [1.443, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.390, 10.100], loss: 0.140297, mae: 0.362515, mean_q: 4.003788
 16118/100000: episode: 188, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 47.656, mean reward: 1.833 [1.519, 2.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.216, 10.100], loss: 0.115581, mae: 0.328989, mean_q: 3.997184
 16132/100000: episode: 189, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 35.393, mean reward: 2.528 [1.973, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.445, 10.100], loss: 0.136832, mae: 0.356028, mean_q: 3.968914
 16232/100000: episode: 190, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 201.728, mean reward: 2.017 [1.493, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-0.577, 10.191], loss: 0.127951, mae: 0.348508, mean_q: 3.991539
 16246/100000: episode: 191, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 39.745, mean reward: 2.839 [2.045, 4.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.305, 10.100], loss: 0.121771, mae: 0.329237, mean_q: 4.015300
 16283/100000: episode: 192, duration: 0.181s, episode steps: 37, steps per second: 205, episode reward: 83.843, mean reward: 2.266 [1.785, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.260, 10.329], loss: 0.156091, mae: 0.371149, mean_q: 3.997533
 16299/100000: episode: 193, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 32.286, mean reward: 2.018 [1.698, 2.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.232, 10.100], loss: 0.120637, mae: 0.347403, mean_q: 4.010513
 16336/100000: episode: 194, duration: 0.182s, episode steps: 37, steps per second: 203, episode reward: 83.942, mean reward: 2.269 [1.686, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.588, 10.268], loss: 0.141208, mae: 0.351894, mean_q: 3.948549
 16356/100000: episode: 195, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 51.471, mean reward: 2.574 [1.995, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.446, 10.100], loss: 0.112110, mae: 0.339428, mean_q: 4.031987
 16393/100000: episode: 196, duration: 0.192s, episode steps: 37, steps per second: 193, episode reward: 79.348, mean reward: 2.145 [1.474, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.787, 10.411], loss: 0.124203, mae: 0.341798, mean_q: 3.959200
 16419/100000: episode: 197, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 51.877, mean reward: 1.995 [1.621, 2.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.212, 10.334], loss: 0.114744, mae: 0.328681, mean_q: 3.999452
 16456/100000: episode: 198, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 99.989, mean reward: 2.702 [1.649, 5.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.736, 10.281], loss: 0.127059, mae: 0.337831, mean_q: 3.951786
 16524/100000: episode: 199, duration: 0.334s, episode steps: 68, steps per second: 204, episode reward: 127.363, mean reward: 1.873 [1.451, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.759 [-0.501, 10.240], loss: 0.131552, mae: 0.354935, mean_q: 3.981086
 16561/100000: episode: 200, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 86.401, mean reward: 2.335 [1.500, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.608, 10.100], loss: 0.127371, mae: 0.349420, mean_q: 3.988510
 16596/100000: episode: 201, duration: 0.178s, episode steps: 35, steps per second: 196, episode reward: 75.594, mean reward: 2.160 [1.488, 5.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.358, 10.238], loss: 0.138165, mae: 0.365356, mean_q: 4.028708
 16612/100000: episode: 202, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 35.384, mean reward: 2.211 [1.483, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.066, 10.108], loss: 0.127178, mae: 0.355061, mean_q: 4.000684
 16628/100000: episode: 203, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 33.516, mean reward: 2.095 [1.823, 2.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.297, 10.100], loss: 0.129660, mae: 0.345196, mean_q: 4.024959
 16646/100000: episode: 204, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 47.918, mean reward: 2.662 [1.930, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.123, 10.100], loss: 0.138486, mae: 0.351924, mean_q: 3.986342
 16664/100000: episode: 205, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 51.672, mean reward: 2.871 [1.934, 5.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.372, 10.100], loss: 0.136469, mae: 0.346598, mean_q: 3.929199
 16732/100000: episode: 206, duration: 0.337s, episode steps: 68, steps per second: 202, episode reward: 126.730, mean reward: 1.864 [1.474, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.757 [-1.439, 10.178], loss: 0.131839, mae: 0.354839, mean_q: 4.005291
 16748/100000: episode: 207, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 35.984, mean reward: 2.249 [2.029, 2.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.344, 10.100], loss: 0.135999, mae: 0.342882, mean_q: 3.980338
 16764/100000: episode: 208, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 32.336, mean reward: 2.021 [1.526, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.990, 10.100], loss: 0.173747, mae: 0.399934, mean_q: 4.042208
 16776/100000: episode: 209, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 29.817, mean reward: 2.485 [2.133, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.218, 10.100], loss: 0.113487, mae: 0.331776, mean_q: 3.925797
 16790/100000: episode: 210, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 42.327, mean reward: 3.023 [2.320, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.170, 10.100], loss: 0.143354, mae: 0.357514, mean_q: 4.025234
 16825/100000: episode: 211, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 85.974, mean reward: 2.456 [1.742, 5.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.585, 10.100], loss: 0.140861, mae: 0.355312, mean_q: 4.002376
 16862/100000: episode: 212, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 77.054, mean reward: 2.083 [1.459, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.187, 10.100], loss: 0.149323, mae: 0.365500, mean_q: 4.021883
 16874/100000: episode: 213, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 24.638, mean reward: 2.053 [1.648, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.119, 10.100], loss: 0.172092, mae: 0.399905, mean_q: 4.016094
 16900/100000: episode: 214, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 44.249, mean reward: 1.702 [1.462, 2.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.281], loss: 0.167651, mae: 0.378705, mean_q: 3.979773
 16912/100000: episode: 215, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 23.247, mean reward: 1.937 [1.481, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.192, 10.100], loss: 0.156296, mae: 0.361084, mean_q: 4.001462
 16930/100000: episode: 216, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 53.487, mean reward: 2.972 [2.283, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.409, 10.100], loss: 0.167634, mae: 0.393827, mean_q: 4.057606
 16944/100000: episode: 217, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 33.749, mean reward: 2.411 [2.037, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.249, 10.100], loss: 0.123473, mae: 0.356717, mean_q: 4.036813
 16958/100000: episode: 218, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 39.194, mean reward: 2.800 [2.099, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.266, 10.100], loss: 0.133433, mae: 0.358459, mean_q: 4.058428
 16995/100000: episode: 219, duration: 0.186s, episode steps: 37, steps per second: 198, episode reward: 72.790, mean reward: 1.967 [1.540, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.503, 10.294], loss: 0.132185, mae: 0.346970, mean_q: 4.001929
 17021/100000: episode: 220, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 44.713, mean reward: 1.720 [1.451, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.544, 10.100], loss: 0.143118, mae: 0.355178, mean_q: 3.983401
 17039/100000: episode: 221, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 39.893, mean reward: 2.216 [1.767, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.519, 10.100], loss: 0.120188, mae: 0.343436, mean_q: 4.056006
 17107/100000: episode: 222, duration: 0.339s, episode steps: 68, steps per second: 200, episode reward: 132.672, mean reward: 1.951 [1.512, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.759 [-0.563, 10.100], loss: 0.140307, mae: 0.362541, mean_q: 3.987655
 17121/100000: episode: 223, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 45.048, mean reward: 3.218 [2.609, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.796, 10.100], loss: 0.174732, mae: 0.402208, mean_q: 4.068072
 17139/100000: episode: 224, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 42.080, mean reward: 2.338 [1.893, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.024, 10.100], loss: 0.154885, mae: 0.380087, mean_q: 4.048323
 17207/100000: episode: 225, duration: 0.342s, episode steps: 68, steps per second: 199, episode reward: 140.072, mean reward: 2.060 [1.456, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.758 [-0.754, 10.252], loss: 0.142984, mae: 0.366265, mean_q: 3.963140
 17244/100000: episode: 226, duration: 0.179s, episode steps: 37, steps per second: 207, episode reward: 77.881, mean reward: 2.105 [1.532, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.486, 10.271], loss: 0.149651, mae: 0.364644, mean_q: 4.018534
 17281/100000: episode: 227, duration: 0.193s, episode steps: 37, steps per second: 192, episode reward: 74.556, mean reward: 2.015 [1.452, 2.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.690, 10.100], loss: 0.143900, mae: 0.355977, mean_q: 4.006837
 17381/100000: episode: 228, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.039, mean reward: 1.910 [1.445, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.392, 10.278], loss: 0.132258, mae: 0.359674, mean_q: 4.004343
 17416/100000: episode: 229, duration: 0.174s, episode steps: 35, steps per second: 202, episode reward: 67.799, mean reward: 1.937 [1.597, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.035, 10.100], loss: 0.131498, mae: 0.350788, mean_q: 4.005014
 17430/100000: episode: 230, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 37.620, mean reward: 2.687 [2.275, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.234, 10.100], loss: 0.153835, mae: 0.376089, mean_q: 4.052845
 17444/100000: episode: 231, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 33.718, mean reward: 2.408 [1.816, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.364, 10.100], loss: 0.134955, mae: 0.353807, mean_q: 3.998381
 17460/100000: episode: 232, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 34.354, mean reward: 2.147 [1.859, 2.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.074, 10.100], loss: 0.129612, mae: 0.357655, mean_q: 4.082125
 17495/100000: episode: 233, duration: 0.167s, episode steps: 35, steps per second: 209, episode reward: 66.739, mean reward: 1.907 [1.446, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.035, 10.276], loss: 0.125837, mae: 0.342696, mean_q: 4.006583
 17563/100000: episode: 234, duration: 0.338s, episode steps: 68, steps per second: 201, episode reward: 129.109, mean reward: 1.899 [1.457, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.764 [-0.952, 10.118], loss: 0.132326, mae: 0.354619, mean_q: 4.034548
 17598/100000: episode: 235, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 72.724, mean reward: 2.078 [1.459, 5.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.308], loss: 0.147921, mae: 0.369129, mean_q: 4.052355
 17698/100000: episode: 236, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 194.955, mean reward: 1.950 [1.440, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.537, 10.100], loss: 0.143900, mae: 0.365645, mean_q: 4.025117
 17718/100000: episode: 237, duration: 0.098s, episode steps: 20, steps per second: 203, episode reward: 56.012, mean reward: 2.801 [2.069, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.193, 10.100], loss: 0.138255, mae: 0.359503, mean_q: 4.085755
 17755/100000: episode: 238, duration: 0.182s, episode steps: 37, steps per second: 203, episode reward: 79.111, mean reward: 2.138 [1.637, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.239, 10.352], loss: 0.168697, mae: 0.381331, mean_q: 4.080478
 17781/100000: episode: 239, duration: 0.128s, episode steps: 26, steps per second: 202, episode reward: 46.080, mean reward: 1.772 [1.481, 2.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.449, 10.117], loss: 0.150128, mae: 0.386563, mean_q: 4.051095
[Info] 2-TH LEVEL FOUND: 5.852388381958008, Considering 14/86 traces
 17797/100000: episode: 240, duration: 4.285s, episode steps: 16, steps per second: 4, episode reward: 33.792, mean reward: 2.112 [1.793, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.097, 10.100], loss: 0.154040, mae: 0.370091, mean_q: 4.054318
 17837/100000: episode: 241, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 135.210, mean reward: 3.380 [2.262, 6.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.801, 10.474], loss: 0.158932, mae: 0.390831, mean_q: 4.080746
 17877/100000: episode: 242, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 89.214, mean reward: 2.230 [1.488, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-1.437, 10.164], loss: 0.146329, mae: 0.370627, mean_q: 4.060353
 17917/100000: episode: 243, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 119.638, mean reward: 2.991 [1.984, 13.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.350, 10.323], loss: 0.191148, mae: 0.383337, mean_q: 4.092801
 17957/100000: episode: 244, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 127.927, mean reward: 3.198 [2.205, 7.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.220, 10.339], loss: 0.149973, mae: 0.374551, mean_q: 4.048897
 17997/100000: episode: 245, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 88.555, mean reward: 2.214 [1.703, 5.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-1.029, 10.259], loss: 0.157709, mae: 0.372045, mean_q: 4.097596
 18037/100000: episode: 246, duration: 0.193s, episode steps: 40, steps per second: 207, episode reward: 109.632, mean reward: 2.741 [1.899, 4.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.452, 10.309], loss: 0.165257, mae: 0.390541, mean_q: 4.055093
 18077/100000: episode: 247, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 106.631, mean reward: 2.666 [2.136, 3.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.229, 10.346], loss: 0.147964, mae: 0.372488, mean_q: 4.040685
 18117/100000: episode: 248, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 101.036, mean reward: 2.526 [1.846, 6.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.720, 10.384], loss: 0.175586, mae: 0.389508, mean_q: 4.126046
 18157/100000: episode: 249, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 106.628, mean reward: 2.666 [2.159, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.572, 10.423], loss: 0.162729, mae: 0.376450, mean_q: 4.094285
 18197/100000: episode: 250, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 131.294, mean reward: 3.282 [2.131, 9.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.313, 10.484], loss: 0.185641, mae: 0.382297, mean_q: 4.102435
 18237/100000: episode: 251, duration: 0.209s, episode steps: 40, steps per second: 191, episode reward: 119.267, mean reward: 2.982 [1.834, 5.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.136, 10.266], loss: 0.164978, mae: 0.385162, mean_q: 4.104822
 18277/100000: episode: 252, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 81.476, mean reward: 2.037 [1.535, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.242, 10.105], loss: 0.165212, mae: 0.409066, mean_q: 4.119227
 18317/100000: episode: 253, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 117.125, mean reward: 2.928 [1.449, 7.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.704, 10.249], loss: 0.151705, mae: 0.373356, mean_q: 4.131862
 18357/100000: episode: 254, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 86.952, mean reward: 2.174 [1.691, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.122, 10.216], loss: 0.142663, mae: 0.359257, mean_q: 4.135299
 18397/100000: episode: 255, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 107.515, mean reward: 2.688 [2.028, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.538, 10.365], loss: 0.189520, mae: 0.419909, mean_q: 4.162137
 18437/100000: episode: 256, duration: 0.195s, episode steps: 40, steps per second: 205, episode reward: 99.914, mean reward: 2.498 [1.607, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-1.374, 10.100], loss: 0.187730, mae: 0.403548, mean_q: 4.166385
 18477/100000: episode: 257, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 116.254, mean reward: 2.906 [1.534, 10.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.160, 10.142], loss: 0.229171, mae: 0.401959, mean_q: 4.181808
 18517/100000: episode: 258, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 149.369, mean reward: 3.734 [1.795, 10.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.141, 10.238], loss: 0.163821, mae: 0.391829, mean_q: 4.216279
 18557/100000: episode: 259, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 92.026, mean reward: 2.301 [1.481, 4.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.740, 10.100], loss: 0.153084, mae: 0.372408, mean_q: 4.179032
 18597/100000: episode: 260, duration: 0.204s, episode steps: 40, steps per second: 196, episode reward: 127.746, mean reward: 3.194 [2.102, 8.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.874, 10.378], loss: 0.194806, mae: 0.410559, mean_q: 4.227668
 18637/100000: episode: 261, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 95.380, mean reward: 2.385 [1.589, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.418, 10.130], loss: 0.208626, mae: 0.414466, mean_q: 4.262320
 18677/100000: episode: 262, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 113.123, mean reward: 2.828 [1.567, 5.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.315, 10.100], loss: 0.186371, mae: 0.416545, mean_q: 4.258626
 18717/100000: episode: 263, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 101.091, mean reward: 2.527 [1.692, 4.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.750, 10.225], loss: 0.194282, mae: 0.410615, mean_q: 4.225230
 18757/100000: episode: 264, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 99.000, mean reward: 2.475 [1.486, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.916, 10.155], loss: 0.197751, mae: 0.422268, mean_q: 4.266957
 18797/100000: episode: 265, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 100.091, mean reward: 2.502 [1.628, 8.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.708, 10.153], loss: 0.215619, mae: 0.413177, mean_q: 4.241583
 18837/100000: episode: 266, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 131.704, mean reward: 3.293 [2.433, 4.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.456, 10.371], loss: 0.225002, mae: 0.416286, mean_q: 4.336098
 18877/100000: episode: 267, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 142.814, mean reward: 3.570 [2.093, 5.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.452, 10.287], loss: 0.155454, mae: 0.380348, mean_q: 4.317870
 18917/100000: episode: 268, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 97.887, mean reward: 2.447 [1.516, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.603, 10.100], loss: 0.251923, mae: 0.447626, mean_q: 4.347474
 18957/100000: episode: 269, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 90.862, mean reward: 2.272 [1.579, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.509, 10.100], loss: 0.213574, mae: 0.433472, mean_q: 4.414813
 18997/100000: episode: 270, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 124.285, mean reward: 3.107 [1.926, 5.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.238, 10.411], loss: 0.164148, mae: 0.398562, mean_q: 4.258323
 19037/100000: episode: 271, duration: 0.207s, episode steps: 40, steps per second: 194, episode reward: 116.938, mean reward: 2.923 [1.684, 14.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.324, 10.247], loss: 0.208470, mae: 0.424127, mean_q: 4.332625
 19077/100000: episode: 272, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 111.710, mean reward: 2.793 [1.899, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.122, 10.309], loss: 0.218228, mae: 0.438783, mean_q: 4.360045
 19117/100000: episode: 273, duration: 0.204s, episode steps: 40, steps per second: 196, episode reward: 99.601, mean reward: 2.490 [1.887, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.598, 10.382], loss: 0.216991, mae: 0.424368, mean_q: 4.323796
 19157/100000: episode: 274, duration: 0.196s, episode steps: 40, steps per second: 205, episode reward: 120.148, mean reward: 3.004 [2.038, 5.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.808, 10.267], loss: 0.295835, mae: 0.438653, mean_q: 4.411107
 19197/100000: episode: 275, duration: 0.211s, episode steps: 40, steps per second: 189, episode reward: 129.649, mean reward: 3.241 [1.568, 13.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.443, 10.166], loss: 0.241523, mae: 0.438705, mean_q: 4.421121
 19237/100000: episode: 276, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 118.412, mean reward: 2.960 [1.770, 14.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.263, 10.228], loss: 0.224074, mae: 0.425595, mean_q: 4.444107
 19277/100000: episode: 277, duration: 0.196s, episode steps: 40, steps per second: 205, episode reward: 111.951, mean reward: 2.799 [1.590, 5.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.672, 10.249], loss: 0.202785, mae: 0.437593, mean_q: 4.448640
 19317/100000: episode: 278, duration: 0.204s, episode steps: 40, steps per second: 196, episode reward: 115.814, mean reward: 2.895 [1.987, 4.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.742, 10.338], loss: 0.270041, mae: 0.464920, mean_q: 4.456328
 19357/100000: episode: 279, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 124.761, mean reward: 3.119 [1.716, 5.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.717, 10.233], loss: 0.417129, mae: 0.495835, mean_q: 4.609986
 19397/100000: episode: 280, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 114.857, mean reward: 2.871 [1.819, 5.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-1.377, 10.339], loss: 0.284677, mae: 0.443362, mean_q: 4.471205
 19437/100000: episode: 281, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 129.035, mean reward: 3.226 [1.713, 6.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.539, 10.292], loss: 0.242715, mae: 0.434824, mean_q: 4.549426
 19477/100000: episode: 282, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 108.978, mean reward: 2.724 [1.767, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.625, 10.300], loss: 0.195784, mae: 0.423721, mean_q: 4.567675
 19517/100000: episode: 283, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 115.978, mean reward: 2.899 [1.550, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-1.049, 10.249], loss: 0.325754, mae: 0.484629, mean_q: 4.586151
 19557/100000: episode: 284, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 114.754, mean reward: 2.869 [1.712, 6.232], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.018, 10.222], loss: 0.253613, mae: 0.453634, mean_q: 4.620510
 19597/100000: episode: 285, duration: 0.191s, episode steps: 40, steps per second: 209, episode reward: 208.536, mean reward: 5.213 [2.361, 11.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.119, 10.449], loss: 0.423368, mae: 0.506440, mean_q: 4.628727
 19637/100000: episode: 286, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 126.258, mean reward: 3.156 [1.779, 6.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.995, 10.196], loss: 0.333744, mae: 0.495676, mean_q: 4.653171
 19677/100000: episode: 287, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 94.772, mean reward: 2.369 [1.436, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.260, 10.215], loss: 0.358316, mae: 0.514540, mean_q: 4.643918
 19717/100000: episode: 288, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 90.868, mean reward: 2.272 [1.482, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.267, 10.106], loss: 0.265590, mae: 0.463162, mean_q: 4.660479
 19757/100000: episode: 289, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 88.774, mean reward: 2.219 [1.525, 4.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.720, 10.270], loss: 0.334784, mae: 0.487328, mean_q: 4.668790
 19797/100000: episode: 290, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 106.124, mean reward: 2.653 [1.849, 4.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.578, 10.332], loss: 0.318262, mae: 0.488104, mean_q: 4.741418
 19837/100000: episode: 291, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 102.505, mean reward: 2.563 [1.530, 4.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.674, 10.199], loss: 0.400463, mae: 0.508620, mean_q: 4.680298
 19877/100000: episode: 292, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 99.090, mean reward: 2.477 [1.658, 4.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.611, 10.358], loss: 0.364895, mae: 0.521955, mean_q: 4.688652
 19917/100000: episode: 293, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 84.708, mean reward: 2.118 [1.565, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-1.021, 10.204], loss: 0.393730, mae: 0.512316, mean_q: 4.739577
 19957/100000: episode: 294, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 107.822, mean reward: 2.696 [1.632, 10.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.210, 10.348], loss: 0.293859, mae: 0.497404, mean_q: 4.665911
 19997/100000: episode: 295, duration: 0.195s, episode steps: 40, steps per second: 205, episode reward: 93.017, mean reward: 2.325 [1.611, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.570, 10.288], loss: 0.247392, mae: 0.465351, mean_q: 4.722626
 20037/100000: episode: 296, duration: 0.194s, episode steps: 40, steps per second: 207, episode reward: 110.714, mean reward: 2.768 [1.732, 5.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.162, 10.441], loss: 0.294250, mae: 0.466279, mean_q: 4.729380
 20077/100000: episode: 297, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 98.586, mean reward: 2.465 [1.470, 5.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.978, 10.100], loss: 0.265511, mae: 0.494040, mean_q: 4.663985
 20117/100000: episode: 298, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 117.279, mean reward: 2.932 [2.191, 5.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.108, 10.432], loss: 0.281201, mae: 0.488718, mean_q: 4.605892
 20157/100000: episode: 299, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 97.867, mean reward: 2.447 [1.566, 6.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.664, 10.152], loss: 0.297815, mae: 0.492226, mean_q: 4.670872
 20197/100000: episode: 300, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 104.196, mean reward: 2.605 [1.509, 5.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.249, 10.190], loss: 0.273111, mae: 0.442123, mean_q: 4.657320
 20237/100000: episode: 301, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 259.851, mean reward: 6.496 [2.020, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-1.064, 10.390], loss: 0.236265, mae: 0.467564, mean_q: 4.723225
 20277/100000: episode: 302, duration: 0.195s, episode steps: 40, steps per second: 205, episode reward: 119.060, mean reward: 2.977 [1.522, 6.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.276, 10.277], loss: 3.993629, mae: 0.618894, mean_q: 4.755620
 20317/100000: episode: 303, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 121.868, mean reward: 3.047 [1.747, 7.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.106, 10.256], loss: 4.195822, mae: 0.767140, mean_q: 4.803124
 20357/100000: episode: 304, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 85.969, mean reward: 2.149 [1.532, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.573, 10.113], loss: 0.285999, mae: 0.482463, mean_q: 4.704060
 20397/100000: episode: 305, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 94.119, mean reward: 2.353 [1.705, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.513, 10.226], loss: 0.460974, mae: 0.495325, mean_q: 4.742742
 20437/100000: episode: 306, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 107.495, mean reward: 2.687 [1.710, 5.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.835, 10.182], loss: 0.286754, mae: 0.475888, mean_q: 4.744922
 20477/100000: episode: 307, duration: 0.194s, episode steps: 40, steps per second: 206, episode reward: 92.257, mean reward: 2.306 [1.482, 4.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.777, 10.264], loss: 4.212578, mae: 0.788015, mean_q: 4.784097
 20517/100000: episode: 308, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 116.158, mean reward: 2.904 [2.117, 5.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.385, 10.404], loss: 0.368652, mae: 0.535154, mean_q: 4.780427
 20557/100000: episode: 309, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 115.347, mean reward: 2.884 [1.784, 5.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.203, 10.310], loss: 0.536745, mae: 0.515470, mean_q: 4.878872
 20597/100000: episode: 310, duration: 0.195s, episode steps: 40, steps per second: 205, episode reward: 95.280, mean reward: 2.382 [1.600, 5.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.991, 10.254], loss: 0.401635, mae: 0.542489, mean_q: 4.856846
 20637/100000: episode: 311, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 100.048, mean reward: 2.501 [1.774, 6.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.209, 10.330], loss: 0.352740, mae: 0.488961, mean_q: 4.865778
 20677/100000: episode: 312, duration: 0.195s, episode steps: 40, steps per second: 205, episode reward: 119.143, mean reward: 2.979 [2.282, 5.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.251, 10.355], loss: 0.463027, mae: 0.511650, mean_q: 4.783395
 20717/100000: episode: 313, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 108.927, mean reward: 2.723 [1.495, 4.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.388, 10.198], loss: 0.359328, mae: 0.491297, mean_q: 4.786554
 20757/100000: episode: 314, duration: 0.210s, episode steps: 40, steps per second: 191, episode reward: 110.471, mean reward: 2.762 [1.553, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.856, 10.154], loss: 0.202861, mae: 0.420910, mean_q: 4.842783
 20797/100000: episode: 315, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 102.854, mean reward: 2.571 [1.858, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.459, 10.338], loss: 0.368205, mae: 0.509926, mean_q: 4.884112
 20837/100000: episode: 316, duration: 0.207s, episode steps: 40, steps per second: 193, episode reward: 133.992, mean reward: 3.350 [2.362, 5.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.215, 10.470], loss: 4.172534, mae: 0.699136, mean_q: 4.900381
 20877/100000: episode: 317, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 101.887, mean reward: 2.547 [1.436, 5.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.950, 10.100], loss: 0.372848, mae: 0.561716, mean_q: 4.900143
 20917/100000: episode: 318, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 132.978, mean reward: 3.324 [2.619, 5.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.124, 10.425], loss: 0.447793, mae: 0.490916, mean_q: 4.965432
 20957/100000: episode: 319, duration: 0.194s, episode steps: 40, steps per second: 206, episode reward: 90.653, mean reward: 2.266 [1.898, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.520, 10.282], loss: 0.311308, mae: 0.489988, mean_q: 4.920018
 20997/100000: episode: 320, duration: 0.207s, episode steps: 40, steps per second: 193, episode reward: 123.958, mean reward: 3.099 [2.033, 7.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.367, 10.407], loss: 0.359111, mae: 0.507824, mean_q: 4.985505
 21037/100000: episode: 321, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 128.382, mean reward: 3.210 [2.103, 5.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-1.448, 10.306], loss: 0.292200, mae: 0.484672, mean_q: 5.022972
 21077/100000: episode: 322, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 96.137, mean reward: 2.403 [1.511, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.269, 10.100], loss: 4.240574, mae: 0.816636, mean_q: 5.056779
 21117/100000: episode: 323, duration: 0.218s, episode steps: 40, steps per second: 183, episode reward: 158.984, mean reward: 3.975 [2.349, 6.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.118, 10.445], loss: 4.175681, mae: 0.761473, mean_q: 5.062762
 21157/100000: episode: 324, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 93.804, mean reward: 2.345 [1.730, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.378, 10.323], loss: 0.275765, mae: 0.506860, mean_q: 4.936706
 21197/100000: episode: 325, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 96.574, mean reward: 2.414 [1.544, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.236, 10.190], loss: 0.249621, mae: 0.471514, mean_q: 4.960167
[Info] 3-TH LEVEL FOUND: 8.659969329833984, Considering 10/90 traces
 21237/100000: episode: 326, duration: 4.385s, episode steps: 40, steps per second: 9, episode reward: 126.448, mean reward: 3.161 [1.875, 7.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.595, 10.309], loss: 0.371905, mae: 0.507749, mean_q: 5.058194
 21274/100000: episode: 327, duration: 0.195s, episode steps: 37, steps per second: 190, episode reward: 131.573, mean reward: 3.556 [1.758, 5.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-1.064, 10.316], loss: 4.294160, mae: 0.642548, mean_q: 5.091888
 21303/100000: episode: 328, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 133.344, mean reward: 4.598 [1.742, 8.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.434, 10.358], loss: 0.575870, mae: 0.643715, mean_q: 4.904051
 21337/100000: episode: 329, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 124.750, mean reward: 3.669 [1.950, 6.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.943, 10.257], loss: 0.408088, mae: 0.531359, mean_q: 4.960413
 21366/100000: episode: 330, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 76.856, mean reward: 2.650 [1.826, 5.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.350, 10.258], loss: 0.210270, mae: 0.448987, mean_q: 4.937144
 21394/100000: episode: 331, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 87.898, mean reward: 3.139 [1.883, 5.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.924, 10.328], loss: 0.314974, mae: 0.480525, mean_q: 4.973140
 21429/100000: episode: 332, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 146.785, mean reward: 4.194 [2.880, 5.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.759, 10.572], loss: 0.399403, mae: 0.535728, mean_q: 5.107906
 21466/100000: episode: 333, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 148.185, mean reward: 4.005 [2.609, 8.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.357, 10.458], loss: 4.405385, mae: 0.768419, mean_q: 5.106907
 21500/100000: episode: 334, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 94.284, mean reward: 2.773 [1.499, 8.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-1.466, 10.232], loss: 0.296808, mae: 0.515222, mean_q: 5.134268
 21533/100000: episode: 335, duration: 0.160s, episode steps: 33, steps per second: 206, episode reward: 152.583, mean reward: 4.624 [2.867, 8.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.716, 10.475], loss: 4.776788, mae: 0.729234, mean_q: 5.099530
 21568/100000: episode: 336, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 105.924, mean reward: 3.026 [1.947, 4.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.987, 10.366], loss: 4.603559, mae: 0.695119, mean_q: 5.185062
 21603/100000: episode: 337, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 141.699, mean reward: 4.049 [2.411, 10.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.238, 10.429], loss: 0.639525, mae: 0.728300, mean_q: 5.257581
 21640/100000: episode: 338, duration: 0.187s, episode steps: 37, steps per second: 197, episode reward: 107.239, mean reward: 2.898 [1.510, 7.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.084, 10.158], loss: 0.478123, mae: 0.580532, mean_q: 5.349483
 21668/100000: episode: 339, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 104.931, mean reward: 3.748 [1.887, 6.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.313, 10.235], loss: 0.416990, mae: 0.553477, mean_q: 5.259060
 21701/100000: episode: 340, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 97.889, mean reward: 2.966 [2.216, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.808, 10.169], loss: 0.387592, mae: 0.536624, mean_q: 5.225372
 21729/100000: episode: 341, duration: 0.133s, episode steps: 28, steps per second: 210, episode reward: 93.981, mean reward: 3.356 [2.956, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.868, 10.448], loss: 0.255082, mae: 0.488119, mean_q: 5.148487
 21766/100000: episode: 342, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 133.527, mean reward: 3.609 [2.658, 5.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.470, 10.433], loss: 0.345153, mae: 0.551792, mean_q: 5.274147
 21801/100000: episode: 343, duration: 0.174s, episode steps: 35, steps per second: 202, episode reward: 97.527, mean reward: 2.786 [1.862, 4.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.042, 10.299], loss: 0.340386, mae: 0.552139, mean_q: 5.298580
 21838/100000: episode: 344, duration: 0.186s, episode steps: 37, steps per second: 199, episode reward: 212.100, mean reward: 5.732 [2.012, 23.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.151, 10.337], loss: 0.520692, mae: 0.585364, mean_q: 5.377202
 21873/100000: episode: 345, duration: 0.181s, episode steps: 35, steps per second: 193, episode reward: 191.985, mean reward: 5.485 [2.251, 15.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.306, 10.402], loss: 0.663644, mae: 0.647465, mean_q: 5.486063
 21906/100000: episode: 346, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 125.738, mean reward: 3.810 [1.893, 16.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.834, 10.349], loss: 0.488042, mae: 0.563277, mean_q: 5.358214
 21940/100000: episode: 347, duration: 0.164s, episode steps: 34, steps per second: 207, episode reward: 142.748, mean reward: 4.198 [3.091, 8.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.993, 10.562], loss: 0.585947, mae: 0.615963, mean_q: 5.458989
 21969/100000: episode: 348, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 115.174, mean reward: 3.972 [1.709, 9.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.103, 10.263], loss: 0.723095, mae: 0.655817, mean_q: 5.485010
 22003/100000: episode: 349, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 116.818, mean reward: 3.436 [1.903, 7.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.454, 10.373], loss: 0.407759, mae: 0.587923, mean_q: 5.496419
 22036/100000: episode: 350, duration: 0.158s, episode steps: 33, steps per second: 209, episode reward: 97.128, mean reward: 2.943 [1.918, 5.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.821, 10.294], loss: 0.400786, mae: 0.576211, mean_q: 5.406855
 22069/100000: episode: 351, duration: 0.158s, episode steps: 33, steps per second: 208, episode reward: 100.122, mean reward: 3.034 [1.936, 6.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.310, 10.390], loss: 5.367791, mae: 0.960662, mean_q: 5.589538
 22097/100000: episode: 352, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 70.176, mean reward: 2.506 [2.167, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.363], loss: 0.533782, mae: 0.623995, mean_q: 5.485444
 22130/100000: episode: 353, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 160.010, mean reward: 4.849 [1.783, 12.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.532, 10.341], loss: 0.772240, mae: 0.782196, mean_q: 5.750534
 22158/100000: episode: 354, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 68.590, mean reward: 2.450 [1.803, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.083, 10.279], loss: 0.365071, mae: 0.573323, mean_q: 5.496048
 22191/100000: episode: 355, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 178.593, mean reward: 5.412 [3.804, 8.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.504, 10.572], loss: 0.648261, mae: 0.644688, mean_q: 5.589370
 22229/100000: episode: 356, duration: 0.192s, episode steps: 38, steps per second: 197, episode reward: 101.683, mean reward: 2.676 [1.650, 4.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.164, 10.350], loss: 0.542145, mae: 0.616600, mean_q: 5.565566
 22264/100000: episode: 357, duration: 0.177s, episode steps: 35, steps per second: 197, episode reward: 192.125, mean reward: 5.489 [2.493, 42.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.052, 10.459], loss: 4.708511, mae: 0.780609, mean_q: 5.699896
 22298/100000: episode: 358, duration: 0.166s, episode steps: 34, steps per second: 205, episode reward: 91.591, mean reward: 2.694 [1.500, 6.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.932, 10.164], loss: 0.652351, mae: 0.757302, mean_q: 5.525340
 22327/100000: episode: 359, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 147.998, mean reward: 5.103 [1.978, 11.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.397, 10.303], loss: 1.269210, mae: 0.664312, mean_q: 5.604349
 22360/100000: episode: 360, duration: 0.163s, episode steps: 33, steps per second: 203, episode reward: 112.588, mean reward: 3.412 [1.957, 6.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.714, 10.351], loss: 0.739990, mae: 0.733756, mean_q: 5.791378
 22394/100000: episode: 361, duration: 0.167s, episode steps: 34, steps per second: 204, episode reward: 94.458, mean reward: 2.778 [1.670, 7.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.088, 10.252], loss: 4.985308, mae: 0.814285, mean_q: 5.709091
 22423/100000: episode: 362, duration: 0.142s, episode steps: 29, steps per second: 205, episode reward: 93.562, mean reward: 3.226 [1.813, 5.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.096, 10.281], loss: 0.452521, mae: 0.592458, mean_q: 5.725055
 22456/100000: episode: 363, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 166.290, mean reward: 5.039 [2.984, 9.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.247, 10.547], loss: 0.758041, mae: 0.673338, mean_q: 5.710087
 22490/100000: episode: 364, duration: 0.167s, episode steps: 34, steps per second: 204, episode reward: 108.841, mean reward: 3.201 [1.998, 10.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.562, 10.377], loss: 0.658236, mae: 0.682482, mean_q: 5.770386
 22528/100000: episode: 365, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 138.564, mean reward: 3.646 [2.465, 5.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.561, 10.424], loss: 0.784636, mae: 0.692115, mean_q: 5.806863
 22556/100000: episode: 366, duration: 0.135s, episode steps: 28, steps per second: 208, episode reward: 128.015, mean reward: 4.572 [3.208, 7.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.881, 10.359], loss: 0.703836, mae: 0.716105, mean_q: 5.875439
 22591/100000: episode: 367, duration: 0.177s, episode steps: 35, steps per second: 197, episode reward: 144.748, mean reward: 4.136 [1.800, 21.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.056, 10.297], loss: 5.606404, mae: 0.948264, mean_q: 5.833684
 22626/100000: episode: 368, duration: 0.179s, episode steps: 35, steps per second: 196, episode reward: 127.577, mean reward: 3.645 [2.445, 7.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-1.077, 10.449], loss: 0.828264, mae: 0.722854, mean_q: 6.088675
 22660/100000: episode: 369, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 92.662, mean reward: 2.725 [1.542, 6.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.212, 10.273], loss: 0.611149, mae: 0.664361, mean_q: 5.830239
 22688/100000: episode: 370, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 95.462, mean reward: 3.409 [1.893, 5.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.767, 10.318], loss: 1.472289, mae: 0.757734, mean_q: 5.856328
 22717/100000: episode: 371, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 106.214, mean reward: 3.663 [2.216, 6.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.049, 10.354], loss: 5.532193, mae: 0.744711, mean_q: 5.794049
 22751/100000: episode: 372, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 108.759, mean reward: 3.199 [1.572, 14.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.119, 10.194], loss: 1.559827, mae: 0.874344, mean_q: 6.116882
 22784/100000: episode: 373, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 121.309, mean reward: 3.676 [2.541, 5.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.492, 10.510], loss: 0.458464, mae: 0.600904, mean_q: 5.827275
 22812/100000: episode: 374, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 86.022, mean reward: 3.072 [2.148, 5.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.215, 10.324], loss: 0.580537, mae: 0.632071, mean_q: 5.792886
 22840/100000: episode: 375, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 69.182, mean reward: 2.471 [1.806, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.062, 10.232], loss: 1.387870, mae: 0.761438, mean_q: 5.866248
 22875/100000: episode: 376, duration: 0.172s, episode steps: 35, steps per second: 204, episode reward: 118.378, mean reward: 3.382 [1.735, 5.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.636, 10.245], loss: 0.523472, mae: 0.610472, mean_q: 5.847450
 22908/100000: episode: 377, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 113.976, mean reward: 3.454 [1.761, 6.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.741, 10.238], loss: 5.185493, mae: 0.900861, mean_q: 6.080996
 22941/100000: episode: 378, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 101.421, mean reward: 3.073 [1.744, 5.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.939, 10.269], loss: 5.485737, mae: 1.115346, mean_q: 6.146136
 22976/100000: episode: 379, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 89.412, mean reward: 2.555 [1.499, 6.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.795, 10.118], loss: 0.703847, mae: 0.724520, mean_q: 5.959842
 23014/100000: episode: 380, duration: 0.188s, episode steps: 38, steps per second: 202, episode reward: 133.146, mean reward: 3.504 [1.760, 8.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.112, 10.293], loss: 0.666784, mae: 0.684223, mean_q: 6.001158
 23048/100000: episode: 381, duration: 0.166s, episode steps: 34, steps per second: 205, episode reward: 115.157, mean reward: 3.387 [2.201, 6.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.572, 10.428], loss: 0.555813, mae: 0.611105, mean_q: 5.872529
 23085/100000: episode: 382, duration: 0.181s, episode steps: 37, steps per second: 205, episode reward: 134.956, mean reward: 3.647 [2.196, 5.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.728, 10.450], loss: 0.514541, mae: 0.630361, mean_q: 5.899192
 23118/100000: episode: 383, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 134.434, mean reward: 4.074 [2.182, 9.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.035, 10.342], loss: 0.565848, mae: 0.655165, mean_q: 5.927254
 23152/100000: episode: 384, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 108.126, mean reward: 3.180 [1.930, 7.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.777, 10.423], loss: 0.673149, mae: 0.676760, mean_q: 6.032425
 23186/100000: episode: 385, duration: 0.162s, episode steps: 34, steps per second: 210, episode reward: 103.503, mean reward: 3.044 [1.724, 8.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.248, 10.239], loss: 0.569554, mae: 0.674229, mean_q: 5.997857
 23219/100000: episode: 386, duration: 0.167s, episode steps: 33, steps per second: 197, episode reward: 197.952, mean reward: 5.999 [2.495, 12.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.118, 10.527], loss: 0.575116, mae: 0.693361, mean_q: 5.981140
 23253/100000: episode: 387, duration: 0.172s, episode steps: 34, steps per second: 198, episode reward: 122.413, mean reward: 3.600 [2.250, 7.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.149, 10.443], loss: 0.634824, mae: 0.695621, mean_q: 6.003623
 23290/100000: episode: 388, duration: 0.184s, episode steps: 37, steps per second: 201, episode reward: 135.105, mean reward: 3.651 [2.011, 6.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.068, 10.437], loss: 0.583656, mae: 0.674286, mean_q: 6.066352
 23325/100000: episode: 389, duration: 0.177s, episode steps: 35, steps per second: 197, episode reward: 129.047, mean reward: 3.687 [2.515, 8.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.761, 10.398], loss: 0.940121, mae: 0.762521, mean_q: 6.066895
 23354/100000: episode: 390, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 208.338, mean reward: 7.184 [2.914, 13.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.536], loss: 0.681400, mae: 0.734309, mean_q: 6.149799
 23389/100000: episode: 391, duration: 0.175s, episode steps: 35, steps per second: 200, episode reward: 95.669, mean reward: 2.733 [1.840, 9.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.394, 10.251], loss: 0.608052, mae: 0.716903, mean_q: 6.075601
 23426/100000: episode: 392, duration: 0.180s, episode steps: 37, steps per second: 206, episode reward: 151.270, mean reward: 4.088 [2.684, 8.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.504, 10.395], loss: 0.703567, mae: 0.718957, mean_q: 6.130792
 23455/100000: episode: 393, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 90.674, mean reward: 3.127 [2.038, 5.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.457, 10.427], loss: 5.761125, mae: 1.023158, mean_q: 6.167821
 23488/100000: episode: 394, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 110.912, mean reward: 3.361 [2.095, 5.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.182, 10.312], loss: 6.073083, mae: 1.141245, mean_q: 6.315378
 23522/100000: episode: 395, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 94.821, mean reward: 2.789 [1.812, 8.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-1.234, 10.362], loss: 0.600943, mae: 0.722413, mean_q: 6.135939
 23551/100000: episode: 396, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 137.891, mean reward: 4.755 [2.211, 9.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.348, 10.451], loss: 0.515774, mae: 0.682109, mean_q: 6.121526
 23579/100000: episode: 397, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 115.810, mean reward: 4.136 [2.145, 7.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.713, 10.437], loss: 1.005706, mae: 0.769849, mean_q: 6.191392
 23612/100000: episode: 398, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 108.344, mean reward: 3.283 [1.569, 7.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.638, 10.241], loss: 0.893432, mae: 0.723575, mean_q: 6.212756
 23645/100000: episode: 399, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 95.044, mean reward: 2.880 [1.962, 5.392], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.358, 10.318], loss: 5.712221, mae: 0.842400, mean_q: 6.312654
 23680/100000: episode: 400, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 101.480, mean reward: 2.899 [1.675, 6.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.280, 10.146], loss: 0.749881, mae: 0.825143, mean_q: 6.304247
 23714/100000: episode: 401, duration: 0.172s, episode steps: 34, steps per second: 198, episode reward: 87.964, mean reward: 2.587 [1.959, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.606, 10.330], loss: 0.629877, mae: 0.707028, mean_q: 6.234601
 23747/100000: episode: 402, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 99.583, mean reward: 3.018 [1.896, 7.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.309], loss: 0.739430, mae: 0.776190, mean_q: 6.283199
 23781/100000: episode: 403, duration: 0.165s, episode steps: 34, steps per second: 206, episode reward: 125.962, mean reward: 3.705 [1.894, 8.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.332, 10.333], loss: 0.949694, mae: 0.790124, mean_q: 6.251787
 23814/100000: episode: 404, duration: 0.159s, episode steps: 33, steps per second: 208, episode reward: 125.297, mean reward: 3.797 [2.818, 6.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.508], loss: 0.651584, mae: 0.784379, mean_q: 6.308473
 23849/100000: episode: 405, duration: 0.177s, episode steps: 35, steps per second: 197, episode reward: 147.009, mean reward: 4.200 [2.478, 13.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.042, 10.433], loss: 1.388594, mae: 0.822226, mean_q: 6.358525
 23886/100000: episode: 406, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 121.220, mean reward: 3.276 [2.082, 5.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-1.057, 10.343], loss: 5.124483, mae: 0.868004, mean_q: 6.384855
 23921/100000: episode: 407, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 103.412, mean reward: 2.955 [1.876, 5.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.499, 10.519], loss: 0.979645, mae: 0.984369, mean_q: 6.260790
 23959/100000: episode: 408, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 117.244, mean reward: 3.085 [1.720, 6.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.657, 10.200], loss: 1.289275, mae: 0.784734, mean_q: 6.396270
 23994/100000: episode: 409, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 88.473, mean reward: 2.528 [1.679, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.665, 10.284], loss: 0.707407, mae: 0.723637, mean_q: 6.289195
 24032/100000: episode: 410, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 113.930, mean reward: 2.998 [1.716, 4.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.769, 10.226], loss: 0.654649, mae: 0.692971, mean_q: 6.208074
 24070/100000: episode: 411, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 123.000, mean reward: 3.237 [2.045, 5.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.109, 10.416], loss: 0.762918, mae: 0.762257, mean_q: 6.253109
 24108/100000: episode: 412, duration: 0.186s, episode steps: 38, steps per second: 204, episode reward: 183.119, mean reward: 4.819 [2.991, 7.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.214, 10.599], loss: 4.933478, mae: 1.005424, mean_q: 6.445933
 24143/100000: episode: 413, duration: 0.170s, episode steps: 35, steps per second: 206, episode reward: 97.465, mean reward: 2.785 [1.812, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.256, 10.297], loss: 0.759459, mae: 0.774100, mean_q: 6.258848
 24178/100000: episode: 414, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 133.798, mean reward: 3.823 [2.181, 7.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.135, 10.317], loss: 0.759199, mae: 0.778393, mean_q: 6.356697
 24207/100000: episode: 415, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 134.849, mean reward: 4.650 [1.869, 16.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.334], loss: 5.812580, mae: 0.931608, mean_q: 6.528590
[Info] 4-TH LEVEL FOUND: 12.47624397277832, Considering 10/90 traces
 24235/100000: episode: 416, duration: 4.318s, episode steps: 28, steps per second: 6, episode reward: 105.626, mean reward: 3.772 [2.139, 7.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.435], loss: 0.997782, mae: 0.842226, mean_q: 6.393345
 24263/100000: episode: 417, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 154.207, mean reward: 5.507 [3.070, 10.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.257, 10.574], loss: 5.952226, mae: 0.963888, mean_q: 6.385197
 24293/100000: episode: 418, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 109.278, mean reward: 3.643 [2.264, 7.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.287, 10.454], loss: 0.987217, mae: 0.880158, mean_q: 6.402337
 24326/100000: episode: 419, duration: 0.160s, episode steps: 33, steps per second: 206, episode reward: 159.044, mean reward: 4.820 [1.851, 25.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.047, 10.253], loss: 2.147377, mae: 0.909996, mean_q: 6.398949
 24359/100000: episode: 420, duration: 0.172s, episode steps: 33, steps per second: 191, episode reward: 121.947, mean reward: 3.695 [2.467, 5.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.601, 10.458], loss: 1.278456, mae: 0.790973, mean_q: 6.396728
 24392/100000: episode: 421, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 137.485, mean reward: 4.166 [1.629, 10.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.954, 10.274], loss: 5.257528, mae: 0.973581, mean_q: 6.437708
 24423/100000: episode: 422, duration: 0.155s, episode steps: 31, steps per second: 201, episode reward: 176.740, mean reward: 5.701 [2.106, 23.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.500, 10.346], loss: 10.544326, mae: 1.234780, mean_q: 6.493454
 24451/100000: episode: 423, duration: 0.134s, episode steps: 28, steps per second: 208, episode reward: 84.367, mean reward: 3.013 [2.388, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.552, 10.430], loss: 0.881875, mae: 0.837425, mean_q: 6.528766
 24481/100000: episode: 424, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 168.984, mean reward: 5.633 [2.182, 11.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.400, 10.395], loss: 0.767215, mae: 0.781589, mean_q: 6.292367
 24514/100000: episode: 425, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 162.666, mean reward: 4.929 [2.072, 9.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.276, 10.367], loss: 0.797951, mae: 0.782894, mean_q: 6.492437
 24545/100000: episode: 426, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 253.886, mean reward: 8.190 [1.963, 53.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.356, 10.312], loss: 1.143139, mae: 0.863852, mean_q: 6.567439
 24576/100000: episode: 427, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 133.849, mean reward: 4.318 [2.233, 9.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.608], loss: 1.466954, mae: 0.919377, mean_q: 6.492588
 24607/100000: episode: 428, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 228.156, mean reward: 7.360 [4.341, 16.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.662, 10.555], loss: 1.167326, mae: 0.867880, mean_q: 6.770270
 24640/100000: episode: 429, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 164.596, mean reward: 4.988 [2.388, 10.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.694, 10.426], loss: 10.584618, mae: 1.296244, mean_q: 7.030980
 24670/100000: episode: 430, duration: 0.146s, episode steps: 30, steps per second: 206, episode reward: 170.719, mean reward: 5.691 [2.121, 19.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.390], loss: 1.733340, mae: 0.895116, mean_q: 6.506625
 24701/100000: episode: 431, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 146.100, mean reward: 4.713 [2.223, 9.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-1.101, 10.548], loss: 1.330921, mae: 0.905846, mean_q: 6.822581
 24734/100000: episode: 432, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 119.019, mean reward: 3.607 [2.721, 6.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.016, 10.485], loss: 1.528967, mae: 0.893233, mean_q: 6.816693
[Info] FALSIFICATION!
[Info] Levels: [4.7044506, 5.8523884, 8.659969, 12.476244, 14.162311]
[Info] Cond. Prob: [0.1, 0.14, 0.1, 0.1, 0.03]
[Info] Error Prob: 4.2000000000000004e-06

 24741/100000: episode: 433, duration: 4.669s, episode steps: 7, steps per second: 1, episode reward: 187.143, mean reward: 26.735 [8.768, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.831 [-0.320, 8.837], loss: 2.470580, mae: 1.006504, mean_q: 6.788963
 24841/100000: episode: 434, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 208.422, mean reward: 2.084 [1.494, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.650, 10.098], loss: 2.786580, mae: 0.922219, mean_q: 6.719855
 24941/100000: episode: 435, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 196.877, mean reward: 1.969 [1.442, 4.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.507, 10.098], loss: 3.048868, mae: 0.972506, mean_q: 6.745520
 25041/100000: episode: 436, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 212.621, mean reward: 2.126 [1.487, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.567, 10.542], loss: 2.541950, mae: 0.968353, mean_q: 6.694563
 25141/100000: episode: 437, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 188.910, mean reward: 1.889 [1.454, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.562, 10.098], loss: 1.596600, mae: 0.888459, mean_q: 6.609039
 25241/100000: episode: 438, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 198.229, mean reward: 1.982 [1.487, 4.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.491, 10.098], loss: 1.013690, mae: 0.850312, mean_q: 6.680490
 25341/100000: episode: 439, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 188.747, mean reward: 1.887 [1.465, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.637, 10.098], loss: 3.542089, mae: 0.930299, mean_q: 6.730315
 25441/100000: episode: 440, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 185.156, mean reward: 1.852 [1.469, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.395, 10.098], loss: 4.117881, mae: 1.023621, mean_q: 6.686097
 25541/100000: episode: 441, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 204.742, mean reward: 2.047 [1.485, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.706, 10.098], loss: 2.403205, mae: 0.922341, mean_q: 6.682233
 25641/100000: episode: 442, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 203.378, mean reward: 2.034 [1.452, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.799, 10.423], loss: 2.524063, mae: 0.909721, mean_q: 6.562264
 25741/100000: episode: 443, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 187.105, mean reward: 1.871 [1.469, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.568, 10.140], loss: 2.371914, mae: 0.869416, mean_q: 6.569357
 25841/100000: episode: 444, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 191.541, mean reward: 1.915 [1.448, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.607, 10.099], loss: 1.683840, mae: 0.854180, mean_q: 6.441712
 25941/100000: episode: 445, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 255.215, mean reward: 2.552 [1.456, 5.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.075, 10.185], loss: 1.480132, mae: 0.869379, mean_q: 6.450518
 26041/100000: episode: 446, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 196.888, mean reward: 1.969 [1.464, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.566, 10.402], loss: 2.493019, mae: 0.898334, mean_q: 6.379525
 26141/100000: episode: 447, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 226.573, mean reward: 2.266 [1.467, 5.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.776, 10.515], loss: 1.264291, mae: 0.834697, mean_q: 6.343269
 26241/100000: episode: 448, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 211.944, mean reward: 2.119 [1.473, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.759, 10.098], loss: 2.917436, mae: 1.031558, mean_q: 6.521210
 26341/100000: episode: 449, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 188.009, mean reward: 1.880 [1.451, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.366, 10.215], loss: 0.887689, mae: 0.746616, mean_q: 6.149410
 26441/100000: episode: 450, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 184.747, mean reward: 1.847 [1.447, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.216, 10.098], loss: 1.337837, mae: 0.813964, mean_q: 6.201858
 26541/100000: episode: 451, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 179.794, mean reward: 1.798 [1.455, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.417, 10.242], loss: 1.858051, mae: 0.852694, mean_q: 6.236800
 26641/100000: episode: 452, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 203.821, mean reward: 2.038 [1.461, 4.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.679, 10.098], loss: 1.731305, mae: 0.842464, mean_q: 6.030506
 26741/100000: episode: 453, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 186.849, mean reward: 1.868 [1.442, 2.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.607, 10.118], loss: 1.572654, mae: 0.843287, mean_q: 6.159781
 26841/100000: episode: 454, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 183.399, mean reward: 1.834 [1.450, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.488, 10.302], loss: 1.611089, mae: 0.749747, mean_q: 5.936874
 26941/100000: episode: 455, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 214.050, mean reward: 2.140 [1.451, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.958, 10.098], loss: 0.972604, mae: 0.715158, mean_q: 5.876062
 27041/100000: episode: 456, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 178.396, mean reward: 1.784 [1.472, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.529, 10.098], loss: 2.453514, mae: 0.833345, mean_q: 5.832578
 27141/100000: episode: 457, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 188.651, mean reward: 1.887 [1.476, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.633, 10.098], loss: 5.367740, mae: 1.059052, mean_q: 5.899590
 27241/100000: episode: 458, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 187.292, mean reward: 1.873 [1.443, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.434, 10.312], loss: 0.706150, mae: 0.661262, mean_q: 5.680755
 27341/100000: episode: 459, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 199.677, mean reward: 1.997 [1.465, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.497, 10.098], loss: 1.895405, mae: 0.691814, mean_q: 5.564476
 27441/100000: episode: 460, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 189.570, mean reward: 1.896 [1.447, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.701, 10.098], loss: 2.551082, mae: 0.746269, mean_q: 5.645090
 27541/100000: episode: 461, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 187.820, mean reward: 1.878 [1.447, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.089, 10.098], loss: 1.864063, mae: 0.675729, mean_q: 5.514030
 27641/100000: episode: 462, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 195.563, mean reward: 1.956 [1.481, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.690, 10.098], loss: 1.920859, mae: 0.687008, mean_q: 5.241055
 27741/100000: episode: 463, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 215.015, mean reward: 2.150 [1.459, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.961, 10.098], loss: 0.688125, mae: 0.563358, mean_q: 5.333274
 27841/100000: episode: 464, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 183.011, mean reward: 1.830 [1.466, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.214, 10.098], loss: 0.629040, mae: 0.588597, mean_q: 5.316585
 27941/100000: episode: 465, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 186.319, mean reward: 1.863 [1.448, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.751, 10.140], loss: 0.960084, mae: 0.600641, mean_q: 5.249985
 28041/100000: episode: 466, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 208.049, mean reward: 2.080 [1.485, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.638, 10.098], loss: 2.544590, mae: 0.733845, mean_q: 5.185657
 28141/100000: episode: 467, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 194.278, mean reward: 1.943 [1.490, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.020, 10.098], loss: 2.157248, mae: 0.715258, mean_q: 5.146256
 28241/100000: episode: 468, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 189.809, mean reward: 1.898 [1.447, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.066, 10.238], loss: 1.191112, mae: 0.651954, mean_q: 5.119317
 28341/100000: episode: 469, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 182.520, mean reward: 1.825 [1.486, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.734, 10.098], loss: 2.288924, mae: 0.683839, mean_q: 5.049778
 28441/100000: episode: 470, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 199.314, mean reward: 1.993 [1.447, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.456, 10.124], loss: 1.724985, mae: 0.607916, mean_q: 4.894913
 28541/100000: episode: 471, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 179.855, mean reward: 1.799 [1.472, 2.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.636, 10.098], loss: 1.997726, mae: 0.592067, mean_q: 4.844918
 28641/100000: episode: 472, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 185.876, mean reward: 1.859 [1.462, 2.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.873, 10.098], loss: 1.790953, mae: 0.592470, mean_q: 4.778595
 28741/100000: episode: 473, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.231, mean reward: 1.932 [1.471, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.020, 10.098], loss: 1.912840, mae: 0.596084, mean_q: 4.836182
 28841/100000: episode: 474, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 206.324, mean reward: 2.063 [1.483, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.582, 10.098], loss: 1.225384, mae: 0.554373, mean_q: 4.684433
 28941/100000: episode: 475, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 179.370, mean reward: 1.794 [1.455, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.938, 10.145], loss: 1.819996, mae: 0.542666, mean_q: 4.685164
 29041/100000: episode: 476, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 190.940, mean reward: 1.909 [1.446, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.602, 10.098], loss: 0.460747, mae: 0.456005, mean_q: 4.631022
 29141/100000: episode: 477, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 184.513, mean reward: 1.845 [1.448, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.568, 10.098], loss: 1.814200, mae: 0.580444, mean_q: 4.600798
 29241/100000: episode: 478, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 192.262, mean reward: 1.923 [1.485, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.294, 10.098], loss: 0.970586, mae: 0.508229, mean_q: 4.452111
 29341/100000: episode: 479, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 186.215, mean reward: 1.862 [1.490, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.673, 10.198], loss: 5.043508, mae: 0.647067, mean_q: 4.381807
 29441/100000: episode: 480, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 192.144, mean reward: 1.921 [1.468, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.811, 10.216], loss: 0.389137, mae: 0.412150, mean_q: 4.235231
 29541/100000: episode: 481, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 227.778, mean reward: 2.278 [1.450, 4.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.327, 10.300], loss: 0.260010, mae: 0.359524, mean_q: 4.072436
 29641/100000: episode: 482, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 196.897, mean reward: 1.969 [1.453, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.491, 10.175], loss: 0.245606, mae: 0.342108, mean_q: 3.980190
 29741/100000: episode: 483, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 179.501, mean reward: 1.795 [1.462, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.952, 10.222], loss: 1.236214, mae: 0.380309, mean_q: 3.921782
 29841/100000: episode: 484, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 183.698, mean reward: 1.837 [1.452, 2.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.682, 10.098], loss: 0.088575, mae: 0.296281, mean_q: 3.872259
 29941/100000: episode: 485, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 196.089, mean reward: 1.961 [1.449, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.006, 10.098], loss: 0.099050, mae: 0.305087, mean_q: 3.857678
 30041/100000: episode: 486, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 200.489, mean reward: 2.005 [1.450, 4.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.684, 10.254], loss: 0.082021, mae: 0.283837, mean_q: 3.861325
 30141/100000: episode: 487, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 192.094, mean reward: 1.921 [1.483, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.428, 10.216], loss: 0.084386, mae: 0.297023, mean_q: 3.860639
 30241/100000: episode: 488, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 185.555, mean reward: 1.856 [1.442, 2.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.787, 10.227], loss: 0.086667, mae: 0.289323, mean_q: 3.830956
 30341/100000: episode: 489, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 187.080, mean reward: 1.871 [1.458, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.030, 10.098], loss: 0.077915, mae: 0.285201, mean_q: 3.849035
 30441/100000: episode: 490, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 187.285, mean reward: 1.873 [1.475, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.325, 10.098], loss: 0.080235, mae: 0.284120, mean_q: 3.830643
 30541/100000: episode: 491, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 202.789, mean reward: 2.028 [1.499, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.009, 10.181], loss: 0.077120, mae: 0.278104, mean_q: 3.826222
 30641/100000: episode: 492, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 178.018, mean reward: 1.780 [1.454, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.534, 10.201], loss: 0.083704, mae: 0.297323, mean_q: 3.862125
 30741/100000: episode: 493, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 192.272, mean reward: 1.923 [1.431, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.266, 10.098], loss: 0.081490, mae: 0.285586, mean_q: 3.834415
 30841/100000: episode: 494, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 193.514, mean reward: 1.935 [1.522, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.609, 10.098], loss: 0.073512, mae: 0.277932, mean_q: 3.827224
 30941/100000: episode: 495, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 185.170, mean reward: 1.852 [1.444, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.568, 10.255], loss: 0.075298, mae: 0.283752, mean_q: 3.838246
 31041/100000: episode: 496, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 208.324, mean reward: 2.083 [1.521, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.944, 10.240], loss: 0.077569, mae: 0.278634, mean_q: 3.820328
 31141/100000: episode: 497, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 198.194, mean reward: 1.982 [1.479, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.297, 10.399], loss: 0.071328, mae: 0.274414, mean_q: 3.812906
 31241/100000: episode: 498, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 191.333, mean reward: 1.913 [1.459, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.362, 10.098], loss: 0.072702, mae: 0.271930, mean_q: 3.787639
 31341/100000: episode: 499, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 195.928, mean reward: 1.959 [1.429, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.890, 10.161], loss: 0.071491, mae: 0.277803, mean_q: 3.808905
 31441/100000: episode: 500, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 210.157, mean reward: 2.102 [1.519, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.926, 10.306], loss: 0.076871, mae: 0.276870, mean_q: 3.806425
 31541/100000: episode: 501, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 217.383, mean reward: 2.174 [1.489, 3.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.372, 10.208], loss: 0.075773, mae: 0.281779, mean_q: 3.831949
 31641/100000: episode: 502, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 185.482, mean reward: 1.855 [1.443, 2.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.099, 10.098], loss: 0.074107, mae: 0.274947, mean_q: 3.814071
 31741/100000: episode: 503, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.269, mean reward: 1.933 [1.498, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.325, 10.098], loss: 0.067657, mae: 0.269819, mean_q: 3.820032
 31841/100000: episode: 504, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 218.818, mean reward: 2.188 [1.461, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.465, 10.098], loss: 0.068592, mae: 0.269622, mean_q: 3.800885
 31941/100000: episode: 505, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 176.426, mean reward: 1.764 [1.449, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.443, 10.098], loss: 0.070053, mae: 0.274695, mean_q: 3.843251
 32041/100000: episode: 506, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.521, mean reward: 1.865 [1.455, 4.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.895, 10.389], loss: 0.072854, mae: 0.275445, mean_q: 3.819806
 32141/100000: episode: 507, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.152, mean reward: 1.872 [1.457, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.930, 10.235], loss: 0.068220, mae: 0.268383, mean_q: 3.815095
 32241/100000: episode: 508, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 179.469, mean reward: 1.795 [1.440, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.531, 10.140], loss: 0.074659, mae: 0.280789, mean_q: 3.835235
 32341/100000: episode: 509, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 189.080, mean reward: 1.891 [1.441, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.671, 10.157], loss: 0.067021, mae: 0.265301, mean_q: 3.801040
 32441/100000: episode: 510, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 214.049, mean reward: 2.140 [1.498, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.673, 10.261], loss: 0.075322, mae: 0.280785, mean_q: 3.812855
 32541/100000: episode: 511, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 193.406, mean reward: 1.934 [1.446, 4.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.809, 10.305], loss: 0.078167, mae: 0.285883, mean_q: 3.816510
 32641/100000: episode: 512, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 204.387, mean reward: 2.044 [1.476, 5.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.149, 10.399], loss: 0.078648, mae: 0.281414, mean_q: 3.825433
 32741/100000: episode: 513, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.244, mean reward: 1.882 [1.473, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.647, 10.201], loss: 0.085811, mae: 0.287760, mean_q: 3.818162
 32841/100000: episode: 514, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 198.999, mean reward: 1.990 [1.449, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.271, 10.247], loss: 0.065972, mae: 0.266688, mean_q: 3.813331
 32941/100000: episode: 515, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 203.911, mean reward: 2.039 [1.472, 4.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.940, 10.200], loss: 0.079191, mae: 0.285886, mean_q: 3.828645
 33041/100000: episode: 516, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 195.149, mean reward: 1.951 [1.492, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.794, 10.098], loss: 0.090832, mae: 0.298217, mean_q: 3.844993
 33141/100000: episode: 517, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 223.408, mean reward: 2.234 [1.457, 5.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.908, 10.098], loss: 0.078672, mae: 0.277773, mean_q: 3.836634
 33241/100000: episode: 518, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 184.354, mean reward: 1.844 [1.493, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.595, 10.186], loss: 0.067926, mae: 0.263691, mean_q: 3.825975
 33341/100000: episode: 519, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.781, mean reward: 1.898 [1.453, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.384, 10.227], loss: 0.082832, mae: 0.289472, mean_q: 3.840176
 33441/100000: episode: 520, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.558, mean reward: 1.836 [1.443, 2.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.577, 10.098], loss: 0.075323, mae: 0.280346, mean_q: 3.826021
 33541/100000: episode: 521, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 185.908, mean reward: 1.859 [1.460, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.609, 10.228], loss: 0.081129, mae: 0.282934, mean_q: 3.840308
 33641/100000: episode: 522, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 201.931, mean reward: 2.019 [1.452, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.468, 10.098], loss: 0.082974, mae: 0.288045, mean_q: 3.836079
 33741/100000: episode: 523, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 181.238, mean reward: 1.812 [1.451, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.698, 10.099], loss: 0.083412, mae: 0.287688, mean_q: 3.840974
 33841/100000: episode: 524, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.789, mean reward: 1.948 [1.499, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.370, 10.098], loss: 0.080693, mae: 0.278685, mean_q: 3.837451
 33941/100000: episode: 525, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 191.219, mean reward: 1.912 [1.461, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.223, 10.196], loss: 0.072677, mae: 0.268494, mean_q: 3.842016
 34041/100000: episode: 526, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 187.853, mean reward: 1.879 [1.464, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.410, 10.157], loss: 0.076816, mae: 0.281763, mean_q: 3.851181
 34141/100000: episode: 527, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 187.966, mean reward: 1.880 [1.450, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.992, 10.244], loss: 0.086471, mae: 0.289330, mean_q: 3.857291
 34241/100000: episode: 528, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 191.254, mean reward: 1.913 [1.494, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.258, 10.098], loss: 0.079699, mae: 0.276941, mean_q: 3.837105
 34341/100000: episode: 529, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.552, mean reward: 1.966 [1.483, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.893, 10.098], loss: 0.082920, mae: 0.286970, mean_q: 3.846236
 34441/100000: episode: 530, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 223.748, mean reward: 2.237 [1.475, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.186, 10.477], loss: 0.084806, mae: 0.291102, mean_q: 3.863466
 34541/100000: episode: 531, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 179.659, mean reward: 1.797 [1.451, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.004, 10.124], loss: 0.073036, mae: 0.271480, mean_q: 3.846261
 34641/100000: episode: 532, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 180.181, mean reward: 1.802 [1.449, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.661, 10.098], loss: 0.069258, mae: 0.268595, mean_q: 3.810878
[Info] 1-TH LEVEL FOUND: 5.419106960296631, Considering 10/90 traces
 34741/100000: episode: 533, duration: 4.720s, episode steps: 100, steps per second: 21, episode reward: 191.113, mean reward: 1.911 [1.447, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.579, 10.377], loss: 0.081052, mae: 0.278498, mean_q: 3.846326
 34777/100000: episode: 534, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 78.691, mean reward: 2.186 [1.529, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.545, 10.117], loss: 0.068335, mae: 0.263872, mean_q: 3.851316
 34799/100000: episode: 535, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 54.351, mean reward: 2.470 [2.143, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.643, 10.378], loss: 0.073081, mae: 0.273473, mean_q: 3.833616
 34821/100000: episode: 536, duration: 0.111s, episode steps: 22, steps per second: 197, episode reward: 53.677, mean reward: 2.440 [1.952, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.509, 10.345], loss: 0.068798, mae: 0.263080, mean_q: 3.861597
 34855/100000: episode: 537, duration: 0.163s, episode steps: 34, steps per second: 209, episode reward: 73.257, mean reward: 2.155 [1.582, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.288, 10.169], loss: 0.086002, mae: 0.286291, mean_q: 3.843021
 34891/100000: episode: 538, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 80.365, mean reward: 2.232 [1.638, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.244, 10.302], loss: 0.110580, mae: 0.311661, mean_q: 3.898128
 34925/100000: episode: 539, duration: 0.172s, episode steps: 34, steps per second: 198, episode reward: 70.968, mean reward: 2.087 [1.630, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.933, 10.304], loss: 0.086368, mae: 0.279966, mean_q: 3.842500
 34961/100000: episode: 540, duration: 0.175s, episode steps: 36, steps per second: 206, episode reward: 85.327, mean reward: 2.370 [1.919, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.036, 10.314], loss: 0.071222, mae: 0.274033, mean_q: 3.854298
 35002/100000: episode: 541, duration: 0.204s, episode steps: 41, steps per second: 201, episode reward: 113.931, mean reward: 2.779 [1.715, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.490, 10.100], loss: 0.072443, mae: 0.275916, mean_q: 3.855972
 35036/100000: episode: 542, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 70.389, mean reward: 2.070 [1.729, 2.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.408, 10.301], loss: 0.089361, mae: 0.295890, mean_q: 3.872848
 35077/100000: episode: 543, duration: 0.212s, episode steps: 41, steps per second: 193, episode reward: 115.363, mean reward: 2.814 [2.036, 5.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-1.211, 10.100], loss: 0.076911, mae: 0.285466, mean_q: 3.883112
 35113/100000: episode: 544, duration: 0.181s, episode steps: 36, steps per second: 198, episode reward: 85.891, mean reward: 2.386 [1.757, 4.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.788, 10.465], loss: 0.101607, mae: 0.301266, mean_q: 3.892211
 35120/100000: episode: 545, duration: 0.037s, episode steps: 7, steps per second: 192, episode reward: 20.443, mean reward: 2.920 [2.594, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.471], loss: 0.084558, mae: 0.285454, mean_q: 3.917136
 35154/100000: episode: 546, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 76.579, mean reward: 2.252 [1.594, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.392, 10.269], loss: 0.088581, mae: 0.284690, mean_q: 3.863965
 35190/100000: episode: 547, duration: 0.177s, episode steps: 36, steps per second: 203, episode reward: 84.932, mean reward: 2.359 [1.629, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.035, 10.100], loss: 0.089017, mae: 0.296222, mean_q: 3.919564
 35212/100000: episode: 548, duration: 0.109s, episode steps: 22, steps per second: 201, episode reward: 60.005, mean reward: 2.728 [1.979, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.995, 10.496], loss: 0.094076, mae: 0.300076, mean_q: 3.892949
 35253/100000: episode: 549, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 128.808, mean reward: 3.142 [1.707, 4.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.419, 10.100], loss: 0.081643, mae: 0.293730, mean_q: 3.926692
 35282/100000: episode: 550, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 73.741, mean reward: 2.543 [1.854, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.278], loss: 0.081475, mae: 0.292858, mean_q: 3.924090
 35323/100000: episode: 551, duration: 0.207s, episode steps: 41, steps per second: 198, episode reward: 82.506, mean reward: 2.012 [1.671, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.219, 10.100], loss: 0.070905, mae: 0.269179, mean_q: 3.908334
 35357/100000: episode: 552, duration: 0.163s, episode steps: 34, steps per second: 209, episode reward: 83.465, mean reward: 2.455 [1.495, 4.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.262, 10.129], loss: 0.093330, mae: 0.291726, mean_q: 3.950523
 35379/100000: episode: 553, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 58.000, mean reward: 2.636 [1.958, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.761, 10.299], loss: 0.085725, mae: 0.289174, mean_q: 3.972842
 35418/100000: episode: 554, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 98.042, mean reward: 2.514 [1.534, 4.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.889, 10.100], loss: 0.104272, mae: 0.325067, mean_q: 3.986574
 35440/100000: episode: 555, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 54.031, mean reward: 2.456 [1.999, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.336, 10.100], loss: 0.088939, mae: 0.296939, mean_q: 3.989148
 35474/100000: episode: 556, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 92.758, mean reward: 2.728 [1.971, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.490], loss: 0.086537, mae: 0.291404, mean_q: 3.932370
 35515/100000: episode: 557, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 151.229, mean reward: 3.689 [2.288, 5.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.373, 10.100], loss: 0.094097, mae: 0.303619, mean_q: 3.996409
 35554/100000: episode: 558, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 104.185, mean reward: 2.671 [1.858, 4.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.215, 10.100], loss: 0.095533, mae: 0.305748, mean_q: 3.992702
 35576/100000: episode: 559, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 49.588, mean reward: 2.254 [1.812, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.176, 10.295], loss: 0.096909, mae: 0.313581, mean_q: 4.027146
 35612/100000: episode: 560, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 90.110, mean reward: 2.503 [1.903, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.035, 10.299], loss: 0.090747, mae: 0.298303, mean_q: 4.022845
 35646/100000: episode: 561, duration: 0.170s, episode steps: 34, steps per second: 199, episode reward: 86.425, mean reward: 2.542 [1.865, 4.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.586, 10.432], loss: 0.080673, mae: 0.283321, mean_q: 3.982656
 35680/100000: episode: 562, duration: 0.164s, episode steps: 34, steps per second: 207, episode reward: 82.301, mean reward: 2.421 [1.654, 5.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.427, 10.227], loss: 0.098740, mae: 0.307632, mean_q: 4.057383
 35716/100000: episode: 563, duration: 0.177s, episode steps: 36, steps per second: 204, episode reward: 88.261, mean reward: 2.452 [1.662, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.446, 10.360], loss: 0.090168, mae: 0.304755, mean_q: 4.019875
 35745/100000: episode: 564, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 64.096, mean reward: 2.210 [1.752, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.089, 10.336], loss: 0.092952, mae: 0.305665, mean_q: 4.041609
 35779/100000: episode: 565, duration: 0.168s, episode steps: 34, steps per second: 203, episode reward: 85.534, mean reward: 2.516 [1.611, 4.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.731, 10.261], loss: 0.091658, mae: 0.308963, mean_q: 4.023853
 35801/100000: episode: 566, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 54.081, mean reward: 2.458 [1.688, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.819, 10.100], loss: 0.113709, mae: 0.325959, mean_q: 4.056497
 35837/100000: episode: 567, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 93.038, mean reward: 2.584 [1.922, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.252, 10.403], loss: 0.102524, mae: 0.317154, mean_q: 4.047184
 35871/100000: episode: 568, duration: 0.170s, episode steps: 34, steps per second: 200, episode reward: 191.732, mean reward: 5.639 [2.230, 78.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.195, 10.336], loss: 0.099535, mae: 0.305899, mean_q: 4.100954
 35910/100000: episode: 569, duration: 0.220s, episode steps: 39, steps per second: 177, episode reward: 90.997, mean reward: 2.333 [1.522, 4.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.579, 10.100], loss: 0.098260, mae: 0.317628, mean_q: 4.125832
 35917/100000: episode: 570, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 24.198, mean reward: 3.457 [3.097, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.509], loss: 0.091247, mae: 0.315549, mean_q: 4.102007
 35953/100000: episode: 571, duration: 0.176s, episode steps: 36, steps per second: 204, episode reward: 81.077, mean reward: 2.252 [1.695, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.126, 10.291], loss: 0.114019, mae: 0.340079, mean_q: 4.124146
 35987/100000: episode: 572, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 110.385, mean reward: 3.247 [2.161, 7.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.927, 10.360], loss: 0.118342, mae: 0.338457, mean_q: 4.072878
 36009/100000: episode: 573, duration: 0.111s, episode steps: 22, steps per second: 199, episode reward: 74.878, mean reward: 3.404 [2.298, 7.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.580, 10.100], loss: 0.097495, mae: 0.316100, mean_q: 4.135490
 36048/100000: episode: 574, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 105.916, mean reward: 2.716 [1.544, 6.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.152, 10.120], loss: 0.104764, mae: 0.320043, mean_q: 4.072370
 36089/100000: episode: 575, duration: 0.207s, episode steps: 41, steps per second: 198, episode reward: 99.764, mean reward: 2.433 [1.818, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.171, 10.100], loss: 0.114849, mae: 0.333291, mean_q: 4.141393
 36130/100000: episode: 576, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 121.128, mean reward: 2.954 [2.133, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.633, 10.100], loss: 0.119436, mae: 0.337308, mean_q: 4.159498
 36152/100000: episode: 577, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 47.275, mean reward: 2.149 [1.649, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.243], loss: 0.113161, mae: 0.331977, mean_q: 4.163618
 36174/100000: episode: 578, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 59.365, mean reward: 2.698 [1.881, 5.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.256, 10.100], loss: 0.118664, mae: 0.332978, mean_q: 4.158838
 36196/100000: episode: 579, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 48.904, mean reward: 2.223 [1.623, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.420, 10.178], loss: 0.097546, mae: 0.319061, mean_q: 4.192520
 36235/100000: episode: 580, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 112.609, mean reward: 2.887 [2.167, 4.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.698, 10.100], loss: 0.221674, mae: 0.468805, mean_q: 4.248550
 36276/100000: episode: 581, duration: 0.204s, episode steps: 41, steps per second: 201, episode reward: 131.573, mean reward: 3.209 [1.903, 5.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.369, 10.100], loss: 0.129275, mae: 0.346195, mean_q: 4.203169
 36310/100000: episode: 582, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 80.245, mean reward: 2.360 [1.819, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.320, 10.237], loss: 0.136548, mae: 0.350473, mean_q: 4.214409
 36339/100000: episode: 583, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 59.659, mean reward: 2.057 [1.601, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.177, 10.205], loss: 0.112980, mae: 0.331152, mean_q: 4.198555
 36346/100000: episode: 584, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 25.468, mean reward: 3.638 [3.324, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.514], loss: 0.159110, mae: 0.318920, mean_q: 4.119796
 36368/100000: episode: 585, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 57.390, mean reward: 2.609 [1.799, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.961, 10.100], loss: 4.287938, mae: 0.668348, mean_q: 4.180922
 36404/100000: episode: 586, duration: 0.177s, episode steps: 36, steps per second: 204, episode reward: 86.731, mean reward: 2.409 [1.504, 5.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.101, 10.127], loss: 2.619173, mae: 0.534130, mean_q: 4.227007
 36438/100000: episode: 587, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 82.880, mean reward: 2.438 [1.665, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.131, 10.108], loss: 0.125173, mae: 0.362837, mean_q: 4.171926
 36472/100000: episode: 588, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 103.970, mean reward: 3.058 [2.144, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.807, 10.427], loss: 0.118281, mae: 0.347898, mean_q: 4.229356
 36501/100000: episode: 589, duration: 0.141s, episode steps: 29, steps per second: 205, episode reward: 63.979, mean reward: 2.206 [1.561, 5.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.035, 10.286], loss: 0.129761, mae: 0.365791, mean_q: 4.204399
 36530/100000: episode: 590, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 66.467, mean reward: 2.292 [1.653, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.705, 10.157], loss: 0.106734, mae: 0.330177, mean_q: 4.243284
 36566/100000: episode: 591, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 234.675, mean reward: 6.519 [2.643, 19.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.554, 10.467], loss: 2.572617, mae: 0.426396, mean_q: 4.312235
 36588/100000: episode: 592, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 58.668, mean reward: 2.667 [2.013, 3.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.685, 10.100], loss: 0.326028, mae: 0.548968, mean_q: 4.250455
 36610/100000: episode: 593, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 62.303, mean reward: 2.832 [2.047, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.498, 10.100], loss: 0.499942, mae: 0.455813, mean_q: 4.353104
 36649/100000: episode: 594, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 91.194, mean reward: 2.338 [1.454, 6.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.596, 10.229], loss: 0.136313, mae: 0.336842, mean_q: 4.278150
 36683/100000: episode: 595, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 79.574, mean reward: 2.340 [1.636, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.193], loss: 0.272520, mae: 0.372784, mean_q: 4.281370
 36705/100000: episode: 596, duration: 0.114s, episode steps: 22, steps per second: 194, episode reward: 60.750, mean reward: 2.761 [2.072, 5.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.247, 10.100], loss: 0.315200, mae: 0.424804, mean_q: 4.398644
 36746/100000: episode: 597, duration: 0.213s, episode steps: 41, steps per second: 192, episode reward: 83.332, mean reward: 2.032 [1.678, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.650, 10.100], loss: 0.300727, mae: 0.435971, mean_q: 4.307495
 36780/100000: episode: 598, duration: 0.172s, episode steps: 34, steps per second: 198, episode reward: 131.141, mean reward: 3.857 [2.197, 6.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.035, 10.499], loss: 0.238239, mae: 0.390145, mean_q: 4.300706
 36814/100000: episode: 599, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 78.239, mean reward: 2.301 [1.502, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.652, 10.261], loss: 0.149888, mae: 0.376458, mean_q: 4.227680
 36836/100000: episode: 600, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 44.757, mean reward: 2.034 [1.696, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.161, 10.100], loss: 0.303025, mae: 0.520061, mean_q: 4.342949
 36872/100000: episode: 601, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 86.186, mean reward: 2.394 [1.643, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.176, 10.251], loss: 0.161744, mae: 0.387814, mean_q: 4.308747
 36913/100000: episode: 602, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 114.636, mean reward: 2.796 [1.964, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.532, 10.100], loss: 4.662211, mae: 0.691203, mean_q: 4.457144
 36954/100000: episode: 603, duration: 0.212s, episode steps: 41, steps per second: 193, episode reward: 104.053, mean reward: 2.538 [1.640, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.494, 10.100], loss: 0.210864, mae: 0.434429, mean_q: 4.302131
 36988/100000: episode: 604, duration: 0.163s, episode steps: 34, steps per second: 208, episode reward: 79.533, mean reward: 2.339 [1.987, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.050, 10.440], loss: 0.403913, mae: 0.450460, mean_q: 4.397876
 37010/100000: episode: 605, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 73.746, mean reward: 3.352 [2.490, 6.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.441], loss: 0.200160, mae: 0.416528, mean_q: 4.429590
 37039/100000: episode: 606, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 97.436, mean reward: 3.360 [2.079, 6.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.660, 10.616], loss: 0.447901, mae: 0.452504, mean_q: 4.435891
 37061/100000: episode: 607, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 58.953, mean reward: 2.680 [1.983, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.336, 10.363], loss: 0.353482, mae: 0.415037, mean_q: 4.442794
 37097/100000: episode: 608, duration: 0.176s, episode steps: 36, steps per second: 204, episode reward: 85.811, mean reward: 2.384 [1.475, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.737, 10.199], loss: 2.571266, mae: 0.465160, mean_q: 4.351312
 37119/100000: episode: 609, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 61.605, mean reward: 2.800 [2.020, 4.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.320], loss: 0.282028, mae: 0.518696, mean_q: 4.397830
 37141/100000: episode: 610, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 50.080, mean reward: 2.276 [1.871, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.955, 10.285], loss: 0.172823, mae: 0.382099, mean_q: 4.419854
 37148/100000: episode: 611, duration: 0.043s, episode steps: 7, steps per second: 165, episode reward: 21.090, mean reward: 3.013 [2.536, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.398], loss: 0.217734, mae: 0.393998, mean_q: 4.566052
 37170/100000: episode: 612, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 60.852, mean reward: 2.766 [2.399, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.321, 10.100], loss: 0.341580, mae: 0.404580, mean_q: 4.476225
 37192/100000: episode: 613, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 48.300, mean reward: 2.195 [1.597, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.576, 10.177], loss: 0.366731, mae: 0.419018, mean_q: 4.422152
 37228/100000: episode: 614, duration: 0.197s, episode steps: 36, steps per second: 182, episode reward: 83.794, mean reward: 2.328 [1.529, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.272, 10.202], loss: 0.177067, mae: 0.395241, mean_q: 4.378437
 37250/100000: episode: 615, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 58.195, mean reward: 2.645 [1.837, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.323, 10.315], loss: 0.163331, mae: 0.377944, mean_q: 4.351418
 37286/100000: episode: 616, duration: 0.185s, episode steps: 36, steps per second: 194, episode reward: 75.208, mean reward: 2.089 [1.542, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.352, 10.214], loss: 2.853616, mae: 0.608436, mean_q: 4.570174
 37308/100000: episode: 617, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 49.523, mean reward: 2.251 [1.827, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.418, 10.464], loss: 0.155355, mae: 0.396447, mean_q: 4.474749
 37349/100000: episode: 618, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 114.885, mean reward: 2.802 [2.092, 4.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.552, 10.100], loss: 0.256106, mae: 0.409150, mean_q: 4.469088
 37383/100000: episode: 619, duration: 0.171s, episode steps: 34, steps per second: 199, episode reward: 77.037, mean reward: 2.266 [1.490, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.256, 10.104], loss: 0.156840, mae: 0.362333, mean_q: 4.375024
 37390/100000: episode: 620, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 21.101, mean reward: 3.014 [2.162, 6.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.333], loss: 0.825397, mae: 0.642721, mean_q: 4.782547
 37426/100000: episode: 621, duration: 0.183s, episode steps: 36, steps per second: 196, episode reward: 94.012, mean reward: 2.611 [1.901, 7.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.988, 10.334], loss: 0.170646, mae: 0.398374, mean_q: 4.466527
 37455/100000: episode: 622, duration: 0.154s, episode steps: 29, steps per second: 189, episode reward: 89.191, mean reward: 3.076 [2.508, 4.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.539, 10.454], loss: 3.174146, mae: 0.468086, mean_q: 4.494823
[Info] 2-TH LEVEL FOUND: 7.7813544273376465, Considering 10/90 traces
 37477/100000: episode: 623, duration: 4.329s, episode steps: 22, steps per second: 5, episode reward: 59.564, mean reward: 2.707 [2.029, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.373], loss: 0.927098, mae: 0.651888, mean_q: 4.613497
 37506/100000: episode: 624, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 103.490, mean reward: 3.569 [2.133, 9.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.439], loss: 0.179420, mae: 0.411326, mean_q: 4.459464
 37523/100000: episode: 625, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 44.886, mean reward: 2.640 [2.242, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.236, 10.372], loss: 0.200482, mae: 0.413755, mean_q: 4.479878
 37555/100000: episode: 626, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 109.041, mean reward: 3.408 [1.647, 6.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.432, 10.253], loss: 0.184681, mae: 0.396328, mean_q: 4.538026
 37572/100000: episode: 627, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 36.956, mean reward: 2.174 [1.752, 3.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.258], loss: 5.924397, mae: 0.873576, mean_q: 4.937626
 37601/100000: episode: 628, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 135.103, mean reward: 4.659 [1.936, 13.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.365, 10.331], loss: 0.263711, mae: 0.497263, mean_q: 4.492093
 37620/100000: episode: 629, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 54.056, mean reward: 2.845 [2.183, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.461, 10.407], loss: 0.186404, mae: 0.412246, mean_q: 4.596345
 37644/100000: episode: 630, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 75.954, mean reward: 3.165 [1.776, 5.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.366], loss: 0.265092, mae: 0.393723, mean_q: 4.558538
 37671/100000: episode: 631, duration: 0.151s, episode steps: 27, steps per second: 178, episode reward: 82.969, mean reward: 3.073 [1.949, 4.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.478, 10.388], loss: 0.541066, mae: 0.591188, mean_q: 4.502558
 37688/100000: episode: 632, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 64.015, mean reward: 3.766 [2.450, 5.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.615, 10.400], loss: 0.546621, mae: 0.589879, mean_q: 4.721157
 37709/100000: episode: 633, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 54.627, mean reward: 2.601 [1.926, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.495, 10.275], loss: 0.308182, mae: 0.444928, mean_q: 4.647279
 37730/100000: episode: 634, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 64.251, mean reward: 3.060 [2.500, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.379, 10.428], loss: 0.192540, mae: 0.441115, mean_q: 4.598102
 37754/100000: episode: 635, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 55.599, mean reward: 2.317 [1.871, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.323], loss: 0.612560, mae: 0.554241, mean_q: 4.693958
 37781/100000: episode: 636, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 82.773, mean reward: 3.066 [2.174, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.568], loss: 0.243298, mae: 0.490788, mean_q: 4.662986
 37800/100000: episode: 637, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 67.461, mean reward: 3.551 [2.660, 4.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.562], loss: 0.266835, mae: 0.442409, mean_q: 4.635278
 37827/100000: episode: 638, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 76.370, mean reward: 2.829 [2.153, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.692, 10.403], loss: 0.282482, mae: 0.417741, mean_q: 4.618620
 37844/100000: episode: 639, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 80.258, mean reward: 4.721 [3.027, 7.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.177, 10.535], loss: 5.623423, mae: 0.792449, mean_q: 4.663299
 37868/100000: episode: 640, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 75.566, mean reward: 3.149 [2.664, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.221, 10.504], loss: 0.355242, mae: 0.567018, mean_q: 4.750087
 37896/100000: episode: 641, duration: 0.138s, episode steps: 28, steps per second: 204, episode reward: 65.423, mean reward: 2.337 [1.579, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.036, 10.180], loss: 3.367952, mae: 0.539563, mean_q: 4.683418
 37913/100000: episode: 642, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 50.824, mean reward: 2.990 [2.380, 4.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.571], loss: 0.277384, mae: 0.504484, mean_q: 4.642436
 37941/100000: episode: 643, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 81.853, mean reward: 2.923 [1.795, 5.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.308, 10.280], loss: 3.280097, mae: 0.494664, mean_q: 4.593905
 37969/100000: episode: 644, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 81.598, mean reward: 2.914 [2.394, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.239, 10.497], loss: 6.938935, mae: 0.921170, mean_q: 4.942504
 37997/100000: episode: 645, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 77.187, mean reward: 2.757 [1.909, 4.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.272, 10.414], loss: 0.211439, mae: 0.449064, mean_q: 4.734264
 38021/100000: episode: 646, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 70.819, mean reward: 2.951 [1.909, 5.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.646, 10.393], loss: 0.381171, mae: 0.458191, mean_q: 4.739465
 38049/100000: episode: 647, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 84.732, mean reward: 3.026 [2.430, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.752, 10.485], loss: 0.360696, mae: 0.442346, mean_q: 4.643258
 38070/100000: episode: 648, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 46.153, mean reward: 2.198 [1.460, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.076, 10.115], loss: 4.603348, mae: 0.691210, mean_q: 5.032799
 38098/100000: episode: 649, duration: 0.136s, episode steps: 28, steps per second: 205, episode reward: 85.830, mean reward: 3.065 [2.433, 4.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.144, 10.365], loss: 0.198950, mae: 0.414775, mean_q: 4.824081
 38117/100000: episode: 650, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 58.078, mean reward: 3.057 [2.620, 5.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.047, 10.533], loss: 5.065600, mae: 0.687279, mean_q: 4.918950
 38145/100000: episode: 651, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 69.399, mean reward: 2.479 [1.902, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.503, 10.271], loss: 0.181641, mae: 0.427870, mean_q: 4.770324
 38177/100000: episode: 652, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 75.078, mean reward: 2.346 [1.622, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.449, 10.285], loss: 0.204836, mae: 0.427540, mean_q: 4.781044
 38209/100000: episode: 653, duration: 0.158s, episode steps: 32, steps per second: 203, episode reward: 119.816, mean reward: 3.744 [2.669, 5.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.276, 10.356], loss: 0.211150, mae: 0.434457, mean_q: 4.780187
 38237/100000: episode: 654, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 87.422, mean reward: 3.122 [2.031, 5.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.460], loss: 0.374513, mae: 0.446898, mean_q: 4.798350
 38269/100000: episode: 655, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 119.467, mean reward: 3.733 [2.762, 6.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.613, 10.498], loss: 0.349579, mae: 0.463802, mean_q: 4.859435
 38293/100000: episode: 656, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 92.026, mean reward: 3.834 [2.554, 6.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.791, 10.561], loss: 0.253955, mae: 0.431332, mean_q: 4.839874
 38312/100000: episode: 657, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 62.903, mean reward: 3.311 [2.594, 4.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-1.157, 10.569], loss: 0.167237, mae: 0.394830, mean_q: 4.938792
 38344/100000: episode: 658, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 91.059, mean reward: 2.846 [1.785, 5.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.242, 10.254], loss: 0.190246, mae: 0.436494, mean_q: 4.949345
 38368/100000: episode: 659, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 64.907, mean reward: 2.704 [1.738, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.037, 10.318], loss: 0.191547, mae: 0.407943, mean_q: 4.803782
 38387/100000: episode: 660, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 91.829, mean reward: 4.833 [3.219, 7.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.606], loss: 0.502067, mae: 0.539008, mean_q: 4.950100
 38406/100000: episode: 661, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 63.284, mean reward: 3.331 [2.531, 5.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.475], loss: 0.350993, mae: 0.445219, mean_q: 4.897609
 38434/100000: episode: 662, duration: 0.134s, episode steps: 28, steps per second: 209, episode reward: 78.359, mean reward: 2.799 [2.324, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.414, 10.379], loss: 0.272211, mae: 0.475413, mean_q: 4.883616
 38453/100000: episode: 663, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 45.472, mean reward: 2.393 [1.827, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.307], loss: 0.228010, mae: 0.413487, mean_q: 4.784944
 38481/100000: episode: 664, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 129.594, mean reward: 4.628 [2.705, 7.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.103, 10.550], loss: 0.204049, mae: 0.450418, mean_q: 4.996854
 38505/100000: episode: 665, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 66.200, mean reward: 2.758 [2.028, 3.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.599, 10.393], loss: 0.235866, mae: 0.467401, mean_q: 5.022750
 38524/100000: episode: 666, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 53.660, mean reward: 2.824 [2.139, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.257, 10.477], loss: 0.365191, mae: 0.440587, mean_q: 5.013710
 38553/100000: episode: 667, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 107.878, mean reward: 3.720 [2.139, 5.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.151, 10.361], loss: 0.195952, mae: 0.436089, mean_q: 4.880511
 38585/100000: episode: 668, duration: 0.158s, episode steps: 32, steps per second: 203, episode reward: 93.607, mean reward: 2.925 [1.951, 4.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.581, 10.554], loss: 0.181838, mae: 0.420576, mean_q: 4.960254
 38612/100000: episode: 669, duration: 0.131s, episode steps: 27, steps per second: 206, episode reward: 62.478, mean reward: 2.314 [1.537, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.147, 10.217], loss: 0.372646, mae: 0.475888, mean_q: 4.988541
 38636/100000: episode: 670, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 56.527, mean reward: 2.355 [1.657, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.605, 10.285], loss: 0.251123, mae: 0.497481, mean_q: 5.067493
 38663/100000: episode: 671, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 82.402, mean reward: 3.052 [1.784, 6.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.970, 10.252], loss: 0.333370, mae: 0.489389, mean_q: 4.972297
 38690/100000: episode: 672, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 72.754, mean reward: 2.695 [1.899, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.031, 10.315], loss: 7.436609, mae: 0.958664, mean_q: 5.164295
 38719/100000: episode: 673, duration: 0.148s, episode steps: 29, steps per second: 197, episode reward: 182.617, mean reward: 6.297 [2.218, 16.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.820, 10.361], loss: 0.927366, mae: 0.686782, mean_q: 5.096570
 38738/100000: episode: 674, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 65.773, mean reward: 3.462 [2.267, 5.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.476, 10.467], loss: 0.385246, mae: 0.541824, mean_q: 5.069639
 38759/100000: episode: 675, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 42.603, mean reward: 2.029 [1.600, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.228, 10.210], loss: 0.376125, mae: 0.518275, mean_q: 5.089420
 38791/100000: episode: 676, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 116.853, mean reward: 3.652 [2.231, 8.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.233, 10.427], loss: 0.373464, mae: 0.486257, mean_q: 5.043513
 38810/100000: episode: 677, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 46.126, mean reward: 2.428 [2.036, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.496, 10.394], loss: 0.354010, mae: 0.542971, mean_q: 5.190524
 38834/100000: episode: 678, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 80.335, mean reward: 3.347 [2.511, 5.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.209, 10.382], loss: 0.362225, mae: 0.507580, mean_q: 5.104431
 38866/100000: episode: 679, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 81.818, mean reward: 2.557 [1.467, 4.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.792, 10.100], loss: 3.302597, mae: 0.686527, mean_q: 5.240573
 38887/100000: episode: 680, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 53.601, mean reward: 2.552 [1.934, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.269], loss: 4.858195, mae: 0.913704, mean_q: 5.143014
 38914/100000: episode: 681, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 69.343, mean reward: 2.568 [1.874, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.272, 10.328], loss: 0.457223, mae: 0.549625, mean_q: 4.949623
 38946/100000: episode: 682, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 72.592, mean reward: 2.268 [1.589, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.844, 10.236], loss: 0.414452, mae: 0.519344, mean_q: 5.137650
 38965/100000: episode: 683, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 73.152, mean reward: 3.850 [2.452, 5.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.589], loss: 0.453535, mae: 0.541710, mean_q: 5.286920
 38992/100000: episode: 684, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 76.600, mean reward: 2.837 [2.171, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.405], loss: 0.258269, mae: 0.487509, mean_q: 5.027671
 39020/100000: episode: 685, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 77.628, mean reward: 2.772 [2.065, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.436], loss: 0.387582, mae: 0.532370, mean_q: 5.253638
 39044/100000: episode: 686, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 57.227, mean reward: 2.384 [1.721, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.091, 10.282], loss: 0.239762, mae: 0.499031, mean_q: 5.219061
 39072/100000: episode: 687, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 151.378, mean reward: 5.406 [2.410, 12.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.423], loss: 0.433526, mae: 0.529652, mean_q: 5.190037
 39091/100000: episode: 688, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 71.226, mean reward: 3.749 [2.662, 5.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.819, 10.445], loss: 0.518869, mae: 0.555679, mean_q: 5.356822
 39118/100000: episode: 689, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 71.457, mean reward: 2.647 [1.874, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.081, 10.308], loss: 0.520682, mae: 0.574302, mean_q: 5.357088
 39146/100000: episode: 690, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 96.807, mean reward: 3.457 [2.771, 4.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.462], loss: 0.412675, mae: 0.536350, mean_q: 5.263667
 39163/100000: episode: 691, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 56.119, mean reward: 3.301 [2.395, 4.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.537], loss: 0.540141, mae: 0.538186, mean_q: 5.266478
 39195/100000: episode: 692, duration: 0.162s, episode steps: 32, steps per second: 198, episode reward: 102.012, mean reward: 3.188 [2.014, 5.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.346, 10.355], loss: 3.252736, mae: 0.693256, mean_q: 5.414381
 39222/100000: episode: 693, duration: 0.133s, episode steps: 27, steps per second: 204, episode reward: 81.920, mean reward: 3.034 [2.253, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.407], loss: 0.483965, mae: 0.608397, mean_q: 5.101211
 39241/100000: episode: 694, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 78.198, mean reward: 4.116 [2.070, 9.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.683, 10.364], loss: 0.466787, mae: 0.567637, mean_q: 5.404376
 39260/100000: episode: 695, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 54.533, mean reward: 2.870 [2.441, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.290, 10.441], loss: 0.556365, mae: 0.580585, mean_q: 5.302211
 39279/100000: episode: 696, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 62.303, mean reward: 3.279 [2.446, 5.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.774, 10.528], loss: 0.624706, mae: 0.601783, mean_q: 5.275589
 39298/100000: episode: 697, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 45.398, mean reward: 2.389 [2.071, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.368, 10.339], loss: 0.730104, mae: 0.602819, mean_q: 5.424558
 39325/100000: episode: 698, duration: 0.129s, episode steps: 27, steps per second: 209, episode reward: 63.890, mean reward: 2.366 [1.646, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.695, 10.322], loss: 0.554535, mae: 0.586043, mean_q: 5.296908
 39352/100000: episode: 699, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 81.521, mean reward: 3.019 [1.898, 11.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.294, 10.367], loss: 3.698853, mae: 0.634032, mean_q: 5.434894
 39376/100000: episode: 700, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 70.138, mean reward: 2.922 [2.303, 4.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.578, 10.437], loss: 0.608642, mae: 0.725484, mean_q: 5.340765
 39403/100000: episode: 701, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 103.758, mean reward: 3.843 [2.925, 6.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.473, 10.483], loss: 0.811269, mae: 0.666775, mean_q: 5.479322
 39430/100000: episode: 702, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 106.814, mean reward: 3.956 [2.690, 5.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.220, 10.584], loss: 0.510929, mae: 0.548284, mean_q: 5.440001
 39459/100000: episode: 703, duration: 0.148s, episode steps: 29, steps per second: 195, episode reward: 93.481, mean reward: 3.223 [2.195, 4.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.183, 10.399], loss: 0.342191, mae: 0.543865, mean_q: 5.374863
 39486/100000: episode: 704, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 76.773, mean reward: 2.843 [1.961, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.065, 10.290], loss: 4.091665, mae: 0.950896, mean_q: 5.594463
 39514/100000: episode: 705, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 80.857, mean reward: 2.888 [2.241, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.386, 10.399], loss: 0.566413, mae: 0.650014, mean_q: 5.359141
 39543/100000: episode: 706, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 114.234, mean reward: 3.939 [2.311, 6.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.819, 10.431], loss: 0.493257, mae: 0.542542, mean_q: 5.386345
 39572/100000: episode: 707, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 87.226, mean reward: 3.008 [1.844, 6.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.303, 10.344], loss: 0.591498, mae: 0.595209, mean_q: 5.535970
 39591/100000: episode: 708, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 54.056, mean reward: 2.845 [2.158, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.450], loss: 0.537068, mae: 0.612661, mean_q: 5.544472
 39620/100000: episode: 709, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 98.977, mean reward: 3.413 [2.038, 20.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.470, 10.408], loss: 0.472729, mae: 0.595164, mean_q: 5.568399
 39652/100000: episode: 710, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 119.225, mean reward: 3.726 [2.041, 4.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.345, 10.389], loss: 0.630727, mae: 0.551293, mean_q: 5.461191
 39681/100000: episode: 711, duration: 0.141s, episode steps: 29, steps per second: 206, episode reward: 79.974, mean reward: 2.758 [2.115, 6.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.239, 10.486], loss: 0.477065, mae: 0.592742, mean_q: 5.493170
 39705/100000: episode: 712, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 61.535, mean reward: 2.564 [2.135, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.150, 10.379], loss: 4.505355, mae: 0.783394, mean_q: 5.593117
[Info] 3-TH LEVEL FOUND: 8.074874877929688, Considering 10/90 traces
 39724/100000: episode: 713, duration: 4.319s, episode steps: 19, steps per second: 4, episode reward: 60.709, mean reward: 3.195 [2.327, 4.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.352, 10.450], loss: 0.536825, mae: 0.711752, mean_q: 5.523619
 39748/100000: episode: 714, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 117.856, mean reward: 4.911 [2.564, 9.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.624, 10.475], loss: 0.544259, mae: 0.604673, mean_q: 5.477004
 39774/100000: episode: 715, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 114.615, mean reward: 4.408 [2.643, 6.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.439], loss: 0.660482, mae: 0.625163, mean_q: 5.521172
 39791/100000: episode: 716, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 77.203, mean reward: 4.541 [3.725, 5.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.492], loss: 0.391754, mae: 0.572185, mean_q: 5.510088
 39808/100000: episode: 717, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 66.953, mean reward: 3.938 [3.113, 4.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.551], loss: 5.738678, mae: 0.849889, mean_q: 5.711437
 39835/100000: episode: 718, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 129.877, mean reward: 4.810 [2.809, 10.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.639, 10.607], loss: 0.402060, mae: 0.587876, mean_q: 5.656086
 39862/100000: episode: 719, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 142.305, mean reward: 5.271 [2.731, 9.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.682], loss: 6.933762, mae: 0.923525, mean_q: 5.843050
 39887/100000: episode: 720, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 106.110, mean reward: 4.244 [2.147, 10.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.796, 10.338], loss: 4.069681, mae: 0.798248, mean_q: 5.859637
 39905/100000: episode: 721, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 65.822, mean reward: 3.657 [2.865, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.162, 10.533], loss: 0.963095, mae: 0.715346, mean_q: 5.662132
 39930/100000: episode: 722, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 96.143, mean reward: 3.846 [2.295, 6.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.806, 10.376], loss: 0.509145, mae: 0.599483, mean_q: 5.711361
 39956/100000: episode: 723, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 78.513, mean reward: 3.020 [1.983, 5.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.346], loss: 0.470903, mae: 0.583666, mean_q: 5.716189
 39982/100000: episode: 724, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 85.593, mean reward: 3.292 [1.971, 5.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.040, 10.315], loss: 0.485216, mae: 0.597426, mean_q: 5.718338
 40008/100000: episode: 725, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 134.269, mean reward: 5.164 [3.397, 10.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.643], loss: 0.351473, mae: 0.572960, mean_q: 5.778146
 40034/100000: episode: 726, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 122.375, mean reward: 4.707 [3.169, 7.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.613], loss: 0.277577, mae: 0.523779, mean_q: 5.662410
 40059/100000: episode: 727, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 66.349, mean reward: 2.654 [1.785, 4.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.311], loss: 0.640502, mae: 0.666214, mean_q: 5.805915
 40084/100000: episode: 728, duration: 0.123s, episode steps: 25, steps per second: 203, episode reward: 97.022, mean reward: 3.881 [2.897, 6.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.572], loss: 0.434621, mae: 0.569489, mean_q: 5.718859
[Info] FALSIFICATION!
[Info] Levels: [5.419107, 7.7813544, 8.074875, 10.036419]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.15]
[Info] Error Prob: 0.00015000000000000004

 40097/100000: episode: 729, duration: 4.447s, episode steps: 13, steps per second: 3, episode reward: 180.311, mean reward: 13.870 [3.257, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.901, 9.695], loss: 1.136372, mae: 0.717792, mean_q: 5.857760
 40197/100000: episode: 730, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 201.747, mean reward: 2.017 [1.478, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.385, 10.470], loss: 1.968293, mae: 0.727021, mean_q: 5.783925
 40297/100000: episode: 731, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 197.079, mean reward: 1.971 [1.498, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.721, 10.098], loss: 1.994702, mae: 0.745986, mean_q: 5.759805
 40397/100000: episode: 732, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 194.026, mean reward: 1.940 [1.444, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.804, 10.311], loss: 1.975663, mae: 0.699496, mean_q: 5.706097
 40497/100000: episode: 733, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 217.986, mean reward: 2.180 [1.497, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.859, 10.408], loss: 0.602395, mae: 0.613945, mean_q: 5.654679
 40597/100000: episode: 734, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 185.101, mean reward: 1.851 [1.461, 2.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.747, 10.098], loss: 1.376566, mae: 0.608928, mean_q: 5.695844
 40697/100000: episode: 735, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 194.537, mean reward: 1.945 [1.462, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.623, 10.098], loss: 0.668309, mae: 0.651338, mean_q: 5.655663
 40797/100000: episode: 736, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 181.449, mean reward: 1.814 [1.481, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.684, 10.143], loss: 1.938792, mae: 0.673486, mean_q: 5.634532
 40897/100000: episode: 737, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.293, mean reward: 1.853 [1.475, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.143, 10.232], loss: 1.869572, mae: 0.702716, mean_q: 5.665350
 40997/100000: episode: 738, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 191.661, mean reward: 1.917 [1.466, 4.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.435, 10.098], loss: 0.494446, mae: 0.576306, mean_q: 5.521860
 41097/100000: episode: 739, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 190.615, mean reward: 1.906 [1.481, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.986, 10.098], loss: 0.477155, mae: 0.558211, mean_q: 5.521572
 41197/100000: episode: 740, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 183.872, mean reward: 1.839 [1.479, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.703, 10.127], loss: 4.712133, mae: 0.859553, mean_q: 5.578401
 41297/100000: episode: 741, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 209.096, mean reward: 2.091 [1.435, 9.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.654, 10.415], loss: 0.640159, mae: 0.614978, mean_q: 5.452158
 41397/100000: episode: 742, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 178.143, mean reward: 1.781 [1.449, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.653, 10.160], loss: 0.384218, mae: 0.539341, mean_q: 5.379846
 41497/100000: episode: 743, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.602, mean reward: 1.916 [1.484, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.342, 10.242], loss: 1.816127, mae: 0.688036, mean_q: 5.367096
 41597/100000: episode: 744, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 192.679, mean reward: 1.927 [1.494, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.911, 10.098], loss: 3.322101, mae: 0.718283, mean_q: 5.399438
 41697/100000: episode: 745, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 198.062, mean reward: 1.981 [1.487, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.039, 10.255], loss: 0.445989, mae: 0.549406, mean_q: 5.362934
 41797/100000: episode: 746, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 179.113, mean reward: 1.791 [1.468, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.140, 10.098], loss: 0.419709, mae: 0.520235, mean_q: 5.323164
 41897/100000: episode: 747, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 192.439, mean reward: 1.924 [1.458, 3.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.688, 10.098], loss: 0.343940, mae: 0.485922, mean_q: 5.191183
 41997/100000: episode: 748, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 184.202, mean reward: 1.842 [1.464, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.624, 10.136], loss: 0.379669, mae: 0.514574, mean_q: 5.285658
 42097/100000: episode: 749, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 186.899, mean reward: 1.869 [1.451, 2.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.573, 10.290], loss: 1.989127, mae: 0.651401, mean_q: 5.312483
 42197/100000: episode: 750, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 199.172, mean reward: 1.992 [1.500, 3.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.656, 10.211], loss: 1.847625, mae: 0.651049, mean_q: 5.312025
 42297/100000: episode: 751, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 200.135, mean reward: 2.001 [1.450, 9.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.218, 10.243], loss: 3.155164, mae: 0.621819, mean_q: 5.219389
 42397/100000: episode: 752, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 183.384, mean reward: 1.834 [1.460, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.901, 10.241], loss: 1.730703, mae: 0.591526, mean_q: 5.176752
 42497/100000: episode: 753, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 183.829, mean reward: 1.838 [1.456, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.881, 10.168], loss: 0.413830, mae: 0.513291, mean_q: 5.039343
 42597/100000: episode: 754, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 193.559, mean reward: 1.936 [1.460, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.994, 10.211], loss: 0.504524, mae: 0.517444, mean_q: 5.025248
 42697/100000: episode: 755, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 185.343, mean reward: 1.853 [1.464, 2.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.574, 10.098], loss: 0.381840, mae: 0.490501, mean_q: 5.030762
 42797/100000: episode: 756, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 197.020, mean reward: 1.970 [1.497, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.572, 10.321], loss: 1.702496, mae: 0.562696, mean_q: 4.931469
 42897/100000: episode: 757, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 219.825, mean reward: 2.198 [1.456, 5.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.499, 10.358], loss: 3.099760, mae: 0.598627, mean_q: 4.893214
 42997/100000: episode: 758, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 210.143, mean reward: 2.101 [1.483, 4.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.036, 10.098], loss: 3.255388, mae: 0.705590, mean_q: 5.055891
 43097/100000: episode: 759, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 219.984, mean reward: 2.200 [1.461, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.458, 10.098], loss: 0.445553, mae: 0.475930, mean_q: 4.883684
 43197/100000: episode: 760, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 200.999, mean reward: 2.010 [1.495, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.453, 10.266], loss: 1.782263, mae: 0.529452, mean_q: 4.880630
 43297/100000: episode: 761, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 190.254, mean reward: 1.903 [1.454, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.438, 10.098], loss: 1.701879, mae: 0.568551, mean_q: 4.861595
 43397/100000: episode: 762, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 196.191, mean reward: 1.962 [1.460, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.276, 10.098], loss: 0.293929, mae: 0.433305, mean_q: 4.609500
 43497/100000: episode: 763, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 203.307, mean reward: 2.033 [1.499, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.415, 10.183], loss: 0.408945, mae: 0.521169, mean_q: 4.679625
 43597/100000: episode: 764, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 198.367, mean reward: 1.984 [1.469, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.423, 10.098], loss: 0.371881, mae: 0.446222, mean_q: 4.690921
 43697/100000: episode: 765, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 181.966, mean reward: 1.820 [1.447, 2.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.302, 10.098], loss: 0.448654, mae: 0.451110, mean_q: 4.622225
 43797/100000: episode: 766, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 192.741, mean reward: 1.927 [1.443, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.732, 10.141], loss: 1.733567, mae: 0.505585, mean_q: 4.587429
 43897/100000: episode: 767, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 194.494, mean reward: 1.945 [1.489, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.916, 10.098], loss: 0.388628, mae: 0.466757, mean_q: 4.562908
 43997/100000: episode: 768, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 218.445, mean reward: 2.184 [1.468, 4.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.959, 10.098], loss: 0.272795, mae: 0.418017, mean_q: 4.528306
 44097/100000: episode: 769, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 203.642, mean reward: 2.036 [1.553, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.135, 10.098], loss: 2.982093, mae: 0.547712, mean_q: 4.541233
 44197/100000: episode: 770, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.380, mean reward: 1.964 [1.498, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.940, 10.291], loss: 0.481795, mae: 0.434285, mean_q: 4.471771
 44297/100000: episode: 771, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 213.521, mean reward: 2.135 [1.474, 8.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.990, 10.403], loss: 0.287921, mae: 0.388001, mean_q: 4.370474
 44397/100000: episode: 772, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 186.089, mean reward: 1.861 [1.447, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.228, 10.098], loss: 1.635681, mae: 0.467456, mean_q: 4.397600
 44497/100000: episode: 773, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 191.026, mean reward: 1.910 [1.466, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.544, 10.222], loss: 0.371084, mae: 0.436045, mean_q: 4.274051
 44597/100000: episode: 774, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 189.252, mean reward: 1.893 [1.449, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.912, 10.257], loss: 1.535976, mae: 0.441320, mean_q: 4.261306
 44697/100000: episode: 775, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 186.538, mean reward: 1.865 [1.462, 2.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.693, 10.326], loss: 0.165481, mae: 0.341409, mean_q: 4.186983
 44797/100000: episode: 776, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 185.452, mean reward: 1.855 [1.463, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.242, 10.098], loss: 0.156135, mae: 0.330046, mean_q: 4.105913
 44897/100000: episode: 777, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 191.573, mean reward: 1.916 [1.482, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.406, 10.259], loss: 1.565554, mae: 0.412515, mean_q: 4.051383
 44997/100000: episode: 778, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 188.865, mean reward: 1.889 [1.466, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.413, 10.180], loss: 0.167870, mae: 0.324898, mean_q: 3.944207
 45097/100000: episode: 779, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.029, mean reward: 1.910 [1.439, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.542, 10.098], loss: 0.112009, mae: 0.301401, mean_q: 3.851855
 45197/100000: episode: 780, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 195.948, mean reward: 1.959 [1.483, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.839, 10.098], loss: 0.091874, mae: 0.283555, mean_q: 3.851872
 45297/100000: episode: 781, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 196.606, mean reward: 1.966 [1.501, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.214, 10.383], loss: 0.093324, mae: 0.281637, mean_q: 3.842554
 45397/100000: episode: 782, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 188.196, mean reward: 1.882 [1.447, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.839, 10.098], loss: 0.091772, mae: 0.272620, mean_q: 3.840022
 45497/100000: episode: 783, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 184.820, mean reward: 1.848 [1.480, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.787, 10.186], loss: 0.100448, mae: 0.284279, mean_q: 3.844365
 45597/100000: episode: 784, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 190.356, mean reward: 1.904 [1.460, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.518, 10.127], loss: 0.097073, mae: 0.282520, mean_q: 3.827329
 45697/100000: episode: 785, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 270.437, mean reward: 2.704 [1.444, 8.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.452, 10.098], loss: 0.094834, mae: 0.278543, mean_q: 3.836446
 45797/100000: episode: 786, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 208.356, mean reward: 2.084 [1.468, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.268, 10.098], loss: 0.106752, mae: 0.300313, mean_q: 3.867519
 45897/100000: episode: 787, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 207.930, mean reward: 2.079 [1.504, 5.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.018, 10.208], loss: 0.105634, mae: 0.304449, mean_q: 3.897482
 45997/100000: episode: 788, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 186.393, mean reward: 1.864 [1.481, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.010, 10.206], loss: 0.100503, mae: 0.295168, mean_q: 3.864376
 46097/100000: episode: 789, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 188.733, mean reward: 1.887 [1.460, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.362, 10.098], loss: 0.103861, mae: 0.296731, mean_q: 3.878580
 46197/100000: episode: 790, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 201.097, mean reward: 2.011 [1.461, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.602, 10.098], loss: 0.108676, mae: 0.293879, mean_q: 3.886158
 46297/100000: episode: 791, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 193.953, mean reward: 1.940 [1.449, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.053, 10.277], loss: 0.098738, mae: 0.293168, mean_q: 3.865820
 46397/100000: episode: 792, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 178.603, mean reward: 1.786 [1.443, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.551, 10.098], loss: 0.094767, mae: 0.295297, mean_q: 3.876536
 46497/100000: episode: 793, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 182.236, mean reward: 1.822 [1.447, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.998, 10.130], loss: 0.105379, mae: 0.300119, mean_q: 3.883075
 46597/100000: episode: 794, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 213.485, mean reward: 2.135 [1.455, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.739, 10.430], loss: 0.093913, mae: 0.290509, mean_q: 3.875724
 46697/100000: episode: 795, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 195.982, mean reward: 1.960 [1.462, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.804, 10.287], loss: 0.090611, mae: 0.284144, mean_q: 3.877406
 46797/100000: episode: 796, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 207.026, mean reward: 2.070 [1.509, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.465, 10.317], loss: 0.100088, mae: 0.285883, mean_q: 3.885141
 46897/100000: episode: 797, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 186.974, mean reward: 1.870 [1.483, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.999, 10.098], loss: 0.094870, mae: 0.298115, mean_q: 3.897705
 46997/100000: episode: 798, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 184.796, mean reward: 1.848 [1.465, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.514, 10.098], loss: 0.082925, mae: 0.282913, mean_q: 3.884335
 47097/100000: episode: 799, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 181.959, mean reward: 1.820 [1.460, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.433, 10.140], loss: 0.103933, mae: 0.294586, mean_q: 3.890366
 47197/100000: episode: 800, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 371.928, mean reward: 3.719 [1.576, 68.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.650, 10.350], loss: 0.079786, mae: 0.282301, mean_q: 3.892153
 47297/100000: episode: 801, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 200.320, mean reward: 2.003 [1.449, 5.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.955, 10.182], loss: 0.210963, mae: 0.321457, mean_q: 3.925187
 47397/100000: episode: 802, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 197.514, mean reward: 1.975 [1.483, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.374, 10.178], loss: 0.092907, mae: 0.300062, mean_q: 3.930672
 47497/100000: episode: 803, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 194.659, mean reward: 1.947 [1.446, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.686, 10.098], loss: 0.097497, mae: 0.285584, mean_q: 3.914283
 47597/100000: episode: 804, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 213.289, mean reward: 2.133 [1.483, 4.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.941, 10.243], loss: 0.108620, mae: 0.307370, mean_q: 3.935519
 47697/100000: episode: 805, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.144, mean reward: 1.861 [1.440, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.538, 10.146], loss: 0.894594, mae: 0.379675, mean_q: 3.938424
 47797/100000: episode: 806, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 199.831, mean reward: 1.998 [1.430, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.611, 10.319], loss: 1.526623, mae: 0.399603, mean_q: 3.942851
 47897/100000: episode: 807, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 206.194, mean reward: 2.062 [1.495, 5.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.485, 10.282], loss: 0.156410, mae: 0.349153, mean_q: 3.897378
 47997/100000: episode: 808, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 187.906, mean reward: 1.879 [1.433, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.449, 10.174], loss: 0.714648, mae: 0.323148, mean_q: 3.894944
 48097/100000: episode: 809, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 199.708, mean reward: 1.997 [1.516, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.278, 10.098], loss: 0.169576, mae: 0.355283, mean_q: 3.943173
 48197/100000: episode: 810, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 189.372, mean reward: 1.894 [1.481, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.201, 10.098], loss: 0.234884, mae: 0.322984, mean_q: 3.929564
 48297/100000: episode: 811, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 197.746, mean reward: 1.977 [1.513, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.151, 10.098], loss: 1.435564, mae: 0.386696, mean_q: 3.946699
 48397/100000: episode: 812, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 196.809, mean reward: 1.968 [1.538, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.397, 10.098], loss: 1.435489, mae: 0.415957, mean_q: 4.008038
 48497/100000: episode: 813, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 190.590, mean reward: 1.906 [1.454, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.916, 10.098], loss: 0.399399, mae: 0.350214, mean_q: 3.943807
 48597/100000: episode: 814, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 297.274, mean reward: 2.973 [1.487, 9.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.441, 10.308], loss: 1.464418, mae: 0.401448, mean_q: 3.954423
 48697/100000: episode: 815, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 193.814, mean reward: 1.938 [1.443, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.996, 10.098], loss: 0.765718, mae: 0.355277, mean_q: 3.971120
 48797/100000: episode: 816, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 196.261, mean reward: 1.963 [1.455, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.582, 10.098], loss: 0.136128, mae: 0.324943, mean_q: 3.959450
 48897/100000: episode: 817, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 197.147, mean reward: 1.971 [1.485, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.034, 10.354], loss: 0.774442, mae: 0.367720, mean_q: 3.988768
 48997/100000: episode: 818, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 198.531, mean reward: 1.985 [1.444, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.056, 10.392], loss: 0.137813, mae: 0.321182, mean_q: 3.959986
 49097/100000: episode: 819, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 198.335, mean reward: 1.983 [1.469, 4.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.029, 10.098], loss: 0.106534, mae: 0.309651, mean_q: 3.959616
 49197/100000: episode: 820, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 186.531, mean reward: 1.865 [1.462, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.734, 10.227], loss: 0.755731, mae: 0.352739, mean_q: 3.948744
 49297/100000: episode: 821, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 191.213, mean reward: 1.912 [1.478, 4.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.427, 10.248], loss: 0.291855, mae: 0.335968, mean_q: 3.955560
 49397/100000: episode: 822, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 184.605, mean reward: 1.846 [1.480, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.861, 10.098], loss: 0.191757, mae: 0.310284, mean_q: 3.926549
 49497/100000: episode: 823, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 186.107, mean reward: 1.861 [1.469, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.447, 10.128], loss: 0.127702, mae: 0.324751, mean_q: 3.949070
 49597/100000: episode: 824, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 188.974, mean reward: 1.890 [1.464, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.703, 10.098], loss: 0.209472, mae: 0.324762, mean_q: 3.940138
 49697/100000: episode: 825, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 184.980, mean reward: 1.850 [1.468, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.417, 10.098], loss: 0.212847, mae: 0.333057, mean_q: 3.976739
 49797/100000: episode: 826, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 181.502, mean reward: 1.815 [1.453, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.286, 10.157], loss: 0.093007, mae: 0.289916, mean_q: 3.906409
 49897/100000: episode: 827, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 187.783, mean reward: 1.878 [1.439, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.315, 10.137], loss: 0.103311, mae: 0.292153, mean_q: 3.919145
 49997/100000: episode: 828, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 181.628, mean reward: 1.816 [1.476, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.533, 10.177], loss: 0.104414, mae: 0.294642, mean_q: 3.916439
[Info] 1-TH LEVEL FOUND: 10.08839225769043, Considering 10/90 traces
 50097/100000: episode: 829, duration: 4.772s, episode steps: 100, steps per second: 21, episode reward: 197.103, mean reward: 1.971 [1.457, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.989, 10.177], loss: 0.989723, mae: 0.361920, mean_q: 3.962273
 50124/100000: episode: 830, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 66.330, mean reward: 2.457 [1.563, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.095, 10.204], loss: 0.233606, mae: 0.414032, mean_q: 3.922769
 50147/100000: episode: 831, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 58.150, mean reward: 2.528 [1.846, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.717, 10.404], loss: 0.105240, mae: 0.312370, mean_q: 3.942671
 50165/100000: episode: 832, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 40.389, mean reward: 2.244 [1.812, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.395], loss: 0.136512, mae: 0.310528, mean_q: 3.943470
 50194/100000: episode: 833, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 55.901, mean reward: 1.928 [1.564, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.135, 10.204], loss: 0.094018, mae: 0.301698, mean_q: 3.939348
 50221/100000: episode: 834, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 53.932, mean reward: 1.997 [1.522, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.363, 10.100], loss: 0.107415, mae: 0.306636, mean_q: 3.944819
 50261/100000: episode: 835, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 143.175, mean reward: 3.579 [1.723, 9.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.643, 10.304], loss: 1.737310, mae: 0.443765, mean_q: 4.035667
 50301/100000: episode: 836, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 89.519, mean reward: 2.238 [1.588, 5.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.648, 10.258], loss: 0.404008, mae: 0.372817, mean_q: 4.049909
 50339/100000: episode: 837, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 93.080, mean reward: 2.449 [1.940, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.080, 10.327], loss: 0.102590, mae: 0.305181, mean_q: 3.905905
 50364/100000: episode: 838, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 63.807, mean reward: 2.552 [1.653, 4.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.389, 10.282], loss: 0.161164, mae: 0.333644, mean_q: 3.956535
 50391/100000: episode: 839, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 58.147, mean reward: 2.154 [1.541, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.859, 10.371], loss: 0.491608, mae: 0.369619, mean_q: 4.005671
 50416/100000: episode: 840, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 66.543, mean reward: 2.662 [2.094, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.361, 10.446], loss: 0.145448, mae: 0.340445, mean_q: 3.967025
 50445/100000: episode: 841, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 67.267, mean reward: 2.320 [1.631, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.340], loss: 0.117608, mae: 0.326407, mean_q: 4.004125
 50468/100000: episode: 842, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 59.669, mean reward: 2.594 [1.937, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.167, 10.311], loss: 0.090945, mae: 0.295990, mean_q: 3.994338
 50506/100000: episode: 843, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 127.338, mean reward: 3.351 [1.711, 8.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.838, 10.311], loss: 0.107966, mae: 0.310229, mean_q: 3.967551
 50536/100000: episode: 844, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 68.730, mean reward: 2.291 [1.750, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.396, 10.406], loss: 0.132420, mae: 0.329281, mean_q: 4.024952
 50576/100000: episode: 845, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 103.855, mean reward: 2.596 [1.837, 4.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-1.301, 10.331], loss: 0.136328, mae: 0.330335, mean_q: 3.979031
 50614/100000: episode: 846, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 89.127, mean reward: 2.345 [1.546, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.934, 10.146], loss: 0.122094, mae: 0.329279, mean_q: 4.019163
 50654/100000: episode: 847, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 108.060, mean reward: 2.701 [1.654, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.804, 10.245], loss: 0.110056, mae: 0.320349, mean_q: 3.993352
 50694/100000: episode: 848, duration: 0.194s, episode steps: 40, steps per second: 206, episode reward: 102.452, mean reward: 2.561 [1.923, 4.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.248, 10.266], loss: 1.914326, mae: 0.455969, mean_q: 4.167634
 50723/100000: episode: 849, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 67.961, mean reward: 2.343 [1.709, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.214, 10.320], loss: 4.520643, mae: 0.623815, mean_q: 4.070504
 50746/100000: episode: 850, duration: 0.115s, episode steps: 23, steps per second: 199, episode reward: 47.141, mean reward: 2.050 [1.541, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.231, 10.256], loss: 0.171510, mae: 0.386444, mean_q: 3.979841
 50772/100000: episode: 851, duration: 0.128s, episode steps: 26, steps per second: 203, episode reward: 72.029, mean reward: 2.770 [1.798, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.460, 10.487], loss: 0.133790, mae: 0.344178, mean_q: 4.019377
 50802/100000: episode: 852, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 62.819, mean reward: 2.094 [1.690, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.492, 10.371], loss: 2.352761, mae: 0.483463, mean_q: 4.207040
 50827/100000: episode: 853, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 60.472, mean reward: 2.419 [1.674, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.362], loss: 0.191108, mae: 0.386323, mean_q: 3.998195
 50853/100000: episode: 854, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 77.837, mean reward: 2.994 [2.015, 4.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.206, 10.356], loss: 0.491412, mae: 0.330479, mean_q: 3.977630
 50871/100000: episode: 855, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 37.054, mean reward: 2.059 [1.540, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.446, 10.232], loss: 0.117206, mae: 0.336637, mean_q: 4.040266
 50911/100000: episode: 856, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 100.525, mean reward: 2.513 [1.480, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.220, 10.202], loss: 0.129393, mae: 0.324905, mean_q: 4.039310
 50949/100000: episode: 857, duration: 0.184s, episode steps: 38, steps per second: 207, episode reward: 195.993, mean reward: 5.158 [1.676, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.294, 10.246], loss: 0.157103, mae: 0.350448, mean_q: 4.062294
 50978/100000: episode: 858, duration: 0.143s, episode steps: 29, steps per second: 202, episode reward: 77.501, mean reward: 2.672 [1.937, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.991, 10.323], loss: 0.427440, mae: 0.364982, mean_q: 4.107110
 51018/100000: episode: 859, duration: 0.191s, episode steps: 40, steps per second: 209, episode reward: 123.237, mean reward: 3.081 [1.844, 6.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.679, 10.283], loss: 3.928704, mae: 0.484475, mean_q: 4.173826
 51047/100000: episode: 860, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 65.117, mean reward: 2.245 [1.723, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.166, 10.227], loss: 2.481107, mae: 0.564557, mean_q: 4.354931
 51070/100000: episode: 861, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 45.455, mean reward: 1.976 [1.481, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.132, 10.183], loss: 0.144030, mae: 0.367997, mean_q: 4.096876
 51093/100000: episode: 862, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 47.593, mean reward: 2.069 [1.683, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.734, 10.193], loss: 2.665839, mae: 0.451389, mean_q: 4.200868
 51120/100000: episode: 863, duration: 0.130s, episode steps: 27, steps per second: 207, episode reward: 68.107, mean reward: 2.522 [1.754, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.416, 10.258], loss: 2.651371, mae: 0.719374, mean_q: 4.240043
 51146/100000: episode: 864, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 48.199, mean reward: 1.854 [1.530, 2.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.280, 10.179], loss: 5.909510, mae: 0.557424, mean_q: 4.205452
 51175/100000: episode: 865, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 57.436, mean reward: 1.981 [1.483, 2.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.134, 10.125], loss: 0.184629, mae: 0.408971, mean_q: 4.111046
 51204/100000: episode: 866, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 60.956, mean reward: 2.102 [1.673, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.297], loss: 0.151798, mae: 0.365735, mean_q: 4.130667
 51227/100000: episode: 867, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 47.676, mean reward: 2.073 [1.602, 4.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.233, 10.100], loss: 0.152843, mae: 0.344999, mean_q: 4.177661
 51252/100000: episode: 868, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 51.947, mean reward: 2.078 [1.582, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.155, 10.242], loss: 0.116820, mae: 0.329642, mean_q: 4.111508
 51290/100000: episode: 869, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 90.154, mean reward: 2.372 [1.556, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-1.580, 10.173], loss: 6.021989, mae: 0.681061, mean_q: 4.381056
 51328/100000: episode: 870, duration: 0.186s, episode steps: 38, steps per second: 204, episode reward: 100.209, mean reward: 2.637 [1.705, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.529, 10.521], loss: 4.182896, mae: 0.535799, mean_q: 4.136062
 51351/100000: episode: 871, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 64.719, mean reward: 2.814 [1.609, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.850, 10.228], loss: 0.169587, mae: 0.387987, mean_q: 4.166927
 51380/100000: episode: 872, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 76.548, mean reward: 2.640 [2.105, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.406], loss: 0.149858, mae: 0.362271, mean_q: 4.165944
 51418/100000: episode: 873, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 126.464, mean reward: 3.328 [2.292, 5.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.793, 10.478], loss: 1.820119, mae: 0.462357, mean_q: 4.256008
 51436/100000: episode: 874, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 35.482, mean reward: 1.971 [1.651, 2.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.230], loss: 0.189975, mae: 0.386306, mean_q: 4.089437
 51454/100000: episode: 875, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 51.714, mean reward: 2.873 [2.266, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.461], loss: 11.821722, mae: 0.818559, mean_q: 4.544266
 51484/100000: episode: 876, duration: 0.146s, episode steps: 30, steps per second: 206, episode reward: 71.289, mean reward: 2.376 [1.786, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.395, 10.289], loss: 0.330750, mae: 0.475046, mean_q: 4.306295
 51513/100000: episode: 877, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 64.095, mean reward: 2.210 [1.569, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.175], loss: 0.484424, mae: 0.405559, mean_q: 4.237927
 51551/100000: episode: 878, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 105.345, mean reward: 2.772 [2.093, 3.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-1.279, 10.341], loss: 4.313275, mae: 0.549409, mean_q: 4.372349
 51577/100000: episode: 879, duration: 0.130s, episode steps: 26, steps per second: 199, episode reward: 56.691, mean reward: 2.180 [1.668, 3.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.373, 10.201], loss: 3.028949, mae: 0.551809, mean_q: 4.315930
 51602/100000: episode: 880, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 59.869, mean reward: 2.395 [1.865, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.875, 10.430], loss: 0.147216, mae: 0.361269, mean_q: 4.249048
 51631/100000: episode: 881, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 58.811, mean reward: 2.028 [1.544, 2.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.035, 10.171], loss: 2.368942, mae: 0.509009, mean_q: 4.338633
 51654/100000: episode: 882, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 48.792, mean reward: 2.121 [1.800, 2.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.295], loss: 0.153676, mae: 0.378755, mean_q: 4.198010
 51680/100000: episode: 883, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 51.390, mean reward: 1.977 [1.671, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.036, 10.245], loss: 0.127880, mae: 0.347567, mean_q: 4.143330
 51703/100000: episode: 884, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 47.849, mean reward: 2.080 [1.543, 2.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.202], loss: 0.154626, mae: 0.355287, mean_q: 4.187564
 51741/100000: episode: 885, duration: 0.187s, episode steps: 38, steps per second: 203, episode reward: 102.639, mean reward: 2.701 [1.491, 7.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.523, 10.223], loss: 0.406837, mae: 0.518809, mean_q: 4.269217
 51764/100000: episode: 886, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 74.827, mean reward: 3.253 [2.062, 5.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.910, 10.484], loss: 0.173966, mae: 0.390051, mean_q: 4.254146
 51787/100000: episode: 887, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 61.352, mean reward: 2.667 [2.032, 5.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.311, 10.467], loss: 0.172293, mae: 0.365368, mean_q: 4.255066
 51816/100000: episode: 888, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 73.118, mean reward: 2.521 [1.965, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.278], loss: 0.139965, mae: 0.329019, mean_q: 4.235791
 51846/100000: episode: 889, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 75.121, mean reward: 2.504 [1.545, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.247], loss: 0.153165, mae: 0.359747, mean_q: 4.244350
 51886/100000: episode: 890, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 115.330, mean reward: 2.883 [1.718, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.178, 10.354], loss: 1.784294, mae: 0.487822, mean_q: 4.410958
 51904/100000: episode: 891, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 42.721, mean reward: 2.373 [1.859, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.418], loss: 0.198780, mae: 0.409450, mean_q: 4.040826
 51930/100000: episode: 892, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 52.618, mean reward: 2.024 [1.453, 2.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.737, 10.100], loss: 0.174014, mae: 0.392902, mean_q: 4.316112
 51968/100000: episode: 893, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 156.796, mean reward: 4.126 [2.533, 8.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.632, 10.501], loss: 0.141903, mae: 0.360597, mean_q: 4.280878
 51993/100000: episode: 894, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 50.329, mean reward: 2.013 [1.626, 2.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.575, 10.215], loss: 0.548462, mae: 0.453614, mean_q: 4.340963
 52033/100000: episode: 895, duration: 0.212s, episode steps: 40, steps per second: 189, episode reward: 104.469, mean reward: 2.612 [1.764, 4.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.174, 10.249], loss: 10.713161, mae: 0.793252, mean_q: 4.563705
 52056/100000: episode: 896, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 59.969, mean reward: 2.607 [2.013, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.475], loss: 3.051891, mae: 0.633684, mean_q: 4.541154
 52074/100000: episode: 897, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 43.392, mean reward: 2.411 [1.900, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.794, 10.352], loss: 0.175245, mae: 0.376459, mean_q: 4.245580
 52097/100000: episode: 898, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 55.463, mean reward: 2.411 [2.013, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.209, 10.452], loss: 0.170530, mae: 0.388962, mean_q: 4.373807
 52115/100000: episode: 899, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 48.836, mean reward: 2.713 [1.960, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.116, 10.486], loss: 0.124825, mae: 0.359833, mean_q: 4.340746
 52153/100000: episode: 900, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 100.568, mean reward: 2.647 [1.664, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.065, 10.264], loss: 4.044751, mae: 0.459632, mean_q: 4.322871
 52193/100000: episode: 901, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 108.268, mean reward: 2.707 [1.816, 4.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.172, 10.372], loss: 0.184622, mae: 0.412720, mean_q: 4.390686
 52211/100000: episode: 902, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 36.736, mean reward: 2.041 [1.768, 2.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.369], loss: 0.143078, mae: 0.358494, mean_q: 4.280527
 52251/100000: episode: 903, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 106.182, mean reward: 2.655 [1.880, 6.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.234, 10.366], loss: 0.165521, mae: 0.377211, mean_q: 4.311246
 52274/100000: episode: 904, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 50.330, mean reward: 2.188 [1.717, 4.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.519, 10.246], loss: 0.140216, mae: 0.352608, mean_q: 4.301235
 52299/100000: episode: 905, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 52.233, mean reward: 2.089 [1.497, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.905, 10.148], loss: 0.172432, mae: 0.374531, mean_q: 4.374106
 52322/100000: episode: 906, duration: 0.125s, episode steps: 23, steps per second: 183, episode reward: 51.384, mean reward: 2.234 [1.581, 4.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.137, 10.304], loss: 0.170417, mae: 0.368040, mean_q: 4.298053
 52345/100000: episode: 907, duration: 0.115s, episode steps: 23, steps per second: 199, episode reward: 44.254, mean reward: 1.924 [1.587, 2.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.133, 10.199], loss: 0.121727, mae: 0.344393, mean_q: 4.320412
 52368/100000: episode: 908, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 52.553, mean reward: 2.285 [1.844, 2.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.819, 10.345], loss: 0.171580, mae: 0.384694, mean_q: 4.419503
 52397/100000: episode: 909, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 64.847, mean reward: 2.236 [1.726, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.592, 10.353], loss: 0.145982, mae: 0.370252, mean_q: 4.371284
 52420/100000: episode: 910, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 45.774, mean reward: 1.990 [1.464, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.598, 10.145], loss: 0.135924, mae: 0.349504, mean_q: 4.334623
 52458/100000: episode: 911, duration: 0.194s, episode steps: 38, steps per second: 196, episode reward: 129.053, mean reward: 3.396 [2.389, 5.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.505, 10.478], loss: 0.152516, mae: 0.364940, mean_q: 4.342223
 52485/100000: episode: 912, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 65.566, mean reward: 2.428 [1.686, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.283, 10.421], loss: 0.143091, mae: 0.360190, mean_q: 4.324184
 52510/100000: episode: 913, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 47.901, mean reward: 1.916 [1.601, 2.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.194], loss: 0.167291, mae: 0.360652, mean_q: 4.357780
 52550/100000: episode: 914, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 97.354, mean reward: 2.434 [1.791, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.166, 10.371], loss: 0.138388, mae: 0.354260, mean_q: 4.340426
 52588/100000: episode: 915, duration: 0.190s, episode steps: 38, steps per second: 200, episode reward: 136.751, mean reward: 3.599 [2.450, 6.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.288, 10.436], loss: 4.102818, mae: 0.515586, mean_q: 4.445181
 52611/100000: episode: 916, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 48.334, mean reward: 2.101 [1.579, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.173], loss: 0.164669, mae: 0.377005, mean_q: 4.379238
 52637/100000: episode: 917, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 79.431, mean reward: 3.055 [1.918, 8.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.760, 10.382], loss: 5.866563, mae: 0.513877, mean_q: 4.488006
 52664/100000: episode: 918, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 78.010, mean reward: 2.889 [1.842, 4.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.035, 10.337], loss: 0.198623, mae: 0.406746, mean_q: 4.336899
[Info] NOT FOUND NEW LEVEL, Current Best Level is 10.08839225769043
 52682/100000: episode: 919, duration: 4.152s, episode steps: 18, steps per second: 4, episode reward: 44.087, mean reward: 2.449 [1.779, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.356, 10.243], loss: 0.136284, mae: 0.366549, mean_q: 4.398330
 52782/100000: episode: 920, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 199.321, mean reward: 1.993 [1.489, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.122, 10.174], loss: 1.661063, mae: 0.431213, mean_q: 4.447701
 52882/100000: episode: 921, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 205.373, mean reward: 2.054 [1.481, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.267, 10.332], loss: 0.126242, mae: 0.342950, mean_q: 4.354667
 52982/100000: episode: 922, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 238.219, mean reward: 2.382 [1.458, 5.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.585, 10.378], loss: 0.159916, mae: 0.360498, mean_q: 4.375532
 53082/100000: episode: 923, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 201.943, mean reward: 2.019 [1.528, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.401, 10.349], loss: 0.165417, mae: 0.380716, mean_q: 4.431627
 53182/100000: episode: 924, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 190.073, mean reward: 1.901 [1.466, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.638, 10.259], loss: 1.649583, mae: 0.424115, mean_q: 4.460755
 53282/100000: episode: 925, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 199.746, mean reward: 1.997 [1.474, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.562, 10.243], loss: 1.633438, mae: 0.413507, mean_q: 4.442357
 53382/100000: episode: 926, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 199.000, mean reward: 1.990 [1.470, 4.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.154, 10.098], loss: 1.628800, mae: 0.412629, mean_q: 4.446854
 53482/100000: episode: 927, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 175.630, mean reward: 1.756 [1.478, 2.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.370, 10.342], loss: 0.172435, mae: 0.378702, mean_q: 4.425290
 53582/100000: episode: 928, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 177.160, mean reward: 1.772 [1.455, 2.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.866, 10.185], loss: 1.611601, mae: 0.378503, mean_q: 4.373094
 53682/100000: episode: 929, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 196.357, mean reward: 1.964 [1.458, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.232, 10.098], loss: 0.180364, mae: 0.400823, mean_q: 4.392049
 53782/100000: episode: 930, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.361, mean reward: 1.924 [1.437, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.871, 10.098], loss: 0.135222, mae: 0.351682, mean_q: 4.350327
 53882/100000: episode: 931, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 207.502, mean reward: 2.075 [1.464, 4.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.781, 10.313], loss: 0.140373, mae: 0.347692, mean_q: 4.378196
 53982/100000: episode: 932, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.961, mean reward: 1.890 [1.466, 2.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.747, 10.182], loss: 0.134996, mae: 0.353250, mean_q: 4.376337
 54082/100000: episode: 933, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 183.132, mean reward: 1.831 [1.489, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.096, 10.213], loss: 1.633504, mae: 0.421065, mean_q: 4.403036
 54182/100000: episode: 934, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 194.302, mean reward: 1.943 [1.504, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.481, 10.098], loss: 0.130031, mae: 0.335915, mean_q: 4.321897
 54282/100000: episode: 935, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 181.612, mean reward: 1.816 [1.436, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.859, 10.323], loss: 0.126794, mae: 0.336359, mean_q: 4.317725
 54382/100000: episode: 936, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 185.584, mean reward: 1.856 [1.475, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.295, 10.098], loss: 1.662852, mae: 0.432860, mean_q: 4.360349
 54482/100000: episode: 937, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 181.692, mean reward: 1.817 [1.440, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.433, 10.098], loss: 1.674041, mae: 0.458759, mean_q: 4.438570
 54582/100000: episode: 938, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 181.437, mean reward: 1.814 [1.433, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.843, 10.098], loss: 1.662453, mae: 0.435198, mean_q: 4.436413
 54682/100000: episode: 939, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 184.721, mean reward: 1.847 [1.478, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.016, 10.098], loss: 1.639581, mae: 0.414383, mean_q: 4.383556
 54782/100000: episode: 940, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 227.816, mean reward: 2.278 [1.604, 4.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.592, 10.344], loss: 3.150245, mae: 0.496148, mean_q: 4.443037
 54882/100000: episode: 941, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 188.870, mean reward: 1.889 [1.468, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.541, 10.098], loss: 1.662237, mae: 0.431259, mean_q: 4.413596
 54982/100000: episode: 942, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 181.793, mean reward: 1.818 [1.454, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.166, 10.115], loss: 1.629896, mae: 0.414270, mean_q: 4.421151
 55082/100000: episode: 943, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 189.545, mean reward: 1.895 [1.505, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.859, 10.232], loss: 0.123281, mae: 0.340140, mean_q: 4.357677
 55182/100000: episode: 944, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 184.320, mean reward: 1.843 [1.458, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.459, 10.121], loss: 0.125905, mae: 0.344759, mean_q: 4.338240
 55282/100000: episode: 945, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 179.879, mean reward: 1.799 [1.452, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.838, 10.098], loss: 0.132130, mae: 0.349511, mean_q: 4.328426
 55382/100000: episode: 946, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 188.669, mean reward: 1.887 [1.483, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.307, 10.098], loss: 0.128785, mae: 0.333576, mean_q: 4.287337
 55482/100000: episode: 947, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 187.049, mean reward: 1.870 [1.451, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.382, 10.193], loss: 1.633640, mae: 0.411888, mean_q: 4.305113
 55582/100000: episode: 948, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 198.623, mean reward: 1.986 [1.510, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.655, 10.098], loss: 1.632680, mae: 0.410908, mean_q: 4.276525
 55682/100000: episode: 949, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 188.801, mean reward: 1.888 [1.439, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.574, 10.251], loss: 0.112312, mae: 0.327623, mean_q: 4.190095
 55782/100000: episode: 950, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 185.697, mean reward: 1.857 [1.439, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.047, 10.198], loss: 1.606232, mae: 0.389093, mean_q: 4.265344
 55882/100000: episode: 951, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 199.639, mean reward: 1.996 [1.467, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.081, 10.098], loss: 0.105101, mae: 0.316003, mean_q: 4.187334
 55982/100000: episode: 952, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 187.712, mean reward: 1.877 [1.473, 2.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.676, 10.274], loss: 0.104409, mae: 0.315275, mean_q: 4.153441
 56082/100000: episode: 953, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 190.315, mean reward: 1.903 [1.470, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-2.305, 10.360], loss: 0.108694, mae: 0.319117, mean_q: 4.169693
 56182/100000: episode: 954, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 179.275, mean reward: 1.793 [1.450, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.602, 10.141], loss: 0.093760, mae: 0.307526, mean_q: 4.112617
 56282/100000: episode: 955, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 194.242, mean reward: 1.942 [1.447, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.749, 10.130], loss: 0.091348, mae: 0.301698, mean_q: 4.101152
 56382/100000: episode: 956, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 192.742, mean reward: 1.927 [1.459, 4.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.732, 10.156], loss: 0.105330, mae: 0.305675, mean_q: 4.099367
 56482/100000: episode: 957, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.343, mean reward: 1.913 [1.460, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.231, 10.244], loss: 0.096068, mae: 0.300402, mean_q: 4.095339
 56582/100000: episode: 958, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 191.639, mean reward: 1.916 [1.474, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.622, 10.250], loss: 0.091604, mae: 0.288925, mean_q: 4.028628
 56682/100000: episode: 959, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 210.009, mean reward: 2.100 [1.477, 11.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.867, 10.270], loss: 0.113930, mae: 0.297292, mean_q: 4.064087
 56782/100000: episode: 960, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 212.683, mean reward: 2.127 [1.441, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-1.183, 10.098], loss: 0.091873, mae: 0.292264, mean_q: 4.027159
 56882/100000: episode: 961, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 192.931, mean reward: 1.929 [1.476, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.602, 10.200], loss: 0.106687, mae: 0.298989, mean_q: 4.031826
 56982/100000: episode: 962, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 188.157, mean reward: 1.882 [1.456, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.098, 10.098], loss: 0.088062, mae: 0.290314, mean_q: 3.964416
 57082/100000: episode: 963, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 239.172, mean reward: 2.392 [1.453, 7.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.451, 10.098], loss: 0.084508, mae: 0.289971, mean_q: 3.945421
 57182/100000: episode: 964, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 184.619, mean reward: 1.846 [1.430, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.310, 10.098], loss: 0.093184, mae: 0.294686, mean_q: 3.931924
 57282/100000: episode: 965, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 192.644, mean reward: 1.926 [1.452, 3.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.041, 10.194], loss: 0.079340, mae: 0.283790, mean_q: 3.896349
 57382/100000: episode: 966, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 186.342, mean reward: 1.863 [1.444, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.610, 10.293], loss: 0.104307, mae: 0.296078, mean_q: 3.919860
 57482/100000: episode: 967, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 192.488, mean reward: 1.925 [1.469, 2.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.517, 10.141], loss: 0.079038, mae: 0.280319, mean_q: 3.865382
 57582/100000: episode: 968, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 191.741, mean reward: 1.917 [1.447, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.566, 10.098], loss: 0.085698, mae: 0.277818, mean_q: 3.870486
 57682/100000: episode: 969, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 241.190, mean reward: 2.412 [1.447, 4.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.481, 10.098], loss: 0.081552, mae: 0.276615, mean_q: 3.821136
 57782/100000: episode: 970, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 283.154, mean reward: 2.832 [1.470, 87.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.144, 10.098], loss: 0.084753, mae: 0.276562, mean_q: 3.853975
 57882/100000: episode: 971, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 205.020, mean reward: 2.050 [1.498, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.834, 10.098], loss: 2.415563, mae: 0.387670, mean_q: 3.894741
 57982/100000: episode: 972, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 205.355, mean reward: 2.054 [1.454, 13.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.414, 10.187], loss: 1.252227, mae: 0.335373, mean_q: 3.881367
 58082/100000: episode: 973, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 195.277, mean reward: 1.953 [1.449, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.287, 10.179], loss: 1.220248, mae: 0.311910, mean_q: 3.855952
 58182/100000: episode: 974, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 191.500, mean reward: 1.915 [1.491, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.830, 10.098], loss: 1.251743, mae: 0.324394, mean_q: 3.868381
 58282/100000: episode: 975, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 193.071, mean reward: 1.931 [1.493, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.156, 10.098], loss: 0.122184, mae: 0.287916, mean_q: 3.853112
 58382/100000: episode: 976, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 193.411, mean reward: 1.934 [1.469, 5.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.289, 10.171], loss: 2.396596, mae: 0.373705, mean_q: 3.886351
 58482/100000: episode: 977, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 188.817, mean reward: 1.888 [1.474, 2.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.661, 10.280], loss: 2.389188, mae: 0.358569, mean_q: 3.894421
 58582/100000: episode: 978, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 192.017, mean reward: 1.920 [1.485, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.256, 10.098], loss: 2.374804, mae: 0.363837, mean_q: 3.937653
 58682/100000: episode: 979, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 187.358, mean reward: 1.874 [1.477, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.237, 10.102], loss: 1.229821, mae: 0.314051, mean_q: 3.880744
 58782/100000: episode: 980, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 184.933, mean reward: 1.849 [1.466, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.556, 10.098], loss: 1.254711, mae: 0.318886, mean_q: 3.876309
 58882/100000: episode: 981, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 188.319, mean reward: 1.883 [1.461, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.062, 10.188], loss: 1.218322, mae: 0.316594, mean_q: 3.884598
 58982/100000: episode: 982, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 193.027, mean reward: 1.930 [1.511, 3.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.849, 10.198], loss: 1.214814, mae: 0.308241, mean_q: 3.851798
 59082/100000: episode: 983, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.409, mean reward: 1.914 [1.517, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.649, 10.098], loss: 1.243409, mae: 0.332495, mean_q: 3.852202
 59182/100000: episode: 984, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.655, mean reward: 1.917 [1.480, 2.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.200, 10.098], loss: 0.118166, mae: 0.284733, mean_q: 3.858588
 59282/100000: episode: 985, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 187.850, mean reward: 1.878 [1.463, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.846, 10.098], loss: 0.094552, mae: 0.280455, mean_q: 3.839125
 59382/100000: episode: 986, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 184.203, mean reward: 1.842 [1.438, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.939, 10.235], loss: 1.213887, mae: 0.307682, mean_q: 3.854348
 59482/100000: episode: 987, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 200.674, mean reward: 2.007 [1.521, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.151, 10.098], loss: 0.123034, mae: 0.282358, mean_q: 3.836190
 59582/100000: episode: 988, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 194.490, mean reward: 1.945 [1.439, 6.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.574, 10.301], loss: 0.076851, mae: 0.278619, mean_q: 3.847147
 59682/100000: episode: 989, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 200.490, mean reward: 2.005 [1.465, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.656, 10.189], loss: 0.103302, mae: 0.277706, mean_q: 3.834037
 59782/100000: episode: 990, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 200.689, mean reward: 2.007 [1.486, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.026, 10.098], loss: 0.136484, mae: 0.281599, mean_q: 3.833019
 59882/100000: episode: 991, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 190.673, mean reward: 1.907 [1.471, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.504, 10.182], loss: 0.138552, mae: 0.287765, mean_q: 3.839685
 59982/100000: episode: 992, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 186.742, mean reward: 1.867 [1.436, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.028, 10.134], loss: 0.106907, mae: 0.286828, mean_q: 3.834457
 60082/100000: episode: 993, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 181.591, mean reward: 1.816 [1.467, 2.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.424, 10.178], loss: 0.068578, mae: 0.270541, mean_q: 3.837096
 60182/100000: episode: 994, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 211.070, mean reward: 2.111 [1.481, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.425, 10.486], loss: 0.099501, mae: 0.286630, mean_q: 3.840480
 60282/100000: episode: 995, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 203.964, mean reward: 2.040 [1.463, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.530, 10.098], loss: 0.069402, mae: 0.272987, mean_q: 3.845542
 60382/100000: episode: 996, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 184.578, mean reward: 1.846 [1.453, 4.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.695, 10.224], loss: 0.104127, mae: 0.278055, mean_q: 3.835841
 60482/100000: episode: 997, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 207.937, mean reward: 2.079 [1.493, 5.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.462, 10.390], loss: 1.240813, mae: 0.325839, mean_q: 3.872810
 60582/100000: episode: 998, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 212.032, mean reward: 2.120 [1.445, 4.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.389, 10.383], loss: 1.308180, mae: 0.351825, mean_q: 3.905718
 60682/100000: episode: 999, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 184.112, mean reward: 1.841 [1.455, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.355, 10.221], loss: 1.238432, mae: 0.331109, mean_q: 3.899448
 60782/100000: episode: 1000, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 189.562, mean reward: 1.896 [1.443, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.127, 10.098], loss: 0.075450, mae: 0.284219, mean_q: 3.858475
 60882/100000: episode: 1001, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 197.498, mean reward: 1.975 [1.467, 6.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.879, 10.098], loss: 1.270169, mae: 0.326045, mean_q: 3.906534
 60982/100000: episode: 1002, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 193.705, mean reward: 1.937 [1.474, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.327, 10.389], loss: 0.160735, mae: 0.314677, mean_q: 3.898517
 61082/100000: episode: 1003, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 194.254, mean reward: 1.943 [1.455, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.165, 10.098], loss: 0.124114, mae: 0.294667, mean_q: 3.864389
 61182/100000: episode: 1004, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 188.110, mean reward: 1.881 [1.457, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.566, 10.301], loss: 0.113879, mae: 0.293888, mean_q: 3.890541
 61282/100000: episode: 1005, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 206.524, mean reward: 2.065 [1.430, 7.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.694, 10.098], loss: 1.249101, mae: 0.329805, mean_q: 3.890772
 61382/100000: episode: 1006, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.420, mean reward: 1.874 [1.450, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.559, 10.206], loss: 2.380944, mae: 0.375497, mean_q: 3.924004
 61482/100000: episode: 1007, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.374, mean reward: 1.954 [1.491, 4.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.591, 10.245], loss: 0.116063, mae: 0.299292, mean_q: 3.900735
 61582/100000: episode: 1008, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 184.875, mean reward: 1.849 [1.486, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.176, 10.133], loss: 1.262276, mae: 0.342856, mean_q: 3.915641
 61682/100000: episode: 1009, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.113, mean reward: 1.861 [1.457, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.206, 10.109], loss: 0.089816, mae: 0.283117, mean_q: 3.871810
 61782/100000: episode: 1010, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.521, mean reward: 1.875 [1.458, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.623, 10.260], loss: 0.095354, mae: 0.277211, mean_q: 3.852473
 61882/100000: episode: 1011, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 200.701, mean reward: 2.007 [1.471, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.801, 10.291], loss: 0.102382, mae: 0.295708, mean_q: 3.865520
 61982/100000: episode: 1012, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 184.580, mean reward: 1.846 [1.477, 2.601], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.874, 10.098], loss: 2.404957, mae: 0.363284, mean_q: 3.914505
 62082/100000: episode: 1013, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 212.594, mean reward: 2.126 [1.475, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.223, 10.513], loss: 2.369825, mae: 0.374875, mean_q: 3.914145
 62182/100000: episode: 1014, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 190.096, mean reward: 1.901 [1.470, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.625, 10.098], loss: 1.228985, mae: 0.314705, mean_q: 3.885169
 62282/100000: episode: 1015, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 176.176, mean reward: 1.762 [1.475, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.029, 10.098], loss: 1.222622, mae: 0.317488, mean_q: 3.883292
 62382/100000: episode: 1016, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 188.702, mean reward: 1.887 [1.481, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.651, 10.156], loss: 1.281149, mae: 0.351241, mean_q: 3.905605
 62482/100000: episode: 1017, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 186.440, mean reward: 1.864 [1.474, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.205, 10.236], loss: 2.433748, mae: 0.400233, mean_q: 3.933166
 62582/100000: episode: 1018, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 197.385, mean reward: 1.974 [1.480, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.912, 10.118], loss: 1.253445, mae: 0.354015, mean_q: 3.893700
[Info] 1-TH LEVEL FOUND: 6.064570426940918, Considering 10/90 traces
 62682/100000: episode: 1019, duration: 4.821s, episode steps: 100, steps per second: 21, episode reward: 198.604, mean reward: 1.986 [1.472, 3.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.291, 10.098], loss: 0.083906, mae: 0.281113, mean_q: 3.831602
 62709/100000: episode: 1020, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 79.829, mean reward: 2.957 [2.439, 3.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.417], loss: 0.081564, mae: 0.295926, mean_q: 3.872561
 62729/100000: episode: 1021, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 60.020, mean reward: 3.001 [2.293, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.442, 10.100], loss: 0.098733, mae: 0.302009, mean_q: 3.869307
 62757/100000: episode: 1022, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 102.968, mean reward: 3.677 [2.314, 6.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.137, 10.400], loss: 0.083194, mae: 0.289906, mean_q: 3.913378
 62785/100000: episode: 1023, duration: 0.135s, episode steps: 28, steps per second: 208, episode reward: 58.280, mean reward: 2.081 [1.493, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.205], loss: 0.079366, mae: 0.268626, mean_q: 3.842125
 62813/100000: episode: 1024, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 71.756, mean reward: 2.563 [1.886, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.497, 10.494], loss: 0.098216, mae: 0.293322, mean_q: 3.886440
 62835/100000: episode: 1025, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 59.669, mean reward: 2.712 [2.092, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.425], loss: 0.115260, mae: 0.306147, mean_q: 3.941427
 62856/100000: episode: 1026, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 52.674, mean reward: 2.508 [1.930, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.128, 10.400], loss: 0.084568, mae: 0.287734, mean_q: 3.870328
 62884/100000: episode: 1027, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 104.827, mean reward: 3.744 [2.294, 7.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.887, 10.458], loss: 0.152629, mae: 0.292150, mean_q: 3.918387
 62912/100000: episode: 1028, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 132.291, mean reward: 4.725 [2.552, 8.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.620, 10.523], loss: 0.100109, mae: 0.301066, mean_q: 3.916707
 62932/100000: episode: 1029, duration: 0.099s, episode steps: 20, steps per second: 201, episode reward: 64.742, mean reward: 3.237 [2.306, 4.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.399, 10.100], loss: 0.089408, mae: 0.299025, mean_q: 3.897789
 62953/100000: episode: 1030, duration: 0.107s, episode steps: 21, steps per second: 195, episode reward: 56.484, mean reward: 2.690 [2.248, 4.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.132, 10.419], loss: 0.081432, mae: 0.296328, mean_q: 3.902779
 62980/100000: episode: 1031, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 62.920, mean reward: 2.330 [1.476, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.628, 10.147], loss: 0.113070, mae: 0.303733, mean_q: 3.937353
 63001/100000: episode: 1032, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 60.565, mean reward: 2.884 [2.114, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.488], loss: 0.098350, mae: 0.306593, mean_q: 3.924206
 63022/100000: episode: 1033, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 49.173, mean reward: 2.342 [1.950, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.361, 10.371], loss: 0.079901, mae: 0.290617, mean_q: 3.919142
 63049/100000: episode: 1034, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 93.251, mean reward: 3.454 [2.348, 7.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.502], loss: 0.109681, mae: 0.322571, mean_q: 3.943870
 63070/100000: episode: 1035, duration: 0.106s, episode steps: 21, steps per second: 199, episode reward: 58.911, mean reward: 2.805 [2.103, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.137, 10.462], loss: 0.147013, mae: 0.344474, mean_q: 4.090585
 63097/100000: episode: 1036, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 67.337, mean reward: 2.494 [1.863, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.250], loss: 0.098394, mae: 0.297430, mean_q: 3.964605
 63122/100000: episode: 1037, duration: 0.122s, episode steps: 25, steps per second: 205, episode reward: 62.717, mean reward: 2.509 [2.112, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.201, 10.225], loss: 0.079673, mae: 0.283170, mean_q: 3.998331
 63142/100000: episode: 1038, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 93.775, mean reward: 4.689 [2.994, 9.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.574, 10.100], loss: 0.105827, mae: 0.304795, mean_q: 4.013149
 63174/100000: episode: 1039, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 80.837, mean reward: 2.526 [1.525, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.569, 10.100], loss: 0.119559, mae: 0.314447, mean_q: 4.023931
 63196/100000: episode: 1040, duration: 0.106s, episode steps: 22, steps per second: 207, episode reward: 55.649, mean reward: 2.530 [2.204, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.716, 10.450], loss: 0.137251, mae: 0.321039, mean_q: 4.104426
 63221/100000: episode: 1041, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 53.982, mean reward: 2.159 [1.540, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.352, 10.154], loss: 0.084266, mae: 0.292755, mean_q: 3.956175
 63249/100000: episode: 1042, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 63.557, mean reward: 2.270 [1.670, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.253], loss: 0.127295, mae: 0.315109, mean_q: 4.019792
 63270/100000: episode: 1043, duration: 0.102s, episode steps: 21, steps per second: 205, episode reward: 50.138, mean reward: 2.388 [1.924, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.366, 10.379], loss: 0.098450, mae: 0.299459, mean_q: 3.951751
 63297/100000: episode: 1044, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 136.899, mean reward: 5.070 [2.257, 12.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.872, 10.389], loss: 0.156294, mae: 0.338541, mean_q: 4.077650
 63318/100000: episode: 1045, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 65.412, mean reward: 3.115 [1.941, 3.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.744, 10.542], loss: 0.112239, mae: 0.309597, mean_q: 4.041192
 63340/100000: episode: 1046, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 65.988, mean reward: 2.999 [2.365, 4.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.555, 10.414], loss: 0.118024, mae: 0.328316, mean_q: 4.035079
 63370/100000: episode: 1047, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 66.882, mean reward: 2.229 [1.500, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.056, 10.222], loss: 0.161280, mae: 0.343730, mean_q: 4.023017
 63391/100000: episode: 1048, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 52.653, mean reward: 2.507 [1.990, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.401, 10.461], loss: 0.152897, mae: 0.363521, mean_q: 4.043096
 63412/100000: episode: 1049, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 43.634, mean reward: 2.078 [1.880, 2.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.036, 10.307], loss: 0.169242, mae: 0.368940, mean_q: 4.090978
 63432/100000: episode: 1050, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 64.745, mean reward: 3.237 [2.563, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.474, 10.100], loss: 0.114022, mae: 0.300077, mean_q: 4.002178
 63457/100000: episode: 1051, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 64.774, mean reward: 2.591 [1.812, 4.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.392, 10.161], loss: 0.221112, mae: 0.361138, mean_q: 4.120981
 63489/100000: episode: 1052, duration: 0.163s, episode steps: 32, steps per second: 197, episode reward: 117.749, mean reward: 3.680 [2.509, 6.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.028, 10.608], loss: 0.155289, mae: 0.342185, mean_q: 4.096069
 63509/100000: episode: 1053, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 69.024, mean reward: 3.451 [2.333, 5.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.402, 10.100], loss: 0.148129, mae: 0.343060, mean_q: 4.105746
 63530/100000: episode: 1054, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 63.184, mean reward: 3.009 [2.040, 4.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.119, 10.465], loss: 0.104517, mae: 0.309078, mean_q: 4.118273
 63557/100000: episode: 1055, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 78.014, mean reward: 2.889 [2.244, 4.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.481], loss: 0.111839, mae: 0.331580, mean_q: 4.150268
 63577/100000: episode: 1056, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 55.391, mean reward: 2.770 [2.080, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.161, 10.100], loss: 0.190678, mae: 0.350328, mean_q: 4.190092
 63599/100000: episode: 1057, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 49.406, mean reward: 2.246 [1.696, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.188, 10.258], loss: 0.158515, mae: 0.353936, mean_q: 4.252664
 63619/100000: episode: 1058, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 66.318, mean reward: 3.316 [2.257, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.641, 10.100], loss: 0.147363, mae: 0.322199, mean_q: 4.080657
 63624/100000: episode: 1059, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 15.936, mean reward: 3.187 [2.397, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.876, 10.392], loss: 0.124597, mae: 0.354493, mean_q: 4.261630
 63656/100000: episode: 1060, duration: 0.164s, episode steps: 32, steps per second: 196, episode reward: 82.444, mean reward: 2.576 [1.944, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.695, 10.429], loss: 0.195946, mae: 0.376788, mean_q: 4.168737
 63677/100000: episode: 1061, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 47.224, mean reward: 2.249 [1.574, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.807, 10.263], loss: 0.139322, mae: 0.355831, mean_q: 4.104118
 63682/100000: episode: 1062, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 11.896, mean reward: 2.379 [2.058, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.432], loss: 0.105942, mae: 0.307547, mean_q: 3.997030
 63703/100000: episode: 1063, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 62.729, mean reward: 2.987 [2.282, 4.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.509], loss: 0.154595, mae: 0.339495, mean_q: 4.207938
 63724/100000: episode: 1064, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 63.156, mean reward: 3.007 [2.213, 4.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.467], loss: 0.118955, mae: 0.323793, mean_q: 4.148306
 63745/100000: episode: 1065, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 67.043, mean reward: 3.193 [2.317, 5.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.380, 10.453], loss: 0.131446, mae: 0.347491, mean_q: 4.208242
 63777/100000: episode: 1066, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 98.401, mean reward: 3.075 [1.966, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.639, 10.413], loss: 0.178054, mae: 0.364489, mean_q: 4.258301
 63807/100000: episode: 1067, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 92.150, mean reward: 3.072 [2.133, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.579, 10.427], loss: 0.148095, mae: 0.365489, mean_q: 4.212267
 63812/100000: episode: 1068, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 11.858, mean reward: 2.372 [2.263, 2.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.354], loss: 0.091125, mae: 0.307884, mean_q: 4.219028
 63840/100000: episode: 1069, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 74.065, mean reward: 2.645 [1.915, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.612, 10.407], loss: 0.173438, mae: 0.368028, mean_q: 4.300826
 63861/100000: episode: 1070, duration: 0.106s, episode steps: 21, steps per second: 197, episode reward: 62.582, mean reward: 2.980 [1.858, 9.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.952, 10.340], loss: 0.185121, mae: 0.377007, mean_q: 4.206970
 63866/100000: episode: 1071, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 11.168, mean reward: 2.234 [2.048, 2.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.150, 10.343], loss: 0.157282, mae: 0.382814, mean_q: 4.202787
 63871/100000: episode: 1072, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 12.986, mean reward: 2.597 [2.375, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.367], loss: 0.223815, mae: 0.475286, mean_q: 4.430756
 63899/100000: episode: 1073, duration: 0.139s, episode steps: 28, steps per second: 202, episode reward: 76.210, mean reward: 2.722 [2.108, 4.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.621, 10.393], loss: 0.128764, mae: 0.348456, mean_q: 4.232746
 63904/100000: episode: 1074, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 16.695, mean reward: 3.339 [2.212, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.472, 10.450], loss: 0.136384, mae: 0.349679, mean_q: 4.071805
 63909/100000: episode: 1075, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 9.665, mean reward: 1.933 [1.786, 2.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.247], loss: 0.126048, mae: 0.312663, mean_q: 4.085381
 63930/100000: episode: 1076, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 45.690, mean reward: 2.176 [1.832, 2.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.308, 10.352], loss: 0.187843, mae: 0.386252, mean_q: 4.344590
 63935/100000: episode: 1077, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 11.825, mean reward: 2.365 [2.209, 2.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.391], loss: 0.142845, mae: 0.357352, mean_q: 4.101316
 63956/100000: episode: 1078, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 60.568, mean reward: 2.884 [2.272, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.690, 10.411], loss: 0.181857, mae: 0.409144, mean_q: 4.309895
 63986/100000: episode: 1079, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 77.352, mean reward: 2.578 [1.865, 3.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.398], loss: 0.192105, mae: 0.375891, mean_q: 4.313446
 64006/100000: episode: 1080, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 68.183, mean reward: 3.409 [2.581, 5.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.396, 10.100], loss: 0.191161, mae: 0.360170, mean_q: 4.329228
 64034/100000: episode: 1081, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 111.225, mean reward: 3.972 [2.659, 5.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.074, 10.328], loss: 0.161989, mae: 0.374138, mean_q: 4.280874
 64061/100000: episode: 1082, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 60.911, mean reward: 2.256 [1.741, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.354], loss: 0.227309, mae: 0.391617, mean_q: 4.339547
 64081/100000: episode: 1083, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 72.501, mean reward: 3.625 [2.178, 5.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.247, 10.100], loss: 0.124521, mae: 0.342993, mean_q: 4.377032
 64086/100000: episode: 1084, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 13.797, mean reward: 2.759 [2.492, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.462], loss: 0.241607, mae: 0.409470, mean_q: 4.319898
 64091/100000: episode: 1085, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 15.023, mean reward: 3.005 [2.323, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.185, 10.351], loss: 0.283595, mae: 0.404641, mean_q: 4.426234
 64112/100000: episode: 1086, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 64.341, mean reward: 3.064 [2.263, 5.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.505], loss: 0.147248, mae: 0.353240, mean_q: 4.286137
 64133/100000: episode: 1087, duration: 0.106s, episode steps: 21, steps per second: 198, episode reward: 67.087, mean reward: 3.195 [2.206, 5.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.565], loss: 0.129439, mae: 0.358324, mean_q: 4.240945
 64153/100000: episode: 1088, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 73.078, mean reward: 3.654 [2.933, 4.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.257, 10.100], loss: 0.240169, mae: 0.430278, mean_q: 4.360304
 64178/100000: episode: 1089, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 62.254, mean reward: 2.490 [1.978, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.125, 10.385], loss: 0.219268, mae: 0.417539, mean_q: 4.296772
 64206/100000: episode: 1090, duration: 0.145s, episode steps: 28, steps per second: 194, episode reward: 79.165, mean reward: 2.827 [2.227, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.772, 10.410], loss: 0.208941, mae: 0.389500, mean_q: 4.339452
 64236/100000: episode: 1091, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 97.452, mean reward: 3.248 [2.350, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.589, 10.548], loss: 0.291594, mae: 0.433719, mean_q: 4.431449
 64264/100000: episode: 1092, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 95.950, mean reward: 3.427 [2.547, 4.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.035, 10.522], loss: 0.183745, mae: 0.400921, mean_q: 4.363818
 64286/100000: episode: 1093, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 79.975, mean reward: 3.635 [2.282, 8.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.529], loss: 0.183617, mae: 0.371888, mean_q: 4.470488
 64306/100000: episode: 1094, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 69.165, mean reward: 3.458 [2.903, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.299, 10.100], loss: 0.162335, mae: 0.365039, mean_q: 4.360347
 64327/100000: episode: 1095, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 63.717, mean reward: 3.034 [2.170, 4.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.456], loss: 0.180714, mae: 0.375756, mean_q: 4.424066
 64349/100000: episode: 1096, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 58.060, mean reward: 2.639 [1.876, 6.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.346, 10.311], loss: 0.174199, mae: 0.415123, mean_q: 4.434067
 64374/100000: episode: 1097, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 62.867, mean reward: 2.515 [1.937, 4.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.590, 10.378], loss: 0.181516, mae: 0.407500, mean_q: 4.489052
 64395/100000: episode: 1098, duration: 0.106s, episode steps: 21, steps per second: 197, episode reward: 90.673, mean reward: 4.318 [2.022, 6.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.570, 10.501], loss: 0.182450, mae: 0.377319, mean_q: 4.413744
 64423/100000: episode: 1099, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 82.257, mean reward: 2.938 [2.225, 5.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.973, 10.371], loss: 0.216421, mae: 0.386126, mean_q: 4.412478
 64451/100000: episode: 1100, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 68.621, mean reward: 2.451 [1.836, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.772, 10.360], loss: 0.270817, mae: 0.431720, mean_q: 4.490489
 64476/100000: episode: 1101, duration: 0.127s, episode steps: 25, steps per second: 196, episode reward: 49.654, mean reward: 1.986 [1.540, 2.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.187, 10.158], loss: 0.199052, mae: 0.396000, mean_q: 4.449304
 64497/100000: episode: 1102, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 49.781, mean reward: 2.371 [1.948, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.360, 10.275], loss: 0.197041, mae: 0.384009, mean_q: 4.499424
 64527/100000: episode: 1103, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 67.148, mean reward: 2.238 [1.828, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.100, 10.279], loss: 0.223011, mae: 0.394387, mean_q: 4.470909
 64548/100000: episode: 1104, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 60.586, mean reward: 2.885 [2.096, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.343], loss: 0.174492, mae: 0.390063, mean_q: 4.477831
 64569/100000: episode: 1105, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 53.000, mean reward: 2.524 [2.153, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.898, 10.409], loss: 0.212884, mae: 0.392860, mean_q: 4.535497
 64594/100000: episode: 1106, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 55.473, mean reward: 2.219 [1.772, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.530, 10.240], loss: 0.193877, mae: 0.390213, mean_q: 4.480361
 64599/100000: episode: 1107, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 12.982, mean reward: 2.596 [2.081, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.438], loss: 0.426854, mae: 0.488449, mean_q: 4.552240
 64620/100000: episode: 1108, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 62.254, mean reward: 2.964 [2.220, 4.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.238, 10.493], loss: 0.245195, mae: 0.440193, mean_q: 4.483823
[Info] 2-TH LEVEL FOUND: 8.064720153808594, Considering 10/90 traces
 64652/100000: episode: 1109, duration: 4.391s, episode steps: 32, steps per second: 7, episode reward: 84.395, mean reward: 2.637 [2.149, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.026, 10.347], loss: 0.188349, mae: 0.404663, mean_q: 4.491358
 64675/100000: episode: 1110, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 100.471, mean reward: 4.368 [3.369, 7.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.596], loss: 0.164477, mae: 0.381738, mean_q: 4.601038
 64695/100000: episode: 1111, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 86.339, mean reward: 4.317 [3.360, 7.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.200, 10.472], loss: 0.201902, mae: 0.392916, mean_q: 4.544569
 64716/100000: episode: 1112, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 89.939, mean reward: 4.283 [2.992, 6.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.252, 10.460], loss: 0.153235, mae: 0.390142, mean_q: 4.568139
 64737/100000: episode: 1113, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 92.749, mean reward: 4.417 [3.274, 6.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.132, 10.635], loss: 0.192949, mae: 0.389428, mean_q: 4.648463
 64757/100000: episode: 1114, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 68.664, mean reward: 3.433 [2.686, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.433], loss: 0.134018, mae: 0.354214, mean_q: 4.538840
 64767/100000: episode: 1115, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 40.703, mean reward: 4.070 [3.155, 5.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.507], loss: 0.236229, mae: 0.398705, mean_q: 4.644515
 64777/100000: episode: 1116, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 37.040, mean reward: 3.704 [2.485, 4.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.543, 10.486], loss: 0.193709, mae: 0.397043, mean_q: 4.613252
 64794/100000: episode: 1117, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 67.069, mean reward: 3.945 [3.274, 4.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.243, 10.100], loss: 0.253023, mae: 0.445244, mean_q: 4.607111
 64804/100000: episode: 1118, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 33.386, mean reward: 3.339 [2.535, 4.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.460], loss: 0.135047, mae: 0.377807, mean_q: 4.693278
 64822/100000: episode: 1119, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 58.200, mean reward: 3.233 [1.892, 4.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.068, 10.100], loss: 0.161177, mae: 0.388047, mean_q: 4.574068
 64839/100000: episode: 1120, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 58.197, mean reward: 3.423 [2.448, 5.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.702, 10.100], loss: 0.139466, mae: 0.356122, mean_q: 4.515825
 64862/100000: episode: 1121, duration: 0.114s, episode steps: 23, steps per second: 201, episode reward: 158.676, mean reward: 6.899 [4.390, 12.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.467, 10.554], loss: 0.187437, mae: 0.388437, mean_q: 4.641192
 64879/100000: episode: 1122, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 67.980, mean reward: 3.999 [2.645, 12.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.896, 10.100], loss: 0.318760, mae: 0.496555, mean_q: 4.617956
 64898/100000: episode: 1123, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 84.144, mean reward: 4.429 [3.170, 6.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.692, 10.100], loss: 0.384147, mae: 0.499887, mean_q: 4.792984
 64915/100000: episode: 1124, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 77.966, mean reward: 4.586 [3.204, 7.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.938, 10.100], loss: 0.220626, mae: 0.449204, mean_q: 4.650519
 64933/100000: episode: 1125, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 94.376, mean reward: 5.243 [3.111, 10.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.093, 10.100], loss: 0.241086, mae: 0.453403, mean_q: 4.799481
 64953/100000: episode: 1126, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 98.418, mean reward: 4.921 [3.270, 7.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.550], loss: 0.245435, mae: 0.439051, mean_q: 4.694201
 64970/100000: episode: 1127, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 51.197, mean reward: 3.012 [2.386, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.308, 10.100], loss: 0.241633, mae: 0.455371, mean_q: 4.771688
 64993/100000: episode: 1128, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 102.225, mean reward: 4.445 [2.588, 9.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.532], loss: 0.367194, mae: 0.502861, mean_q: 4.900467
 65006/100000: episode: 1129, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 61.014, mean reward: 4.693 [3.422, 7.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.630], loss: 0.311441, mae: 0.510975, mean_q: 4.941093
 65025/100000: episode: 1130, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 59.774, mean reward: 3.146 [2.226, 5.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.248, 10.100], loss: 0.248962, mae: 0.449522, mean_q: 4.816258
 65042/100000: episode: 1131, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 47.669, mean reward: 2.804 [1.858, 5.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.212, 10.100], loss: 0.411175, mae: 0.505788, mean_q: 4.781077
 65065/100000: episode: 1132, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 137.190, mean reward: 5.965 [4.420, 8.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.666, 10.605], loss: 0.330707, mae: 0.484077, mean_q: 4.933531
 65082/100000: episode: 1133, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 59.232, mean reward: 3.484 [1.880, 5.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.229, 10.100], loss: 0.288040, mae: 0.463189, mean_q: 4.815644
 65102/100000: episode: 1134, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 112.619, mean reward: 5.631 [3.559, 12.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.561, 10.617], loss: 0.288742, mae: 0.452483, mean_q: 4.826796
 65119/100000: episode: 1135, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 86.795, mean reward: 5.106 [3.121, 13.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.363, 10.100], loss: 0.237940, mae: 0.459470, mean_q: 4.887263
 65142/100000: episode: 1136, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 148.275, mean reward: 6.447 [3.297, 9.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.122, 10.474], loss: 0.383531, mae: 0.520005, mean_q: 4.952496
 65162/100000: episode: 1137, duration: 0.107s, episode steps: 20, steps per second: 188, episode reward: 107.865, mean reward: 5.393 [3.925, 9.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.528, 10.530], loss: 0.299764, mae: 0.487861, mean_q: 4.956178
 65185/100000: episode: 1138, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 122.318, mean reward: 5.318 [3.468, 7.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.156, 10.653], loss: 0.415084, mae: 0.499760, mean_q: 4.954303
 65206/100000: episode: 1139, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 64.514, mean reward: 3.072 [2.399, 4.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.452], loss: 0.510085, mae: 0.610047, mean_q: 4.971241
 65219/100000: episode: 1140, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 52.119, mean reward: 4.009 [2.901, 9.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.450], loss: 0.386554, mae: 0.565678, mean_q: 5.175400
 65237/100000: episode: 1141, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 86.195, mean reward: 4.789 [3.006, 7.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.433, 10.100], loss: 0.433939, mae: 0.538575, mean_q: 5.033165
 65255/100000: episode: 1142, duration: 0.090s, episode steps: 18, steps per second: 199, episode reward: 71.787, mean reward: 3.988 [3.026, 5.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.194, 10.100], loss: 0.403001, mae: 0.566535, mean_q: 5.076317
 65265/100000: episode: 1143, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 37.222, mean reward: 3.722 [2.655, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.513], loss: 0.383570, mae: 0.549772, mean_q: 5.037539
 65283/100000: episode: 1144, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 61.703, mean reward: 3.428 [2.676, 4.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.318, 10.100], loss: 0.294208, mae: 0.487895, mean_q: 5.275477
 65306/100000: episode: 1145, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 134.462, mean reward: 5.846 [2.198, 20.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.690, 10.346], loss: 0.278512, mae: 0.468696, mean_q: 4.953978
 65327/100000: episode: 1146, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 60.707, mean reward: 2.891 [1.743, 4.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.326], loss: 0.355623, mae: 0.534034, mean_q: 5.147712
 65340/100000: episode: 1147, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 54.928, mean reward: 4.225 [2.700, 7.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.233, 10.426], loss: 0.423003, mae: 0.538820, mean_q: 5.296078
 65357/100000: episode: 1148, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 117.100, mean reward: 6.888 [2.198, 25.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.452, 10.100], loss: 0.396820, mae: 0.513882, mean_q: 5.084506
 65377/100000: episode: 1149, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 75.910, mean reward: 3.796 [2.582, 5.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.720, 10.457], loss: 0.284050, mae: 0.476632, mean_q: 5.021087
 65398/100000: episode: 1150, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 82.600, mean reward: 3.933 [2.609, 5.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.414, 10.400], loss: 0.287620, mae: 0.478213, mean_q: 5.122987
[Info] FALSIFICATION!
[Info] Levels: [6.0645704, 8.06472, 10.254876]
[Info] Cond. Prob: [0.1, 0.1, 0.16]
[Info] Error Prob: 0.0016000000000000003

 65407/100000: episode: 1151, duration: 4.539s, episode steps: 9, steps per second: 2, episode reward: 151.285, mean reward: 16.809 [3.832, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.020, 10.374], loss: 0.196704, mae: 0.400215, mean_q: 5.041590
 65507/100000: episode: 1152, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 183.978, mean reward: 1.840 [1.460, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.667, 10.157], loss: 1.749434, mae: 0.577977, mean_q: 5.163497
 65607/100000: episode: 1153, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 200.278, mean reward: 2.003 [1.481, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.269, 10.098], loss: 0.634881, mae: 0.666039, mean_q: 5.176253
 65707/100000: episode: 1154, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 188.362, mean reward: 1.884 [1.459, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.414, 10.190], loss: 1.853508, mae: 0.640462, mean_q: 5.115564
 65807/100000: episode: 1155, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 215.858, mean reward: 2.159 [1.443, 4.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.395, 10.098], loss: 0.460356, mae: 0.546791, mean_q: 5.210245
 65907/100000: episode: 1156, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 183.654, mean reward: 1.837 [1.452, 4.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.358, 10.177], loss: 0.412630, mae: 0.507540, mean_q: 5.131439
 66007/100000: episode: 1157, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 193.832, mean reward: 1.938 [1.515, 2.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.443, 10.223], loss: 0.385027, mae: 0.501841, mean_q: 5.163439
 66107/100000: episode: 1158, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 197.525, mean reward: 1.975 [1.450, 5.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.027, 10.127], loss: 4.607553, mae: 0.838048, mean_q: 5.231432
 66207/100000: episode: 1159, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 200.883, mean reward: 2.009 [1.446, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.862, 10.163], loss: 1.942657, mae: 0.698433, mean_q: 5.264804
 66307/100000: episode: 1160, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 189.352, mean reward: 1.894 [1.467, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.725, 10.157], loss: 1.785488, mae: 0.619351, mean_q: 5.235069
 66407/100000: episode: 1161, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 188.950, mean reward: 1.889 [1.484, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.541, 10.117], loss: 1.753506, mae: 0.609049, mean_q: 5.266439
 66507/100000: episode: 1162, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 196.751, mean reward: 1.968 [1.463, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.152, 10.518], loss: 0.524096, mae: 0.545062, mean_q: 5.315873
 66607/100000: episode: 1163, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 185.031, mean reward: 1.850 [1.432, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.517, 10.167], loss: 1.716434, mae: 0.575822, mean_q: 5.213432
 66707/100000: episode: 1164, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 188.236, mean reward: 1.882 [1.458, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.869, 10.129], loss: 0.608740, mae: 0.592901, mean_q: 5.357474
 66807/100000: episode: 1165, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 202.420, mean reward: 2.024 [1.535, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.520, 10.330], loss: 3.130006, mae: 0.666958, mean_q: 5.227352
 66907/100000: episode: 1166, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 190.832, mean reward: 1.908 [1.474, 4.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.557, 10.140], loss: 3.148388, mae: 0.724518, mean_q: 5.275877
 67007/100000: episode: 1167, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 187.440, mean reward: 1.874 [1.460, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.043, 10.139], loss: 0.496723, mae: 0.535096, mean_q: 5.260633
 67107/100000: episode: 1168, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 199.472, mean reward: 1.995 [1.459, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.904, 10.458], loss: 1.808710, mae: 0.623111, mean_q: 5.258922
 67207/100000: episode: 1169, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 185.300, mean reward: 1.853 [1.479, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.330, 10.098], loss: 1.731602, mae: 0.591380, mean_q: 5.215012
 67307/100000: episode: 1170, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 201.499, mean reward: 2.015 [1.474, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.917, 10.360], loss: 2.099938, mae: 0.684889, mean_q: 5.352316
 67407/100000: episode: 1171, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 189.717, mean reward: 1.897 [1.447, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.752, 10.098], loss: 0.552386, mae: 0.557566, mean_q: 5.322541
 67507/100000: episode: 1172, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 192.968, mean reward: 1.930 [1.448, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.800, 10.234], loss: 1.647251, mae: 0.581842, mean_q: 5.305378
 67607/100000: episode: 1173, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 192.391, mean reward: 1.924 [1.452, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.117, 10.098], loss: 0.422998, mae: 0.508037, mean_q: 5.208138
 67707/100000: episode: 1174, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 190.187, mean reward: 1.902 [1.443, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.530, 10.138], loss: 0.444669, mae: 0.533774, mean_q: 5.210834
 67807/100000: episode: 1175, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 199.741, mean reward: 1.997 [1.482, 4.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.473, 10.098], loss: 0.507899, mae: 0.535482, mean_q: 5.166220
 67907/100000: episode: 1176, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 196.834, mean reward: 1.968 [1.473, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.883, 10.140], loss: 0.436583, mae: 0.511047, mean_q: 5.072283
 68007/100000: episode: 1177, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 193.450, mean reward: 1.935 [1.435, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.735, 10.171], loss: 0.304098, mae: 0.480660, mean_q: 5.032686
 68107/100000: episode: 1178, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 188.198, mean reward: 1.882 [1.467, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.351, 10.141], loss: 1.786080, mae: 0.609924, mean_q: 5.060215
 68207/100000: episode: 1179, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 182.183, mean reward: 1.822 [1.438, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.726, 10.098], loss: 1.712693, mae: 0.606255, mean_q: 5.018107
 68307/100000: episode: 1180, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 207.641, mean reward: 2.076 [1.512, 2.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.688, 10.344], loss: 3.033354, mae: 0.664165, mean_q: 5.002731
 68407/100000: episode: 1181, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 191.089, mean reward: 1.911 [1.492, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.591, 10.110], loss: 1.684625, mae: 0.548292, mean_q: 4.887887
 68507/100000: episode: 1182, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 178.904, mean reward: 1.789 [1.468, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.656, 10.141], loss: 0.472073, mae: 0.494094, mean_q: 4.862949
 68607/100000: episode: 1183, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 185.944, mean reward: 1.859 [1.472, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.783, 10.240], loss: 0.324072, mae: 0.453600, mean_q: 4.777105
 68707/100000: episode: 1184, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 218.818, mean reward: 2.188 [1.479, 7.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.002, 10.098], loss: 0.613916, mae: 0.518447, mean_q: 4.911027
 68807/100000: episode: 1185, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 217.418, mean reward: 2.174 [1.490, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.105, 10.098], loss: 0.336895, mae: 0.453177, mean_q: 4.772546
 68907/100000: episode: 1186, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 180.689, mean reward: 1.807 [1.447, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.146, 10.158], loss: 3.059143, mae: 0.642012, mean_q: 4.894788
 69007/100000: episode: 1187, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 188.089, mean reward: 1.881 [1.457, 2.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.759, 10.112], loss: 0.341732, mae: 0.453080, mean_q: 4.698984
 69107/100000: episode: 1188, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 190.122, mean reward: 1.901 [1.458, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.215, 10.182], loss: 1.612699, mae: 0.497609, mean_q: 4.717298
 69207/100000: episode: 1189, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 186.490, mean reward: 1.865 [1.463, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.093, 10.315], loss: 1.620381, mae: 0.481084, mean_q: 4.717803
 69307/100000: episode: 1190, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 209.684, mean reward: 2.097 [1.438, 3.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.855, 10.383], loss: 0.426021, mae: 0.466917, mean_q: 4.627130
 69407/100000: episode: 1191, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 201.994, mean reward: 2.020 [1.432, 5.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.748, 10.098], loss: 1.768508, mae: 0.528778, mean_q: 4.602952
 69507/100000: episode: 1192, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 189.315, mean reward: 1.893 [1.467, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.624, 10.098], loss: 0.461367, mae: 0.447660, mean_q: 4.611541
 69607/100000: episode: 1193, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 185.396, mean reward: 1.854 [1.440, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.644, 10.098], loss: 0.320093, mae: 0.419158, mean_q: 4.523289
 69707/100000: episode: 1194, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 188.328, mean reward: 1.883 [1.447, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.288, 10.266], loss: 2.882288, mae: 0.534362, mean_q: 4.551038
 69807/100000: episode: 1195, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 191.058, mean reward: 1.911 [1.462, 4.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.599, 10.115], loss: 0.322419, mae: 0.408887, mean_q: 4.410808
 69907/100000: episode: 1196, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 195.945, mean reward: 1.959 [1.468, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.895, 10.135], loss: 0.229773, mae: 0.365607, mean_q: 4.300983
 70007/100000: episode: 1197, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 220.556, mean reward: 2.206 [1.509, 4.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.821, 10.170], loss: 1.489025, mae: 0.420850, mean_q: 4.190998
 70107/100000: episode: 1198, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 197.835, mean reward: 1.978 [1.447, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.347, 10.098], loss: 0.197626, mae: 0.355437, mean_q: 4.114889
 70207/100000: episode: 1199, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 220.710, mean reward: 2.207 [1.480, 4.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.696, 10.320], loss: 1.496971, mae: 0.401768, mean_q: 4.030191
 70307/100000: episode: 1200, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 190.636, mean reward: 1.906 [1.478, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.361, 10.098], loss: 1.368768, mae: 0.388795, mean_q: 3.940717
 70407/100000: episode: 1201, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 203.871, mean reward: 2.039 [1.478, 5.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.181, 10.373], loss: 0.099302, mae: 0.297354, mean_q: 3.856247
 70507/100000: episode: 1202, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 198.581, mean reward: 1.986 [1.491, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.520, 10.227], loss: 0.096663, mae: 0.299270, mean_q: 3.861339
 70607/100000: episode: 1203, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.414, mean reward: 1.914 [1.507, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.565, 10.182], loss: 0.089990, mae: 0.288636, mean_q: 3.872035
 70707/100000: episode: 1204, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 201.172, mean reward: 2.012 [1.539, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.697, 10.098], loss: 0.091225, mae: 0.294524, mean_q: 3.888617
 70807/100000: episode: 1205, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 198.468, mean reward: 1.985 [1.444, 4.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.470, 10.098], loss: 0.094009, mae: 0.293012, mean_q: 3.873206
 70907/100000: episode: 1206, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 184.184, mean reward: 1.842 [1.476, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.737, 10.277], loss: 0.096127, mae: 0.292182, mean_q: 3.861466
 71007/100000: episode: 1207, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 199.521, mean reward: 1.995 [1.457, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.548, 10.303], loss: 0.078623, mae: 0.275052, mean_q: 3.843881
 71107/100000: episode: 1208, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 207.551, mean reward: 2.076 [1.502, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.585, 10.178], loss: 0.094844, mae: 0.296674, mean_q: 3.872596
 71207/100000: episode: 1209, duration: 0.477s, episode steps: 100, steps per second: 209, episode reward: 205.336, mean reward: 2.053 [1.484, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.901, 10.121], loss: 0.102902, mae: 0.296533, mean_q: 3.864969
 71307/100000: episode: 1210, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 204.212, mean reward: 2.042 [1.498, 3.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.684, 10.330], loss: 0.097547, mae: 0.298290, mean_q: 3.871004
 71407/100000: episode: 1211, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 185.630, mean reward: 1.856 [1.480, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.732, 10.098], loss: 0.085915, mae: 0.286198, mean_q: 3.884295
 71507/100000: episode: 1212, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 185.291, mean reward: 1.853 [1.447, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.550, 10.098], loss: 0.079184, mae: 0.283866, mean_q: 3.883627
 71607/100000: episode: 1213, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 190.324, mean reward: 1.903 [1.439, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.638, 10.098], loss: 0.075477, mae: 0.268246, mean_q: 3.843608
 71707/100000: episode: 1214, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 198.327, mean reward: 1.983 [1.474, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.583, 10.098], loss: 0.089808, mae: 0.292619, mean_q: 3.853953
 71807/100000: episode: 1215, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 187.286, mean reward: 1.873 [1.484, 2.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.424, 10.276], loss: 0.095702, mae: 0.304094, mean_q: 3.884054
 71907/100000: episode: 1216, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 188.103, mean reward: 1.881 [1.442, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.332, 10.172], loss: 0.092334, mae: 0.293713, mean_q: 3.883982
 72007/100000: episode: 1217, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 203.354, mean reward: 2.034 [1.470, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.584, 10.119], loss: 0.095060, mae: 0.296918, mean_q: 3.878775
 72107/100000: episode: 1218, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 211.167, mean reward: 2.112 [1.482, 5.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.460, 10.098], loss: 0.098670, mae: 0.298122, mean_q: 3.866444
 72207/100000: episode: 1219, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 219.218, mean reward: 2.192 [1.487, 8.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.231, 10.098], loss: 0.084741, mae: 0.277115, mean_q: 3.852651
 72307/100000: episode: 1220, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 191.130, mean reward: 1.911 [1.445, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.950, 10.263], loss: 0.093994, mae: 0.293050, mean_q: 3.868094
 72407/100000: episode: 1221, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: 215.790, mean reward: 2.158 [1.453, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.193, 10.452], loss: 0.094766, mae: 0.295856, mean_q: 3.876640
 72507/100000: episode: 1222, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 189.949, mean reward: 1.899 [1.467, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.464, 10.098], loss: 0.097368, mae: 0.292500, mean_q: 3.884176
 72607/100000: episode: 1223, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 181.926, mean reward: 1.819 [1.431, 2.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.542, 10.317], loss: 0.116505, mae: 0.312372, mean_q: 3.910358
 72707/100000: episode: 1224, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 194.817, mean reward: 1.948 [1.450, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.870, 10.098], loss: 0.092303, mae: 0.290807, mean_q: 3.888018
 72807/100000: episode: 1225, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 191.668, mean reward: 1.917 [1.452, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.374, 10.102], loss: 0.106083, mae: 0.295435, mean_q: 3.910351
 72907/100000: episode: 1226, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 206.635, mean reward: 2.066 [1.456, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.487, 10.233], loss: 0.091239, mae: 0.295637, mean_q: 3.896697
 73007/100000: episode: 1227, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 195.607, mean reward: 1.956 [1.434, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.324, 10.103], loss: 0.096222, mae: 0.303455, mean_q: 3.902250
 73107/100000: episode: 1228, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 206.801, mean reward: 2.068 [1.455, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.687, 10.134], loss: 0.088874, mae: 0.290319, mean_q: 3.906703
 73207/100000: episode: 1229, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 191.087, mean reward: 1.911 [1.459, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.606, 10.151], loss: 0.095501, mae: 0.285343, mean_q: 3.890007
 73307/100000: episode: 1230, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 189.494, mean reward: 1.895 [1.441, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.161, 10.098], loss: 0.080548, mae: 0.285971, mean_q: 3.889657
 73407/100000: episode: 1231, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 190.350, mean reward: 1.904 [1.446, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.548, 10.181], loss: 0.101520, mae: 0.302264, mean_q: 3.903049
 73507/100000: episode: 1232, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 193.714, mean reward: 1.937 [1.450, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.535, 10.138], loss: 0.128420, mae: 0.314177, mean_q: 3.922801
 73607/100000: episode: 1233, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.511, mean reward: 1.895 [1.474, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.607, 10.182], loss: 0.097521, mae: 0.301373, mean_q: 3.926363
 73707/100000: episode: 1234, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 193.563, mean reward: 1.936 [1.457, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.180, 10.098], loss: 0.099330, mae: 0.304303, mean_q: 3.913761
 73807/100000: episode: 1235, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 207.106, mean reward: 2.071 [1.466, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.552, 10.126], loss: 0.088911, mae: 0.294647, mean_q: 3.908262
 73907/100000: episode: 1236, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 192.797, mean reward: 1.928 [1.445, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.192, 10.116], loss: 0.081784, mae: 0.289558, mean_q: 3.888962
 74007/100000: episode: 1237, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 213.234, mean reward: 2.132 [1.494, 5.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-2.062, 10.188], loss: 0.096110, mae: 0.296174, mean_q: 3.912338
 74107/100000: episode: 1238, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 192.733, mean reward: 1.927 [1.467, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.528, 10.098], loss: 0.078892, mae: 0.285983, mean_q: 3.897094
 74207/100000: episode: 1239, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 190.568, mean reward: 1.906 [1.439, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.114, 10.098], loss: 0.099232, mae: 0.298042, mean_q: 3.917067
 74307/100000: episode: 1240, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 197.433, mean reward: 1.974 [1.446, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.470, 10.098], loss: 0.096163, mae: 0.304502, mean_q: 3.916625
 74407/100000: episode: 1241, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 206.721, mean reward: 2.067 [1.529, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.293, 10.098], loss: 0.100155, mae: 0.304641, mean_q: 3.922412
 74507/100000: episode: 1242, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 193.145, mean reward: 1.931 [1.473, 5.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.491, 10.177], loss: 0.096706, mae: 0.297735, mean_q: 3.908033
 74607/100000: episode: 1243, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 202.527, mean reward: 2.025 [1.454, 5.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.282, 10.382], loss: 0.096141, mae: 0.297837, mean_q: 3.899477
 74707/100000: episode: 1244, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 184.467, mean reward: 1.845 [1.484, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.928, 10.098], loss: 0.103770, mae: 0.303517, mean_q: 3.909175
 74807/100000: episode: 1245, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 189.748, mean reward: 1.897 [1.479, 3.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.791, 10.212], loss: 0.096128, mae: 0.294693, mean_q: 3.907233
 74907/100000: episode: 1246, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 193.571, mean reward: 1.936 [1.445, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.313, 10.098], loss: 0.088010, mae: 0.288969, mean_q: 3.892380
 75007/100000: episode: 1247, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 191.029, mean reward: 1.910 [1.441, 4.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.708, 10.223], loss: 0.094934, mae: 0.300425, mean_q: 3.903084
 75107/100000: episode: 1248, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 189.120, mean reward: 1.891 [1.474, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.379, 10.290], loss: 0.089755, mae: 0.288130, mean_q: 3.904088
 75207/100000: episode: 1249, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 188.958, mean reward: 1.890 [1.463, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.433, 10.098], loss: 0.089235, mae: 0.292660, mean_q: 3.877953
 75307/100000: episode: 1250, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.169, mean reward: 1.852 [1.457, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.000, 10.142], loss: 0.078521, mae: 0.276719, mean_q: 3.880695
[Info] 1-TH LEVEL FOUND: 5.084628582000732, Considering 10/90 traces
 75407/100000: episode: 1251, duration: 4.728s, episode steps: 100, steps per second: 21, episode reward: 193.577, mean reward: 1.936 [1.448, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.882, 10.243], loss: 0.091752, mae: 0.297822, mean_q: 3.889621
 75431/100000: episode: 1252, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 74.606, mean reward: 3.109 [2.095, 5.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.444, 10.556], loss: 0.086957, mae: 0.278016, mean_q: 3.889954
 75440/100000: episode: 1253, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 25.393, mean reward: 2.821 [1.720, 4.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.260], loss: 0.102145, mae: 0.321551, mean_q: 3.970872
 75455/100000: episode: 1254, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 36.384, mean reward: 2.426 [1.943, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.308, 10.327], loss: 0.076216, mae: 0.274789, mean_q: 3.893603
 75476/100000: episode: 1255, duration: 0.103s, episode steps: 21, steps per second: 204, episode reward: 46.508, mean reward: 2.215 [1.775, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.284], loss: 0.109804, mae: 0.302782, mean_q: 3.883706
 75524/100000: episode: 1256, duration: 0.232s, episode steps: 48, steps per second: 207, episode reward: 108.755, mean reward: 2.266 [1.504, 3.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.308, 10.284], loss: 0.083087, mae: 0.293293, mean_q: 3.906121
 75539/100000: episode: 1257, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 35.675, mean reward: 2.378 [2.022, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.926, 10.308], loss: 0.088367, mae: 0.315113, mean_q: 3.954059
 75587/100000: episode: 1258, duration: 0.235s, episode steps: 48, steps per second: 205, episode reward: 175.618, mean reward: 3.659 [2.081, 8.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.752, 10.429], loss: 0.088816, mae: 0.295359, mean_q: 3.916476
 75611/100000: episode: 1259, duration: 0.127s, episode steps: 24, steps per second: 188, episode reward: 59.040, mean reward: 2.460 [1.934, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.250, 10.444], loss: 0.091868, mae: 0.303962, mean_q: 3.914232
 75632/100000: episode: 1260, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 57.696, mean reward: 2.747 [1.769, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.544, 10.343], loss: 0.093308, mae: 0.303876, mean_q: 3.947471
 75650/100000: episode: 1261, duration: 0.090s, episode steps: 18, steps per second: 201, episode reward: 39.468, mean reward: 2.193 [1.648, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.213], loss: 0.080795, mae: 0.284110, mean_q: 3.892349
 75665/100000: episode: 1262, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 30.091, mean reward: 2.006 [1.576, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.235], loss: 0.083409, mae: 0.283042, mean_q: 3.931808
 75717/100000: episode: 1263, duration: 0.261s, episode steps: 52, steps per second: 199, episode reward: 128.255, mean reward: 2.466 [1.544, 5.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.428, 10.149], loss: 0.108817, mae: 0.303632, mean_q: 3.935253
 75732/100000: episode: 1264, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 30.455, mean reward: 2.030 [1.783, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.247, 10.265], loss: 0.092880, mae: 0.292908, mean_q: 3.887937
 75750/100000: episode: 1265, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 42.581, mean reward: 2.366 [1.709, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.271], loss: 0.114679, mae: 0.301950, mean_q: 3.961357
 75765/100000: episode: 1266, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 27.472, mean reward: 1.831 [1.551, 2.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-1.128, 10.153], loss: 0.081634, mae: 0.272191, mean_q: 3.898277
 75798/100000: episode: 1267, duration: 0.172s, episode steps: 33, steps per second: 191, episode reward: 80.082, mean reward: 2.427 [1.692, 5.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.956, 10.284], loss: 0.134867, mae: 0.323705, mean_q: 3.933386
 75831/100000: episode: 1268, duration: 0.160s, episode steps: 33, steps per second: 206, episode reward: 113.144, mean reward: 3.429 [2.450, 6.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.653], loss: 0.095875, mae: 0.311192, mean_q: 3.974146
 75879/100000: episode: 1269, duration: 0.237s, episode steps: 48, steps per second: 203, episode reward: 102.735, mean reward: 2.140 [1.454, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.305, 10.100], loss: 0.096488, mae: 0.304116, mean_q: 3.962228
 75897/100000: episode: 1270, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 48.133, mean reward: 2.674 [2.184, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.414], loss: 0.109759, mae: 0.316242, mean_q: 3.998262
 75945/100000: episode: 1271, duration: 0.242s, episode steps: 48, steps per second: 199, episode reward: 101.152, mean reward: 2.107 [1.510, 2.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.298, 10.208], loss: 0.090127, mae: 0.286874, mean_q: 3.923510
 75954/100000: episode: 1272, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 25.920, mean reward: 2.880 [2.318, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.176, 10.335], loss: 0.069035, mae: 0.290164, mean_q: 4.000460
 75987/100000: episode: 1273, duration: 0.164s, episode steps: 33, steps per second: 201, episode reward: 82.207, mean reward: 2.491 [1.726, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.345], loss: 0.093260, mae: 0.290125, mean_q: 3.974494
 76035/100000: episode: 1274, duration: 0.237s, episode steps: 48, steps per second: 202, episode reward: 120.605, mean reward: 2.513 [1.542, 14.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.654, 10.195], loss: 0.110573, mae: 0.322717, mean_q: 3.987912
 76083/100000: episode: 1275, duration: 0.239s, episode steps: 48, steps per second: 201, episode reward: 105.855, mean reward: 2.205 [1.566, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.309, 10.100], loss: 0.099658, mae: 0.311386, mean_q: 4.006067
 76098/100000: episode: 1276, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 33.384, mean reward: 2.226 [1.957, 2.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.364, 10.389], loss: 0.160896, mae: 0.333441, mean_q: 4.003012
 76122/100000: episode: 1277, duration: 0.125s, episode steps: 24, steps per second: 191, episode reward: 90.374, mean reward: 3.766 [1.943, 5.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.143, 10.476], loss: 0.101176, mae: 0.299506, mean_q: 3.929978
 76155/100000: episode: 1278, duration: 0.164s, episode steps: 33, steps per second: 202, episode reward: 61.422, mean reward: 1.861 [1.459, 2.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.441, 10.173], loss: 0.097536, mae: 0.315506, mean_q: 4.042528
 76173/100000: episode: 1279, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 43.060, mean reward: 2.392 [1.680, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.108, 10.274], loss: 0.124911, mae: 0.312143, mean_q: 4.010870
 76182/100000: episode: 1280, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 18.641, mean reward: 2.071 [1.707, 2.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.255], loss: 0.135538, mae: 0.327368, mean_q: 4.076322
 76197/100000: episode: 1281, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 34.637, mean reward: 2.309 [1.867, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.121, 10.326], loss: 0.121977, mae: 0.333392, mean_q: 4.081006
 76249/100000: episode: 1282, duration: 0.256s, episode steps: 52, steps per second: 203, episode reward: 125.019, mean reward: 2.404 [1.645, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-0.883, 10.225], loss: 0.113662, mae: 0.331713, mean_q: 3.992068
 76264/100000: episode: 1283, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 36.005, mean reward: 2.400 [2.020, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-1.300, 10.334], loss: 0.084973, mae: 0.288962, mean_q: 3.987235
 76312/100000: episode: 1284, duration: 0.234s, episode steps: 48, steps per second: 205, episode reward: 113.004, mean reward: 2.354 [1.876, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.297, 10.270], loss: 0.086968, mae: 0.293575, mean_q: 4.011286
 76327/100000: episode: 1285, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 41.759, mean reward: 2.784 [2.150, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.330, 10.435], loss: 0.096075, mae: 0.286158, mean_q: 3.950743
 76336/100000: episode: 1286, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 28.771, mean reward: 3.197 [2.259, 5.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.627, 10.309], loss: 0.105825, mae: 0.314936, mean_q: 3.986749
 76345/100000: episode: 1287, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 20.300, mean reward: 2.256 [1.895, 2.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.383], loss: 0.079749, mae: 0.294726, mean_q: 4.043742
 76354/100000: episode: 1288, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 18.602, mean reward: 2.067 [1.532, 2.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.154], loss: 0.124923, mae: 0.328744, mean_q: 3.976747
 76402/100000: episode: 1289, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 98.216, mean reward: 2.046 [1.455, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.596, 10.100], loss: 0.139317, mae: 0.324032, mean_q: 4.023676
 76454/100000: episode: 1290, duration: 0.263s, episode steps: 52, steps per second: 198, episode reward: 153.321, mean reward: 2.948 [1.869, 9.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.408, 10.310], loss: 0.110439, mae: 0.312112, mean_q: 4.040120
 76463/100000: episode: 1291, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 21.353, mean reward: 2.373 [2.138, 2.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.571, 10.407], loss: 0.100101, mae: 0.323114, mean_q: 4.004597
 76478/100000: episode: 1292, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 35.721, mean reward: 2.381 [2.087, 2.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.121, 10.386], loss: 0.104244, mae: 0.326539, mean_q: 4.077402
 76526/100000: episode: 1293, duration: 0.243s, episode steps: 48, steps per second: 197, episode reward: 126.104, mean reward: 2.627 [1.695, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.311, 10.502], loss: 0.110171, mae: 0.315690, mean_q: 4.039017
 76574/100000: episode: 1294, duration: 0.246s, episode steps: 48, steps per second: 195, episode reward: 99.496, mean reward: 2.073 [1.517, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.769, 10.100], loss: 0.104790, mae: 0.312170, mean_q: 4.092607
 76589/100000: episode: 1295, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 31.305, mean reward: 2.087 [1.846, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.420, 10.210], loss: 0.168855, mae: 0.347688, mean_q: 4.135781
 76604/100000: episode: 1296, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 35.061, mean reward: 2.337 [1.590, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.166, 10.338], loss: 0.097346, mae: 0.294768, mean_q: 4.082202
 76656/100000: episode: 1297, duration: 0.250s, episode steps: 52, steps per second: 208, episode reward: 154.874, mean reward: 2.978 [1.848, 5.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-1.349, 10.297], loss: 0.099195, mae: 0.309988, mean_q: 4.051549
 76680/100000: episode: 1298, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 56.349, mean reward: 2.348 [1.765, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.923, 10.273], loss: 0.281588, mae: 0.349627, mean_q: 4.111013
 76695/100000: episode: 1299, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 42.596, mean reward: 2.840 [2.041, 5.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.041, 10.326], loss: 0.082098, mae: 0.298723, mean_q: 4.048571
 76719/100000: episode: 1300, duration: 0.123s, episode steps: 24, steps per second: 196, episode reward: 75.702, mean reward: 3.154 [2.167, 5.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.407], loss: 0.112164, mae: 0.324543, mean_q: 4.184278
 76734/100000: episode: 1301, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 31.933, mean reward: 2.129 [1.844, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.354, 10.290], loss: 0.204679, mae: 0.393073, mean_q: 4.168055
 76767/100000: episode: 1302, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 78.301, mean reward: 2.373 [1.522, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.417, 10.226], loss: 0.126972, mae: 0.338526, mean_q: 4.125636
 76782/100000: episode: 1303, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 33.004, mean reward: 2.200 [1.987, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.411], loss: 0.116196, mae: 0.337858, mean_q: 4.080451
 76797/100000: episode: 1304, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 38.268, mean reward: 2.551 [1.980, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.056, 10.415], loss: 0.094297, mae: 0.302070, mean_q: 4.058275
 76849/100000: episode: 1305, duration: 0.253s, episode steps: 52, steps per second: 205, episode reward: 121.031, mean reward: 2.328 [1.501, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.792, 10.113], loss: 0.108967, mae: 0.324947, mean_q: 4.134630
 76867/100000: episode: 1306, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 62.383, mean reward: 3.466 [2.356, 5.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.525], loss: 0.145065, mae: 0.358082, mean_q: 4.161988
 76915/100000: episode: 1307, duration: 0.240s, episode steps: 48, steps per second: 200, episode reward: 107.753, mean reward: 2.245 [1.566, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.421, 10.176], loss: 0.132692, mae: 0.333653, mean_q: 4.153049
 76967/100000: episode: 1308, duration: 0.262s, episode steps: 52, steps per second: 198, episode reward: 159.866, mean reward: 3.074 [2.103, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.809, 10.538], loss: 0.180180, mae: 0.352012, mean_q: 4.194142
 76985/100000: episode: 1309, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 43.833, mean reward: 2.435 [1.998, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.469, 10.351], loss: 0.132078, mae: 0.357445, mean_q: 4.190154
 77037/100000: episode: 1310, duration: 0.256s, episode steps: 52, steps per second: 203, episode reward: 220.783, mean reward: 4.246 [2.491, 6.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.945, 10.506], loss: 0.147660, mae: 0.316667, mean_q: 4.161118
 77058/100000: episode: 1311, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 53.059, mean reward: 2.527 [1.734, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.259], loss: 0.206554, mae: 0.354150, mean_q: 4.152724
 77082/100000: episode: 1312, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 51.565, mean reward: 2.149 [1.848, 2.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.339], loss: 0.124665, mae: 0.333434, mean_q: 4.199184
 77091/100000: episode: 1313, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 19.814, mean reward: 2.202 [1.846, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.319], loss: 0.110535, mae: 0.329862, mean_q: 4.178249
 77139/100000: episode: 1314, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 103.057, mean reward: 2.147 [1.627, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.567, 10.100], loss: 0.115030, mae: 0.323451, mean_q: 4.202098
 77154/100000: episode: 1315, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 41.219, mean reward: 2.748 [2.186, 3.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.311, 10.378], loss: 0.212695, mae: 0.372369, mean_q: 4.187468
 77172/100000: episode: 1316, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 50.902, mean reward: 2.828 [2.153, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.421, 10.399], loss: 0.147280, mae: 0.352270, mean_q: 4.226535
 77196/100000: episode: 1317, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 58.237, mean reward: 2.427 [1.827, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.173, 10.309], loss: 0.135290, mae: 0.320945, mean_q: 4.256137
 77205/100000: episode: 1318, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 17.667, mean reward: 1.963 [1.703, 2.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.043, 10.345], loss: 0.105767, mae: 0.332904, mean_q: 4.287126
 77214/100000: episode: 1319, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 20.824, mean reward: 2.314 [1.659, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.289], loss: 0.085850, mae: 0.296223, mean_q: 4.116547
 77262/100000: episode: 1320, duration: 0.253s, episode steps: 48, steps per second: 189, episode reward: 115.783, mean reward: 2.412 [1.460, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.317, 10.100], loss: 0.116239, mae: 0.324643, mean_q: 4.246889
 77314/100000: episode: 1321, duration: 0.261s, episode steps: 52, steps per second: 199, episode reward: 122.503, mean reward: 2.356 [1.790, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.785, 10.388], loss: 0.125558, mae: 0.342575, mean_q: 4.228190
 77329/100000: episode: 1322, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 50.816, mean reward: 3.388 [2.116, 7.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.309, 10.621], loss: 0.169138, mae: 0.363706, mean_q: 4.276726
 77344/100000: episode: 1323, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 31.008, mean reward: 2.067 [1.669, 2.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.211], loss: 0.287556, mae: 0.390958, mean_q: 4.257431
 77368/100000: episode: 1324, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 64.743, mean reward: 2.698 [1.895, 5.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.576, 10.498], loss: 0.139524, mae: 0.366994, mean_q: 4.169848
 77420/100000: episode: 1325, duration: 0.255s, episode steps: 52, steps per second: 204, episode reward: 125.145, mean reward: 2.407 [1.907, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.461, 10.265], loss: 0.132053, mae: 0.350259, mean_q: 4.252022
 77435/100000: episode: 1326, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 39.822, mean reward: 2.655 [1.839, 4.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.546], loss: 0.138354, mae: 0.322453, mean_q: 4.297466
 77456/100000: episode: 1327, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 40.782, mean reward: 1.942 [1.656, 2.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.418, 10.281], loss: 0.158453, mae: 0.344063, mean_q: 4.265643
 77489/100000: episode: 1328, duration: 0.164s, episode steps: 33, steps per second: 202, episode reward: 78.634, mean reward: 2.383 [1.802, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.445, 10.349], loss: 0.213544, mae: 0.380534, mean_q: 4.270379
 77537/100000: episode: 1329, duration: 0.231s, episode steps: 48, steps per second: 207, episode reward: 112.352, mean reward: 2.341 [1.720, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.536, 10.100], loss: 0.159694, mae: 0.374952, mean_q: 4.251093
 77555/100000: episode: 1330, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 43.890, mean reward: 2.438 [1.744, 5.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.030, 10.215], loss: 0.111050, mae: 0.334274, mean_q: 4.247260
 77564/100000: episode: 1331, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 22.898, mean reward: 2.544 [1.882, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.060, 10.297], loss: 0.199690, mae: 0.422542, mean_q: 4.500134
 77582/100000: episode: 1332, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 40.842, mean reward: 2.269 [1.837, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.295], loss: 0.189125, mae: 0.395491, mean_q: 4.328953
 77600/100000: episode: 1333, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 42.759, mean reward: 2.375 [2.072, 2.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.376], loss: 0.112900, mae: 0.339876, mean_q: 4.177202
 77615/100000: episode: 1334, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 37.977, mean reward: 2.532 [1.793, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.272, 10.299], loss: 0.121621, mae: 0.345022, mean_q: 4.284791
 77630/100000: episode: 1335, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 33.566, mean reward: 2.238 [1.886, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.319], loss: 0.139433, mae: 0.359293, mean_q: 4.331416
 77678/100000: episode: 1336, duration: 0.237s, episode steps: 48, steps per second: 203, episode reward: 90.526, mean reward: 1.886 [1.474, 2.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.287, 10.158], loss: 0.155056, mae: 0.375780, mean_q: 4.294301
 77693/100000: episode: 1337, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 36.846, mean reward: 2.456 [1.871, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.392], loss: 0.126355, mae: 0.345986, mean_q: 4.311225
 77717/100000: episode: 1338, duration: 0.115s, episode steps: 24, steps per second: 208, episode reward: 56.137, mean reward: 2.339 [1.936, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.336], loss: 0.165533, mae: 0.372643, mean_q: 4.340711
 77735/100000: episode: 1339, duration: 0.107s, episode steps: 18, steps per second: 168, episode reward: 54.100, mean reward: 3.006 [2.253, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-1.115, 10.371], loss: 0.138042, mae: 0.336712, mean_q: 4.272378
 77744/100000: episode: 1340, duration: 0.045s, episode steps: 9, steps per second: 200, episode reward: 20.713, mean reward: 2.301 [1.927, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.821, 10.265], loss: 0.137755, mae: 0.371527, mean_q: 4.363958
[Info] 2-TH LEVEL FOUND: 7.755878925323486, Considering 10/90 traces
 77753/100000: episode: 1341, duration: 4.215s, episode steps: 9, steps per second: 2, episode reward: 19.780, mean reward: 2.198 [1.919, 2.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.486, 10.361], loss: 0.382116, mae: 0.416449, mean_q: 4.443592
 77766/100000: episode: 1342, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 53.925, mean reward: 4.148 [3.382, 6.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.160, 10.443], loss: 0.182451, mae: 0.378580, mean_q: 4.295480
 77813/100000: episode: 1343, duration: 0.238s, episode steps: 47, steps per second: 197, episode reward: 150.434, mean reward: 3.201 [2.233, 5.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.414, 10.435], loss: 0.130309, mae: 0.344305, mean_q: 4.344906
 77854/100000: episode: 1344, duration: 0.200s, episode steps: 41, steps per second: 205, episode reward: 125.693, mean reward: 3.066 [1.834, 5.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.309, 10.312], loss: 0.222585, mae: 0.382592, mean_q: 4.355569
 77899/100000: episode: 1345, duration: 0.222s, episode steps: 45, steps per second: 203, episode reward: 104.252, mean reward: 2.317 [1.502, 3.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.717, 10.100], loss: 0.186388, mae: 0.360702, mean_q: 4.358270
 77912/100000: episode: 1346, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 53.091, mean reward: 4.084 [3.104, 5.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.559], loss: 0.107518, mae: 0.336281, mean_q: 4.324128
[Info] FALSIFICATION!
[Info] Levels: [5.0846286, 7.755879, 7.9624934]
[Info] Cond. Prob: [0.1, 0.1, 0.05]
[Info] Error Prob: 0.0005000000000000001

 77923/100000: episode: 1347, duration: 4.533s, episode steps: 11, steps per second: 2, episode reward: 158.332, mean reward: 14.394 [4.171, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.605 [-0.775, 7.961], loss: 0.116104, mae: 0.334793, mean_q: 4.369240
 78023/100000: episode: 1348, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 197.400, mean reward: 1.974 [1.462, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.652, 10.184], loss: 1.580661, mae: 0.496464, mean_q: 4.399863
 78123/100000: episode: 1349, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 185.264, mean reward: 1.853 [1.461, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.180, 10.311], loss: 2.850013, mae: 0.506998, mean_q: 4.417656
 78223/100000: episode: 1350, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 193.166, mean reward: 1.932 [1.482, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.421, 10.098], loss: 1.521933, mae: 0.443959, mean_q: 4.424312
 78323/100000: episode: 1351, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 193.750, mean reward: 1.938 [1.473, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.617, 10.098], loss: 1.487256, mae: 0.444940, mean_q: 4.418664
 78423/100000: episode: 1352, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 195.852, mean reward: 1.959 [1.492, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-2.055, 10.098], loss: 1.492033, mae: 0.407135, mean_q: 4.405113
 78523/100000: episode: 1353, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 238.135, mean reward: 2.381 [1.489, 4.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.875, 10.098], loss: 1.564245, mae: 0.508499, mean_q: 4.407334
 78623/100000: episode: 1354, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 180.813, mean reward: 1.808 [1.454, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.203, 10.098], loss: 0.144833, mae: 0.363898, mean_q: 4.400738
 78723/100000: episode: 1355, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 201.307, mean reward: 2.013 [1.438, 5.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.103, 10.098], loss: 0.182293, mae: 0.379810, mean_q: 4.391063
 78823/100000: episode: 1356, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 179.779, mean reward: 1.798 [1.485, 5.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.316, 10.098], loss: 1.483577, mae: 0.439640, mean_q: 4.449861
 78923/100000: episode: 1357, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.265, mean reward: 1.873 [1.450, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.639, 10.098], loss: 2.753168, mae: 0.507974, mean_q: 4.468513
 79023/100000: episode: 1358, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 198.006, mean reward: 1.980 [1.444, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.065, 10.241], loss: 1.486841, mae: 0.434973, mean_q: 4.441624
 79123/100000: episode: 1359, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 198.474, mean reward: 1.985 [1.473, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.335, 10.235], loss: 0.138775, mae: 0.355547, mean_q: 4.404331
 79223/100000: episode: 1360, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 192.341, mean reward: 1.923 [1.448, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.981, 10.169], loss: 1.483390, mae: 0.447789, mean_q: 4.449840
 79323/100000: episode: 1361, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 181.299, mean reward: 1.813 [1.450, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.583, 10.098], loss: 2.748563, mae: 0.520747, mean_q: 4.473027
 79423/100000: episode: 1362, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.166, mean reward: 1.922 [1.463, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.514, 10.098], loss: 1.453601, mae: 0.442668, mean_q: 4.392267
 79523/100000: episode: 1363, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 197.346, mean reward: 1.973 [1.438, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.244, 10.185], loss: 1.444069, mae: 0.428274, mean_q: 4.441212
 79623/100000: episode: 1364, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 182.124, mean reward: 1.821 [1.457, 2.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.605, 10.098], loss: 1.474651, mae: 0.445990, mean_q: 4.455006
 79723/100000: episode: 1365, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 182.269, mean reward: 1.823 [1.439, 2.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.559, 10.110], loss: 0.173399, mae: 0.372049, mean_q: 4.409042
 79823/100000: episode: 1366, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 194.331, mean reward: 1.943 [1.468, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.024, 10.265], loss: 0.189017, mae: 0.379544, mean_q: 4.382481
 79923/100000: episode: 1367, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 181.531, mean reward: 1.815 [1.435, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.816, 10.098], loss: 1.448434, mae: 0.418536, mean_q: 4.415941
 80023/100000: episode: 1368, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 194.393, mean reward: 1.944 [1.457, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.701, 10.098], loss: 0.172045, mae: 0.380604, mean_q: 4.418559
 80123/100000: episode: 1369, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 192.490, mean reward: 1.925 [1.477, 6.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.855, 10.248], loss: 0.151397, mae: 0.362641, mean_q: 4.294920
 80223/100000: episode: 1370, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 209.254, mean reward: 2.093 [1.459, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.914, 10.098], loss: 0.166075, mae: 0.355179, mean_q: 4.376240
 80323/100000: episode: 1371, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 213.684, mean reward: 2.137 [1.463, 4.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.537, 10.577], loss: 1.453911, mae: 0.425388, mean_q: 4.390758
 80423/100000: episode: 1372, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 178.343, mean reward: 1.783 [1.470, 2.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.441, 10.098], loss: 0.156580, mae: 0.360548, mean_q: 4.374758
 80523/100000: episode: 1373, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 186.687, mean reward: 1.867 [1.483, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.750, 10.282], loss: 0.134769, mae: 0.350112, mean_q: 4.359142
 80623/100000: episode: 1374, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 182.162, mean reward: 1.822 [1.465, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.759, 10.098], loss: 1.481189, mae: 0.433382, mean_q: 4.296693
 80723/100000: episode: 1375, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 202.743, mean reward: 2.027 [1.510, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.298, 10.098], loss: 1.458384, mae: 0.418458, mean_q: 4.320370
 80823/100000: episode: 1376, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 200.119, mean reward: 2.001 [1.462, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.157, 10.417], loss: 1.442961, mae: 0.416330, mean_q: 4.298587
 80923/100000: episode: 1377, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 193.189, mean reward: 1.932 [1.451, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.811, 10.297], loss: 3.946399, mae: 0.525928, mean_q: 4.414300
 81023/100000: episode: 1378, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 207.552, mean reward: 2.076 [1.434, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.095, 10.098], loss: 2.681553, mae: 0.532117, mean_q: 4.300343
 81123/100000: episode: 1379, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 206.777, mean reward: 2.068 [1.474, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.910, 10.098], loss: 0.145743, mae: 0.355521, mean_q: 4.224522
 81223/100000: episode: 1380, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 188.224, mean reward: 1.882 [1.481, 5.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.722, 10.169], loss: 0.121874, mae: 0.331347, mean_q: 4.204319
 81323/100000: episode: 1381, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 189.295, mean reward: 1.893 [1.451, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.580, 10.098], loss: 0.126861, mae: 0.336341, mean_q: 4.223574
 81423/100000: episode: 1382, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 202.034, mean reward: 2.020 [1.485, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.830, 10.118], loss: 1.425348, mae: 0.410349, mean_q: 4.227195
 81523/100000: episode: 1383, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 190.647, mean reward: 1.906 [1.454, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.823, 10.098], loss: 0.102064, mae: 0.319125, mean_q: 4.136612
 81623/100000: episode: 1384, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 186.825, mean reward: 1.868 [1.442, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.837, 10.098], loss: 0.122298, mae: 0.324424, mean_q: 4.134892
 81723/100000: episode: 1385, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 204.659, mean reward: 2.047 [1.478, 5.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.035, 10.153], loss: 0.106639, mae: 0.319034, mean_q: 4.144884
 81823/100000: episode: 1386, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 182.330, mean reward: 1.823 [1.468, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.325, 10.136], loss: 0.113677, mae: 0.315987, mean_q: 4.091993
 81923/100000: episode: 1387, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 213.685, mean reward: 2.137 [1.457, 4.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.485, 10.475], loss: 0.109626, mae: 0.312202, mean_q: 4.064729
 82023/100000: episode: 1388, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 197.741, mean reward: 1.977 [1.455, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.756, 10.294], loss: 1.401386, mae: 0.386555, mean_q: 4.080529
 82123/100000: episode: 1389, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 188.256, mean reward: 1.883 [1.468, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.521, 10.098], loss: 1.347619, mae: 0.337612, mean_q: 4.026073
 82223/100000: episode: 1390, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 192.445, mean reward: 1.924 [1.434, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.125, 10.106], loss: 0.131323, mae: 0.337008, mean_q: 3.975201
 82323/100000: episode: 1391, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 181.581, mean reward: 1.816 [1.444, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.697, 10.130], loss: 0.084839, mae: 0.289220, mean_q: 3.972532
 82423/100000: episode: 1392, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.400, mean reward: 1.814 [1.446, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.774, 10.168], loss: 2.609828, mae: 0.416815, mean_q: 3.998835
 82523/100000: episode: 1393, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 181.828, mean reward: 1.818 [1.454, 2.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.555, 10.098], loss: 2.522398, mae: 0.409549, mean_q: 3.990087
 82623/100000: episode: 1394, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 211.830, mean reward: 2.118 [1.485, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.739, 10.370], loss: 1.321029, mae: 0.371428, mean_q: 3.992437
 82723/100000: episode: 1395, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 182.800, mean reward: 1.828 [1.468, 2.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.855, 10.098], loss: 0.104226, mae: 0.298810, mean_q: 3.942727
 82823/100000: episode: 1396, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 195.133, mean reward: 1.951 [1.464, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.130, 10.098], loss: 1.288007, mae: 0.342859, mean_q: 3.909498
 82923/100000: episode: 1397, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 187.938, mean reward: 1.879 [1.451, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.925, 10.098], loss: 0.078848, mae: 0.279283, mean_q: 3.848219
 83023/100000: episode: 1398, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 189.846, mean reward: 1.898 [1.480, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.748, 10.397], loss: 0.075717, mae: 0.273751, mean_q: 3.852421
 83123/100000: episode: 1399, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 222.162, mean reward: 2.222 [1.508, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.712, 10.411], loss: 0.074946, mae: 0.271666, mean_q: 3.855617
 83223/100000: episode: 1400, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 197.702, mean reward: 1.977 [1.500, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.480, 10.451], loss: 0.069019, mae: 0.262153, mean_q: 3.841720
 83323/100000: episode: 1401, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.373, mean reward: 1.904 [1.476, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.839, 10.215], loss: 0.067523, mae: 0.265061, mean_q: 3.826541
 83423/100000: episode: 1402, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 192.337, mean reward: 1.923 [1.451, 5.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.611, 10.098], loss: 0.073536, mae: 0.273385, mean_q: 3.829881
 83523/100000: episode: 1403, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 182.603, mean reward: 1.826 [1.462, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.241, 10.098], loss: 0.080258, mae: 0.268272, mean_q: 3.835054
 83623/100000: episode: 1404, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.213, mean reward: 1.962 [1.478, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.440, 10.267], loss: 0.070970, mae: 0.267365, mean_q: 3.823403
 83723/100000: episode: 1405, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.779, mean reward: 1.888 [1.511, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.336, 10.098], loss: 0.069979, mae: 0.268592, mean_q: 3.826676
 83823/100000: episode: 1406, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 202.197, mean reward: 2.022 [1.442, 9.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.990, 10.098], loss: 0.065328, mae: 0.264654, mean_q: 3.817530
 83923/100000: episode: 1407, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 193.106, mean reward: 1.931 [1.441, 5.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.517, 10.176], loss: 0.087666, mae: 0.272946, mean_q: 3.849199
 84023/100000: episode: 1408, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 193.006, mean reward: 1.930 [1.470, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.703, 10.270], loss: 0.077151, mae: 0.274658, mean_q: 3.841059
 84123/100000: episode: 1409, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.874, mean reward: 1.869 [1.509, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.379, 10.098], loss: 0.071441, mae: 0.272069, mean_q: 3.833651
 84223/100000: episode: 1410, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 182.261, mean reward: 1.823 [1.437, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.017, 10.257], loss: 0.072839, mae: 0.267842, mean_q: 3.817324
 84323/100000: episode: 1411, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 185.406, mean reward: 1.854 [1.439, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.752, 10.098], loss: 0.071563, mae: 0.266514, mean_q: 3.822783
 84423/100000: episode: 1412, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 185.275, mean reward: 1.853 [1.458, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.158, 10.168], loss: 0.079717, mae: 0.266073, mean_q: 3.828916
 84523/100000: episode: 1413, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 202.037, mean reward: 2.020 [1.462, 4.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.795, 10.098], loss: 0.072861, mae: 0.264843, mean_q: 3.807768
 84623/100000: episode: 1414, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 215.703, mean reward: 2.157 [1.493, 6.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.745, 10.098], loss: 0.076331, mae: 0.263788, mean_q: 3.839426
 84723/100000: episode: 1415, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 217.307, mean reward: 2.173 [1.464, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.685, 10.241], loss: 0.104425, mae: 0.283480, mean_q: 3.842052
 84823/100000: episode: 1416, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 197.030, mean reward: 1.970 [1.469, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.296, 10.277], loss: 0.079073, mae: 0.274566, mean_q: 3.848920
 84923/100000: episode: 1417, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 195.379, mean reward: 1.954 [1.444, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.085, 10.213], loss: 0.083489, mae: 0.286123, mean_q: 3.871207
 85023/100000: episode: 1418, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 196.775, mean reward: 1.968 [1.434, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.741, 10.098], loss: 0.094950, mae: 0.280703, mean_q: 3.853975
 85123/100000: episode: 1419, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 194.900, mean reward: 1.949 [1.488, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.992, 10.098], loss: 0.081870, mae: 0.285133, mean_q: 3.860518
 85223/100000: episode: 1420, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 184.698, mean reward: 1.847 [1.461, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.538, 10.098], loss: 0.091948, mae: 0.286955, mean_q: 3.866549
 85323/100000: episode: 1421, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 188.740, mean reward: 1.887 [1.462, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.646, 10.261], loss: 0.064220, mae: 0.262179, mean_q: 3.829929
 85423/100000: episode: 1422, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 181.097, mean reward: 1.811 [1.431, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.422, 10.098], loss: 0.078369, mae: 0.275029, mean_q: 3.855008
 85523/100000: episode: 1423, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 201.421, mean reward: 2.014 [1.523, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.095, 10.098], loss: 0.085354, mae: 0.276251, mean_q: 3.829735
 85623/100000: episode: 1424, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 209.488, mean reward: 2.095 [1.486, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.227, 10.134], loss: 0.074130, mae: 0.269936, mean_q: 3.843921
 85723/100000: episode: 1425, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 202.711, mean reward: 2.027 [1.500, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.961, 10.193], loss: 0.069980, mae: 0.264764, mean_q: 3.845586
 85823/100000: episode: 1426, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 187.531, mean reward: 1.875 [1.479, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.437, 10.098], loss: 0.095950, mae: 0.283780, mean_q: 3.856330
 85923/100000: episode: 1427, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 193.185, mean reward: 1.932 [1.501, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.430, 10.098], loss: 0.091408, mae: 0.283028, mean_q: 3.859864
 86023/100000: episode: 1428, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 193.642, mean reward: 1.936 [1.506, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.950, 10.252], loss: 0.065549, mae: 0.264655, mean_q: 3.846303
 86123/100000: episode: 1429, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 190.203, mean reward: 1.902 [1.486, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.732, 10.237], loss: 0.065414, mae: 0.262734, mean_q: 3.834447
 86223/100000: episode: 1430, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 209.423, mean reward: 2.094 [1.476, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.408 [-1.470, 10.098], loss: 0.076010, mae: 0.275277, mean_q: 3.845456
 86323/100000: episode: 1431, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 194.596, mean reward: 1.946 [1.468, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.916, 10.152], loss: 0.079810, mae: 0.273140, mean_q: 3.839841
 86423/100000: episode: 1432, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 186.617, mean reward: 1.866 [1.477, 2.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.208, 10.098], loss: 0.079580, mae: 0.273386, mean_q: 3.832948
 86523/100000: episode: 1433, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.591, mean reward: 1.866 [1.509, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.575, 10.098], loss: 0.109168, mae: 0.288906, mean_q: 3.867023
 86623/100000: episode: 1434, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 193.643, mean reward: 1.936 [1.478, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.009, 10.098], loss: 0.072219, mae: 0.258850, mean_q: 3.817466
 86723/100000: episode: 1435, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 189.885, mean reward: 1.899 [1.451, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.614, 10.098], loss: 0.068772, mae: 0.269028, mean_q: 3.833117
 86823/100000: episode: 1436, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 185.961, mean reward: 1.860 [1.448, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.768, 10.098], loss: 0.088782, mae: 0.281772, mean_q: 3.847935
 86923/100000: episode: 1437, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 175.542, mean reward: 1.755 [1.456, 3.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.046, 10.134], loss: 0.078751, mae: 0.271615, mean_q: 3.841868
 87023/100000: episode: 1438, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 233.281, mean reward: 2.333 [1.472, 5.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.632, 10.421], loss: 0.068056, mae: 0.266437, mean_q: 3.802539
 87123/100000: episode: 1439, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 186.799, mean reward: 1.868 [1.487, 2.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.679, 10.098], loss: 0.076394, mae: 0.271424, mean_q: 3.834425
 87223/100000: episode: 1440, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 200.766, mean reward: 2.008 [1.455, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.592, 10.186], loss: 0.080026, mae: 0.279714, mean_q: 3.837648
 87323/100000: episode: 1441, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 193.064, mean reward: 1.931 [1.438, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.477, 10.098], loss: 0.094307, mae: 0.283561, mean_q: 3.839081
 87423/100000: episode: 1442, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 196.375, mean reward: 1.964 [1.507, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.521, 10.098], loss: 0.079485, mae: 0.281814, mean_q: 3.853153
 87523/100000: episode: 1443, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 213.581, mean reward: 2.136 [1.476, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.238, 10.098], loss: 0.066911, mae: 0.261778, mean_q: 3.822737
 87623/100000: episode: 1444, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 196.478, mean reward: 1.965 [1.460, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.995, 10.098], loss: 0.072945, mae: 0.268473, mean_q: 3.852830
 87723/100000: episode: 1445, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 196.871, mean reward: 1.969 [1.460, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.625, 10.098], loss: 0.071163, mae: 0.273340, mean_q: 3.851810
 87823/100000: episode: 1446, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 185.244, mean reward: 1.852 [1.519, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.226, 10.135], loss: 0.104823, mae: 0.283901, mean_q: 3.855589
[Info] 1-TH LEVEL FOUND: 5.01239013671875, Considering 10/90 traces
 87923/100000: episode: 1447, duration: 4.816s, episode steps: 100, steps per second: 21, episode reward: 222.421, mean reward: 2.224 [1.439, 5.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.649, 10.329], loss: 0.079501, mae: 0.277940, mean_q: 3.848190
 87955/100000: episode: 1448, duration: 0.176s, episode steps: 32, steps per second: 181, episode reward: 82.008, mean reward: 2.563 [1.644, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.350, 10.431], loss: 0.074144, mae: 0.271151, mean_q: 3.879696
 87969/100000: episode: 1449, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 38.775, mean reward: 2.770 [2.228, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-1.276, 10.434], loss: 0.087587, mae: 0.284909, mean_q: 3.881026
 88001/100000: episode: 1450, duration: 0.157s, episode steps: 32, steps per second: 203, episode reward: 87.509, mean reward: 2.735 [1.944, 4.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.426, 10.238], loss: 0.097522, mae: 0.289500, mean_q: 3.878273
 88033/100000: episode: 1451, duration: 0.168s, episode steps: 32, steps per second: 191, episode reward: 81.798, mean reward: 2.556 [1.893, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.115, 10.262], loss: 0.086234, mae: 0.286239, mean_q: 3.906350
 88047/100000: episode: 1452, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 42.177, mean reward: 3.013 [2.104, 5.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.727, 10.437], loss: 0.109599, mae: 0.310402, mean_q: 3.929537
 88060/100000: episode: 1453, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 34.476, mean reward: 2.652 [2.325, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.635, 10.374], loss: 0.074211, mae: 0.271803, mean_q: 3.856304
 88065/100000: episode: 1454, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 11.188, mean reward: 2.238 [1.769, 2.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.257], loss: 0.062200, mae: 0.272595, mean_q: 3.911573
 88077/100000: episode: 1455, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 28.437, mean reward: 2.370 [2.097, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.773, 10.100], loss: 0.082397, mae: 0.271009, mean_q: 3.837558
 88123/100000: episode: 1456, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 89.135, mean reward: 1.938 [1.540, 2.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.901, 10.100], loss: 0.089104, mae: 0.289998, mean_q: 3.896623
 88154/100000: episode: 1457, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 70.153, mean reward: 2.263 [1.756, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.056, 10.341], loss: 0.076535, mae: 0.282579, mean_q: 3.859196
 88167/100000: episode: 1458, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 35.769, mean reward: 2.751 [2.112, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.517], loss: 0.069074, mae: 0.254218, mean_q: 3.886638
 88213/100000: episode: 1459, duration: 0.233s, episode steps: 46, steps per second: 197, episode reward: 132.572, mean reward: 2.882 [2.016, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.256, 10.437], loss: 0.085267, mae: 0.282759, mean_q: 3.908188
 88226/100000: episode: 1460, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 33.324, mean reward: 2.563 [1.993, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.360], loss: 0.083366, mae: 0.277067, mean_q: 3.917137
 88239/100000: episode: 1461, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 33.281, mean reward: 2.560 [2.153, 3.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.044, 10.325], loss: 0.091261, mae: 0.305628, mean_q: 3.977022
 88257/100000: episode: 1462, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 55.806, mean reward: 3.100 [2.443, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.448], loss: 0.086122, mae: 0.293579, mean_q: 3.940073
 88270/100000: episode: 1463, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 32.574, mean reward: 2.506 [2.027, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.606, 10.389], loss: 0.098874, mae: 0.305595, mean_q: 3.922883
 88301/100000: episode: 1464, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 92.102, mean reward: 2.971 [2.032, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.063, 10.308], loss: 0.083651, mae: 0.292888, mean_q: 3.934033
 88332/100000: episode: 1465, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 74.956, mean reward: 2.418 [1.678, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.244, 10.261], loss: 0.106491, mae: 0.306905, mean_q: 3.939154
 88345/100000: episode: 1466, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 29.162, mean reward: 2.243 [1.974, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.046, 10.310], loss: 0.112927, mae: 0.315705, mean_q: 3.990262
 88363/100000: episode: 1467, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 45.918, mean reward: 2.551 [1.936, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.414], loss: 0.069310, mae: 0.265919, mean_q: 3.846684
 88375/100000: episode: 1468, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 31.593, mean reward: 2.633 [2.171, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.100, 10.100], loss: 0.085974, mae: 0.311781, mean_q: 3.997264
 88387/100000: episode: 1469, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 28.233, mean reward: 2.353 [1.834, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.507, 10.100], loss: 0.099803, mae: 0.302202, mean_q: 3.933337
 88392/100000: episode: 1470, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 13.047, mean reward: 2.609 [2.337, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.385], loss: 0.083829, mae: 0.297365, mean_q: 3.988056
 88410/100000: episode: 1471, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 44.461, mean reward: 2.470 [1.641, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.234], loss: 0.078611, mae: 0.289746, mean_q: 3.932405
 88456/100000: episode: 1472, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 112.764, mean reward: 2.451 [1.824, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.199, 10.377], loss: 0.076693, mae: 0.279100, mean_q: 3.948391
 88461/100000: episode: 1473, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 13.563, mean reward: 2.713 [2.537, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.440], loss: 0.087514, mae: 0.267659, mean_q: 3.978602
 88475/100000: episode: 1474, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 32.964, mean reward: 2.355 [1.835, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-1.045, 10.361], loss: 0.072980, mae: 0.275807, mean_q: 3.956594
 88488/100000: episode: 1475, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 34.057, mean reward: 2.620 [2.176, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-1.211, 10.369], loss: 0.184597, mae: 0.316089, mean_q: 3.943860
 88520/100000: episode: 1476, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 62.760, mean reward: 1.961 [1.456, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.467, 10.237], loss: 0.083806, mae: 0.282953, mean_q: 3.959894
 88533/100000: episode: 1477, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 33.476, mean reward: 2.575 [2.092, 4.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.517, 10.338], loss: 0.084806, mae: 0.309892, mean_q: 3.996337
 88546/100000: episode: 1478, duration: 0.070s, episode steps: 13, steps per second: 184, episode reward: 37.596, mean reward: 2.892 [2.178, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.395], loss: 0.109052, mae: 0.313603, mean_q: 3.966783
 88558/100000: episode: 1479, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 38.150, mean reward: 3.179 [2.137, 4.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.382, 10.100], loss: 0.071218, mae: 0.265188, mean_q: 3.952970
 88563/100000: episode: 1480, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 12.105, mean reward: 2.421 [2.139, 3.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-1.112, 10.328], loss: 0.072159, mae: 0.288584, mean_q: 3.981129
 88568/100000: episode: 1481, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 11.874, mean reward: 2.375 [2.003, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.314], loss: 0.067040, mae: 0.285234, mean_q: 4.064242
 88582/100000: episode: 1482, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 32.247, mean reward: 2.303 [1.907, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.983, 10.337], loss: 0.166239, mae: 0.325232, mean_q: 3.988657
 88595/100000: episode: 1483, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 29.360, mean reward: 2.258 [1.859, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.274], loss: 0.095241, mae: 0.309696, mean_q: 3.993297
 88613/100000: episode: 1484, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 51.875, mean reward: 2.882 [2.105, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.156, 10.388], loss: 0.101359, mae: 0.307682, mean_q: 3.989476
 88626/100000: episode: 1485, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 35.100, mean reward: 2.700 [1.938, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.434], loss: 0.110624, mae: 0.324384, mean_q: 3.955425
 88639/100000: episode: 1486, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 29.662, mean reward: 2.282 [2.024, 2.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.053, 10.336], loss: 0.105671, mae: 0.336561, mean_q: 4.013229
 88652/100000: episode: 1487, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 34.339, mean reward: 2.641 [2.281, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.339], loss: 0.159430, mae: 0.302969, mean_q: 3.903099
 88666/100000: episode: 1488, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 32.426, mean reward: 2.316 [1.968, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.271], loss: 0.074987, mae: 0.294705, mean_q: 4.032644
 88698/100000: episode: 1489, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 73.354, mean reward: 2.292 [1.633, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.585, 10.190], loss: 0.086286, mae: 0.298924, mean_q: 4.036243
 88716/100000: episode: 1490, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 56.219, mean reward: 3.123 [2.078, 6.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.334], loss: 0.081263, mae: 0.289403, mean_q: 3.996911
 88734/100000: episode: 1491, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 47.261, mean reward: 2.626 [1.950, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.054, 10.365], loss: 0.098312, mae: 0.302570, mean_q: 4.014547
 88739/100000: episode: 1492, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 13.121, mean reward: 2.624 [2.320, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.462], loss: 0.094640, mae: 0.324591, mean_q: 4.109936
 88751/100000: episode: 1493, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 31.469, mean reward: 2.622 [2.135, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.288, 10.100], loss: 0.097778, mae: 0.304296, mean_q: 4.041962
 88782/100000: episode: 1494, duration: 0.166s, episode steps: 31, steps per second: 186, episode reward: 84.588, mean reward: 2.729 [1.742, 7.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.819, 10.239], loss: 0.080361, mae: 0.289740, mean_q: 3.984212
 88800/100000: episode: 1495, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 43.493, mean reward: 2.416 [1.801, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.941, 10.297], loss: 0.103362, mae: 0.317968, mean_q: 4.023270
 88832/100000: episode: 1496, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 71.701, mean reward: 2.241 [1.505, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.281, 10.288], loss: 0.086778, mae: 0.300071, mean_q: 4.015862
 88845/100000: episode: 1497, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 32.287, mean reward: 2.484 [2.078, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.972, 10.427], loss: 0.113357, mae: 0.343846, mean_q: 4.074462
 88860/100000: episode: 1498, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 47.192, mean reward: 3.146 [2.383, 4.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.299, 10.436], loss: 0.104930, mae: 0.335618, mean_q: 4.033709
 88878/100000: episode: 1499, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 61.586, mean reward: 3.421 [2.143, 5.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.397], loss: 0.099288, mae: 0.312267, mean_q: 3.991983
 88924/100000: episode: 1500, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 108.803, mean reward: 2.365 [1.487, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.209, 10.100], loss: 0.107344, mae: 0.324085, mean_q: 4.063453
 88955/100000: episode: 1501, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 78.439, mean reward: 2.530 [1.746, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.197, 10.438], loss: 0.095735, mae: 0.308280, mean_q: 4.049286
 88968/100000: episode: 1502, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 32.045, mean reward: 2.465 [2.181, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.229, 10.416], loss: 0.089279, mae: 0.301353, mean_q: 4.079298
 88986/100000: episode: 1503, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 39.796, mean reward: 2.211 [1.529, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.223], loss: 0.089937, mae: 0.305518, mean_q: 4.019016
 88998/100000: episode: 1504, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 35.316, mean reward: 2.943 [2.097, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.418, 10.100], loss: 0.089884, mae: 0.296488, mean_q: 4.116345
 89030/100000: episode: 1505, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 64.084, mean reward: 2.003 [1.467, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.035, 10.108], loss: 0.091773, mae: 0.313760, mean_q: 4.095367
 89044/100000: episode: 1506, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 30.897, mean reward: 2.207 [1.763, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.284], loss: 0.095192, mae: 0.300764, mean_q: 4.134777
 89058/100000: episode: 1507, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 36.756, mean reward: 2.625 [2.038, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.134, 10.510], loss: 0.092932, mae: 0.313011, mean_q: 4.019634
 89063/100000: episode: 1508, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 11.112, mean reward: 2.222 [1.995, 2.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.359], loss: 0.103111, mae: 0.302299, mean_q: 4.132056
 89094/100000: episode: 1509, duration: 0.163s, episode steps: 31, steps per second: 191, episode reward: 78.683, mean reward: 2.538 [1.900, 4.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-1.065, 10.306], loss: 0.094111, mae: 0.299052, mean_q: 4.068727
 89140/100000: episode: 1510, duration: 0.224s, episode steps: 46, steps per second: 205, episode reward: 108.064, mean reward: 2.349 [1.655, 5.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.588, 10.254], loss: 0.098394, mae: 0.313894, mean_q: 4.063437
 89171/100000: episode: 1511, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 80.855, mean reward: 2.608 [2.039, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.816, 10.419], loss: 0.111206, mae: 0.326473, mean_q: 4.086164
 89185/100000: episode: 1512, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 33.916, mean reward: 2.423 [2.122, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.154, 10.390], loss: 0.096928, mae: 0.306317, mean_q: 4.068825
 89197/100000: episode: 1513, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 35.930, mean reward: 2.994 [2.331, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.382, 10.100], loss: 0.092174, mae: 0.304207, mean_q: 4.148479
 89202/100000: episode: 1514, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 13.303, mean reward: 2.661 [2.158, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.464], loss: 0.056566, mae: 0.242624, mean_q: 4.074252
 89214/100000: episode: 1515, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 28.632, mean reward: 2.386 [1.987, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.282, 10.100], loss: 0.082706, mae: 0.298208, mean_q: 4.170666
 89219/100000: episode: 1516, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 13.658, mean reward: 2.732 [2.532, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.488, 10.361], loss: 0.072295, mae: 0.278728, mean_q: 3.963187
 89224/100000: episode: 1517, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 13.059, mean reward: 2.612 [2.453, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.422], loss: 0.087564, mae: 0.306927, mean_q: 4.184409
 89236/100000: episode: 1518, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 47.619, mean reward: 3.968 [2.182, 9.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.372, 10.100], loss: 0.106106, mae: 0.295921, mean_q: 4.112578
 89241/100000: episode: 1519, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 14.628, mean reward: 2.926 [2.639, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.197, 10.474], loss: 0.089526, mae: 0.311306, mean_q: 4.146679
 89287/100000: episode: 1520, duration: 0.240s, episode steps: 46, steps per second: 191, episode reward: 122.081, mean reward: 2.654 [2.042, 3.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.949, 10.290], loss: 0.123075, mae: 0.335998, mean_q: 4.158355
 89299/100000: episode: 1521, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 35.803, mean reward: 2.984 [2.357, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.360, 10.100], loss: 0.133849, mae: 0.346932, mean_q: 4.163669
 89345/100000: episode: 1522, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 87.881, mean reward: 1.910 [1.490, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.499, 10.190], loss: 0.096515, mae: 0.324700, mean_q: 4.174727
 89358/100000: episode: 1523, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 36.031, mean reward: 2.772 [1.976, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.320], loss: 0.114335, mae: 0.350720, mean_q: 4.217286
 89372/100000: episode: 1524, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 33.442, mean reward: 2.389 [1.889, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.889, 10.366], loss: 0.088551, mae: 0.314277, mean_q: 4.221683
 89377/100000: episode: 1525, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 13.681, mean reward: 2.736 [2.438, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.467], loss: 0.124691, mae: 0.332120, mean_q: 4.233015
 89423/100000: episode: 1526, duration: 0.235s, episode steps: 46, steps per second: 195, episode reward: 95.776, mean reward: 2.082 [1.509, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.269, 10.103], loss: 0.117501, mae: 0.340279, mean_q: 4.193244
 89435/100000: episode: 1527, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 30.007, mean reward: 2.501 [2.213, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.246, 10.100], loss: 0.155955, mae: 0.357768, mean_q: 4.250998
 89466/100000: episode: 1528, duration: 0.170s, episode steps: 31, steps per second: 182, episode reward: 75.129, mean reward: 2.424 [1.631, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.753, 10.100], loss: 0.114999, mae: 0.325314, mean_q: 4.147645
 89481/100000: episode: 1529, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 33.490, mean reward: 2.233 [1.499, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.218, 10.287], loss: 0.112828, mae: 0.336815, mean_q: 4.149909
 89512/100000: episode: 1530, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 81.940, mean reward: 2.643 [1.914, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.366, 10.317], loss: 0.127485, mae: 0.330380, mean_q: 4.177737
 89527/100000: episode: 1531, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 34.333, mean reward: 2.289 [1.818, 2.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.209, 10.250], loss: 0.105372, mae: 0.328434, mean_q: 4.225425
 89540/100000: episode: 1532, duration: 0.066s, episode steps: 13, steps per second: 198, episode reward: 46.189, mean reward: 3.553 [2.464, 5.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.239, 10.564], loss: 0.078508, mae: 0.286873, mean_q: 4.147405
 89554/100000: episode: 1533, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 34.720, mean reward: 2.480 [1.802, 4.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.195], loss: 0.104026, mae: 0.331750, mean_q: 4.198445
 89559/100000: episode: 1534, duration: 0.049s, episode steps: 5, steps per second: 102, episode reward: 13.376, mean reward: 2.675 [2.512, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.295], loss: 0.246678, mae: 0.371578, mean_q: 4.324819
 89573/100000: episode: 1535, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 25.912, mean reward: 1.851 [1.534, 2.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.064, 10.189], loss: 0.089748, mae: 0.298531, mean_q: 4.091973
 89587/100000: episode: 1536, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 37.183, mean reward: 2.656 [1.980, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.474], loss: 0.101261, mae: 0.327515, mean_q: 4.201101
[Info] 2-TH LEVEL FOUND: 5.970310688018799, Considering 10/90 traces
 89618/100000: episode: 1537, duration: 4.421s, episode steps: 31, steps per second: 7, episode reward: 82.503, mean reward: 2.661 [1.870, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.035, 10.546], loss: 0.099080, mae: 0.315818, mean_q: 4.192005
 89642/100000: episode: 1538, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 68.885, mean reward: 2.870 [2.228, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.111, 10.347], loss: 0.114737, mae: 0.333320, mean_q: 4.153316
 89651/100000: episode: 1539, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 25.256, mean reward: 2.806 [2.411, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.231, 10.447], loss: 0.090769, mae: 0.331166, mean_q: 4.106423
 89661/100000: episode: 1540, duration: 0.060s, episode steps: 10, steps per second: 165, episode reward: 28.773, mean reward: 2.877 [2.566, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.560, 10.413], loss: 0.099059, mae: 0.312849, mean_q: 4.264439
 89667/100000: episode: 1541, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 22.793, mean reward: 3.799 [3.178, 5.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.145, 10.580], loss: 0.094471, mae: 0.319405, mean_q: 4.195868
 89682/100000: episode: 1542, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 40.843, mean reward: 2.723 [2.196, 3.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-1.645, 10.421], loss: 0.116108, mae: 0.317554, mean_q: 4.125831
 89700/100000: episode: 1543, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 51.173, mean reward: 2.843 [2.271, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.133, 10.434], loss: 0.073348, mae: 0.277691, mean_q: 4.247184
 89716/100000: episode: 1544, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 40.450, mean reward: 2.528 [1.889, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.390], loss: 0.097165, mae: 0.319924, mean_q: 4.154809
 89742/100000: episode: 1545, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 69.507, mean reward: 2.673 [1.961, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.393, 10.308], loss: 0.096952, mae: 0.318146, mean_q: 4.174238
 89752/100000: episode: 1546, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 41.044, mean reward: 4.104 [2.864, 5.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.526, 10.603], loss: 0.099425, mae: 0.329201, mean_q: 4.143688
 89778/100000: episode: 1547, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 69.745, mean reward: 2.683 [2.206, 3.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.548, 10.377], loss: 0.110131, mae: 0.335291, mean_q: 4.246101
 89802/100000: episode: 1548, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 94.649, mean reward: 3.944 [2.130, 9.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.463, 10.417], loss: 0.111767, mae: 0.336243, mean_q: 4.203228
 89817/100000: episode: 1549, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 42.619, mean reward: 2.841 [1.936, 4.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-1.278, 10.362], loss: 0.107240, mae: 0.327213, mean_q: 4.242513
 89833/100000: episode: 1550, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 55.622, mean reward: 3.476 [2.898, 7.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-1.947, 10.417], loss: 0.170073, mae: 0.344830, mean_q: 4.211006
 89857/100000: episode: 1551, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 58.305, mean reward: 2.429 [1.743, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.425, 10.269], loss: 0.136848, mae: 0.349907, mean_q: 4.291283
 89872/100000: episode: 1552, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 43.082, mean reward: 2.872 [1.929, 4.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.412], loss: 0.091824, mae: 0.306666, mean_q: 4.285179
 89890/100000: episode: 1553, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 45.321, mean reward: 2.518 [2.069, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-1.058, 10.409], loss: 0.111656, mae: 0.330274, mean_q: 4.208303
 89914/100000: episode: 1554, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 83.998, mean reward: 3.500 [2.456, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.417], loss: 0.118150, mae: 0.329706, mean_q: 4.279787
 89940/100000: episode: 1555, duration: 0.128s, episode steps: 26, steps per second: 202, episode reward: 81.785, mean reward: 3.146 [2.011, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.856, 10.333], loss: 0.103055, mae: 0.325552, mean_q: 4.271268
 89950/100000: episode: 1556, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 31.830, mean reward: 3.183 [2.676, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.470, 10.497], loss: 0.073237, mae: 0.282944, mean_q: 4.179958
 89974/100000: episode: 1557, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 63.623, mean reward: 2.651 [1.816, 6.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.264], loss: 0.131310, mae: 0.348280, mean_q: 4.335327
 89983/100000: episode: 1558, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 23.103, mean reward: 2.567 [1.992, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.430], loss: 0.103410, mae: 0.325267, mean_q: 4.210463
 89989/100000: episode: 1559, duration: 0.043s, episode steps: 6, steps per second: 138, episode reward: 40.130, mean reward: 6.688 [3.632, 11.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.160, 10.319], loss: 0.100443, mae: 0.307622, mean_q: 4.188148
 89999/100000: episode: 1560, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 35.715, mean reward: 3.572 [2.794, 4.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.565], loss: 0.187377, mae: 0.358630, mean_q: 4.325863
 90017/100000: episode: 1561, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 42.939, mean reward: 2.385 [2.032, 2.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.130, 10.366], loss: 0.118058, mae: 0.359630, mean_q: 4.309265
 90034/100000: episode: 1562, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 49.763, mean reward: 2.927 [2.530, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.123, 10.470], loss: 0.156710, mae: 0.346079, mean_q: 4.260114
 90051/100000: episode: 1563, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 43.021, mean reward: 2.531 [2.112, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.074, 10.418], loss: 0.110941, mae: 0.357466, mean_q: 4.408882
 90077/100000: episode: 1564, duration: 0.132s, episode steps: 26, steps per second: 198, episode reward: 79.313, mean reward: 3.051 [2.160, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.378], loss: 0.133544, mae: 0.369093, mean_q: 4.316245
 90103/100000: episode: 1565, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 66.676, mean reward: 2.564 [1.901, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.058, 10.363], loss: 0.154709, mae: 0.382767, mean_q: 4.406335
 90120/100000: episode: 1566, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 50.476, mean reward: 2.969 [2.355, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.300, 10.437], loss: 0.203813, mae: 0.405223, mean_q: 4.351384
 90146/100000: episode: 1567, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 84.778, mean reward: 3.261 [2.588, 4.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.481], loss: 0.118693, mae: 0.340579, mean_q: 4.390066
 90164/100000: episode: 1568, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 44.980, mean reward: 2.499 [1.976, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.996, 10.398], loss: 0.229935, mae: 0.370762, mean_q: 4.424854
 90174/100000: episode: 1569, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 33.397, mean reward: 3.340 [3.153, 3.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.412], loss: 0.135394, mae: 0.349362, mean_q: 4.338179
 90191/100000: episode: 1570, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 50.810, mean reward: 2.989 [2.341, 3.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.456], loss: 0.094973, mae: 0.323900, mean_q: 4.359316
 90217/100000: episode: 1571, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 65.403, mean reward: 2.516 [1.985, 3.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.222, 10.347], loss: 0.153585, mae: 0.366517, mean_q: 4.346395
 90232/100000: episode: 1572, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 48.106, mean reward: 3.207 [2.589, 4.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.303, 10.464], loss: 0.182588, mae: 0.381670, mean_q: 4.381437
 90250/100000: episode: 1573, duration: 0.099s, episode steps: 18, steps per second: 183, episode reward: 46.603, mean reward: 2.589 [2.015, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.334], loss: 0.130672, mae: 0.373234, mean_q: 4.402040
 90256/100000: episode: 1574, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 30.232, mean reward: 5.039 [4.321, 6.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.587], loss: 0.115861, mae: 0.336415, mean_q: 4.481212
 90272/100000: episode: 1575, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 54.329, mean reward: 3.396 [1.898, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.313, 10.304], loss: 0.120604, mae: 0.347927, mean_q: 4.373168
 90282/100000: episode: 1576, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 38.171, mean reward: 3.817 [3.131, 5.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.500], loss: 0.128884, mae: 0.356580, mean_q: 4.499202
 90288/100000: episode: 1577, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 20.099, mean reward: 3.350 [2.595, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.461], loss: 0.173113, mae: 0.387175, mean_q: 4.501906
 90304/100000: episode: 1578, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 54.042, mean reward: 3.378 [3.058, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.330, 10.484], loss: 0.122476, mae: 0.351658, mean_q: 4.331381
 90313/100000: episode: 1579, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 40.189, mean reward: 4.465 [2.850, 5.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.158, 10.617], loss: 0.146818, mae: 0.393982, mean_q: 4.466361
 90339/100000: episode: 1580, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 68.305, mean reward: 2.627 [2.280, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.627, 10.411], loss: 0.176163, mae: 0.412449, mean_q: 4.460083
 90355/100000: episode: 1581, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 73.139, mean reward: 4.571 [2.923, 7.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.153, 10.558], loss: 0.143449, mae: 0.381511, mean_q: 4.491970
 90379/100000: episode: 1582, duration: 0.123s, episode steps: 24, steps per second: 194, episode reward: 64.812, mean reward: 2.701 [2.066, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.002, 10.498], loss: 0.143999, mae: 0.381261, mean_q: 4.384665
 90389/100000: episode: 1583, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 30.539, mean reward: 3.054 [2.576, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-1.192, 10.391], loss: 0.154111, mae: 0.378018, mean_q: 4.482959
 90407/100000: episode: 1584, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 44.645, mean reward: 2.480 [2.084, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.395, 10.330], loss: 0.132019, mae: 0.358394, mean_q: 4.527079
 90424/100000: episode: 1585, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 50.630, mean reward: 2.978 [2.433, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.272, 10.448], loss: 0.148948, mae: 0.352734, mean_q: 4.407402
 90440/100000: episode: 1586, duration: 0.079s, episode steps: 16, steps per second: 202, episode reward: 45.428, mean reward: 2.839 [2.021, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.141, 10.350], loss: 0.245969, mae: 0.420995, mean_q: 4.506226
 90446/100000: episode: 1587, duration: 0.040s, episode steps: 6, steps per second: 151, episode reward: 20.088, mean reward: 3.348 [2.981, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.495], loss: 0.173695, mae: 0.408173, mean_q: 4.385184
 90463/100000: episode: 1588, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 59.284, mean reward: 3.487 [2.980, 4.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.439], loss: 0.153170, mae: 0.370931, mean_q: 4.445447
 90473/100000: episode: 1589, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 100.363, mean reward: 10.036 [3.335, 27.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.731], loss: 0.191555, mae: 0.419781, mean_q: 4.504568
 90497/100000: episode: 1590, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 69.932, mean reward: 2.914 [2.127, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.499], loss: 0.129808, mae: 0.359494, mean_q: 4.472792
 90514/100000: episode: 1591, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 42.019, mean reward: 2.472 [1.687, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.292], loss: 0.158302, mae: 0.374465, mean_q: 4.475529
 90542/100000: episode: 1592, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 87.666, mean reward: 3.131 [2.126, 5.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.397, 10.464], loss: 0.277694, mae: 0.416313, mean_q: 4.567048
 90570/100000: episode: 1593, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 92.769, mean reward: 3.313 [2.269, 8.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.125, 10.440], loss: 0.227738, mae: 0.397749, mean_q: 4.488615
 90579/100000: episode: 1594, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 33.152, mean reward: 3.684 [2.462, 5.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.601], loss: 0.208249, mae: 0.422869, mean_q: 4.579813
 90607/100000: episode: 1595, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 73.416, mean reward: 2.622 [2.095, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.672, 10.454], loss: 0.199264, mae: 0.403993, mean_q: 4.564204
 90631/100000: episode: 1596, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 81.007, mean reward: 3.375 [2.310, 4.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.369, 10.469], loss: 0.397172, mae: 0.467151, mean_q: 4.610347
 90646/100000: episode: 1597, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 42.441, mean reward: 2.829 [2.034, 4.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.385], loss: 0.184361, mae: 0.425996, mean_q: 4.538327
 90663/100000: episode: 1598, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 52.485, mean reward: 3.087 [2.515, 4.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.151, 10.442], loss: 0.180505, mae: 0.412068, mean_q: 4.632157
 90689/100000: episode: 1599, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 77.215, mean reward: 2.970 [1.875, 5.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.210, 10.259], loss: 0.300204, mae: 0.437578, mean_q: 4.600466
 90698/100000: episode: 1600, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 33.391, mean reward: 3.710 [2.591, 7.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.204, 10.408], loss: 0.425226, mae: 0.444310, mean_q: 4.639406
 90708/100000: episode: 1601, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 29.872, mean reward: 2.987 [2.686, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.275, 10.445], loss: 0.246379, mae: 0.448767, mean_q: 4.587555
 90724/100000: episode: 1602, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 40.776, mean reward: 2.549 [1.846, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.882, 10.319], loss: 0.158000, mae: 0.404901, mean_q: 4.623623
 90741/100000: episode: 1603, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 45.909, mean reward: 2.701 [2.057, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.565, 10.385], loss: 0.173244, mae: 0.425818, mean_q: 4.703345
 90769/100000: episode: 1604, duration: 0.137s, episode steps: 28, steps per second: 205, episode reward: 104.879, mean reward: 3.746 [2.808, 6.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.820, 10.456], loss: 0.152927, mae: 0.377589, mean_q: 4.563645
 90797/100000: episode: 1605, duration: 0.133s, episode steps: 28, steps per second: 211, episode reward: 63.530, mean reward: 2.269 [1.730, 3.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.493, 10.317], loss: 0.234339, mae: 0.430183, mean_q: 4.691891
 90803/100000: episode: 1606, duration: 0.036s, episode steps: 6, steps per second: 169, episode reward: 22.291, mean reward: 3.715 [3.228, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.055, 10.405], loss: 0.188047, mae: 0.442554, mean_q: 4.666072
 90819/100000: episode: 1607, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 39.626, mean reward: 2.477 [1.917, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.528, 10.250], loss: 0.209120, mae: 0.402131, mean_q: 4.682084
 90843/100000: episode: 1608, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 67.689, mean reward: 2.820 [1.864, 4.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.351], loss: 0.193574, mae: 0.393076, mean_q: 4.659853
 90860/100000: episode: 1609, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 46.926, mean reward: 2.760 [2.228, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.390], loss: 0.131377, mae: 0.373548, mean_q: 4.682622
 90876/100000: episode: 1610, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 36.721, mean reward: 2.295 [1.822, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.467, 10.306], loss: 0.118733, mae: 0.368744, mean_q: 4.620102
 90882/100000: episode: 1611, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 21.422, mean reward: 3.570 [2.979, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.561], loss: 0.152768, mae: 0.378477, mean_q: 4.681196
 90891/100000: episode: 1612, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 30.162, mean reward: 3.351 [2.434, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.477], loss: 0.123971, mae: 0.351617, mean_q: 4.454905
 90907/100000: episode: 1613, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 47.747, mean reward: 2.984 [2.276, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.070, 10.386], loss: 0.190939, mae: 0.409631, mean_q: 4.680501
 90922/100000: episode: 1614, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 70.330, mean reward: 4.689 [2.609, 16.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.799, 10.558], loss: 0.221353, mae: 0.419125, mean_q: 4.664871
 90946/100000: episode: 1615, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 81.682, mean reward: 3.403 [2.473, 6.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.369], loss: 0.769654, mae: 0.535653, mean_q: 4.797213
 90952/100000: episode: 1616, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 20.806, mean reward: 3.468 [3.291, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.124, 10.498], loss: 0.114943, mae: 0.329746, mean_q: 4.634596
 90961/100000: episode: 1617, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 27.074, mean reward: 3.008 [2.778, 3.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-1.054, 10.528], loss: 0.108015, mae: 0.343174, mean_q: 4.620570
 90977/100000: episode: 1618, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 50.260, mean reward: 3.141 [2.805, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.742, 10.418], loss: 0.161557, mae: 0.380153, mean_q: 4.664099
 90986/100000: episode: 1619, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 28.103, mean reward: 3.123 [2.297, 4.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.194, 10.443], loss: 0.211526, mae: 0.460006, mean_q: 4.669185
 90995/100000: episode: 1620, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 21.713, mean reward: 2.413 [1.961, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-1.533, 10.416], loss: 0.208641, mae: 0.436377, mean_q: 4.729736
 91013/100000: episode: 1621, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 47.817, mean reward: 2.656 [2.302, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.159, 10.401], loss: 0.137254, mae: 0.376917, mean_q: 4.717790
 91037/100000: episode: 1622, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 62.893, mean reward: 2.621 [2.080, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.093, 10.337], loss: 0.264980, mae: 0.435120, mean_q: 4.786108
 91055/100000: episode: 1623, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 42.641, mean reward: 2.369 [1.815, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.193, 10.360], loss: 0.295431, mae: 0.451979, mean_q: 4.732879
 91071/100000: episode: 1624, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 58.560, mean reward: 3.660 [2.442, 5.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.073, 10.486], loss: 0.301460, mae: 0.390288, mean_q: 4.594461
 91087/100000: episode: 1625, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 51.329, mean reward: 3.208 [2.133, 6.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.103, 10.368], loss: 0.850160, mae: 0.504335, mean_q: 4.783698
 91115/100000: episode: 1626, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 129.162, mean reward: 4.613 [2.720, 7.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.436, 10.367], loss: 0.165352, mae: 0.389017, mean_q: 4.686511
[Info] 3-TH LEVEL FOUND: 7.9151611328125, Considering 10/90 traces
 91139/100000: episode: 1627, duration: 4.410s, episode steps: 24, steps per second: 5, episode reward: 65.300, mean reward: 2.721 [2.172, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.140, 10.404], loss: 0.423615, mae: 0.466991, mean_q: 4.773382
 91151/100000: episode: 1628, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 49.304, mean reward: 4.109 [3.383, 4.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.581], loss: 0.205613, mae: 0.426082, mean_q: 4.929483
 91155/100000: episode: 1629, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 35.543, mean reward: 8.886 [5.958, 13.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.720], loss: 0.237476, mae: 0.452806, mean_q: 4.521428
 91159/100000: episode: 1630, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 19.190, mean reward: 4.798 [3.468, 7.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.478], loss: 0.126836, mae: 0.373524, mean_q: 4.834310
 91176/100000: episode: 1631, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 80.288, mean reward: 4.723 [3.630, 6.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.368, 10.461], loss: 0.213295, mae: 0.440611, mean_q: 4.768244
 91179/100000: episode: 1632, duration: 0.019s, episode steps: 3, steps per second: 162, episode reward: 35.267, mean reward: 11.756 [8.009, 17.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.379 [-0.035, 10.742], loss: 0.153807, mae: 0.366377, mean_q: 4.695484
 91196/100000: episode: 1633, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 72.735, mean reward: 4.279 [3.168, 5.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.572], loss: 0.224071, mae: 0.442335, mean_q: 4.969269
 91208/100000: episode: 1634, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 41.987, mean reward: 3.499 [2.767, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.043, 10.401], loss: 0.161345, mae: 0.378564, mean_q: 4.866306
 91212/100000: episode: 1635, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 18.298, mean reward: 4.575 [4.224, 5.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.204, 10.444], loss: 0.976333, mae: 0.577068, mean_q: 4.952950
 91216/100000: episode: 1636, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 12.447, mean reward: 3.112 [2.548, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.333], loss: 0.715197, mae: 0.567318, mean_q: 4.948089
 91227/100000: episode: 1637, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 44.168, mean reward: 4.015 [3.149, 5.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.617], loss: 0.231035, mae: 0.468556, mean_q: 4.676141
 91231/100000: episode: 1638, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: 20.149, mean reward: 5.037 [4.621, 5.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.564], loss: 0.207205, mae: 0.491490, mean_q: 5.206967
 91242/100000: episode: 1639, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 32.595, mean reward: 2.963 [2.283, 4.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.408], loss: 0.203604, mae: 0.438469, mean_q: 4.909721
 91253/100000: episode: 1640, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 43.148, mean reward: 3.923 [3.537, 4.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.466], loss: 0.612616, mae: 0.550583, mean_q: 4.978243
 91257/100000: episode: 1641, duration: 0.023s, episode steps: 4, steps per second: 177, episode reward: 24.928, mean reward: 6.232 [5.300, 7.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.613], loss: 0.685308, mae: 0.489969, mean_q: 4.888219
 91261/100000: episode: 1642, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 18.813, mean reward: 4.703 [4.339, 5.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.135, 10.513], loss: 0.270582, mae: 0.452851, mean_q: 4.863722
 91265/100000: episode: 1643, duration: 0.031s, episode steps: 4, steps per second: 128, episode reward: 28.968, mean reward: 7.242 [3.916, 10.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.382], loss: 0.292508, mae: 0.463387, mean_q: 4.847650
 91280/100000: episode: 1644, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 51.488, mean reward: 3.433 [2.395, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.769, 10.417], loss: 0.508485, mae: 0.511711, mean_q: 4.989586
 91284/100000: episode: 1645, duration: 0.024s, episode steps: 4, steps per second: 167, episode reward: 23.324, mean reward: 5.831 [4.284, 7.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.344 [-0.035, 10.627], loss: 0.161027, mae: 0.415476, mean_q: 4.573536
 91288/100000: episode: 1646, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 13.651, mean reward: 3.413 [3.068, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.293], loss: 0.509398, mae: 0.568953, mean_q: 4.995458
 91292/100000: episode: 1647, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 18.946, mean reward: 4.736 [4.042, 5.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.511], loss: 0.246459, mae: 0.492206, mean_q: 5.152889
 91303/100000: episode: 1648, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 36.177, mean reward: 3.289 [2.685, 4.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.472], loss: 1.166070, mae: 0.626462, mean_q: 5.135967
 91307/100000: episode: 1649, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 18.047, mean reward: 4.512 [4.064, 4.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.572], loss: 0.305889, mae: 0.517354, mean_q: 4.714160
 91310/100000: episode: 1650, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: 26.153, mean reward: 8.718 [6.986, 10.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.682], loss: 0.192154, mae: 0.469326, mean_q: 4.659521
 91322/100000: episode: 1651, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 57.654, mean reward: 4.804 [3.170, 6.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.435], loss: 0.349378, mae: 0.544151, mean_q: 5.156028
 91326/100000: episode: 1652, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 16.318, mean reward: 4.079 [3.891, 4.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.526], loss: 0.169456, mae: 0.394694, mean_q: 5.080586
 91330/100000: episode: 1653, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 16.766, mean reward: 4.192 [3.080, 6.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.436], loss: 0.348286, mae: 0.445775, mean_q: 4.949064
 91341/100000: episode: 1654, duration: 0.056s, episode steps: 11, steps per second: 195, episode reward: 55.653, mean reward: 5.059 [3.384, 11.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.283, 10.541], loss: 0.208033, mae: 0.416820, mean_q: 4.865796
 91358/100000: episode: 1655, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 73.376, mean reward: 4.316 [2.582, 6.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.552], loss: 0.194182, mae: 0.413179, mean_q: 4.990348
 91375/100000: episode: 1656, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 49.354, mean reward: 2.903 [2.095, 5.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.210, 10.319], loss: 0.247881, mae: 0.436404, mean_q: 5.001189
 91392/100000: episode: 1657, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 72.468, mean reward: 4.263 [2.670, 6.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.240, 10.421], loss: 0.379973, mae: 0.484279, mean_q: 4.937170
 91404/100000: episode: 1658, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 55.605, mean reward: 4.634 [3.628, 5.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.569], loss: 0.439476, mae: 0.427924, mean_q: 4.812831
 91415/100000: episode: 1659, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 79.929, mean reward: 7.266 [3.012, 14.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.499, 10.567], loss: 0.236694, mae: 0.474262, mean_q: 5.134543
 91419/100000: episode: 1660, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 12.554, mean reward: 3.138 [2.900, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.394], loss: 0.532041, mae: 0.493293, mean_q: 4.951938
 91431/100000: episode: 1661, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 142.164, mean reward: 11.847 [2.885, 62.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.249, 10.641], loss: 0.311216, mae: 0.512271, mean_q: 4.941146
 91443/100000: episode: 1662, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 55.249, mean reward: 4.604 [2.762, 9.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.136, 10.680], loss: 0.400577, mae: 0.479805, mean_q: 5.008947
 91447/100000: episode: 1663, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 18.295, mean reward: 4.574 [4.471, 4.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.911, 10.534], loss: 0.255672, mae: 0.490743, mean_q: 4.959346
 91451/100000: episode: 1664, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 30.318, mean reward: 7.579 [5.372, 11.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.492, 10.522], loss: 0.197207, mae: 0.433090, mean_q: 5.137766
 91463/100000: episode: 1665, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 41.325, mean reward: 3.444 [2.953, 4.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.468, 10.564], loss: 0.453462, mae: 0.557459, mean_q: 5.242529
 91475/100000: episode: 1666, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 48.044, mean reward: 4.004 [2.722, 6.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.435, 10.440], loss: 1.336890, mae: 0.633019, mean_q: 5.223293
 91486/100000: episode: 1667, duration: 0.066s, episode steps: 11, steps per second: 168, episode reward: 48.431, mean reward: 4.403 [2.884, 9.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.334, 10.529], loss: 1.150670, mae: 0.605042, mean_q: 5.129070
 91493/100000: episode: 1668, duration: 0.041s, episode steps: 7, steps per second: 173, episode reward: 38.857, mean reward: 5.551 [3.483, 9.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-1.080, 10.466], loss: 0.425465, mae: 0.517208, mean_q: 5.023323
 91497/100000: episode: 1669, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 13.957, mean reward: 3.489 [3.115, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.497], loss: 0.342732, mae: 0.500518, mean_q: 5.106895
 91508/100000: episode: 1670, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 36.342, mean reward: 3.304 [2.713, 4.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.342], loss: 0.289297, mae: 0.491194, mean_q: 5.126724
 91515/100000: episode: 1671, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 35.946, mean reward: 5.135 [4.182, 9.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.428], loss: 0.265365, mae: 0.462006, mean_q: 4.985567
 91526/100000: episode: 1672, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 57.984, mean reward: 5.271 [3.347, 7.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.662], loss: 0.460053, mae: 0.512798, mean_q: 5.025250
 91529/100000: episode: 1673, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 18.561, mean reward: 6.187 [5.207, 7.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.648], loss: 0.447932, mae: 0.487285, mean_q: 5.061517
 91532/100000: episode: 1674, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 21.276, mean reward: 7.092 [5.261, 8.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.651], loss: 0.624448, mae: 0.699649, mean_q: 5.689266
 91536/100000: episode: 1675, duration: 0.026s, episode steps: 4, steps per second: 154, episode reward: 22.257, mean reward: 5.564 [3.498, 7.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.470], loss: 0.245431, mae: 0.467225, mean_q: 4.916999
 91551/100000: episode: 1676, duration: 0.078s, episode steps: 15, steps per second: 191, episode reward: 51.103, mean reward: 3.407 [2.818, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.398, 10.535], loss: 0.389877, mae: 0.602407, mean_q: 5.116376
 91555/100000: episode: 1677, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 11.370, mean reward: 2.843 [2.719, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-1.114, 10.385], loss: 0.420851, mae: 0.570974, mean_q: 5.465045
 91566/100000: episode: 1678, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 45.410, mean reward: 4.128 [3.249, 5.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-1.619, 10.529], loss: 0.383127, mae: 0.569723, mean_q: 5.198534
 91581/100000: episode: 1679, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 84.335, mean reward: 5.622 [3.414, 10.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.543, 10.550], loss: 0.995681, mae: 0.617608, mean_q: 5.224960
 91592/100000: episode: 1680, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 38.721, mean reward: 3.520 [2.415, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.076, 10.518], loss: 0.343684, mae: 0.537442, mean_q: 5.285435
 91596/100000: episode: 1681, duration: 0.031s, episode steps: 4, steps per second: 129, episode reward: 18.273, mean reward: 4.568 [3.886, 5.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.601], loss: 0.389140, mae: 0.581689, mean_q: 5.595612
 91611/100000: episode: 1682, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 60.083, mean reward: 4.006 [3.161, 5.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.751, 10.502], loss: 0.688179, mae: 0.658239, mean_q: 5.439694
 91623/100000: episode: 1683, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 36.314, mean reward: 3.026 [2.291, 4.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.441], loss: 0.537890, mae: 0.728256, mean_q: 5.230781
 91634/100000: episode: 1684, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 43.492, mean reward: 3.954 [3.202, 4.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.044, 10.518], loss: 0.473157, mae: 0.598011, mean_q: 5.312035
 91641/100000: episode: 1685, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 26.320, mean reward: 3.760 [2.891, 4.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.574, 10.499], loss: 0.579482, mae: 0.644263, mean_q: 5.437122
 91658/100000: episode: 1686, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 77.004, mean reward: 4.530 [2.531, 7.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.485], loss: 0.529201, mae: 0.597419, mean_q: 5.471502
 91669/100000: episode: 1687, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 104.026, mean reward: 9.457 [3.425, 36.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.389, 10.730], loss: 1.295433, mae: 0.703470, mean_q: 5.287673
 91680/100000: episode: 1688, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 39.533, mean reward: 3.594 [2.513, 4.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.694, 10.520], loss: 0.494181, mae: 0.639714, mean_q: 5.238511
 91683/100000: episode: 1689, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 32.944, mean reward: 10.981 [8.222, 12.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.694], loss: 0.479011, mae: 0.649172, mean_q: 5.461772
 91694/100000: episode: 1690, duration: 0.060s, episode steps: 11, steps per second: 184, episode reward: 36.977, mean reward: 3.362 [2.776, 4.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.432], loss: 0.753044, mae: 0.677471, mean_q: 5.453331
 91705/100000: episode: 1691, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 80.701, mean reward: 7.336 [3.686, 14.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.082, 10.715], loss: 0.287767, mae: 0.513377, mean_q: 5.194866
 91716/100000: episode: 1692, duration: 0.057s, episode steps: 11, steps per second: 192, episode reward: 44.641, mean reward: 4.058 [3.230, 4.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.312, 10.490], loss: 0.510497, mae: 0.610953, mean_q: 5.398407
 91720/100000: episode: 1693, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 14.572, mean reward: 3.643 [3.363, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.294], loss: 0.578688, mae: 0.621221, mean_q: 4.853078
 91724/100000: episode: 1694, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 12.972, mean reward: 3.243 [2.948, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.501], loss: 0.400132, mae: 0.659335, mean_q: 5.415262
 91735/100000: episode: 1695, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 37.177, mean reward: 3.380 [2.729, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.529], loss: 0.330316, mae: 0.522451, mean_q: 5.267560
 91739/100000: episode: 1696, duration: 0.029s, episode steps: 4, steps per second: 138, episode reward: 18.887, mean reward: 4.722 [3.328, 7.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.463, 10.331], loss: 1.003505, mae: 0.671342, mean_q: 5.226253
 91742/100000: episode: 1697, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 21.127, mean reward: 7.042 [6.532, 7.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.451, 10.625], loss: 1.549822, mae: 0.909526, mean_q: 5.866187
 91746/100000: episode: 1698, duration: 0.022s, episode steps: 4, steps per second: 179, episode reward: 21.418, mean reward: 5.355 [5.026, 6.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.057, 10.601], loss: 0.327131, mae: 0.569671, mean_q: 5.146504
 91750/100000: episode: 1699, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 14.929, mean reward: 3.732 [3.264, 4.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.456], loss: 0.591178, mae: 0.726852, mean_q: 5.197473
 91754/100000: episode: 1700, duration: 0.032s, episode steps: 4, steps per second: 127, episode reward: 17.035, mean reward: 4.259 [3.888, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.565], loss: 0.725042, mae: 0.735424, mean_q: 5.672729
 91758/100000: episode: 1701, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 16.954, mean reward: 4.239 [3.716, 5.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.562], loss: 0.737523, mae: 0.668108, mean_q: 5.450337
 91769/100000: episode: 1702, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 44.926, mean reward: 4.084 [3.148, 4.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.549], loss: 0.579533, mae: 0.615828, mean_q: 5.253947
 91773/100000: episode: 1703, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 19.579, mean reward: 4.895 [4.607, 5.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.572], loss: 0.272279, mae: 0.537710, mean_q: 5.398646
 91790/100000: episode: 1704, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 68.437, mean reward: 4.026 [2.850, 10.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.118, 10.390], loss: 3.428429, mae: 0.812923, mean_q: 5.607487
 91807/100000: episode: 1705, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 64.201, mean reward: 3.777 [2.538, 8.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.926, 10.381], loss: 0.549939, mae: 0.655427, mean_q: 5.241085
 91824/100000: episode: 1706, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 66.150, mean reward: 3.891 [3.359, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.331, 10.582], loss: 0.484684, mae: 0.653489, mean_q: 5.309997
 91831/100000: episode: 1707, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 33.256, mean reward: 4.751 [3.833, 5.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.552], loss: 0.727467, mae: 0.915837, mean_q: 5.920726
 91835/100000: episode: 1708, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: 16.586, mean reward: 4.146 [4.050, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.484], loss: 1.315530, mae: 0.870850, mean_q: 4.783609
 91847/100000: episode: 1709, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 40.988, mean reward: 3.416 [2.582, 4.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.491], loss: 1.764826, mae: 0.831814, mean_q: 5.296690
 91851/100000: episode: 1710, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 32.927, mean reward: 8.232 [4.057, 12.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.544], loss: 0.908574, mae: 0.964697, mean_q: 6.109863
 91854/100000: episode: 1711, duration: 0.022s, episode steps: 3, steps per second: 138, episode reward: 21.180, mean reward: 7.060 [6.109, 8.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.035, 10.655], loss: 0.388916, mae: 0.578110, mean_q: 5.296314
 91858/100000: episode: 1712, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 14.475, mean reward: 3.619 [3.265, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.363 [-0.035, 10.536], loss: 0.453275, mae: 0.669762, mean_q: 4.822794
 91870/100000: episode: 1713, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 43.131, mean reward: 3.594 [3.014, 5.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.098, 10.568], loss: 0.788054, mae: 0.850249, mean_q: 5.932621
 91882/100000: episode: 1714, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 46.985, mean reward: 3.915 [2.597, 5.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.343, 10.425], loss: 0.504373, mae: 0.592576, mean_q: 5.296176
 91897/100000: episode: 1715, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 79.555, mean reward: 5.304 [3.666, 7.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.507], loss: 1.395517, mae: 0.735847, mean_q: 5.562369
 91901/100000: episode: 1716, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 20.411, mean reward: 5.103 [4.514, 5.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.595], loss: 0.548640, mae: 0.677144, mean_q: 5.327838
[Info] 4-TH LEVEL FOUND: 10.450905799865723, Considering 10/90 traces
 91908/100000: episode: 1717, duration: 4.327s, episode steps: 7, steps per second: 2, episode reward: 29.329, mean reward: 4.190 [3.697, 5.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.563], loss: 1.066361, mae: 0.798922, mean_q: 5.826826
 91915/100000: episode: 1718, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 39.269, mean reward: 5.610 [4.797, 6.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.553], loss: 2.472876, mae: 0.931361, mean_q: 5.716475
 91921/100000: episode: 1719, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 31.460, mean reward: 5.243 [4.423, 7.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.647], loss: 0.806570, mae: 0.651534, mean_q: 5.519577
 91925/100000: episode: 1720, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 20.440, mean reward: 5.110 [3.807, 7.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.657], loss: 0.814240, mae: 0.669156, mean_q: 5.445794
 91932/100000: episode: 1721, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 70.104, mean reward: 10.015 [5.734, 16.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.515], loss: 0.734104, mae: 0.706384, mean_q: 5.883952
 91935/100000: episode: 1722, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 17.643, mean reward: 5.881 [5.559, 6.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.538], loss: 1.531772, mae: 0.936400, mean_q: 6.179106
 91941/100000: episode: 1723, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 36.789, mean reward: 6.132 [5.197, 10.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.568, 10.483], loss: 1.983389, mae: 0.763739, mean_q: 5.266562
 91948/100000: episode: 1724, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 44.082, mean reward: 6.297 [4.031, 9.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.529, 10.672], loss: 0.511553, mae: 0.631190, mean_q: 5.468622
 91949/100000: episode: 1725, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 18.776, mean reward: 18.776 [18.776, 18.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.380 [-0.035, 10.669], loss: 1.316190, mae: 0.789782, mean_q: 5.765267
 91956/100000: episode: 1726, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 29.136, mean reward: 4.162 [3.671, 5.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.494], loss: 0.862347, mae: 0.750841, mean_q: 5.735631
 91959/100000: episode: 1727, duration: 0.025s, episode steps: 3, steps per second: 122, episode reward: 17.958, mean reward: 5.986 [5.611, 6.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.507], loss: 0.575953, mae: 0.737576, mean_q: 5.266427
 91963/100000: episode: 1728, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 12.775, mean reward: 3.194 [3.029, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.354, 10.459], loss: 1.176684, mae: 0.715449, mean_q: 5.435057
 91967/100000: episode: 1729, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 15.158, mean reward: 3.790 [3.396, 4.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.491], loss: 1.228486, mae: 0.836027, mean_q: 5.833341
 91971/100000: episode: 1730, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 18.258, mean reward: 4.565 [4.099, 5.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.644, 10.550], loss: 0.986360, mae: 0.800776, mean_q: 5.863065
 91972/100000: episode: 1731, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 14.522, mean reward: 14.522 [14.522, 14.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.035, 10.645], loss: 0.505119, mae: 0.682332, mean_q: 5.125134
 91979/100000: episode: 1732, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 79.622, mean reward: 11.375 [5.975, 18.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.765, 10.549], loss: 0.539203, mae: 0.674449, mean_q: 5.321650
 91982/100000: episode: 1733, duration: 0.018s, episode steps: 3, steps per second: 163, episode reward: 16.854, mean reward: 5.618 [4.686, 6.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.318, 10.521], loss: 0.654735, mae: 0.686653, mean_q: 5.691292
 91983/100000: episode: 1734, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 4.078, mean reward: 4.078 [4.078, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.404 [-0.035, 10.504], loss: 0.521465, mae: 0.721529, mean_q: 5.517103
 91990/100000: episode: 1735, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 32.565, mean reward: 4.652 [3.222, 7.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.615], loss: 0.580009, mae: 0.679867, mean_q: 5.597266
 91996/100000: episode: 1736, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 26.479, mean reward: 4.413 [3.925, 4.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.531], loss: 0.806516, mae: 0.639911, mean_q: 5.636799
 91997/100000: episode: 1737, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 8.720, mean reward: 8.720 [8.720, 8.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.070, 10.540], loss: 0.221978, mae: 0.498464, mean_q: 5.573453
 91998/100000: episode: 1738, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 8.266, mean reward: 8.266 [8.266, 8.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.070, 10.520], loss: 1.335594, mae: 0.888992, mean_q: 5.565450
 91999/100000: episode: 1739, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 18.581, mean reward: 18.581 [18.581, 18.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.551], loss: 1.720581, mae: 0.827461, mean_q: 5.031519
 92004/100000: episode: 1740, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 57.796, mean reward: 11.559 [4.116, 26.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.565], loss: 1.987342, mae: 0.839048, mean_q: 5.618369
 92011/100000: episode: 1741, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 37.320, mean reward: 5.331 [4.720, 6.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.584], loss: 0.904907, mae: 0.795046, mean_q: 5.999746
 92018/100000: episode: 1742, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 52.498, mean reward: 7.500 [5.446, 11.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.706], loss: 0.729300, mae: 0.697940, mean_q: 5.658803
 92019/100000: episode: 1743, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 6.750, mean reward: 6.750 [6.750, 6.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.385 [-0.070, 10.591], loss: 0.224938, mae: 0.574409, mean_q: 5.502346
 92026/100000: episode: 1744, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 101.074, mean reward: 14.439 [4.298, 28.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.951, 10.658], loss: 1.880395, mae: 0.761086, mean_q: 5.986453
 92031/100000: episode: 1745, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 71.354, mean reward: 14.271 [8.557, 29.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.611], loss: 0.567501, mae: 0.685315, mean_q: 5.732505
 92036/100000: episode: 1746, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 36.687, mean reward: 7.337 [5.397, 9.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.687], loss: 0.557080, mae: 0.644859, mean_q: 5.243095
 92040/100000: episode: 1747, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 18.358, mean reward: 4.589 [3.838, 5.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.529], loss: 0.924184, mae: 0.862579, mean_q: 5.876740
 92046/100000: episode: 1748, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 37.368, mean reward: 6.228 [3.881, 10.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.410], loss: 3.520210, mae: 1.039594, mean_q: 6.020966
 92050/100000: episode: 1749, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 16.255, mean reward: 4.064 [3.316, 5.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.611], loss: 0.971814, mae: 0.770250, mean_q: 5.505918
[Info] FALSIFICATION!
[Info] Levels: [5.01239, 5.9703107, 7.915161, 10.450906, 10.870684]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.27]
[Info] Error Prob: 2.700000000000001e-05

 92051/100000: episode: 1750, duration: 4.623s, episode steps: 1, steps per second: 0, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.088, 10.536], loss: 0.789741, mae: 0.827429, mean_q: 6.018809
 92151/100000: episode: 1751, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 196.722, mean reward: 1.967 [1.491, 4.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.222, 10.098], loss: 2.343172, mae: 0.817083, mean_q: 5.737883
 92251/100000: episode: 1752, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 185.924, mean reward: 1.859 [1.441, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.002, 10.098], loss: 2.550308, mae: 0.899982, mean_q: 5.749304
 92351/100000: episode: 1753, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 193.071, mean reward: 1.931 [1.457, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.064, 10.098], loss: 4.210648, mae: 0.947162, mean_q: 5.784561
 92451/100000: episode: 1754, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 212.262, mean reward: 2.123 [1.469, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.708, 10.098], loss: 4.106564, mae: 0.976267, mean_q: 5.717205
 92551/100000: episode: 1755, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 192.262, mean reward: 1.923 [1.450, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.621, 10.103], loss: 3.754185, mae: 0.922546, mean_q: 5.811367
 92651/100000: episode: 1756, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 208.003, mean reward: 2.080 [1.467, 5.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.371, 10.373], loss: 3.645934, mae: 0.869878, mean_q: 5.896929
 92751/100000: episode: 1757, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 201.760, mean reward: 2.018 [1.454, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.823, 10.098], loss: 3.254233, mae: 0.888406, mean_q: 5.799664
 92851/100000: episode: 1758, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 180.831, mean reward: 1.808 [1.462, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.625, 10.098], loss: 1.408408, mae: 0.775239, mean_q: 5.659098
 92951/100000: episode: 1759, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 180.406, mean reward: 1.804 [1.445, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.071, 10.168], loss: 2.338648, mae: 0.824707, mean_q: 5.694678
 93051/100000: episode: 1760, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 186.165, mean reward: 1.862 [1.463, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.865, 10.098], loss: 5.602133, mae: 0.965192, mean_q: 5.756313
 93151/100000: episode: 1761, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 201.558, mean reward: 2.016 [1.484, 4.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.980, 10.098], loss: 3.025879, mae: 0.853861, mean_q: 5.718447
 93251/100000: episode: 1762, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 224.688, mean reward: 2.247 [1.504, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.322, 10.439], loss: 1.277972, mae: 0.748637, mean_q: 5.675128
 93351/100000: episode: 1763, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 203.043, mean reward: 2.030 [1.471, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.086, 10.400], loss: 1.088246, mae: 0.727774, mean_q: 5.632346
 93451/100000: episode: 1764, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 197.313, mean reward: 1.973 [1.444, 5.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.808, 10.132], loss: 1.186520, mae: 0.727079, mean_q: 5.536865
 93551/100000: episode: 1765, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 200.982, mean reward: 2.010 [1.441, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.325, 10.098], loss: 5.022327, mae: 0.858785, mean_q: 5.629279
 93651/100000: episode: 1766, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 186.016, mean reward: 1.860 [1.456, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.143, 10.098], loss: 0.834259, mae: 0.691432, mean_q: 5.567888
 93751/100000: episode: 1767, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 212.197, mean reward: 2.122 [1.482, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.816, 10.483], loss: 3.055175, mae: 0.924630, mean_q: 5.574197
 93851/100000: episode: 1768, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 181.696, mean reward: 1.817 [1.452, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.490, 10.098], loss: 1.579459, mae: 0.772181, mean_q: 5.623574
 93951/100000: episode: 1769, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.675, mean reward: 1.877 [1.447, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.758, 10.140], loss: 1.106340, mae: 0.709110, mean_q: 5.543112
 94051/100000: episode: 1770, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 217.285, mean reward: 2.173 [1.502, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.597, 10.339], loss: 1.147258, mae: 0.699965, mean_q: 5.529997
 94151/100000: episode: 1771, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 182.683, mean reward: 1.827 [1.446, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.688, 10.098], loss: 0.648115, mae: 0.619460, mean_q: 5.381467
 94251/100000: episode: 1772, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 226.510, mean reward: 2.265 [1.482, 4.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.817, 10.402], loss: 1.219146, mae: 0.669496, mean_q: 5.348367
 94351/100000: episode: 1773, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 203.572, mean reward: 2.036 [1.491, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.857, 10.140], loss: 1.585208, mae: 0.766350, mean_q: 5.566731
 94451/100000: episode: 1774, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 178.344, mean reward: 1.783 [1.454, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.645, 10.116], loss: 1.458152, mae: 0.681789, mean_q: 5.276138
 94551/100000: episode: 1775, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 198.059, mean reward: 1.981 [1.474, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.855, 10.234], loss: 1.469968, mae: 0.714901, mean_q: 5.387425
 94651/100000: episode: 1776, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 184.067, mean reward: 1.841 [1.501, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.860, 10.098], loss: 0.865632, mae: 0.641688, mean_q: 5.323261
 94751/100000: episode: 1777, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.319, mean reward: 1.913 [1.441, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.907, 10.205], loss: 1.023003, mae: 0.638514, mean_q: 5.285958
 94851/100000: episode: 1778, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 216.915, mean reward: 2.169 [1.496, 4.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.565, 10.098], loss: 1.071195, mae: 0.675260, mean_q: 5.316824
 94951/100000: episode: 1779, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 188.931, mean reward: 1.889 [1.475, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.715, 10.251], loss: 3.037049, mae: 0.776378, mean_q: 5.254780
 95051/100000: episode: 1780, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 192.822, mean reward: 1.928 [1.473, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.178, 10.295], loss: 3.007915, mae: 0.794796, mean_q: 5.281803
 95151/100000: episode: 1781, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 197.654, mean reward: 1.977 [1.471, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.211, 10.098], loss: 1.725596, mae: 0.770316, mean_q: 5.194449
 95251/100000: episode: 1782, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 185.385, mean reward: 1.854 [1.501, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.567, 10.098], loss: 3.951677, mae: 0.767001, mean_q: 5.199713
 95351/100000: episode: 1783, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 177.612, mean reward: 1.776 [1.444, 2.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.320, 10.109], loss: 0.650979, mae: 0.564204, mean_q: 4.993039
 95451/100000: episode: 1784, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 202.310, mean reward: 2.023 [1.469, 4.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.175, 10.098], loss: 4.340553, mae: 0.819965, mean_q: 5.136420
 95551/100000: episode: 1785, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 189.248, mean reward: 1.892 [1.439, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.910, 10.098], loss: 0.750844, mae: 0.610236, mean_q: 5.052934
 95651/100000: episode: 1786, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 188.006, mean reward: 1.880 [1.506, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.116, 10.171], loss: 3.029435, mae: 0.738636, mean_q: 5.022967
 95751/100000: episode: 1787, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.541, mean reward: 1.885 [1.492, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.661, 10.098], loss: 2.656202, mae: 0.677916, mean_q: 4.932569
 95851/100000: episode: 1788, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 196.667, mean reward: 1.967 [1.484, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.510, 10.197], loss: 2.152457, mae: 0.614742, mean_q: 4.871606
 95951/100000: episode: 1789, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 192.961, mean reward: 1.930 [1.441, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.615, 10.098], loss: 1.523955, mae: 0.600989, mean_q: 4.811016
 96051/100000: episode: 1790, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 247.676, mean reward: 2.477 [1.483, 6.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.213, 10.278], loss: 1.982696, mae: 0.695492, mean_q: 4.909173
 96151/100000: episode: 1791, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.372, mean reward: 1.874 [1.524, 4.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.115, 10.098], loss: 1.341447, mae: 0.586259, mean_q: 4.762733
 96251/100000: episode: 1792, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.497, mean reward: 1.875 [1.490, 2.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.764, 10.098], loss: 0.980894, mae: 0.518592, mean_q: 4.628567
 96351/100000: episode: 1793, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 194.401, mean reward: 1.944 [1.476, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.282, 10.098], loss: 0.515965, mae: 0.492672, mean_q: 4.573883
 96451/100000: episode: 1794, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 195.577, mean reward: 1.956 [1.431, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.244, 10.099], loss: 0.557459, mae: 0.454748, mean_q: 4.450895
 96551/100000: episode: 1795, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 247.389, mean reward: 2.474 [1.453, 6.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.765, 10.098], loss: 3.320603, mae: 0.609508, mean_q: 4.456097
 96651/100000: episode: 1796, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 204.080, mean reward: 2.041 [1.485, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.259, 10.098], loss: 2.289043, mae: 0.573149, mean_q: 4.445639
 96751/100000: episode: 1797, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 181.757, mean reward: 1.818 [1.452, 2.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.975, 10.098], loss: 0.539713, mae: 0.437996, mean_q: 4.269746
 96851/100000: episode: 1798, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 194.252, mean reward: 1.943 [1.484, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.708, 10.384], loss: 0.415923, mae: 0.401574, mean_q: 4.177849
 96951/100000: episode: 1799, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 194.289, mean reward: 1.943 [1.458, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.587, 10.414], loss: 0.271643, mae: 0.347547, mean_q: 4.009550
 97051/100000: episode: 1800, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 192.613, mean reward: 1.926 [1.500, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.911, 10.122], loss: 1.467951, mae: 0.379577, mean_q: 3.939154
 97151/100000: episode: 1801, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 184.866, mean reward: 1.849 [1.476, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.264, 10.098], loss: 0.089938, mae: 0.298623, mean_q: 3.872401
 97251/100000: episode: 1802, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 193.551, mean reward: 1.936 [1.470, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.550, 10.098], loss: 0.091286, mae: 0.299785, mean_q: 3.894896
 97351/100000: episode: 1803, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 228.806, mean reward: 2.288 [1.433, 5.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.103, 10.455], loss: 0.090700, mae: 0.303167, mean_q: 3.889497
 97451/100000: episode: 1804, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 203.191, mean reward: 2.032 [1.510, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.664, 10.472], loss: 0.104640, mae: 0.317534, mean_q: 3.913154
 97551/100000: episode: 1805, duration: 0.480s, episode steps: 100, steps per second: 209, episode reward: 183.558, mean reward: 1.836 [1.444, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.440, 10.124], loss: 0.081553, mae: 0.289407, mean_q: 3.903491
 97651/100000: episode: 1806, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 197.042, mean reward: 1.970 [1.460, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.852, 10.098], loss: 0.084529, mae: 0.295849, mean_q: 3.904913
 97751/100000: episode: 1807, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 232.463, mean reward: 2.325 [1.448, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.637, 10.098], loss: 0.098837, mae: 0.311111, mean_q: 3.927171
 97851/100000: episode: 1808, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 191.284, mean reward: 1.913 [1.435, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.762, 10.164], loss: 0.082285, mae: 0.295336, mean_q: 3.921912
 97951/100000: episode: 1809, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 199.293, mean reward: 1.993 [1.509, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.788, 10.098], loss: 0.093464, mae: 0.304565, mean_q: 3.911952
 98051/100000: episode: 1810, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 202.053, mean reward: 2.021 [1.445, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.941, 10.231], loss: 0.089633, mae: 0.301833, mean_q: 3.912338
 98151/100000: episode: 1811, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 178.409, mean reward: 1.784 [1.482, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.838, 10.257], loss: 0.084101, mae: 0.295672, mean_q: 3.913136
 98251/100000: episode: 1812, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 182.162, mean reward: 1.822 [1.453, 2.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.280, 10.098], loss: 0.103440, mae: 0.313400, mean_q: 3.930875
 98351/100000: episode: 1813, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 195.121, mean reward: 1.951 [1.450, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.667, 10.098], loss: 0.095890, mae: 0.302597, mean_q: 3.905128
 98451/100000: episode: 1814, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 203.047, mean reward: 2.030 [1.463, 5.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.098, 10.206], loss: 0.086989, mae: 0.292589, mean_q: 3.902515
 98551/100000: episode: 1815, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 185.721, mean reward: 1.857 [1.487, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.758, 10.193], loss: 0.099230, mae: 0.312002, mean_q: 3.929262
 98651/100000: episode: 1816, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 193.771, mean reward: 1.938 [1.466, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.430, 10.146], loss: 0.096939, mae: 0.305134, mean_q: 3.915647
 98751/100000: episode: 1817, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 221.891, mean reward: 2.219 [1.442, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.889, 10.455], loss: 0.082954, mae: 0.295007, mean_q: 3.894116
 98851/100000: episode: 1818, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 181.182, mean reward: 1.812 [1.444, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.009, 10.168], loss: 0.089417, mae: 0.300737, mean_q: 3.926340
 98951/100000: episode: 1819, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 206.937, mean reward: 2.069 [1.495, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.775, 10.416], loss: 0.094686, mae: 0.307703, mean_q: 3.917377
 99051/100000: episode: 1820, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 190.324, mean reward: 1.903 [1.436, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.953, 10.324], loss: 0.085480, mae: 0.294346, mean_q: 3.897799
 99151/100000: episode: 1821, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.611, mean reward: 1.826 [1.440, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.351, 10.131], loss: 0.090033, mae: 0.294848, mean_q: 3.907182
 99251/100000: episode: 1822, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 185.543, mean reward: 1.855 [1.464, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.179, 10.388], loss: 0.097234, mae: 0.309316, mean_q: 3.894840
 99351/100000: episode: 1823, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 188.816, mean reward: 1.888 [1.443, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.069, 10.268], loss: 0.093208, mae: 0.296942, mean_q: 3.893993
 99451/100000: episode: 1824, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 188.980, mean reward: 1.890 [1.450, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.743, 10.233], loss: 0.092332, mae: 0.289075, mean_q: 3.908032
 99551/100000: episode: 1825, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 199.133, mean reward: 1.991 [1.483, 11.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.975, 10.142], loss: 0.110813, mae: 0.303503, mean_q: 3.888932
 99651/100000: episode: 1826, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 195.024, mean reward: 1.950 [1.484, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.355, 10.098], loss: 0.090568, mae: 0.296892, mean_q: 3.883075
 99751/100000: episode: 1827, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 194.942, mean reward: 1.949 [1.437, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.209, 10.098], loss: 0.110428, mae: 0.306423, mean_q: 3.904128
 99851/100000: episode: 1828, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 187.493, mean reward: 1.875 [1.457, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.053, 10.101], loss: 0.104254, mae: 0.305362, mean_q: 3.894294
 99951/100000: episode: 1829, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.263, mean reward: 1.953 [1.485, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.179, 10.334], loss: 0.091616, mae: 0.298290, mean_q: 3.883131
done, took 576.501 seconds
[Info] End Importance Splitting. Falsification occurred 5 times.
