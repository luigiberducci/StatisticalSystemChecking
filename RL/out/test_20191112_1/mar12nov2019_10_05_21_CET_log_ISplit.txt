Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 576, episode reward: 208.740, mean reward: 2.087 [1.572, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.995, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.063s, episode steps: 100, steps per second: 1593, episode reward: 188.137, mean reward: 1.881 [1.437, 2.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.373, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.063s, episode steps: 100, steps per second: 1589, episode reward: 191.776, mean reward: 1.918 [1.435, 2.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.183, 10.146], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.070s, episode steps: 100, steps per second: 1435, episode reward: 210.604, mean reward: 2.106 [1.516, 3.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.171, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.063s, episode steps: 100, steps per second: 1591, episode reward: 230.388, mean reward: 2.304 [1.497, 5.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.094, 10.551], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 205.899, mean reward: 2.059 [1.507, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.785, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.067s, episode steps: 100, steps per second: 1498, episode reward: 199.177, mean reward: 1.992 [1.448, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.846, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.067s, episode steps: 100, steps per second: 1497, episode reward: 192.803, mean reward: 1.928 [1.543, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.518, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.060s, episode steps: 100, steps per second: 1668, episode reward: 182.981, mean reward: 1.830 [1.477, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.172, 10.363], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.061s, episode steps: 100, steps per second: 1647, episode reward: 191.050, mean reward: 1.911 [1.443, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.842, 10.111], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.061s, episode steps: 100, steps per second: 1653, episode reward: 194.099, mean reward: 1.941 [1.455, 4.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.991, 10.187], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 188.669, mean reward: 1.887 [1.468, 2.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.482, 10.322], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.060s, episode steps: 100, steps per second: 1658, episode reward: 186.640, mean reward: 1.866 [1.476, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.463, 10.274], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 197.572, mean reward: 1.976 [1.456, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.624, 10.135], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.062s, episode steps: 100, steps per second: 1600, episode reward: 201.782, mean reward: 2.018 [1.486, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.357, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.061s, episode steps: 100, steps per second: 1648, episode reward: 190.764, mean reward: 1.908 [1.435, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.075, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.066s, episode steps: 100, steps per second: 1521, episode reward: 189.405, mean reward: 1.894 [1.437, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.616, 10.153], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.061s, episode steps: 100, steps per second: 1652, episode reward: 198.593, mean reward: 1.986 [1.439, 4.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.335, 10.306], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 207.234, mean reward: 2.072 [1.446, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.421, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.067s, episode steps: 100, steps per second: 1482, episode reward: 193.435, mean reward: 1.934 [1.458, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-1.030, 10.310], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 193.033, mean reward: 1.930 [1.492, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.634, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.061s, episode steps: 100, steps per second: 1644, episode reward: 191.807, mean reward: 1.918 [1.457, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.112, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.061s, episode steps: 100, steps per second: 1651, episode reward: 184.228, mean reward: 1.842 [1.457, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.557, 10.230], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.061s, episode steps: 100, steps per second: 1652, episode reward: 185.398, mean reward: 1.854 [1.449, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.186, 10.118], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.063s, episode steps: 100, steps per second: 1599, episode reward: 185.378, mean reward: 1.854 [1.479, 4.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.121, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.060s, episode steps: 100, steps per second: 1655, episode reward: 196.661, mean reward: 1.967 [1.482, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.258, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 186.022, mean reward: 1.860 [1.453, 2.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.712, 10.294], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 195.136, mean reward: 1.951 [1.443, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.902, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 185.914, mean reward: 1.859 [1.463, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.516, 10.098], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.071s, episode steps: 100, steps per second: 1418, episode reward: 176.189, mean reward: 1.762 [1.444, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.175, 10.152], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.084s, episode steps: 100, steps per second: 1195, episode reward: 194.517, mean reward: 1.945 [1.493, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.305, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.061s, episode steps: 100, steps per second: 1653, episode reward: 204.036, mean reward: 2.040 [1.508, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.801, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 199.547, mean reward: 1.995 [1.471, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.671, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.061s, episode steps: 100, steps per second: 1627, episode reward: 193.353, mean reward: 1.934 [1.444, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.902, 10.261], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.067s, episode steps: 100, steps per second: 1496, episode reward: 182.520, mean reward: 1.825 [1.432, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.060, 10.116], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.061s, episode steps: 100, steps per second: 1644, episode reward: 186.065, mean reward: 1.861 [1.489, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.932, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.061s, episode steps: 100, steps per second: 1651, episode reward: 212.713, mean reward: 2.127 [1.457, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.336, 10.301], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 189.274, mean reward: 1.893 [1.435, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.877, 10.161], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.061s, episode steps: 100, steps per second: 1649, episode reward: 189.935, mean reward: 1.899 [1.469, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.178, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.061s, episode steps: 100, steps per second: 1647, episode reward: 191.062, mean reward: 1.911 [1.476, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.903, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 181.358, mean reward: 1.814 [1.431, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.937, 10.103], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.066s, episode steps: 100, steps per second: 1520, episode reward: 200.449, mean reward: 2.004 [1.470, 9.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.090, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 183.483, mean reward: 1.835 [1.465, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.340, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.061s, episode steps: 100, steps per second: 1630, episode reward: 187.440, mean reward: 1.874 [1.470, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.655, 10.161], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.061s, episode steps: 100, steps per second: 1648, episode reward: 180.413, mean reward: 1.804 [1.442, 2.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.375, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 188.100, mean reward: 1.881 [1.468, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.285, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 180.410, mean reward: 1.804 [1.445, 2.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.619, 10.215], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.061s, episode steps: 100, steps per second: 1652, episode reward: 211.192, mean reward: 2.112 [1.461, 3.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.920, 10.098], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.068s, episode steps: 100, steps per second: 1476, episode reward: 192.192, mean reward: 1.922 [1.454, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.278, 10.173], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.061s, episode steps: 100, steps per second: 1644, episode reward: 195.402, mean reward: 1.954 [1.484, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.659, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.267s, episode steps: 100, steps per second: 79, episode reward: 180.161, mean reward: 1.802 [1.451, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.362, 10.175], loss: 0.245424, mae: 0.493171, mean_q: 2.109727
  5200/100000: episode: 52, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 207.418, mean reward: 2.074 [1.493, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.765, 10.151], loss: 0.099229, mae: 0.309654, mean_q: 2.810466
  5300/100000: episode: 53, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 242.251, mean reward: 2.423 [1.476, 5.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.842, 10.184], loss: 0.101033, mae: 0.308520, mean_q: 3.176283
  5400/100000: episode: 54, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 209.735, mean reward: 2.097 [1.452, 7.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.722, 10.098], loss: 0.098487, mae: 0.304086, mean_q: 3.418130
  5500/100000: episode: 55, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 206.406, mean reward: 2.064 [1.475, 4.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.418, 10.268], loss: 0.104658, mae: 0.298069, mean_q: 3.580917
  5600/100000: episode: 56, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 190.929, mean reward: 1.909 [1.492, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.973, 10.117], loss: 0.116402, mae: 0.314540, mean_q: 3.668665
  5700/100000: episode: 57, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 208.621, mean reward: 2.086 [1.461, 7.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.902, 10.338], loss: 0.096433, mae: 0.296540, mean_q: 3.741737
  5800/100000: episode: 58, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 198.614, mean reward: 1.986 [1.459, 4.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.559, 10.098], loss: 0.132554, mae: 0.314723, mean_q: 3.784075
  5900/100000: episode: 59, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 196.195, mean reward: 1.962 [1.532, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.989, 10.163], loss: 0.112716, mae: 0.302859, mean_q: 3.791803
  6000/100000: episode: 60, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.357, mean reward: 1.844 [1.470, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.708, 10.229], loss: 0.105733, mae: 0.313187, mean_q: 3.803839
  6100/100000: episode: 61, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 186.165, mean reward: 1.862 [1.456, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.365, 10.206], loss: 0.111087, mae: 0.310681, mean_q: 3.838970
  6200/100000: episode: 62, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 223.351, mean reward: 2.234 [1.445, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.932, 10.098], loss: 0.098781, mae: 0.292788, mean_q: 3.812018
  6300/100000: episode: 63, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 183.998, mean reward: 1.840 [1.463, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.942, 10.218], loss: 0.124374, mae: 0.319537, mean_q: 3.852726
  6400/100000: episode: 64, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 205.895, mean reward: 2.059 [1.501, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.557, 10.347], loss: 0.099457, mae: 0.293523, mean_q: 3.817303
  6500/100000: episode: 65, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 203.148, mean reward: 2.031 [1.436, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.329, 10.326], loss: 0.113637, mae: 0.312159, mean_q: 3.838752
  6600/100000: episode: 66, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 199.094, mean reward: 1.991 [1.480, 5.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.653, 10.312], loss: 0.111726, mae: 0.317951, mean_q: 3.833582
  6700/100000: episode: 67, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 212.036, mean reward: 2.120 [1.474, 6.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.335, 10.298], loss: 0.134668, mae: 0.328700, mean_q: 3.845830
  6800/100000: episode: 68, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.635, mean reward: 1.816 [1.452, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.574, 10.187], loss: 0.121221, mae: 0.316275, mean_q: 3.857098
  6900/100000: episode: 69, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 201.346, mean reward: 2.013 [1.472, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.894, 10.151], loss: 0.095884, mae: 0.298640, mean_q: 3.849134
  7000/100000: episode: 70, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 195.214, mean reward: 1.952 [1.432, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.488, 10.098], loss: 0.140405, mae: 0.328013, mean_q: 3.868661
  7100/100000: episode: 71, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.523, mean reward: 1.925 [1.442, 4.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.572, 10.098], loss: 0.114047, mae: 0.310155, mean_q: 3.848154
  7200/100000: episode: 72, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 188.672, mean reward: 1.887 [1.491, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.253, 10.198], loss: 0.118689, mae: 0.305415, mean_q: 3.849967
  7300/100000: episode: 73, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.927, mean reward: 1.899 [1.454, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.310, 10.098], loss: 0.112902, mae: 0.310397, mean_q: 3.859707
  7400/100000: episode: 74, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 184.178, mean reward: 1.842 [1.454, 3.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.310, 10.183], loss: 0.139472, mae: 0.320326, mean_q: 3.863680
  7500/100000: episode: 75, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.064, mean reward: 1.951 [1.452, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.555, 10.191], loss: 0.113621, mae: 0.321541, mean_q: 3.866452
  7600/100000: episode: 76, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 196.788, mean reward: 1.968 [1.447, 8.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.098, 10.171], loss: 0.139275, mae: 0.323657, mean_q: 3.864479
  7700/100000: episode: 77, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 215.391, mean reward: 2.154 [1.530, 4.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.503, 10.170], loss: 0.145343, mae: 0.323101, mean_q: 3.854500
  7800/100000: episode: 78, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 177.496, mean reward: 1.775 [1.448, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.494, 10.098], loss: 0.119766, mae: 0.312029, mean_q: 3.860824
  7900/100000: episode: 79, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.191, mean reward: 1.932 [1.472, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.551, 10.098], loss: 0.119186, mae: 0.322342, mean_q: 3.871397
  8000/100000: episode: 80, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 207.546, mean reward: 2.075 [1.531, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.694, 10.098], loss: 0.125549, mae: 0.317536, mean_q: 3.869021
  8100/100000: episode: 81, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 193.592, mean reward: 1.936 [1.463, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.304, 10.098], loss: 0.101923, mae: 0.311095, mean_q: 3.873513
  8200/100000: episode: 82, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 182.590, mean reward: 1.826 [1.458, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.375, 10.174], loss: 0.131146, mae: 0.327961, mean_q: 3.881901
  8300/100000: episode: 83, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 194.555, mean reward: 1.946 [1.468, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.951, 10.262], loss: 0.127740, mae: 0.318811, mean_q: 3.867759
  8400/100000: episode: 84, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.256, mean reward: 1.923 [1.486, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.014, 10.098], loss: 0.144245, mae: 0.335750, mean_q: 3.873131
  8500/100000: episode: 85, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 207.168, mean reward: 2.072 [1.456, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.835, 10.098], loss: 0.129463, mae: 0.310890, mean_q: 3.858358
  8600/100000: episode: 86, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 220.643, mean reward: 2.206 [1.632, 4.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.406, 10.098], loss: 0.147609, mae: 0.337278, mean_q: 3.874885
  8700/100000: episode: 87, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 181.140, mean reward: 1.811 [1.459, 2.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.077, 10.208], loss: 0.128322, mae: 0.329360, mean_q: 3.875677
  8800/100000: episode: 88, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 203.465, mean reward: 2.035 [1.471, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.864, 10.331], loss: 0.158226, mae: 0.343094, mean_q: 3.883652
  8900/100000: episode: 89, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 207.240, mean reward: 2.072 [1.520, 5.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.965, 10.156], loss: 0.128358, mae: 0.336177, mean_q: 3.889678
  9000/100000: episode: 90, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 191.820, mean reward: 1.918 [1.451, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.826, 10.215], loss: 0.105791, mae: 0.312782, mean_q: 3.855519
  9100/100000: episode: 91, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.637, mean reward: 1.886 [1.482, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.503, 10.156], loss: 0.115524, mae: 0.322761, mean_q: 3.888288
  9200/100000: episode: 92, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 182.154, mean reward: 1.822 [1.441, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.213, 10.098], loss: 0.137495, mae: 0.335872, mean_q: 3.909062
  9300/100000: episode: 93, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 218.826, mean reward: 2.188 [1.480, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.243, 10.098], loss: 0.121604, mae: 0.331422, mean_q: 3.882333
  9400/100000: episode: 94, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 196.845, mean reward: 1.968 [1.471, 4.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.587, 10.223], loss: 0.131414, mae: 0.337792, mean_q: 3.913409
  9500/100000: episode: 95, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 209.707, mean reward: 2.097 [1.470, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.489, 10.098], loss: 0.135676, mae: 0.341303, mean_q: 3.909595
  9600/100000: episode: 96, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 195.590, mean reward: 1.956 [1.481, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.507, 10.098], loss: 0.124155, mae: 0.328546, mean_q: 3.920694
  9700/100000: episode: 97, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 184.624, mean reward: 1.846 [1.439, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.194, 10.123], loss: 0.136143, mae: 0.343736, mean_q: 3.902061
  9800/100000: episode: 98, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 217.988, mean reward: 2.180 [1.483, 4.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.540, 10.259], loss: 0.118325, mae: 0.323009, mean_q: 3.901139
  9900/100000: episode: 99, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 197.645, mean reward: 1.976 [1.456, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.549, 10.098], loss: 0.119170, mae: 0.327939, mean_q: 3.909900
 10000/100000: episode: 100, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 203.662, mean reward: 2.037 [1.493, 8.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.437, 10.098], loss: 0.114785, mae: 0.335529, mean_q: 3.922881
 10100/100000: episode: 101, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 188.862, mean reward: 1.889 [1.464, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-0.962, 10.098], loss: 0.123326, mae: 0.338670, mean_q: 3.916579
 10200/100000: episode: 102, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 191.259, mean reward: 1.913 [1.431, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.072, 10.302], loss: 0.140887, mae: 0.343411, mean_q: 3.927578
 10300/100000: episode: 103, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 193.068, mean reward: 1.931 [1.456, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.680, 10.098], loss: 0.127196, mae: 0.336393, mean_q: 3.914219
 10400/100000: episode: 104, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 199.423, mean reward: 1.994 [1.463, 4.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.756, 10.098], loss: 0.123399, mae: 0.327192, mean_q: 3.894112
 10500/100000: episode: 105, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 209.593, mean reward: 2.096 [1.454, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.802, 10.098], loss: 0.118425, mae: 0.327566, mean_q: 3.888977
 10600/100000: episode: 106, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.117, mean reward: 1.891 [1.461, 3.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.063, 10.098], loss: 0.122201, mae: 0.325810, mean_q: 3.902965
 10700/100000: episode: 107, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 197.147, mean reward: 1.971 [1.458, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.852, 10.236], loss: 0.127451, mae: 0.334629, mean_q: 3.888551
 10800/100000: episode: 108, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 198.235, mean reward: 1.982 [1.507, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.185, 10.098], loss: 0.113097, mae: 0.327904, mean_q: 3.884099
 10900/100000: episode: 109, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.665, mean reward: 1.887 [1.513, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.923, 10.249], loss: 0.120038, mae: 0.332269, mean_q: 3.898796
 11000/100000: episode: 110, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 198.170, mean reward: 1.982 [1.443, 5.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.802, 10.162], loss: 0.106927, mae: 0.324622, mean_q: 3.884760
 11100/100000: episode: 111, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 188.936, mean reward: 1.889 [1.438, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.531, 10.098], loss: 0.112519, mae: 0.330632, mean_q: 3.891012
 11200/100000: episode: 112, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 206.542, mean reward: 2.065 [1.512, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.764, 10.098], loss: 0.091744, mae: 0.311502, mean_q: 3.888805
 11300/100000: episode: 113, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.315, mean reward: 1.903 [1.469, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.960, 10.098], loss: 0.112790, mae: 0.329026, mean_q: 3.893571
 11400/100000: episode: 114, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 198.467, mean reward: 1.985 [1.440, 4.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.326, 10.130], loss: 0.118473, mae: 0.320609, mean_q: 3.875627
 11500/100000: episode: 115, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 196.509, mean reward: 1.965 [1.441, 6.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.034, 10.098], loss: 0.105720, mae: 0.324082, mean_q: 3.889876
 11600/100000: episode: 116, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 189.079, mean reward: 1.891 [1.491, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.847, 10.217], loss: 0.103348, mae: 0.322268, mean_q: 3.896672
 11700/100000: episode: 117, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 192.523, mean reward: 1.925 [1.461, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.870, 10.098], loss: 0.115260, mae: 0.318357, mean_q: 3.874688
 11800/100000: episode: 118, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 213.190, mean reward: 2.132 [1.462, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.625, 10.098], loss: 0.123462, mae: 0.331979, mean_q: 3.890786
 11900/100000: episode: 119, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 231.118, mean reward: 2.311 [1.457, 4.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.965, 10.098], loss: 0.118258, mae: 0.326859, mean_q: 3.896208
 12000/100000: episode: 120, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 177.212, mean reward: 1.772 [1.439, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.555, 10.137], loss: 0.109010, mae: 0.327923, mean_q: 3.892416
 12100/100000: episode: 121, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.183, mean reward: 1.912 [1.523, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.389, 10.098], loss: 0.110480, mae: 0.321472, mean_q: 3.888134
 12200/100000: episode: 122, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 183.925, mean reward: 1.839 [1.458, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.733, 10.175], loss: 0.099860, mae: 0.322556, mean_q: 3.883383
 12300/100000: episode: 123, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 191.143, mean reward: 1.911 [1.456, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.710, 10.145], loss: 0.105005, mae: 0.317658, mean_q: 3.885365
 12400/100000: episode: 124, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 183.328, mean reward: 1.833 [1.462, 2.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.965, 10.157], loss: 0.104669, mae: 0.315959, mean_q: 3.898368
 12500/100000: episode: 125, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 203.697, mean reward: 2.037 [1.481, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.691, 10.098], loss: 0.110041, mae: 0.333817, mean_q: 3.891136
 12600/100000: episode: 126, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 195.513, mean reward: 1.955 [1.442, 5.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.856, 10.226], loss: 0.091099, mae: 0.318421, mean_q: 3.894719
 12700/100000: episode: 127, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 195.560, mean reward: 1.956 [1.542, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.983, 10.121], loss: 0.089952, mae: 0.309053, mean_q: 3.872527
 12800/100000: episode: 128, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 201.834, mean reward: 2.018 [1.481, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.073, 10.098], loss: 0.097300, mae: 0.311948, mean_q: 3.890872
 12900/100000: episode: 129, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 191.585, mean reward: 1.916 [1.464, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.432, 10.098], loss: 0.096992, mae: 0.308330, mean_q: 3.879075
 13000/100000: episode: 130, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 202.012, mean reward: 2.020 [1.444, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.446, 10.098], loss: 0.098780, mae: 0.319598, mean_q: 3.892214
 13100/100000: episode: 131, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 195.345, mean reward: 1.953 [1.432, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.912, 10.098], loss: 0.087793, mae: 0.303221, mean_q: 3.885975
 13200/100000: episode: 132, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 192.647, mean reward: 1.926 [1.460, 4.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.626, 10.349], loss: 0.117299, mae: 0.328041, mean_q: 3.898125
 13300/100000: episode: 133, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 187.242, mean reward: 1.872 [1.434, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.124, 10.379], loss: 0.096368, mae: 0.317726, mean_q: 3.876666
 13400/100000: episode: 134, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 191.323, mean reward: 1.913 [1.498, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.870, 10.098], loss: 0.106414, mae: 0.313106, mean_q: 3.888275
 13500/100000: episode: 135, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 191.580, mean reward: 1.916 [1.463, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.667, 10.098], loss: 0.105561, mae: 0.318006, mean_q: 3.871264
 13600/100000: episode: 136, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 202.798, mean reward: 2.028 [1.484, 4.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.070, 10.098], loss: 0.099740, mae: 0.310107, mean_q: 3.885146
 13700/100000: episode: 137, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 178.162, mean reward: 1.782 [1.479, 2.610], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.161, 10.149], loss: 0.103516, mae: 0.313269, mean_q: 3.896614
 13800/100000: episode: 138, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 190.312, mean reward: 1.903 [1.465, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.410, 10.293], loss: 0.096334, mae: 0.309336, mean_q: 3.889686
 13900/100000: episode: 139, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 202.451, mean reward: 2.025 [1.484, 5.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.637, 10.098], loss: 0.092975, mae: 0.309550, mean_q: 3.894941
 14000/100000: episode: 140, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 186.752, mean reward: 1.868 [1.466, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.356, 10.194], loss: 0.105322, mae: 0.318162, mean_q: 3.902807
 14100/100000: episode: 141, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 187.199, mean reward: 1.872 [1.447, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.640, 10.150], loss: 0.091594, mae: 0.300891, mean_q: 3.865183
 14200/100000: episode: 142, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 194.666, mean reward: 1.947 [1.470, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.680, 10.098], loss: 0.096894, mae: 0.311878, mean_q: 3.875057
 14300/100000: episode: 143, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 233.954, mean reward: 2.340 [1.440, 5.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.794, 10.098], loss: 0.086973, mae: 0.306465, mean_q: 3.863537
 14400/100000: episode: 144, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 200.616, mean reward: 2.006 [1.480, 3.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.700, 10.279], loss: 0.096112, mae: 0.307112, mean_q: 3.873476
 14500/100000: episode: 145, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 192.183, mean reward: 1.922 [1.459, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.510, 10.098], loss: 0.105083, mae: 0.314727, mean_q: 3.875113
 14600/100000: episode: 146, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 176.156, mean reward: 1.762 [1.445, 2.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.568, 10.098], loss: 0.084525, mae: 0.297643, mean_q: 3.860599
 14700/100000: episode: 147, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 208.577, mean reward: 2.086 [1.509, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.187, 10.148], loss: 0.095978, mae: 0.305307, mean_q: 3.867005
 14800/100000: episode: 148, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 216.233, mean reward: 2.162 [1.478, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.200, 10.379], loss: 0.089750, mae: 0.297583, mean_q: 3.858629
 14900/100000: episode: 149, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 189.770, mean reward: 1.898 [1.495, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.246, 10.240], loss: 0.090885, mae: 0.305599, mean_q: 3.866190
[Info] 1-TH LEVEL FOUND: 4.60849666595459, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.153s, episode steps: 100, steps per second: 19, episode reward: 186.032, mean reward: 1.860 [1.477, 2.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.721, 10.267], loss: 0.085087, mae: 0.303513, mean_q: 3.865845
 15018/100000: episode: 151, duration: 0.116s, episode steps: 18, steps per second: 155, episode reward: 41.662, mean reward: 2.315 [1.883, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.889, 10.100], loss: 0.114403, mae: 0.330769, mean_q: 3.880731
 15045/100000: episode: 152, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 64.240, mean reward: 2.379 [1.836, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.429, 10.100], loss: 0.085885, mae: 0.307295, mean_q: 3.898298
 15066/100000: episode: 153, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 65.709, mean reward: 3.129 [2.365, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.641, 10.100], loss: 0.082300, mae: 0.291429, mean_q: 3.884119
 15081/100000: episode: 154, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 50.571, mean reward: 3.371 [2.378, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.191, 10.100], loss: 0.091714, mae: 0.308099, mean_q: 3.878386
 15101/100000: episode: 155, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 52.924, mean reward: 2.646 [2.173, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.307, 10.100], loss: 0.108745, mae: 0.320726, mean_q: 3.863637
 15119/100000: episode: 156, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 46.436, mean reward: 2.580 [1.910, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.444, 10.100], loss: 0.086314, mae: 0.300038, mean_q: 3.888673
 15145/100000: episode: 157, duration: 0.141s, episode steps: 26, steps per second: 185, episode reward: 83.357, mean reward: 3.206 [2.065, 4.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.380, 10.100], loss: 0.103001, mae: 0.327473, mean_q: 3.910638
 15163/100000: episode: 158, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 43.333, mean reward: 2.407 [1.856, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.402, 10.100], loss: 0.108248, mae: 0.333634, mean_q: 3.938830
 15178/100000: episode: 159, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 57.372, mean reward: 3.825 [2.782, 5.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.262, 10.100], loss: 0.100381, mae: 0.328053, mean_q: 3.916322
 15205/100000: episode: 160, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 90.415, mean reward: 3.349 [2.293, 5.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.301, 10.100], loss: 0.109351, mae: 0.316802, mean_q: 3.934959
 15220/100000: episode: 161, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 40.376, mean reward: 2.692 [1.934, 5.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.204, 10.100], loss: 0.090569, mae: 0.298808, mean_q: 3.921941
 15236/100000: episode: 162, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 37.995, mean reward: 2.375 [2.023, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.761, 10.100], loss: 0.109442, mae: 0.315538, mean_q: 3.908087
 15257/100000: episode: 163, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 46.568, mean reward: 2.218 [1.985, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.160, 10.100], loss: 0.102494, mae: 0.317028, mean_q: 3.917010
 15283/100000: episode: 164, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 66.898, mean reward: 2.573 [1.826, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.449, 10.100], loss: 0.092187, mae: 0.319935, mean_q: 3.932726
 15299/100000: episode: 165, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 45.029, mean reward: 2.814 [2.098, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.204, 10.100], loss: 0.081895, mae: 0.302232, mean_q: 3.931026
 15325/100000: episode: 166, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 70.708, mean reward: 2.720 [1.707, 9.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.560, 10.100], loss: 0.138134, mae: 0.337516, mean_q: 3.959522
 15351/100000: episode: 167, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 55.254, mean reward: 2.125 [1.565, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.704, 10.122], loss: 0.104499, mae: 0.332238, mean_q: 3.948990
 15365/100000: episode: 168, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 41.992, mean reward: 2.999 [2.422, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.429, 10.100], loss: 0.095109, mae: 0.301826, mean_q: 3.919073
 15383/100000: episode: 169, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 40.693, mean reward: 2.261 [1.801, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.055, 10.100], loss: 0.085393, mae: 0.304209, mean_q: 3.945085
 15401/100000: episode: 170, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 49.334, mean reward: 2.741 [2.176, 5.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.367, 10.100], loss: 0.119521, mae: 0.333220, mean_q: 3.935954
 15428/100000: episode: 171, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 71.885, mean reward: 2.662 [1.937, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.269, 10.100], loss: 0.133378, mae: 0.357722, mean_q: 3.992178
 15454/100000: episode: 172, duration: 0.155s, episode steps: 26, steps per second: 168, episode reward: 56.379, mean reward: 2.168 [1.724, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.287, 10.100], loss: 0.109346, mae: 0.333193, mean_q: 3.957897
 15470/100000: episode: 173, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 51.977, mean reward: 3.249 [2.097, 7.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.216, 10.100], loss: 0.117499, mae: 0.323322, mean_q: 3.944284
 15484/100000: episode: 174, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 39.168, mean reward: 2.798 [2.195, 4.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.341, 10.100], loss: 0.102555, mae: 0.329653, mean_q: 4.032101
 15504/100000: episode: 175, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 47.278, mean reward: 2.364 [1.863, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.285, 10.100], loss: 0.126065, mae: 0.341899, mean_q: 4.029080
 15524/100000: episode: 176, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 61.772, mean reward: 3.089 [2.525, 4.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.321, 10.100], loss: 0.205635, mae: 0.409619, mean_q: 4.049668
 15542/100000: episode: 177, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 43.619, mean reward: 2.423 [1.757, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.174, 10.100], loss: 0.115969, mae: 0.348033, mean_q: 4.015874
 15562/100000: episode: 178, duration: 0.096s, episode steps: 20, steps per second: 207, episode reward: 47.628, mean reward: 2.381 [2.002, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.183, 10.100], loss: 0.127678, mae: 0.337615, mean_q: 4.015210
 15577/100000: episode: 179, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 37.491, mean reward: 2.499 [1.688, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.698, 10.100], loss: 0.096397, mae: 0.318254, mean_q: 4.007318
 15603/100000: episode: 180, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 56.377, mean reward: 2.168 [1.562, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.229, 10.100], loss: 0.130314, mae: 0.347013, mean_q: 4.035762
 15624/100000: episode: 181, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 49.206, mean reward: 2.343 [1.626, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.395, 10.100], loss: 0.115110, mae: 0.335656, mean_q: 3.988868
 15644/100000: episode: 182, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 58.774, mean reward: 2.939 [2.210, 4.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.787, 10.100], loss: 0.119640, mae: 0.346569, mean_q: 4.055478
 15662/100000: episode: 183, duration: 0.091s, episode steps: 18, steps per second: 198, episode reward: 52.143, mean reward: 2.897 [2.359, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.651, 10.100], loss: 0.082592, mae: 0.301965, mean_q: 4.020278
 15680/100000: episode: 184, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 33.986, mean reward: 1.888 [1.565, 2.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.100], loss: 0.105393, mae: 0.324724, mean_q: 3.995333
 15695/100000: episode: 185, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 35.454, mean reward: 2.364 [1.921, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.155, 10.100], loss: 0.110617, mae: 0.330105, mean_q: 4.084929
 15709/100000: episode: 186, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 33.657, mean reward: 2.404 [2.057, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.152, 10.100], loss: 0.152411, mae: 0.351443, mean_q: 4.057545
 15723/100000: episode: 187, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 34.675, mean reward: 2.477 [2.142, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.379, 10.100], loss: 0.161191, mae: 0.376593, mean_q: 4.033412
 15741/100000: episode: 188, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 51.459, mean reward: 2.859 [2.083, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.777, 10.100], loss: 0.116496, mae: 0.327018, mean_q: 4.090221
 15755/100000: episode: 189, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 45.612, mean reward: 3.258 [2.425, 5.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.215, 10.100], loss: 0.095575, mae: 0.325039, mean_q: 3.981774
 15775/100000: episode: 190, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 63.904, mean reward: 3.195 [2.380, 4.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.237, 10.100], loss: 0.115359, mae: 0.334091, mean_q: 4.078016
 15795/100000: episode: 191, duration: 0.102s, episode steps: 20, steps per second: 197, episode reward: 53.798, mean reward: 2.690 [2.142, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.591, 10.100], loss: 0.121300, mae: 0.344100, mean_q: 4.001881
 15816/100000: episode: 192, duration: 0.110s, episode steps: 21, steps per second: 190, episode reward: 58.218, mean reward: 2.772 [2.014, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.518, 10.100], loss: 0.133312, mae: 0.361707, mean_q: 4.110960
 15836/100000: episode: 193, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 53.017, mean reward: 2.651 [1.876, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.173, 10.100], loss: 0.087816, mae: 0.303340, mean_q: 4.023250
 15851/100000: episode: 194, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 36.914, mean reward: 2.461 [2.070, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.235, 10.100], loss: 0.103192, mae: 0.315203, mean_q: 4.075593
 15865/100000: episode: 195, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 44.844, mean reward: 3.203 [2.430, 4.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.320, 10.100], loss: 0.136921, mae: 0.378550, mean_q: 4.136991
 15886/100000: episode: 196, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 58.022, mean reward: 2.763 [2.061, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.309, 10.100], loss: 0.123359, mae: 0.354850, mean_q: 4.072380
 15906/100000: episode: 197, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 56.539, mean reward: 2.827 [1.959, 4.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.413, 10.100], loss: 0.106740, mae: 0.324757, mean_q: 4.087099
 15926/100000: episode: 198, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 50.155, mean reward: 2.508 [2.113, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.163, 10.100], loss: 0.105204, mae: 0.330930, mean_q: 4.134653
 15940/100000: episode: 199, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 38.208, mean reward: 2.729 [2.328, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.254, 10.100], loss: 0.093891, mae: 0.319812, mean_q: 4.131338
 15961/100000: episode: 200, duration: 0.121s, episode steps: 21, steps per second: 173, episode reward: 58.468, mean reward: 2.784 [2.275, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.258, 10.100], loss: 0.129396, mae: 0.340277, mean_q: 4.126298
 15979/100000: episode: 201, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 45.695, mean reward: 2.539 [1.916, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.144, 10.100], loss: 0.115397, mae: 0.343799, mean_q: 4.085614
 15997/100000: episode: 202, duration: 0.090s, episode steps: 18, steps per second: 201, episode reward: 61.743, mean reward: 3.430 [2.215, 6.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.375, 10.100], loss: 0.115920, mae: 0.334743, mean_q: 4.153480
 16012/100000: episode: 203, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 45.111, mean reward: 3.007 [2.379, 4.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.324, 10.100], loss: 0.119291, mae: 0.342051, mean_q: 4.092049
 16030/100000: episode: 204, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 42.894, mean reward: 2.383 [2.049, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.808, 10.100], loss: 0.151642, mae: 0.393148, mean_q: 4.139107
 16044/100000: episode: 205, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 42.787, mean reward: 3.056 [2.387, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.278, 10.100], loss: 0.140631, mae: 0.354410, mean_q: 4.067583
 16070/100000: episode: 206, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 62.544, mean reward: 2.406 [1.827, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.097, 10.100], loss: 0.118501, mae: 0.342186, mean_q: 4.136272
 16084/100000: episode: 207, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 38.649, mean reward: 2.761 [2.378, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.277, 10.100], loss: 0.095490, mae: 0.320742, mean_q: 4.126959
 16111/100000: episode: 208, duration: 0.145s, episode steps: 27, steps per second: 187, episode reward: 104.806, mean reward: 3.882 [2.416, 5.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.515, 10.100], loss: 0.112397, mae: 0.332404, mean_q: 4.113235
 16131/100000: episode: 209, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 42.963, mean reward: 2.148 [1.521, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.206], loss: 0.104615, mae: 0.337834, mean_q: 4.119318
 16149/100000: episode: 210, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 40.911, mean reward: 2.273 [1.774, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.721, 10.100], loss: 0.162609, mae: 0.341178, mean_q: 4.133158
 16164/100000: episode: 211, duration: 0.099s, episode steps: 15, steps per second: 152, episode reward: 34.076, mean reward: 2.272 [1.747, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.585, 10.100], loss: 0.129772, mae: 0.356071, mean_q: 4.189632
 16184/100000: episode: 212, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 50.922, mean reward: 2.546 [1.955, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.134, 10.100], loss: 0.096013, mae: 0.312874, mean_q: 4.134037
 16205/100000: episode: 213, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 46.400, mean reward: 2.210 [1.716, 2.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-2.053, 10.100], loss: 0.109528, mae: 0.348625, mean_q: 4.126641
 16220/100000: episode: 214, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 30.742, mean reward: 2.049 [1.641, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.048, 10.100], loss: 0.133370, mae: 0.352981, mean_q: 4.115795
 16236/100000: episode: 215, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 42.333, mean reward: 2.646 [2.144, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.263, 10.100], loss: 0.159154, mae: 0.396780, mean_q: 4.163118
 16263/100000: episode: 216, duration: 0.153s, episode steps: 27, steps per second: 177, episode reward: 90.377, mean reward: 3.347 [2.408, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.161, 10.100], loss: 0.133718, mae: 0.341750, mean_q: 4.127079
 16279/100000: episode: 217, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 46.297, mean reward: 2.894 [1.937, 6.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.569, 10.100], loss: 0.111501, mae: 0.344792, mean_q: 4.189280
 16295/100000: episode: 218, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 45.986, mean reward: 2.874 [2.202, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.302, 10.100], loss: 0.140095, mae: 0.345555, mean_q: 4.163492
 16311/100000: episode: 219, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 47.493, mean reward: 2.968 [1.924, 5.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.279, 10.100], loss: 0.137421, mae: 0.371119, mean_q: 4.262216
 16329/100000: episode: 220, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 48.213, mean reward: 2.679 [1.899, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.146, 10.100], loss: 0.152989, mae: 0.382147, mean_q: 4.174139
 16347/100000: episode: 221, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 40.029, mean reward: 2.224 [1.904, 2.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.257, 10.100], loss: 0.174011, mae: 0.387801, mean_q: 4.276686
 16373/100000: episode: 222, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 53.570, mean reward: 2.060 [1.541, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.558, 10.100], loss: 0.133435, mae: 0.360173, mean_q: 4.223072
 16394/100000: episode: 223, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 66.690, mean reward: 3.176 [2.335, 5.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.197, 10.100], loss: 0.154341, mae: 0.386261, mean_q: 4.261821
 16412/100000: episode: 224, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 48.018, mean reward: 2.668 [1.975, 7.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.275, 10.100], loss: 0.125479, mae: 0.355020, mean_q: 4.207646
 16438/100000: episode: 225, duration: 0.149s, episode steps: 26, steps per second: 174, episode reward: 53.455, mean reward: 2.056 [1.524, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.001, 10.244], loss: 0.129250, mae: 0.354749, mean_q: 4.268815
 16456/100000: episode: 226, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 46.323, mean reward: 2.573 [2.115, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.307, 10.100], loss: 0.143504, mae: 0.372781, mean_q: 4.246115
 16476/100000: episode: 227, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 50.576, mean reward: 2.529 [2.005, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.113, 10.100], loss: 0.133746, mae: 0.342490, mean_q: 4.245330
 16496/100000: episode: 228, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 50.635, mean reward: 2.532 [2.227, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.217, 10.100], loss: 0.124211, mae: 0.341995, mean_q: 4.239515
 16514/100000: episode: 229, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 44.368, mean reward: 2.465 [1.804, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.362, 10.100], loss: 0.186950, mae: 0.375415, mean_q: 4.184628
 16534/100000: episode: 230, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 47.501, mean reward: 2.375 [2.067, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.721, 10.100], loss: 0.181991, mae: 0.375650, mean_q: 4.207085
 16552/100000: episode: 231, duration: 0.101s, episode steps: 18, steps per second: 177, episode reward: 47.125, mean reward: 2.618 [1.925, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.168, 10.100], loss: 0.124683, mae: 0.357158, mean_q: 4.261593
 16570/100000: episode: 232, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 50.596, mean reward: 2.811 [1.894, 3.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.982, 10.100], loss: 0.115372, mae: 0.335847, mean_q: 4.170452
 16588/100000: episode: 233, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 52.606, mean reward: 2.923 [2.185, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.332, 10.100], loss: 0.168322, mae: 0.418891, mean_q: 4.354738
 16608/100000: episode: 234, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: 55.435, mean reward: 2.772 [1.740, 6.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.063, 10.100], loss: 0.147041, mae: 0.374430, mean_q: 4.226438
 16626/100000: episode: 235, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 36.193, mean reward: 2.011 [1.711, 2.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.306, 10.100], loss: 0.137826, mae: 0.371657, mean_q: 4.237175
 16653/100000: episode: 236, duration: 0.145s, episode steps: 27, steps per second: 187, episode reward: 80.976, mean reward: 2.999 [2.151, 4.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.533, 10.100], loss: 0.141672, mae: 0.354800, mean_q: 4.213539
 16671/100000: episode: 237, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 46.987, mean reward: 2.610 [2.259, 4.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.250, 10.100], loss: 0.201711, mae: 0.378832, mean_q: 4.332831
 16689/100000: episode: 238, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 41.650, mean reward: 2.314 [1.884, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.310, 10.100], loss: 0.095036, mae: 0.315971, mean_q: 4.258421
 16709/100000: episode: 239, duration: 0.099s, episode steps: 20, steps per second: 201, episode reward: 56.232, mean reward: 2.812 [2.023, 5.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.976, 10.100], loss: 0.108189, mae: 0.334612, mean_q: 4.238964
[Info] 2-TH LEVEL FOUND: 5.910622596740723, Considering 10/90 traces
 16735/100000: episode: 240, duration: 4.338s, episode steps: 26, steps per second: 6, episode reward: 57.930, mean reward: 2.228 [1.622, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.105, 10.100], loss: 0.158095, mae: 0.375833, mean_q: 4.272653
 16748/100000: episode: 241, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 49.888, mean reward: 3.838 [2.622, 5.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.564, 10.100], loss: 0.115973, mae: 0.353029, mean_q: 4.205272
 16761/100000: episode: 242, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 42.426, mean reward: 3.264 [2.727, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.321, 10.100], loss: 0.102811, mae: 0.335271, mean_q: 4.216445
 16771/100000: episode: 243, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 31.429, mean reward: 3.143 [2.688, 5.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.492, 10.100], loss: 0.195116, mae: 0.388924, mean_q: 4.290793
 16788/100000: episode: 244, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 61.094, mean reward: 3.594 [2.664, 5.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.417, 10.100], loss: 0.116604, mae: 0.340040, mean_q: 4.285998
 16807/100000: episode: 245, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 85.149, mean reward: 4.482 [3.035, 9.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.483, 10.100], loss: 0.123130, mae: 0.350639, mean_q: 4.284990
 16822/100000: episode: 246, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 71.362, mean reward: 4.757 [2.925, 7.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.377, 10.100], loss: 0.148228, mae: 0.360209, mean_q: 4.267859
 16835/100000: episode: 247, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 37.688, mean reward: 2.899 [2.253, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.363, 10.100], loss: 0.140428, mae: 0.362674, mean_q: 4.261236
 16848/100000: episode: 248, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 59.717, mean reward: 4.594 [2.325, 10.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.528, 10.100], loss: 0.114963, mae: 0.342757, mean_q: 4.275956
 16861/100000: episode: 249, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 44.053, mean reward: 3.389 [2.533, 4.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.621, 10.100], loss: 0.137072, mae: 0.356914, mean_q: 4.290488
 16881/100000: episode: 250, duration: 0.113s, episode steps: 20, steps per second: 177, episode reward: 179.402, mean reward: 8.970 [3.995, 24.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.421, 10.100], loss: 0.147417, mae: 0.361812, mean_q: 4.327372
 16891/100000: episode: 251, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 35.044, mean reward: 3.504 [2.995, 4.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.348, 10.100], loss: 1.361924, mae: 0.577761, mean_q: 4.537405
 16904/100000: episode: 252, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 40.564, mean reward: 3.120 [2.570, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.408, 10.100], loss: 0.193375, mae: 0.412078, mean_q: 4.372524
 16917/100000: episode: 253, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 50.483, mean reward: 3.883 [2.656, 5.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.453, 10.100], loss: 0.655026, mae: 0.478212, mean_q: 4.428344
 16937/100000: episode: 254, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 76.965, mean reward: 3.848 [2.888, 6.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.520, 10.100], loss: 0.175766, mae: 0.409112, mean_q: 4.462213
 16957/100000: episode: 255, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 93.880, mean reward: 4.694 [3.177, 7.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.497, 10.100], loss: 0.145505, mae: 0.365313, mean_q: 4.376089
 16970/100000: episode: 256, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 41.750, mean reward: 3.212 [2.523, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.547, 10.100], loss: 0.178065, mae: 0.405314, mean_q: 4.427947
 16989/100000: episode: 257, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 114.993, mean reward: 6.052 [3.112, 18.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.173, 10.100], loss: 0.542185, mae: 0.453218, mean_q: 4.476787
 16997/100000: episode: 258, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 25.786, mean reward: 3.223 [2.787, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.319, 10.100], loss: 0.285898, mae: 0.529553, mean_q: 4.606172
 17014/100000: episode: 259, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 51.025, mean reward: 3.001 [2.437, 3.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.292, 10.100], loss: 0.895816, mae: 0.561250, mean_q: 4.437838
 17029/100000: episode: 260, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 39.918, mean reward: 2.661 [1.926, 4.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.155, 10.100], loss: 0.230378, mae: 0.439444, mean_q: 4.508093
 17048/100000: episode: 261, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 60.296, mean reward: 3.173 [2.481, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.358, 10.100], loss: 0.278861, mae: 0.447079, mean_q: 4.490210
 17058/100000: episode: 262, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 29.035, mean reward: 2.904 [2.399, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.454, 10.100], loss: 0.308468, mae: 0.472238, mean_q: 4.590651
 17066/100000: episode: 263, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 28.152, mean reward: 3.519 [2.801, 4.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.314, 10.100], loss: 0.219175, mae: 0.397779, mean_q: 4.439447
 17083/100000: episode: 264, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 57.084, mean reward: 3.358 [2.356, 9.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.621, 10.100], loss: 0.633351, mae: 0.488120, mean_q: 4.598676
 17100/100000: episode: 265, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 52.047, mean reward: 3.062 [2.404, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.139, 10.100], loss: 0.361014, mae: 0.442366, mean_q: 4.559346
 17110/100000: episode: 266, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 27.894, mean reward: 2.789 [2.416, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.129, 10.100], loss: 0.471214, mae: 0.526178, mean_q: 4.789373
 17120/100000: episode: 267, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 27.432, mean reward: 2.743 [2.034, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.291, 10.100], loss: 0.227831, mae: 0.462979, mean_q: 4.506228
 17135/100000: episode: 268, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 52.658, mean reward: 3.511 [2.860, 4.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.397, 10.100], loss: 0.204046, mae: 0.380312, mean_q: 4.502809
 17143/100000: episode: 269, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 32.744, mean reward: 4.093 [3.319, 4.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.372, 10.100], loss: 0.546362, mae: 0.533957, mean_q: 4.712061
 17153/100000: episode: 270, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 39.075, mean reward: 3.908 [2.778, 11.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.174, 10.100], loss: 0.111812, mae: 0.347593, mean_q: 4.388816
 17166/100000: episode: 271, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 47.193, mean reward: 3.630 [3.060, 5.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.356, 10.100], loss: 0.160204, mae: 0.411983, mean_q: 4.617761
 17185/100000: episode: 272, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 71.606, mean reward: 3.769 [3.059, 6.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.308, 10.100], loss: 0.270555, mae: 0.427641, mean_q: 4.554612
 17205/100000: episode: 273, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 60.788, mean reward: 3.039 [2.488, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.173, 10.100], loss: 0.508152, mae: 0.568350, mean_q: 4.699297
 17218/100000: episode: 274, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 38.216, mean reward: 2.940 [2.569, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.263, 10.100], loss: 0.244328, mae: 0.441400, mean_q: 4.556500
 17231/100000: episode: 275, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 37.089, mean reward: 2.853 [2.472, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.592, 10.100], loss: 0.646376, mae: 0.514542, mean_q: 4.609637
 17244/100000: episode: 276, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 44.023, mean reward: 3.386 [2.454, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.255, 10.100], loss: 0.406876, mae: 0.510690, mean_q: 4.583248
 17254/100000: episode: 277, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 32.451, mean reward: 3.245 [2.410, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.513, 10.100], loss: 0.230773, mae: 0.471611, mean_q: 4.668853
 17273/100000: episode: 278, duration: 0.094s, episode steps: 19, steps per second: 203, episode reward: 74.173, mean reward: 3.904 [3.328, 5.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.465, 10.100], loss: 0.653433, mae: 0.523678, mean_q: 4.679001
 17282/100000: episode: 279, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 35.945, mean reward: 3.994 [3.381, 4.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.466, 10.100], loss: 0.209275, mae: 0.464825, mean_q: 4.486517
 17302/100000: episode: 280, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 206.060, mean reward: 10.303 [4.298, 19.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.435, 10.100], loss: 0.339585, mae: 0.469068, mean_q: 4.669576
 17319/100000: episode: 281, duration: 0.105s, episode steps: 17, steps per second: 163, episode reward: 72.619, mean reward: 4.272 [2.923, 6.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.232, 10.100], loss: 0.778307, mae: 0.540640, mean_q: 4.739789
 17332/100000: episode: 282, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 49.569, mean reward: 3.813 [2.642, 6.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.461, 10.100], loss: 0.237166, mae: 0.463698, mean_q: 4.706889
 17347/100000: episode: 283, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 71.991, mean reward: 4.799 [2.954, 7.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-1.131, 10.100], loss: 0.304929, mae: 0.446750, mean_q: 4.546765
 17355/100000: episode: 284, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 33.881, mean reward: 4.235 [3.518, 6.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.358, 10.100], loss: 0.206189, mae: 0.429996, mean_q: 4.632496
 17363/100000: episode: 285, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 26.481, mean reward: 3.310 [2.319, 5.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.308, 10.100], loss: 0.665872, mae: 0.475632, mean_q: 4.709727
 17382/100000: episode: 286, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 68.630, mean reward: 3.612 [2.613, 5.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.399, 10.100], loss: 0.497017, mae: 0.528414, mean_q: 4.832434
 17392/100000: episode: 287, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 32.420, mean reward: 3.242 [2.294, 7.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.308, 10.100], loss: 0.346990, mae: 0.539041, mean_q: 4.895413
 17407/100000: episode: 288, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 40.285, mean reward: 2.686 [2.190, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.311, 10.100], loss: 0.429652, mae: 0.499721, mean_q: 4.590654
 17417/100000: episode: 289, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 30.876, mean reward: 3.088 [2.828, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.305, 10.100], loss: 0.479895, mae: 0.524020, mean_q: 4.746309
 17430/100000: episode: 290, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 41.588, mean reward: 3.199 [2.631, 4.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.319, 10.100], loss: 0.405316, mae: 0.559372, mean_q: 4.776806
 17449/100000: episode: 291, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 83.195, mean reward: 4.379 [3.189, 7.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.498, 10.100], loss: 0.324135, mae: 0.461550, mean_q: 4.692108
 17459/100000: episode: 292, duration: 0.050s, episode steps: 10, steps per second: 199, episode reward: 24.262, mean reward: 2.426 [2.144, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.945, 10.100], loss: 0.853419, mae: 0.561512, mean_q: 4.883966
 17474/100000: episode: 293, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 57.723, mean reward: 3.848 [2.601, 6.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.807, 10.100], loss: 0.419798, mae: 0.561759, mean_q: 4.981819
 17483/100000: episode: 294, duration: 0.046s, episode steps: 9, steps per second: 198, episode reward: 66.101, mean reward: 7.345 [3.416, 26.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.473, 10.100], loss: 0.362934, mae: 0.518578, mean_q: 4.828020
 17493/100000: episode: 295, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 27.590, mean reward: 2.759 [1.946, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.472, 10.100], loss: 0.229987, mae: 0.423469, mean_q: 4.688144
 17501/100000: episode: 296, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 23.883, mean reward: 2.985 [2.367, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-2.092, 10.100], loss: 0.307205, mae: 0.541375, mean_q: 4.861526
 17509/100000: episode: 297, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 37.306, mean reward: 4.663 [3.739, 5.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.468, 10.100], loss: 1.244882, mae: 0.611812, mean_q: 4.742657
 17518/100000: episode: 298, duration: 0.046s, episode steps: 9, steps per second: 198, episode reward: 41.774, mean reward: 4.642 [3.577, 6.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.374, 10.100], loss: 1.619821, mae: 0.797453, mean_q: 5.225162
 17537/100000: episode: 299, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 173.067, mean reward: 9.109 [2.850, 76.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.648, 10.100], loss: 0.440345, mae: 0.529547, mean_q: 4.854321
 17547/100000: episode: 300, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 36.942, mean reward: 3.694 [3.026, 4.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.499, 10.100], loss: 0.662986, mae: 0.529904, mean_q: 4.912593
 17560/100000: episode: 301, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 37.115, mean reward: 2.855 [2.102, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.348, 10.100], loss: 0.259297, mae: 0.459888, mean_q: 4.809438
 17568/100000: episode: 302, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 23.176, mean reward: 2.897 [2.340, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.375, 10.100], loss: 0.621860, mae: 0.519672, mean_q: 4.973517
 17578/100000: episode: 303, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 70.824, mean reward: 7.082 [4.517, 11.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.519, 10.100], loss: 0.496108, mae: 0.564703, mean_q: 4.850535
 17588/100000: episode: 304, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 26.759, mean reward: 2.676 [2.366, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.601, 10.100], loss: 0.295082, mae: 0.493172, mean_q: 4.841686
 17605/100000: episode: 305, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 58.572, mean reward: 3.445 [2.768, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.406, 10.100], loss: 0.353077, mae: 0.513868, mean_q: 4.876333
 17615/100000: episode: 306, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 40.108, mean reward: 4.011 [2.506, 6.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.429, 10.100], loss: 0.779065, mae: 0.607732, mean_q: 5.082788
 17632/100000: episode: 307, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 46.956, mean reward: 2.762 [2.408, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.302, 10.100], loss: 0.479306, mae: 0.561505, mean_q: 4.879442
 17641/100000: episode: 308, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 42.838, mean reward: 4.760 [4.053, 5.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.335, 10.100], loss: 0.368847, mae: 0.548244, mean_q: 5.003868
 17658/100000: episode: 309, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 57.529, mean reward: 3.384 [2.726, 6.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.444, 10.100], loss: 0.671470, mae: 0.556139, mean_q: 4.841390
 17671/100000: episode: 310, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 39.455, mean reward: 3.035 [2.588, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.421, 10.100], loss: 0.279673, mae: 0.468768, mean_q: 4.919074
 17690/100000: episode: 311, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 90.306, mean reward: 4.753 [2.695, 9.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.419, 10.100], loss: 0.502654, mae: 0.512392, mean_q: 4.984347
 17709/100000: episode: 312, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 45.762, mean reward: 2.409 [1.816, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.112, 10.100], loss: 0.401975, mae: 0.516812, mean_q: 4.917910
 17722/100000: episode: 313, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 41.200, mean reward: 3.169 [2.202, 6.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.473, 10.100], loss: 1.195391, mae: 0.682622, mean_q: 4.992480
 17735/100000: episode: 314, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 39.957, mean reward: 3.074 [2.366, 4.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.641, 10.100], loss: 0.933481, mae: 0.653809, mean_q: 5.140318
 17743/100000: episode: 315, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 29.029, mean reward: 3.629 [2.776, 4.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.299, 10.100], loss: 1.060325, mae: 0.653886, mean_q: 4.870892
 17760/100000: episode: 316, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 46.041, mean reward: 2.708 [2.417, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.360, 10.100], loss: 0.477337, mae: 0.594166, mean_q: 5.099715
 17769/100000: episode: 317, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 40.430, mean reward: 4.492 [3.427, 6.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.282, 10.100], loss: 0.365591, mae: 0.483492, mean_q: 4.917501
 17777/100000: episode: 318, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 29.621, mean reward: 3.703 [3.378, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.380, 10.100], loss: 0.258929, mae: 0.472948, mean_q: 4.823212
 17792/100000: episode: 319, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 51.703, mean reward: 3.447 [2.530, 4.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.441, 10.100], loss: 0.396139, mae: 0.497864, mean_q: 5.049744
 17802/100000: episode: 320, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 43.149, mean reward: 4.315 [2.245, 10.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.414, 10.100], loss: 0.401017, mae: 0.523053, mean_q: 4.943541
 17812/100000: episode: 321, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 28.880, mean reward: 2.888 [2.326, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.337, 10.100], loss: 0.585253, mae: 0.646012, mean_q: 5.276774
 17827/100000: episode: 322, duration: 0.102s, episode steps: 15, steps per second: 147, episode reward: 48.132, mean reward: 3.209 [2.397, 5.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.348, 10.100], loss: 0.917245, mae: 0.635377, mean_q: 5.121152
 17835/100000: episode: 323, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 24.946, mean reward: 3.118 [2.445, 3.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.326, 10.100], loss: 0.734439, mae: 0.633811, mean_q: 5.115855
 17855/100000: episode: 324, duration: 0.113s, episode steps: 20, steps per second: 178, episode reward: 87.676, mean reward: 4.384 [2.729, 6.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.346, 10.100], loss: 0.316128, mae: 0.539377, mean_q: 4.968381
 17864/100000: episode: 325, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 34.232, mean reward: 3.804 [3.336, 4.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.381, 10.100], loss: 0.956846, mae: 0.672412, mean_q: 5.262793
 17873/100000: episode: 326, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 40.214, mean reward: 4.468 [2.888, 5.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.100, 10.100], loss: 1.305673, mae: 0.623486, mean_q: 4.965501
 17883/100000: episode: 327, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 31.483, mean reward: 3.148 [2.535, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.475, 10.100], loss: 0.360189, mae: 0.527711, mean_q: 4.868140
 17898/100000: episode: 328, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 49.649, mean reward: 3.310 [2.638, 4.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.459, 10.100], loss: 0.484403, mae: 0.653409, mean_q: 5.084679
 17906/100000: episode: 329, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 25.772, mean reward: 3.222 [2.633, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.485, 10.100], loss: 0.524755, mae: 0.601408, mean_q: 4.987523
[Info] 3-TH LEVEL FOUND: 9.823883056640625, Considering 10/90 traces
 17916/100000: episode: 330, duration: 4.293s, episode steps: 10, steps per second: 2, episode reward: 31.551, mean reward: 3.155 [2.118, 7.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.470, 10.100], loss: 1.560199, mae: 0.733369, mean_q: 5.061683
 17923/100000: episode: 331, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 26.687, mean reward: 3.812 [2.979, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.430, 10.100], loss: 0.637634, mae: 0.592196, mean_q: 4.812782
 17932/100000: episode: 332, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 179.646, mean reward: 19.961 [7.324, 80.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.349, 10.100], loss: 0.564734, mae: 0.644518, mean_q: 5.153513
 17939/100000: episode: 333, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 29.098, mean reward: 4.157 [3.431, 5.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.592, 10.100], loss: 0.760445, mae: 0.664321, mean_q: 5.286725
 17948/100000: episode: 334, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 29.405, mean reward: 3.267 [2.604, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.537, 10.100], loss: 0.529170, mae: 0.549558, mean_q: 5.143467
 17964/100000: episode: 335, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 201.879, mean reward: 12.617 [6.240, 27.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.498, 10.100], loss: 5.619651, mae: 0.888892, mean_q: 5.194103
 17977/100000: episode: 336, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 76.714, mean reward: 5.901 [3.082, 21.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.526, 10.100], loss: 1.012838, mae: 0.621510, mean_q: 5.033715
 17986/100000: episode: 337, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 115.913, mean reward: 12.879 [6.251, 22.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.328, 10.100], loss: 1.715607, mae: 0.725791, mean_q: 5.459346
 17999/100000: episode: 338, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 58.799, mean reward: 4.523 [2.898, 7.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.450, 10.100], loss: 0.795514, mae: 0.624569, mean_q: 5.194315
 18007/100000: episode: 339, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 40.312, mean reward: 5.039 [4.397, 5.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.414, 10.100], loss: 1.242690, mae: 0.742370, mean_q: 5.479884
 18016/100000: episode: 340, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 36.404, mean reward: 4.045 [2.627, 5.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.439, 10.100], loss: 1.245174, mae: 0.595129, mean_q: 5.125790
 18024/100000: episode: 341, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 48.786, mean reward: 6.098 [4.122, 8.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.478, 10.100], loss: 11.762759, mae: 1.555099, mean_q: 6.040818
 18033/100000: episode: 342, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 97.823, mean reward: 10.869 [5.435, 32.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.504, 10.100], loss: 1.569600, mae: 0.911929, mean_q: 4.782649
 18047/100000: episode: 343, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 69.970, mean reward: 4.998 [3.709, 5.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.392, 10.100], loss: 0.601802, mae: 0.727859, mean_q: 5.138752
 18055/100000: episode: 344, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 34.656, mean reward: 4.332 [3.524, 5.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.460, 10.100], loss: 1.634425, mae: 0.828054, mean_q: 5.503170
 18068/100000: episode: 345, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 50.533, mean reward: 3.887 [3.056, 5.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.795, 10.100], loss: 0.461033, mae: 0.581998, mean_q: 4.967182
 18075/100000: episode: 346, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 33.923, mean reward: 4.846 [3.623, 6.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.543, 10.100], loss: 0.330218, mae: 0.538701, mean_q: 5.240911
 18080/100000: episode: 347, duration: 0.037s, episode steps: 5, steps per second: 136, episode reward: 26.833, mean reward: 5.367 [3.358, 9.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.567, 10.100], loss: 1.470321, mae: 0.676525, mean_q: 5.118463
 18085/100000: episode: 348, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 18.193, mean reward: 3.639 [2.828, 4.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.545, 10.100], loss: 0.270281, mae: 0.512779, mean_q: 5.098074
 18093/100000: episode: 349, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 82.296, mean reward: 10.287 [4.788, 22.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.582, 10.100], loss: 0.585631, mae: 0.594661, mean_q: 5.146585
 18107/100000: episode: 350, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 112.287, mean reward: 8.021 [4.088, 15.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.818, 10.100], loss: 12.240685, mae: 1.324330, mean_q: 5.765884
 18116/100000: episode: 351, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 76.327, mean reward: 8.481 [6.522, 11.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.519, 10.100], loss: 0.412809, mae: 0.613405, mean_q: 5.111420
 18132/100000: episode: 352, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 109.913, mean reward: 6.870 [2.486, 25.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.696, 10.100], loss: 0.803342, mae: 0.708026, mean_q: 5.237831
 18139/100000: episode: 353, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 31.316, mean reward: 4.474 [3.670, 5.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.452, 10.100], loss: 0.543543, mae: 0.694115, mean_q: 5.157018
 18146/100000: episode: 354, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 55.054, mean reward: 7.865 [5.358, 13.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.488, 10.100], loss: 0.642822, mae: 0.627015, mean_q: 5.060044
 18154/100000: episode: 355, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 40.731, mean reward: 5.091 [4.086, 6.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.389, 10.100], loss: 9.606313, mae: 1.215830, mean_q: 6.141158
 18162/100000: episode: 356, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 52.296, mean reward: 6.537 [4.870, 9.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.469, 10.100], loss: 3.744510, mae: 1.087373, mean_q: 5.181234
 18170/100000: episode: 357, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 45.296, mean reward: 5.662 [4.119, 8.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.432, 10.100], loss: 1.207903, mae: 0.822176, mean_q: 5.557397
 18179/100000: episode: 358, duration: 0.050s, episode steps: 9, steps per second: 182, episode reward: 68.915, mean reward: 7.657 [4.038, 14.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.472, 10.100], loss: 0.752114, mae: 0.710710, mean_q: 5.403790
 18184/100000: episode: 359, duration: 0.040s, episode steps: 5, steps per second: 125, episode reward: 29.219, mean reward: 5.844 [4.344, 7.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.593, 10.100], loss: 19.755619, mae: 1.619626, mean_q: 5.912158
 18200/100000: episode: 360, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 112.910, mean reward: 7.057 [3.188, 13.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.333, 10.100], loss: 1.499446, mae: 1.031503, mean_q: 5.390434
 18211/100000: episode: 361, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 47.638, mean reward: 4.331 [2.936, 6.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.435, 10.100], loss: 1.396748, mae: 0.813994, mean_q: 5.255142
 18227/100000: episode: 362, duration: 0.078s, episode steps: 16, steps per second: 206, episode reward: 103.900, mean reward: 6.494 [3.379, 10.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.340, 10.100], loss: 1.717324, mae: 0.779543, mean_q: 5.541756
 18241/100000: episode: 363, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 56.246, mean reward: 4.018 [3.014, 6.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.461, 10.100], loss: 1.136202, mae: 0.687585, mean_q: 5.542344
 18255/100000: episode: 364, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 70.208, mean reward: 5.015 [2.673, 8.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.420, 10.100], loss: 0.733466, mae: 0.677657, mean_q: 5.402802
 18264/100000: episode: 365, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 39.869, mean reward: 4.430 [2.650, 6.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.515, 10.100], loss: 8.600703, mae: 0.939754, mean_q: 5.629045
 18273/100000: episode: 366, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 61.065, mean reward: 6.785 [3.492, 14.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.366, 10.100], loss: 0.945286, mae: 0.957319, mean_q: 6.232757
 18282/100000: episode: 367, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 29.656, mean reward: 3.295 [2.566, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.610, 10.100], loss: 0.957141, mae: 0.827592, mean_q: 5.385625
 18289/100000: episode: 368, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 31.672, mean reward: 4.525 [4.075, 5.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.629, 10.100], loss: 2.731693, mae: 0.869294, mean_q: 5.660222
 18294/100000: episode: 369, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 20.159, mean reward: 4.032 [3.456, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.676, 10.100], loss: 1.446042, mae: 0.772111, mean_q: 5.691391
 18302/100000: episode: 370, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 63.764, mean reward: 7.970 [4.786, 16.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.563, 10.100], loss: 11.746496, mae: 1.094819, mean_q: 5.800422
 18310/100000: episode: 371, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 36.397, mean reward: 4.550 [3.529, 5.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.528, 10.100], loss: 2.356033, mae: 1.061974, mean_q: 5.906137
 18318/100000: episode: 372, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 38.576, mean reward: 4.822 [3.537, 6.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.036, 10.100], loss: 0.871799, mae: 0.765539, mean_q: 5.456852
 18323/100000: episode: 373, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 22.429, mean reward: 4.486 [3.271, 6.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.793, 10.100], loss: 1.961692, mae: 1.014995, mean_q: 6.117888
 18336/100000: episode: 374, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 59.348, mean reward: 4.565 [3.198, 8.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.493, 10.100], loss: 8.180759, mae: 1.094694, mean_q: 5.877094
 18344/100000: episode: 375, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 45.872, mean reward: 5.734 [4.031, 11.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.390, 10.100], loss: 2.349480, mae: 0.837962, mean_q: 5.648162
 18358/100000: episode: 376, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 72.099, mean reward: 5.150 [3.219, 14.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.384, 10.100], loss: 2.020591, mae: 0.821109, mean_q: 5.787978
 18374/100000: episode: 377, duration: 0.089s, episode steps: 16, steps per second: 181, episode reward: 83.927, mean reward: 5.245 [3.584, 10.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.386, 10.100], loss: 5.367012, mae: 0.976830, mean_q: 6.109302
 18381/100000: episode: 378, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 35.940, mean reward: 5.134 [3.829, 8.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.519, 10.100], loss: 10.881805, mae: 1.027342, mean_q: 5.668822
 18389/100000: episode: 379, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 33.826, mean reward: 4.228 [3.693, 4.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.395, 10.100], loss: 2.442296, mae: 0.989150, mean_q: 5.940830
 18397/100000: episode: 380, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 53.685, mean reward: 6.711 [4.731, 9.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.402, 10.100], loss: 0.869153, mae: 0.776317, mean_q: 5.719890
 18406/100000: episode: 381, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 35.289, mean reward: 3.921 [2.631, 6.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.632, 10.100], loss: 0.793868, mae: 0.812398, mean_q: 5.662912
 18411/100000: episode: 382, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 27.278, mean reward: 5.456 [3.771, 7.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.554, 10.100], loss: 0.997049, mae: 0.802873, mean_q: 5.735835
 18425/100000: episode: 383, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 75.522, mean reward: 5.394 [3.487, 10.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.422, 10.100], loss: 1.622723, mae: 0.778194, mean_q: 5.739482
 18436/100000: episode: 384, duration: 0.055s, episode steps: 11, steps per second: 201, episode reward: 48.182, mean reward: 4.380 [2.661, 6.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.409, 10.100], loss: 0.760649, mae: 0.742629, mean_q: 5.751249
 18444/100000: episode: 385, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 39.803, mean reward: 4.975 [3.452, 6.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.405, 10.100], loss: 1.726921, mae: 0.923383, mean_q: 5.988491
 18452/100000: episode: 386, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 39.283, mean reward: 4.910 [3.971, 6.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.556, 10.100], loss: 1.778523, mae: 0.858129, mean_q: 5.681409
 18461/100000: episode: 387, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 99.470, mean reward: 11.052 [4.151, 23.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.368, 10.100], loss: 0.903851, mae: 0.794451, mean_q: 5.992533
 18468/100000: episode: 388, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 39.104, mean reward: 5.586 [3.715, 7.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.951, 10.100], loss: 2.260518, mae: 0.951869, mean_q: 5.796535
 18477/100000: episode: 389, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 38.071, mean reward: 4.230 [3.081, 4.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.496, 10.100], loss: 0.733869, mae: 0.717494, mean_q: 5.455633
 18485/100000: episode: 390, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 42.132, mean reward: 5.266 [3.886, 6.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.435, 10.100], loss: 0.736102, mae: 0.731006, mean_q: 5.825512
 18490/100000: episode: 391, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 22.531, mean reward: 4.506 [3.084, 7.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.556, 10.100], loss: 0.796088, mae: 0.885085, mean_q: 6.317016
 18499/100000: episode: 392, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 117.123, mean reward: 13.014 [5.486, 28.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.400, 10.100], loss: 2.047090, mae: 1.022795, mean_q: 6.064829
 18508/100000: episode: 393, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 126.017, mean reward: 14.002 [6.527, 34.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.549, 10.100], loss: 1.334277, mae: 0.989173, mean_q: 6.128874
 18517/100000: episode: 394, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 50.568, mean reward: 5.619 [2.637, 10.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.511, 10.100], loss: 1.022295, mae: 0.813318, mean_q: 6.019494
 18522/100000: episode: 395, duration: 0.027s, episode steps: 5, steps per second: 183, episode reward: 23.675, mean reward: 4.735 [3.275, 7.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.460, 10.100], loss: 1.368983, mae: 0.882623, mean_q: 6.153791
 18535/100000: episode: 396, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 121.092, mean reward: 9.315 [3.167, 36.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.572, 10.100], loss: 2.151404, mae: 1.012863, mean_q: 6.160106
 18548/100000: episode: 397, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 55.605, mean reward: 4.277 [2.684, 5.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.452, 10.100], loss: 16.866858, mae: 2.100285, mean_q: 6.971095
 18562/100000: episode: 398, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 65.556, mean reward: 4.683 [3.941, 6.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.399, 10.100], loss: 1.076840, mae: 0.894659, mean_q: 5.904731
 18567/100000: episode: 399, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 15.167, mean reward: 3.033 [2.634, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.498, 10.100], loss: 0.610635, mae: 0.741472, mean_q: 6.151956
 18575/100000: episode: 400, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 31.379, mean reward: 3.922 [3.319, 4.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.391, 10.100], loss: 2.649420, mae: 0.933945, mean_q: 6.301574
 18583/100000: episode: 401, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 45.931, mean reward: 5.741 [4.868, 6.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.409, 10.100], loss: 1.020616, mae: 0.877199, mean_q: 6.141847
 18596/100000: episode: 402, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 50.471, mean reward: 3.882 [2.860, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.344, 10.100], loss: 1.018438, mae: 0.846699, mean_q: 5.894114
 18605/100000: episode: 403, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 29.612, mean reward: 3.290 [2.643, 4.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.385, 10.100], loss: 1.414066, mae: 0.807190, mean_q: 5.852047
 18621/100000: episode: 404, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 94.232, mean reward: 5.890 [2.993, 11.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.317, 10.100], loss: 8.144459, mae: 1.305923, mean_q: 6.478257
 18634/100000: episode: 405, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 57.997, mean reward: 4.461 [3.618, 5.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.513, 10.100], loss: 2.389341, mae: 1.050716, mean_q: 6.081401
 18642/100000: episode: 406, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 42.629, mean reward: 5.329 [4.226, 7.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.575, 10.100], loss: 1.121509, mae: 0.984800, mean_q: 6.594619
 18656/100000: episode: 407, duration: 0.094s, episode steps: 14, steps per second: 148, episode reward: 74.135, mean reward: 5.295 [4.030, 7.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.065, 10.100], loss: 0.933278, mae: 0.812049, mean_q: 5.958640
 18664/100000: episode: 408, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 53.147, mean reward: 6.643 [4.821, 10.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.443, 10.100], loss: 2.120706, mae: 0.919888, mean_q: 6.063550
 18669/100000: episode: 409, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 16.767, mean reward: 3.353 [2.903, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.935, 10.100], loss: 1.272797, mae: 0.780888, mean_q: 6.022969
 18677/100000: episode: 410, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 41.093, mean reward: 5.137 [4.469, 5.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-2.090, 10.100], loss: 1.027830, mae: 0.810693, mean_q: 6.228615
 18688/100000: episode: 411, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 66.810, mean reward: 6.074 [3.144, 10.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.412, 10.100], loss: 8.441547, mae: 1.162878, mean_q: 6.255833
 18699/100000: episode: 412, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 48.675, mean reward: 4.425 [3.549, 5.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.293, 10.100], loss: 0.855110, mae: 0.762087, mean_q: 5.902796
 18706/100000: episode: 413, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 96.261, mean reward: 13.752 [4.615, 52.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.568, 10.100], loss: 1.506125, mae: 1.004013, mean_q: 6.507590
[Info] FALSIFICATION!
[Info] Levels: [4.6084967, 5.9106226, 9.823883, 14.574873]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.07]
[Info] Error Prob: 7.000000000000002e-05

 18709/100000: episode: 414, duration: 4.673s, episode steps: 3, steps per second: 1, episode reward: 135.655, mean reward: 45.218 [15.903, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.156, 9.695], loss: 30.110008, mae: 1.515744, mean_q: 5.840451
 18809/100000: episode: 415, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 185.356, mean reward: 1.854 [1.460, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.601, 10.098], loss: 5.155590, mae: 1.194826, mean_q: 6.377156
 18909/100000: episode: 416, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.596, mean reward: 1.886 [1.438, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.171, 10.378], loss: 6.296134, mae: 1.150298, mean_q: 6.428168
 19009/100000: episode: 417, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.354, mean reward: 1.954 [1.452, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.877, 10.098], loss: 4.273376, mae: 1.074396, mean_q: 6.391964
 19109/100000: episode: 418, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.427, mean reward: 1.904 [1.459, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.091, 10.217], loss: 3.057040, mae: 1.052249, mean_q: 6.409400
 19209/100000: episode: 419, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 211.459, mean reward: 2.115 [1.462, 5.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.186, 10.192], loss: 2.653893, mae: 1.005333, mean_q: 6.295506
 19309/100000: episode: 420, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 185.817, mean reward: 1.858 [1.448, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.387, 10.253], loss: 3.040291, mae: 1.006193, mean_q: 6.298922
 19409/100000: episode: 421, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.062, mean reward: 1.931 [1.461, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.816, 10.098], loss: 5.250218, mae: 1.143911, mean_q: 6.411566
 19509/100000: episode: 422, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 190.777, mean reward: 1.908 [1.437, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.535, 10.098], loss: 6.030097, mae: 1.167000, mean_q: 6.453866
 19609/100000: episode: 423, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 187.410, mean reward: 1.874 [1.446, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.379, 10.098], loss: 5.178617, mae: 1.111921, mean_q: 6.381016
 19709/100000: episode: 424, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 181.237, mean reward: 1.812 [1.450, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.990, 10.213], loss: 3.386872, mae: 1.001490, mean_q: 6.284428
 19809/100000: episode: 425, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.905, mean reward: 1.849 [1.465, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.736, 10.162], loss: 3.809016, mae: 1.039758, mean_q: 6.476792
 19909/100000: episode: 426, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 197.061, mean reward: 1.971 [1.466, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.262, 10.098], loss: 2.165602, mae: 1.005742, mean_q: 6.173852
 20009/100000: episode: 427, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 200.867, mean reward: 2.009 [1.451, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.346, 10.288], loss: 3.746624, mae: 1.045496, mean_q: 6.284967
 20109/100000: episode: 428, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 187.661, mean reward: 1.877 [1.499, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.844, 10.098], loss: 2.947922, mae: 0.950713, mean_q: 6.250253
 20209/100000: episode: 429, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 182.301, mean reward: 1.823 [1.433, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.953, 10.304], loss: 4.315607, mae: 1.006200, mean_q: 6.113658
 20309/100000: episode: 430, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 195.046, mean reward: 1.950 [1.438, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.466, 10.098], loss: 3.726678, mae: 1.048837, mean_q: 6.206328
 20409/100000: episode: 431, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 212.379, mean reward: 2.124 [1.495, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.901, 10.098], loss: 5.579056, mae: 1.096990, mean_q: 6.199069
 20509/100000: episode: 432, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 201.847, mean reward: 2.018 [1.482, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.034, 10.289], loss: 1.708726, mae: 0.870264, mean_q: 6.064825
 20609/100000: episode: 433, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 194.236, mean reward: 1.942 [1.486, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.010, 10.295], loss: 3.181464, mae: 0.932074, mean_q: 6.071497
 20709/100000: episode: 434, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.942, mean reward: 1.919 [1.463, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.863, 10.255], loss: 1.978166, mae: 0.940622, mean_q: 6.001835
 20809/100000: episode: 435, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 199.391, mean reward: 1.994 [1.517, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.330, 10.270], loss: 2.503831, mae: 0.983667, mean_q: 6.043465
 20909/100000: episode: 436, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 212.389, mean reward: 2.124 [1.459, 8.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.906, 10.098], loss: 4.466764, mae: 1.086648, mean_q: 6.090885
 21009/100000: episode: 437, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 190.967, mean reward: 1.910 [1.492, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.625, 10.128], loss: 5.740788, mae: 1.124859, mean_q: 6.098051
 21109/100000: episode: 438, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 199.599, mean reward: 1.996 [1.455, 5.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.912, 10.182], loss: 2.449720, mae: 1.016140, mean_q: 6.154153
 21209/100000: episode: 439, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 182.035, mean reward: 1.820 [1.492, 2.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.734, 10.169], loss: 6.504349, mae: 1.053479, mean_q: 6.114383
 21309/100000: episode: 440, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 197.745, mean reward: 1.977 [1.464, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.861, 10.311], loss: 2.727794, mae: 0.929606, mean_q: 5.945939
 21409/100000: episode: 441, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 221.161, mean reward: 2.212 [1.507, 5.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.081, 10.098], loss: 5.821520, mae: 1.085907, mean_q: 5.874889
 21509/100000: episode: 442, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 198.007, mean reward: 1.980 [1.483, 4.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.099, 10.130], loss: 3.661132, mae: 0.932678, mean_q: 5.802156
 21609/100000: episode: 443, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 183.656, mean reward: 1.837 [1.470, 4.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.425, 10.098], loss: 2.672211, mae: 0.915141, mean_q: 5.830762
 21709/100000: episode: 444, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 186.591, mean reward: 1.866 [1.446, 2.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.084, 10.098], loss: 4.622694, mae: 1.001239, mean_q: 5.828784
 21809/100000: episode: 445, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 189.705, mean reward: 1.897 [1.495, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.168, 10.098], loss: 3.276007, mae: 0.951196, mean_q: 5.582784
 21909/100000: episode: 446, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 184.451, mean reward: 1.845 [1.440, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.709, 10.289], loss: 5.071703, mae: 0.971332, mean_q: 5.680926
 22009/100000: episode: 447, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 195.198, mean reward: 1.952 [1.464, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.503, 10.098], loss: 4.985894, mae: 0.978516, mean_q: 5.679655
 22109/100000: episode: 448, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 187.698, mean reward: 1.877 [1.447, 4.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.346, 10.098], loss: 3.552279, mae: 0.865271, mean_q: 5.498126
 22209/100000: episode: 449, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 188.910, mean reward: 1.889 [1.478, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.832, 10.098], loss: 5.986168, mae: 0.922920, mean_q: 5.492904
 22309/100000: episode: 450, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 204.327, mean reward: 2.043 [1.472, 4.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.077, 10.098], loss: 4.874856, mae: 0.968156, mean_q: 5.472239
 22409/100000: episode: 451, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 209.762, mean reward: 2.098 [1.561, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.385, 10.098], loss: 4.300091, mae: 0.932741, mean_q: 5.369664
 22509/100000: episode: 452, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 189.187, mean reward: 1.892 [1.465, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.809, 10.229], loss: 3.756083, mae: 0.839786, mean_q: 5.337082
 22609/100000: episode: 453, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 192.902, mean reward: 1.929 [1.440, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.125, 10.098], loss: 4.712155, mae: 0.974036, mean_q: 5.279190
 22709/100000: episode: 454, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 230.039, mean reward: 2.300 [1.622, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.205, 10.427], loss: 2.500892, mae: 0.754231, mean_q: 5.149911
 22809/100000: episode: 455, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.116, mean reward: 1.881 [1.451, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.784, 10.235], loss: 3.587807, mae: 0.749805, mean_q: 5.032201
 22909/100000: episode: 456, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 218.129, mean reward: 2.181 [1.465, 4.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.623, 10.372], loss: 2.507064, mae: 0.671942, mean_q: 4.866546
 23009/100000: episode: 457, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 182.452, mean reward: 1.825 [1.433, 2.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.762, 10.386], loss: 2.382363, mae: 0.671167, mean_q: 4.835974
 23109/100000: episode: 458, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 186.686, mean reward: 1.867 [1.435, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.694, 10.098], loss: 0.425442, mae: 0.464779, mean_q: 4.544392
 23209/100000: episode: 459, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 204.367, mean reward: 2.044 [1.461, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.821, 10.098], loss: 2.258466, mae: 0.562332, mean_q: 4.503068
 23309/100000: episode: 460, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 193.175, mean reward: 1.932 [1.449, 5.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.353, 10.131], loss: 1.207708, mae: 0.510382, mean_q: 4.317679
 23409/100000: episode: 461, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 236.148, mean reward: 2.361 [1.485, 4.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.156, 10.098], loss: 0.734084, mae: 0.445703, mean_q: 4.297511
 23509/100000: episode: 462, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 198.671, mean reward: 1.987 [1.490, 4.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.096, 10.172], loss: 2.718246, mae: 0.468280, mean_q: 4.151441
 23609/100000: episode: 463, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 185.484, mean reward: 1.855 [1.434, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.574, 10.222], loss: 2.651772, mae: 0.481748, mean_q: 4.069694
 23709/100000: episode: 464, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.557, mean reward: 1.966 [1.488, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.593, 10.280], loss: 0.166819, mae: 0.326147, mean_q: 3.921813
 23809/100000: episode: 465, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 177.623, mean reward: 1.776 [1.472, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.345, 10.228], loss: 0.091151, mae: 0.299913, mean_q: 3.875089
 23909/100000: episode: 466, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 205.373, mean reward: 2.054 [1.453, 4.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.720, 10.244], loss: 0.093231, mae: 0.298714, mean_q: 3.864667
 24009/100000: episode: 467, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 203.316, mean reward: 2.033 [1.490, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.319, 10.195], loss: 0.099461, mae: 0.304472, mean_q: 3.883874
 24109/100000: episode: 468, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 185.445, mean reward: 1.854 [1.459, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.736, 10.099], loss: 0.094734, mae: 0.300651, mean_q: 3.874767
 24209/100000: episode: 469, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 211.007, mean reward: 2.110 [1.468, 5.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.542, 10.098], loss: 0.108272, mae: 0.311577, mean_q: 3.880161
 24309/100000: episode: 470, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 179.171, mean reward: 1.792 [1.446, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.601, 10.176], loss: 0.095382, mae: 0.300214, mean_q: 3.888031
 24409/100000: episode: 471, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 195.740, mean reward: 1.957 [1.465, 5.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.204, 10.115], loss: 0.094767, mae: 0.297931, mean_q: 3.882793
 24509/100000: episode: 472, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 191.171, mean reward: 1.912 [1.454, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.280, 10.098], loss: 0.097419, mae: 0.303390, mean_q: 3.890959
 24609/100000: episode: 473, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 195.299, mean reward: 1.953 [1.494, 3.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.643, 10.288], loss: 0.089779, mae: 0.288588, mean_q: 3.886140
 24709/100000: episode: 474, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 187.861, mean reward: 1.879 [1.485, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.062, 10.248], loss: 0.084836, mae: 0.290854, mean_q: 3.868561
 24809/100000: episode: 475, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 209.637, mean reward: 2.096 [1.470, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.869, 10.329], loss: 0.094603, mae: 0.299562, mean_q: 3.861791
 24909/100000: episode: 476, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 213.807, mean reward: 2.138 [1.450, 5.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.761, 10.098], loss: 0.094116, mae: 0.299248, mean_q: 3.885024
 25009/100000: episode: 477, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 184.366, mean reward: 1.844 [1.479, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.634, 10.154], loss: 0.095251, mae: 0.298875, mean_q: 3.895680
 25109/100000: episode: 478, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 212.719, mean reward: 2.127 [1.483, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.887, 10.098], loss: 0.094048, mae: 0.301574, mean_q: 3.875245
 25209/100000: episode: 479, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 188.046, mean reward: 1.880 [1.451, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.418, 10.098], loss: 0.104735, mae: 0.310496, mean_q: 3.896587
 25309/100000: episode: 480, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 181.239, mean reward: 1.812 [1.444, 2.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.851, 10.156], loss: 0.094473, mae: 0.301212, mean_q: 3.905600
 25409/100000: episode: 481, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 200.489, mean reward: 2.005 [1.470, 5.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.732, 10.146], loss: 0.091749, mae: 0.295329, mean_q: 3.905289
 25509/100000: episode: 482, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 209.860, mean reward: 2.099 [1.487, 3.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.897, 10.364], loss: 0.094035, mae: 0.293929, mean_q: 3.892365
 25609/100000: episode: 483, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 183.803, mean reward: 1.838 [1.471, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.175, 10.244], loss: 0.093607, mae: 0.303437, mean_q: 3.878582
 25709/100000: episode: 484, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 211.725, mean reward: 2.117 [1.462, 5.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.471, 10.297], loss: 0.100664, mae: 0.294862, mean_q: 3.893111
 25809/100000: episode: 485, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 187.643, mean reward: 1.876 [1.468, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.885, 10.098], loss: 0.097907, mae: 0.300090, mean_q: 3.891295
 25909/100000: episode: 486, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 181.285, mean reward: 1.813 [1.472, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.773, 10.156], loss: 0.093069, mae: 0.299506, mean_q: 3.893564
 26009/100000: episode: 487, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 253.854, mean reward: 2.539 [1.518, 6.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.105, 10.098], loss: 0.094734, mae: 0.297358, mean_q: 3.894403
 26109/100000: episode: 488, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 203.504, mean reward: 2.035 [1.481, 6.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.035, 10.286], loss: 0.085783, mae: 0.288963, mean_q: 3.894483
 26209/100000: episode: 489, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 210.212, mean reward: 2.102 [1.479, 4.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.651, 10.098], loss: 0.106911, mae: 0.316486, mean_q: 3.936432
 26309/100000: episode: 490, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 229.538, mean reward: 2.295 [1.490, 4.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.497, 10.098], loss: 0.100382, mae: 0.303943, mean_q: 3.909896
 26409/100000: episode: 491, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 196.856, mean reward: 1.969 [1.485, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.694, 10.098], loss: 0.099683, mae: 0.312041, mean_q: 3.920999
 26509/100000: episode: 492, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 213.656, mean reward: 2.137 [1.511, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.194, 10.272], loss: 0.105480, mae: 0.318925, mean_q: 3.940515
 26609/100000: episode: 493, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 183.762, mean reward: 1.838 [1.456, 2.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.512, 10.157], loss: 0.097321, mae: 0.302405, mean_q: 3.909953
 26709/100000: episode: 494, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 193.116, mean reward: 1.931 [1.447, 2.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.660, 10.154], loss: 0.088754, mae: 0.293812, mean_q: 3.920969
 26809/100000: episode: 495, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 182.772, mean reward: 1.828 [1.446, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.766, 10.223], loss: 0.099464, mae: 0.311810, mean_q: 3.908583
 26909/100000: episode: 496, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 192.218, mean reward: 1.922 [1.440, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.922, 10.098], loss: 0.096106, mae: 0.297455, mean_q: 3.920979
 27009/100000: episode: 497, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 186.788, mean reward: 1.868 [1.458, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.484, 10.377], loss: 0.094644, mae: 0.304228, mean_q: 3.928344
 27109/100000: episode: 498, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 191.805, mean reward: 1.918 [1.464, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.046, 10.098], loss: 0.104251, mae: 0.315933, mean_q: 3.937207
 27209/100000: episode: 499, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 203.645, mean reward: 2.036 [1.452, 5.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.897, 10.098], loss: 0.094867, mae: 0.305370, mean_q: 3.938114
 27309/100000: episode: 500, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.687, mean reward: 1.857 [1.450, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.765, 10.166], loss: 0.095077, mae: 0.297580, mean_q: 3.918066
 27409/100000: episode: 501, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 178.065, mean reward: 1.781 [1.454, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.523, 10.098], loss: 0.102078, mae: 0.309376, mean_q: 3.933032
 27509/100000: episode: 502, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 182.272, mean reward: 1.823 [1.443, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.367, 10.346], loss: 0.094707, mae: 0.295493, mean_q: 3.910268
 27609/100000: episode: 503, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 195.825, mean reward: 1.958 [1.439, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.677, 10.256], loss: 0.097269, mae: 0.299235, mean_q: 3.918770
 27709/100000: episode: 504, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 232.633, mean reward: 2.326 [1.465, 3.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.499, 10.479], loss: 0.095353, mae: 0.298311, mean_q: 3.915158
 27809/100000: episode: 505, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 182.754, mean reward: 1.828 [1.440, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.872, 10.098], loss: 0.099291, mae: 0.300702, mean_q: 3.921899
 27909/100000: episode: 506, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 190.026, mean reward: 1.900 [1.456, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.473, 10.428], loss: 0.108294, mae: 0.318614, mean_q: 3.913845
 28009/100000: episode: 507, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 186.330, mean reward: 1.863 [1.482, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.280, 10.148], loss: 0.106338, mae: 0.315887, mean_q: 3.927840
 28109/100000: episode: 508, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 182.557, mean reward: 1.826 [1.476, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.526, 10.280], loss: 0.098589, mae: 0.304000, mean_q: 3.901554
 28209/100000: episode: 509, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 185.837, mean reward: 1.858 [1.477, 3.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.818, 10.098], loss: 0.108235, mae: 0.311747, mean_q: 3.909026
 28309/100000: episode: 510, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 208.846, mean reward: 2.088 [1.489, 5.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.391, 10.098], loss: 0.098237, mae: 0.307196, mean_q: 3.901989
 28409/100000: episode: 511, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 186.682, mean reward: 1.867 [1.435, 4.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.650, 10.124], loss: 0.108393, mae: 0.314806, mean_q: 3.887142
 28509/100000: episode: 512, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 185.787, mean reward: 1.858 [1.476, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.359, 10.141], loss: 0.084671, mae: 0.294032, mean_q: 3.877075
 28609/100000: episode: 513, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 194.655, mean reward: 1.947 [1.471, 11.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.128, 10.098], loss: 0.121062, mae: 0.321563, mean_q: 3.894748
[Info] 1-TH LEVEL FOUND: 5.115283012390137, Considering 10/90 traces
 28709/100000: episode: 514, duration: 4.726s, episode steps: 100, steps per second: 21, episode reward: 193.029, mean reward: 1.930 [1.445, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.001, 10.098], loss: 0.102994, mae: 0.308748, mean_q: 3.871686
 28737/100000: episode: 515, duration: 0.146s, episode steps: 28, steps per second: 191, episode reward: 65.814, mean reward: 2.351 [1.819, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.066, 10.100], loss: 0.112466, mae: 0.324904, mean_q: 3.836701
 28761/100000: episode: 516, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 52.988, mean reward: 2.208 [1.581, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.159, 10.100], loss: 0.107412, mae: 0.327203, mean_q: 3.913100
 28786/100000: episode: 517, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 65.155, mean reward: 2.606 [2.059, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.499, 10.100], loss: 0.101241, mae: 0.311637, mean_q: 3.899166
 28822/100000: episode: 518, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 76.285, mean reward: 2.119 [1.472, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.216, 10.100], loss: 0.108611, mae: 0.312191, mean_q: 3.910445
 28831/100000: episode: 519, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 26.953, mean reward: 2.995 [2.198, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.237, 10.100], loss: 0.087262, mae: 0.301890, mean_q: 3.932065
 28867/100000: episode: 520, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 96.878, mean reward: 2.691 [1.930, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.182, 10.100], loss: 0.098823, mae: 0.299243, mean_q: 3.876020
 28895/100000: episode: 521, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 68.818, mean reward: 2.458 [1.726, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.603, 10.100], loss: 0.085073, mae: 0.288629, mean_q: 3.853053
 28913/100000: episode: 522, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 67.672, mean reward: 3.760 [2.641, 5.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.782, 10.100], loss: 0.116019, mae: 0.314198, mean_q: 3.894591
 28931/100000: episode: 523, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 51.610, mean reward: 2.867 [1.983, 8.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.369, 10.100], loss: 0.149617, mae: 0.331060, mean_q: 3.936112
 28940/100000: episode: 524, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 21.469, mean reward: 2.385 [1.733, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.631, 10.100], loss: 0.095010, mae: 0.309753, mean_q: 3.941215
 28964/100000: episode: 525, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 54.708, mean reward: 2.280 [1.483, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.472, 10.100], loss: 0.129397, mae: 0.325017, mean_q: 3.916962
 28982/100000: episode: 526, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 59.177, mean reward: 3.288 [2.677, 4.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.280, 10.100], loss: 0.164656, mae: 0.372634, mean_q: 3.966813
 29025/100000: episode: 527, duration: 0.223s, episode steps: 43, steps per second: 193, episode reward: 126.177, mean reward: 2.934 [1.623, 5.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.966, 10.100], loss: 0.127870, mae: 0.359151, mean_q: 3.950033
 29043/100000: episode: 528, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 51.799, mean reward: 2.878 [1.918, 4.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.303, 10.100], loss: 0.119734, mae: 0.329636, mean_q: 3.959612
 29079/100000: episode: 529, duration: 0.183s, episode steps: 36, steps per second: 196, episode reward: 86.017, mean reward: 2.389 [1.801, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.425, 10.100], loss: 0.108742, mae: 0.317262, mean_q: 3.934702
 29115/100000: episode: 530, duration: 0.195s, episode steps: 36, steps per second: 185, episode reward: 80.375, mean reward: 2.233 [1.731, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.969, 10.100], loss: 0.117681, mae: 0.325346, mean_q: 3.962585
 29139/100000: episode: 531, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 70.912, mean reward: 2.955 [1.890, 8.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.193, 10.100], loss: 0.142328, mae: 0.350328, mean_q: 4.019918
 29167/100000: episode: 532, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 64.616, mean reward: 2.308 [1.543, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.350, 10.268], loss: 0.120810, mae: 0.329578, mean_q: 3.938078
 29195/100000: episode: 533, duration: 0.168s, episode steps: 28, steps per second: 167, episode reward: 65.521, mean reward: 2.340 [1.638, 5.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.090, 10.142], loss: 0.128559, mae: 0.358050, mean_q: 3.924257
 29213/100000: episode: 534, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 45.324, mean reward: 2.518 [2.120, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.902, 10.100], loss: 0.128146, mae: 0.362068, mean_q: 3.992221
 29231/100000: episode: 535, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 59.910, mean reward: 3.328 [2.777, 4.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.888, 10.100], loss: 0.101113, mae: 0.311533, mean_q: 3.993165
 29240/100000: episode: 536, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 20.528, mean reward: 2.281 [1.842, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.264, 10.100], loss: 0.091743, mae: 0.294985, mean_q: 3.985491
 29258/100000: episode: 537, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 53.464, mean reward: 2.970 [2.235, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.321, 10.100], loss: 0.123306, mae: 0.330091, mean_q: 4.017515
 29301/100000: episode: 538, duration: 0.226s, episode steps: 43, steps per second: 190, episode reward: 118.128, mean reward: 2.747 [1.573, 5.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.793, 10.100], loss: 0.102288, mae: 0.308587, mean_q: 3.983523
 29329/100000: episode: 539, duration: 0.145s, episode steps: 28, steps per second: 192, episode reward: 62.892, mean reward: 2.246 [1.481, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.076, 10.110], loss: 0.087782, mae: 0.297543, mean_q: 4.014941
 29365/100000: episode: 540, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 82.106, mean reward: 2.281 [1.839, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.162, 10.100], loss: 0.101648, mae: 0.314408, mean_q: 3.947445
 29389/100000: episode: 541, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 61.510, mean reward: 2.563 [1.858, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.208, 10.100], loss: 0.135433, mae: 0.356369, mean_q: 3.999906
 29432/100000: episode: 542, duration: 0.212s, episode steps: 43, steps per second: 203, episode reward: 103.898, mean reward: 2.416 [1.480, 3.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-1.046, 10.100], loss: 0.133128, mae: 0.351120, mean_q: 4.045540
 29456/100000: episode: 543, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 56.048, mean reward: 2.335 [1.809, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.440, 10.100], loss: 0.127383, mae: 0.336891, mean_q: 3.966366
 29492/100000: episode: 544, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 89.261, mean reward: 2.479 [1.864, 4.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.283, 10.100], loss: 0.122846, mae: 0.347497, mean_q: 4.036711
 29528/100000: episode: 545, duration: 0.205s, episode steps: 36, steps per second: 175, episode reward: 87.287, mean reward: 2.425 [1.752, 4.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.168, 10.100], loss: 0.098774, mae: 0.319159, mean_q: 4.052389
 29556/100000: episode: 546, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 74.379, mean reward: 2.656 [1.848, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.164, 10.100], loss: 0.152059, mae: 0.334939, mean_q: 4.064401
 29574/100000: episode: 547, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 44.191, mean reward: 2.455 [1.985, 3.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.389, 10.100], loss: 0.149481, mae: 0.354833, mean_q: 3.968585
 29599/100000: episode: 548, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 78.121, mean reward: 3.125 [2.320, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.470, 10.100], loss: 0.120134, mae: 0.341512, mean_q: 4.071652
 29627/100000: episode: 549, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 68.982, mean reward: 2.464 [1.764, 5.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.774, 10.100], loss: 0.117625, mae: 0.335066, mean_q: 4.051204
 29655/100000: episode: 550, duration: 0.136s, episode steps: 28, steps per second: 206, episode reward: 65.914, mean reward: 2.354 [1.951, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.339, 10.100], loss: 0.170307, mae: 0.364788, mean_q: 4.059193
 29679/100000: episode: 551, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 71.307, mean reward: 2.971 [2.089, 5.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.255, 10.100], loss: 0.167589, mae: 0.369193, mean_q: 4.052457
 29703/100000: episode: 552, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 64.776, mean reward: 2.699 [1.959, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.189, 10.100], loss: 0.159046, mae: 0.377053, mean_q: 4.068921
 29728/100000: episode: 553, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 62.486, mean reward: 2.499 [1.882, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.487, 10.100], loss: 0.169406, mae: 0.394607, mean_q: 4.124171
 29756/100000: episode: 554, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 69.150, mean reward: 2.470 [1.746, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.435, 10.100], loss: 0.178770, mae: 0.355421, mean_q: 4.087039
 29780/100000: episode: 555, duration: 0.115s, episode steps: 24, steps per second: 210, episode reward: 70.488, mean reward: 2.937 [1.873, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.291, 10.100], loss: 0.130419, mae: 0.359278, mean_q: 4.089571
 29804/100000: episode: 556, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 61.646, mean reward: 2.569 [1.849, 5.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.694, 10.100], loss: 0.158815, mae: 0.382588, mean_q: 4.118957
 29828/100000: episode: 557, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 50.765, mean reward: 2.115 [1.442, 3.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.363, 10.161], loss: 0.119169, mae: 0.325160, mean_q: 4.088389
 29853/100000: episode: 558, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 144.070, mean reward: 5.763 [3.016, 14.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.533, 10.100], loss: 0.123305, mae: 0.351478, mean_q: 4.149856
 29889/100000: episode: 559, duration: 0.178s, episode steps: 36, steps per second: 202, episode reward: 122.101, mean reward: 3.392 [2.120, 4.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.263, 10.100], loss: 0.222443, mae: 0.402911, mean_q: 4.121663
 29913/100000: episode: 560, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 77.234, mean reward: 3.218 [2.422, 4.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.401, 10.100], loss: 0.228515, mae: 0.434141, mean_q: 4.157510
 29937/100000: episode: 561, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 61.598, mean reward: 2.567 [1.920, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.433, 10.100], loss: 0.254638, mae: 0.437669, mean_q: 4.176611
 29965/100000: episode: 562, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 62.431, mean reward: 2.230 [1.742, 4.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.815, 10.100], loss: 0.157873, mae: 0.382734, mean_q: 4.136704
 29989/100000: episode: 563, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 45.474, mean reward: 1.895 [1.551, 2.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.453, 10.184], loss: 0.156075, mae: 0.358030, mean_q: 4.164356
 30025/100000: episode: 564, duration: 0.207s, episode steps: 36, steps per second: 174, episode reward: 130.211, mean reward: 3.617 [1.929, 7.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.661, 10.100], loss: 0.173262, mae: 0.379130, mean_q: 4.171068
 30053/100000: episode: 565, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 63.299, mean reward: 2.261 [1.750, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.036, 10.100], loss: 0.159978, mae: 0.375484, mean_q: 4.155145
 30062/100000: episode: 566, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 24.565, mean reward: 2.729 [2.167, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.211, 10.100], loss: 0.143439, mae: 0.368601, mean_q: 4.163093
 30090/100000: episode: 567, duration: 0.152s, episode steps: 28, steps per second: 185, episode reward: 64.508, mean reward: 2.304 [1.654, 3.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.209, 10.100], loss: 0.186501, mae: 0.382112, mean_q: 4.196453
 30126/100000: episode: 568, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 117.611, mean reward: 3.267 [2.129, 4.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.794, 10.100], loss: 0.143984, mae: 0.348253, mean_q: 4.198763
 30169/100000: episode: 569, duration: 0.215s, episode steps: 43, steps per second: 200, episode reward: 156.880, mean reward: 3.648 [2.619, 6.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.515, 10.100], loss: 0.148976, mae: 0.373561, mean_q: 4.226130
[Info] FALSIFICATION!
[Info] Levels: [5.115283, 6.581467]
[Info] Cond. Prob: [0.1, 0.1]
[Info] Error Prob: 0.010000000000000002

 30202/100000: episode: 570, duration: 4.643s, episode steps: 33, steps per second: 7, episode reward: 208.443, mean reward: 6.316 [2.521, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-1.079, 9.864], loss: 0.216465, mae: 0.416988, mean_q: 4.233411
 30302/100000: episode: 571, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 181.670, mean reward: 1.817 [1.445, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.283, 10.098], loss: 1.604114, mae: 0.498786, mean_q: 4.265432
 30402/100000: episode: 572, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 184.190, mean reward: 1.842 [1.442, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.462, 10.466], loss: 1.611872, mae: 0.499290, mean_q: 4.321331
 30502/100000: episode: 573, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 188.568, mean reward: 1.886 [1.475, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.827, 10.098], loss: 0.205943, mae: 0.402229, mean_q: 4.303402
 30602/100000: episode: 574, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 177.887, mean reward: 1.779 [1.457, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.221, 10.246], loss: 1.710034, mae: 0.531121, mean_q: 4.289171
 30702/100000: episode: 575, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 183.477, mean reward: 1.835 [1.451, 2.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.453, 10.157], loss: 2.941331, mae: 0.512322, mean_q: 4.324733
 30802/100000: episode: 576, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 195.086, mean reward: 1.951 [1.456, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.934, 10.452], loss: 2.867615, mae: 0.502138, mean_q: 4.299725
 30902/100000: episode: 577, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 198.788, mean reward: 1.988 [1.450, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.439, 10.098], loss: 1.532363, mae: 0.458820, mean_q: 4.296828
 31002/100000: episode: 578, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.081, mean reward: 1.941 [1.484, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.411, 10.183], loss: 1.567486, mae: 0.460925, mean_q: 4.280114
 31102/100000: episode: 579, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 189.335, mean reward: 1.893 [1.451, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.278, 10.280], loss: 0.185618, mae: 0.379222, mean_q: 4.288898
 31202/100000: episode: 580, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.078, mean reward: 1.831 [1.452, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.462, 10.098], loss: 1.536127, mae: 0.433310, mean_q: 4.298414
 31302/100000: episode: 581, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 200.700, mean reward: 2.007 [1.461, 10.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.755, 10.201], loss: 0.175116, mae: 0.367958, mean_q: 4.248228
 31402/100000: episode: 582, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 201.355, mean reward: 2.014 [1.494, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.182, 10.098], loss: 0.171242, mae: 0.375635, mean_q: 4.183714
 31502/100000: episode: 583, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 187.195, mean reward: 1.872 [1.455, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.095, 10.098], loss: 0.183800, mae: 0.381825, mean_q: 4.261047
 31602/100000: episode: 584, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 192.540, mean reward: 1.925 [1.463, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.543, 10.138], loss: 1.537716, mae: 0.436538, mean_q: 4.242477
 31702/100000: episode: 585, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 186.059, mean reward: 1.861 [1.461, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.865, 10.098], loss: 0.175847, mae: 0.380870, mean_q: 4.213859
 31802/100000: episode: 586, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 215.742, mean reward: 2.157 [1.456, 12.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.869, 10.098], loss: 1.680320, mae: 0.514739, mean_q: 4.258476
 31902/100000: episode: 587, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 195.177, mean reward: 1.952 [1.461, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.856, 10.279], loss: 2.938283, mae: 0.525985, mean_q: 4.339807
 32002/100000: episode: 588, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 195.556, mean reward: 1.956 [1.431, 4.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.529, 10.098], loss: 0.233691, mae: 0.400782, mean_q: 4.258410
 32102/100000: episode: 589, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 175.643, mean reward: 1.756 [1.448, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.862, 10.102], loss: 1.506469, mae: 0.415057, mean_q: 4.279673
 32202/100000: episode: 590, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 189.236, mean reward: 1.892 [1.459, 3.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.273, 10.197], loss: 1.572558, mae: 0.473820, mean_q: 4.244869
 32302/100000: episode: 591, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 205.527, mean reward: 2.055 [1.445, 4.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.494, 10.172], loss: 0.170716, mae: 0.374943, mean_q: 4.215520
 32402/100000: episode: 592, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 186.886, mean reward: 1.869 [1.485, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.470, 10.098], loss: 0.245549, mae: 0.412085, mean_q: 4.239823
 32502/100000: episode: 593, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 179.312, mean reward: 1.793 [1.434, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.165, 10.120], loss: 1.589535, mae: 0.454693, mean_q: 4.270056
 32602/100000: episode: 594, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.082, mean reward: 1.811 [1.456, 2.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.003, 10.098], loss: 1.565033, mae: 0.473929, mean_q: 4.254488
 32702/100000: episode: 595, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 202.035, mean reward: 2.020 [1.448, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.634, 10.098], loss: 1.562237, mae: 0.444414, mean_q: 4.233927
 32802/100000: episode: 596, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 194.394, mean reward: 1.944 [1.449, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.739, 10.098], loss: 0.197906, mae: 0.390077, mean_q: 4.217764
 32902/100000: episode: 597, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 195.752, mean reward: 1.958 [1.451, 4.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.036, 10.325], loss: 0.175909, mae: 0.371523, mean_q: 4.225013
 33002/100000: episode: 598, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 193.805, mean reward: 1.938 [1.464, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.641, 10.203], loss: 1.572765, mae: 0.457846, mean_q: 4.226612
 33102/100000: episode: 599, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 191.701, mean reward: 1.917 [1.464, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.070, 10.277], loss: 0.154850, mae: 0.363684, mean_q: 4.172548
 33202/100000: episode: 600, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 198.541, mean reward: 1.985 [1.465, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.731, 10.120], loss: 0.183437, mae: 0.369304, mean_q: 4.207592
 33302/100000: episode: 601, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 182.062, mean reward: 1.821 [1.446, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.976, 10.098], loss: 1.594264, mae: 0.482328, mean_q: 4.262555
 33402/100000: episode: 602, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 192.463, mean reward: 1.925 [1.497, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.900, 10.098], loss: 1.547434, mae: 0.442475, mean_q: 4.278612
 33502/100000: episode: 603, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 200.917, mean reward: 2.009 [1.481, 4.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.550, 10.245], loss: 0.192220, mae: 0.386615, mean_q: 4.205651
 33602/100000: episode: 604, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 204.405, mean reward: 2.044 [1.473, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.678, 10.098], loss: 0.222489, mae: 0.397312, mean_q: 4.266082
 33702/100000: episode: 605, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 217.457, mean reward: 2.175 [1.452, 4.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.890, 10.098], loss: 2.993755, mae: 0.568906, mean_q: 4.298100
 33802/100000: episode: 606, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 197.605, mean reward: 1.976 [1.474, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.569, 10.098], loss: 0.168178, mae: 0.373699, mean_q: 4.169905
 33902/100000: episode: 607, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 204.018, mean reward: 2.040 [1.489, 5.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.017, 10.225], loss: 0.180120, mae: 0.375561, mean_q: 4.200966
 34002/100000: episode: 608, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 184.716, mean reward: 1.847 [1.445, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.010, 10.098], loss: 0.140779, mae: 0.350748, mean_q: 4.164064
 34102/100000: episode: 609, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 193.429, mean reward: 1.934 [1.470, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.500, 10.098], loss: 0.178156, mae: 0.359227, mean_q: 4.147833
 34202/100000: episode: 610, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 204.088, mean reward: 2.041 [1.475, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.148, 10.189], loss: 1.574348, mae: 0.453931, mean_q: 4.163261
 34302/100000: episode: 611, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 198.575, mean reward: 1.986 [1.455, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.941, 10.278], loss: 1.548364, mae: 0.452972, mean_q: 4.129091
 34402/100000: episode: 612, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 188.617, mean reward: 1.886 [1.487, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.924, 10.192], loss: 0.157938, mae: 0.350976, mean_q: 4.085673
 34502/100000: episode: 613, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 199.923, mean reward: 1.999 [1.468, 7.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.194, 10.161], loss: 2.870838, mae: 0.508279, mean_q: 4.092920
 34602/100000: episode: 614, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 225.973, mean reward: 2.260 [1.523, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.213, 10.514], loss: 0.134680, mae: 0.343846, mean_q: 4.028305
 34702/100000: episode: 615, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 211.984, mean reward: 2.120 [1.547, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.383, 10.264], loss: 0.154511, mae: 0.336214, mean_q: 3.986996
 34802/100000: episode: 616, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 193.153, mean reward: 1.932 [1.459, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.831, 10.203], loss: 0.159452, mae: 0.335698, mean_q: 3.973038
 34902/100000: episode: 617, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 185.147, mean reward: 1.851 [1.471, 4.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.286, 10.098], loss: 1.492739, mae: 0.414466, mean_q: 3.978207
 35002/100000: episode: 618, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 181.901, mean reward: 1.819 [1.465, 2.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.556, 10.226], loss: 0.127975, mae: 0.320285, mean_q: 3.906756
 35102/100000: episode: 619, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.912, mean reward: 1.859 [1.432, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.252, 10.144], loss: 1.481598, mae: 0.390901, mean_q: 3.909994
 35202/100000: episode: 620, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 182.090, mean reward: 1.821 [1.465, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.015, 10.138], loss: 0.130106, mae: 0.317289, mean_q: 3.844598
 35302/100000: episode: 621, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 194.935, mean reward: 1.949 [1.442, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.634, 10.098], loss: 0.123568, mae: 0.300175, mean_q: 3.813807
 35402/100000: episode: 622, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 193.087, mean reward: 1.931 [1.464, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.703, 10.098], loss: 0.146411, mae: 0.308623, mean_q: 3.814400
 35502/100000: episode: 623, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 178.972, mean reward: 1.790 [1.468, 2.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.175, 10.098], loss: 0.124338, mae: 0.310558, mean_q: 3.828532
 35602/100000: episode: 624, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 198.151, mean reward: 1.982 [1.470, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.196, 10.098], loss: 0.123518, mae: 0.313558, mean_q: 3.822639
 35702/100000: episode: 625, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 184.502, mean reward: 1.845 [1.464, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.490, 10.098], loss: 0.120909, mae: 0.306805, mean_q: 3.830504
 35802/100000: episode: 626, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 226.705, mean reward: 2.267 [1.474, 4.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.767, 10.098], loss: 0.087196, mae: 0.289545, mean_q: 3.826925
 35902/100000: episode: 627, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 188.975, mean reward: 1.890 [1.451, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.486, 10.204], loss: 0.125159, mae: 0.311833, mean_q: 3.842572
 36002/100000: episode: 628, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 207.256, mean reward: 2.073 [1.483, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.657, 10.432], loss: 0.127468, mae: 0.309587, mean_q: 3.838027
 36102/100000: episode: 629, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.946, mean reward: 1.879 [1.459, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.998, 10.192], loss: 0.100610, mae: 0.305917, mean_q: 3.858033
 36202/100000: episode: 630, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 184.876, mean reward: 1.849 [1.442, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.363, 10.098], loss: 0.118079, mae: 0.310581, mean_q: 3.856365
 36302/100000: episode: 631, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 181.714, mean reward: 1.817 [1.448, 3.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.257, 10.098], loss: 0.112041, mae: 0.306231, mean_q: 3.844000
 36402/100000: episode: 632, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 189.275, mean reward: 1.893 [1.452, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.131, 10.098], loss: 0.114288, mae: 0.314581, mean_q: 3.844825
 36502/100000: episode: 633, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 202.899, mean reward: 2.029 [1.445, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.329, 10.291], loss: 0.098087, mae: 0.306071, mean_q: 3.860225
 36602/100000: episode: 634, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 182.733, mean reward: 1.827 [1.500, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.455, 10.204], loss: 0.121002, mae: 0.306724, mean_q: 3.844847
 36702/100000: episode: 635, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 191.907, mean reward: 1.919 [1.476, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.069, 10.119], loss: 0.101738, mae: 0.304049, mean_q: 3.843308
 36802/100000: episode: 636, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.593, mean reward: 1.886 [1.468, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.893, 10.098], loss: 0.103522, mae: 0.306803, mean_q: 3.860524
 36902/100000: episode: 637, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 182.608, mean reward: 1.826 [1.478, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.352, 10.132], loss: 0.087482, mae: 0.291687, mean_q: 3.856538
 37002/100000: episode: 638, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 197.429, mean reward: 1.974 [1.481, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.700, 10.098], loss: 0.088916, mae: 0.291995, mean_q: 3.849662
 37102/100000: episode: 639, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 182.874, mean reward: 1.829 [1.430, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.938, 10.321], loss: 0.098048, mae: 0.311143, mean_q: 3.857448
 37202/100000: episode: 640, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 189.740, mean reward: 1.897 [1.494, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.971, 10.301], loss: 0.088864, mae: 0.293698, mean_q: 3.857106
 37302/100000: episode: 641, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 184.923, mean reward: 1.849 [1.441, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.299, 10.180], loss: 0.094220, mae: 0.299162, mean_q: 3.835708
 37402/100000: episode: 642, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 179.576, mean reward: 1.796 [1.459, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.491, 10.099], loss: 0.083219, mae: 0.288978, mean_q: 3.815307
 37502/100000: episode: 643, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 222.909, mean reward: 2.229 [1.441, 4.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.023, 10.175], loss: 0.093687, mae: 0.301062, mean_q: 3.846724
 37602/100000: episode: 644, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 181.673, mean reward: 1.817 [1.465, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.558, 10.098], loss: 0.094075, mae: 0.307814, mean_q: 3.858752
 37702/100000: episode: 645, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 202.043, mean reward: 2.020 [1.538, 2.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.902, 10.358], loss: 0.096488, mae: 0.307057, mean_q: 3.857731
 37802/100000: episode: 646, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.575, mean reward: 1.886 [1.434, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.132, 10.187], loss: 0.083336, mae: 0.289582, mean_q: 3.839138
 37902/100000: episode: 647, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 191.602, mean reward: 1.916 [1.448, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.474, 10.119], loss: 0.089563, mae: 0.295222, mean_q: 3.836095
 38002/100000: episode: 648, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 199.053, mean reward: 1.991 [1.496, 5.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.600, 10.189], loss: 0.090289, mae: 0.298475, mean_q: 3.845201
 38102/100000: episode: 649, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.950, mean reward: 1.919 [1.443, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.897, 10.098], loss: 0.094456, mae: 0.301651, mean_q: 3.850623
 38202/100000: episode: 650, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 184.514, mean reward: 1.845 [1.467, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.921, 10.142], loss: 0.084989, mae: 0.295404, mean_q: 3.821308
 38302/100000: episode: 651, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 191.008, mean reward: 1.910 [1.456, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.236, 10.229], loss: 0.078752, mae: 0.282272, mean_q: 3.817010
 38402/100000: episode: 652, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.149, mean reward: 1.831 [1.446, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.398, 10.098], loss: 0.096618, mae: 0.308958, mean_q: 3.827678
 38502/100000: episode: 653, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 189.510, mean reward: 1.895 [1.453, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.190, 10.098], loss: 0.098529, mae: 0.308332, mean_q: 3.847349
 38602/100000: episode: 654, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.656, mean reward: 1.987 [1.448, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.525, 10.098], loss: 0.087025, mae: 0.293821, mean_q: 3.833440
 38702/100000: episode: 655, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 182.950, mean reward: 1.830 [1.450, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.024, 10.191], loss: 0.089342, mae: 0.291518, mean_q: 3.821969
 38802/100000: episode: 656, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 194.767, mean reward: 1.948 [1.452, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.899, 10.098], loss: 0.089938, mae: 0.293325, mean_q: 3.806259
 38902/100000: episode: 657, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 226.504, mean reward: 2.265 [1.513, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.247, 10.270], loss: 0.086209, mae: 0.291317, mean_q: 3.821634
 39002/100000: episode: 658, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 189.919, mean reward: 1.899 [1.453, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.968, 10.238], loss: 0.081985, mae: 0.290999, mean_q: 3.842556
 39102/100000: episode: 659, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.411, mean reward: 1.884 [1.509, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.406, 10.098], loss: 0.087489, mae: 0.293294, mean_q: 3.799365
 39202/100000: episode: 660, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 202.238, mean reward: 2.022 [1.442, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.699, 10.098], loss: 0.088524, mae: 0.297360, mean_q: 3.811141
 39302/100000: episode: 661, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 235.460, mean reward: 2.355 [1.497, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.549, 10.271], loss: 0.088293, mae: 0.291598, mean_q: 3.808187
 39402/100000: episode: 662, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 202.117, mean reward: 2.021 [1.434, 14.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.738, 10.098], loss: 0.090791, mae: 0.299839, mean_q: 3.816183
 39502/100000: episode: 663, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 185.702, mean reward: 1.857 [1.454, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.523, 10.161], loss: 0.110747, mae: 0.305623, mean_q: 3.823081
 39602/100000: episode: 664, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 195.166, mean reward: 1.952 [1.462, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.752, 10.195], loss: 0.106725, mae: 0.302562, mean_q: 3.830461
 39702/100000: episode: 665, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.967, mean reward: 1.940 [1.453, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.235, 10.098], loss: 0.109731, mae: 0.305005, mean_q: 3.799677
 39802/100000: episode: 666, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 187.578, mean reward: 1.876 [1.468, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.326, 10.098], loss: 0.091101, mae: 0.308402, mean_q: 3.798440
 39902/100000: episode: 667, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 202.997, mean reward: 2.030 [1.483, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.365, 10.274], loss: 0.110272, mae: 0.308568, mean_q: 3.807501
 40002/100000: episode: 668, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 204.949, mean reward: 2.049 [1.456, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.640, 10.098], loss: 0.086038, mae: 0.296846, mean_q: 3.800750
 40102/100000: episode: 669, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 177.332, mean reward: 1.773 [1.462, 2.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.256, 10.196], loss: 0.090080, mae: 0.306835, mean_q: 3.841499
[Info] 1-TH LEVEL FOUND: 6.258990287780762, Considering 10/90 traces
 40202/100000: episode: 670, duration: 4.757s, episode steps: 100, steps per second: 21, episode reward: 182.267, mean reward: 1.823 [1.455, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.019, 10.098], loss: 0.137852, mae: 0.317160, mean_q: 3.837931
 40272/100000: episode: 671, duration: 0.374s, episode steps: 70, steps per second: 187, episode reward: 144.445, mean reward: 2.064 [1.463, 5.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.733 [-1.399, 10.297], loss: 0.096419, mae: 0.307922, mean_q: 3.816931
 40342/100000: episode: 672, duration: 0.366s, episode steps: 70, steps per second: 191, episode reward: 144.785, mean reward: 2.068 [1.473, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.736 [-1.014, 10.100], loss: 0.085663, mae: 0.293685, mean_q: 3.819592
 40422/100000: episode: 673, duration: 0.413s, episode steps: 80, steps per second: 194, episode reward: 185.168, mean reward: 2.315 [1.516, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.636 [-0.956, 10.353], loss: 0.104363, mae: 0.318717, mean_q: 3.852038
 40502/100000: episode: 674, duration: 0.399s, episode steps: 80, steps per second: 200, episode reward: 162.923, mean reward: 2.037 [1.505, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.648 [-0.560, 10.329], loss: 0.106018, mae: 0.319475, mean_q: 3.830740
 40517/100000: episode: 675, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 45.056, mean reward: 3.004 [2.152, 5.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.405, 10.100], loss: 0.122899, mae: 0.336665, mean_q: 3.839452
 40597/100000: episode: 676, duration: 0.428s, episode steps: 80, steps per second: 187, episode reward: 143.958, mean reward: 1.799 [1.438, 2.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.652 [-0.984, 10.281], loss: 0.148863, mae: 0.334186, mean_q: 3.854347
 40612/100000: episode: 677, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 44.757, mean reward: 2.984 [2.264, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.347, 10.100], loss: 0.101603, mae: 0.317144, mean_q: 3.888760
 40692/100000: episode: 678, duration: 0.392s, episode steps: 80, steps per second: 204, episode reward: 143.546, mean reward: 1.794 [1.432, 2.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.639 [-1.854, 10.100], loss: 0.113619, mae: 0.312118, mean_q: 3.856225
 40769/100000: episode: 679, duration: 0.409s, episode steps: 77, steps per second: 188, episode reward: 171.345, mean reward: 2.225 [1.434, 4.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.675 [-1.283, 10.520], loss: 0.151995, mae: 0.324222, mean_q: 3.854607
 40784/100000: episode: 680, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 39.723, mean reward: 2.648 [2.156, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.290, 10.100], loss: 0.110865, mae: 0.327325, mean_q: 3.818258
 40864/100000: episode: 681, duration: 0.402s, episode steps: 80, steps per second: 199, episode reward: 142.705, mean reward: 1.784 [1.454, 2.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.649 [-0.925, 10.320], loss: 0.101548, mae: 0.315819, mean_q: 3.853770
 40933/100000: episode: 682, duration: 0.357s, episode steps: 69, steps per second: 194, episode reward: 130.120, mean reward: 1.886 [1.461, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.739 [-0.982, 10.100], loss: 0.095157, mae: 0.307967, mean_q: 3.831071
 41013/100000: episode: 683, duration: 0.425s, episode steps: 80, steps per second: 188, episode reward: 161.787, mean reward: 2.022 [1.444, 4.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.638 [-0.980, 10.100], loss: 0.100976, mae: 0.310065, mean_q: 3.836643
 41090/100000: episode: 684, duration: 0.400s, episode steps: 77, steps per second: 192, episode reward: 139.763, mean reward: 1.815 [1.475, 2.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.675 [-1.366, 10.213], loss: 0.131503, mae: 0.324813, mean_q: 3.836542
 41170/100000: episode: 685, duration: 0.415s, episode steps: 80, steps per second: 193, episode reward: 176.790, mean reward: 2.210 [1.487, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.634 [-0.988, 10.100], loss: 0.125883, mae: 0.323881, mean_q: 3.854702
 41182/100000: episode: 686, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 34.391, mean reward: 2.866 [2.288, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.471, 10.100], loss: 0.111948, mae: 0.337055, mean_q: 3.874949
 41197/100000: episode: 687, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 41.865, mean reward: 2.791 [2.393, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.608, 10.100], loss: 0.125818, mae: 0.364419, mean_q: 3.915703
 41266/100000: episode: 688, duration: 0.366s, episode steps: 69, steps per second: 189, episode reward: 131.104, mean reward: 1.900 [1.466, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.745 [-0.705, 10.238], loss: 0.095431, mae: 0.310182, mean_q: 3.847290
 41335/100000: episode: 689, duration: 0.358s, episode steps: 69, steps per second: 193, episode reward: 146.144, mean reward: 2.118 [1.679, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.748 [-0.519, 10.406], loss: 0.134666, mae: 0.325538, mean_q: 3.858934
 41412/100000: episode: 690, duration: 0.416s, episode steps: 77, steps per second: 185, episode reward: 158.004, mean reward: 2.052 [1.468, 4.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.675 [-1.302, 10.352], loss: 0.100654, mae: 0.308931, mean_q: 3.869134
 41424/100000: episode: 691, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 38.927, mean reward: 3.244 [2.221, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.463, 10.100], loss: 0.071775, mae: 0.266964, mean_q: 3.796567
 41493/100000: episode: 692, duration: 0.349s, episode steps: 69, steps per second: 198, episode reward: 156.053, mean reward: 2.262 [1.464, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.761 [-1.825, 10.489], loss: 0.088029, mae: 0.300785, mean_q: 3.887501
 41562/100000: episode: 693, duration: 0.353s, episode steps: 69, steps per second: 195, episode reward: 137.034, mean reward: 1.986 [1.474, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.837, 10.100], loss: 0.102836, mae: 0.320382, mean_q: 3.895854
 41577/100000: episode: 694, duration: 0.073s, episode steps: 15, steps per second: 207, episode reward: 44.505, mean reward: 2.967 [2.189, 4.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.485, 10.100], loss: 0.098973, mae: 0.309341, mean_q: 3.889806
 41654/100000: episode: 695, duration: 0.392s, episode steps: 77, steps per second: 197, episode reward: 140.560, mean reward: 1.825 [1.481, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.674 [-0.185, 10.100], loss: 0.105819, mae: 0.321308, mean_q: 3.926113
 41731/100000: episode: 696, duration: 0.385s, episode steps: 77, steps per second: 200, episode reward: 180.136, mean reward: 2.339 [1.454, 4.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.674 [-1.044, 10.355], loss: 0.157421, mae: 0.336559, mean_q: 3.908608
 41808/100000: episode: 697, duration: 0.398s, episode steps: 77, steps per second: 194, episode reward: 152.554, mean reward: 1.981 [1.528, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.667 [-1.742, 10.213], loss: 0.100630, mae: 0.317555, mean_q: 3.911050
 41823/100000: episode: 698, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 41.156, mean reward: 2.744 [2.200, 4.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.423, 10.100], loss: 0.115151, mae: 0.330224, mean_q: 3.946430
 41900/100000: episode: 699, duration: 0.432s, episode steps: 77, steps per second: 178, episode reward: 152.577, mean reward: 1.982 [1.483, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.667 [-1.887, 10.100], loss: 0.112815, mae: 0.332309, mean_q: 3.906847
 41980/100000: episode: 700, duration: 0.426s, episode steps: 80, steps per second: 188, episode reward: 161.699, mean reward: 2.021 [1.437, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.640 [-1.305, 10.100], loss: 0.109078, mae: 0.327379, mean_q: 3.890252
 42050/100000: episode: 701, duration: 0.360s, episode steps: 70, steps per second: 194, episode reward: 153.792, mean reward: 2.197 [1.572, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.736 [-0.664, 10.100], loss: 0.109838, mae: 0.325942, mean_q: 3.923949
 42062/100000: episode: 702, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 32.405, mean reward: 2.700 [2.079, 5.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.447, 10.100], loss: 0.084991, mae: 0.289659, mean_q: 3.942241
 42142/100000: episode: 703, duration: 0.416s, episode steps: 80, steps per second: 192, episode reward: 148.364, mean reward: 1.855 [1.459, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.644 [-1.110, 10.106], loss: 0.115448, mae: 0.335418, mean_q: 3.919955
 42222/100000: episode: 704, duration: 0.434s, episode steps: 80, steps per second: 184, episode reward: 161.867, mean reward: 2.023 [1.465, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.636 [-1.201, 10.190], loss: 0.138831, mae: 0.343679, mean_q: 3.933128
 42237/100000: episode: 705, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 40.203, mean reward: 2.680 [1.976, 4.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.448, 10.100], loss: 0.102419, mae: 0.331513, mean_q: 3.982090
 42252/100000: episode: 706, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 42.564, mean reward: 2.838 [2.372, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.293, 10.100], loss: 0.109880, mae: 0.332543, mean_q: 3.966069
 42329/100000: episode: 707, duration: 0.404s, episode steps: 77, steps per second: 191, episode reward: 154.734, mean reward: 2.010 [1.475, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.664 [-1.806, 10.100], loss: 0.116129, mae: 0.339475, mean_q: 3.953052
 42406/100000: episode: 708, duration: 0.421s, episode steps: 77, steps per second: 183, episode reward: 172.346, mean reward: 2.238 [1.515, 5.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.678 [-0.327, 10.259], loss: 0.110115, mae: 0.329888, mean_q: 3.937258
 42486/100000: episode: 709, duration: 0.421s, episode steps: 80, steps per second: 190, episode reward: 178.918, mean reward: 2.236 [1.440, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.647 [-0.119, 10.100], loss: 0.118796, mae: 0.333763, mean_q: 3.948443
 42498/100000: episode: 710, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 35.366, mean reward: 2.947 [2.354, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-1.516, 10.100], loss: 0.118596, mae: 0.348514, mean_q: 3.947838
 42578/100000: episode: 711, duration: 0.423s, episode steps: 80, steps per second: 189, episode reward: 140.923, mean reward: 1.762 [1.470, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.642 [-1.238, 10.100], loss: 0.114806, mae: 0.335566, mean_q: 3.946824
 42658/100000: episode: 712, duration: 0.399s, episode steps: 80, steps per second: 201, episode reward: 148.396, mean reward: 1.855 [1.463, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.651 [-0.955, 10.168], loss: 0.137097, mae: 0.333449, mean_q: 3.957072
 42727/100000: episode: 713, duration: 0.368s, episode steps: 69, steps per second: 187, episode reward: 139.988, mean reward: 2.029 [1.498, 4.623], mean action: 0.000 [0.000, 0.000], mean observation: 1.740 [-1.187, 10.200], loss: 0.121793, mae: 0.343223, mean_q: 3.932546
 42739/100000: episode: 714, duration: 0.068s, episode steps: 12, steps per second: 178, episode reward: 29.039, mean reward: 2.420 [2.099, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.332, 10.100], loss: 0.125701, mae: 0.361168, mean_q: 3.984918
 42819/100000: episode: 715, duration: 0.418s, episode steps: 80, steps per second: 191, episode reward: 161.147, mean reward: 2.014 [1.532, 5.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.634 [-1.193, 10.100], loss: 0.154397, mae: 0.353590, mean_q: 3.957541
 42899/100000: episode: 716, duration: 0.422s, episode steps: 80, steps per second: 190, episode reward: 147.112, mean reward: 1.839 [1.477, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.639 [-1.176, 10.100], loss: 0.134293, mae: 0.329380, mean_q: 3.945442
 42974/100000: episode: 717, duration: 0.385s, episode steps: 75, steps per second: 195, episode reward: 145.380, mean reward: 1.938 [1.464, 4.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.690 [-0.459, 10.232], loss: 0.140497, mae: 0.335929, mean_q: 3.959201
 43051/100000: episode: 718, duration: 0.390s, episode steps: 77, steps per second: 197, episode reward: 150.338, mean reward: 1.952 [1.467, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.668 [-1.451, 10.100], loss: 0.119488, mae: 0.336834, mean_q: 3.953760
 43063/100000: episode: 719, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 58.870, mean reward: 4.906 [2.821, 7.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.513, 10.100], loss: 0.128472, mae: 0.344471, mean_q: 3.937932
 43078/100000: episode: 720, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 34.067, mean reward: 2.271 [1.818, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.755, 10.100], loss: 0.118252, mae: 0.344288, mean_q: 4.021817
 43153/100000: episode: 721, duration: 0.393s, episode steps: 75, steps per second: 191, episode reward: 147.221, mean reward: 1.963 [1.513, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.691 [-0.669, 10.343], loss: 0.123355, mae: 0.338619, mean_q: 3.973910
 43233/100000: episode: 722, duration: 0.405s, episode steps: 80, steps per second: 197, episode reward: 173.112, mean reward: 2.164 [1.499, 4.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.643 [-0.848, 10.340], loss: 0.134171, mae: 0.351631, mean_q: 3.978489
 43308/100000: episode: 723, duration: 0.409s, episode steps: 75, steps per second: 183, episode reward: 175.324, mean reward: 2.338 [1.444, 4.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.693 [-0.452, 10.100], loss: 0.135838, mae: 0.318644, mean_q: 3.955388
 43377/100000: episode: 724, duration: 0.384s, episode steps: 69, steps per second: 180, episode reward: 140.612, mean reward: 2.038 [1.457, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.741 [-0.428, 10.100], loss: 0.119065, mae: 0.332729, mean_q: 3.968247
 43446/100000: episode: 725, duration: 0.344s, episode steps: 69, steps per second: 201, episode reward: 169.850, mean reward: 2.462 [1.837, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.737 [-0.750, 10.100], loss: 0.185576, mae: 0.358873, mean_q: 3.977132
 43523/100000: episode: 726, duration: 0.411s, episode steps: 77, steps per second: 187, episode reward: 151.709, mean reward: 1.970 [1.463, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.679 [-1.722, 10.261], loss: 0.135509, mae: 0.360556, mean_q: 3.999591
 43600/100000: episode: 727, duration: 0.391s, episode steps: 77, steps per second: 197, episode reward: 147.278, mean reward: 1.913 [1.461, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.671 [-0.644, 10.100], loss: 0.155459, mae: 0.350169, mean_q: 4.005929
 43677/100000: episode: 728, duration: 0.396s, episode steps: 77, steps per second: 195, episode reward: 142.367, mean reward: 1.849 [1.450, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.667 [-1.534, 10.100], loss: 0.140279, mae: 0.335248, mean_q: 3.990364
 43752/100000: episode: 729, duration: 0.373s, episode steps: 75, steps per second: 201, episode reward: 136.126, mean reward: 1.815 [1.442, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.689 [-0.783, 10.100], loss: 0.116367, mae: 0.328744, mean_q: 3.998679
 43832/100000: episode: 730, duration: 0.403s, episode steps: 80, steps per second: 198, episode reward: 186.379, mean reward: 2.330 [1.545, 5.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.648 [-1.378, 10.459], loss: 0.122603, mae: 0.341711, mean_q: 4.013629
 43902/100000: episode: 731, duration: 0.358s, episode steps: 70, steps per second: 196, episode reward: 155.034, mean reward: 2.215 [1.474, 4.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.740 [-0.433, 10.335], loss: 0.134109, mae: 0.352709, mean_q: 3.994329
 43917/100000: episode: 732, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 44.697, mean reward: 2.980 [2.434, 4.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.775, 10.100], loss: 0.103543, mae: 0.317715, mean_q: 3.978788
 43992/100000: episode: 733, duration: 0.389s, episode steps: 75, steps per second: 193, episode reward: 139.730, mean reward: 1.863 [1.461, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.690 [-1.276, 10.160], loss: 0.111012, mae: 0.327281, mean_q: 3.988220
 44062/100000: episode: 734, duration: 0.371s, episode steps: 70, steps per second: 189, episode reward: 141.905, mean reward: 2.027 [1.626, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.729 [-0.356, 10.100], loss: 0.122005, mae: 0.336562, mean_q: 3.977766
 44142/100000: episode: 735, duration: 0.402s, episode steps: 80, steps per second: 199, episode reward: 146.803, mean reward: 1.835 [1.450, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.646 [-0.990, 10.195], loss: 0.114960, mae: 0.329946, mean_q: 4.017838
 44222/100000: episode: 736, duration: 0.420s, episode steps: 80, steps per second: 191, episode reward: 145.015, mean reward: 1.813 [1.471, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.641 [-1.006, 10.109], loss: 0.115085, mae: 0.337686, mean_q: 4.014718
 44302/100000: episode: 737, duration: 0.404s, episode steps: 80, steps per second: 198, episode reward: 155.063, mean reward: 1.938 [1.465, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.652 [-0.573, 10.291], loss: 0.109770, mae: 0.324424, mean_q: 3.974222
 44382/100000: episode: 738, duration: 0.404s, episode steps: 80, steps per second: 198, episode reward: 171.275, mean reward: 2.141 [1.474, 4.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.657 [-0.121, 10.382], loss: 0.130466, mae: 0.353699, mean_q: 3.995267
 44397/100000: episode: 739, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 49.595, mean reward: 3.306 [2.277, 5.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.459, 10.100], loss: 0.149033, mae: 0.361984, mean_q: 4.044382
 44474/100000: episode: 740, duration: 0.405s, episode steps: 77, steps per second: 190, episode reward: 146.467, mean reward: 1.902 [1.464, 2.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.675 [-0.412, 10.129], loss: 0.111118, mae: 0.319736, mean_q: 4.000976
 44554/100000: episode: 741, duration: 0.416s, episode steps: 80, steps per second: 192, episode reward: 144.207, mean reward: 1.803 [1.456, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.643 [-0.502, 10.100], loss: 0.139590, mae: 0.357600, mean_q: 4.012938
 44569/100000: episode: 742, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 39.404, mean reward: 2.627 [2.140, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.408, 10.100], loss: 0.096161, mae: 0.308704, mean_q: 4.021533
 44649/100000: episode: 743, duration: 0.406s, episode steps: 80, steps per second: 197, episode reward: 161.989, mean reward: 2.025 [1.439, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.651 [-0.595, 10.405], loss: 0.124207, mae: 0.343072, mean_q: 4.018029
 44718/100000: episode: 744, duration: 0.365s, episode steps: 69, steps per second: 189, episode reward: 143.803, mean reward: 2.084 [1.479, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.745 [-0.691, 10.100], loss: 0.104409, mae: 0.319081, mean_q: 4.002457
 44730/100000: episode: 745, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 27.741, mean reward: 2.312 [1.987, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.625, 10.100], loss: 0.119838, mae: 0.338016, mean_q: 4.069120
 44800/100000: episode: 746, duration: 0.384s, episode steps: 70, steps per second: 182, episode reward: 134.115, mean reward: 1.916 [1.438, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.738 [-1.299, 10.356], loss: 0.110454, mae: 0.326264, mean_q: 4.000320
 44812/100000: episode: 747, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 31.370, mean reward: 2.614 [2.137, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.482, 10.100], loss: 0.151494, mae: 0.372882, mean_q: 4.122630
 44889/100000: episode: 748, duration: 0.399s, episode steps: 77, steps per second: 193, episode reward: 180.132, mean reward: 2.339 [1.450, 7.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.673 [-0.612, 10.100], loss: 0.133388, mae: 0.354088, mean_q: 4.016741
 44901/100000: episode: 749, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 39.827, mean reward: 3.319 [2.174, 6.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.380, 10.100], loss: 0.148392, mae: 0.364544, mean_q: 4.096910
 44913/100000: episode: 750, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 30.325, mean reward: 2.527 [2.229, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.264, 10.100], loss: 0.111916, mae: 0.323144, mean_q: 3.984419
 44988/100000: episode: 751, duration: 0.375s, episode steps: 75, steps per second: 200, episode reward: 138.812, mean reward: 1.851 [1.461, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.690 [-0.970, 10.233], loss: 0.122729, mae: 0.335908, mean_q: 4.017253
 45065/100000: episode: 752, duration: 0.404s, episode steps: 77, steps per second: 190, episode reward: 140.841, mean reward: 1.829 [1.444, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.675 [-0.353, 10.368], loss: 0.135258, mae: 0.349585, mean_q: 4.048277
 45145/100000: episode: 753, duration: 0.390s, episode steps: 80, steps per second: 205, episode reward: 153.593, mean reward: 1.920 [1.486, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.647 [-1.000, 10.100], loss: 0.126287, mae: 0.343904, mean_q: 4.042750
 45225/100000: episode: 754, duration: 0.406s, episode steps: 80, steps per second: 197, episode reward: 157.136, mean reward: 1.964 [1.486, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.660 [-1.007, 10.150], loss: 0.127901, mae: 0.344738, mean_q: 4.046175
 45240/100000: episode: 755, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 50.473, mean reward: 3.365 [2.655, 5.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.756, 10.100], loss: 0.142430, mae: 0.361612, mean_q: 4.039010
 45320/100000: episode: 756, duration: 0.415s, episode steps: 80, steps per second: 193, episode reward: 171.797, mean reward: 2.147 [1.519, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.643 [-0.373, 10.305], loss: 0.133722, mae: 0.346073, mean_q: 4.043197
 45335/100000: episode: 757, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 51.567, mean reward: 3.438 [2.213, 5.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.178, 10.100], loss: 0.115140, mae: 0.318997, mean_q: 4.044662
 45415/100000: episode: 758, duration: 0.419s, episode steps: 80, steps per second: 191, episode reward: 168.966, mean reward: 2.112 [1.506, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.643 [-0.675, 10.100], loss: 0.117914, mae: 0.332560, mean_q: 4.033239
 45485/100000: episode: 759, duration: 0.401s, episode steps: 70, steps per second: 174, episode reward: 127.744, mean reward: 1.825 [1.445, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.739 [-1.427, 10.102], loss: 0.123448, mae: 0.335385, mean_q: 4.044360
[Info] NOT FOUND NEW LEVEL, Current Best Level is 6.258990287780762
 45497/100000: episode: 760, duration: 4.187s, episode steps: 12, steps per second: 3, episode reward: 27.000, mean reward: 2.250 [1.833, 2.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.216, 10.100], loss: 0.124869, mae: 0.338184, mean_q: 4.030227
 45597/100000: episode: 761, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 191.386, mean reward: 1.914 [1.478, 4.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.468, 10.191], loss: 0.129044, mae: 0.341672, mean_q: 4.030153
 45697/100000: episode: 762, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 185.814, mean reward: 1.858 [1.440, 2.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.505, 10.192], loss: 0.133148, mae: 0.357536, mean_q: 4.035963
 45797/100000: episode: 763, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 194.785, mean reward: 1.948 [1.487, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.975, 10.098], loss: 0.128404, mae: 0.347491, mean_q: 4.031594
 45897/100000: episode: 764, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 178.365, mean reward: 1.784 [1.472, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.622, 10.098], loss: 0.114937, mae: 0.337454, mean_q: 4.007545
 45997/100000: episode: 765, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 226.089, mean reward: 2.261 [1.458, 13.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.895, 10.452], loss: 0.126643, mae: 0.340108, mean_q: 4.021488
 46097/100000: episode: 766, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 189.592, mean reward: 1.896 [1.436, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.237, 10.113], loss: 0.129962, mae: 0.346479, mean_q: 4.005374
 46197/100000: episode: 767, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 189.568, mean reward: 1.896 [1.466, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.999, 10.098], loss: 0.148440, mae: 0.353023, mean_q: 3.978419
 46297/100000: episode: 768, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 200.472, mean reward: 2.005 [1.435, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.749, 10.098], loss: 0.168871, mae: 0.364093, mean_q: 4.033264
 46397/100000: episode: 769, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 188.954, mean reward: 1.890 [1.481, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.809, 10.214], loss: 0.144758, mae: 0.349760, mean_q: 4.025775
 46497/100000: episode: 770, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 218.439, mean reward: 2.184 [1.477, 4.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.970, 10.098], loss: 0.123213, mae: 0.342371, mean_q: 3.986285
 46597/100000: episode: 771, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 192.726, mean reward: 1.927 [1.459, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.667, 10.152], loss: 0.165377, mae: 0.361898, mean_q: 4.038963
 46697/100000: episode: 772, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 208.142, mean reward: 2.081 [1.492, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.790, 10.098], loss: 0.140298, mae: 0.348766, mean_q: 4.002225
 46797/100000: episode: 773, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 244.797, mean reward: 2.448 [1.435, 20.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.919, 10.098], loss: 0.137501, mae: 0.344448, mean_q: 4.008605
 46897/100000: episode: 774, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 193.429, mean reward: 1.934 [1.478, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.866, 10.306], loss: 0.363691, mae: 0.400306, mean_q: 4.044196
 46997/100000: episode: 775, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.168, mean reward: 1.862 [1.472, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.158, 10.107], loss: 0.190768, mae: 0.356400, mean_q: 4.023012
 47097/100000: episode: 776, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.695, mean reward: 1.867 [1.458, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.946, 10.162], loss: 0.201318, mae: 0.353964, mean_q: 3.998991
 47197/100000: episode: 777, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 198.984, mean reward: 1.990 [1.471, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.632, 10.269], loss: 0.259683, mae: 0.368545, mean_q: 4.016026
 47297/100000: episode: 778, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 189.139, mean reward: 1.891 [1.445, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.821, 10.123], loss: 0.164530, mae: 0.352681, mean_q: 3.992519
 47397/100000: episode: 779, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 182.543, mean reward: 1.825 [1.450, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.221, 10.316], loss: 0.191162, mae: 0.348212, mean_q: 3.960665
 47497/100000: episode: 780, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 180.872, mean reward: 1.809 [1.467, 2.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.663, 10.098], loss: 0.166356, mae: 0.343968, mean_q: 3.963575
 47597/100000: episode: 781, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 190.480, mean reward: 1.905 [1.489, 4.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.967, 10.162], loss: 0.257748, mae: 0.368694, mean_q: 3.962564
 47697/100000: episode: 782, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.196, mean reward: 1.892 [1.454, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.220, 10.098], loss: 0.223949, mae: 0.363753, mean_q: 3.964933
 47797/100000: episode: 783, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 182.211, mean reward: 1.822 [1.501, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.390, 10.125], loss: 0.186719, mae: 0.345103, mean_q: 3.976272
 47897/100000: episode: 784, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 194.222, mean reward: 1.942 [1.441, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.165, 10.098], loss: 0.248233, mae: 0.372493, mean_q: 3.964285
 47997/100000: episode: 785, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 215.322, mean reward: 2.153 [1.485, 4.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.083, 10.098], loss: 0.146404, mae: 0.340750, mean_q: 3.943912
 48097/100000: episode: 786, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 177.845, mean reward: 1.778 [1.455, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.556, 10.098], loss: 0.159415, mae: 0.330542, mean_q: 3.926151
 48197/100000: episode: 787, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 196.361, mean reward: 1.964 [1.492, 4.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.601, 10.317], loss: 0.115048, mae: 0.313878, mean_q: 3.917483
 48297/100000: episode: 788, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.633, mean reward: 1.976 [1.435, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.638, 10.210], loss: 0.106858, mae: 0.313741, mean_q: 3.911892
 48397/100000: episode: 789, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 197.104, mean reward: 1.971 [1.446, 4.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.532, 10.115], loss: 0.105039, mae: 0.318731, mean_q: 3.900109
 48497/100000: episode: 790, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 191.229, mean reward: 1.912 [1.474, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.604, 10.248], loss: 0.234980, mae: 0.353374, mean_q: 3.914901
 48597/100000: episode: 791, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 218.378, mean reward: 2.184 [1.522, 4.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.733, 10.302], loss: 0.164133, mae: 0.336960, mean_q: 3.907452
 48697/100000: episode: 792, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.881, mean reward: 1.979 [1.465, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.580, 10.098], loss: 0.272058, mae: 0.369485, mean_q: 3.962603
 48797/100000: episode: 793, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 223.051, mean reward: 2.231 [1.504, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.689, 10.098], loss: 0.270316, mae: 0.371895, mean_q: 3.941180
 48897/100000: episode: 794, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 183.303, mean reward: 1.833 [1.468, 2.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.845, 10.207], loss: 0.196898, mae: 0.348136, mean_q: 3.920076
 48997/100000: episode: 795, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 179.099, mean reward: 1.791 [1.456, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.300, 10.270], loss: 0.211407, mae: 0.351063, mean_q: 3.916077
 49097/100000: episode: 796, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 181.986, mean reward: 1.820 [1.442, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.781, 10.246], loss: 0.128003, mae: 0.334569, mean_q: 3.911238
 49197/100000: episode: 797, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 231.325, mean reward: 2.313 [1.477, 7.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.666, 10.406], loss: 0.176960, mae: 0.344851, mean_q: 3.924664
 49297/100000: episode: 798, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 187.595, mean reward: 1.876 [1.439, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.776, 10.098], loss: 0.099694, mae: 0.310121, mean_q: 3.917233
 49397/100000: episode: 799, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 185.420, mean reward: 1.854 [1.474, 2.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.973, 10.098], loss: 0.154712, mae: 0.319276, mean_q: 3.888717
 49497/100000: episode: 800, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 225.491, mean reward: 2.255 [1.495, 13.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.974, 10.493], loss: 0.261758, mae: 0.345624, mean_q: 3.921682
 49597/100000: episode: 801, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 190.276, mean reward: 1.903 [1.437, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.859, 10.098], loss: 0.225074, mae: 0.356818, mean_q: 3.925938
 49697/100000: episode: 802, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 191.162, mean reward: 1.912 [1.436, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.601, 10.149], loss: 0.236032, mae: 0.357047, mean_q: 3.901382
 49797/100000: episode: 803, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 187.341, mean reward: 1.873 [1.441, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.006, 10.323], loss: 0.117792, mae: 0.320795, mean_q: 3.895584
 49897/100000: episode: 804, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 205.356, mean reward: 2.054 [1.467, 4.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.542, 10.098], loss: 0.141579, mae: 0.335180, mean_q: 3.877911
 49997/100000: episode: 805, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 196.706, mean reward: 1.967 [1.445, 3.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.303, 10.201], loss: 0.169286, mae: 0.330948, mean_q: 3.884392
 50097/100000: episode: 806, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.151, mean reward: 1.822 [1.490, 2.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.777, 10.256], loss: 0.127176, mae: 0.323649, mean_q: 3.902367
 50197/100000: episode: 807, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.136, mean reward: 1.881 [1.466, 4.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.751, 10.245], loss: 0.106600, mae: 0.319985, mean_q: 3.866343
 50297/100000: episode: 808, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 177.773, mean reward: 1.778 [1.447, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.804, 10.098], loss: 0.193953, mae: 0.349278, mean_q: 3.886016
 50397/100000: episode: 809, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 202.338, mean reward: 2.023 [1.454, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.940, 10.238], loss: 0.101209, mae: 0.313258, mean_q: 3.877543
 50497/100000: episode: 810, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 190.358, mean reward: 1.904 [1.464, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.948, 10.098], loss: 0.158875, mae: 0.321970, mean_q: 3.870698
 50597/100000: episode: 811, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 221.318, mean reward: 2.213 [1.481, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.220, 10.098], loss: 0.340069, mae: 0.374619, mean_q: 3.896691
 50697/100000: episode: 812, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 183.470, mean reward: 1.835 [1.466, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.687, 10.098], loss: 0.147487, mae: 0.337543, mean_q: 3.877681
 50797/100000: episode: 813, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 191.892, mean reward: 1.919 [1.440, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.592, 10.200], loss: 0.215786, mae: 0.362852, mean_q: 3.904711
 50897/100000: episode: 814, duration: 0.482s, episode steps: 100, steps per second: 208, episode reward: 179.594, mean reward: 1.796 [1.436, 2.865], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.641, 10.282], loss: 0.175325, mae: 0.357816, mean_q: 3.901639
 50997/100000: episode: 815, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 205.463, mean reward: 2.055 [1.471, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.445, 10.282], loss: 0.293206, mae: 0.356318, mean_q: 3.892900
 51097/100000: episode: 816, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 201.451, mean reward: 2.015 [1.445, 7.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.578, 10.159], loss: 0.226986, mae: 0.366941, mean_q: 3.898195
 51197/100000: episode: 817, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 191.366, mean reward: 1.914 [1.446, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.927, 10.098], loss: 0.180030, mae: 0.331825, mean_q: 3.885162
 51297/100000: episode: 818, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 196.158, mean reward: 1.962 [1.453, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.218, 10.183], loss: 0.173448, mae: 0.323091, mean_q: 3.873217
 51397/100000: episode: 819, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 214.154, mean reward: 2.142 [1.500, 4.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.648, 10.243], loss: 0.092362, mae: 0.310442, mean_q: 3.856388
 51497/100000: episode: 820, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 199.208, mean reward: 1.992 [1.444, 4.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.711, 10.157], loss: 0.227982, mae: 0.346417, mean_q: 3.872197
 51597/100000: episode: 821, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 191.891, mean reward: 1.919 [1.435, 3.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.379, 10.216], loss: 0.221331, mae: 0.353389, mean_q: 3.896694
 51697/100000: episode: 822, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 197.163, mean reward: 1.972 [1.569, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.281, 10.220], loss: 0.098020, mae: 0.307014, mean_q: 3.844290
 51797/100000: episode: 823, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 202.537, mean reward: 2.025 [1.489, 7.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.149, 10.098], loss: 0.135417, mae: 0.320878, mean_q: 3.847575
 51897/100000: episode: 824, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 182.992, mean reward: 1.830 [1.439, 2.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.672, 10.098], loss: 0.106473, mae: 0.319982, mean_q: 3.830310
 51997/100000: episode: 825, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 192.644, mean reward: 1.926 [1.446, 3.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.237, 10.098], loss: 0.155685, mae: 0.321328, mean_q: 3.851110
 52097/100000: episode: 826, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 184.453, mean reward: 1.845 [1.459, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.099, 10.178], loss: 0.128557, mae: 0.321445, mean_q: 3.846570
 52197/100000: episode: 827, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 204.398, mean reward: 2.044 [1.471, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.799, 10.249], loss: 0.136422, mae: 0.329850, mean_q: 3.849002
 52297/100000: episode: 828, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 190.350, mean reward: 1.903 [1.457, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.803, 10.344], loss: 0.105875, mae: 0.316384, mean_q: 3.825242
 52397/100000: episode: 829, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 189.790, mean reward: 1.898 [1.480, 4.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.555, 10.182], loss: 0.138491, mae: 0.329635, mean_q: 3.873206
 52497/100000: episode: 830, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.098, mean reward: 1.851 [1.482, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.692, 10.245], loss: 0.130269, mae: 0.324667, mean_q: 3.841533
 52597/100000: episode: 831, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 185.205, mean reward: 1.852 [1.478, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.873, 10.162], loss: 0.090707, mae: 0.297956, mean_q: 3.846103
 52697/100000: episode: 832, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.664, mean reward: 1.887 [1.449, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.747, 10.224], loss: 0.114086, mae: 0.318518, mean_q: 3.859893
 52797/100000: episode: 833, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.358, mean reward: 1.954 [1.510, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.989, 10.098], loss: 0.104559, mae: 0.308738, mean_q: 3.844456
 52897/100000: episode: 834, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 200.946, mean reward: 2.009 [1.451, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.858, 10.098], loss: 0.113646, mae: 0.308850, mean_q: 3.851969
 52997/100000: episode: 835, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 192.937, mean reward: 1.929 [1.451, 4.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.051, 10.098], loss: 0.135595, mae: 0.327726, mean_q: 3.880171
 53097/100000: episode: 836, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.364, mean reward: 1.844 [1.461, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.039, 10.205], loss: 0.107333, mae: 0.316403, mean_q: 3.850797
 53197/100000: episode: 837, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 213.564, mean reward: 2.136 [1.458, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.466, 10.443], loss: 0.112642, mae: 0.318870, mean_q: 3.860944
 53297/100000: episode: 838, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 195.738, mean reward: 1.957 [1.467, 2.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.520, 10.098], loss: 0.100101, mae: 0.312011, mean_q: 3.853762
 53397/100000: episode: 839, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 202.601, mean reward: 2.026 [1.496, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.986, 10.098], loss: 0.130414, mae: 0.317744, mean_q: 3.838830
 53497/100000: episode: 840, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 206.086, mean reward: 2.061 [1.454, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.944, 10.098], loss: 0.091540, mae: 0.297390, mean_q: 3.840355
 53597/100000: episode: 841, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.364, mean reward: 1.884 [1.443, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.229, 10.098], loss: 0.113898, mae: 0.326551, mean_q: 3.867059
 53697/100000: episode: 842, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.775, mean reward: 1.928 [1.482, 2.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.161, 10.098], loss: 0.130727, mae: 0.320561, mean_q: 3.860802
 53797/100000: episode: 843, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 185.252, mean reward: 1.853 [1.479, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.779, 10.270], loss: 0.086064, mae: 0.288570, mean_q: 3.819988
 53897/100000: episode: 844, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 189.847, mean reward: 1.898 [1.484, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.965, 10.136], loss: 0.145862, mae: 0.317062, mean_q: 3.835035
 53997/100000: episode: 845, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 190.282, mean reward: 1.903 [1.484, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.776, 10.098], loss: 0.118168, mae: 0.309612, mean_q: 3.849857
 54097/100000: episode: 846, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 183.891, mean reward: 1.839 [1.442, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.122, 10.098], loss: 0.102147, mae: 0.309953, mean_q: 3.851271
 54197/100000: episode: 847, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 199.607, mean reward: 1.996 [1.440, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.725, 10.196], loss: 0.085555, mae: 0.291763, mean_q: 3.811804
 54297/100000: episode: 848, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 189.389, mean reward: 1.894 [1.440, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.482, 10.098], loss: 0.096016, mae: 0.298410, mean_q: 3.808747
 54397/100000: episode: 849, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 184.278, mean reward: 1.843 [1.445, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.120, 10.098], loss: 0.094916, mae: 0.299315, mean_q: 3.817486
 54497/100000: episode: 850, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 210.287, mean reward: 2.103 [1.440, 4.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.154, 10.098], loss: 0.099533, mae: 0.309821, mean_q: 3.825537
 54597/100000: episode: 851, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 204.591, mean reward: 2.046 [1.484, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.590, 10.261], loss: 0.091468, mae: 0.293666, mean_q: 3.825248
 54697/100000: episode: 852, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 211.219, mean reward: 2.112 [1.437, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.885, 10.207], loss: 0.089534, mae: 0.294162, mean_q: 3.827695
 54797/100000: episode: 853, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 220.452, mean reward: 2.205 [1.441, 4.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.730, 10.098], loss: 0.107370, mae: 0.327027, mean_q: 3.852279
 54897/100000: episode: 854, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 270.532, mean reward: 2.705 [1.485, 8.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.653, 10.098], loss: 0.092315, mae: 0.302101, mean_q: 3.857185
 54997/100000: episode: 855, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 187.167, mean reward: 1.872 [1.467, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.973, 10.098], loss: 0.098082, mae: 0.310149, mean_q: 3.843249
 55097/100000: episode: 856, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 202.056, mean reward: 2.021 [1.473, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.468, 10.240], loss: 0.096281, mae: 0.303434, mean_q: 3.869462
 55197/100000: episode: 857, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 187.900, mean reward: 1.879 [1.467, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.569, 10.179], loss: 0.104040, mae: 0.312349, mean_q: 3.874968
 55297/100000: episode: 858, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 184.244, mean reward: 1.842 [1.442, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.048, 10.244], loss: 0.105906, mae: 0.314991, mean_q: 3.883205
 55397/100000: episode: 859, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 189.250, mean reward: 1.892 [1.487, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.097, 10.138], loss: 0.117240, mae: 0.321379, mean_q: 3.884129
[Info] 1-TH LEVEL FOUND: 5.58680534362793, Considering 10/90 traces
 55497/100000: episode: 860, duration: 4.745s, episode steps: 100, steps per second: 21, episode reward: 191.203, mean reward: 1.912 [1.455, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.873, 10.339], loss: 0.113246, mae: 0.310952, mean_q: 3.873841
 55540/100000: episode: 861, duration: 0.232s, episode steps: 43, steps per second: 186, episode reward: 90.083, mean reward: 2.095 [1.560, 7.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.141, 10.251], loss: 0.129717, mae: 0.320083, mean_q: 3.868257
 55583/100000: episode: 862, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 101.919, mean reward: 2.370 [1.774, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.629, 10.496], loss: 0.128016, mae: 0.326876, mean_q: 3.908091
 55601/100000: episode: 863, duration: 0.101s, episode steps: 18, steps per second: 177, episode reward: 38.220, mean reward: 2.123 [1.840, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.231, 10.100], loss: 0.091319, mae: 0.311589, mean_q: 3.871613
 55644/100000: episode: 864, duration: 0.227s, episode steps: 43, steps per second: 189, episode reward: 97.051, mean reward: 2.257 [1.466, 5.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.224, 10.164], loss: 0.139198, mae: 0.344046, mean_q: 3.928060
 55669/100000: episode: 865, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 67.498, mean reward: 2.700 [1.854, 8.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.076, 10.100], loss: 0.143307, mae: 0.336987, mean_q: 3.939282
 55712/100000: episode: 866, duration: 0.228s, episode steps: 43, steps per second: 188, episode reward: 104.747, mean reward: 2.436 [1.562, 5.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.146, 10.265], loss: 0.107725, mae: 0.318116, mean_q: 3.895863
 55728/100000: episode: 867, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 38.928, mean reward: 2.433 [2.065, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.585, 10.100], loss: 0.108842, mae: 0.325723, mean_q: 3.877505
 55743/100000: episode: 868, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 29.819, mean reward: 1.988 [1.512, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.153, 10.100], loss: 0.160427, mae: 0.347649, mean_q: 3.961310
 55759/100000: episode: 869, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 36.320, mean reward: 2.270 [1.652, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.248, 10.100], loss: 0.102817, mae: 0.309865, mean_q: 3.891665
 55774/100000: episode: 870, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 34.830, mean reward: 2.322 [1.895, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.084, 10.100], loss: 0.126471, mae: 0.339292, mean_q: 3.868458
 55790/100000: episode: 871, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 46.370, mean reward: 2.898 [1.772, 5.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.382, 10.100], loss: 0.191386, mae: 0.377317, mean_q: 3.969574
 55806/100000: episode: 872, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 43.832, mean reward: 2.740 [1.973, 4.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.301, 10.100], loss: 0.124751, mae: 0.313622, mean_q: 3.904730
 55822/100000: episode: 873, duration: 0.081s, episode steps: 16, steps per second: 196, episode reward: 43.934, mean reward: 2.746 [2.342, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.407, 10.100], loss: 0.129530, mae: 0.334371, mean_q: 3.890224
 55849/100000: episode: 874, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 71.793, mean reward: 2.659 [2.255, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.273, 10.100], loss: 0.088684, mae: 0.322320, mean_q: 3.913123
 55874/100000: episode: 875, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 50.093, mean reward: 2.004 [1.609, 2.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.478, 10.100], loss: 0.150133, mae: 0.344579, mean_q: 3.967348
 55886/100000: episode: 876, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 37.669, mean reward: 3.139 [2.747, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.309, 10.100], loss: 0.156916, mae: 0.360400, mean_q: 3.946124
 55898/100000: episode: 877, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 39.151, mean reward: 3.263 [2.643, 5.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.818, 10.100], loss: 0.106827, mae: 0.322702, mean_q: 3.946603
 55913/100000: episode: 878, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 35.520, mean reward: 2.368 [1.740, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.600, 10.100], loss: 0.090167, mae: 0.301965, mean_q: 3.941742
 55929/100000: episode: 879, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 34.976, mean reward: 2.186 [1.714, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.668, 10.100], loss: 0.123049, mae: 0.328982, mean_q: 3.957570
 55947/100000: episode: 880, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 43.625, mean reward: 2.424 [1.984, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.189, 10.100], loss: 0.117422, mae: 0.333343, mean_q: 3.980548
 55974/100000: episode: 881, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 77.797, mean reward: 2.881 [2.385, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.487, 10.100], loss: 0.111857, mae: 0.320714, mean_q: 3.974525
 55997/100000: episode: 882, duration: 0.125s, episode steps: 23, steps per second: 185, episode reward: 69.788, mean reward: 3.034 [2.628, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.331, 10.100], loss: 0.088673, mae: 0.297406, mean_q: 3.998686
 56040/100000: episode: 883, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 149.910, mean reward: 3.486 [1.900, 7.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.229, 10.344], loss: 0.119109, mae: 0.337000, mean_q: 3.966203
 56065/100000: episode: 884, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 65.573, mean reward: 2.623 [1.783, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.146, 10.100], loss: 0.165488, mae: 0.362523, mean_q: 3.994428
 56083/100000: episode: 885, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 51.904, mean reward: 2.884 [2.103, 5.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.344, 10.100], loss: 0.130220, mae: 0.340682, mean_q: 4.041971
 56098/100000: episode: 886, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 31.428, mean reward: 2.095 [1.809, 2.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.754, 10.100], loss: 0.137216, mae: 0.346199, mean_q: 4.013383
 56125/100000: episode: 887, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 77.871, mean reward: 2.884 [2.078, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.353, 10.100], loss: 0.117771, mae: 0.336598, mean_q: 3.948416
 56140/100000: episode: 888, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 45.751, mean reward: 3.050 [2.168, 4.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-1.292, 10.100], loss: 0.121632, mae: 0.334880, mean_q: 3.987742
 56155/100000: episode: 889, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 31.053, mean reward: 2.070 [1.792, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.121, 10.100], loss: 0.131488, mae: 0.336657, mean_q: 4.036275
 56178/100000: episode: 890, duration: 0.123s, episode steps: 23, steps per second: 188, episode reward: 57.901, mean reward: 2.517 [2.016, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.733, 10.100], loss: 0.175455, mae: 0.379090, mean_q: 3.994480
 56205/100000: episode: 891, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 99.014, mean reward: 3.667 [1.989, 6.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.690, 10.100], loss: 0.140087, mae: 0.349787, mean_q: 4.000458
 56221/100000: episode: 892, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 35.487, mean reward: 2.218 [1.847, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.614, 10.100], loss: 0.176919, mae: 0.368251, mean_q: 4.053763
 56236/100000: episode: 893, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 37.600, mean reward: 2.507 [2.002, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.432, 10.100], loss: 0.160025, mae: 0.380287, mean_q: 4.064628
 56251/100000: episode: 894, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 32.871, mean reward: 2.191 [1.711, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.474, 10.100], loss: 0.116133, mae: 0.335946, mean_q: 3.982024
 56266/100000: episode: 895, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 38.597, mean reward: 2.573 [1.990, 3.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.272, 10.100], loss: 0.168376, mae: 0.388365, mean_q: 4.134983
 56291/100000: episode: 896, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 69.462, mean reward: 2.778 [2.034, 3.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.301, 10.100], loss: 0.143566, mae: 0.355130, mean_q: 4.090295
 56334/100000: episode: 897, duration: 0.227s, episode steps: 43, steps per second: 190, episode reward: 94.158, mean reward: 2.190 [1.525, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-1.573, 10.100], loss: 0.160799, mae: 0.374449, mean_q: 4.041174
 56350/100000: episode: 898, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 43.531, mean reward: 2.721 [1.925, 4.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.313, 10.100], loss: 0.190819, mae: 0.378527, mean_q: 4.014141
 56366/100000: episode: 899, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 40.761, mean reward: 2.548 [2.216, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.392, 10.100], loss: 0.116903, mae: 0.331425, mean_q: 4.067465
 56393/100000: episode: 900, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 78.164, mean reward: 2.895 [1.830, 4.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.051, 10.100], loss: 0.165555, mae: 0.372449, mean_q: 4.072968
 56409/100000: episode: 901, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 39.930, mean reward: 2.496 [1.760, 8.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.418, 10.100], loss: 0.153066, mae: 0.361237, mean_q: 4.153201
 56425/100000: episode: 902, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 50.003, mean reward: 3.125 [2.125, 7.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.100], loss: 0.161004, mae: 0.346339, mean_q: 4.066034
 56468/100000: episode: 903, duration: 0.217s, episode steps: 43, steps per second: 198, episode reward: 87.833, mean reward: 2.043 [1.546, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.285, 10.137], loss: 0.152710, mae: 0.365763, mean_q: 4.039457
 56511/100000: episode: 904, duration: 0.221s, episode steps: 43, steps per second: 194, episode reward: 85.429, mean reward: 1.987 [1.569, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-1.299, 10.222], loss: 0.146244, mae: 0.371159, mean_q: 4.062354
 56534/100000: episode: 905, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 66.126, mean reward: 2.875 [2.161, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.349, 10.100], loss: 0.148438, mae: 0.344213, mean_q: 4.089147
 56577/100000: episode: 906, duration: 0.228s, episode steps: 43, steps per second: 189, episode reward: 95.379, mean reward: 2.218 [1.572, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.349, 10.177], loss: 0.136660, mae: 0.363789, mean_q: 4.099298
 56592/100000: episode: 907, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 33.502, mean reward: 2.233 [1.673, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.177, 10.100], loss: 0.117174, mae: 0.333304, mean_q: 4.069076
 56610/100000: episode: 908, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 51.386, mean reward: 2.855 [2.109, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.703, 10.100], loss: 0.096476, mae: 0.311722, mean_q: 4.035458
 56628/100000: episode: 909, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 42.623, mean reward: 2.368 [2.032, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.353, 10.100], loss: 0.158479, mae: 0.366323, mean_q: 4.150854
 56655/100000: episode: 910, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 84.772, mean reward: 3.140 [2.439, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.571, 10.100], loss: 0.133605, mae: 0.345767, mean_q: 4.110096
 56678/100000: episode: 911, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 64.810, mean reward: 2.818 [2.477, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.356, 10.100], loss: 0.128014, mae: 0.370405, mean_q: 4.189364
 56696/100000: episode: 912, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 51.112, mean reward: 2.840 [2.090, 8.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.639, 10.100], loss: 0.202215, mae: 0.382101, mean_q: 4.146593
 56739/100000: episode: 913, duration: 0.212s, episode steps: 43, steps per second: 203, episode reward: 101.681, mean reward: 2.365 [1.859, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.400, 10.331], loss: 0.151547, mae: 0.370734, mean_q: 4.149202
 56754/100000: episode: 914, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 32.296, mean reward: 2.153 [1.886, 2.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.268, 10.100], loss: 0.118072, mae: 0.321593, mean_q: 4.175151
 56766/100000: episode: 915, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 27.765, mean reward: 2.314 [1.722, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.260, 10.100], loss: 0.234338, mae: 0.400496, mean_q: 4.196422
 56782/100000: episode: 916, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 33.469, mean reward: 2.092 [1.772, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.249, 10.100], loss: 0.149752, mae: 0.363383, mean_q: 4.273885
 56807/100000: episode: 917, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 71.045, mean reward: 2.842 [1.768, 5.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-1.276, 10.100], loss: 0.196908, mae: 0.398883, mean_q: 4.177443
 56822/100000: episode: 918, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 40.962, mean reward: 2.731 [1.927, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.195, 10.100], loss: 0.141068, mae: 0.377710, mean_q: 4.174698
 56845/100000: episode: 919, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 49.894, mean reward: 2.169 [1.782, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.085, 10.100], loss: 0.153697, mae: 0.361392, mean_q: 4.168287
 56872/100000: episode: 920, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 81.685, mean reward: 3.025 [1.795, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.154, 10.100], loss: 0.126268, mae: 0.337869, mean_q: 4.143541
 56915/100000: episode: 921, duration: 0.236s, episode steps: 43, steps per second: 182, episode reward: 107.904, mean reward: 2.509 [1.633, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-1.079, 10.193], loss: 0.194038, mae: 0.391470, mean_q: 4.173120
 56938/100000: episode: 922, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 67.336, mean reward: 2.928 [2.121, 5.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.362, 10.100], loss: 0.209397, mae: 0.407770, mean_q: 4.214871
 56953/100000: episode: 923, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 30.893, mean reward: 2.060 [1.492, 2.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.107, 10.127], loss: 0.120428, mae: 0.356913, mean_q: 4.178213
 56968/100000: episode: 924, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 35.410, mean reward: 2.361 [1.981, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.859, 10.100], loss: 0.126687, mae: 0.357408, mean_q: 4.143358
 56984/100000: episode: 925, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 33.461, mean reward: 2.091 [1.579, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-1.042, 10.100], loss: 0.146582, mae: 0.367568, mean_q: 4.258809
 57007/100000: episode: 926, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 58.558, mean reward: 2.546 [1.824, 3.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.357, 10.100], loss: 0.117894, mae: 0.342706, mean_q: 4.179104
 57022/100000: episode: 927, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 34.299, mean reward: 2.287 [1.976, 3.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.327, 10.100], loss: 0.205141, mae: 0.426036, mean_q: 4.276613
 57037/100000: episode: 928, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 33.402, mean reward: 2.227 [1.990, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.287, 10.100], loss: 0.120091, mae: 0.334450, mean_q: 4.161819
 57060/100000: episode: 929, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 95.036, mean reward: 4.132 [2.433, 8.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.412, 10.100], loss: 0.172217, mae: 0.372170, mean_q: 4.140423
 57075/100000: episode: 930, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 31.065, mean reward: 2.071 [1.555, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.611, 10.128], loss: 0.158513, mae: 0.382603, mean_q: 4.223828
 57093/100000: episode: 931, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 44.702, mean reward: 2.483 [1.866, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.812, 10.100], loss: 0.202277, mae: 0.399970, mean_q: 4.189772
 57111/100000: episode: 932, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 55.905, mean reward: 3.106 [2.253, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.441, 10.100], loss: 0.179042, mae: 0.379116, mean_q: 4.222521
 57123/100000: episode: 933, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 31.885, mean reward: 2.657 [2.108, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.323, 10.100], loss: 0.172091, mae: 0.380327, mean_q: 4.152852
 57148/100000: episode: 934, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 91.658, mean reward: 3.666 [2.061, 6.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.601, 10.100], loss: 0.170393, mae: 0.384859, mean_q: 4.247881
 57175/100000: episode: 935, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 95.825, mean reward: 3.549 [2.192, 5.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.831, 10.100], loss: 0.175295, mae: 0.380246, mean_q: 4.329318
 57190/100000: episode: 936, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 31.398, mean reward: 2.093 [1.778, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.235, 10.100], loss: 0.158984, mae: 0.373020, mean_q: 4.260925
 57213/100000: episode: 937, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 55.591, mean reward: 2.417 [1.705, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.199, 10.100], loss: 0.201517, mae: 0.399064, mean_q: 4.226172
 57231/100000: episode: 938, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 48.596, mean reward: 2.700 [2.200, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.280, 10.100], loss: 0.143291, mae: 0.377347, mean_q: 4.223783
 57247/100000: episode: 939, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 42.304, mean reward: 2.644 [2.122, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.289, 10.100], loss: 0.153734, mae: 0.359502, mean_q: 4.194791
 57262/100000: episode: 940, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 40.618, mean reward: 2.708 [1.987, 3.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.769, 10.100], loss: 0.138995, mae: 0.350319, mean_q: 4.212550
 57277/100000: episode: 941, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 30.514, mean reward: 2.034 [1.569, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.761, 10.100], loss: 0.138401, mae: 0.360385, mean_q: 4.273843
 57293/100000: episode: 942, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 43.542, mean reward: 2.721 [1.856, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.356, 10.100], loss: 0.197705, mae: 0.395659, mean_q: 4.254874
 57318/100000: episode: 943, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 49.163, mean reward: 1.967 [1.616, 2.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.222, 10.100], loss: 0.201176, mae: 0.432314, mean_q: 4.256438
 57333/100000: episode: 944, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 34.066, mean reward: 2.271 [1.974, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.314, 10.100], loss: 0.147863, mae: 0.386082, mean_q: 4.373474
 57360/100000: episode: 945, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 78.868, mean reward: 2.921 [2.256, 5.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.783, 10.100], loss: 0.134226, mae: 0.357577, mean_q: 4.265764
 57376/100000: episode: 946, duration: 0.092s, episode steps: 16, steps per second: 173, episode reward: 33.560, mean reward: 2.097 [1.743, 2.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.188, 10.100], loss: 0.165615, mae: 0.388107, mean_q: 4.315206
 57403/100000: episode: 947, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 147.128, mean reward: 5.449 [2.456, 11.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.512, 10.100], loss: 0.201760, mae: 0.421533, mean_q: 4.319936
 57415/100000: episode: 948, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 30.819, mean reward: 2.568 [2.170, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.943, 10.100], loss: 0.226398, mae: 0.463967, mean_q: 4.395019
 57427/100000: episode: 949, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 38.745, mean reward: 3.229 [2.313, 6.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.627, 10.100], loss: 0.148566, mae: 0.396252, mean_q: 4.286066
[Info] 2-TH LEVEL FOUND: 6.687321186065674, Considering 10/90 traces
 57442/100000: episode: 950, duration: 4.370s, episode steps: 15, steps per second: 3, episode reward: 44.448, mean reward: 2.963 [1.962, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.204, 10.100], loss: 0.195607, mae: 0.388736, mean_q: 4.331862
 57464/100000: episode: 951, duration: 0.110s, episode steps: 22, steps per second: 201, episode reward: 71.923, mean reward: 3.269 [2.662, 4.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-1.534, 10.100], loss: 0.205535, mae: 0.420385, mean_q: 4.309164
 57487/100000: episode: 952, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 98.290, mean reward: 4.273 [3.098, 6.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.867, 10.100], loss: 0.156432, mae: 0.372172, mean_q: 4.319705
 57506/100000: episode: 953, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 69.997, mean reward: 3.684 [2.928, 5.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.544, 10.100], loss: 0.217928, mae: 0.420895, mean_q: 4.406923
 57529/100000: episode: 954, duration: 0.116s, episode steps: 23, steps per second: 199, episode reward: 130.317, mean reward: 5.666 [3.438, 10.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.573, 10.100], loss: 0.202080, mae: 0.415545, mean_q: 4.379220
 57546/100000: episode: 955, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 67.339, mean reward: 3.961 [2.986, 5.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.298, 10.100], loss: 0.147076, mae: 0.382257, mean_q: 4.337242
 57563/100000: episode: 956, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 69.626, mean reward: 4.096 [2.990, 11.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.142, 10.100], loss: 0.208377, mae: 0.411270, mean_q: 4.415128
 57582/100000: episode: 957, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 88.791, mean reward: 4.673 [3.388, 8.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.327, 10.100], loss: 0.271673, mae: 0.459160, mean_q: 4.518168
 57601/100000: episode: 958, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 65.811, mean reward: 3.464 [2.317, 6.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.212, 10.100], loss: 0.251020, mae: 0.444748, mean_q: 4.528744
 57623/100000: episode: 959, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 74.259, mean reward: 3.375 [2.583, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.379, 10.100], loss: 0.293946, mae: 0.464705, mean_q: 4.540750
 57645/100000: episode: 960, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 96.929, mean reward: 4.406 [3.629, 5.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.229, 10.100], loss: 0.227934, mae: 0.445570, mean_q: 4.573453
 57662/100000: episode: 961, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 79.880, mean reward: 4.699 [3.127, 6.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.417, 10.100], loss: 0.209581, mae: 0.414165, mean_q: 4.482435
 57677/100000: episode: 962, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 71.379, mean reward: 4.759 [3.406, 7.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.903, 10.100], loss: 0.164610, mae: 0.417290, mean_q: 4.522747
 57698/100000: episode: 963, duration: 0.105s, episode steps: 21, steps per second: 200, episode reward: 72.797, mean reward: 3.467 [2.540, 4.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.443, 10.100], loss: 0.279415, mae: 0.463327, mean_q: 4.505021
 57715/100000: episode: 964, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 58.055, mean reward: 3.415 [2.698, 5.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.475, 10.100], loss: 0.228818, mae: 0.449676, mean_q: 4.461611
 57737/100000: episode: 965, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 74.208, mean reward: 3.373 [2.381, 5.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.768, 10.100], loss: 0.298669, mae: 0.480756, mean_q: 4.582935
 57756/100000: episode: 966, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 70.283, mean reward: 3.699 [2.780, 6.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.386, 10.100], loss: 0.277156, mae: 0.448141, mean_q: 4.638939
 57773/100000: episode: 967, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 55.130, mean reward: 3.243 [2.540, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.232, 10.100], loss: 0.184985, mae: 0.426539, mean_q: 4.573454
 57784/100000: episode: 968, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 43.164, mean reward: 3.924 [2.578, 5.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.434, 10.100], loss: 0.212202, mae: 0.423298, mean_q: 4.651268
 57801/100000: episode: 969, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 56.888, mean reward: 3.346 [3.062, 4.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.310, 10.100], loss: 0.281699, mae: 0.489583, mean_q: 4.570917
 57816/100000: episode: 970, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 81.275, mean reward: 5.418 [4.081, 9.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.409, 10.100], loss: 0.432947, mae: 0.528207, mean_q: 4.652482
 57835/100000: episode: 971, duration: 0.120s, episode steps: 19, steps per second: 159, episode reward: 71.830, mean reward: 3.781 [2.542, 6.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.274, 10.100], loss: 0.224981, mae: 0.425541, mean_q: 4.472218
 57854/100000: episode: 972, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 71.543, mean reward: 3.765 [2.786, 5.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-1.008, 10.100], loss: 0.238422, mae: 0.478840, mean_q: 4.638199
 57871/100000: episode: 973, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 57.408, mean reward: 3.377 [2.744, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.445, 10.100], loss: 0.191027, mae: 0.439721, mean_q: 4.640057
 57890/100000: episode: 974, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 64.509, mean reward: 3.395 [2.841, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.369, 10.100], loss: 0.273515, mae: 0.472583, mean_q: 4.617052
 57910/100000: episode: 975, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 88.352, mean reward: 4.418 [2.479, 7.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.566, 10.100], loss: 0.214750, mae: 0.443751, mean_q: 4.651824
 57931/100000: episode: 976, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 106.552, mean reward: 5.074 [3.255, 8.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.625, 10.100], loss: 0.310945, mae: 0.493828, mean_q: 4.798461
 57948/100000: episode: 977, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 53.274, mean reward: 3.134 [2.141, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.090, 10.100], loss: 0.345104, mae: 0.486902, mean_q: 4.663156
 57965/100000: episode: 978, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 92.413, mean reward: 5.436 [2.809, 16.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.643, 10.100], loss: 0.308406, mae: 0.508591, mean_q: 4.684792
 57982/100000: episode: 979, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 47.087, mean reward: 2.770 [2.043, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.535, 10.100], loss: 0.317966, mae: 0.480771, mean_q: 4.750107
 58005/100000: episode: 980, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 74.164, mean reward: 3.225 [2.559, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.397, 10.100], loss: 0.356515, mae: 0.500664, mean_q: 4.757353
 58026/100000: episode: 981, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 46.891, mean reward: 2.233 [1.685, 3.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.260, 10.100], loss: 0.373890, mae: 0.516014, mean_q: 4.852576
 58047/100000: episode: 982, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 97.057, mean reward: 4.622 [2.231, 9.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.176, 10.100], loss: 0.289048, mae: 0.476116, mean_q: 4.844403
 58064/100000: episode: 983, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 76.031, mean reward: 4.472 [3.374, 6.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.275, 10.100], loss: 0.333302, mae: 0.523046, mean_q: 4.862193
 58081/100000: episode: 984, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 53.787, mean reward: 3.164 [2.379, 4.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.438, 10.100], loss: 0.249232, mae: 0.457283, mean_q: 4.737032
 58102/100000: episode: 985, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 81.984, mean reward: 3.904 [3.044, 5.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.399, 10.100], loss: 0.266875, mae: 0.509286, mean_q: 4.965369
 58113/100000: episode: 986, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 37.836, mean reward: 3.440 [2.802, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.470, 10.100], loss: 0.168170, mae: 0.426288, mean_q: 4.885263
 58130/100000: episode: 987, duration: 0.082s, episode steps: 17, steps per second: 206, episode reward: 38.688, mean reward: 2.276 [1.912, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.256, 10.100], loss: 0.272247, mae: 0.484645, mean_q: 4.903604
 58153/100000: episode: 988, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 69.802, mean reward: 3.035 [2.258, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.160, 10.100], loss: 0.308903, mae: 0.454788, mean_q: 4.872974
 58174/100000: episode: 989, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 61.017, mean reward: 2.906 [2.282, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.315, 10.100], loss: 0.432523, mae: 0.538931, mean_q: 4.893085
 58193/100000: episode: 990, duration: 0.092s, episode steps: 19, steps per second: 206, episode reward: 66.150, mean reward: 3.482 [2.894, 6.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.371, 10.100], loss: 0.273854, mae: 0.458531, mean_q: 4.743733
 58213/100000: episode: 991, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 52.236, mean reward: 2.612 [2.286, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.416, 10.100], loss: 0.279269, mae: 0.490716, mean_q: 4.847836
 58230/100000: episode: 992, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 84.243, mean reward: 4.955 [2.719, 12.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.464, 10.100], loss: 0.328141, mae: 0.532173, mean_q: 4.885145
 58247/100000: episode: 993, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 59.789, mean reward: 3.517 [2.528, 4.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.438, 10.100], loss: 0.307895, mae: 0.540767, mean_q: 4.824861
 58262/100000: episode: 994, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 75.558, mean reward: 5.037 [3.341, 8.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.971, 10.100], loss: 0.687072, mae: 0.636376, mean_q: 5.076048
 58285/100000: episode: 995, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 426.394, mean reward: 18.539 [4.045, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.562, 10.100], loss: 0.411912, mae: 0.518197, mean_q: 4.945099
 58307/100000: episode: 996, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 100.133, mean reward: 4.552 [2.610, 13.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.851, 10.100], loss: 7.174437, mae: 0.809766, mean_q: 5.093121
 58327/100000: episode: 997, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 68.880, mean reward: 3.444 [2.657, 5.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.207, 10.100], loss: 5.730766, mae: 0.864165, mean_q: 5.168393
 58348/100000: episode: 998, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 62.197, mean reward: 2.962 [2.219, 4.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.329, 10.100], loss: 5.828072, mae: 0.902890, mean_q: 5.044764
 58370/100000: episode: 999, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 118.453, mean reward: 5.384 [3.458, 12.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.550, 10.100], loss: 5.961354, mae: 1.058226, mean_q: 5.154297
 58389/100000: episode: 1000, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 137.007, mean reward: 7.211 [3.525, 16.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.343, 10.100], loss: 0.525662, mae: 0.626189, mean_q: 5.063925
 58400/100000: episode: 1001, duration: 0.058s, episode steps: 11, steps per second: 190, episode reward: 44.674, mean reward: 4.061 [2.728, 7.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.433, 10.100], loss: 13.271860, mae: 1.001978, mean_q: 5.489349
 58420/100000: episode: 1002, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 55.559, mean reward: 2.778 [2.263, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.317, 10.100], loss: 0.593986, mae: 0.640745, mean_q: 4.934940
 58431/100000: episode: 1003, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 39.739, mean reward: 3.613 [2.639, 4.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.353, 10.100], loss: 0.990184, mae: 0.687502, mean_q: 5.211838
 58451/100000: episode: 1004, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 80.501, mean reward: 4.025 [2.583, 5.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.240, 10.100], loss: 0.393096, mae: 0.546161, mean_q: 5.077298
 58472/100000: episode: 1005, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 158.458, mean reward: 7.546 [3.382, 21.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.639, 10.100], loss: 7.123855, mae: 0.814337, mean_q: 5.243032
 58487/100000: episode: 1006, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 76.530, mean reward: 5.102 [3.406, 6.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.188, 10.100], loss: 0.664150, mae: 0.762334, mean_q: 5.087756
 58504/100000: episode: 1007, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 69.506, mean reward: 4.089 [2.662, 7.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.438, 10.100], loss: 0.734930, mae: 0.608463, mean_q: 4.914777
 58525/100000: episode: 1008, duration: 0.107s, episode steps: 21, steps per second: 196, episode reward: 79.255, mean reward: 3.774 [2.340, 8.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.388, 10.100], loss: 5.707017, mae: 0.923845, mean_q: 5.342921
 58546/100000: episode: 1009, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 92.017, mean reward: 4.382 [2.935, 6.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.280, 10.100], loss: 5.756549, mae: 0.857824, mean_q: 5.231930
 58557/100000: episode: 1010, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 49.975, mean reward: 4.543 [3.124, 7.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.471, 10.100], loss: 0.830811, mae: 0.634702, mean_q: 5.030456
 58576/100000: episode: 1011, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 97.715, mean reward: 5.143 [2.623, 13.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.398, 10.100], loss: 0.595212, mae: 0.654153, mean_q: 5.137259
 58595/100000: episode: 1012, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 95.758, mean reward: 5.040 [3.119, 8.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.401, 10.100], loss: 8.405925, mae: 0.838416, mean_q: 5.187871
 58614/100000: episode: 1013, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 72.254, mean reward: 3.803 [2.856, 4.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.567, 10.100], loss: 1.174245, mae: 0.852443, mean_q: 5.394495
 58633/100000: episode: 1014, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 91.301, mean reward: 4.805 [2.343, 7.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.286, 10.100], loss: 0.575425, mae: 0.621016, mean_q: 5.157294
 58652/100000: episode: 1015, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 79.961, mean reward: 4.208 [2.531, 7.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.125, 10.100], loss: 0.872540, mae: 0.682546, mean_q: 5.267228
 58671/100000: episode: 1016, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 77.908, mean reward: 4.100 [3.261, 6.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.674, 10.100], loss: 1.223509, mae: 0.722099, mean_q: 5.381968
 58690/100000: episode: 1017, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 62.678, mean reward: 3.299 [2.358, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.365, 10.100], loss: 8.105158, mae: 0.986383, mean_q: 5.562962
 58709/100000: episode: 1018, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 63.796, mean reward: 3.358 [2.658, 5.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.764, 10.100], loss: 0.924503, mae: 0.769814, mean_q: 5.476542
 58731/100000: episode: 1019, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 75.336, mean reward: 3.424 [2.889, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.736, 10.100], loss: 0.624627, mae: 0.670466, mean_q: 5.383523
 58750/100000: episode: 1020, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 56.566, mean reward: 2.977 [2.279, 4.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.229, 10.100], loss: 0.634563, mae: 0.641250, mean_q: 5.510438
 58765/100000: episode: 1021, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 48.512, mean reward: 3.234 [2.130, 5.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.334, 10.100], loss: 1.313172, mae: 0.780957, mean_q: 5.542277
 58780/100000: episode: 1022, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 50.022, mean reward: 3.335 [2.660, 4.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.599, 10.100], loss: 1.710599, mae: 0.825190, mean_q: 5.533321
 58800/100000: episode: 1023, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 121.641, mean reward: 6.082 [2.485, 13.224], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.775, 10.100], loss: 1.522457, mae: 0.811634, mean_q: 5.453846
 58819/100000: episode: 1024, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 72.914, mean reward: 3.838 [2.805, 5.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.181, 10.100], loss: 0.697886, mae: 0.657740, mean_q: 5.504100
 58839/100000: episode: 1025, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 79.803, mean reward: 3.990 [2.594, 6.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.402, 10.100], loss: 0.527434, mae: 0.644190, mean_q: 5.405236
 58856/100000: episode: 1026, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 57.598, mean reward: 3.388 [2.388, 4.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.470, 10.100], loss: 0.858664, mae: 0.690590, mean_q: 5.464527
 58873/100000: episode: 1027, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 59.068, mean reward: 3.475 [2.379, 4.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.900, 10.100], loss: 0.601896, mae: 0.631855, mean_q: 5.549571
 58896/100000: episode: 1028, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 103.485, mean reward: 4.499 [3.081, 8.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.300, 10.100], loss: 0.948650, mae: 0.715706, mean_q: 5.487340
 58919/100000: episode: 1029, duration: 0.110s, episode steps: 23, steps per second: 209, episode reward: 146.998, mean reward: 6.391 [3.196, 17.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.232, 10.100], loss: 0.734194, mae: 0.700643, mean_q: 5.640886
 58938/100000: episode: 1030, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 66.607, mean reward: 3.506 [2.799, 4.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.404, 10.100], loss: 1.102110, mae: 0.730241, mean_q: 5.488514
 58955/100000: episode: 1031, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 57.795, mean reward: 3.400 [2.914, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.290, 10.100], loss: 0.941056, mae: 0.806075, mean_q: 5.664518
 58976/100000: episode: 1032, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 76.020, mean reward: 3.620 [2.733, 5.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.306, 10.100], loss: 5.750379, mae: 0.950727, mean_q: 5.796248
 58987/100000: episode: 1033, duration: 0.062s, episode steps: 11, steps per second: 179, episode reward: 47.463, mean reward: 4.315 [2.370, 13.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.373, 10.100], loss: 0.623122, mae: 0.727274, mean_q: 5.858931
 59002/100000: episode: 1034, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 91.990, mean reward: 6.133 [4.096, 10.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.436, 10.100], loss: 0.734928, mae: 0.702318, mean_q: 5.624511
 59019/100000: episode: 1035, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 68.519, mean reward: 4.031 [3.347, 5.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.645, 10.100], loss: 0.669460, mae: 0.688266, mean_q: 5.593037
 59038/100000: episode: 1036, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 102.290, mean reward: 5.384 [2.194, 11.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.151, 10.100], loss: 0.858900, mae: 0.737700, mean_q: 5.681211
 59057/100000: episode: 1037, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 110.322, mean reward: 5.806 [4.459, 9.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.437, 10.100], loss: 1.187004, mae: 0.790613, mean_q: 5.771997
 59076/100000: episode: 1038, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 87.911, mean reward: 4.627 [3.102, 7.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.383, 10.100], loss: 0.468149, mae: 0.638187, mean_q: 5.809922
 59095/100000: episode: 1039, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 81.315, mean reward: 4.280 [2.881, 5.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.850, 10.100], loss: 0.437138, mae: 0.606489, mean_q: 5.640321
[Info] 3-TH LEVEL FOUND: 11.409151077270508, Considering 10/90 traces
 59106/100000: episode: 1040, duration: 4.366s, episode steps: 11, steps per second: 3, episode reward: 51.462, mean reward: 4.678 [2.671, 14.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.491, 10.100], loss: 1.301397, mae: 0.807388, mean_q: 5.963875
 59114/100000: episode: 1041, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 49.991, mean reward: 6.249 [3.276, 12.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.382, 10.100], loss: 0.630969, mae: 0.746017, mean_q: 5.982666
 59126/100000: episode: 1042, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 39.896, mean reward: 3.325 [2.871, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.354, 10.100], loss: 0.678185, mae: 0.722837, mean_q: 5.800752
 59141/100000: episode: 1043, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 90.829, mean reward: 6.055 [4.192, 8.371], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.434, 10.100], loss: 1.434119, mae: 0.791150, mean_q: 5.736563
 59156/100000: episode: 1044, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 75.382, mean reward: 5.025 [2.487, 8.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.540, 10.100], loss: 1.299803, mae: 0.792020, mean_q: 5.774765
 59166/100000: episode: 1045, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 54.701, mean reward: 5.470 [4.186, 8.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.486, 10.100], loss: 1.551628, mae: 0.777507, mean_q: 5.833467
 59177/100000: episode: 1046, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 80.159, mean reward: 7.287 [3.965, 10.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.297, 10.100], loss: 1.425381, mae: 0.783050, mean_q: 5.891565
 59192/100000: episode: 1047, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 69.872, mean reward: 4.658 [3.011, 6.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.947, 10.100], loss: 0.564628, mae: 0.715148, mean_q: 5.848520
 59202/100000: episode: 1048, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 66.036, mean reward: 6.604 [4.814, 8.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.443, 10.100], loss: 1.250969, mae: 0.828832, mean_q: 5.949540
 59217/100000: episode: 1049, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 90.001, mean reward: 6.000 [4.310, 11.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.404, 10.100], loss: 1.358033, mae: 0.869335, mean_q: 5.883129
 59225/100000: episode: 1050, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 37.730, mean reward: 4.716 [3.290, 7.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.530, 10.100], loss: 1.538857, mae: 0.804187, mean_q: 6.073952
 59240/100000: episode: 1051, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 100.723, mean reward: 6.715 [3.825, 19.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.532, 10.100], loss: 0.736471, mae: 0.724141, mean_q: 5.993053
 59248/100000: episode: 1052, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 35.085, mean reward: 4.386 [3.085, 7.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.504, 10.100], loss: 0.869201, mae: 0.794144, mean_q: 5.842979
 59263/100000: episode: 1053, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 132.342, mean reward: 8.823 [4.128, 12.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.509, 10.100], loss: 0.573496, mae: 0.739715, mean_q: 5.809026
 59278/100000: episode: 1054, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 121.637, mean reward: 8.109 [4.385, 15.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.638, 10.100], loss: 0.911843, mae: 0.761346, mean_q: 6.048038
 59290/100000: episode: 1055, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 106.427, mean reward: 8.869 [3.884, 16.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.189, 10.100], loss: 0.816491, mae: 0.742791, mean_q: 5.769616
 59305/100000: episode: 1056, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 61.164, mean reward: 4.078 [2.818, 6.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.618, 10.100], loss: 14.805730, mae: 1.357028, mean_q: 6.255047
 59316/100000: episode: 1057, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 224.964, mean reward: 20.451 [6.722, 48.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.887, 10.100], loss: 0.970512, mae: 0.993178, mean_q: 5.852051
 59331/100000: episode: 1058, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 75.302, mean reward: 5.020 [2.642, 10.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.341, 10.100], loss: 1.549644, mae: 0.948161, mean_q: 6.374374
 59341/100000: episode: 1059, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 69.702, mean reward: 6.970 [5.832, 8.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.463, 10.100], loss: 1.078899, mae: 0.856275, mean_q: 6.091421
 59356/100000: episode: 1060, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 81.247, mean reward: 5.416 [3.853, 9.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.531, 10.100], loss: 1.111436, mae: 0.886072, mean_q: 6.256655
 59371/100000: episode: 1061, duration: 0.074s, episode steps: 15, steps per second: 204, episode reward: 65.125, mean reward: 4.342 [3.522, 5.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.013, 10.100], loss: 11.967306, mae: 1.197476, mean_q: 6.354314
 59386/100000: episode: 1062, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 77.061, mean reward: 5.137 [3.017, 8.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.631, 10.100], loss: 2.414751, mae: 1.161408, mean_q: 6.131304
 59397/100000: episode: 1063, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 73.114, mean reward: 6.647 [5.210, 9.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.486, 10.100], loss: 13.377181, mae: 1.349036, mean_q: 6.619185
 59413/100000: episode: 1064, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 116.613, mean reward: 7.288 [2.909, 28.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.391, 10.100], loss: 1.736292, mae: 0.997905, mean_q: 6.246638
 59429/100000: episode: 1065, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 124.214, mean reward: 7.763 [2.732, 33.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.405, 10.100], loss: 16.381701, mae: 1.558168, mean_q: 6.784324
 59445/100000: episode: 1066, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 88.512, mean reward: 5.532 [3.280, 11.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.381, 10.100], loss: 10.970552, mae: 1.500052, mean_q: 6.561820
 59460/100000: episode: 1067, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 81.294, mean reward: 5.420 [3.871, 8.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.380, 10.100], loss: 2.825681, mae: 0.997747, mean_q: 6.303335
 59475/100000: episode: 1068, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 65.298, mean reward: 4.353 [3.357, 6.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.445, 10.100], loss: 1.862033, mae: 0.984307, mean_q: 6.300944
[Info] FALSIFICATION!
[Info] Levels: [5.5868053, 6.687321, 11.409151, 15.192022]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.03]
[Info] Error Prob: 3.0000000000000008e-05

 59484/100000: episode: 1069, duration: 4.553s, episode steps: 9, steps per second: 2, episode reward: 254.898, mean reward: 28.322 [7.345, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.244, 9.952], loss: 2.884093, mae: 1.065459, mean_q: 6.497505
 59584/100000: episode: 1070, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 191.614, mean reward: 1.916 [1.459, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.844, 10.322], loss: 4.740230, mae: 1.022052, mean_q: 6.405346
 59684/100000: episode: 1071, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 190.445, mean reward: 1.904 [1.489, 4.904], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.397, 10.098], loss: 4.204108, mae: 1.089965, mean_q: 6.466396
 59784/100000: episode: 1072, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 183.170, mean reward: 1.832 [1.449, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.405, 10.119], loss: 2.961519, mae: 0.979761, mean_q: 6.417518
 59884/100000: episode: 1073, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 189.360, mean reward: 1.894 [1.469, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.626, 10.199], loss: 3.977318, mae: 0.983794, mean_q: 6.396007
 59984/100000: episode: 1074, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.050, mean reward: 1.871 [1.448, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.782, 10.098], loss: 4.696109, mae: 1.021413, mean_q: 6.413979
 60084/100000: episode: 1075, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 205.353, mean reward: 2.054 [1.507, 4.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.346, 10.284], loss: 6.263880, mae: 1.112464, mean_q: 6.506186
 60184/100000: episode: 1076, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.626, mean reward: 1.946 [1.499, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.421, 10.354], loss: 7.209283, mae: 1.168163, mean_q: 6.449662
 60284/100000: episode: 1077, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 212.222, mean reward: 2.122 [1.436, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.385, 10.098], loss: 3.548120, mae: 1.079463, mean_q: 6.504098
 60384/100000: episode: 1078, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 188.526, mean reward: 1.885 [1.451, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.415, 10.098], loss: 2.457707, mae: 0.987767, mean_q: 6.522649
 60484/100000: episode: 1079, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 195.884, mean reward: 1.959 [1.454, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.713, 10.125], loss: 9.463029, mae: 1.351892, mean_q: 6.626333
 60584/100000: episode: 1080, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 193.958, mean reward: 1.940 [1.476, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.937, 10.245], loss: 5.420457, mae: 1.152768, mean_q: 6.598879
 60684/100000: episode: 1081, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 195.641, mean reward: 1.956 [1.504, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.464, 10.267], loss: 3.427786, mae: 0.942711, mean_q: 6.287539
 60784/100000: episode: 1082, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 180.063, mean reward: 1.801 [1.466, 2.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.859, 10.352], loss: 2.940929, mae: 1.013218, mean_q: 6.395410
 60884/100000: episode: 1083, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 208.832, mean reward: 2.088 [1.478, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.102, 10.098], loss: 2.021036, mae: 0.921513, mean_q: 6.319019
 60984/100000: episode: 1084, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.829, mean reward: 1.898 [1.463, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.279, 10.098], loss: 4.145001, mae: 0.985053, mean_q: 6.281221
 61084/100000: episode: 1085, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 184.597, mean reward: 1.846 [1.432, 2.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.649, 10.145], loss: 5.138874, mae: 1.008321, mean_q: 6.343160
 61184/100000: episode: 1086, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.114, mean reward: 1.881 [1.441, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.150, 10.098], loss: 3.035119, mae: 0.923496, mean_q: 6.195014
 61284/100000: episode: 1087, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 198.737, mean reward: 1.987 [1.465, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.273, 10.098], loss: 3.108283, mae: 0.906202, mean_q: 6.120845
 61384/100000: episode: 1088, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 198.550, mean reward: 1.986 [1.509, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.768, 10.250], loss: 6.025274, mae: 1.104717, mean_q: 6.277100
 61484/100000: episode: 1089, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 217.921, mean reward: 2.179 [1.461, 4.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.083, 10.467], loss: 5.067148, mae: 1.000657, mean_q: 6.216430
 61584/100000: episode: 1090, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 204.199, mean reward: 2.042 [1.458, 6.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.385, 10.098], loss: 1.395483, mae: 0.839461, mean_q: 6.074815
 61684/100000: episode: 1091, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 197.521, mean reward: 1.975 [1.462, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.138, 10.098], loss: 2.655140, mae: 0.858136, mean_q: 6.118936
 61784/100000: episode: 1092, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 182.456, mean reward: 1.825 [1.463, 2.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.261, 10.098], loss: 5.841119, mae: 1.035998, mean_q: 6.222521
 61884/100000: episode: 1093, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 183.077, mean reward: 1.831 [1.467, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.979, 10.214], loss: 2.452379, mae: 0.885072, mean_q: 6.116235
 61984/100000: episode: 1094, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 195.411, mean reward: 1.954 [1.477, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.108, 10.275], loss: 7.273910, mae: 1.262340, mean_q: 6.258997
 62084/100000: episode: 1095, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 180.025, mean reward: 1.800 [1.451, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.749, 10.146], loss: 2.560450, mae: 0.829156, mean_q: 5.935428
 62184/100000: episode: 1096, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 183.101, mean reward: 1.831 [1.444, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.136, 10.247], loss: 4.559484, mae: 0.931577, mean_q: 6.000168
 62284/100000: episode: 1097, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 196.528, mean reward: 1.965 [1.447, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.710, 10.122], loss: 3.198330, mae: 0.847647, mean_q: 5.964219
 62384/100000: episode: 1098, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 199.759, mean reward: 1.998 [1.469, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.114, 10.098], loss: 4.437194, mae: 0.898856, mean_q: 5.932335
 62484/100000: episode: 1099, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 202.230, mean reward: 2.022 [1.433, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.464, 10.098], loss: 2.771783, mae: 0.820740, mean_q: 5.781122
 62584/100000: episode: 1100, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 203.245, mean reward: 2.032 [1.503, 3.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.802, 10.100], loss: 5.384330, mae: 0.991398, mean_q: 5.821398
 62684/100000: episode: 1101, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 184.466, mean reward: 1.845 [1.444, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.506, 10.124], loss: 3.061838, mae: 0.888773, mean_q: 5.773963
 62784/100000: episode: 1102, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 195.378, mean reward: 1.954 [1.494, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.545, 10.098], loss: 6.690600, mae: 0.977203, mean_q: 5.748872
 62884/100000: episode: 1103, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 195.658, mean reward: 1.957 [1.495, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.537, 10.098], loss: 3.860445, mae: 0.822712, mean_q: 5.611738
 62984/100000: episode: 1104, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 203.251, mean reward: 2.033 [1.520, 4.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.347, 10.098], loss: 6.488436, mae: 0.948388, mean_q: 5.598314
 63084/100000: episode: 1105, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 182.317, mean reward: 1.823 [1.465, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.998, 10.119], loss: 5.005533, mae: 0.792876, mean_q: 5.318829
 63184/100000: episode: 1106, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 201.915, mean reward: 2.019 [1.573, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.919, 10.098], loss: 2.759527, mae: 0.758448, mean_q: 5.352844
 63284/100000: episode: 1107, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 195.693, mean reward: 1.957 [1.439, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.944, 10.347], loss: 1.300128, mae: 0.646301, mean_q: 5.204829
 63384/100000: episode: 1108, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 186.166, mean reward: 1.862 [1.446, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.774, 10.236], loss: 3.477158, mae: 0.702474, mean_q: 5.115626
 63484/100000: episode: 1109, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 201.261, mean reward: 2.013 [1.475, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.346, 10.098], loss: 2.784842, mae: 0.661960, mean_q: 4.998165
 63584/100000: episode: 1110, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 211.157, mean reward: 2.112 [1.467, 6.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.493, 10.098], loss: 5.942209, mae: 0.780221, mean_q: 5.044301
 63684/100000: episode: 1111, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 184.966, mean reward: 1.850 [1.438, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.735, 10.104], loss: 5.353516, mae: 0.757465, mean_q: 4.983386
 63784/100000: episode: 1112, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 188.904, mean reward: 1.889 [1.474, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.462, 10.207], loss: 2.078911, mae: 0.610974, mean_q: 4.825793
 63884/100000: episode: 1113, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 183.443, mean reward: 1.834 [1.462, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.310, 10.098], loss: 2.799890, mae: 0.636466, mean_q: 4.787482
 63984/100000: episode: 1114, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 189.726, mean reward: 1.897 [1.450, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.851, 10.301], loss: 2.011983, mae: 0.535981, mean_q: 4.619989
 64084/100000: episode: 1115, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 189.572, mean reward: 1.896 [1.485, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.737, 10.098], loss: 1.813375, mae: 0.520580, mean_q: 4.527050
 64184/100000: episode: 1116, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.342, mean reward: 1.883 [1.501, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.271, 10.185], loss: 2.265621, mae: 0.536063, mean_q: 4.338786
 64284/100000: episode: 1117, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 186.593, mean reward: 1.866 [1.442, 2.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.581, 10.280], loss: 0.419095, mae: 0.376210, mean_q: 4.199402
 64384/100000: episode: 1118, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 207.371, mean reward: 2.074 [1.512, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.186, 10.505], loss: 1.346369, mae: 0.375587, mean_q: 4.002150
 64484/100000: episode: 1119, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 216.365, mean reward: 2.164 [1.469, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.184, 10.098], loss: 0.089056, mae: 0.293970, mean_q: 3.877356
 64584/100000: episode: 1120, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 189.347, mean reward: 1.893 [1.462, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.617, 10.229], loss: 0.080565, mae: 0.279017, mean_q: 3.836195
 64684/100000: episode: 1121, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 199.276, mean reward: 1.993 [1.456, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.379, 10.477], loss: 0.075087, mae: 0.279100, mean_q: 3.835741
 64784/100000: episode: 1122, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 208.411, mean reward: 2.084 [1.470, 9.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.045, 10.098], loss: 0.080929, mae: 0.279212, mean_q: 3.845205
 64884/100000: episode: 1123, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 201.917, mean reward: 2.019 [1.449, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.022, 10.098], loss: 0.076307, mae: 0.272947, mean_q: 3.847427
 64984/100000: episode: 1124, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 189.692, mean reward: 1.897 [1.464, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.458, 10.098], loss: 0.083765, mae: 0.281344, mean_q: 3.845266
 65084/100000: episode: 1125, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.593, mean reward: 1.926 [1.461, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.215, 10.098], loss: 0.098694, mae: 0.295925, mean_q: 3.862375
 65184/100000: episode: 1126, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 187.576, mean reward: 1.876 [1.474, 2.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.364, 10.250], loss: 0.084407, mae: 0.283359, mean_q: 3.847108
 65284/100000: episode: 1127, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 197.075, mean reward: 1.971 [1.482, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.412, 10.210], loss: 0.102987, mae: 0.298107, mean_q: 3.853981
 65384/100000: episode: 1128, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 185.429, mean reward: 1.854 [1.452, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.823, 10.269], loss: 0.096639, mae: 0.289644, mean_q: 3.867057
 65484/100000: episode: 1129, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 198.232, mean reward: 1.982 [1.481, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.876, 10.170], loss: 0.080560, mae: 0.277678, mean_q: 3.838182
 65584/100000: episode: 1130, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 186.853, mean reward: 1.869 [1.446, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.491, 10.152], loss: 0.094554, mae: 0.291525, mean_q: 3.863613
 65684/100000: episode: 1131, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 179.207, mean reward: 1.792 [1.440, 2.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.490, 10.128], loss: 0.093066, mae: 0.277507, mean_q: 3.841362
 65784/100000: episode: 1132, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 187.282, mean reward: 1.873 [1.466, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.934, 10.279], loss: 0.085947, mae: 0.281723, mean_q: 3.845493
 65884/100000: episode: 1133, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 195.592, mean reward: 1.956 [1.459, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.240, 10.226], loss: 0.089560, mae: 0.295374, mean_q: 3.842226
 65984/100000: episode: 1134, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 176.768, mean reward: 1.768 [1.465, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.019, 10.167], loss: 0.097286, mae: 0.285792, mean_q: 3.829561
 66084/100000: episode: 1135, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.022, mean reward: 1.900 [1.448, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.686, 10.332], loss: 0.086592, mae: 0.281441, mean_q: 3.816637
 66184/100000: episode: 1136, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 192.313, mean reward: 1.923 [1.460, 4.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.522, 10.320], loss: 0.085160, mae: 0.286330, mean_q: 3.828785
 66284/100000: episode: 1137, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 201.135, mean reward: 2.011 [1.463, 5.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.845, 10.098], loss: 0.111821, mae: 0.300913, mean_q: 3.823702
 66384/100000: episode: 1138, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 180.689, mean reward: 1.807 [1.447, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.442, 10.099], loss: 0.080894, mae: 0.281017, mean_q: 3.833874
 66484/100000: episode: 1139, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 188.512, mean reward: 1.885 [1.464, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.289, 10.105], loss: 0.078163, mae: 0.280634, mean_q: 3.803872
 66584/100000: episode: 1140, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 183.143, mean reward: 1.831 [1.481, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.217, 10.098], loss: 0.086102, mae: 0.279894, mean_q: 3.821960
 66684/100000: episode: 1141, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 210.374, mean reward: 2.104 [1.458, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.594, 10.361], loss: 0.097450, mae: 0.284680, mean_q: 3.805147
 66784/100000: episode: 1142, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 182.806, mean reward: 1.828 [1.460, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.299, 10.113], loss: 0.082240, mae: 0.275499, mean_q: 3.810009
 66884/100000: episode: 1143, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 188.680, mean reward: 1.887 [1.438, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.369, 10.171], loss: 0.086503, mae: 0.282468, mean_q: 3.812238
 66984/100000: episode: 1144, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 183.910, mean reward: 1.839 [1.443, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.893, 10.185], loss: 0.093128, mae: 0.286359, mean_q: 3.805098
 67084/100000: episode: 1145, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 185.329, mean reward: 1.853 [1.443, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.021, 10.150], loss: 0.096216, mae: 0.291578, mean_q: 3.797599
 67184/100000: episode: 1146, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 200.956, mean reward: 2.010 [1.455, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.554, 10.098], loss: 0.086858, mae: 0.284984, mean_q: 3.808525
 67284/100000: episode: 1147, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 232.128, mean reward: 2.321 [1.461, 4.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.889, 10.098], loss: 0.091962, mae: 0.295356, mean_q: 3.816396
 67384/100000: episode: 1148, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.723, mean reward: 1.887 [1.458, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.694, 10.161], loss: 0.095788, mae: 0.300772, mean_q: 3.837918
 67484/100000: episode: 1149, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 179.301, mean reward: 1.793 [1.441, 2.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.340, 10.098], loss: 0.071559, mae: 0.273816, mean_q: 3.816771
 67584/100000: episode: 1150, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 179.554, mean reward: 1.796 [1.469, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.136, 10.103], loss: 0.092259, mae: 0.291739, mean_q: 3.810315
 67684/100000: episode: 1151, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 203.819, mean reward: 2.038 [1.491, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.789, 10.098], loss: 0.089541, mae: 0.287176, mean_q: 3.804420
 67784/100000: episode: 1152, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.027, mean reward: 1.950 [1.440, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.611, 10.098], loss: 0.093957, mae: 0.277778, mean_q: 3.787677
 67884/100000: episode: 1153, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 195.581, mean reward: 1.956 [1.455, 4.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.317, 10.098], loss: 0.083141, mae: 0.283816, mean_q: 3.806485
 67984/100000: episode: 1154, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 207.841, mean reward: 2.078 [1.460, 4.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.544, 10.098], loss: 0.073123, mae: 0.274517, mean_q: 3.808229
 68084/100000: episode: 1155, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.688, mean reward: 1.887 [1.460, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.803, 10.193], loss: 0.086995, mae: 0.285848, mean_q: 3.811972
 68184/100000: episode: 1156, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 209.622, mean reward: 2.096 [1.437, 4.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.648, 10.133], loss: 0.083165, mae: 0.278820, mean_q: 3.802203
 68284/100000: episode: 1157, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.376, mean reward: 1.874 [1.478, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.193, 10.098], loss: 0.108641, mae: 0.297035, mean_q: 3.830428
 68384/100000: episode: 1158, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.096, mean reward: 1.831 [1.437, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.161, 10.295], loss: 0.087362, mae: 0.283838, mean_q: 3.819311
 68484/100000: episode: 1159, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 202.802, mean reward: 2.028 [1.514, 4.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.678, 10.151], loss: 0.083440, mae: 0.278832, mean_q: 3.814843
 68584/100000: episode: 1160, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 182.598, mean reward: 1.826 [1.448, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.429, 10.115], loss: 0.094827, mae: 0.300627, mean_q: 3.832009
 68684/100000: episode: 1161, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 252.187, mean reward: 2.522 [1.466, 6.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.855, 10.142], loss: 0.084931, mae: 0.289291, mean_q: 3.818148
 68784/100000: episode: 1162, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 202.037, mean reward: 2.020 [1.522, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.742, 10.098], loss: 0.083447, mae: 0.280169, mean_q: 3.813064
 68884/100000: episode: 1163, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 188.030, mean reward: 1.880 [1.468, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.006, 10.098], loss: 0.092521, mae: 0.288843, mean_q: 3.836686
 68984/100000: episode: 1164, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 197.178, mean reward: 1.972 [1.460, 3.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.915, 10.365], loss: 0.083105, mae: 0.288273, mean_q: 3.822729
 69084/100000: episode: 1165, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 193.355, mean reward: 1.934 [1.438, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.931, 10.221], loss: 0.083621, mae: 0.289419, mean_q: 3.846400
 69184/100000: episode: 1166, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 221.956, mean reward: 2.220 [1.480, 5.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.639, 10.098], loss: 0.101476, mae: 0.300023, mean_q: 3.827243
 69284/100000: episode: 1167, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 188.135, mean reward: 1.881 [1.448, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.680, 10.190], loss: 0.088459, mae: 0.296112, mean_q: 3.840156
 69384/100000: episode: 1168, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 181.748, mean reward: 1.817 [1.451, 2.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.018, 10.105], loss: 0.111195, mae: 0.298360, mean_q: 3.841337
[Info] 1-TH LEVEL FOUND: 5.488149166107178, Considering 10/90 traces
 69484/100000: episode: 1169, duration: 4.802s, episode steps: 100, steps per second: 21, episode reward: 190.402, mean reward: 1.904 [1.437, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.934, 10.253], loss: 0.086765, mae: 0.288658, mean_q: 3.837316
 69518/100000: episode: 1170, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 110.229, mean reward: 3.242 [2.119, 5.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.885, 10.100], loss: 0.076182, mae: 0.275261, mean_q: 3.799510
 69552/100000: episode: 1171, duration: 0.200s, episode steps: 34, steps per second: 170, episode reward: 132.957, mean reward: 3.911 [2.122, 40.270], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.317, 10.100], loss: 0.088010, mae: 0.297436, mean_q: 3.858263
 69583/100000: episode: 1172, duration: 0.151s, episode steps: 31, steps per second: 205, episode reward: 71.133, mean reward: 2.295 [1.598, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.968, 10.155], loss: 0.085053, mae: 0.289511, mean_q: 3.846413
 69681/100000: episode: 1173, duration: 0.501s, episode steps: 98, steps per second: 195, episode reward: 180.751, mean reward: 1.844 [1.451, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-1.667, 10.193], loss: 0.085134, mae: 0.292075, mean_q: 3.852278
 69706/100000: episode: 1174, duration: 0.134s, episode steps: 25, steps per second: 186, episode reward: 81.908, mean reward: 3.276 [2.243, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-1.276, 10.100], loss: 0.083039, mae: 0.291775, mean_q: 3.843793
 69731/100000: episode: 1175, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 103.050, mean reward: 4.122 [2.697, 6.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.200, 10.100], loss: 0.115942, mae: 0.333431, mean_q: 3.927982
 69766/100000: episode: 1176, duration: 0.184s, episode steps: 35, steps per second: 190, episode reward: 98.119, mean reward: 2.803 [1.676, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.416, 10.100], loss: 0.793933, mae: 0.419281, mean_q: 3.854549
 69779/100000: episode: 1177, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 41.967, mean reward: 3.228 [2.605, 4.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.262, 10.100], loss: 0.151825, mae: 0.425519, mean_q: 4.010302
 69804/100000: episode: 1178, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 94.602, mean reward: 3.784 [2.675, 7.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.535, 10.100], loss: 0.135795, mae: 0.365525, mean_q: 3.943791
 69857/100000: episode: 1179, duration: 0.272s, episode steps: 53, steps per second: 195, episode reward: 108.541, mean reward: 2.048 [1.686, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.786, 10.100], loss: 0.108173, mae: 0.302945, mean_q: 3.894441
 69890/100000: episode: 1180, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 86.028, mean reward: 2.607 [2.179, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.339, 10.100], loss: 0.096971, mae: 0.300600, mean_q: 3.927878
 69988/100000: episode: 1181, duration: 0.493s, episode steps: 98, steps per second: 199, episode reward: 184.303, mean reward: 1.881 [1.438, 3.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-1.203, 10.199], loss: 0.352772, mae: 0.357292, mean_q: 3.939306
 70019/100000: episode: 1182, duration: 0.148s, episode steps: 31, steps per second: 210, episode reward: 68.712, mean reward: 2.217 [1.743, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.769, 10.278], loss: 0.080965, mae: 0.290484, mean_q: 3.921023
 70117/100000: episode: 1183, duration: 0.490s, episode steps: 98, steps per second: 200, episode reward: 206.977, mean reward: 2.112 [1.499, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.767, 10.484], loss: 0.109013, mae: 0.315040, mean_q: 3.938149
 70151/100000: episode: 1184, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 124.515, mean reward: 3.662 [2.237, 7.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-1.211, 10.100], loss: 0.098440, mae: 0.300920, mean_q: 3.929751
 70176/100000: episode: 1185, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 70.736, mean reward: 2.829 [2.037, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.318, 10.100], loss: 1.065450, mae: 0.483299, mean_q: 3.952614
 70229/100000: episode: 1186, duration: 0.260s, episode steps: 53, steps per second: 204, episode reward: 121.337, mean reward: 2.289 [1.456, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.122, 10.100], loss: 0.119527, mae: 0.332303, mean_q: 3.954906
 70264/100000: episode: 1187, duration: 0.187s, episode steps: 35, steps per second: 187, episode reward: 73.174, mean reward: 2.091 [1.493, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.397, 10.100], loss: 0.110057, mae: 0.315739, mean_q: 3.984036
 70292/100000: episode: 1188, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 65.004, mean reward: 2.322 [1.552, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.975, 10.306], loss: 0.138146, mae: 0.336740, mean_q: 4.016772
 70326/100000: episode: 1189, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 131.477, mean reward: 3.867 [2.255, 12.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.402, 10.100], loss: 0.101917, mae: 0.299030, mean_q: 3.963193
 70381/100000: episode: 1190, duration: 0.286s, episode steps: 55, steps per second: 192, episode reward: 142.238, mean reward: 2.586 [1.715, 4.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.862 [-0.685, 10.100], loss: 0.105635, mae: 0.318983, mean_q: 3.979297
 70415/100000: episode: 1191, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 73.653, mean reward: 2.166 [1.660, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.641, 10.100], loss: 0.105318, mae: 0.316151, mean_q: 4.005469
 70470/100000: episode: 1192, duration: 0.288s, episode steps: 55, steps per second: 191, episode reward: 153.872, mean reward: 2.798 [1.513, 5.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.762, 10.171], loss: 0.104738, mae: 0.319065, mean_q: 4.010689
 70495/100000: episode: 1193, duration: 0.167s, episode steps: 25, steps per second: 150, episode reward: 78.689, mean reward: 3.148 [2.299, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.484, 10.100], loss: 0.131025, mae: 0.349285, mean_q: 4.066810
 70520/100000: episode: 1194, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 70.737, mean reward: 2.829 [2.460, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.484, 10.100], loss: 1.072280, mae: 0.480684, mean_q: 4.075114
 70618/100000: episode: 1195, duration: 0.509s, episode steps: 98, steps per second: 192, episode reward: 185.309, mean reward: 1.891 [1.466, 2.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-1.689, 10.271], loss: 0.135809, mae: 0.336401, mean_q: 4.062882
 70643/100000: episode: 1196, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 65.410, mean reward: 2.616 [2.083, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.330, 10.100], loss: 1.039461, mae: 0.467880, mean_q: 4.088056
 70698/100000: episode: 1197, duration: 0.284s, episode steps: 55, steps per second: 194, episode reward: 125.630, mean reward: 2.284 [1.680, 4.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-1.885, 10.100], loss: 0.154354, mae: 0.381697, mean_q: 4.130481
 70729/100000: episode: 1198, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 71.440, mean reward: 2.305 [1.632, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-1.035, 10.216], loss: 0.113441, mae: 0.321922, mean_q: 4.039449
 70754/100000: episode: 1199, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 63.333, mean reward: 2.533 [2.204, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.479, 10.100], loss: 0.095313, mae: 0.318602, mean_q: 4.102591
 70852/100000: episode: 1200, duration: 0.519s, episode steps: 98, steps per second: 189, episode reward: 192.352, mean reward: 1.963 [1.447, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-1.108, 10.100], loss: 0.357239, mae: 0.362483, mean_q: 4.097369
 70887/100000: episode: 1201, duration: 0.171s, episode steps: 35, steps per second: 205, episode reward: 87.646, mean reward: 2.504 [1.862, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.096, 10.100], loss: 0.124631, mae: 0.340551, mean_q: 4.121830
 70922/100000: episode: 1202, duration: 0.183s, episode steps: 35, steps per second: 191, episode reward: 121.748, mean reward: 3.479 [2.038, 6.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.712, 10.100], loss: 0.778871, mae: 0.433657, mean_q: 4.168828
 71020/100000: episode: 1203, duration: 0.495s, episode steps: 98, steps per second: 198, episode reward: 183.540, mean reward: 1.873 [1.438, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-1.079, 10.100], loss: 0.129058, mae: 0.345267, mean_q: 4.159248
 71051/100000: episode: 1204, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 86.872, mean reward: 2.802 [1.598, 13.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.317, 10.217], loss: 1.589959, mae: 0.539137, mean_q: 4.236680
 71076/100000: episode: 1205, duration: 0.130s, episode steps: 25, steps per second: 193, episode reward: 59.714, mean reward: 2.389 [1.596, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.402, 10.100], loss: 0.159125, mae: 0.399985, mean_q: 4.044000
 71131/100000: episode: 1206, duration: 0.295s, episode steps: 55, steps per second: 186, episode reward: 116.770, mean reward: 2.123 [1.504, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-1.047, 10.150], loss: 0.146365, mae: 0.341483, mean_q: 4.147638
 71144/100000: episode: 1207, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 40.226, mean reward: 3.094 [2.508, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.263, 10.100], loss: 1.770855, mae: 0.452393, mean_q: 4.188288
 71157/100000: episode: 1208, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 35.888, mean reward: 2.761 [2.214, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.271, 10.100], loss: 0.308221, mae: 0.457525, mean_q: 4.132685
 71190/100000: episode: 1209, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 75.506, mean reward: 2.288 [1.467, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.082, 10.142], loss: 0.130717, mae: 0.350474, mean_q: 4.209154
 71225/100000: episode: 1210, duration: 0.178s, episode steps: 35, steps per second: 196, episode reward: 84.973, mean reward: 2.428 [1.833, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.891, 10.100], loss: 0.797257, mae: 0.395500, mean_q: 4.192545
 71278/100000: episode: 1211, duration: 0.284s, episode steps: 53, steps per second: 187, episode reward: 128.381, mean reward: 2.422 [1.667, 4.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.866 [-2.311, 10.100], loss: 0.167766, mae: 0.369451, mean_q: 4.163702
 71311/100000: episode: 1212, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 74.125, mean reward: 2.246 [1.664, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.226, 10.100], loss: 0.126638, mae: 0.349871, mean_q: 4.198949
 71366/100000: episode: 1213, duration: 0.290s, episode steps: 55, steps per second: 190, episode reward: 118.195, mean reward: 2.149 [1.529, 4.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-1.058, 10.100], loss: 0.153324, mae: 0.349501, mean_q: 4.157088
 71391/100000: episode: 1214, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 52.444, mean reward: 2.098 [1.604, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.106, 10.100], loss: 0.109188, mae: 0.330996, mean_q: 4.225643
 71424/100000: episode: 1215, duration: 0.182s, episode steps: 33, steps per second: 182, episode reward: 82.709, mean reward: 2.506 [1.782, 5.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.542, 10.100], loss: 0.183910, mae: 0.389752, mean_q: 4.273396
 71459/100000: episode: 1216, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 130.889, mean reward: 3.740 [1.989, 6.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.305, 10.100], loss: 0.172139, mae: 0.369507, mean_q: 4.210988
 71494/100000: episode: 1217, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 101.355, mean reward: 2.896 [1.970, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.793, 10.100], loss: 0.131755, mae: 0.351018, mean_q: 4.209653
 71507/100000: episode: 1218, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 43.617, mean reward: 3.355 [2.632, 5.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.288, 10.100], loss: 0.245681, mae: 0.382742, mean_q: 4.305997
 71562/100000: episode: 1219, duration: 0.289s, episode steps: 55, steps per second: 190, episode reward: 115.242, mean reward: 2.095 [1.455, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.144, 10.200], loss: 1.019940, mae: 0.478047, mean_q: 4.294367
 71597/100000: episode: 1220, duration: 0.169s, episode steps: 35, steps per second: 207, episode reward: 71.668, mean reward: 2.048 [1.561, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.180, 10.113], loss: 0.178693, mae: 0.383769, mean_q: 4.223581
 71610/100000: episode: 1221, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 28.841, mean reward: 2.219 [1.909, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.293, 10.100], loss: 0.191295, mae: 0.423514, mean_q: 4.393213
 71644/100000: episode: 1222, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 93.877, mean reward: 2.761 [1.892, 7.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.671, 10.100], loss: 0.181389, mae: 0.368752, mean_q: 4.271673
 71678/100000: episode: 1223, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 73.588, mean reward: 2.164 [1.503, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.329, 10.100], loss: 0.143232, mae: 0.363068, mean_q: 4.290785
 71703/100000: episode: 1224, duration: 0.133s, episode steps: 25, steps per second: 187, episode reward: 109.109, mean reward: 4.364 [2.443, 29.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.488, 10.100], loss: 0.146356, mae: 0.351797, mean_q: 4.225543
 71758/100000: episode: 1225, duration: 0.280s, episode steps: 55, steps per second: 197, episode reward: 177.891, mean reward: 3.234 [1.507, 6.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.718, 10.152], loss: 0.960040, mae: 0.438699, mean_q: 4.352994
 71811/100000: episode: 1226, duration: 0.255s, episode steps: 53, steps per second: 208, episode reward: 103.191, mean reward: 1.947 [1.461, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.956, 10.100], loss: 0.234643, mae: 0.438966, mean_q: 4.338298
 71844/100000: episode: 1227, duration: 0.168s, episode steps: 33, steps per second: 196, episode reward: 95.977, mean reward: 2.908 [2.227, 5.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.668, 10.100], loss: 0.271369, mae: 0.405573, mean_q: 4.308726
 71942/100000: episode: 1228, duration: 0.512s, episode steps: 98, steps per second: 191, episode reward: 205.128, mean reward: 2.093 [1.495, 4.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-1.140, 10.195], loss: 0.226147, mae: 0.396396, mean_q: 4.340389
 71977/100000: episode: 1229, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 112.862, mean reward: 3.225 [2.275, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.705, 10.100], loss: 0.481812, mae: 0.393742, mean_q: 4.405966
 72010/100000: episode: 1230, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 94.122, mean reward: 2.852 [1.891, 4.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.437, 10.100], loss: 0.215945, mae: 0.385657, mean_q: 4.354706
 72041/100000: episode: 1231, duration: 0.161s, episode steps: 31, steps per second: 192, episode reward: 69.479, mean reward: 2.241 [1.450, 4.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.217, 10.208], loss: 1.016770, mae: 0.508995, mean_q: 4.413316
 72074/100000: episode: 1232, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 70.928, mean reward: 2.149 [1.539, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.344, 10.100], loss: 0.202897, mae: 0.375100, mean_q: 4.334623
 72087/100000: episode: 1233, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 35.643, mean reward: 2.742 [2.255, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.193, 10.100], loss: 1.801102, mae: 0.491626, mean_q: 4.394317
 72115/100000: episode: 1234, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 83.168, mean reward: 2.970 [1.730, 5.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.527, 10.454], loss: 0.196393, mae: 0.422206, mean_q: 4.329900
 72213/100000: episode: 1235, duration: 0.492s, episode steps: 98, steps per second: 199, episode reward: 190.936, mean reward: 1.948 [1.449, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.862, 10.100], loss: 0.427046, mae: 0.411741, mean_q: 4.374484
 72247/100000: episode: 1236, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 117.877, mean reward: 3.467 [2.382, 6.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.293, 10.100], loss: 0.516029, mae: 0.428176, mean_q: 4.422001
 72300/100000: episode: 1237, duration: 0.280s, episode steps: 53, steps per second: 189, episode reward: 105.011, mean reward: 1.981 [1.479, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.035, 10.140], loss: 0.162136, mae: 0.390557, mean_q: 4.371440
 72398/100000: episode: 1238, duration: 0.523s, episode steps: 98, steps per second: 188, episode reward: 192.281, mean reward: 1.962 [1.469, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.458 [-0.321, 10.100], loss: 0.313439, mae: 0.409267, mean_q: 4.412199
 72431/100000: episode: 1239, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 72.938, mean reward: 2.210 [1.501, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.487, 10.100], loss: 0.234187, mae: 0.392686, mean_q: 4.436955
 72462/100000: episode: 1240, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 64.651, mean reward: 2.086 [1.567, 2.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.482, 10.202], loss: 0.195118, mae: 0.395571, mean_q: 4.403044
[Info] FALSIFICATION!
[Info] Levels: [5.488149, 11.131663]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

 72487/100000: episode: 1241, duration: 4.627s, episode steps: 25, steps per second: 5, episode reward: 251.016, mean reward: 10.041 [2.248, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.307, 10.100], loss: 0.281534, mae: 0.432640, mean_q: 4.445949
 72587/100000: episode: 1242, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 198.032, mean reward: 1.980 [1.440, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.027, 10.146], loss: 0.800974, mae: 0.468458, mean_q: 4.460502
 72687/100000: episode: 1243, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 189.618, mean reward: 1.896 [1.448, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.739, 10.205], loss: 0.589433, mae: 0.472571, mean_q: 4.509100
 72787/100000: episode: 1244, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 182.079, mean reward: 1.821 [1.461, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.613, 10.372], loss: 1.613672, mae: 0.440085, mean_q: 4.456847
 72887/100000: episode: 1245, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 192.831, mean reward: 1.928 [1.458, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.184, 10.098], loss: 1.802236, mae: 0.650957, mean_q: 4.590677
 72987/100000: episode: 1246, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 193.458, mean reward: 1.935 [1.460, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.728, 10.303], loss: 2.991353, mae: 0.572562, mean_q: 4.530123
 73087/100000: episode: 1247, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 188.670, mean reward: 1.887 [1.445, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.168, 10.111], loss: 1.778949, mae: 0.570432, mean_q: 4.524556
 73187/100000: episode: 1248, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 193.194, mean reward: 1.932 [1.458, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.858, 10.193], loss: 1.871380, mae: 0.630087, mean_q: 4.492663
 73287/100000: episode: 1249, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 201.662, mean reward: 2.017 [1.489, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.183, 10.098], loss: 1.646345, mae: 0.476067, mean_q: 4.405960
 73387/100000: episode: 1250, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 178.259, mean reward: 1.783 [1.467, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.889, 10.138], loss: 0.484163, mae: 0.607728, mean_q: 4.424644
 73487/100000: episode: 1251, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 221.489, mean reward: 2.215 [1.501, 4.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.919, 10.098], loss: 2.699675, mae: 0.592923, mean_q: 4.514026
 73587/100000: episode: 1252, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 206.312, mean reward: 2.063 [1.489, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.698, 10.098], loss: 0.460223, mae: 0.458959, mean_q: 4.484899
 73687/100000: episode: 1253, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 198.620, mean reward: 1.986 [1.452, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.143, 10.098], loss: 1.402478, mae: 0.570963, mean_q: 4.508718
 73787/100000: episode: 1254, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 190.253, mean reward: 1.903 [1.500, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.296, 10.098], loss: 0.492480, mae: 0.458716, mean_q: 4.455343
 73887/100000: episode: 1255, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 188.704, mean reward: 1.887 [1.446, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.035, 10.219], loss: 0.428228, mae: 0.434087, mean_q: 4.427422
 73987/100000: episode: 1256, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 204.624, mean reward: 2.046 [1.438, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.864, 10.346], loss: 2.952579, mae: 0.833717, mean_q: 4.390369
 74087/100000: episode: 1257, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 181.338, mean reward: 1.813 [1.441, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.724, 10.098], loss: 1.580733, mae: 0.540811, mean_q: 4.420410
 74187/100000: episode: 1258, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 190.478, mean reward: 1.905 [1.452, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.249, 10.098], loss: 1.760281, mae: 0.573542, mean_q: 4.410507
 74287/100000: episode: 1259, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 194.933, mean reward: 1.949 [1.461, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.839, 10.098], loss: 0.802098, mae: 0.487641, mean_q: 4.420836
 74387/100000: episode: 1260, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 187.586, mean reward: 1.876 [1.463, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.098], loss: 1.382928, mae: 0.494671, mean_q: 4.526185
 74487/100000: episode: 1261, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 203.084, mean reward: 2.031 [1.472, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.724, 10.160], loss: 0.501265, mae: 0.456861, mean_q: 4.422993
 74587/100000: episode: 1262, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.797, mean reward: 1.908 [1.466, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.821, 10.301], loss: 1.623097, mae: 0.532953, mean_q: 4.462277
 74687/100000: episode: 1263, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 191.953, mean reward: 1.920 [1.471, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.008, 10.098], loss: 1.221854, mae: 0.502276, mean_q: 4.409688
 74787/100000: episode: 1264, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 181.412, mean reward: 1.814 [1.476, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.096, 10.155], loss: 0.411628, mae: 0.424071, mean_q: 4.323772
 74887/100000: episode: 1265, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 183.906, mean reward: 1.839 [1.482, 2.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.433, 10.284], loss: 1.067493, mae: 0.451862, mean_q: 4.358706
 74987/100000: episode: 1266, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 202.274, mean reward: 2.023 [1.498, 5.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.109, 10.098], loss: 0.401407, mae: 0.443134, mean_q: 4.305949
 75087/100000: episode: 1267, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 207.982, mean reward: 2.080 [1.480, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.097, 10.373], loss: 0.381147, mae: 0.431480, mean_q: 4.377246
 75187/100000: episode: 1268, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 219.799, mean reward: 2.198 [1.487, 6.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.964, 10.639], loss: 1.245824, mae: 0.489809, mean_q: 4.291945
 75287/100000: episode: 1269, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 188.797, mean reward: 1.888 [1.462, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.647, 10.098], loss: 0.325903, mae: 0.394136, mean_q: 4.266320
 75387/100000: episode: 1270, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 230.037, mean reward: 2.300 [1.470, 6.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.375, 10.528], loss: 1.749077, mae: 0.525864, mean_q: 4.392622
 75487/100000: episode: 1271, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 197.615, mean reward: 1.976 [1.466, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.124, 10.306], loss: 1.805585, mae: 0.571839, mean_q: 4.321712
 75587/100000: episode: 1272, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 200.618, mean reward: 2.006 [1.470, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.540, 10.191], loss: 0.361138, mae: 0.411817, mean_q: 4.184937
 75687/100000: episode: 1273, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 189.307, mean reward: 1.893 [1.453, 2.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.682, 10.098], loss: 1.081215, mae: 0.467228, mean_q: 4.253269
 75787/100000: episode: 1274, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 195.175, mean reward: 1.952 [1.439, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.149, 10.098], loss: 0.776815, mae: 0.440649, mean_q: 4.228288
 75887/100000: episode: 1275, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 179.310, mean reward: 1.793 [1.472, 2.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.821, 10.098], loss: 0.500221, mae: 0.432472, mean_q: 4.203635
 75987/100000: episode: 1276, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 183.630, mean reward: 1.836 [1.445, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.517, 10.098], loss: 0.346536, mae: 0.393698, mean_q: 4.177049
 76087/100000: episode: 1277, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 196.731, mean reward: 1.967 [1.475, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.655, 10.125], loss: 0.901602, mae: 0.423512, mean_q: 4.221375
 76187/100000: episode: 1278, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 183.752, mean reward: 1.838 [1.486, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.548, 10.098], loss: 0.639597, mae: 0.420562, mean_q: 4.150122
 76287/100000: episode: 1279, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 208.516, mean reward: 2.085 [1.468, 4.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.868, 10.098], loss: 0.407782, mae: 0.381197, mean_q: 4.122216
 76387/100000: episode: 1280, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.006, mean reward: 1.920 [1.451, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.421, 10.098], loss: 1.203491, mae: 0.435585, mean_q: 4.185210
 76487/100000: episode: 1281, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 180.737, mean reward: 1.807 [1.439, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.579, 10.211], loss: 0.285683, mae: 0.415911, mean_q: 4.036348
 76587/100000: episode: 1282, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 191.257, mean reward: 1.913 [1.461, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.616, 10.303], loss: 0.144591, mae: 0.343590, mean_q: 4.006978
 76687/100000: episode: 1283, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 182.668, mean reward: 1.827 [1.467, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.453, 10.194], loss: 0.156124, mae: 0.355201, mean_q: 4.014297
 76787/100000: episode: 1284, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 188.246, mean reward: 1.882 [1.473, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.729, 10.098], loss: 0.161921, mae: 0.348465, mean_q: 4.001552
 76887/100000: episode: 1285, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 213.065, mean reward: 2.131 [1.483, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.311, 10.162], loss: 1.206938, mae: 0.433956, mean_q: 4.044281
 76987/100000: episode: 1286, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 187.734, mean reward: 1.877 [1.450, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.916, 10.215], loss: 0.457563, mae: 0.387100, mean_q: 3.967432
 77087/100000: episode: 1287, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 194.141, mean reward: 1.941 [1.432, 3.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.047, 10.117], loss: 0.138041, mae: 0.326731, mean_q: 3.927264
 77187/100000: episode: 1288, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 186.036, mean reward: 1.860 [1.464, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.991, 10.258], loss: 0.174679, mae: 0.331774, mean_q: 3.988958
 77287/100000: episode: 1289, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 185.733, mean reward: 1.857 [1.457, 2.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.754, 10.242], loss: 0.746922, mae: 0.377621, mean_q: 3.970997
 77387/100000: episode: 1290, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 184.134, mean reward: 1.841 [1.463, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.984, 10.169], loss: 0.148002, mae: 0.329625, mean_q: 3.889841
 77487/100000: episode: 1291, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.693, mean reward: 1.857 [1.440, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.169, 10.098], loss: 0.139032, mae: 0.318703, mean_q: 3.891256
 77587/100000: episode: 1292, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 194.162, mean reward: 1.942 [1.462, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.138, 10.098], loss: 0.096584, mae: 0.307359, mean_q: 3.825276
 77687/100000: episode: 1293, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 179.117, mean reward: 1.791 [1.473, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.525, 10.193], loss: 0.092121, mae: 0.296377, mean_q: 3.839797
 77787/100000: episode: 1294, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 186.052, mean reward: 1.861 [1.494, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.426, 10.161], loss: 0.080009, mae: 0.283843, mean_q: 3.823047
 77887/100000: episode: 1295, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 198.311, mean reward: 1.983 [1.443, 3.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.782, 10.098], loss: 0.086610, mae: 0.288033, mean_q: 3.816192
 77987/100000: episode: 1296, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 202.198, mean reward: 2.022 [1.438, 5.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.292, 10.098], loss: 0.078143, mae: 0.278416, mean_q: 3.812378
 78087/100000: episode: 1297, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 194.988, mean reward: 1.950 [1.479, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.104, 10.098], loss: 0.087730, mae: 0.293650, mean_q: 3.836425
 78187/100000: episode: 1298, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 203.800, mean reward: 2.038 [1.473, 4.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.688, 10.349], loss: 0.095275, mae: 0.303024, mean_q: 3.836349
 78287/100000: episode: 1299, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 198.435, mean reward: 1.984 [1.450, 4.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.114, 10.098], loss: 0.094122, mae: 0.296927, mean_q: 3.834503
 78387/100000: episode: 1300, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 217.797, mean reward: 2.178 [1.449, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.392, 10.098], loss: 0.088375, mae: 0.291785, mean_q: 3.824986
 78487/100000: episode: 1301, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 192.605, mean reward: 1.926 [1.436, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.657, 10.109], loss: 0.094008, mae: 0.300820, mean_q: 3.844038
 78587/100000: episode: 1302, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.685, mean reward: 1.897 [1.440, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.217, 10.176], loss: 0.094711, mae: 0.291789, mean_q: 3.828215
 78687/100000: episode: 1303, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 181.538, mean reward: 1.815 [1.459, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.753, 10.238], loss: 0.086523, mae: 0.292535, mean_q: 3.836766
 78787/100000: episode: 1304, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 190.351, mean reward: 1.904 [1.490, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.529, 10.177], loss: 0.091412, mae: 0.295961, mean_q: 3.809971
 78887/100000: episode: 1305, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 199.686, mean reward: 1.997 [1.465, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.255, 10.167], loss: 0.102540, mae: 0.304542, mean_q: 3.826882
 78987/100000: episode: 1306, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 207.090, mean reward: 2.071 [1.453, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.685, 10.098], loss: 0.093074, mae: 0.299519, mean_q: 3.822183
 79087/100000: episode: 1307, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 184.712, mean reward: 1.847 [1.471, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.704, 10.281], loss: 0.091193, mae: 0.295807, mean_q: 3.855770
 79187/100000: episode: 1308, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.929, mean reward: 1.889 [1.459, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.116, 10.250], loss: 0.086715, mae: 0.290855, mean_q: 3.813933
 79287/100000: episode: 1309, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 194.407, mean reward: 1.944 [1.462, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.803, 10.098], loss: 0.084564, mae: 0.287680, mean_q: 3.842178
 79387/100000: episode: 1310, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 187.256, mean reward: 1.873 [1.493, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.700, 10.098], loss: 0.083545, mae: 0.282920, mean_q: 3.815106
 79487/100000: episode: 1311, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 186.133, mean reward: 1.861 [1.457, 5.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.798, 10.098], loss: 0.085612, mae: 0.291094, mean_q: 3.813634
 79587/100000: episode: 1312, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 188.890, mean reward: 1.889 [1.459, 5.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.429, 10.098], loss: 0.088004, mae: 0.284916, mean_q: 3.814428
 79687/100000: episode: 1313, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 203.475, mean reward: 2.035 [1.473, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.965, 10.395], loss: 0.103041, mae: 0.297239, mean_q: 3.816905
 79787/100000: episode: 1314, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 190.974, mean reward: 1.910 [1.462, 4.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.792, 10.150], loss: 0.088896, mae: 0.292496, mean_q: 3.834278
 79887/100000: episode: 1315, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 201.903, mean reward: 2.019 [1.498, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.775, 10.380], loss: 0.096125, mae: 0.294183, mean_q: 3.845577
 79987/100000: episode: 1316, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 196.684, mean reward: 1.967 [1.473, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.744, 10.098], loss: 0.090297, mae: 0.290638, mean_q: 3.814075
 80087/100000: episode: 1317, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 218.229, mean reward: 2.182 [1.526, 3.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.365, 10.422], loss: 0.096549, mae: 0.296490, mean_q: 3.835348
 80187/100000: episode: 1318, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 199.010, mean reward: 1.990 [1.479, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.463, 10.098], loss: 0.099633, mae: 0.310249, mean_q: 3.851611
 80287/100000: episode: 1319, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 209.135, mean reward: 2.091 [1.446, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.322, 10.107], loss: 0.091189, mae: 0.295248, mean_q: 3.834622
 80387/100000: episode: 1320, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 201.657, mean reward: 2.017 [1.467, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.474, 10.247], loss: 0.083335, mae: 0.285688, mean_q: 3.818469
 80487/100000: episode: 1321, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 222.057, mean reward: 2.221 [1.484, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.531, 10.098], loss: 0.095481, mae: 0.303929, mean_q: 3.836429
 80587/100000: episode: 1322, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 176.040, mean reward: 1.760 [1.437, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.679, 10.112], loss: 0.091687, mae: 0.294583, mean_q: 3.856385
 80687/100000: episode: 1323, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 199.454, mean reward: 1.995 [1.530, 2.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.680, 10.098], loss: 0.084503, mae: 0.282567, mean_q: 3.841126
 80787/100000: episode: 1324, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 179.360, mean reward: 1.794 [1.488, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.115, 10.178], loss: 0.090956, mae: 0.301547, mean_q: 3.853533
 80887/100000: episode: 1325, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 196.396, mean reward: 1.964 [1.444, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-2.378, 10.184], loss: 0.099960, mae: 0.309880, mean_q: 3.869183
 80987/100000: episode: 1326, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 232.679, mean reward: 2.327 [1.444, 4.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.889, 10.354], loss: 0.087044, mae: 0.298641, mean_q: 3.857644
 81087/100000: episode: 1327, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 188.370, mean reward: 1.884 [1.485, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.960, 10.098], loss: 0.095639, mae: 0.302260, mean_q: 3.842933
 81187/100000: episode: 1328, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 192.147, mean reward: 1.921 [1.442, 5.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.581, 10.098], loss: 0.101293, mae: 0.309088, mean_q: 3.870907
 81287/100000: episode: 1329, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 201.399, mean reward: 2.014 [1.478, 4.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.537, 10.322], loss: 0.113791, mae: 0.317686, mean_q: 3.879462
 81387/100000: episode: 1330, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.165, mean reward: 1.852 [1.442, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.265, 10.098], loss: 0.092612, mae: 0.298601, mean_q: 3.859814
 81487/100000: episode: 1331, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 205.942, mean reward: 2.059 [1.472, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.802, 10.098], loss: 0.099619, mae: 0.306983, mean_q: 3.872921
 81587/100000: episode: 1332, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 178.858, mean reward: 1.789 [1.439, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.604, 10.098], loss: 0.091633, mae: 0.298214, mean_q: 3.868996
 81687/100000: episode: 1333, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 200.022, mean reward: 2.000 [1.481, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.404, 10.160], loss: 0.091576, mae: 0.299586, mean_q: 3.885839
 81787/100000: episode: 1334, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 185.587, mean reward: 1.856 [1.489, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.380, 10.098], loss: 0.101134, mae: 0.307872, mean_q: 3.882635
 81887/100000: episode: 1335, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 182.278, mean reward: 1.823 [1.464, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.724, 10.194], loss: 0.086306, mae: 0.289394, mean_q: 3.851402
 81987/100000: episode: 1336, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.953, mean reward: 1.860 [1.474, 2.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.036, 10.098], loss: 0.087908, mae: 0.289328, mean_q: 3.830857
 82087/100000: episode: 1337, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.530, mean reward: 1.855 [1.441, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.150, 10.098], loss: 0.080544, mae: 0.280840, mean_q: 3.827421
 82187/100000: episode: 1338, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 179.915, mean reward: 1.799 [1.451, 2.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.765, 10.103], loss: 0.077465, mae: 0.279489, mean_q: 3.835739
 82287/100000: episode: 1339, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.045, mean reward: 1.910 [1.435, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.787, 10.098], loss: 0.087894, mae: 0.291360, mean_q: 3.830110
 82387/100000: episode: 1340, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 196.391, mean reward: 1.964 [1.470, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.118, 10.098], loss: 0.102187, mae: 0.301821, mean_q: 3.843509
[Info] 1-TH LEVEL FOUND: 5.639873504638672, Considering 10/90 traces
 82487/100000: episode: 1341, duration: 4.696s, episode steps: 100, steps per second: 21, episode reward: 184.204, mean reward: 1.842 [1.469, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.708, 10.137], loss: 0.088069, mae: 0.292496, mean_q: 3.855953
 82496/100000: episode: 1342, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 27.718, mean reward: 3.080 [2.565, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.492, 10.100], loss: 0.074645, mae: 0.285074, mean_q: 3.824995
 82518/100000: episode: 1343, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 59.299, mean reward: 2.695 [1.972, 5.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.309, 10.346], loss: 0.274592, mae: 0.325923, mean_q: 3.818165
 82537/100000: episode: 1344, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 60.602, mean reward: 3.190 [2.379, 4.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.186, 10.464], loss: 0.116810, mae: 0.348797, mean_q: 3.867804
 82551/100000: episode: 1345, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 50.582, mean reward: 3.613 [2.727, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.563], loss: 0.128108, mae: 0.330696, mean_q: 3.810948
 82565/100000: episode: 1346, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 54.788, mean reward: 3.913 [2.765, 8.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.631], loss: 0.090547, mae: 0.314677, mean_q: 3.885860
 82574/100000: episode: 1347, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 24.213, mean reward: 2.690 [2.093, 4.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.785, 10.451], loss: 0.112075, mae: 0.322708, mean_q: 3.883681
 82593/100000: episode: 1348, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 49.141, mean reward: 2.586 [2.050, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.045, 10.364], loss: 0.117105, mae: 0.333038, mean_q: 3.841168
 82601/100000: episode: 1349, duration: 0.043s, episode steps: 8, steps per second: 184, episode reward: 19.500, mean reward: 2.438 [2.056, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.281, 10.100], loss: 0.125537, mae: 0.348089, mean_q: 3.806257
 82620/100000: episode: 1350, duration: 0.102s, episode steps: 19, steps per second: 185, episode reward: 78.845, mean reward: 4.150 [2.921, 7.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.580], loss: 0.117234, mae: 0.343644, mean_q: 3.889431
 82642/100000: episode: 1351, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 60.733, mean reward: 2.761 [1.946, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.355], loss: 0.146794, mae: 0.349958, mean_q: 3.851115
 82664/100000: episode: 1352, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 103.016, mean reward: 4.683 [3.378, 10.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.560, 10.615], loss: 0.117816, mae: 0.330898, mean_q: 3.905187
 82672/100000: episode: 1353, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 21.972, mean reward: 2.746 [2.188, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-1.083, 10.100], loss: 0.168895, mae: 0.358165, mean_q: 3.877629
 82720/100000: episode: 1354, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 96.636, mean reward: 2.013 [1.475, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.373, 10.100], loss: 0.118895, mae: 0.325139, mean_q: 3.927808
 82738/100000: episode: 1355, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 55.137, mean reward: 3.063 [2.485, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.262, 10.100], loss: 0.137035, mae: 0.328031, mean_q: 3.929821
 82747/100000: episode: 1356, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 31.931, mean reward: 3.548 [2.373, 7.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.455], loss: 0.168904, mae: 0.391412, mean_q: 3.982991
 82756/100000: episode: 1357, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 34.615, mean reward: 3.846 [3.387, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.707, 10.100], loss: 0.098216, mae: 0.326208, mean_q: 4.029936
[Info] FALSIFICATION!
[Info] Levels: [5.6398735, 5.3806624]
[Info] Cond. Prob: [0.1, 0.27]
[Info] Error Prob: 0.027000000000000003

 82774/100000: episode: 1358, duration: 4.555s, episode steps: 18, steps per second: 4, episode reward: 275.758, mean reward: 15.320 [2.561, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.603, 10.100], loss: 0.133744, mae: 0.342521, mean_q: 3.955460
 82874/100000: episode: 1359, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 208.614, mean reward: 2.086 [1.471, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.653, 10.121], loss: 1.635187, mae: 0.393302, mean_q: 3.973322
 82974/100000: episode: 1360, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 187.593, mean reward: 1.876 [1.446, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.132, 10.151], loss: 4.510314, mae: 0.615961, mean_q: 3.998419
 83074/100000: episode: 1361, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 206.666, mean reward: 2.067 [1.496, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.807, 10.098], loss: 4.173835, mae: 0.528618, mean_q: 3.996440
 83174/100000: episode: 1362, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 184.178, mean reward: 1.842 [1.499, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.572, 10.171], loss: 2.657262, mae: 0.508145, mean_q: 4.005270
 83274/100000: episode: 1363, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 206.397, mean reward: 2.064 [1.542, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.463, 10.344], loss: 0.206188, mae: 0.403403, mean_q: 3.974304
 83374/100000: episode: 1364, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 222.296, mean reward: 2.223 [1.501, 8.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.927, 10.098], loss: 2.456703, mae: 0.452248, mean_q: 4.023043
 83474/100000: episode: 1365, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 189.822, mean reward: 1.898 [1.464, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.629, 10.119], loss: 0.173310, mae: 0.375772, mean_q: 3.980865
 83574/100000: episode: 1366, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 215.585, mean reward: 2.156 [1.491, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.950, 10.337], loss: 1.504795, mae: 0.422319, mean_q: 4.006760
 83674/100000: episode: 1367, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 195.775, mean reward: 1.958 [1.458, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.993, 10.210], loss: 3.785091, mae: 0.568712, mean_q: 4.128475
 83774/100000: episode: 1368, duration: 0.519s, episode steps: 100, steps per second: 192, episode reward: 196.167, mean reward: 1.962 [1.446, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.518, 10.098], loss: 3.568317, mae: 0.527125, mean_q: 4.139660
 83874/100000: episode: 1369, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 194.725, mean reward: 1.947 [1.476, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.778, 10.168], loss: 1.339087, mae: 0.436455, mean_q: 4.069727
 83974/100000: episode: 1370, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 211.005, mean reward: 2.110 [1.463, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.893, 10.165], loss: 2.897810, mae: 0.507052, mean_q: 4.055963
 84074/100000: episode: 1371, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 238.933, mean reward: 2.389 [1.457, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.704, 10.098], loss: 2.906627, mae: 0.506560, mean_q: 4.130280
 84174/100000: episode: 1372, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 211.398, mean reward: 2.114 [1.513, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.981, 10.098], loss: 1.906340, mae: 0.497971, mean_q: 4.106608
 84274/100000: episode: 1373, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 183.062, mean reward: 1.831 [1.433, 3.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.910, 10.156], loss: 3.170362, mae: 0.527822, mean_q: 4.140883
 84374/100000: episode: 1374, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 185.836, mean reward: 1.858 [1.470, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.072, 10.098], loss: 3.121044, mae: 0.554221, mean_q: 4.197579
 84474/100000: episode: 1375, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 227.612, mean reward: 2.276 [1.459, 7.526], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.399, 10.494], loss: 0.213203, mae: 0.406965, mean_q: 4.057308
 84574/100000: episode: 1376, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.046, mean reward: 1.910 [1.460, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.228, 10.098], loss: 0.233691, mae: 0.395041, mean_q: 4.081562
 84674/100000: episode: 1377, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 196.218, mean reward: 1.962 [1.485, 5.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.858, 10.098], loss: 1.053285, mae: 0.422507, mean_q: 4.102257
 84774/100000: episode: 1378, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 178.637, mean reward: 1.786 [1.457, 2.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.271, 10.157], loss: 1.532962, mae: 0.461831, mean_q: 4.135845
 84874/100000: episode: 1379, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 176.224, mean reward: 1.762 [1.477, 2.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.692, 10.154], loss: 1.055012, mae: 0.457660, mean_q: 4.099085
 84974/100000: episode: 1380, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 181.339, mean reward: 1.813 [1.437, 2.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.517, 10.108], loss: 2.922627, mae: 0.526840, mean_q: 4.153820
 85074/100000: episode: 1381, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 193.212, mean reward: 1.932 [1.473, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.625, 10.098], loss: 1.826614, mae: 0.517638, mean_q: 4.074649
 85174/100000: episode: 1382, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 203.392, mean reward: 2.034 [1.437, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.353, 10.117], loss: 2.543998, mae: 0.479923, mean_q: 4.087496
 85274/100000: episode: 1383, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 194.439, mean reward: 1.944 [1.489, 4.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.098, 10.098], loss: 0.358350, mae: 0.417944, mean_q: 4.019917
 85374/100000: episode: 1384, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 188.051, mean reward: 1.881 [1.437, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.176, 10.098], loss: 1.653039, mae: 0.419840, mean_q: 4.057540
 85474/100000: episode: 1385, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 182.305, mean reward: 1.823 [1.459, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.855, 10.098], loss: 0.588195, mae: 0.394968, mean_q: 3.959343
 85574/100000: episode: 1386, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 181.957, mean reward: 1.820 [1.469, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.095, 10.124], loss: 1.656822, mae: 0.421377, mean_q: 4.041567
 85674/100000: episode: 1387, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 188.609, mean reward: 1.886 [1.462, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.350, 10.161], loss: 1.623286, mae: 0.437211, mean_q: 4.072931
 85774/100000: episode: 1388, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 182.743, mean reward: 1.827 [1.452, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.716, 10.195], loss: 1.999017, mae: 0.487904, mean_q: 4.106592
 85874/100000: episode: 1389, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 190.126, mean reward: 1.901 [1.509, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.155, 10.098], loss: 0.931027, mae: 0.439265, mean_q: 3.989554
 85974/100000: episode: 1390, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 197.238, mean reward: 1.972 [1.488, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.627, 10.098], loss: 2.191134, mae: 0.498768, mean_q: 4.064700
 86074/100000: episode: 1391, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 209.671, mean reward: 2.097 [1.523, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.592, 10.326], loss: 1.435181, mae: 0.468374, mean_q: 4.067801
 86174/100000: episode: 1392, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 191.653, mean reward: 1.917 [1.496, 4.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.115, 10.177], loss: 1.373084, mae: 0.436000, mean_q: 4.061477
 86274/100000: episode: 1393, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 198.447, mean reward: 1.984 [1.471, 2.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.421, 10.098], loss: 0.164165, mae: 0.369763, mean_q: 3.999913
 86374/100000: episode: 1394, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 202.789, mean reward: 2.028 [1.466, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.069, 10.154], loss: 0.325124, mae: 0.392742, mean_q: 4.065045
 86474/100000: episode: 1395, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.311, mean reward: 1.963 [1.472, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.635, 10.166], loss: 0.380067, mae: 0.400053, mean_q: 4.000481
 86574/100000: episode: 1396, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 230.233, mean reward: 2.302 [1.523, 4.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.763, 10.393], loss: 3.786465, mae: 0.507425, mean_q: 4.091087
 86674/100000: episode: 1397, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 204.714, mean reward: 2.047 [1.465, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.629, 10.176], loss: 2.449904, mae: 0.474727, mean_q: 4.139090
 86774/100000: episode: 1398, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 223.341, mean reward: 2.233 [1.443, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.553, 10.144], loss: 1.729578, mae: 0.429832, mean_q: 4.074592
 86874/100000: episode: 1399, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 211.570, mean reward: 2.116 [1.529, 4.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.636, 10.098], loss: 0.397118, mae: 0.398617, mean_q: 4.049895
 86974/100000: episode: 1400, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 193.566, mean reward: 1.936 [1.459, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.654, 10.098], loss: 2.471746, mae: 0.451053, mean_q: 4.135391
 87074/100000: episode: 1401, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 190.749, mean reward: 1.907 [1.493, 4.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.848, 10.184], loss: 1.263108, mae: 0.450203, mean_q: 4.131798
 87174/100000: episode: 1402, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 188.840, mean reward: 1.888 [1.439, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.719, 10.098], loss: 3.960731, mae: 0.520993, mean_q: 4.196260
 87274/100000: episode: 1403, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 188.205, mean reward: 1.882 [1.451, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.185, 10.103], loss: 0.473366, mae: 0.399395, mean_q: 4.099097
 87374/100000: episode: 1404, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 205.719, mean reward: 2.057 [1.461, 5.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.864, 10.349], loss: 1.964332, mae: 0.463402, mean_q: 4.120113
 87474/100000: episode: 1405, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.639, mean reward: 1.936 [1.460, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.901, 10.214], loss: 1.214141, mae: 0.424115, mean_q: 4.077097
 87574/100000: episode: 1406, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.974, mean reward: 1.840 [1.480, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.361, 10.130], loss: 0.197979, mae: 0.369529, mean_q: 4.059619
 87674/100000: episode: 1407, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.257, mean reward: 1.893 [1.477, 4.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.613, 10.142], loss: 0.200804, mae: 0.367199, mean_q: 3.991079
 87774/100000: episode: 1408, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 202.493, mean reward: 2.025 [1.482, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.703, 10.098], loss: 0.957853, mae: 0.370071, mean_q: 3.983414
 87874/100000: episode: 1409, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.206, mean reward: 1.912 [1.464, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.083, 10.098], loss: 0.115716, mae: 0.330442, mean_q: 3.909603
 87974/100000: episode: 1410, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 181.701, mean reward: 1.817 [1.447, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.882, 10.098], loss: 0.123990, mae: 0.339622, mean_q: 3.923354
 88074/100000: episode: 1411, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 186.092, mean reward: 1.861 [1.434, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.280, 10.169], loss: 0.104286, mae: 0.312061, mean_q: 3.903872
 88174/100000: episode: 1412, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 198.226, mean reward: 1.982 [1.463, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.875, 10.353], loss: 0.116186, mae: 0.330093, mean_q: 3.921999
 88274/100000: episode: 1413, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.872, mean reward: 1.899 [1.451, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.356, 10.098], loss: 0.102228, mae: 0.314246, mean_q: 3.884954
 88374/100000: episode: 1414, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 194.293, mean reward: 1.943 [1.445, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.638, 10.208], loss: 0.103855, mae: 0.321615, mean_q: 3.885311
 88474/100000: episode: 1415, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 210.334, mean reward: 2.103 [1.458, 4.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.507, 10.298], loss: 0.110233, mae: 0.332986, mean_q: 3.916554
 88574/100000: episode: 1416, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 218.847, mean reward: 2.188 [1.520, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.427, 10.098], loss: 0.111174, mae: 0.330903, mean_q: 3.907017
 88674/100000: episode: 1417, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 224.820, mean reward: 2.248 [1.448, 6.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.090, 10.340], loss: 0.116397, mae: 0.332027, mean_q: 3.906800
 88774/100000: episode: 1418, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 207.448, mean reward: 2.074 [1.478, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.425, 10.306], loss: 0.118908, mae: 0.327470, mean_q: 3.915517
 88874/100000: episode: 1419, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 184.554, mean reward: 1.846 [1.455, 4.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.848, 10.195], loss: 0.111920, mae: 0.330747, mean_q: 3.920386
 88974/100000: episode: 1420, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 184.441, mean reward: 1.844 [1.433, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.495, 10.179], loss: 0.116434, mae: 0.331027, mean_q: 3.904939
 89074/100000: episode: 1421, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 196.072, mean reward: 1.961 [1.453, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.921, 10.098], loss: 0.102974, mae: 0.315401, mean_q: 3.898720
 89174/100000: episode: 1422, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 190.482, mean reward: 1.905 [1.443, 3.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.184, 10.098], loss: 0.110056, mae: 0.322646, mean_q: 3.902479
 89274/100000: episode: 1423, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.698, mean reward: 1.907 [1.438, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.361, 10.263], loss: 0.096005, mae: 0.305524, mean_q: 3.887844
 89374/100000: episode: 1424, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 195.614, mean reward: 1.956 [1.450, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.714, 10.276], loss: 0.095880, mae: 0.311161, mean_q: 3.881115
 89474/100000: episode: 1425, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 199.050, mean reward: 1.990 [1.486, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.280, 10.439], loss: 0.111449, mae: 0.322841, mean_q: 3.890997
 89574/100000: episode: 1426, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 207.292, mean reward: 2.073 [1.479, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.288, 10.188], loss: 0.098132, mae: 0.308493, mean_q: 3.869467
 89674/100000: episode: 1427, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.187, mean reward: 1.942 [1.489, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.427, 10.098], loss: 0.100591, mae: 0.312287, mean_q: 3.858820
 89774/100000: episode: 1428, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 214.750, mean reward: 2.148 [1.487, 5.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.925, 10.359], loss: 0.097139, mae: 0.306884, mean_q: 3.866348
 89874/100000: episode: 1429, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 208.729, mean reward: 2.087 [1.468, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.515, 10.098], loss: 0.105559, mae: 0.320212, mean_q: 3.885304
 89974/100000: episode: 1430, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 182.862, mean reward: 1.829 [1.443, 4.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.479, 10.297], loss: 0.108880, mae: 0.324255, mean_q: 3.901175
 90074/100000: episode: 1431, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 190.603, mean reward: 1.906 [1.460, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.773, 10.291], loss: 0.108166, mae: 0.322049, mean_q: 3.906159
 90174/100000: episode: 1432, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 201.399, mean reward: 2.014 [1.459, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.899, 10.435], loss: 0.099565, mae: 0.314101, mean_q: 3.885150
 90274/100000: episode: 1433, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 192.780, mean reward: 1.928 [1.500, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.744, 10.098], loss: 0.108182, mae: 0.323017, mean_q: 3.902126
 90374/100000: episode: 1434, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 179.708, mean reward: 1.797 [1.433, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.852, 10.098], loss: 0.113669, mae: 0.331039, mean_q: 3.928164
 90474/100000: episode: 1435, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 221.823, mean reward: 2.218 [1.438, 5.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.090, 10.098], loss: 0.106185, mae: 0.322154, mean_q: 3.907447
 90574/100000: episode: 1436, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 199.201, mean reward: 1.992 [1.443, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.440, 10.108], loss: 0.123191, mae: 0.340729, mean_q: 3.907580
 90674/100000: episode: 1437, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 185.620, mean reward: 1.856 [1.465, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.676, 10.098], loss: 0.101797, mae: 0.311962, mean_q: 3.890203
 90774/100000: episode: 1438, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 177.617, mean reward: 1.776 [1.461, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.363, 10.177], loss: 0.115355, mae: 0.329862, mean_q: 3.911603
 90874/100000: episode: 1439, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 196.234, mean reward: 1.962 [1.464, 3.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.700, 10.098], loss: 0.120518, mae: 0.332255, mean_q: 3.914236
 90974/100000: episode: 1440, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 202.051, mean reward: 2.021 [1.464, 4.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.404, 10.216], loss: 0.108578, mae: 0.325316, mean_q: 3.933172
 91074/100000: episode: 1441, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 185.309, mean reward: 1.853 [1.469, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.858, 10.098], loss: 0.109612, mae: 0.327939, mean_q: 3.924860
 91174/100000: episode: 1442, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 188.711, mean reward: 1.887 [1.442, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.802, 10.098], loss: 0.113192, mae: 0.327909, mean_q: 3.928633
 91274/100000: episode: 1443, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 198.887, mean reward: 1.989 [1.467, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.796, 10.116], loss: 0.112302, mae: 0.332625, mean_q: 3.930104
 91374/100000: episode: 1444, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 216.705, mean reward: 2.167 [1.503, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.656, 10.306], loss: 0.109687, mae: 0.320825, mean_q: 3.917797
 91474/100000: episode: 1445, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 216.774, mean reward: 2.168 [1.437, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.348, 10.314], loss: 0.113279, mae: 0.334442, mean_q: 3.932112
 91574/100000: episode: 1446, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 184.020, mean reward: 1.840 [1.454, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.673, 10.143], loss: 0.110459, mae: 0.323595, mean_q: 3.914235
 91674/100000: episode: 1447, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 198.205, mean reward: 1.982 [1.484, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.053, 10.418], loss: 0.106351, mae: 0.322021, mean_q: 3.899659
 91774/100000: episode: 1448, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 187.867, mean reward: 1.879 [1.449, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.171, 10.181], loss: 0.099864, mae: 0.310998, mean_q: 3.880821
 91874/100000: episode: 1449, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 230.530, mean reward: 2.305 [1.486, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.729, 10.098], loss: 0.100528, mae: 0.315409, mean_q: 3.897260
 91974/100000: episode: 1450, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 216.710, mean reward: 2.167 [1.440, 8.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.722, 10.098], loss: 0.113675, mae: 0.329749, mean_q: 3.904116
 92074/100000: episode: 1451, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 188.282, mean reward: 1.883 [1.457, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.813, 10.098], loss: 0.109218, mae: 0.322093, mean_q: 3.890399
 92174/100000: episode: 1452, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 185.897, mean reward: 1.859 [1.461, 2.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.270, 10.227], loss: 0.111708, mae: 0.332629, mean_q: 3.908108
 92274/100000: episode: 1453, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 191.983, mean reward: 1.920 [1.446, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.125, 10.163], loss: 0.112593, mae: 0.330338, mean_q: 3.922228
 92374/100000: episode: 1454, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 195.304, mean reward: 1.953 [1.465, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.252, 10.122], loss: 0.102589, mae: 0.316507, mean_q: 3.918807
 92474/100000: episode: 1455, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 182.179, mean reward: 1.822 [1.453, 4.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.957, 10.098], loss: 0.099594, mae: 0.314184, mean_q: 3.871264
 92574/100000: episode: 1456, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 233.608, mean reward: 2.336 [1.469, 7.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.471, 10.438], loss: 0.109492, mae: 0.322748, mean_q: 3.882407
 92674/100000: episode: 1457, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 184.530, mean reward: 1.845 [1.447, 2.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.990, 10.098], loss: 0.113263, mae: 0.320388, mean_q: 3.908738
[Info] 1-TH LEVEL FOUND: 5.505505084991455, Considering 10/90 traces
 92774/100000: episode: 1458, duration: 4.702s, episode steps: 100, steps per second: 21, episode reward: 187.869, mean reward: 1.879 [1.457, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.846, 10.252], loss: 0.105892, mae: 0.326375, mean_q: 3.894245
 92808/100000: episode: 1459, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 81.286, mean reward: 2.391 [1.933, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.585, 10.312], loss: 0.111740, mae: 0.322451, mean_q: 3.909001
 92830/100000: episode: 1460, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 49.971, mean reward: 2.271 [1.655, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.284], loss: 0.111735, mae: 0.326316, mean_q: 3.913294
 92886/100000: episode: 1461, duration: 0.294s, episode steps: 56, steps per second: 190, episode reward: 117.290, mean reward: 2.094 [1.754, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-1.220, 10.285], loss: 0.096223, mae: 0.303285, mean_q: 3.906484
 92899/100000: episode: 1462, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 38.805, mean reward: 2.985 [2.354, 4.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.233, 10.552], loss: 0.105042, mae: 0.321372, mean_q: 3.934166
 92921/100000: episode: 1463, duration: 0.132s, episode steps: 22, steps per second: 166, episode reward: 59.070, mean reward: 2.685 [1.925, 5.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.460], loss: 0.127138, mae: 0.314490, mean_q: 3.924548
 92950/100000: episode: 1464, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 76.388, mean reward: 2.634 [2.138, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.426, 10.529], loss: 0.108378, mae: 0.318519, mean_q: 3.908015
 92984/100000: episode: 1465, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 75.464, mean reward: 2.220 [1.694, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.931, 10.228], loss: 0.114834, mae: 0.321044, mean_q: 3.909497
 93006/100000: episode: 1466, duration: 0.129s, episode steps: 22, steps per second: 171, episode reward: 58.361, mean reward: 2.653 [2.088, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-1.476, 10.433], loss: 0.096770, mae: 0.314846, mean_q: 3.940437
 93028/100000: episode: 1467, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 43.752, mean reward: 1.989 [1.571, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.130, 10.225], loss: 0.096505, mae: 0.318255, mean_q: 3.973232
 93053/100000: episode: 1468, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 130.620, mean reward: 5.225 [2.705, 8.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.773, 10.550], loss: 0.125232, mae: 0.344823, mean_q: 3.922876
 93087/100000: episode: 1469, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 91.231, mean reward: 2.683 [1.730, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.244, 10.352], loss: 0.151518, mae: 0.349429, mean_q: 3.979298
 93109/100000: episode: 1470, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 42.849, mean reward: 1.948 [1.630, 2.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.364, 10.264], loss: 0.112992, mae: 0.336208, mean_q: 3.973674
 93122/100000: episode: 1471, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 36.063, mean reward: 2.774 [2.359, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.487], loss: 0.158511, mae: 0.347836, mean_q: 3.899395
 93178/100000: episode: 1472, duration: 0.293s, episode steps: 56, steps per second: 191, episode reward: 131.435, mean reward: 2.347 [1.471, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.839 [-0.758, 10.111], loss: 0.127420, mae: 0.345963, mean_q: 4.010761
 93191/100000: episode: 1473, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 38.995, mean reward: 3.000 [2.471, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.824, 10.463], loss: 0.150698, mae: 0.372319, mean_q: 4.010888
 93220/100000: episode: 1474, duration: 0.149s, episode steps: 29, steps per second: 194, episode reward: 88.177, mean reward: 3.041 [2.101, 4.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.755, 10.508], loss: 0.137843, mae: 0.335651, mean_q: 3.979086
 93233/100000: episode: 1475, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 31.560, mean reward: 2.428 [1.927, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.154, 10.350], loss: 0.125674, mae: 0.359212, mean_q: 3.993897
 93255/100000: episode: 1476, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 49.375, mean reward: 2.244 [1.651, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.383], loss: 0.130763, mae: 0.351337, mean_q: 3.993188
 93311/100000: episode: 1477, duration: 0.296s, episode steps: 56, steps per second: 189, episode reward: 121.979, mean reward: 2.178 [1.557, 4.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-1.284, 10.266], loss: 0.138777, mae: 0.355656, mean_q: 4.025492
 93324/100000: episode: 1478, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 29.493, mean reward: 2.269 [1.762, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.293], loss: 0.170466, mae: 0.355794, mean_q: 4.002862
 93346/100000: episode: 1479, duration: 0.134s, episode steps: 22, steps per second: 164, episode reward: 51.608, mean reward: 2.346 [1.982, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.634, 10.467], loss: 0.137999, mae: 0.345768, mean_q: 3.988224
 93380/100000: episode: 1480, duration: 0.186s, episode steps: 34, steps per second: 182, episode reward: 68.318, mean reward: 2.009 [1.561, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.316, 10.207], loss: 0.139820, mae: 0.343719, mean_q: 4.027019
 93409/100000: episode: 1481, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 69.308, mean reward: 2.390 [1.546, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.750, 10.224], loss: 0.113982, mae: 0.331581, mean_q: 3.995794
 93434/100000: episode: 1482, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 93.216, mean reward: 3.729 [2.345, 5.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.376, 10.567], loss: 0.121380, mae: 0.323859, mean_q: 3.995684
 93459/100000: episode: 1483, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 72.606, mean reward: 2.904 [2.055, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.256, 10.487], loss: 0.108614, mae: 0.335639, mean_q: 4.033424
 93515/100000: episode: 1484, duration: 0.277s, episode steps: 56, steps per second: 202, episode reward: 109.660, mean reward: 1.958 [1.440, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.581, 10.100], loss: 0.159832, mae: 0.360222, mean_q: 4.040630
 93547/100000: episode: 1485, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 60.217, mean reward: 1.882 [1.469, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.185, 10.300], loss: 0.164355, mae: 0.373510, mean_q: 4.060895
 93560/100000: episode: 1486, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 38.365, mean reward: 2.951 [2.346, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.077, 10.401], loss: 0.158862, mae: 0.371597, mean_q: 3.999462
 93561/100000: episode: 1487, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 2.178, mean reward: 2.178 [2.178, 2.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.333, 10.100], loss: 0.154084, mae: 0.396714, mean_q: 4.300690
 93593/100000: episode: 1488, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 56.989, mean reward: 1.781 [1.456, 2.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.035, 10.209], loss: 0.121376, mae: 0.350093, mean_q: 4.092046
 93606/100000: episode: 1489, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 33.810, mean reward: 2.601 [1.987, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.344], loss: 0.209196, mae: 0.379739, mean_q: 3.994351
 93635/100000: episode: 1490, duration: 0.143s, episode steps: 29, steps per second: 202, episode reward: 81.896, mean reward: 2.824 [2.003, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.549, 10.313], loss: 0.185727, mae: 0.378005, mean_q: 3.999516
 93667/100000: episode: 1491, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 56.088, mean reward: 1.753 [1.472, 2.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.613, 10.163], loss: 0.176182, mae: 0.383879, mean_q: 4.091190
 93699/100000: episode: 1492, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 64.779, mean reward: 2.024 [1.484, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.916, 10.129], loss: 0.148106, mae: 0.351874, mean_q: 4.055993
 93724/100000: episode: 1493, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 99.522, mean reward: 3.981 [2.527, 9.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.334, 10.455], loss: 0.118979, mae: 0.330966, mean_q: 4.033083
 93749/100000: episode: 1494, duration: 0.129s, episode steps: 25, steps per second: 193, episode reward: 58.320, mean reward: 2.333 [2.004, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.739, 10.342], loss: 0.137623, mae: 0.367768, mean_q: 4.030235
 93783/100000: episode: 1495, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 77.278, mean reward: 2.273 [1.536, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.035, 10.349], loss: 0.173530, mae: 0.378878, mean_q: 4.089985
 93815/100000: episode: 1496, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 63.628, mean reward: 1.988 [1.458, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.090, 10.245], loss: 0.149714, mae: 0.362419, mean_q: 4.114735
 93816/100000: episode: 1497, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 2.032, mean reward: 2.032 [2.032, 2.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.329, 10.100], loss: 0.157226, mae: 0.383893, mean_q: 4.140841
 93845/100000: episode: 1498, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 93.002, mean reward: 3.207 [2.261, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.581, 10.427], loss: 0.152608, mae: 0.363452, mean_q: 4.109343
 93870/100000: episode: 1499, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 122.238, mean reward: 4.890 [2.922, 10.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.292, 10.576], loss: 0.147301, mae: 0.383579, mean_q: 4.082153
 93871/100000: episode: 1500, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 2.101, mean reward: 2.101 [2.101, 2.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.282, 10.100], loss: 0.109983, mae: 0.376628, mean_q: 4.077240
 93893/100000: episode: 1501, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 43.101, mean reward: 1.959 [1.581, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.523, 10.204], loss: 0.179920, mae: 0.386636, mean_q: 4.094598
 93915/100000: episode: 1502, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 51.997, mean reward: 2.363 [1.893, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.530, 10.359], loss: 0.192357, mae: 0.422568, mean_q: 4.170234
 93937/100000: episode: 1503, duration: 0.123s, episode steps: 22, steps per second: 180, episode reward: 46.339, mean reward: 2.106 [1.671, 2.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.171, 10.239], loss: 0.169225, mae: 0.372497, mean_q: 4.080223
 93938/100000: episode: 1504, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 2.348, mean reward: 2.348 [2.348, 2.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.324, 10.100], loss: 0.075088, mae: 0.293473, mean_q: 3.742585
 93960/100000: episode: 1505, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 54.663, mean reward: 2.485 [1.863, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.393], loss: 0.255984, mae: 0.411835, mean_q: 4.123774
 93982/100000: episode: 1506, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 43.179, mean reward: 1.963 [1.501, 2.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.735, 10.209], loss: 0.200895, mae: 0.371740, mean_q: 4.083263
 93995/100000: episode: 1507, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 35.084, mean reward: 2.699 [1.971, 3.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.529, 10.274], loss: 0.128220, mae: 0.327964, mean_q: 4.023060
 94024/100000: episode: 1508, duration: 0.145s, episode steps: 29, steps per second: 200, episode reward: 85.231, mean reward: 2.939 [2.070, 5.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-1.050, 10.356], loss: 0.164611, mae: 0.387188, mean_q: 4.121525
 94049/100000: episode: 1509, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 77.075, mean reward: 3.083 [2.412, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-1.052, 10.470], loss: 0.148802, mae: 0.359111, mean_q: 4.116830
 94071/100000: episode: 1510, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 68.599, mean reward: 3.118 [2.306, 7.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.082, 10.466], loss: 0.139892, mae: 0.358231, mean_q: 4.082807
 94072/100000: episode: 1511, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 2.179, mean reward: 2.179 [2.179, 2.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.368, 10.100], loss: 0.094190, mae: 0.329889, mean_q: 4.134641
 94101/100000: episode: 1512, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 65.844, mean reward: 2.270 [1.720, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.076, 10.150], loss: 0.150243, mae: 0.372984, mean_q: 4.140150
 94133/100000: episode: 1513, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 69.877, mean reward: 2.184 [1.636, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.837, 10.333], loss: 0.237073, mae: 0.421406, mean_q: 4.165252
 94158/100000: episode: 1514, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 77.265, mean reward: 3.091 [2.335, 4.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.035, 10.487], loss: 0.151796, mae: 0.362222, mean_q: 4.095899
 94187/100000: episode: 1515, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 73.072, mean reward: 2.520 [1.856, 4.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.318, 10.333], loss: 0.178850, mae: 0.399630, mean_q: 4.221061
 94209/100000: episode: 1516, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 54.363, mean reward: 2.471 [1.889, 4.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.323], loss: 0.186591, mae: 0.381133, mean_q: 4.198866
 94231/100000: episode: 1517, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 47.733, mean reward: 2.170 [1.684, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.142, 10.200], loss: 0.216955, mae: 0.391268, mean_q: 4.161414
 94253/100000: episode: 1518, duration: 0.104s, episode steps: 22, steps per second: 211, episode reward: 58.516, mean reward: 2.660 [1.752, 3.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.419], loss: 0.160171, mae: 0.376819, mean_q: 4.178764
 94275/100000: episode: 1519, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 53.165, mean reward: 2.417 [1.975, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.291, 10.347], loss: 0.206731, mae: 0.414117, mean_q: 4.197826
 94276/100000: episode: 1520, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 2.088, mean reward: 2.088 [2.088, 2.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.330, 10.100], loss: 0.086089, mae: 0.314498, mean_q: 4.220807
 94308/100000: episode: 1521, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 70.865, mean reward: 2.215 [1.624, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.999, 10.252], loss: 0.176905, mae: 0.389295, mean_q: 4.192135
 94330/100000: episode: 1522, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 44.769, mean reward: 2.035 [1.696, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.635, 10.247], loss: 0.141735, mae: 0.349498, mean_q: 4.142625
 94352/100000: episode: 1523, duration: 0.130s, episode steps: 22, steps per second: 170, episode reward: 49.766, mean reward: 2.262 [1.564, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.355, 10.224], loss: 0.194525, mae: 0.404474, mean_q: 4.183783
 94353/100000: episode: 1524, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 2.122, mean reward: 2.122 [2.122, 2.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.349, 10.100], loss: 0.108182, mae: 0.339472, mean_q: 4.268034
 94375/100000: episode: 1525, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 50.997, mean reward: 2.318 [1.890, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.484, 10.315], loss: 0.248793, mae: 0.397481, mean_q: 4.181674
 94409/100000: episode: 1526, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 82.169, mean reward: 2.417 [1.722, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.108, 10.317], loss: 0.152644, mae: 0.389411, mean_q: 4.164157
 94422/100000: episode: 1527, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 51.833, mean reward: 3.987 [2.615, 6.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.335, 10.485], loss: 0.145740, mae: 0.376356, mean_q: 4.215105
 94435/100000: episode: 1528, duration: 0.063s, episode steps: 13, steps per second: 205, episode reward: 38.600, mean reward: 2.969 [2.372, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.898, 10.402], loss: 0.142762, mae: 0.355400, mean_q: 4.160941
 94491/100000: episode: 1529, duration: 0.314s, episode steps: 56, steps per second: 178, episode reward: 126.759, mean reward: 2.264 [1.496, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.846 [-0.520, 10.350], loss: 0.188467, mae: 0.392273, mean_q: 4.195991
 94513/100000: episode: 1530, duration: 0.111s, episode steps: 22, steps per second: 197, episode reward: 51.191, mean reward: 2.327 [1.988, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.528, 10.302], loss: 0.175371, mae: 0.400931, mean_q: 4.136764
 94526/100000: episode: 1531, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 30.041, mean reward: 2.311 [2.034, 2.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.413], loss: 0.176256, mae: 0.417395, mean_q: 4.204272
 94548/100000: episode: 1532, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 45.890, mean reward: 2.086 [1.621, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.181], loss: 0.239265, mae: 0.435141, mean_q: 4.240289
 94604/100000: episode: 1533, duration: 0.304s, episode steps: 56, steps per second: 184, episode reward: 154.440, mean reward: 2.758 [1.903, 6.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.852 [-0.612, 10.296], loss: 0.202595, mae: 0.414896, mean_q: 4.237082
 94605/100000: episode: 1534, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 2.131, mean reward: 2.131 [2.131, 2.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.322, 10.100], loss: 0.146503, mae: 0.406315, mean_q: 4.200990
 94630/100000: episode: 1535, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 119.076, mean reward: 4.763 [2.003, 9.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.720, 10.335], loss: 0.260470, mae: 0.424424, mean_q: 4.216525
 94686/100000: episode: 1536, duration: 0.304s, episode steps: 56, steps per second: 184, episode reward: 123.313, mean reward: 2.202 [1.500, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.323, 10.185], loss: 0.163902, mae: 0.382088, mean_q: 4.207050
 94711/100000: episode: 1537, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 101.046, mean reward: 4.042 [2.546, 8.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.102, 10.645], loss: 0.239852, mae: 0.422807, mean_q: 4.270922
 94724/100000: episode: 1538, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 34.167, mean reward: 2.628 [2.363, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.135, 10.367], loss: 0.254762, mae: 0.450661, mean_q: 4.303485
 94725/100000: episode: 1539, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 2.153, mean reward: 2.153 [2.153, 2.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.365, 10.100], loss: 0.195655, mae: 0.489977, mean_q: 4.107099
 94726/100000: episode: 1540, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 2.287, mean reward: 2.287 [2.287, 2.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.364, 10.100], loss: 0.240855, mae: 0.558368, mean_q: 4.633824
 94748/100000: episode: 1541, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 46.452, mean reward: 2.111 [1.760, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.283, 10.260], loss: 0.260640, mae: 0.458455, mean_q: 4.251141
 94777/100000: episode: 1542, duration: 0.143s, episode steps: 29, steps per second: 203, episode reward: 67.925, mean reward: 2.342 [1.624, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.496, 10.240], loss: 0.218484, mae: 0.416297, mean_q: 4.303182
 94806/100000: episode: 1543, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 72.626, mean reward: 2.504 [1.943, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.411, 10.417], loss: 0.239064, mae: 0.426525, mean_q: 4.292809
 94828/100000: episode: 1544, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 58.257, mean reward: 2.648 [1.871, 5.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.278, 10.283], loss: 0.220334, mae: 0.407659, mean_q: 4.308280
 94884/100000: episode: 1545, duration: 0.317s, episode steps: 56, steps per second: 177, episode reward: 114.898, mean reward: 2.052 [1.631, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.602, 10.264], loss: 0.238228, mae: 0.418440, mean_q: 4.284901
 94940/100000: episode: 1546, duration: 0.280s, episode steps: 56, steps per second: 200, episode reward: 128.403, mean reward: 2.293 [1.457, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.842 [-1.055, 10.160], loss: 0.238553, mae: 0.421618, mean_q: 4.275689
 94974/100000: episode: 1547, duration: 0.172s, episode steps: 34, steps per second: 198, episode reward: 76.508, mean reward: 2.250 [1.602, 3.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.378, 10.233], loss: 0.243133, mae: 0.425312, mean_q: 4.323926
[Info] 2-TH LEVEL FOUND: 7.236527442932129, Considering 10/90 traces
 94975/100000: episode: 1548, duration: 4.282s, episode steps: 1, steps per second: 0, episode reward: 1.943, mean reward: 1.943 [1.943, 1.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.342, 10.100], loss: 0.141104, mae: 0.391120, mean_q: 4.079511
 94994/100000: episode: 1549, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 67.440, mean reward: 3.549 [3.054, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.400, 10.495], loss: 0.207376, mae: 0.412849, mean_q: 4.335644
 95009/100000: episode: 1550, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 46.515, mean reward: 3.101 [2.322, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.494], loss: 0.214345, mae: 0.446040, mean_q: 4.335635
 95024/100000: episode: 1551, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 59.502, mean reward: 3.967 [3.041, 5.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.545], loss: 0.172329, mae: 0.377683, mean_q: 4.390439
 95041/100000: episode: 1552, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 50.944, mean reward: 2.997 [2.268, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.134, 10.398], loss: 0.186751, mae: 0.416144, mean_q: 4.364037
 95056/100000: episode: 1553, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 48.775, mean reward: 3.252 [2.782, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.510, 10.490], loss: 0.247823, mae: 0.451966, mean_q: 4.351043
 95073/100000: episode: 1554, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 52.917, mean reward: 3.113 [1.877, 6.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.547, 10.383], loss: 0.232112, mae: 0.427215, mean_q: 4.430017
 95090/100000: episode: 1555, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 53.899, mean reward: 3.171 [2.412, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.310], loss: 0.179312, mae: 0.422588, mean_q: 4.338276
 95107/100000: episode: 1556, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 64.040, mean reward: 3.767 [2.340, 6.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.453, 10.403], loss: 0.194612, mae: 0.432927, mean_q: 4.374790
 95124/100000: episode: 1557, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 51.279, mean reward: 3.016 [2.196, 5.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.133, 10.614], loss: 0.267968, mae: 0.457735, mean_q: 4.390085
 95135/100000: episode: 1558, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 29.345, mean reward: 2.668 [1.725, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.253], loss: 0.251673, mae: 0.448059, mean_q: 4.393648
 95142/100000: episode: 1559, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 88.361, mean reward: 12.623 [7.257, 38.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.050, 10.684], loss: 0.266076, mae: 0.468050, mean_q: 4.429025
 95157/100000: episode: 1560, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 53.483, mean reward: 3.566 [2.468, 7.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-2.132, 10.271], loss: 0.360927, mae: 0.475259, mean_q: 4.386336
 95172/100000: episode: 1561, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 63.377, mean reward: 4.225 [3.295, 6.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.526, 10.548], loss: 0.339781, mae: 0.484298, mean_q: 4.524595
 95187/100000: episode: 1562, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 76.906, mean reward: 5.127 [3.486, 8.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.568], loss: 0.260635, mae: 0.416999, mean_q: 4.381125
 95192/100000: episode: 1563, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 17.888, mean reward: 3.578 [3.379, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.487], loss: 0.225948, mae: 0.452374, mean_q: 4.254851
 95214/100000: episode: 1564, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 68.581, mean reward: 3.117 [2.279, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.051, 10.454], loss: 1.160041, mae: 0.559152, mean_q: 4.498842
 95233/100000: episode: 1565, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 65.504, mean reward: 3.448 [2.468, 4.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.230, 10.401], loss: 0.317354, mae: 0.502979, mean_q: 4.601146
 95250/100000: episode: 1566, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 46.859, mean reward: 2.756 [2.208, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.075, 10.417], loss: 0.331478, mae: 0.482110, mean_q: 4.395270
 95265/100000: episode: 1567, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 42.676, mean reward: 2.845 [1.786, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.161, 10.380], loss: 0.261638, mae: 0.482039, mean_q: 4.441892
 95287/100000: episode: 1568, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 110.918, mean reward: 5.042 [2.966, 8.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.124, 10.602], loss: 0.238299, mae: 0.443703, mean_q: 4.343254
 95309/100000: episode: 1569, duration: 0.116s, episode steps: 22, steps per second: 190, episode reward: 63.197, mean reward: 2.873 [2.270, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.457], loss: 0.249252, mae: 0.447643, mean_q: 4.443647
 95320/100000: episode: 1570, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 40.463, mean reward: 3.678 [2.993, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.521], loss: 0.375198, mae: 0.535986, mean_q: 4.608885
 95327/100000: episode: 1571, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 39.610, mean reward: 5.659 [3.994, 6.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.569], loss: 3.133932, mae: 0.758642, mean_q: 4.695776
 95332/100000: episode: 1572, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 24.731, mean reward: 4.946 [4.407, 6.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.607], loss: 0.313950, mae: 0.477432, mean_q: 4.318818
 95344/100000: episode: 1573, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 48.502, mean reward: 4.042 [2.768, 9.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.714, 10.483], loss: 0.340488, mae: 0.537401, mean_q: 4.625964
 95359/100000: episode: 1574, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 101.193, mean reward: 6.746 [3.684, 11.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.397, 10.640], loss: 0.274542, mae: 0.490070, mean_q: 4.436740
 95364/100000: episode: 1575, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 25.330, mean reward: 5.066 [4.414, 6.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.622], loss: 0.171128, mae: 0.403564, mean_q: 4.446515
 95383/100000: episode: 1576, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 76.936, mean reward: 4.049 [2.397, 6.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-1.130, 10.375], loss: 0.288386, mae: 0.468434, mean_q: 4.509830
 95402/100000: episode: 1577, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 68.264, mean reward: 3.593 [2.334, 6.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.469, 10.451], loss: 0.262375, mae: 0.437293, mean_q: 4.528924
 95409/100000: episode: 1578, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 37.625, mean reward: 5.375 [4.599, 6.475], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.550], loss: 0.234271, mae: 0.418660, mean_q: 4.455884
 95424/100000: episode: 1579, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 73.799, mean reward: 4.920 [3.396, 7.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.629], loss: 0.359140, mae: 0.523318, mean_q: 4.594873
 95436/100000: episode: 1580, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 35.694, mean reward: 2.974 [2.137, 4.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.857, 10.436], loss: 0.372030, mae: 0.510468, mean_q: 4.473909
 95451/100000: episode: 1581, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 56.990, mean reward: 3.799 [3.083, 4.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.077, 10.534], loss: 0.268802, mae: 0.466791, mean_q: 4.573071
 95468/100000: episode: 1582, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 80.939, mean reward: 4.761 [3.296, 7.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.534], loss: 0.253071, mae: 0.458267, mean_q: 4.552205
 95483/100000: episode: 1583, duration: 0.098s, episode steps: 15, steps per second: 154, episode reward: 64.686, mean reward: 4.312 [3.333, 5.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.532], loss: 0.297691, mae: 0.474500, mean_q: 4.627139
 95505/100000: episode: 1584, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 81.954, mean reward: 3.725 [2.865, 5.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.973, 10.430], loss: 0.301854, mae: 0.502304, mean_q: 4.617128
 95512/100000: episode: 1585, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 65.278, mean reward: 9.325 [5.316, 13.765], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.666], loss: 0.355453, mae: 0.524237, mean_q: 4.544695
 95519/100000: episode: 1586, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 118.107, mean reward: 16.872 [9.929, 33.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.696], loss: 0.324127, mae: 0.548956, mean_q: 4.858507
 95534/100000: episode: 1587, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 61.959, mean reward: 4.131 [3.287, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.569], loss: 1.878812, mae: 0.638123, mean_q: 4.846392
 95545/100000: episode: 1588, duration: 0.073s, episode steps: 11, steps per second: 150, episode reward: 39.730, mean reward: 3.612 [3.182, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.459], loss: 0.449939, mae: 0.615533, mean_q: 4.691960
 95560/100000: episode: 1589, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 67.622, mean reward: 4.508 [3.220, 8.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-1.523, 10.512], loss: 0.504373, mae: 0.547957, mean_q: 4.814212
 95565/100000: episode: 1590, duration: 0.040s, episode steps: 5, steps per second: 126, episode reward: 22.363, mean reward: 4.473 [3.940, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.574], loss: 0.271624, mae: 0.491945, mean_q: 4.927855
 95580/100000: episode: 1591, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 57.040, mean reward: 3.803 [2.925, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.361, 10.474], loss: 0.345095, mae: 0.509696, mean_q: 4.748129
 95597/100000: episode: 1592, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 58.721, mean reward: 3.454 [2.495, 4.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.503, 10.364], loss: 0.374624, mae: 0.516004, mean_q: 4.792340
 95609/100000: episode: 1593, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 44.527, mean reward: 3.711 [2.847, 4.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.809, 10.425], loss: 0.348982, mae: 0.549652, mean_q: 4.872553
 95624/100000: episode: 1594, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 62.892, mean reward: 4.193 [3.015, 5.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.256, 10.555], loss: 1.441557, mae: 0.755745, mean_q: 5.006882
 95629/100000: episode: 1595, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 21.182, mean reward: 4.236 [3.908, 4.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.534, 10.553], loss: 0.383801, mae: 0.582970, mean_q: 5.086523
 95648/100000: episode: 1596, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 86.887, mean reward: 4.573 [3.036, 10.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.139, 10.491], loss: 0.288111, mae: 0.466200, mean_q: 4.681545
 95665/100000: episode: 1597, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 49.381, mean reward: 2.905 [2.189, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.198, 10.324], loss: 0.259576, mae: 0.484432, mean_q: 4.695744
 95670/100000: episode: 1598, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 46.054, mean reward: 9.211 [5.401, 16.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.066, 10.722], loss: 0.471943, mae: 0.638888, mean_q: 5.010354
 95681/100000: episode: 1599, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 27.707, mean reward: 2.519 [1.745, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.440], loss: 0.489673, mae: 0.543941, mean_q: 4.636514
 95700/100000: episode: 1600, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 78.415, mean reward: 4.127 [2.767, 6.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.404, 10.446], loss: 0.391754, mae: 0.530285, mean_q: 4.866707
 95717/100000: episode: 1601, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 63.802, mean reward: 3.753 [2.737, 5.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.449, 10.465], loss: 0.453553, mae: 0.544464, mean_q: 4.811967
 95722/100000: episode: 1602, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 23.590, mean reward: 4.718 [4.031, 6.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-1.265, 10.602], loss: 0.608033, mae: 0.592016, mean_q: 4.795403
 95733/100000: episode: 1603, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 39.715, mean reward: 3.610 [3.006, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.551], loss: 2.119775, mae: 0.710410, mean_q: 5.029664
 95744/100000: episode: 1604, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 36.335, mean reward: 3.303 [2.308, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.435], loss: 0.314808, mae: 0.520964, mean_q: 4.647799
 95761/100000: episode: 1605, duration: 0.083s, episode steps: 17, steps per second: 204, episode reward: 73.298, mean reward: 4.312 [2.389, 9.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.438], loss: 0.759140, mae: 0.635408, mean_q: 4.991731
 95772/100000: episode: 1606, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 28.683, mean reward: 2.608 [2.340, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.619, 10.443], loss: 0.359410, mae: 0.516927, mean_q: 4.782540
 95783/100000: episode: 1607, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 35.913, mean reward: 3.265 [2.854, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.053, 10.403], loss: 0.346727, mae: 0.533981, mean_q: 4.933547
 95805/100000: episode: 1608, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 72.785, mean reward: 3.308 [2.461, 4.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.523, 10.484], loss: 0.918396, mae: 0.560531, mean_q: 4.929996
 95810/100000: episode: 1609, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 21.888, mean reward: 4.378 [3.492, 5.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.575], loss: 0.583363, mae: 0.602420, mean_q: 4.654316
 95825/100000: episode: 1610, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 85.541, mean reward: 5.703 [3.762, 12.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.569], loss: 0.396042, mae: 0.478475, mean_q: 4.823525
 95836/100000: episode: 1611, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 37.865, mean reward: 3.442 [3.107, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.528], loss: 0.339305, mae: 0.514685, mean_q: 4.979588
 95851/100000: episode: 1612, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 69.447, mean reward: 4.630 [2.804, 7.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.185, 10.571], loss: 0.522830, mae: 0.618839, mean_q: 4.971087
 95873/100000: episode: 1613, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 76.517, mean reward: 3.478 [2.116, 6.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-1.056, 10.345], loss: 1.175173, mae: 0.641702, mean_q: 5.096953
 95890/100000: episode: 1614, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 50.482, mean reward: 2.970 [2.174, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.296, 10.345], loss: 0.398003, mae: 0.549219, mean_q: 4.899081
 95902/100000: episode: 1615, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 34.809, mean reward: 2.901 [2.178, 4.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.417], loss: 1.904255, mae: 0.782603, mean_q: 5.252362
 95924/100000: episode: 1616, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 61.035, mean reward: 2.774 [1.953, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.340], loss: 0.282217, mae: 0.518583, mean_q: 4.759268
 95946/100000: episode: 1617, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 60.319, mean reward: 2.742 [2.075, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.369], loss: 0.382357, mae: 0.530650, mean_q: 4.930912
 95958/100000: episode: 1618, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 36.282, mean reward: 3.024 [2.223, 4.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.295, 10.378], loss: 0.420066, mae: 0.546071, mean_q: 5.088749
 95970/100000: episode: 1619, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 51.399, mean reward: 4.283 [3.200, 6.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.590], loss: 0.673266, mae: 0.564959, mean_q: 4.965493
 95987/100000: episode: 1620, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 57.207, mean reward: 3.365 [1.853, 12.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.746, 10.400], loss: 0.532960, mae: 0.610040, mean_q: 5.009710
 96004/100000: episode: 1621, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 90.907, mean reward: 5.347 [3.627, 7.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.508, 10.630], loss: 0.600607, mae: 0.580436, mean_q: 5.070576
 96019/100000: episode: 1622, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 43.153, mean reward: 2.877 [2.210, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-1.470, 10.398], loss: 0.437618, mae: 0.573751, mean_q: 5.115065
 96036/100000: episode: 1623, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 74.633, mean reward: 4.390 [2.829, 5.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.633, 10.609], loss: 0.415886, mae: 0.546505, mean_q: 4.963586
 96055/100000: episode: 1624, duration: 0.095s, episode steps: 19, steps per second: 201, episode reward: 74.138, mean reward: 3.902 [2.658, 8.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.807, 10.354], loss: 0.532863, mae: 0.575871, mean_q: 5.163407
 96066/100000: episode: 1625, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 33.592, mean reward: 3.054 [2.398, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.713, 10.367], loss: 0.539287, mae: 0.585914, mean_q: 5.023507
 96083/100000: episode: 1626, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 58.563, mean reward: 3.445 [2.070, 7.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.490], loss: 0.335595, mae: 0.521909, mean_q: 5.082983
 96088/100000: episode: 1627, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 24.287, mean reward: 4.857 [4.381, 5.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.604], loss: 0.668838, mae: 0.614231, mean_q: 5.119360
 96105/100000: episode: 1628, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 88.946, mean reward: 5.232 [2.929, 11.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.615], loss: 0.437078, mae: 0.582403, mean_q: 5.109012
 96120/100000: episode: 1629, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 70.999, mean reward: 4.733 [3.013, 8.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.465], loss: 0.439518, mae: 0.591333, mean_q: 5.108454
 96139/100000: episode: 1630, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 161.818, mean reward: 8.517 [4.042, 15.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.715, 10.551], loss: 0.745868, mae: 0.678638, mean_q: 5.261102
 96151/100000: episode: 1631, duration: 0.064s, episode steps: 12, steps per second: 186, episode reward: 44.205, mean reward: 3.684 [3.174, 4.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.253, 10.447], loss: 0.594309, mae: 0.568722, mean_q: 4.976467
 96158/100000: episode: 1632, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 54.249, mean reward: 7.750 [6.085, 11.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.391, 10.553], loss: 0.815850, mae: 0.729044, mean_q: 5.313049
 96180/100000: episode: 1633, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 68.692, mean reward: 3.122 [2.263, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.276, 10.395], loss: 0.678329, mae: 0.634719, mean_q: 5.256999
 96197/100000: episode: 1634, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 42.767, mean reward: 2.516 [2.044, 3.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.056, 10.344], loss: 0.469414, mae: 0.556322, mean_q: 5.138659
 96219/100000: episode: 1635, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 104.320, mean reward: 4.742 [3.022, 8.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.422, 10.580], loss: 0.558559, mae: 0.618048, mean_q: 5.237759
 96231/100000: episode: 1636, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 42.826, mean reward: 3.569 [2.571, 5.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.392], loss: 0.424335, mae: 0.580441, mean_q: 5.125309
 96242/100000: episode: 1637, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 41.158, mean reward: 3.742 [2.838, 4.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.242, 10.504], loss: 0.342725, mae: 0.518719, mean_q: 5.015635
[Info] 3-TH LEVEL FOUND: 9.907567024230957, Considering 10/90 traces
 96259/100000: episode: 1638, duration: 4.409s, episode steps: 17, steps per second: 4, episode reward: 92.245, mean reward: 5.426 [3.809, 8.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.747, 10.529], loss: 0.541658, mae: 0.609243, mean_q: 5.203649
 96276/100000: episode: 1639, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 94.612, mean reward: 5.565 [3.113, 22.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.978, 10.606], loss: 0.367594, mae: 0.566628, mean_q: 5.226860
 96282/100000: episode: 1640, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 49.299, mean reward: 8.216 [5.841, 12.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.693], loss: 0.500661, mae: 0.588795, mean_q: 4.888918
 96289/100000: episode: 1641, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 52.069, mean reward: 7.438 [5.531, 10.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.576], loss: 0.680045, mae: 0.699139, mean_q: 5.659408
 96297/100000: episode: 1642, duration: 0.051s, episode steps: 8, steps per second: 155, episode reward: 49.681, mean reward: 6.210 [4.847, 7.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.814, 10.632], loss: 0.411223, mae: 0.551241, mean_q: 4.865767
 96304/100000: episode: 1643, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 37.240, mean reward: 5.320 [3.961, 8.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.493], loss: 0.350772, mae: 0.609332, mean_q: 5.330410
 96314/100000: episode: 1644, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 55.495, mean reward: 5.549 [4.019, 9.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.548, 10.689], loss: 0.412571, mae: 0.568271, mean_q: 5.096425
 96322/100000: episode: 1645, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 43.188, mean reward: 5.399 [4.142, 6.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.797, 10.590], loss: 1.023127, mae: 0.665085, mean_q: 5.313566
 96334/100000: episode: 1646, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 54.541, mean reward: 4.545 [2.148, 8.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.445], loss: 0.455776, mae: 0.597741, mean_q: 5.301575
 96344/100000: episode: 1647, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 48.832, mean reward: 4.883 [3.482, 6.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.178, 10.543], loss: 0.337887, mae: 0.529771, mean_q: 5.322375
 96357/100000: episode: 1648, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 44.886, mean reward: 3.453 [2.736, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.186, 10.484], loss: 0.425493, mae: 0.578540, mean_q: 5.157371
 96365/100000: episode: 1649, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 81.696, mean reward: 10.212 [6.096, 28.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.253, 10.605], loss: 0.861730, mae: 0.766845, mean_q: 5.434092
 96375/100000: episode: 1650, duration: 0.060s, episode steps: 10, steps per second: 165, episode reward: 49.226, mean reward: 4.923 [3.768, 6.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.558], loss: 0.417101, mae: 0.555501, mean_q: 5.209504
 96387/100000: episode: 1651, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 70.582, mean reward: 5.882 [3.797, 10.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.680], loss: 0.632975, mae: 0.695560, mean_q: 5.574780
 96399/100000: episode: 1652, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 96.982, mean reward: 8.082 [4.430, 16.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.989, 10.723], loss: 0.386854, mae: 0.581880, mean_q: 5.381658
 96405/100000: episode: 1653, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 52.493, mean reward: 8.749 [4.690, 23.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.355, 10.756], loss: 0.381889, mae: 0.578787, mean_q: 5.131765
 96413/100000: episode: 1654, duration: 0.065s, episode steps: 8, steps per second: 123, episode reward: 29.240, mean reward: 3.655 [3.047, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.454], loss: 0.544974, mae: 0.691557, mean_q: 5.774491
 96426/100000: episode: 1655, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 53.652, mean reward: 4.127 [2.780, 6.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.080, 10.447], loss: 2.178413, mae: 0.860110, mean_q: 5.733584
 96443/100000: episode: 1656, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 95.749, mean reward: 5.632 [4.113, 9.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.481], loss: 0.556513, mae: 0.641879, mean_q: 5.437951
 96456/100000: episode: 1657, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 60.648, mean reward: 4.665 [3.198, 9.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.965, 10.573], loss: 0.763130, mae: 0.733985, mean_q: 5.485644
 96462/100000: episode: 1658, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 31.128, mean reward: 5.188 [3.680, 6.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.592], loss: 0.406862, mae: 0.596389, mean_q: 5.108610
 96469/100000: episode: 1659, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 108.267, mean reward: 15.467 [9.408, 28.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.325 [-0.035, 10.698], loss: 1.116338, mae: 0.771306, mean_q: 5.627802
 96482/100000: episode: 1660, duration: 0.070s, episode steps: 13, steps per second: 184, episode reward: 56.556, mean reward: 4.350 [3.416, 7.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.544], loss: 1.021684, mae: 0.744497, mean_q: 5.554052
 96492/100000: episode: 1661, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 79.843, mean reward: 7.984 [4.382, 18.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.846, 10.623], loss: 0.660778, mae: 0.692857, mean_q: 5.471667
 96504/100000: episode: 1662, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 37.492, mean reward: 3.124 [2.635, 4.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.446], loss: 3.951167, mae: 1.007357, mean_q: 5.867302
 96521/100000: episode: 1663, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 128.987, mean reward: 7.587 [3.941, 14.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.523, 10.545], loss: 0.804867, mae: 0.750706, mean_q: 5.586340
 96533/100000: episode: 1664, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 53.911, mean reward: 4.493 [3.575, 6.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.517], loss: 2.419921, mae: 0.891368, mean_q: 5.688734
 96543/100000: episode: 1665, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 214.137, mean reward: 21.414 [4.866, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.759], loss: 1.768731, mae: 0.966758, mean_q: 5.963635
 96556/100000: episode: 1666, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 50.967, mean reward: 3.921 [2.716, 5.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.432], loss: 1.043043, mae: 0.851117, mean_q: 6.080569
 96562/100000: episode: 1667, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 30.239, mean reward: 5.040 [3.740, 6.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.637], loss: 2.860458, mae: 1.052979, mean_q: 6.027958
 96568/100000: episode: 1668, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 36.843, mean reward: 6.141 [4.389, 8.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.563], loss: 0.820415, mae: 0.742911, mean_q: 5.854204
 96585/100000: episode: 1669, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 61.038, mean reward: 3.590 [2.540, 5.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.071, 10.456], loss: 0.852303, mae: 0.727951, mean_q: 5.795650
 96595/100000: episode: 1670, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 47.534, mean reward: 4.753 [3.872, 6.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.562], loss: 15.216670, mae: 1.121453, mean_q: 5.806200
 96603/100000: episode: 1671, duration: 0.044s, episode steps: 8, steps per second: 182, episode reward: 50.454, mean reward: 6.307 [4.898, 8.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.045, 10.503], loss: 1.642684, mae: 1.052260, mean_q: 5.833068
 96615/100000: episode: 1672, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 53.560, mean reward: 4.463 [3.441, 6.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.602], loss: 0.813848, mae: 0.760379, mean_q: 5.715785
 96627/100000: episode: 1673, duration: 0.070s, episode steps: 12, steps per second: 173, episode reward: 37.587, mean reward: 3.132 [2.305, 7.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.359], loss: 1.246381, mae: 0.700971, mean_q: 5.611454
 96639/100000: episode: 1674, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 57.447, mean reward: 4.787 [3.295, 7.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.607, 10.531], loss: 1.155052, mae: 0.891531, mean_q: 6.043799
 96647/100000: episode: 1675, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 45.750, mean reward: 5.719 [4.235, 7.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.616], loss: 1.083564, mae: 0.712881, mean_q: 5.704964
 96657/100000: episode: 1676, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 57.302, mean reward: 5.730 [3.890, 12.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.500], loss: 1.596004, mae: 0.809040, mean_q: 5.849502
 96674/100000: episode: 1677, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 61.257, mean reward: 3.603 [2.725, 4.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.510], loss: 0.694228, mae: 0.738561, mean_q: 5.902938
 96684/100000: episode: 1678, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 89.187, mean reward: 8.919 [5.469, 14.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.656], loss: 1.943837, mae: 0.962033, mean_q: 6.079895
 96694/100000: episode: 1679, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 85.045, mean reward: 8.504 [4.830, 17.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.422, 10.675], loss: 1.402113, mae: 0.798760, mean_q: 5.648160
 96711/100000: episode: 1680, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 129.710, mean reward: 7.630 [5.018, 15.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.118, 10.651], loss: 1.864752, mae: 0.852439, mean_q: 5.967710
 96721/100000: episode: 1681, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 68.064, mean reward: 6.806 [4.092, 14.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.699], loss: 0.809287, mae: 0.789602, mean_q: 5.548376
 96727/100000: episode: 1682, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 28.019, mean reward: 4.670 [3.699, 6.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.558], loss: 0.804997, mae: 0.879262, mean_q: 5.882820
 96739/100000: episode: 1683, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 47.221, mean reward: 3.935 [2.753, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.496], loss: 1.784899, mae: 0.883191, mean_q: 5.968812
 96745/100000: episode: 1684, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 43.981, mean reward: 7.330 [5.953, 9.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.654], loss: 0.695347, mae: 0.660883, mean_q: 5.656911
 96762/100000: episode: 1685, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 148.902, mean reward: 8.759 [3.594, 32.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.364, 10.662], loss: 0.521775, mae: 0.691105, mean_q: 5.853947
 96772/100000: episode: 1686, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 59.386, mean reward: 5.939 [4.196, 9.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.549], loss: 1.595454, mae: 0.964427, mean_q: 6.041068
 96778/100000: episode: 1687, duration: 0.043s, episode steps: 6, steps per second: 140, episode reward: 57.950, mean reward: 9.658 [7.275, 13.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.617], loss: 1.552395, mae: 0.822853, mean_q: 5.740526
 96788/100000: episode: 1688, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 45.327, mean reward: 4.533 [3.070, 7.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.265, 10.469], loss: 1.552532, mae: 0.910917, mean_q: 6.285228
 96800/100000: episode: 1689, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 39.272, mean reward: 3.273 [1.945, 7.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.421], loss: 1.941622, mae: 0.903033, mean_q: 5.967087
 96808/100000: episode: 1690, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 60.507, mean reward: 7.563 [5.814, 14.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.521], loss: 0.764371, mae: 0.730329, mean_q: 5.753052
 96825/100000: episode: 1691, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 94.095, mean reward: 5.535 [3.479, 9.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.212, 10.650], loss: 0.950170, mae: 0.792033, mean_q: 5.996327
 96831/100000: episode: 1692, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 49.955, mean reward: 8.326 [5.452, 11.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.339 [-0.035, 10.711], loss: 2.335718, mae: 1.035428, mean_q: 6.280878
 96848/100000: episode: 1693, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 106.002, mean reward: 6.235 [4.409, 10.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.612], loss: 1.078294, mae: 0.864809, mean_q: 5.898205
 96858/100000: episode: 1694, duration: 0.062s, episode steps: 10, steps per second: 163, episode reward: 116.261, mean reward: 11.626 [4.757, 23.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.748], loss: 2.604624, mae: 0.939168, mean_q: 6.167243
 96871/100000: episode: 1695, duration: 0.070s, episode steps: 13, steps per second: 187, episode reward: 71.438, mean reward: 5.495 [2.827, 10.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.519], loss: 1.544860, mae: 0.915269, mean_q: 6.107254
 96878/100000: episode: 1696, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 50.491, mean reward: 7.213 [5.190, 11.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.705], loss: 0.616111, mae: 0.747301, mean_q: 6.129716
 96888/100000: episode: 1697, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 44.275, mean reward: 4.427 [3.185, 6.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.434], loss: 2.251178, mae: 1.002431, mean_q: 6.042695
 96894/100000: episode: 1698, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 75.710, mean reward: 12.618 [9.134, 17.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.532], loss: 0.878924, mae: 0.839236, mean_q: 5.813871
 96904/100000: episode: 1699, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 76.037, mean reward: 7.604 [4.249, 16.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.528], loss: 0.961410, mae: 0.843544, mean_q: 6.224652
 96916/100000: episode: 1700, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 65.828, mean reward: 5.486 [3.934, 9.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.047, 10.595], loss: 0.772211, mae: 0.810634, mean_q: 6.161308
 96923/100000: episode: 1701, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 75.512, mean reward: 10.787 [7.882, 13.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.647], loss: 1.250308, mae: 0.860732, mean_q: 6.508595
 96931/100000: episode: 1702, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 48.626, mean reward: 6.078 [4.292, 7.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.632], loss: 1.935169, mae: 0.994079, mean_q: 6.256883
 96948/100000: episode: 1703, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 67.192, mean reward: 3.952 [2.959, 7.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.436], loss: 3.187451, mae: 1.076018, mean_q: 6.275743
 96960/100000: episode: 1704, duration: 0.061s, episode steps: 12, steps per second: 196, episode reward: 79.335, mean reward: 6.611 [5.464, 7.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.243, 10.589], loss: 1.290332, mae: 0.868564, mean_q: 6.312033
 96970/100000: episode: 1705, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 79.040, mean reward: 7.904 [5.417, 13.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.066, 10.689], loss: 0.891194, mae: 0.837526, mean_q: 6.191191
 96983/100000: episode: 1706, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 73.556, mean reward: 5.658 [4.290, 10.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.584, 10.659], loss: 1.312564, mae: 0.898046, mean_q: 6.256437
 96993/100000: episode: 1707, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 62.535, mean reward: 6.254 [4.615, 10.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.224, 10.686], loss: 2.050315, mae: 0.974520, mean_q: 6.430429
 97005/100000: episode: 1708, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 67.618, mean reward: 5.635 [4.055, 7.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.521], loss: 2.558728, mae: 1.095315, mean_q: 6.361347
 97018/100000: episode: 1709, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 69.951, mean reward: 5.381 [3.414, 9.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.585, 10.536], loss: 1.845626, mae: 0.942984, mean_q: 6.387383
 97028/100000: episode: 1710, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 55.670, mean reward: 5.567 [3.739, 9.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.513], loss: 1.470922, mae: 0.983743, mean_q: 6.175719
 97038/100000: episode: 1711, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 60.567, mean reward: 6.057 [4.012, 9.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.638, 10.577], loss: 1.695479, mae: 1.098226, mean_q: 6.786172
 97050/100000: episode: 1712, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 49.318, mean reward: 4.110 [3.278, 4.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.518], loss: 1.427638, mae: 1.036056, mean_q: 6.495153
 97062/100000: episode: 1713, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 53.432, mean reward: 4.453 [2.935, 8.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.496], loss: 1.541968, mae: 0.944851, mean_q: 6.366590
 97068/100000: episode: 1714, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 41.152, mean reward: 6.859 [4.537, 9.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.108, 10.521], loss: 3.197349, mae: 1.195786, mean_q: 6.867582
 97076/100000: episode: 1715, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 182.596, mean reward: 22.825 [6.407, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.763], loss: 0.855448, mae: 0.827876, mean_q: 6.464621
 97084/100000: episode: 1716, duration: 0.043s, episode steps: 8, steps per second: 185, episode reward: 43.685, mean reward: 5.461 [4.384, 7.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.549], loss: 0.956101, mae: 0.874356, mean_q: 6.546100
 97094/100000: episode: 1717, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 70.895, mean reward: 7.089 [3.333, 17.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.661], loss: 17.471212, mae: 1.736969, mean_q: 7.451605
 97107/100000: episode: 1718, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 70.351, mean reward: 5.412 [3.812, 8.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.778, 10.444], loss: 1.450227, mae: 1.051217, mean_q: 5.927636
 97113/100000: episode: 1719, duration: 0.033s, episode steps: 6, steps per second: 180, episode reward: 119.815, mean reward: 19.969 [6.889, 73.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.729], loss: 3.202436, mae: 1.364158, mean_q: 7.511845
 97126/100000: episode: 1720, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 52.770, mean reward: 4.059 [2.877, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.164, 10.530], loss: 1.827794, mae: 1.005708, mean_q: 6.248925
 97138/100000: episode: 1721, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 72.502, mean reward: 6.042 [4.372, 9.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.563], loss: 2.076234, mae: 1.035151, mean_q: 6.710975
 97145/100000: episode: 1722, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 153.401, mean reward: 21.914 [7.836, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.694], loss: 1.985289, mae: 1.171912, mean_q: 6.958150
 97162/100000: episode: 1723, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 74.743, mean reward: 4.397 [3.512, 5.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.541], loss: 1.681347, mae: 1.054825, mean_q: 6.715091
 97168/100000: episode: 1724, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 32.592, mean reward: 5.432 [4.525, 6.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.594], loss: 29.106497, mae: 2.257977, mean_q: 7.767828
 97178/100000: episode: 1725, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 98.517, mean reward: 9.852 [6.548, 14.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.692], loss: 8.419187, mae: 1.142195, mean_q: 6.138789
 97186/100000: episode: 1726, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 52.244, mean reward: 6.530 [5.059, 8.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.678], loss: 2.121684, mae: 1.166032, mean_q: 7.174831
 97192/100000: episode: 1727, duration: 0.040s, episode steps: 6, steps per second: 149, episode reward: 56.432, mean reward: 9.405 [6.382, 17.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.696], loss: 1.107037, mae: 0.962841, mean_q: 6.649656
[Info] 4-TH LEVEL FOUND: 13.598837852478027, Considering 10/90 traces
 97202/100000: episode: 1728, duration: 4.349s, episode steps: 10, steps per second: 2, episode reward: 94.245, mean reward: 9.424 [5.798, 14.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.383, 10.667], loss: 2.063785, mae: 1.030442, mean_q: 6.809692
 97205/100000: episode: 1729, duration: 0.020s, episode steps: 3, steps per second: 149, episode reward: 13.558, mean reward: 4.519 [4.152, 4.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.471], loss: 0.667392, mae: 0.777503, mean_q: 6.227889
 97212/100000: episode: 1730, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 66.736, mean reward: 9.534 [6.522, 11.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.678], loss: 11.590467, mae: 1.383525, mean_q: 7.262023
 97215/100000: episode: 1731, duration: 0.022s, episode steps: 3, steps per second: 134, episode reward: 22.006, mean reward: 7.335 [4.205, 12.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.880, 10.715], loss: 1.297784, mae: 1.126573, mean_q: 7.074188
 97218/100000: episode: 1732, duration: 0.023s, episode steps: 3, steps per second: 130, episode reward: 19.329, mean reward: 6.443 [5.962, 6.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.610], loss: 4.206461, mae: 1.424697, mean_q: 7.205437
 97221/100000: episode: 1733, duration: 0.019s, episode steps: 3, steps per second: 155, episode reward: 70.952, mean reward: 23.651 [14.334, 41.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.035, 10.773], loss: 48.474426, mae: 2.271723, mean_q: 7.480688
 97224/100000: episode: 1734, duration: 0.024s, episode steps: 3, steps per second: 123, episode reward: 24.680, mean reward: 8.227 [6.067, 11.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.035, 10.696], loss: 2.451061, mae: 1.565634, mean_q: 8.481128
[Info] FALSIFICATION!
[Info] Levels: [5.505505, 7.2365274, 9.907567, 13.598838, 14.695351]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.03]
[Info] Error Prob: 3.000000000000001e-06

 97229/100000: episode: 1735, duration: 4.660s, episode steps: 5, steps per second: 1, episode reward: 131.495, mean reward: 26.299 [5.469, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.013, 10.832], loss: 1.556692, mae: 1.038987, mean_q: 6.560533
 97329/100000: episode: 1736, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 187.812, mean reward: 1.878 [1.483, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.270, 10.179], loss: 5.185390, mae: 1.252101, mean_q: 6.995653
 97429/100000: episode: 1737, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 205.017, mean reward: 2.050 [1.435, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.240, 10.098], loss: 8.997027, mae: 1.309457, mean_q: 7.024817
 97529/100000: episode: 1738, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 183.961, mean reward: 1.840 [1.522, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.356, 10.226], loss: 7.842325, mae: 1.304094, mean_q: 7.016246
 97629/100000: episode: 1739, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 194.955, mean reward: 1.950 [1.471, 4.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.131, 10.098], loss: 4.514666, mae: 1.174589, mean_q: 6.854040
 97729/100000: episode: 1740, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 187.431, mean reward: 1.874 [1.444, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.849, 10.151], loss: 4.184041, mae: 1.218010, mean_q: 6.961877
 97829/100000: episode: 1741, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 187.446, mean reward: 1.874 [1.463, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.094, 10.098], loss: 4.850671, mae: 1.181623, mean_q: 6.955203
 97929/100000: episode: 1742, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 182.042, mean reward: 1.820 [1.460, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.720, 10.098], loss: 6.828227, mae: 1.219465, mean_q: 6.907501
 98029/100000: episode: 1743, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 252.037, mean reward: 2.520 [1.447, 9.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.357, 10.453], loss: 10.931801, mae: 1.473113, mean_q: 6.995100
 98129/100000: episode: 1744, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 182.924, mean reward: 1.829 [1.442, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.572, 10.098], loss: 8.455320, mae: 1.294485, mean_q: 6.852142
 98229/100000: episode: 1745, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 200.797, mean reward: 2.008 [1.444, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.744, 10.162], loss: 3.762627, mae: 1.093198, mean_q: 6.796887
 98329/100000: episode: 1746, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 189.886, mean reward: 1.899 [1.491, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.768, 10.098], loss: 4.355282, mae: 1.111704, mean_q: 6.713551
 98429/100000: episode: 1747, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 286.033, mean reward: 2.860 [1.435, 6.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.797, 10.098], loss: 6.060154, mae: 1.207058, mean_q: 6.800154
 98529/100000: episode: 1748, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 193.097, mean reward: 1.931 [1.506, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.671, 10.098], loss: 4.406249, mae: 1.152253, mean_q: 6.757668
 98629/100000: episode: 1749, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 191.735, mean reward: 1.917 [1.489, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.691, 10.121], loss: 8.562765, mae: 1.193770, mean_q: 6.611055
 98729/100000: episode: 1750, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 182.592, mean reward: 1.826 [1.454, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.813, 10.252], loss: 4.414471, mae: 1.108741, mean_q: 6.678424
 98829/100000: episode: 1751, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 188.872, mean reward: 1.889 [1.494, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.620, 10.098], loss: 8.954153, mae: 1.335886, mean_q: 6.864600
 98929/100000: episode: 1752, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 222.534, mean reward: 2.225 [1.443, 6.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.011, 10.561], loss: 2.108798, mae: 1.077794, mean_q: 6.652935
 99029/100000: episode: 1753, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.710, mean reward: 1.927 [1.438, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.595, 10.098], loss: 8.560070, mae: 1.252593, mean_q: 6.789261
 99129/100000: episode: 1754, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 192.099, mean reward: 1.921 [1.520, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.414, 10.098], loss: 6.373063, mae: 1.222243, mean_q: 6.782619
 99229/100000: episode: 1755, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 178.215, mean reward: 1.782 [1.448, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.393, 10.156], loss: 8.500788, mae: 1.216397, mean_q: 6.691188
 99329/100000: episode: 1756, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 190.335, mean reward: 1.903 [1.456, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.669, 10.098], loss: 6.958302, mae: 1.132018, mean_q: 6.566744
 99429/100000: episode: 1757, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 184.204, mean reward: 1.842 [1.493, 2.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.390, 10.098], loss: 6.846410, mae: 1.109138, mean_q: 6.590240
 99529/100000: episode: 1758, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 208.082, mean reward: 2.081 [1.505, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.985, 10.309], loss: 6.406684, mae: 1.091824, mean_q: 6.516508
 99629/100000: episode: 1759, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 196.174, mean reward: 1.962 [1.479, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.580, 10.144], loss: 4.959219, mae: 1.018653, mean_q: 6.359172
 99729/100000: episode: 1760, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 198.957, mean reward: 1.990 [1.486, 5.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.115, 10.445], loss: 4.470081, mae: 1.091265, mean_q: 6.551536
 99829/100000: episode: 1761, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 192.074, mean reward: 1.921 [1.467, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.404, 10.098], loss: 2.850655, mae: 0.962332, mean_q: 6.302412
 99929/100000: episode: 1762, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 184.713, mean reward: 1.847 [1.489, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.886, 10.098], loss: 5.994226, mae: 1.050924, mean_q: 6.384448
done, took 591.341 seconds
[Info] End Importance Splitting. Falsification occurred 6 times.
