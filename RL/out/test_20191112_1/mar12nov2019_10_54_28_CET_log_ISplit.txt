Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.160s, episode steps: 100, steps per second: 624, episode reward: 178.795, mean reward: 1.788 [1.436, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.010, 10.109], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.075s, episode steps: 100, steps per second: 1333, episode reward: 204.465, mean reward: 2.045 [1.447, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.779, 10.273], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.061s, episode steps: 100, steps per second: 1643, episode reward: 190.969, mean reward: 1.910 [1.454, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.384, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.061s, episode steps: 100, steps per second: 1652, episode reward: 190.787, mean reward: 1.908 [1.452, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.068, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 184.762, mean reward: 1.848 [1.470, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.590, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 222.270, mean reward: 2.223 [1.523, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.749, 10.405], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.063s, episode steps: 100, steps per second: 1595, episode reward: 198.572, mean reward: 1.986 [1.474, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.499, 10.383], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.061s, episode steps: 100, steps per second: 1652, episode reward: 189.040, mean reward: 1.890 [1.465, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.720, 10.214], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 186.929, mean reward: 1.869 [1.451, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.779, 10.133], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 200.165, mean reward: 2.002 [1.463, 5.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.472, 10.147], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.069s, episode steps: 100, steps per second: 1454, episode reward: 232.039, mean reward: 2.320 [1.521, 4.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.581, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.098s, episode steps: 100, steps per second: 1022, episode reward: 182.704, mean reward: 1.827 [1.454, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.423, 10.138], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 191.012, mean reward: 1.910 [1.478, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.766, 10.098], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 190.161, mean reward: 1.902 [1.461, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.615, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 199.894, mean reward: 1.999 [1.459, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.343, 10.186], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.063s, episode steps: 100, steps per second: 1584, episode reward: 182.892, mean reward: 1.829 [1.458, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.612, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.085s, episode steps: 100, steps per second: 1176, episode reward: 179.851, mean reward: 1.799 [1.450, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.649, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.061s, episode steps: 100, steps per second: 1651, episode reward: 186.352, mean reward: 1.864 [1.439, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.255, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.061s, episode steps: 100, steps per second: 1644, episode reward: 183.765, mean reward: 1.838 [1.444, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.662, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.061s, episode steps: 100, steps per second: 1649, episode reward: 191.717, mean reward: 1.917 [1.490, 5.254], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.290, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.061s, episode steps: 100, steps per second: 1651, episode reward: 193.401, mean reward: 1.934 [1.479, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.499, 10.238], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 180.904, mean reward: 1.809 [1.458, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.095, 10.116], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.060s, episode steps: 100, steps per second: 1662, episode reward: 207.584, mean reward: 2.076 [1.455, 7.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.845, 10.455], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.060s, episode steps: 100, steps per second: 1655, episode reward: 186.863, mean reward: 1.869 [1.457, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.945, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.061s, episode steps: 100, steps per second: 1631, episode reward: 193.739, mean reward: 1.937 [1.456, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.994, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.072s, episode steps: 100, steps per second: 1394, episode reward: 186.536, mean reward: 1.865 [1.478, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.931, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 181.502, mean reward: 1.815 [1.448, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.261, 10.231], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.060s, episode steps: 100, steps per second: 1661, episode reward: 193.416, mean reward: 1.934 [1.475, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.224, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 185.834, mean reward: 1.858 [1.453, 3.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.428, 10.098], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.060s, episode steps: 100, steps per second: 1658, episode reward: 190.704, mean reward: 1.907 [1.454, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.570, 10.222], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.060s, episode steps: 100, steps per second: 1657, episode reward: 191.004, mean reward: 1.910 [1.463, 5.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.224, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.061s, episode steps: 100, steps per second: 1653, episode reward: 180.794, mean reward: 1.808 [1.468, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.232, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.060s, episode steps: 100, steps per second: 1658, episode reward: 217.534, mean reward: 2.175 [1.514, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.470, 10.471], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 186.518, mean reward: 1.865 [1.441, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.203, 10.284], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.077s, episode steps: 100, steps per second: 1294, episode reward: 194.634, mean reward: 1.946 [1.452, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.821, 10.231], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.087s, episode steps: 100, steps per second: 1144, episode reward: 190.724, mean reward: 1.907 [1.448, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.210, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 184.062, mean reward: 1.841 [1.467, 2.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.811, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 189.754, mean reward: 1.898 [1.464, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.561, 10.098], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 192.867, mean reward: 1.929 [1.455, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.358, 10.257], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.060s, episode steps: 100, steps per second: 1658, episode reward: 199.702, mean reward: 1.997 [1.478, 4.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.474, 10.180], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 227.263, mean reward: 2.273 [1.434, 4.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.439, 10.243], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.060s, episode steps: 100, steps per second: 1655, episode reward: 183.424, mean reward: 1.834 [1.438, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.192, 10.119], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 182.248, mean reward: 1.822 [1.439, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.354, 10.113], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.091s, episode steps: 100, steps per second: 1095, episode reward: 188.607, mean reward: 1.886 [1.491, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.908, 10.363], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.075s, episode steps: 100, steps per second: 1340, episode reward: 186.535, mean reward: 1.865 [1.469, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.638, 10.141], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.061s, episode steps: 100, steps per second: 1648, episode reward: 196.991, mean reward: 1.970 [1.460, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.457, 10.201], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.060s, episode steps: 100, steps per second: 1654, episode reward: 183.296, mean reward: 1.833 [1.441, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.697, 10.224], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.060s, episode steps: 100, steps per second: 1659, episode reward: 196.231, mean reward: 1.962 [1.460, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.670, 10.357], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.082s, episode steps: 100, steps per second: 1215, episode reward: 187.240, mean reward: 1.872 [1.475, 3.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.732, 10.108], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.061s, episode steps: 100, steps per second: 1650, episode reward: 203.620, mean reward: 2.036 [1.445, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.805, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.223s, episode steps: 100, steps per second: 82, episode reward: 183.737, mean reward: 1.837 [1.459, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.394, 10.142], loss: 0.293608, mae: 0.549892, mean_q: 2.005199
  5200/100000: episode: 52, duration: 0.678s, episode steps: 100, steps per second: 147, episode reward: 190.842, mean reward: 1.908 [1.513, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.270, 10.296], loss: 0.094709, mae: 0.307154, mean_q: 2.759128
  5300/100000: episode: 53, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 231.604, mean reward: 2.316 [1.499, 4.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.921, 10.098], loss: 0.087867, mae: 0.298657, mean_q: 3.132235
  5400/100000: episode: 54, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 184.965, mean reward: 1.850 [1.449, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.112, 10.325], loss: 0.095393, mae: 0.303129, mean_q: 3.378865
  5500/100000: episode: 55, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 196.204, mean reward: 1.962 [1.473, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.198, 10.162], loss: 0.104947, mae: 0.310839, mean_q: 3.547565
  5600/100000: episode: 56, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 182.922, mean reward: 1.829 [1.454, 2.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.393, 10.098], loss: 0.105040, mae: 0.313375, mean_q: 3.662239
  5700/100000: episode: 57, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 204.844, mean reward: 2.048 [1.502, 4.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.560, 10.249], loss: 0.099129, mae: 0.301772, mean_q: 3.702906
  5800/100000: episode: 58, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 209.189, mean reward: 2.092 [1.471, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.431, 10.306], loss: 0.113367, mae: 0.317329, mean_q: 3.735974
  5900/100000: episode: 59, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 198.924, mean reward: 1.989 [1.506, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.993, 10.098], loss: 0.104174, mae: 0.312196, mean_q: 3.772854
  6000/100000: episode: 60, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 197.042, mean reward: 1.970 [1.440, 3.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.873, 10.166], loss: 0.101506, mae: 0.302619, mean_q: 3.785168
  6100/100000: episode: 61, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 185.308, mean reward: 1.853 [1.474, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.374, 10.098], loss: 0.103913, mae: 0.308158, mean_q: 3.800821
  6200/100000: episode: 62, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 196.188, mean reward: 1.962 [1.456, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.588, 10.459], loss: 0.096130, mae: 0.301603, mean_q: 3.801285
  6300/100000: episode: 63, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.024, mean reward: 1.820 [1.450, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.925, 10.098], loss: 0.093625, mae: 0.298546, mean_q: 3.796786
  6400/100000: episode: 64, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 173.286, mean reward: 1.733 [1.460, 2.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.030, 10.103], loss: 0.102998, mae: 0.302643, mean_q: 3.810519
  6500/100000: episode: 65, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 181.980, mean reward: 1.820 [1.443, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.899, 10.098], loss: 0.100159, mae: 0.299512, mean_q: 3.799375
  6600/100000: episode: 66, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 195.759, mean reward: 1.958 [1.470, 3.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.631, 10.098], loss: 0.103396, mae: 0.300528, mean_q: 3.802123
  6700/100000: episode: 67, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 198.801, mean reward: 1.988 [1.445, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.227, 10.210], loss: 0.109026, mae: 0.312643, mean_q: 3.813457
  6800/100000: episode: 68, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 190.742, mean reward: 1.907 [1.473, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.455, 10.228], loss: 0.088215, mae: 0.284643, mean_q: 3.801317
  6900/100000: episode: 69, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 219.303, mean reward: 2.193 [1.477, 4.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.696, 10.098], loss: 0.099973, mae: 0.302881, mean_q: 3.810506
  7000/100000: episode: 70, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 226.520, mean reward: 2.265 [1.439, 6.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.761, 10.098], loss: 0.090893, mae: 0.294431, mean_q: 3.810625
  7100/100000: episode: 71, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 196.968, mean reward: 1.970 [1.504, 4.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.247, 10.487], loss: 0.091368, mae: 0.293733, mean_q: 3.806193
  7200/100000: episode: 72, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 197.283, mean reward: 1.973 [1.454, 5.581], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.292, 10.155], loss: 0.114842, mae: 0.310019, mean_q: 3.816083
  7300/100000: episode: 73, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 207.973, mean reward: 2.080 [1.473, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.977, 10.335], loss: 0.114711, mae: 0.314612, mean_q: 3.833968
  7400/100000: episode: 74, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 204.220, mean reward: 2.042 [1.514, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.830, 10.210], loss: 0.100268, mae: 0.309418, mean_q: 3.832992
  7500/100000: episode: 75, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 192.454, mean reward: 1.925 [1.443, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.110, 10.167], loss: 0.103674, mae: 0.316573, mean_q: 3.856114
  7600/100000: episode: 76, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.861, mean reward: 1.939 [1.439, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.845, 10.118], loss: 0.110724, mae: 0.317162, mean_q: 3.849437
  7700/100000: episode: 77, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 226.963, mean reward: 2.270 [1.494, 4.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.099, 10.541], loss: 0.098589, mae: 0.307305, mean_q: 3.848868
  7800/100000: episode: 78, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 199.135, mean reward: 1.991 [1.471, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.418, 10.286], loss: 0.116833, mae: 0.318660, mean_q: 3.859497
  7900/100000: episode: 79, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 190.975, mean reward: 1.910 [1.457, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.375, 10.272], loss: 0.109021, mae: 0.313051, mean_q: 3.854786
  8000/100000: episode: 80, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 204.130, mean reward: 2.041 [1.466, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.523, 10.098], loss: 0.126731, mae: 0.338245, mean_q: 3.890889
  8100/100000: episode: 81, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 218.040, mean reward: 2.180 [1.482, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.398, 10.267], loss: 0.109227, mae: 0.316728, mean_q: 3.875813
  8200/100000: episode: 82, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 179.007, mean reward: 1.790 [1.470, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.909, 10.103], loss: 0.101730, mae: 0.312237, mean_q: 3.870451
  8300/100000: episode: 83, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 204.026, mean reward: 2.040 [1.493, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.788, 10.357], loss: 0.105287, mae: 0.323845, mean_q: 3.869660
  8400/100000: episode: 84, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 202.260, mean reward: 2.023 [1.504, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-1.018, 10.265], loss: 0.099808, mae: 0.313280, mean_q: 3.887202
  8500/100000: episode: 85, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 194.166, mean reward: 1.942 [1.481, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.015, 10.266], loss: 0.102806, mae: 0.319200, mean_q: 3.891645
  8600/100000: episode: 86, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 203.891, mean reward: 2.039 [1.505, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.874, 10.098], loss: 0.101909, mae: 0.315390, mean_q: 3.895233
  8700/100000: episode: 87, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 190.609, mean reward: 1.906 [1.461, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.005, 10.194], loss: 0.115605, mae: 0.327572, mean_q: 3.907762
  8800/100000: episode: 88, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 179.855, mean reward: 1.799 [1.455, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.550, 10.098], loss: 0.104053, mae: 0.320678, mean_q: 3.895559
  8900/100000: episode: 89, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 190.224, mean reward: 1.902 [1.474, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.672, 10.239], loss: 0.105540, mae: 0.313344, mean_q: 3.895579
  9000/100000: episode: 90, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 201.217, mean reward: 2.012 [1.460, 4.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.551, 10.426], loss: 0.106472, mae: 0.317799, mean_q: 3.892530
  9100/100000: episode: 91, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 202.715, mean reward: 2.027 [1.483, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.310, 10.237], loss: 0.124915, mae: 0.336515, mean_q: 3.899261
  9200/100000: episode: 92, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 210.868, mean reward: 2.109 [1.466, 9.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.663, 10.098], loss: 0.102533, mae: 0.312996, mean_q: 3.884988
  9300/100000: episode: 93, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 203.678, mean reward: 2.037 [1.433, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.216, 10.388], loss: 0.120762, mae: 0.329961, mean_q: 3.892005
  9400/100000: episode: 94, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 184.674, mean reward: 1.847 [1.457, 2.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.117, 10.312], loss: 0.126889, mae: 0.328048, mean_q: 3.903429
  9500/100000: episode: 95, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 193.844, mean reward: 1.938 [1.506, 2.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.106, 10.098], loss: 0.123472, mae: 0.336011, mean_q: 3.893999
  9600/100000: episode: 96, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 208.731, mean reward: 2.087 [1.507, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.912, 10.098], loss: 0.120479, mae: 0.328239, mean_q: 3.893636
  9700/100000: episode: 97, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: 191.078, mean reward: 1.911 [1.438, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.251, 10.098], loss: 0.109405, mae: 0.320281, mean_q: 3.909691
  9800/100000: episode: 98, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 195.710, mean reward: 1.957 [1.500, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.585, 10.248], loss: 0.120811, mae: 0.326229, mean_q: 3.921051
  9900/100000: episode: 99, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 186.616, mean reward: 1.866 [1.451, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.576, 10.098], loss: 0.103141, mae: 0.323729, mean_q: 3.911081
 10000/100000: episode: 100, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 204.391, mean reward: 2.044 [1.468, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.223, 10.098], loss: 0.134079, mae: 0.327539, mean_q: 3.912617
 10100/100000: episode: 101, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 198.907, mean reward: 1.989 [1.491, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.108, 10.349], loss: 0.116331, mae: 0.319910, mean_q: 3.908838
 10200/100000: episode: 102, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 191.302, mean reward: 1.913 [1.462, 5.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.886, 10.208], loss: 0.112202, mae: 0.328207, mean_q: 3.921133
 10300/100000: episode: 103, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 196.931, mean reward: 1.969 [1.458, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.897, 10.098], loss: 0.106909, mae: 0.312112, mean_q: 3.916641
 10400/100000: episode: 104, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 196.994, mean reward: 1.970 [1.469, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.181, 10.202], loss: 0.111191, mae: 0.308472, mean_q: 3.888300
 10500/100000: episode: 105, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 188.630, mean reward: 1.886 [1.481, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.970, 10.207], loss: 0.097033, mae: 0.304839, mean_q: 3.894071
 10600/100000: episode: 106, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 215.428, mean reward: 2.154 [1.459, 4.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.875, 10.442], loss: 0.114154, mae: 0.316901, mean_q: 3.894945
 10700/100000: episode: 107, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 193.654, mean reward: 1.937 [1.469, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.234, 10.351], loss: 0.102775, mae: 0.316202, mean_q: 3.908340
 10800/100000: episode: 108, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.357, mean reward: 1.934 [1.453, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.044, 10.098], loss: 0.112178, mae: 0.319712, mean_q: 3.905033
 10900/100000: episode: 109, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 189.033, mean reward: 1.890 [1.435, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.802, 10.098], loss: 0.116931, mae: 0.317605, mean_q: 3.897749
 11000/100000: episode: 110, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.224, mean reward: 1.872 [1.471, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.017, 10.266], loss: 0.117167, mae: 0.318243, mean_q: 3.899179
 11100/100000: episode: 111, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 202.077, mean reward: 2.021 [1.503, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.107, 10.098], loss: 0.112188, mae: 0.319709, mean_q: 3.918431
 11200/100000: episode: 112, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 195.167, mean reward: 1.952 [1.473, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.371, 10.098], loss: 0.112964, mae: 0.326932, mean_q: 3.911653
 11300/100000: episode: 113, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 189.707, mean reward: 1.897 [1.473, 2.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.419, 10.098], loss: 0.119006, mae: 0.319899, mean_q: 3.915689
 11400/100000: episode: 114, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 207.841, mean reward: 2.078 [1.475, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.916, 10.098], loss: 0.111991, mae: 0.318785, mean_q: 3.915848
 11500/100000: episode: 115, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 185.302, mean reward: 1.853 [1.453, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.715, 10.098], loss: 0.115290, mae: 0.316161, mean_q: 3.925956
 11600/100000: episode: 116, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 192.887, mean reward: 1.929 [1.459, 2.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.544, 10.098], loss: 0.118252, mae: 0.331335, mean_q: 3.919884
 11700/100000: episode: 117, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 195.352, mean reward: 1.954 [1.476, 6.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.958, 10.352], loss: 0.124475, mae: 0.330300, mean_q: 3.928523
 11800/100000: episode: 118, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 188.908, mean reward: 1.889 [1.460, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.605, 10.108], loss: 0.099469, mae: 0.306219, mean_q: 3.928821
 11900/100000: episode: 119, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.157, mean reward: 1.882 [1.435, 2.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.022, 10.098], loss: 0.114408, mae: 0.315732, mean_q: 3.908316
 12000/100000: episode: 120, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 206.435, mean reward: 2.064 [1.455, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.604, 10.098], loss: 0.092179, mae: 0.306670, mean_q: 3.915448
 12100/100000: episode: 121, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 218.461, mean reward: 2.185 [1.463, 3.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.553, 10.098], loss: 0.093897, mae: 0.308971, mean_q: 3.908322
 12200/100000: episode: 122, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 187.760, mean reward: 1.878 [1.460, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.626, 10.098], loss: 0.112417, mae: 0.308638, mean_q: 3.896570
 12300/100000: episode: 123, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 201.128, mean reward: 2.011 [1.476, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.570, 10.208], loss: 0.108889, mae: 0.316094, mean_q: 3.915432
 12400/100000: episode: 124, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 190.850, mean reward: 1.908 [1.442, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.686, 10.263], loss: 0.097536, mae: 0.314040, mean_q: 3.904220
 12500/100000: episode: 125, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 196.070, mean reward: 1.961 [1.448, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.193, 10.211], loss: 0.110224, mae: 0.321185, mean_q: 3.914221
 12600/100000: episode: 126, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 195.662, mean reward: 1.957 [1.450, 6.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.749, 10.098], loss: 0.121146, mae: 0.334986, mean_q: 3.919384
 12700/100000: episode: 127, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 188.160, mean reward: 1.882 [1.478, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.487, 10.098], loss: 0.101829, mae: 0.304645, mean_q: 3.888819
 12800/100000: episode: 128, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 200.250, mean reward: 2.003 [1.444, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.525, 10.098], loss: 0.091090, mae: 0.301323, mean_q: 3.868305
 12900/100000: episode: 129, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 198.852, mean reward: 1.989 [1.483, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.376, 10.299], loss: 0.128200, mae: 0.328300, mean_q: 3.899932
 13000/100000: episode: 130, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 216.736, mean reward: 2.167 [1.448, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.776, 10.098], loss: 0.108594, mae: 0.316717, mean_q: 3.890988
 13100/100000: episode: 131, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 200.103, mean reward: 2.001 [1.469, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.721, 10.230], loss: 0.092023, mae: 0.303087, mean_q: 3.871371
 13200/100000: episode: 132, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 200.627, mean reward: 2.006 [1.444, 5.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.282, 10.306], loss: 0.096560, mae: 0.305441, mean_q: 3.880904
 13300/100000: episode: 133, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 192.911, mean reward: 1.929 [1.437, 5.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.208, 10.245], loss: 0.111234, mae: 0.322998, mean_q: 3.889571
 13400/100000: episode: 134, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 214.136, mean reward: 2.141 [1.476, 5.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.511, 10.513], loss: 0.099651, mae: 0.302428, mean_q: 3.889826
 13500/100000: episode: 135, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 188.143, mean reward: 1.881 [1.464, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.630, 10.098], loss: 0.112976, mae: 0.311146, mean_q: 3.861405
 13600/100000: episode: 136, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 193.489, mean reward: 1.935 [1.475, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.372, 10.169], loss: 0.108112, mae: 0.314856, mean_q: 3.896893
 13700/100000: episode: 137, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 234.438, mean reward: 2.344 [1.443, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.794, 10.453], loss: 0.105288, mae: 0.301298, mean_q: 3.872853
 13800/100000: episode: 138, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 182.768, mean reward: 1.828 [1.468, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.849, 10.208], loss: 0.112095, mae: 0.311689, mean_q: 3.889257
 13900/100000: episode: 139, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 183.332, mean reward: 1.833 [1.440, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.936, 10.212], loss: 0.099642, mae: 0.309561, mean_q: 3.905238
 14000/100000: episode: 140, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 181.885, mean reward: 1.819 [1.459, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.250, 10.098], loss: 0.103096, mae: 0.311713, mean_q: 3.905008
 14100/100000: episode: 141, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 234.587, mean reward: 2.346 [1.455, 16.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.039, 10.098], loss: 0.111045, mae: 0.322592, mean_q: 3.907201
 14200/100000: episode: 142, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 191.898, mean reward: 1.919 [1.468, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.943, 10.098], loss: 0.147518, mae: 0.317545, mean_q: 3.913489
 14300/100000: episode: 143, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 190.365, mean reward: 1.904 [1.452, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.002, 10.098], loss: 0.133048, mae: 0.306398, mean_q: 3.879869
 14400/100000: episode: 144, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 188.635, mean reward: 1.886 [1.449, 3.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.384, 10.134], loss: 0.181733, mae: 0.328878, mean_q: 3.912201
 14500/100000: episode: 145, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 205.875, mean reward: 2.059 [1.464, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.537, 10.399], loss: 0.114956, mae: 0.312998, mean_q: 3.907356
 14600/100000: episode: 146, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 222.860, mean reward: 2.229 [1.488, 4.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.867, 10.098], loss: 0.108141, mae: 0.313222, mean_q: 3.905317
 14700/100000: episode: 147, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 182.555, mean reward: 1.826 [1.451, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.772, 10.196], loss: 0.152894, mae: 0.325649, mean_q: 3.919096
 14800/100000: episode: 148, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 183.593, mean reward: 1.836 [1.456, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.840, 10.099], loss: 0.134988, mae: 0.337759, mean_q: 3.938985
 14900/100000: episode: 149, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 179.746, mean reward: 1.797 [1.459, 2.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.141, 10.186], loss: 0.188931, mae: 0.337805, mean_q: 3.902872
[Info] 1-TH LEVEL FOUND: 4.511834144592285, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.097s, episode steps: 100, steps per second: 20, episode reward: 171.796, mean reward: 1.718 [1.442, 2.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.594, 10.177], loss: 0.153231, mae: 0.325778, mean_q: 3.889221
 15045/100000: episode: 151, duration: 0.257s, episode steps: 45, steps per second: 175, episode reward: 101.047, mean reward: 2.245 [1.488, 6.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.400, 10.157], loss: 0.100000, mae: 0.310943, mean_q: 3.916378
 15093/100000: episode: 152, duration: 0.275s, episode steps: 48, steps per second: 175, episode reward: 98.795, mean reward: 2.058 [1.547, 5.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.589, 10.100], loss: 0.113055, mae: 0.320734, mean_q: 3.893677
 15142/100000: episode: 153, duration: 0.294s, episode steps: 49, steps per second: 166, episode reward: 102.246, mean reward: 2.087 [1.439, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.832, 10.245], loss: 0.113479, mae: 0.316222, mean_q: 3.909446
 15191/100000: episode: 154, duration: 0.281s, episode steps: 49, steps per second: 174, episode reward: 91.052, mean reward: 1.858 [1.490, 2.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.291, 10.100], loss: 0.182142, mae: 0.332286, mean_q: 3.890153
 15240/100000: episode: 155, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 119.136, mean reward: 2.431 [1.512, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.001, 10.100], loss: 0.265946, mae: 0.350634, mean_q: 3.907430
 15286/100000: episode: 156, duration: 0.263s, episode steps: 46, steps per second: 175, episode reward: 115.116, mean reward: 2.503 [1.456, 4.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.234, 10.278], loss: 0.106585, mae: 0.314790, mean_q: 3.887479
 15332/100000: episode: 157, duration: 0.261s, episode steps: 46, steps per second: 176, episode reward: 132.102, mean reward: 2.872 [1.852, 4.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.833, 10.352], loss: 0.107478, mae: 0.319310, mean_q: 3.892125
 15380/100000: episode: 158, duration: 0.262s, episode steps: 48, steps per second: 183, episode reward: 112.397, mean reward: 2.342 [1.642, 4.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.240, 10.235], loss: 0.112170, mae: 0.324789, mean_q: 3.919578
 15419/100000: episode: 159, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 123.296, mean reward: 3.161 [1.986, 4.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.253, 10.526], loss: 0.304534, mae: 0.380348, mean_q: 3.957546
 15468/100000: episode: 160, duration: 0.264s, episode steps: 49, steps per second: 186, episode reward: 141.742, mean reward: 2.893 [2.020, 5.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.484, 10.495], loss: 0.111654, mae: 0.324486, mean_q: 3.937909
 15514/100000: episode: 161, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 93.577, mean reward: 2.034 [1.514, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.238, 10.192], loss: 0.208975, mae: 0.347703, mean_q: 3.982158
 15562/100000: episode: 162, duration: 0.263s, episode steps: 48, steps per second: 183, episode reward: 97.474, mean reward: 2.031 [1.520, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.462, 10.100], loss: 0.186690, mae: 0.337875, mean_q: 3.945660
 15613/100000: episode: 163, duration: 0.282s, episode steps: 51, steps per second: 181, episode reward: 93.495, mean reward: 1.833 [1.484, 2.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.631, 10.169], loss: 0.209751, mae: 0.348524, mean_q: 3.968051
 15658/100000: episode: 164, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 88.276, mean reward: 1.962 [1.463, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-2.143, 10.224], loss: 0.154814, mae: 0.368222, mean_q: 3.961569
 15706/100000: episode: 165, duration: 0.246s, episode steps: 48, steps per second: 195, episode reward: 104.885, mean reward: 2.185 [1.641, 4.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.517, 10.257], loss: 0.124767, mae: 0.338563, mean_q: 3.963660
 15756/100000: episode: 166, duration: 0.253s, episode steps: 50, steps per second: 197, episode reward: 111.198, mean reward: 2.224 [1.620, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.267, 10.302], loss: 0.191557, mae: 0.332403, mean_q: 3.981025
 15805/100000: episode: 167, duration: 0.251s, episode steps: 49, steps per second: 195, episode reward: 131.991, mean reward: 2.694 [1.977, 5.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-1.033, 10.367], loss: 0.206672, mae: 0.367883, mean_q: 3.994033
 15844/100000: episode: 168, duration: 0.214s, episode steps: 39, steps per second: 182, episode reward: 99.972, mean reward: 2.563 [1.813, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.689, 10.356], loss: 0.107950, mae: 0.322811, mean_q: 3.995966
 15883/100000: episode: 169, duration: 0.211s, episode steps: 39, steps per second: 185, episode reward: 94.606, mean reward: 2.426 [1.928, 5.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.471, 10.362], loss: 0.205734, mae: 0.337675, mean_q: 3.994779
 15932/100000: episode: 170, duration: 0.264s, episode steps: 49, steps per second: 185, episode reward: 106.173, mean reward: 2.167 [1.471, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.522, 10.221], loss: 0.118954, mae: 0.322467, mean_q: 3.973810
 15980/100000: episode: 171, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 120.290, mean reward: 2.506 [1.775, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.282, 10.420], loss: 0.140920, mae: 0.341056, mean_q: 4.014979
 16030/100000: episode: 172, duration: 0.289s, episode steps: 50, steps per second: 173, episode reward: 100.190, mean reward: 2.004 [1.481, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.480, 10.100], loss: 0.167607, mae: 0.377322, mean_q: 4.035492
 16076/100000: episode: 173, duration: 0.268s, episode steps: 46, steps per second: 172, episode reward: 100.138, mean reward: 2.177 [1.581, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.338, 10.100], loss: 0.112837, mae: 0.342347, mean_q: 4.014872
 16115/100000: episode: 174, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 95.897, mean reward: 2.459 [1.501, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.839, 10.252], loss: 0.208241, mae: 0.350873, mean_q: 4.015638
 16165/100000: episode: 175, duration: 0.280s, episode steps: 50, steps per second: 178, episode reward: 118.019, mean reward: 2.360 [1.913, 3.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.849, 10.253], loss: 0.115461, mae: 0.322329, mean_q: 4.005168
 16210/100000: episode: 176, duration: 0.241s, episode steps: 45, steps per second: 187, episode reward: 105.753, mean reward: 2.350 [1.694, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.321, 10.367], loss: 0.134098, mae: 0.350073, mean_q: 4.048259
 16260/100000: episode: 177, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 120.774, mean reward: 2.415 [1.619, 4.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.437, 10.224], loss: 0.129013, mae: 0.361227, mean_q: 4.064743
 16306/100000: episode: 178, duration: 0.244s, episode steps: 46, steps per second: 188, episode reward: 89.781, mean reward: 1.952 [1.506, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.253, 10.100], loss: 0.221618, mae: 0.394950, mean_q: 4.090635
 16352/100000: episode: 179, duration: 0.247s, episode steps: 46, steps per second: 187, episode reward: 92.980, mean reward: 2.021 [1.600, 2.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.250, 10.225], loss: 0.139880, mae: 0.364862, mean_q: 4.080636
 16400/100000: episode: 180, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 125.014, mean reward: 2.604 [1.711, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.564, 10.444], loss: 0.227923, mae: 0.369273, mean_q: 4.096141
 16448/100000: episode: 181, duration: 0.253s, episode steps: 48, steps per second: 189, episode reward: 97.637, mean reward: 2.034 [1.451, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.591, 10.242], loss: 0.207261, mae: 0.364253, mean_q: 4.049739
 16497/100000: episode: 182, duration: 0.264s, episode steps: 49, steps per second: 185, episode reward: 110.704, mean reward: 2.259 [1.555, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.652, 10.248], loss: 0.211032, mae: 0.382879, mean_q: 4.074246
 16527/100000: episode: 183, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 85.046, mean reward: 2.835 [2.118, 5.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.611, 10.100], loss: 0.235096, mae: 0.359576, mean_q: 4.089739
 16575/100000: episode: 184, duration: 0.238s, episode steps: 48, steps per second: 202, episode reward: 114.788, mean reward: 2.391 [1.491, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.632, 10.100], loss: 0.217654, mae: 0.374656, mean_q: 4.076511
 16614/100000: episode: 185, duration: 0.209s, episode steps: 39, steps per second: 187, episode reward: 92.401, mean reward: 2.369 [1.702, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.656, 10.339], loss: 0.237389, mae: 0.368537, mean_q: 4.136012
 16662/100000: episode: 186, duration: 0.263s, episode steps: 48, steps per second: 183, episode reward: 110.009, mean reward: 2.292 [1.542, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.360, 10.527], loss: 0.119158, mae: 0.334604, mean_q: 4.078210
 16710/100000: episode: 187, duration: 0.263s, episode steps: 48, steps per second: 182, episode reward: 107.359, mean reward: 2.237 [1.498, 10.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.744, 10.236], loss: 0.110534, mae: 0.333018, mean_q: 4.064767
 16759/100000: episode: 188, duration: 0.270s, episode steps: 49, steps per second: 181, episode reward: 98.677, mean reward: 2.014 [1.454, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.747, 10.146], loss: 0.131104, mae: 0.341603, mean_q: 4.051706
 16810/100000: episode: 189, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 93.108, mean reward: 1.826 [1.460, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.212, 10.304], loss: 0.119852, mae: 0.343485, mean_q: 4.082675
 16849/100000: episode: 190, duration: 0.225s, episode steps: 39, steps per second: 173, episode reward: 80.336, mean reward: 2.060 [1.555, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.050, 10.190], loss: 0.171183, mae: 0.381982, mean_q: 4.119022
 16898/100000: episode: 191, duration: 0.272s, episode steps: 49, steps per second: 180, episode reward: 104.940, mean reward: 2.142 [1.568, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.528, 10.145], loss: 0.222425, mae: 0.376758, mean_q: 4.125118
 16949/100000: episode: 192, duration: 0.272s, episode steps: 51, steps per second: 187, episode reward: 97.508, mean reward: 1.912 [1.488, 2.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.931, 10.317], loss: 0.127441, mae: 0.334581, mean_q: 4.100744
 16995/100000: episode: 193, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 96.508, mean reward: 2.098 [1.583, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.506, 10.183], loss: 0.141833, mae: 0.355180, mean_q: 4.079566
 17025/100000: episode: 194, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 64.332, mean reward: 2.144 [1.654, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.823, 10.100], loss: 0.139149, mae: 0.374160, mean_q: 4.102330
 17075/100000: episode: 195, duration: 0.289s, episode steps: 50, steps per second: 173, episode reward: 133.078, mean reward: 2.662 [1.827, 6.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.426, 10.363], loss: 0.143412, mae: 0.359751, mean_q: 4.161179
 17124/100000: episode: 196, duration: 0.256s, episode steps: 49, steps per second: 192, episode reward: 107.243, mean reward: 2.189 [1.579, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-1.410, 10.100], loss: 0.226769, mae: 0.385822, mean_q: 4.093206
 17173/100000: episode: 197, duration: 0.265s, episode steps: 49, steps per second: 185, episode reward: 124.631, mean reward: 2.543 [1.866, 3.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-1.588, 10.317], loss: 0.222085, mae: 0.373754, mean_q: 4.147127
 17219/100000: episode: 198, duration: 0.251s, episode steps: 46, steps per second: 184, episode reward: 109.610, mean reward: 2.383 [1.480, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.256, 10.134], loss: 0.210020, mae: 0.381248, mean_q: 4.120999
 17265/100000: episode: 199, duration: 0.250s, episode steps: 46, steps per second: 184, episode reward: 104.167, mean reward: 2.265 [1.505, 5.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.466, 10.100], loss: 0.128994, mae: 0.355660, mean_q: 4.127095
 17315/100000: episode: 200, duration: 0.278s, episode steps: 50, steps per second: 180, episode reward: 103.132, mean reward: 2.063 [1.495, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.266, 10.143], loss: 0.130723, mae: 0.347039, mean_q: 4.155827
 17363/100000: episode: 201, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 120.151, mean reward: 2.503 [2.023, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.634, 10.355], loss: 0.162335, mae: 0.355607, mean_q: 4.153161
 17414/100000: episode: 202, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 89.817, mean reward: 1.761 [1.462, 2.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.139, 10.113], loss: 0.140195, mae: 0.371812, mean_q: 4.138264
 17464/100000: episode: 203, duration: 0.279s, episode steps: 50, steps per second: 179, episode reward: 101.381, mean reward: 2.028 [1.557, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.607, 10.162], loss: 0.142382, mae: 0.365028, mean_q: 4.186632
 17513/100000: episode: 204, duration: 0.263s, episode steps: 49, steps per second: 186, episode reward: 106.954, mean reward: 2.183 [1.570, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-1.031, 10.151], loss: 0.144285, mae: 0.360240, mean_q: 4.158958
 17559/100000: episode: 205, duration: 0.234s, episode steps: 46, steps per second: 196, episode reward: 100.023, mean reward: 2.174 [1.509, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.530, 10.100], loss: 0.132019, mae: 0.350142, mean_q: 4.145031
 17608/100000: episode: 206, duration: 0.259s, episode steps: 49, steps per second: 189, episode reward: 110.024, mean reward: 2.245 [1.450, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.277, 10.100], loss: 0.187359, mae: 0.346049, mean_q: 4.128725
 17659/100000: episode: 207, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 94.808, mean reward: 1.859 [1.474, 2.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.222, 10.323], loss: 0.138658, mae: 0.348363, mean_q: 4.165697
 17708/100000: episode: 208, duration: 0.274s, episode steps: 49, steps per second: 179, episode reward: 97.567, mean reward: 1.991 [1.501, 2.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-1.584, 10.317], loss: 0.205148, mae: 0.375309, mean_q: 4.179475
 17756/100000: episode: 209, duration: 0.263s, episode steps: 48, steps per second: 182, episode reward: 102.109, mean reward: 2.127 [1.510, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-1.611, 10.207], loss: 0.095960, mae: 0.320004, mean_q: 4.148536
 17786/100000: episode: 210, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 94.734, mean reward: 3.158 [1.740, 5.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.337, 10.100], loss: 0.135192, mae: 0.356716, mean_q: 4.139828
 17836/100000: episode: 211, duration: 0.281s, episode steps: 50, steps per second: 178, episode reward: 125.492, mean reward: 2.510 [1.469, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.264, 10.349], loss: 0.130296, mae: 0.356316, mean_q: 4.170513
 17881/100000: episode: 212, duration: 0.245s, episode steps: 45, steps per second: 184, episode reward: 90.502, mean reward: 2.011 [1.492, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.128, 10.100], loss: 0.155617, mae: 0.378442, mean_q: 4.183670
 17930/100000: episode: 213, duration: 0.264s, episode steps: 49, steps per second: 186, episode reward: 101.798, mean reward: 2.078 [1.680, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.296, 10.196], loss: 0.195396, mae: 0.362970, mean_q: 4.166952
 17979/100000: episode: 214, duration: 0.253s, episode steps: 49, steps per second: 194, episode reward: 128.688, mean reward: 2.626 [1.726, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.277, 10.163], loss: 0.121598, mae: 0.352648, mean_q: 4.148555
 18027/100000: episode: 215, duration: 0.260s, episode steps: 48, steps per second: 185, episode reward: 99.031, mean reward: 2.063 [1.530, 6.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-1.201, 10.172], loss: 0.168253, mae: 0.388157, mean_q: 4.172047
 18057/100000: episode: 216, duration: 0.177s, episode steps: 30, steps per second: 169, episode reward: 78.801, mean reward: 2.627 [1.928, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.306, 10.100], loss: 0.152173, mae: 0.361754, mean_q: 4.174295
 18106/100000: episode: 217, duration: 0.269s, episode steps: 49, steps per second: 182, episode reward: 94.832, mean reward: 1.935 [1.484, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.282, 10.119], loss: 0.174946, mae: 0.393747, mean_q: 4.231315
 18154/100000: episode: 218, duration: 0.260s, episode steps: 48, steps per second: 185, episode reward: 132.024, mean reward: 2.751 [1.660, 12.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.765, 10.148], loss: 0.134224, mae: 0.354050, mean_q: 4.177971
 18200/100000: episode: 219, duration: 0.258s, episode steps: 46, steps per second: 178, episode reward: 95.338, mean reward: 2.073 [1.559, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.500, 10.321], loss: 0.117411, mae: 0.341335, mean_q: 4.176467
 18250/100000: episode: 220, duration: 0.271s, episode steps: 50, steps per second: 184, episode reward: 101.646, mean reward: 2.033 [1.484, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.636, 10.100], loss: 0.141440, mae: 0.352458, mean_q: 4.156314
 18298/100000: episode: 221, duration: 0.273s, episode steps: 48, steps per second: 176, episode reward: 117.178, mean reward: 2.441 [1.577, 3.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.502, 10.217], loss: 0.149383, mae: 0.367383, mean_q: 4.207586
 18344/100000: episode: 222, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 135.215, mean reward: 2.939 [2.012, 4.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.278, 10.347], loss: 0.130098, mae: 0.367862, mean_q: 4.204043
 18389/100000: episode: 223, duration: 0.266s, episode steps: 45, steps per second: 169, episode reward: 83.394, mean reward: 1.853 [1.445, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.145, 10.121], loss: 0.175106, mae: 0.365113, mean_q: 4.224816
 18437/100000: episode: 224, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 90.174, mean reward: 1.879 [1.451, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.551, 10.293], loss: 0.131684, mae: 0.362640, mean_q: 4.220250
 18476/100000: episode: 225, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 87.297, mean reward: 2.238 [1.644, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.798, 10.231], loss: 0.165942, mae: 0.363343, mean_q: 4.165935
 18522/100000: episode: 226, duration: 0.238s, episode steps: 46, steps per second: 194, episode reward: 96.989, mean reward: 2.108 [1.567, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.274, 10.170], loss: 0.210902, mae: 0.366402, mean_q: 4.224637
 18552/100000: episode: 227, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 72.516, mean reward: 2.417 [1.921, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.270, 10.100], loss: 0.184625, mae: 0.364184, mean_q: 4.188571
 18603/100000: episode: 228, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 95.603, mean reward: 1.875 [1.474, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.108, 10.228], loss: 0.159980, mae: 0.364436, mean_q: 4.202569
 18652/100000: episode: 229, duration: 0.273s, episode steps: 49, steps per second: 179, episode reward: 111.986, mean reward: 2.285 [1.537, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.801, 10.302], loss: 0.121581, mae: 0.345264, mean_q: 4.177461
 18700/100000: episode: 230, duration: 0.280s, episode steps: 48, steps per second: 172, episode reward: 89.345, mean reward: 1.861 [1.456, 2.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.881, 10.205], loss: 0.202588, mae: 0.373949, mean_q: 4.223444
 18751/100000: episode: 231, duration: 0.287s, episode steps: 51, steps per second: 178, episode reward: 97.217, mean reward: 1.906 [1.525, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.880, 10.183], loss: 0.218435, mae: 0.379858, mean_q: 4.242599
 18800/100000: episode: 232, duration: 0.264s, episode steps: 49, steps per second: 186, episode reward: 99.440, mean reward: 2.029 [1.660, 2.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.296, 10.231], loss: 0.225565, mae: 0.382873, mean_q: 4.247853
 18850/100000: episode: 233, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 100.560, mean reward: 2.011 [1.580, 2.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.943, 10.276], loss: 0.172286, mae: 0.358055, mean_q: 4.229695
 18901/100000: episode: 234, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 105.134, mean reward: 2.061 [1.509, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.787, 10.322], loss: 0.180359, mae: 0.379282, mean_q: 4.228436
 18950/100000: episode: 235, duration: 0.268s, episode steps: 49, steps per second: 183, episode reward: 96.561, mean reward: 1.971 [1.455, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.531, 10.407], loss: 0.190257, mae: 0.369347, mean_q: 4.244221
 18999/100000: episode: 236, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 113.772, mean reward: 2.322 [1.687, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-1.230, 10.248], loss: 0.138928, mae: 0.371415, mean_q: 4.238362
 19044/100000: episode: 237, duration: 0.248s, episode steps: 45, steps per second: 182, episode reward: 111.551, mean reward: 2.479 [1.653, 7.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.272, 10.259], loss: 0.268588, mae: 0.410796, mean_q: 4.302791
 19090/100000: episode: 238, duration: 0.256s, episode steps: 46, steps per second: 180, episode reward: 101.219, mean reward: 2.200 [1.516, 4.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.288, 10.117], loss: 0.127475, mae: 0.363594, mean_q: 4.268225
 19129/100000: episode: 239, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 84.350, mean reward: 2.163 [1.592, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.057, 10.185], loss: 0.156433, mae: 0.368075, mean_q: 4.265607
[Info] 2-TH LEVEL FOUND: 5.990859031677246, Considering 10/90 traces
 19178/100000: episode: 240, duration: 4.405s, episode steps: 49, steps per second: 11, episode reward: 97.332, mean reward: 1.986 [1.477, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.478, 10.160], loss: 0.147797, mae: 0.366886, mean_q: 4.263764
 19222/100000: episode: 241, duration: 0.245s, episode steps: 44, steps per second: 179, episode reward: 98.180, mean reward: 2.231 [1.619, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.710, 10.233], loss: 0.117483, mae: 0.354947, mean_q: 4.261849
 19264/100000: episode: 242, duration: 0.218s, episode steps: 42, steps per second: 192, episode reward: 88.097, mean reward: 2.098 [1.461, 3.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.200, 10.100], loss: 0.155390, mae: 0.361572, mean_q: 4.245031
 19298/100000: episode: 243, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 119.576, mean reward: 3.517 [2.099, 11.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.078, 10.356], loss: 0.135292, mae: 0.354668, mean_q: 4.284107
 19332/100000: episode: 244, duration: 0.204s, episode steps: 34, steps per second: 167, episode reward: 91.487, mean reward: 2.691 [2.028, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.307], loss: 0.120136, mae: 0.360646, mean_q: 4.292484
 19351/100000: episode: 245, duration: 0.119s, episode steps: 19, steps per second: 160, episode reward: 61.880, mean reward: 3.257 [2.136, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.309], loss: 0.209510, mae: 0.384505, mean_q: 4.332944
 19395/100000: episode: 246, duration: 0.240s, episode steps: 44, steps per second: 183, episode reward: 94.336, mean reward: 2.144 [1.717, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.547, 10.274], loss: 0.195461, mae: 0.396130, mean_q: 4.341768
 19435/100000: episode: 247, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 96.585, mean reward: 2.415 [1.817, 4.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.891, 10.337], loss: 0.125709, mae: 0.359302, mean_q: 4.332742
 19462/100000: episode: 248, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 65.170, mean reward: 2.414 [1.740, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.254, 10.265], loss: 0.158850, mae: 0.371892, mean_q: 4.289027
 19496/100000: episode: 249, duration: 0.181s, episode steps: 34, steps per second: 187, episode reward: 142.073, mean reward: 4.179 [2.179, 9.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.418], loss: 0.133266, mae: 0.378687, mean_q: 4.302323
 19536/100000: episode: 250, duration: 0.208s, episode steps: 40, steps per second: 192, episode reward: 91.674, mean reward: 2.292 [1.612, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.539, 10.182], loss: 0.186399, mae: 0.410563, mean_q: 4.355168
 19573/100000: episode: 251, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 130.663, mean reward: 3.531 [1.932, 8.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.391, 10.377], loss: 0.209765, mae: 0.386347, mean_q: 4.380486
 19598/100000: episode: 252, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 65.872, mean reward: 2.635 [1.948, 4.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.249, 10.351], loss: 0.151150, mae: 0.394025, mean_q: 4.336827
 19642/100000: episode: 253, duration: 0.238s, episode steps: 44, steps per second: 185, episode reward: 88.835, mean reward: 2.019 [1.487, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.971, 10.206], loss: 0.173927, mae: 0.407271, mean_q: 4.288321
 19686/100000: episode: 254, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 109.395, mean reward: 2.486 [1.528, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.076, 10.125], loss: 0.174372, mae: 0.417575, mean_q: 4.343036
 19726/100000: episode: 255, duration: 0.224s, episode steps: 40, steps per second: 179, episode reward: 109.208, mean reward: 2.730 [2.095, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.529, 10.341], loss: 0.201526, mae: 0.414134, mean_q: 4.384466
 19751/100000: episode: 256, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 70.323, mean reward: 2.813 [2.238, 4.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.599, 10.422], loss: 0.143009, mae: 0.391418, mean_q: 4.425260
 19795/100000: episode: 257, duration: 0.244s, episode steps: 44, steps per second: 181, episode reward: 94.671, mean reward: 2.152 [1.575, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.431, 10.104], loss: 0.139802, mae: 0.368431, mean_q: 4.370231
 19835/100000: episode: 258, duration: 0.226s, episode steps: 40, steps per second: 177, episode reward: 108.615, mean reward: 2.715 [1.779, 5.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.097, 10.265], loss: 0.144290, mae: 0.374034, mean_q: 4.377858
 19869/100000: episode: 259, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 125.190, mean reward: 3.682 [2.738, 6.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.588], loss: 0.184845, mae: 0.387416, mean_q: 4.411942
 19894/100000: episode: 260, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 59.265, mean reward: 2.371 [1.829, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.619, 10.278], loss: 0.235899, mae: 0.427964, mean_q: 4.505345
 19934/100000: episode: 261, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 79.994, mean reward: 2.000 [1.497, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.074, 10.208], loss: 0.165237, mae: 0.397339, mean_q: 4.394746
 19968/100000: episode: 262, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 98.861, mean reward: 2.908 [2.227, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.607, 10.404], loss: 0.170129, mae: 0.404891, mean_q: 4.509488
 20005/100000: episode: 263, duration: 0.214s, episode steps: 37, steps per second: 173, episode reward: 96.165, mean reward: 2.599 [2.164, 3.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.533, 10.380], loss: 0.191943, mae: 0.386012, mean_q: 4.454534
 20033/100000: episode: 264, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 79.410, mean reward: 2.836 [2.001, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.349], loss: 0.151118, mae: 0.397683, mean_q: 4.519282
 20058/100000: episode: 265, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 77.331, mean reward: 3.093 [2.347, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-1.178, 10.481], loss: 0.185482, mae: 0.367750, mean_q: 4.497936
 20100/100000: episode: 266, duration: 0.215s, episode steps: 42, steps per second: 196, episode reward: 100.791, mean reward: 2.400 [1.523, 3.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.485, 10.456], loss: 0.153563, mae: 0.388469, mean_q: 4.486734
 20142/100000: episode: 267, duration: 0.209s, episode steps: 42, steps per second: 201, episode reward: 85.481, mean reward: 2.035 [1.526, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-1.008, 10.259], loss: 0.159007, mae: 0.379839, mean_q: 4.488658
 20182/100000: episode: 268, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 105.771, mean reward: 2.644 [2.007, 4.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.174, 10.343], loss: 0.145741, mae: 0.395407, mean_q: 4.502603
 20222/100000: episode: 269, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 118.101, mean reward: 2.953 [2.204, 4.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.106, 10.541], loss: 0.175228, mae: 0.380091, mean_q: 4.480589
 20249/100000: episode: 270, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 100.198, mean reward: 3.711 [3.027, 5.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.483, 10.509], loss: 0.198327, mae: 0.409335, mean_q: 4.484262
 20277/100000: episode: 271, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 107.293, mean reward: 3.832 [2.289, 7.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.501], loss: 0.166967, mae: 0.409681, mean_q: 4.545013
 20317/100000: episode: 272, duration: 0.197s, episode steps: 40, steps per second: 203, episode reward: 96.385, mean reward: 2.410 [1.466, 3.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.551, 10.125], loss: 0.156246, mae: 0.378297, mean_q: 4.468479
[Info] FALSIFICATION!
[Info] Levels: [4.511834, 5.990859, 7.187262]
[Info] Cond. Prob: [0.1, 0.1, 0.04]
[Info] Error Prob: 0.0004000000000000001

 20340/100000: episode: 273, duration: 4.676s, episode steps: 23, steps per second: 5, episode reward: 296.225, mean reward: 12.879 [2.323, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.379, 10.526], loss: 0.184420, mae: 0.414030, mean_q: 4.541902
 20440/100000: episode: 274, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 195.292, mean reward: 1.953 [1.441, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.426, 10.098], loss: 0.245900, mae: 0.414576, mean_q: 4.523288
 20540/100000: episode: 275, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 214.245, mean reward: 2.142 [1.447, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.038, 10.261], loss: 4.543485, mae: 0.663082, mean_q: 4.567858
 20640/100000: episode: 276, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 178.574, mean reward: 1.786 [1.446, 2.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.555, 10.288], loss: 0.218563, mae: 0.410793, mean_q: 4.522508
 20740/100000: episode: 277, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 189.872, mean reward: 1.899 [1.456, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.120, 10.106], loss: 1.680665, mae: 0.495081, mean_q: 4.539004
 20840/100000: episode: 278, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 193.006, mean reward: 1.930 [1.522, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.504, 10.098], loss: 1.636701, mae: 0.497949, mean_q: 4.522829
 20940/100000: episode: 279, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 242.099, mean reward: 2.421 [1.483, 16.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.049, 10.098], loss: 1.617880, mae: 0.487931, mean_q: 4.501650
 21040/100000: episode: 280, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 197.399, mean reward: 1.974 [1.527, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.760, 10.170], loss: 1.757176, mae: 0.551570, mean_q: 4.588779
 21140/100000: episode: 281, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 192.947, mean reward: 1.929 [1.475, 4.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.575, 10.098], loss: 1.648578, mae: 0.490804, mean_q: 4.523660
 21240/100000: episode: 282, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 202.987, mean reward: 2.030 [1.435, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.484, 10.098], loss: 1.745960, mae: 0.516524, mean_q: 4.491050
 21340/100000: episode: 283, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.922, mean reward: 1.929 [1.500, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.057, 10.098], loss: 1.675545, mae: 0.503748, mean_q: 4.519285
 21440/100000: episode: 284, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 192.673, mean reward: 1.927 [1.437, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.937, 10.098], loss: 1.653218, mae: 0.503682, mean_q: 4.481429
 21540/100000: episode: 285, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.813, mean reward: 1.948 [1.454, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.418, 10.098], loss: 0.280584, mae: 0.425439, mean_q: 4.489954
 21640/100000: episode: 286, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 190.925, mean reward: 1.909 [1.454, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.631, 10.098], loss: 3.141926, mae: 0.597715, mean_q: 4.543109
 21740/100000: episode: 287, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.769, mean reward: 1.858 [1.459, 2.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.723, 10.176], loss: 0.451960, mae: 0.436597, mean_q: 4.390640
 21840/100000: episode: 288, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.362, mean reward: 1.864 [1.497, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.000, 10.171], loss: 0.297096, mae: 0.420767, mean_q: 4.423598
 21940/100000: episode: 289, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 194.840, mean reward: 1.948 [1.472, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.617, 10.198], loss: 1.641702, mae: 0.503180, mean_q: 4.438046
 22040/100000: episode: 290, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 183.584, mean reward: 1.836 [1.461, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.563, 10.229], loss: 0.282112, mae: 0.421099, mean_q: 4.435884
 22140/100000: episode: 291, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 229.023, mean reward: 2.290 [1.556, 4.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.015, 10.231], loss: 0.276151, mae: 0.404276, mean_q: 4.397337
 22240/100000: episode: 292, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 188.428, mean reward: 1.884 [1.465, 4.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.965, 10.098], loss: 1.722709, mae: 0.486548, mean_q: 4.446945
 22340/100000: episode: 293, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 194.495, mean reward: 1.945 [1.487, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.913, 10.098], loss: 1.689348, mae: 0.456735, mean_q: 4.414756
 22440/100000: episode: 294, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.988, mean reward: 1.920 [1.462, 4.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.194, 10.098], loss: 1.557139, mae: 0.464737, mean_q: 4.403042
 22540/100000: episode: 295, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 215.076, mean reward: 2.151 [1.470, 5.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.725, 10.098], loss: 1.834135, mae: 0.503169, mean_q: 4.417487
 22640/100000: episode: 296, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 181.428, mean reward: 1.814 [1.447, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.695, 10.100], loss: 0.450808, mae: 0.446649, mean_q: 4.441832
 22740/100000: episode: 297, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 212.561, mean reward: 2.126 [1.485, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.892, 10.098], loss: 0.592915, mae: 0.447081, mean_q: 4.434097
 22840/100000: episode: 298, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 196.267, mean reward: 1.963 [1.457, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.596, 10.215], loss: 4.238177, mae: 0.562145, mean_q: 4.420842
 22940/100000: episode: 299, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 218.949, mean reward: 2.189 [1.516, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.527, 10.355], loss: 0.246833, mae: 0.442037, mean_q: 4.372527
 23040/100000: episode: 300, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.480, mean reward: 1.895 [1.491, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.777, 10.124], loss: 0.222168, mae: 0.383653, mean_q: 4.316130
 23140/100000: episode: 301, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 200.370, mean reward: 2.004 [1.497, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.438, 10.098], loss: 0.246026, mae: 0.379819, mean_q: 4.324697
 23240/100000: episode: 302, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 202.002, mean reward: 2.020 [1.484, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.906, 10.282], loss: 0.306864, mae: 0.405399, mean_q: 4.392508
 23340/100000: episode: 303, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.599, mean reward: 1.956 [1.429, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.832, 10.270], loss: 4.413308, mae: 0.771135, mean_q: 4.386106
 23440/100000: episode: 304, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 192.964, mean reward: 1.930 [1.474, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.124, 10.316], loss: 1.716440, mae: 0.489577, mean_q: 4.216312
 23540/100000: episode: 305, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 195.414, mean reward: 1.954 [1.462, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.261, 10.311], loss: 2.999743, mae: 0.527498, mean_q: 4.325357
 23640/100000: episode: 306, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 191.918, mean reward: 1.919 [1.450, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.140, 10.098], loss: 1.585343, mae: 0.471720, mean_q: 4.310289
 23740/100000: episode: 307, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 195.457, mean reward: 1.955 [1.469, 4.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.464, 10.135], loss: 0.199090, mae: 0.359390, mean_q: 4.248991
 23840/100000: episode: 308, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 192.927, mean reward: 1.929 [1.448, 5.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.839, 10.098], loss: 2.935754, mae: 0.557061, mean_q: 4.335962
 23940/100000: episode: 309, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 187.909, mean reward: 1.879 [1.471, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.597, 10.098], loss: 0.399346, mae: 0.421382, mean_q: 4.343616
 24040/100000: episode: 310, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 205.360, mean reward: 2.054 [1.491, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.307, 10.234], loss: 0.457484, mae: 0.395846, mean_q: 4.292757
 24140/100000: episode: 311, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 204.323, mean reward: 2.043 [1.454, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.825, 10.297], loss: 0.491736, mae: 0.408205, mean_q: 4.280725
 24240/100000: episode: 312, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 212.307, mean reward: 2.123 [1.507, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.164, 10.217], loss: 1.539245, mae: 0.470150, mean_q: 4.309121
 24340/100000: episode: 313, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 214.054, mean reward: 2.141 [1.475, 4.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.051, 10.098], loss: 0.474812, mae: 0.411169, mean_q: 4.241528
 24440/100000: episode: 314, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 215.063, mean reward: 2.151 [1.443, 5.249], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.949, 10.098], loss: 1.549154, mae: 0.480250, mean_q: 4.243509
 24540/100000: episode: 315, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 191.440, mean reward: 1.914 [1.531, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.707, 10.098], loss: 0.158428, mae: 0.337886, mean_q: 4.119540
 24640/100000: episode: 316, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 198.925, mean reward: 1.989 [1.437, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.363, 10.098], loss: 4.247452, mae: 0.590971, mean_q: 4.216952
 24740/100000: episode: 317, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 180.892, mean reward: 1.809 [1.449, 3.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.704, 10.191], loss: 1.507010, mae: 0.433902, mean_q: 4.139815
 24840/100000: episode: 318, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.283, mean reward: 1.923 [1.453, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.900, 10.098], loss: 1.716277, mae: 0.500556, mean_q: 4.124640
 24940/100000: episode: 319, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 199.727, mean reward: 1.997 [1.485, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.615, 10.098], loss: 0.562710, mae: 0.408567, mean_q: 4.116318
 25040/100000: episode: 320, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 202.114, mean reward: 2.021 [1.485, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.729, 10.269], loss: 1.488801, mae: 0.410348, mean_q: 4.071059
 25140/100000: episode: 321, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 200.526, mean reward: 2.005 [1.445, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.148, 10.110], loss: 0.528706, mae: 0.441653, mean_q: 4.059081
 25240/100000: episode: 322, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 176.432, mean reward: 1.764 [1.444, 2.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.772, 10.098], loss: 0.447429, mae: 0.397517, mean_q: 4.045563
 25340/100000: episode: 323, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 188.341, mean reward: 1.883 [1.457, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.676, 10.399], loss: 1.443292, mae: 0.401202, mean_q: 3.949058
 25440/100000: episode: 324, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 201.267, mean reward: 2.013 [1.456, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.113, 10.286], loss: 0.098416, mae: 0.304941, mean_q: 3.920676
 25540/100000: episode: 325, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 187.037, mean reward: 1.870 [1.478, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.395, 10.151], loss: 0.099516, mae: 0.312170, mean_q: 3.917750
 25640/100000: episode: 326, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 179.072, mean reward: 1.791 [1.442, 2.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.798, 10.098], loss: 0.159630, mae: 0.314943, mean_q: 3.900866
 25740/100000: episode: 327, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 186.962, mean reward: 1.870 [1.526, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.419, 10.098], loss: 0.164322, mae: 0.320701, mean_q: 3.927688
 25840/100000: episode: 328, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 183.853, mean reward: 1.839 [1.473, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.840, 10.098], loss: 0.125476, mae: 0.310313, mean_q: 3.904577
 25940/100000: episode: 329, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 178.869, mean reward: 1.789 [1.441, 2.743], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.154, 10.098], loss: 0.085404, mae: 0.289787, mean_q: 3.887223
 26040/100000: episode: 330, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 217.657, mean reward: 2.177 [1.640, 3.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.727, 10.098], loss: 0.085879, mae: 0.293541, mean_q: 3.862115
 26140/100000: episode: 331, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.303, mean reward: 1.883 [1.448, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.236, 10.102], loss: 0.093690, mae: 0.307367, mean_q: 3.887089
 26240/100000: episode: 332, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 198.414, mean reward: 1.984 [1.448, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.377, 10.098], loss: 0.086810, mae: 0.294146, mean_q: 3.883219
 26340/100000: episode: 333, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 199.112, mean reward: 1.991 [1.496, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.903, 10.098], loss: 0.078570, mae: 0.289441, mean_q: 3.857386
 26440/100000: episode: 334, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 180.452, mean reward: 1.805 [1.466, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.662, 10.128], loss: 0.085489, mae: 0.294178, mean_q: 3.875762
 26540/100000: episode: 335, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 193.848, mean reward: 1.938 [1.441, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.782, 10.172], loss: 0.079526, mae: 0.284617, mean_q: 3.857032
 26640/100000: episode: 336, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 197.416, mean reward: 1.974 [1.462, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.244, 10.098], loss: 0.082996, mae: 0.292583, mean_q: 3.872954
 26740/100000: episode: 337, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 185.254, mean reward: 1.853 [1.464, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.509, 10.178], loss: 0.085200, mae: 0.297964, mean_q: 3.894431
 26840/100000: episode: 338, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 174.793, mean reward: 1.748 [1.434, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.563, 10.098], loss: 0.085268, mae: 0.291545, mean_q: 3.873827
 26940/100000: episode: 339, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 189.833, mean reward: 1.898 [1.465, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.023, 10.166], loss: 0.087937, mae: 0.293329, mean_q: 3.888964
 27040/100000: episode: 340, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 182.065, mean reward: 1.821 [1.501, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.069, 10.125], loss: 0.074813, mae: 0.283128, mean_q: 3.855325
 27140/100000: episode: 341, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 188.335, mean reward: 1.883 [1.447, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.489, 10.151], loss: 0.088512, mae: 0.296135, mean_q: 3.868697
 27240/100000: episode: 342, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 181.066, mean reward: 1.811 [1.454, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.977, 10.217], loss: 0.078310, mae: 0.283944, mean_q: 3.859749
 27340/100000: episode: 343, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 184.898, mean reward: 1.849 [1.450, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.591, 10.195], loss: 0.075430, mae: 0.279419, mean_q: 3.847658
 27440/100000: episode: 344, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 192.819, mean reward: 1.928 [1.460, 2.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.333, 10.224], loss: 0.079946, mae: 0.287448, mean_q: 3.848361
 27540/100000: episode: 345, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.517, mean reward: 1.955 [1.445, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.247], loss: 0.083052, mae: 0.290146, mean_q: 3.851790
 27640/100000: episode: 346, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 206.854, mean reward: 2.069 [1.456, 7.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.196, 10.098], loss: 0.084818, mae: 0.294916, mean_q: 3.845289
 27740/100000: episode: 347, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 187.136, mean reward: 1.871 [1.447, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.778, 10.396], loss: 0.071543, mae: 0.278388, mean_q: 3.828297
 27840/100000: episode: 348, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 197.722, mean reward: 1.977 [1.476, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.531, 10.133], loss: 0.079969, mae: 0.284423, mean_q: 3.833618
 27940/100000: episode: 349, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 188.255, mean reward: 1.883 [1.488, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.119, 10.098], loss: 0.084171, mae: 0.284403, mean_q: 3.809891
 28040/100000: episode: 350, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 208.931, mean reward: 2.089 [1.459, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.179, 10.541], loss: 0.076567, mae: 0.277311, mean_q: 3.810021
 28140/100000: episode: 351, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.353, mean reward: 1.974 [1.460, 4.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.585, 10.098], loss: 0.091341, mae: 0.293207, mean_q: 3.831317
 28240/100000: episode: 352, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 190.153, mean reward: 1.902 [1.457, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.295, 10.098], loss: 0.084761, mae: 0.289161, mean_q: 3.834850
 28340/100000: episode: 353, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 189.076, mean reward: 1.891 [1.455, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.283, 10.149], loss: 0.087205, mae: 0.291170, mean_q: 3.820257
 28440/100000: episode: 354, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 208.737, mean reward: 2.087 [1.460, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.505, 10.262], loss: 0.078648, mae: 0.283464, mean_q: 3.815049
 28540/100000: episode: 355, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 192.563, mean reward: 1.926 [1.465, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.327, 10.098], loss: 0.083229, mae: 0.290273, mean_q: 3.823337
 28640/100000: episode: 356, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 196.518, mean reward: 1.965 [1.464, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.930, 10.253], loss: 0.083618, mae: 0.290560, mean_q: 3.836765
 28740/100000: episode: 357, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 211.543, mean reward: 2.115 [1.480, 4.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.337, 10.098], loss: 0.081324, mae: 0.286632, mean_q: 3.826452
 28840/100000: episode: 358, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 195.200, mean reward: 1.952 [1.435, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.744, 10.215], loss: 0.085162, mae: 0.287687, mean_q: 3.823757
 28940/100000: episode: 359, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 199.521, mean reward: 1.995 [1.482, 5.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.344, 10.182], loss: 0.077431, mae: 0.277194, mean_q: 3.831833
 29040/100000: episode: 360, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 190.349, mean reward: 1.903 [1.445, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.890, 10.237], loss: 0.100323, mae: 0.306012, mean_q: 3.841474
 29140/100000: episode: 361, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 230.394, mean reward: 2.304 [1.457, 5.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.367, 10.564], loss: 0.092843, mae: 0.302754, mean_q: 3.847266
 29240/100000: episode: 362, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 209.915, mean reward: 2.099 [1.457, 4.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.184, 10.424], loss: 0.081448, mae: 0.287521, mean_q: 3.847425
 29340/100000: episode: 363, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 183.622, mean reward: 1.836 [1.464, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.876, 10.261], loss: 0.076243, mae: 0.283107, mean_q: 3.854754
 29440/100000: episode: 364, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 184.637, mean reward: 1.846 [1.443, 4.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.021, 10.112], loss: 0.074238, mae: 0.276102, mean_q: 3.813288
 29540/100000: episode: 365, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 186.443, mean reward: 1.864 [1.462, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.398, 10.222], loss: 0.085407, mae: 0.281955, mean_q: 3.834836
 29640/100000: episode: 366, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 185.409, mean reward: 1.854 [1.456, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.078, 10.211], loss: 0.075061, mae: 0.271852, mean_q: 3.808896
 29740/100000: episode: 367, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 183.698, mean reward: 1.837 [1.445, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.892, 10.101], loss: 0.076189, mae: 0.276496, mean_q: 3.812686
 29840/100000: episode: 368, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.104, mean reward: 1.881 [1.467, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.359, 10.114], loss: 0.078421, mae: 0.277787, mean_q: 3.815731
 29940/100000: episode: 369, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 199.693, mean reward: 1.997 [1.437, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.855, 10.098], loss: 0.081533, mae: 0.278516, mean_q: 3.804409
 30040/100000: episode: 370, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 201.272, mean reward: 2.013 [1.476, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.538, 10.098], loss: 0.081827, mae: 0.285101, mean_q: 3.823619
 30140/100000: episode: 371, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 178.308, mean reward: 1.783 [1.448, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.112, 10.126], loss: 0.073127, mae: 0.271077, mean_q: 3.795496
 30240/100000: episode: 372, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 183.556, mean reward: 1.836 [1.450, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.808, 10.098], loss: 0.072774, mae: 0.270410, mean_q: 3.793854
[Info] 1-TH LEVEL FOUND: 4.7096781730651855, Considering 10/90 traces
 30340/100000: episode: 373, duration: 4.709s, episode steps: 100, steps per second: 21, episode reward: 182.275, mean reward: 1.823 [1.465, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.290, 10.105], loss: 0.073348, mae: 0.274557, mean_q: 3.795382
 30363/100000: episode: 374, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 47.539, mean reward: 2.067 [1.818, 2.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.083, 10.330], loss: 0.073203, mae: 0.268371, mean_q: 3.785228
 30391/100000: episode: 375, duration: 0.149s, episode steps: 28, steps per second: 187, episode reward: 70.836, mean reward: 2.530 [1.839, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.268, 10.302], loss: 0.074777, mae: 0.269610, mean_q: 3.767017
 30414/100000: episode: 376, duration: 0.118s, episode steps: 23, steps per second: 196, episode reward: 50.201, mean reward: 2.183 [1.551, 3.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.203], loss: 0.071619, mae: 0.266222, mean_q: 3.796594
 30512/100000: episode: 377, duration: 0.526s, episode steps: 98, steps per second: 186, episode reward: 242.133, mean reward: 2.471 [1.634, 5.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.264, 10.286], loss: 0.077250, mae: 0.271481, mean_q: 3.792285
 30525/100000: episode: 378, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 25.391, mean reward: 1.953 [1.583, 2.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.134], loss: 0.072174, mae: 0.267507, mean_q: 3.812411
 30568/100000: episode: 379, duration: 0.239s, episode steps: 43, steps per second: 180, episode reward: 102.974, mean reward: 2.395 [1.575, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.696, 10.218], loss: 0.085700, mae: 0.296469, mean_q: 3.805969
 30608/100000: episode: 380, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 80.211, mean reward: 2.005 [1.460, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.047, 10.315], loss: 0.094666, mae: 0.307190, mean_q: 3.850879
 30631/100000: episode: 381, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 62.271, mean reward: 2.707 [2.104, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.543, 10.376], loss: 0.068605, mae: 0.272506, mean_q: 3.809030
 30643/100000: episode: 382, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 27.594, mean reward: 2.300 [2.033, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.431], loss: 0.107139, mae: 0.309131, mean_q: 3.857844
 30741/100000: episode: 383, duration: 0.503s, episode steps: 98, steps per second: 195, episode reward: 198.109, mean reward: 2.022 [1.553, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.481 [-0.444, 10.312], loss: 0.079273, mae: 0.283348, mean_q: 3.844410
 30777/100000: episode: 384, duration: 0.199s, episode steps: 36, steps per second: 180, episode reward: 106.798, mean reward: 2.967 [1.874, 4.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.799, 10.438], loss: 0.079438, mae: 0.284662, mean_q: 3.827756
 30805/100000: episode: 385, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 51.563, mean reward: 1.842 [1.495, 2.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.248, 10.199], loss: 0.084628, mae: 0.299532, mean_q: 3.860231
 30817/100000: episode: 386, duration: 0.058s, episode steps: 12, steps per second: 206, episode reward: 34.644, mean reward: 2.887 [2.109, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.441, 10.465], loss: 0.091569, mae: 0.289321, mean_q: 3.834914
 30829/100000: episode: 387, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 26.263, mean reward: 2.189 [1.719, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.367, 10.441], loss: 0.139769, mae: 0.350530, mean_q: 3.913436
 30852/100000: episode: 388, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 47.895, mean reward: 2.082 [1.455, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.197, 10.190], loss: 0.082952, mae: 0.288881, mean_q: 3.887250
 30892/100000: episode: 389, duration: 0.209s, episode steps: 40, steps per second: 191, episode reward: 97.612, mean reward: 2.440 [1.623, 7.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.040, 10.264], loss: 0.077525, mae: 0.282665, mean_q: 3.918154
 30904/100000: episode: 390, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 28.762, mean reward: 2.397 [2.093, 2.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.035, 10.286], loss: 0.101848, mae: 0.325402, mean_q: 3.903070
 30947/100000: episode: 391, duration: 0.235s, episode steps: 43, steps per second: 183, episode reward: 102.809, mean reward: 2.391 [1.852, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.944, 10.392], loss: 0.088177, mae: 0.285817, mean_q: 3.893813
 30987/100000: episode: 392, duration: 0.226s, episode steps: 40, steps per second: 177, episode reward: 78.528, mean reward: 1.963 [1.536, 2.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.488, 10.196], loss: 0.097109, mae: 0.300617, mean_q: 3.935460
 31030/100000: episode: 393, duration: 0.219s, episode steps: 43, steps per second: 197, episode reward: 104.172, mean reward: 2.423 [1.637, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.617, 10.176], loss: 0.099160, mae: 0.297870, mean_q: 3.859565
 31070/100000: episode: 394, duration: 0.196s, episode steps: 40, steps per second: 204, episode reward: 99.810, mean reward: 2.495 [1.660, 4.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.533, 10.248], loss: 0.096813, mae: 0.304107, mean_q: 3.919746
 31110/100000: episode: 395, duration: 0.218s, episode steps: 40, steps per second: 183, episode reward: 93.445, mean reward: 2.336 [1.820, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.167, 10.336], loss: 0.084963, mae: 0.296458, mean_q: 3.899206
 31208/100000: episode: 396, duration: 0.515s, episode steps: 98, steps per second: 190, episode reward: 196.199, mean reward: 2.002 [1.450, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.470 [-0.424, 10.100], loss: 0.086127, mae: 0.296498, mean_q: 3.916295
 31236/100000: episode: 397, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 82.600, mean reward: 2.950 [1.971, 6.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.588, 10.404], loss: 0.092732, mae: 0.289781, mean_q: 3.881830
 31272/100000: episode: 398, duration: 0.208s, episode steps: 36, steps per second: 173, episode reward: 81.543, mean reward: 2.265 [1.581, 4.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.868, 10.250], loss: 0.092982, mae: 0.311372, mean_q: 3.955427
 31295/100000: episode: 399, duration: 0.150s, episode steps: 23, steps per second: 153, episode reward: 54.768, mean reward: 2.381 [1.616, 3.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.468, 10.171], loss: 0.082828, mae: 0.294855, mean_q: 3.920630
 31331/100000: episode: 400, duration: 0.191s, episode steps: 36, steps per second: 188, episode reward: 95.812, mean reward: 2.661 [1.682, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.518, 10.365], loss: 0.084915, mae: 0.296499, mean_q: 3.910286
 31374/100000: episode: 401, duration: 0.216s, episode steps: 43, steps per second: 199, episode reward: 87.662, mean reward: 2.039 [1.482, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.573, 10.185], loss: 0.106716, mae: 0.309600, mean_q: 3.959460
 31402/100000: episode: 402, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 78.678, mean reward: 2.810 [1.950, 6.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.644, 10.573], loss: 0.123341, mae: 0.309034, mean_q: 3.965429
 31445/100000: episode: 403, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 105.366, mean reward: 2.450 [1.659, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.629, 10.180], loss: 0.087616, mae: 0.296001, mean_q: 3.961562
 31492/100000: episode: 404, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 112.817, mean reward: 2.400 [2.035, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.308, 10.385], loss: 0.128483, mae: 0.326514, mean_q: 4.011351
 31515/100000: episode: 405, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 44.047, mean reward: 1.915 [1.463, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.177, 10.110], loss: 0.116677, mae: 0.326923, mean_q: 4.035558
 31562/100000: episode: 406, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 122.327, mean reward: 2.603 [1.443, 7.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-1.750, 10.111], loss: 0.096226, mae: 0.311749, mean_q: 3.972075
 31598/100000: episode: 407, duration: 0.175s, episode steps: 36, steps per second: 205, episode reward: 95.101, mean reward: 2.642 [1.861, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.478, 10.390], loss: 0.092457, mae: 0.308228, mean_q: 3.955253
 31634/100000: episode: 408, duration: 0.203s, episode steps: 36, steps per second: 177, episode reward: 123.320, mean reward: 3.426 [2.427, 5.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.214, 10.461], loss: 0.120491, mae: 0.332498, mean_q: 4.006536
 31674/100000: episode: 409, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 79.772, mean reward: 1.994 [1.487, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.824, 10.208], loss: 0.103759, mae: 0.314226, mean_q: 3.998077
 31687/100000: episode: 410, duration: 0.088s, episode steps: 13, steps per second: 147, episode reward: 33.687, mean reward: 2.591 [2.039, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.076, 10.355], loss: 0.117540, mae: 0.301656, mean_q: 3.992560
 31785/100000: episode: 411, duration: 0.521s, episode steps: 98, steps per second: 188, episode reward: 178.494, mean reward: 1.821 [1.510, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.519, 10.184], loss: 0.106032, mae: 0.312216, mean_q: 4.007279
 31828/100000: episode: 412, duration: 0.225s, episode steps: 43, steps per second: 191, episode reward: 100.777, mean reward: 2.344 [1.486, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.665, 10.209], loss: 0.104507, mae: 0.311315, mean_q: 4.021640
 31875/100000: episode: 413, duration: 0.256s, episode steps: 47, steps per second: 183, episode reward: 89.779, mean reward: 1.910 [1.533, 2.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.615, 10.185], loss: 0.108116, mae: 0.310026, mean_q: 4.042491
 31898/100000: episode: 414, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 47.021, mean reward: 2.044 [1.579, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.584, 10.245], loss: 0.095051, mae: 0.306468, mean_q: 4.057425
 31921/100000: episode: 415, duration: 0.140s, episode steps: 23, steps per second: 164, episode reward: 55.334, mean reward: 2.406 [1.804, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.294, 10.311], loss: 0.091692, mae: 0.310965, mean_q: 4.028956
 31964/100000: episode: 416, duration: 0.226s, episode steps: 43, steps per second: 190, episode reward: 107.618, mean reward: 2.503 [1.842, 7.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.166, 10.381], loss: 0.132172, mae: 0.329964, mean_q: 4.074986
 32004/100000: episode: 417, duration: 0.219s, episode steps: 40, steps per second: 183, episode reward: 92.741, mean reward: 2.319 [1.644, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.225, 10.258], loss: 0.117677, mae: 0.320273, mean_q: 4.056049
 32102/100000: episode: 418, duration: 0.523s, episode steps: 98, steps per second: 187, episode reward: 183.275, mean reward: 1.870 [1.480, 3.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.873, 10.278], loss: 0.094612, mae: 0.301868, mean_q: 4.036289
 32142/100000: episode: 419, duration: 0.235s, episode steps: 40, steps per second: 170, episode reward: 85.871, mean reward: 2.147 [1.716, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.637, 10.258], loss: 0.107267, mae: 0.320635, mean_q: 4.072301
 32155/100000: episode: 420, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 38.894, mean reward: 2.992 [2.522, 4.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.098, 10.396], loss: 0.086998, mae: 0.286607, mean_q: 4.076816
 32183/100000: episode: 421, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 58.645, mean reward: 2.094 [1.651, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.272, 10.307], loss: 0.107878, mae: 0.325954, mean_q: 4.037877
 32226/100000: episode: 422, duration: 0.250s, episode steps: 43, steps per second: 172, episode reward: 84.063, mean reward: 1.955 [1.531, 2.583], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.718, 10.145], loss: 0.111380, mae: 0.318267, mean_q: 4.060006
 32266/100000: episode: 423, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 88.874, mean reward: 2.222 [1.636, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.269, 10.230], loss: 0.130108, mae: 0.333770, mean_q: 4.067683
 32279/100000: episode: 424, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 31.587, mean reward: 2.430 [2.017, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-1.471, 10.302], loss: 0.136498, mae: 0.343822, mean_q: 4.153929
 32326/100000: episode: 425, duration: 0.231s, episode steps: 47, steps per second: 203, episode reward: 142.461, mean reward: 3.031 [1.807, 4.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.689, 10.499], loss: 0.121792, mae: 0.334656, mean_q: 4.101748
 32349/100000: episode: 426, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 46.300, mean reward: 2.013 [1.461, 2.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.179, 10.196], loss: 0.130005, mae: 0.339613, mean_q: 4.103861
 32389/100000: episode: 427, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 85.119, mean reward: 2.128 [1.508, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.308, 10.100], loss: 0.098799, mae: 0.317361, mean_q: 4.115297
 32425/100000: episode: 428, duration: 0.211s, episode steps: 36, steps per second: 171, episode reward: 81.052, mean reward: 2.251 [1.814, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.227, 10.426], loss: 0.091604, mae: 0.309471, mean_q: 4.102679
 32472/100000: episode: 429, duration: 0.268s, episode steps: 47, steps per second: 175, episode reward: 116.392, mean reward: 2.476 [1.781, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-1.159, 10.358], loss: 0.118152, mae: 0.318816, mean_q: 4.130841
 32484/100000: episode: 430, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 31.774, mean reward: 2.648 [1.879, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.752, 10.362], loss: 0.129660, mae: 0.331180, mean_q: 4.141707
 32527/100000: episode: 431, duration: 0.238s, episode steps: 43, steps per second: 180, episode reward: 94.710, mean reward: 2.203 [1.609, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.604, 10.185], loss: 0.105602, mae: 0.315792, mean_q: 4.117421
 32563/100000: episode: 432, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 94.695, mean reward: 2.630 [1.873, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.992, 10.422], loss: 0.133447, mae: 0.340638, mean_q: 4.143083
 32586/100000: episode: 433, duration: 0.143s, episode steps: 23, steps per second: 161, episode reward: 53.069, mean reward: 2.307 [1.516, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.584, 10.166], loss: 0.154026, mae: 0.354932, mean_q: 4.135657
 32609/100000: episode: 434, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 62.931, mean reward: 2.736 [1.902, 4.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.328, 10.277], loss: 0.097581, mae: 0.314305, mean_q: 4.159518
 32621/100000: episode: 435, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 27.107, mean reward: 2.259 [1.927, 2.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.375], loss: 0.083479, mae: 0.292864, mean_q: 4.181347
 32644/100000: episode: 436, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 58.945, mean reward: 2.563 [1.904, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.185, 10.373], loss: 0.116774, mae: 0.341068, mean_q: 4.149757
 32684/100000: episode: 437, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 113.955, mean reward: 2.849 [2.011, 4.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.271, 10.399], loss: 0.110032, mae: 0.332606, mean_q: 4.099069
 32707/100000: episode: 438, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 66.067, mean reward: 2.872 [1.900, 5.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.089, 10.607], loss: 0.089490, mae: 0.311575, mean_q: 4.157681
 32730/100000: episode: 439, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 49.331, mean reward: 2.145 [1.487, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.961, 10.238], loss: 0.099996, mae: 0.300226, mean_q: 4.071634
 32753/100000: episode: 440, duration: 0.150s, episode steps: 23, steps per second: 153, episode reward: 46.968, mean reward: 2.042 [1.635, 2.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.199, 10.277], loss: 0.106258, mae: 0.318391, mean_q: 4.158913
 32766/100000: episode: 441, duration: 0.085s, episode steps: 13, steps per second: 152, episode reward: 32.370, mean reward: 2.490 [2.197, 3.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.456], loss: 0.122109, mae: 0.340649, mean_q: 4.209905
 32802/100000: episode: 442, duration: 0.212s, episode steps: 36, steps per second: 170, episode reward: 100.425, mean reward: 2.790 [2.291, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.665, 10.405], loss: 0.134383, mae: 0.364088, mean_q: 4.168037
 32830/100000: episode: 443, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 68.960, mean reward: 2.463 [1.987, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.035, 10.472], loss: 0.111379, mae: 0.332941, mean_q: 4.168948
 32928/100000: episode: 444, duration: 0.528s, episode steps: 98, steps per second: 185, episode reward: 186.784, mean reward: 1.906 [1.448, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-0.998, 10.100], loss: 0.127892, mae: 0.349702, mean_q: 4.187519
 32951/100000: episode: 445, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 57.114, mean reward: 2.483 [2.030, 4.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.145, 10.406], loss: 0.131600, mae: 0.337892, mean_q: 4.208981
 32964/100000: episode: 446, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 25.864, mean reward: 1.990 [1.476, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.085, 10.236], loss: 0.139511, mae: 0.330689, mean_q: 4.149656
 32992/100000: episode: 447, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 65.854, mean reward: 2.352 [1.897, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.085, 10.316], loss: 0.106146, mae: 0.332224, mean_q: 4.176738
 33090/100000: episode: 448, duration: 0.481s, episode steps: 98, steps per second: 204, episode reward: 197.848, mean reward: 2.019 [1.455, 3.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.469 [-0.669, 10.100], loss: 0.127543, mae: 0.339933, mean_q: 4.154590
 33126/100000: episode: 449, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 83.782, mean reward: 2.327 [1.617, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.093, 10.208], loss: 0.117963, mae: 0.333399, mean_q: 4.152098
 33139/100000: episode: 450, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 36.209, mean reward: 2.785 [2.226, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.500], loss: 0.089372, mae: 0.313347, mean_q: 4.166314
 33179/100000: episode: 451, duration: 0.207s, episode steps: 40, steps per second: 193, episode reward: 88.507, mean reward: 2.213 [1.493, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.035, 10.190], loss: 0.112167, mae: 0.329816, mean_q: 4.185075
 33202/100000: episode: 452, duration: 0.135s, episode steps: 23, steps per second: 170, episode reward: 47.757, mean reward: 2.076 [1.503, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.229, 10.148], loss: 0.160805, mae: 0.365086, mean_q: 4.196115
 33249/100000: episode: 453, duration: 0.255s, episode steps: 47, steps per second: 184, episode reward: 150.892, mean reward: 3.210 [1.948, 5.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.569, 10.342], loss: 0.120400, mae: 0.355444, mean_q: 4.240034
 33296/100000: episode: 454, duration: 0.265s, episode steps: 47, steps per second: 178, episode reward: 199.979, mean reward: 4.255 [1.764, 10.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.316, 10.589], loss: 0.104410, mae: 0.337157, mean_q: 4.191893
 33343/100000: episode: 455, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 149.066, mean reward: 3.172 [1.797, 6.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-1.073, 10.308], loss: 0.145371, mae: 0.356568, mean_q: 4.237735
 33356/100000: episode: 456, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 36.445, mean reward: 2.803 [2.285, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.183, 10.445], loss: 0.193219, mae: 0.391971, mean_q: 4.338237
 33403/100000: episode: 457, duration: 0.260s, episode steps: 47, steps per second: 181, episode reward: 116.935, mean reward: 2.488 [1.907, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-1.274, 10.304], loss: 0.157093, mae: 0.366833, mean_q: 4.277526
 33426/100000: episode: 458, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 59.295, mean reward: 2.578 [2.199, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.347], loss: 0.144173, mae: 0.347945, mean_q: 4.314225
 33466/100000: episode: 459, duration: 0.219s, episode steps: 40, steps per second: 183, episode reward: 101.483, mean reward: 2.537 [1.531, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-2.088, 10.158], loss: 0.155015, mae: 0.371139, mean_q: 4.290185
 33489/100000: episode: 460, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 50.430, mean reward: 2.193 [1.889, 2.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.150, 10.355], loss: 0.132062, mae: 0.347267, mean_q: 4.289358
 33525/100000: episode: 461, duration: 0.196s, episode steps: 36, steps per second: 183, episode reward: 97.049, mean reward: 2.696 [1.945, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.640, 10.395], loss: 0.158640, mae: 0.371602, mean_q: 4.302148
 33537/100000: episode: 462, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 28.481, mean reward: 2.373 [1.868, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.314], loss: 0.160509, mae: 0.351939, mean_q: 4.329235
[Info] 2-TH LEVEL FOUND: 6.4539995193481445, Considering 10/90 traces
 33560/100000: episode: 463, duration: 4.309s, episode steps: 23, steps per second: 5, episode reward: 48.214, mean reward: 2.096 [1.639, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.230, 10.215], loss: 0.153125, mae: 0.359688, mean_q: 4.281503
 33589/100000: episode: 464, duration: 0.167s, episode steps: 29, steps per second: 173, episode reward: 69.015, mean reward: 2.380 [1.690, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.171, 10.251], loss: 0.117769, mae: 0.338522, mean_q: 4.356757
 33619/100000: episode: 465, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 107.577, mean reward: 3.586 [2.221, 5.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.416, 10.407], loss: 0.163613, mae: 0.390530, mean_q: 4.308494
 33649/100000: episode: 466, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 92.711, mean reward: 3.090 [2.603, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.739, 10.475], loss: 0.187878, mae: 0.414841, mean_q: 4.379613
 33679/100000: episode: 467, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 106.426, mean reward: 3.548 [2.013, 7.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.341, 10.316], loss: 0.142291, mae: 0.353829, mean_q: 4.325747
 33710/100000: episode: 468, duration: 0.173s, episode steps: 31, steps per second: 180, episode reward: 110.465, mean reward: 3.563 [2.393, 5.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.663, 10.433], loss: 0.155178, mae: 0.374294, mean_q: 4.325171
 33741/100000: episode: 469, duration: 0.189s, episode steps: 31, steps per second: 164, episode reward: 135.988, mean reward: 4.387 [2.823, 7.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.793, 10.531], loss: 0.140530, mae: 0.366108, mean_q: 4.394432
 33771/100000: episode: 470, duration: 0.155s, episode steps: 30, steps per second: 193, episode reward: 139.137, mean reward: 4.638 [2.283, 8.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.415], loss: 0.187150, mae: 0.401912, mean_q: 4.367433
 33797/100000: episode: 471, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 72.006, mean reward: 2.769 [2.350, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.786, 10.402], loss: 0.159340, mae: 0.375088, mean_q: 4.428999
 33828/100000: episode: 472, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 89.894, mean reward: 2.900 [1.659, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.035, 10.217], loss: 0.195652, mae: 0.412018, mean_q: 4.495095
 33855/100000: episode: 473, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 100.958, mean reward: 3.739 [2.639, 5.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-1.184, 10.523], loss: 0.147895, mae: 0.376239, mean_q: 4.462815
 33879/100000: episode: 474, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 70.636, mean reward: 2.943 [2.140, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.799, 10.485], loss: 0.187874, mae: 0.401482, mean_q: 4.425153
 33908/100000: episode: 475, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 120.174, mean reward: 4.144 [2.840, 12.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.499], loss: 0.174467, mae: 0.398577, mean_q: 4.594037
 33932/100000: episode: 476, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 107.427, mean reward: 4.476 [2.709, 7.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.769, 10.554], loss: 0.265694, mae: 0.418559, mean_q: 4.484553
 33962/100000: episode: 477, duration: 0.182s, episode steps: 30, steps per second: 165, episode reward: 117.910, mean reward: 3.930 [2.823, 6.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.613], loss: 0.195166, mae: 0.433359, mean_q: 4.552474
 33992/100000: episode: 478, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 85.794, mean reward: 2.860 [1.908, 5.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.035, 10.369], loss: 0.226126, mae: 0.424128, mean_q: 4.502736
 34022/100000: episode: 479, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 138.901, mean reward: 4.630 [2.377, 9.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.863, 10.381], loss: 0.211732, mae: 0.436465, mean_q: 4.586434
 34049/100000: episode: 480, duration: 0.146s, episode steps: 27, steps per second: 184, episode reward: 128.948, mean reward: 4.776 [3.228, 7.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.035, 10.667], loss: 0.197751, mae: 0.403028, mean_q: 4.657497
 34078/100000: episode: 481, duration: 0.150s, episode steps: 29, steps per second: 193, episode reward: 92.101, mean reward: 3.176 [1.759, 4.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.870, 10.377], loss: 0.215551, mae: 0.411951, mean_q: 4.669921
 34108/100000: episode: 482, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 92.468, mean reward: 3.082 [1.833, 5.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-1.603, 10.318], loss: 0.212631, mae: 0.427241, mean_q: 4.644024
 34139/100000: episode: 483, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 92.704, mean reward: 2.990 [2.110, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.434, 10.360], loss: 0.204768, mae: 0.417239, mean_q: 4.707765
 34166/100000: episode: 484, duration: 0.145s, episode steps: 27, steps per second: 187, episode reward: 81.887, mean reward: 3.033 [2.135, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.499, 10.392], loss: 0.228674, mae: 0.429127, mean_q: 4.622416
 34197/100000: episode: 485, duration: 0.164s, episode steps: 31, steps per second: 190, episode reward: 89.836, mean reward: 2.898 [1.981, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.474, 10.374], loss: 0.203424, mae: 0.415383, mean_q: 4.675695
 34223/100000: episode: 486, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 76.798, mean reward: 2.954 [1.886, 5.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.948, 10.298], loss: 0.232752, mae: 0.438061, mean_q: 4.672329
 34250/100000: episode: 487, duration: 0.158s, episode steps: 27, steps per second: 170, episode reward: 113.806, mean reward: 4.215 [2.614, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.609], loss: 0.235139, mae: 0.436333, mean_q: 4.723348
 34277/100000: episode: 488, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 88.138, mean reward: 3.264 [1.955, 5.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.129, 10.344], loss: 0.292001, mae: 0.472249, mean_q: 4.757805
 34308/100000: episode: 489, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 96.904, mean reward: 3.126 [1.759, 4.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.622, 10.185], loss: 0.237129, mae: 0.495697, mean_q: 4.756577
 34339/100000: episode: 490, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 111.446, mean reward: 3.595 [2.423, 7.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.108, 10.497], loss: 0.193689, mae: 0.427061, mean_q: 4.691567
 34368/100000: episode: 491, duration: 0.169s, episode steps: 29, steps per second: 171, episode reward: 66.107, mean reward: 2.280 [1.481, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.807, 10.229], loss: 0.191204, mae: 0.423273, mean_q: 4.888017
 34399/100000: episode: 492, duration: 0.159s, episode steps: 31, steps per second: 194, episode reward: 117.408, mean reward: 3.787 [2.335, 5.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.310, 10.424], loss: 0.202280, mae: 0.422247, mean_q: 4.804292
 34429/100000: episode: 493, duration: 0.178s, episode steps: 30, steps per second: 169, episode reward: 120.694, mean reward: 4.023 [1.630, 7.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.574, 10.364], loss: 0.209069, mae: 0.456800, mean_q: 4.769605
 34460/100000: episode: 494, duration: 0.164s, episode steps: 31, steps per second: 190, episode reward: 95.609, mean reward: 3.084 [2.332, 6.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.790, 10.384], loss: 0.174476, mae: 0.413839, mean_q: 4.779590
 34491/100000: episode: 495, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 129.272, mean reward: 4.170 [2.771, 6.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.557], loss: 0.173471, mae: 0.408940, mean_q: 4.783454
 34511/100000: episode: 496, duration: 0.128s, episode steps: 20, steps per second: 157, episode reward: 56.616, mean reward: 2.831 [2.072, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-1.048, 10.332], loss: 0.269865, mae: 0.484075, mean_q: 4.797287
 34541/100000: episode: 497, duration: 0.169s, episode steps: 30, steps per second: 178, episode reward: 94.790, mean reward: 3.160 [2.332, 4.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.035, 10.347], loss: 0.226435, mae: 0.475590, mean_q: 4.842687
 34570/100000: episode: 498, duration: 0.172s, episode steps: 29, steps per second: 168, episode reward: 75.634, mean reward: 2.608 [1.768, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.953, 10.172], loss: 0.239231, mae: 0.461738, mean_q: 4.895103
 34601/100000: episode: 499, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 117.389, mean reward: 3.787 [2.318, 7.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.067, 10.442], loss: 0.257154, mae: 0.486051, mean_q: 4.886993
 34628/100000: episode: 500, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 82.567, mean reward: 3.058 [2.380, 4.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.035, 10.508], loss: 0.277178, mae: 0.464458, mean_q: 4.887341
 34659/100000: episode: 501, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 82.786, mean reward: 2.671 [1.679, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.709, 10.230], loss: 0.214690, mae: 0.452829, mean_q: 4.834496
 34689/100000: episode: 502, duration: 0.185s, episode steps: 30, steps per second: 163, episode reward: 82.054, mean reward: 2.735 [1.718, 4.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.024, 10.282], loss: 0.210254, mae: 0.443910, mean_q: 4.972783
 34713/100000: episode: 503, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 349.770, mean reward: 14.574 [2.333, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.695], loss: 0.231242, mae: 0.457556, mean_q: 4.890012
 34737/100000: episode: 504, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 73.307, mean reward: 3.054 [2.262, 4.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.093, 10.399], loss: 6.607295, mae: 0.975535, mean_q: 5.044696
 34768/100000: episode: 505, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 116.796, mean reward: 3.768 [2.081, 6.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.722, 10.308], loss: 8.393481, mae: 0.995901, mean_q: 5.168967
 34792/100000: episode: 506, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 78.884, mean reward: 3.287 [2.592, 5.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.304, 10.493], loss: 1.206414, mae: 0.690272, mean_q: 5.134926
 34823/100000: episode: 507, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 99.652, mean reward: 3.215 [2.021, 6.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.636, 10.375], loss: 3.128826, mae: 0.683996, mean_q: 5.007991
 34850/100000: episode: 508, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 83.297, mean reward: 3.085 [2.298, 4.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.318, 10.415], loss: 0.613968, mae: 0.531298, mean_q: 4.951223
 34877/100000: episode: 509, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 83.801, mean reward: 3.104 [2.086, 5.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.322, 10.299], loss: 0.224292, mae: 0.464508, mean_q: 4.987719
 34897/100000: episode: 510, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 49.517, mean reward: 2.476 [1.763, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.182], loss: 0.567600, mae: 0.713971, mean_q: 4.980437
 34928/100000: episode: 511, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 121.690, mean reward: 3.925 [2.360, 10.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.129, 10.423], loss: 0.254121, mae: 0.508991, mean_q: 5.153014
 34955/100000: episode: 512, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 74.019, mean reward: 2.741 [1.838, 3.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.269, 10.295], loss: 0.224193, mae: 0.443956, mean_q: 5.102317
 34986/100000: episode: 513, duration: 0.163s, episode steps: 31, steps per second: 191, episode reward: 65.500, mean reward: 2.113 [1.607, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.232, 10.301], loss: 0.502238, mae: 0.492359, mean_q: 5.151943
 35010/100000: episode: 514, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 82.575, mean reward: 3.441 [2.282, 4.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.832, 10.449], loss: 0.443462, mae: 0.572369, mean_q: 5.180123
 35040/100000: episode: 515, duration: 0.174s, episode steps: 30, steps per second: 173, episode reward: 106.564, mean reward: 3.552 [3.004, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.533], loss: 0.308033, mae: 0.492820, mean_q: 5.098168
 35067/100000: episode: 516, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 61.803, mean reward: 2.289 [1.823, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.311], loss: 0.266670, mae: 0.497466, mean_q: 5.232572
 35094/100000: episode: 517, duration: 0.142s, episode steps: 27, steps per second: 189, episode reward: 82.675, mean reward: 3.062 [1.988, 4.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.298], loss: 0.281246, mae: 0.468053, mean_q: 5.083813
 35124/100000: episode: 518, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 116.313, mean reward: 3.877 [2.615, 5.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-1.077, 10.536], loss: 0.266614, mae: 0.517193, mean_q: 5.218912
 35154/100000: episode: 519, duration: 0.163s, episode steps: 30, steps per second: 183, episode reward: 101.001, mean reward: 3.367 [1.907, 5.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.427, 10.314], loss: 3.547706, mae: 0.769944, mean_q: 5.294121
 35174/100000: episode: 520, duration: 0.126s, episode steps: 20, steps per second: 158, episode reward: 47.729, mean reward: 2.386 [1.850, 2.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.334], loss: 0.323402, mae: 0.590719, mean_q: 5.289327
 35203/100000: episode: 521, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 79.801, mean reward: 2.752 [1.765, 4.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.959, 10.290], loss: 5.480159, mae: 0.899396, mean_q: 5.397562
 35233/100000: episode: 522, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 75.639, mean reward: 2.521 [1.790, 3.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.182, 10.447], loss: 0.319161, mae: 0.505669, mean_q: 5.251447
 35253/100000: episode: 523, duration: 0.117s, episode steps: 20, steps per second: 171, episode reward: 52.603, mean reward: 2.630 [2.270, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.988, 10.452], loss: 0.315475, mae: 0.513736, mean_q: 5.313787
 35280/100000: episode: 524, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 94.796, mean reward: 3.511 [2.350, 6.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.120, 10.506], loss: 0.296002, mae: 0.508918, mean_q: 5.246316
 35307/100000: episode: 525, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 82.879, mean reward: 3.070 [1.867, 4.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.520, 10.299], loss: 5.779252, mae: 0.848118, mean_q: 5.340561
 35337/100000: episode: 526, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 85.323, mean reward: 2.844 [2.191, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.324, 10.382], loss: 0.325866, mae: 0.546466, mean_q: 5.295581
 35363/100000: episode: 527, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 70.370, mean reward: 2.707 [2.039, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.626, 10.344], loss: 0.201783, mae: 0.447997, mean_q: 5.153653
 35383/100000: episode: 528, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 49.045, mean reward: 2.452 [1.804, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.346, 10.290], loss: 0.289031, mae: 0.508597, mean_q: 5.267995
 35410/100000: episode: 529, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 81.569, mean reward: 3.021 [2.384, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.125, 10.432], loss: 0.253244, mae: 0.475630, mean_q: 5.293740
 35430/100000: episode: 530, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 58.793, mean reward: 2.940 [2.217, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.431], loss: 0.441578, mae: 0.668754, mean_q: 5.186860
 35461/100000: episode: 531, duration: 0.185s, episode steps: 31, steps per second: 168, episode reward: 109.682, mean reward: 3.538 [2.470, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.035, 10.416], loss: 6.264802, mae: 0.827457, mean_q: 5.485041
 35491/100000: episode: 532, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 195.844, mean reward: 6.528 [3.743, 12.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-1.369, 10.636], loss: 0.361083, mae: 0.564231, mean_q: 5.249817
 35522/100000: episode: 533, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 112.699, mean reward: 3.635 [2.307, 5.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.691, 10.438], loss: 0.260011, mae: 0.475158, mean_q: 5.192419
 35542/100000: episode: 534, duration: 0.128s, episode steps: 20, steps per second: 157, episode reward: 68.795, mean reward: 3.440 [2.406, 6.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.228, 10.424], loss: 4.788061, mae: 0.751325, mean_q: 5.458868
 35572/100000: episode: 535, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 69.750, mean reward: 2.325 [1.616, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.752, 10.251], loss: 0.683837, mae: 0.675509, mean_q: 5.292074
 35599/100000: episode: 536, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 199.248, mean reward: 7.380 [2.700, 43.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.618, 10.409], loss: 5.823436, mae: 1.076089, mean_q: 5.425324
 35623/100000: episode: 537, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 68.812, mean reward: 2.867 [1.701, 3.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.252], loss: 0.453358, mae: 0.660839, mean_q: 5.461729
 35643/100000: episode: 538, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 46.174, mean reward: 2.309 [1.796, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.368], loss: 0.437222, mae: 0.625516, mean_q: 5.514831
 35670/100000: episode: 539, duration: 0.165s, episode steps: 27, steps per second: 163, episode reward: 107.353, mean reward: 3.976 [1.983, 24.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.319, 10.370], loss: 2.396127, mae: 0.958323, mean_q: 5.553414
 35690/100000: episode: 540, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 64.731, mean reward: 3.237 [2.221, 4.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.136, 10.433], loss: 0.334298, mae: 0.584151, mean_q: 5.307933
 35714/100000: episode: 541, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 88.051, mean reward: 3.669 [2.574, 9.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.373, 10.358], loss: 10.077564, mae: 0.923891, mean_q: 5.515636
 35745/100000: episode: 542, duration: 0.163s, episode steps: 31, steps per second: 190, episode reward: 106.938, mean reward: 3.450 [2.228, 6.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.421, 10.296], loss: 1.612165, mae: 0.824313, mean_q: 5.512724
 35775/100000: episode: 543, duration: 0.174s, episode steps: 30, steps per second: 173, episode reward: 140.247, mean reward: 4.675 [3.377, 8.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.133, 10.564], loss: 6.162013, mae: 0.979605, mean_q: 5.686951
 35802/100000: episode: 544, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 63.243, mean reward: 2.342 [1.793, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.135, 10.288], loss: 4.451142, mae: 0.927843, mean_q: 5.460498
 35829/100000: episode: 545, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 108.144, mean reward: 4.005 [2.535, 7.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.662, 10.479], loss: 0.689757, mae: 0.612126, mean_q: 5.601836
 35859/100000: episode: 546, duration: 0.166s, episode steps: 30, steps per second: 181, episode reward: 87.164, mean reward: 2.905 [1.863, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.291, 10.490], loss: 12.481985, mae: 1.061584, mean_q: 5.811957
 35888/100000: episode: 547, duration: 0.168s, episode steps: 29, steps per second: 173, episode reward: 89.927, mean reward: 3.101 [1.777, 6.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.137, 10.290], loss: 0.721794, mae: 0.725925, mean_q: 5.415605
 35918/100000: episode: 548, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 85.398, mean reward: 2.847 [2.166, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.289, 10.413], loss: 0.466150, mae: 0.604490, mean_q: 5.502810
 35942/100000: episode: 549, duration: 0.146s, episode steps: 24, steps per second: 165, episode reward: 69.058, mean reward: 2.877 [2.381, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.466], loss: 0.433883, mae: 0.581531, mean_q: 5.600255
 35962/100000: episode: 550, duration: 0.112s, episode steps: 20, steps per second: 179, episode reward: 112.083, mean reward: 5.604 [3.048, 17.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.240, 10.490], loss: 0.637199, mae: 0.552938, mean_q: 5.486927
 35992/100000: episode: 551, duration: 0.169s, episode steps: 30, steps per second: 177, episode reward: 112.217, mean reward: 3.741 [2.088, 8.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.634, 10.476], loss: 0.857191, mae: 0.606239, mean_q: 5.620312
 36019/100000: episode: 552, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 106.550, mean reward: 3.946 [2.647, 5.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.984, 10.602], loss: 0.291098, mae: 0.526626, mean_q: 5.421477
[Info] 3-TH LEVEL FOUND: 11.190171241760254, Considering 10/90 traces
 36048/100000: episode: 553, duration: 4.308s, episode steps: 29, steps per second: 7, episode reward: 80.565, mean reward: 2.778 [2.003, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.377], loss: 8.184215, mae: 0.833095, mean_q: 5.789755
 36061/100000: episode: 554, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 57.857, mean reward: 4.451 [2.871, 6.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.671, 10.480], loss: 1.059489, mae: 0.720229, mean_q: 5.688535
 36064/100000: episode: 555, duration: 0.022s, episode steps: 3, steps per second: 134, episode reward: 16.891, mean reward: 5.630 [4.827, 6.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.357 [-0.035, 10.648], loss: 0.200024, mae: 0.425352, mean_q: 5.278938
 36075/100000: episode: 556, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 46.772, mean reward: 4.252 [3.489, 5.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.057, 10.543], loss: 0.494281, mae: 0.648628, mean_q: 5.895058
 36088/100000: episode: 557, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 96.657, mean reward: 7.435 [4.219, 19.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.641, 10.538], loss: 0.357729, mae: 0.559300, mean_q: 5.533662
 36091/100000: episode: 558, duration: 0.022s, episode steps: 3, steps per second: 136, episode reward: 17.667, mean reward: 5.889 [5.463, 6.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.359 [-0.035, 10.619], loss: 2.958128, mae: 0.903823, mean_q: 6.012372
 36094/100000: episode: 559, duration: 0.020s, episode steps: 3, steps per second: 147, episode reward: 14.806, mean reward: 4.935 [4.399, 5.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.618], loss: 0.218818, mae: 0.493585, mean_q: 5.633619
 36109/100000: episode: 560, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 93.344, mean reward: 6.223 [4.012, 11.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.420], loss: 0.304887, mae: 0.553377, mean_q: 5.738495
 36122/100000: episode: 561, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 89.124, mean reward: 6.856 [3.888, 12.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.694, 10.560], loss: 2.217319, mae: 0.738448, mean_q: 5.964307
 36128/100000: episode: 562, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 41.975, mean reward: 6.996 [4.005, 15.369], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.507], loss: 0.313456, mae: 0.552280, mean_q: 5.426338
 36131/100000: episode: 563, duration: 0.024s, episode steps: 3, steps per second: 125, episode reward: 18.941, mean reward: 6.314 [5.487, 6.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.494], loss: 0.908343, mae: 0.696623, mean_q: 5.233406
 36137/100000: episode: 564, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 25.348, mean reward: 4.225 [3.308, 4.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.515], loss: 1.940768, mae: 0.845711, mean_q: 6.175907
 36150/100000: episode: 565, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 59.004, mean reward: 4.539 [3.303, 5.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.488], loss: 0.508741, mae: 0.629208, mean_q: 5.649757
 36165/100000: episode: 566, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 119.837, mean reward: 7.989 [5.476, 22.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.483, 10.671], loss: 0.814900, mae: 0.590362, mean_q: 5.791565
 36180/100000: episode: 567, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 54.567, mean reward: 3.638 [2.755, 5.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.411], loss: 0.483714, mae: 0.584026, mean_q: 5.759474
 36183/100000: episode: 568, duration: 0.022s, episode steps: 3, steps per second: 137, episode reward: 18.229, mean reward: 6.076 [5.459, 6.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.613], loss: 0.320827, mae: 0.587331, mean_q: 5.757689
 36195/100000: episode: 569, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 58.375, mean reward: 4.865 [3.390, 6.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.606], loss: 0.418857, mae: 0.562849, mean_q: 5.753473
 36198/100000: episode: 570, duration: 0.021s, episode steps: 3, steps per second: 140, episode reward: 17.391, mean reward: 5.797 [5.080, 6.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.626], loss: 0.440051, mae: 0.638533, mean_q: 6.011711
 36205/100000: episode: 571, duration: 0.054s, episode steps: 7, steps per second: 129, episode reward: 29.507, mean reward: 4.215 [3.503, 5.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.441], loss: 0.483518, mae: 0.690742, mean_q: 6.115305
 36220/100000: episode: 572, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 65.516, mean reward: 4.368 [3.817, 5.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.541], loss: 1.048097, mae: 0.708465, mean_q: 5.780125
 36235/100000: episode: 573, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 81.880, mean reward: 5.459 [3.781, 9.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.520], loss: 15.159051, mae: 1.377080, mean_q: 6.440010
 36248/100000: episode: 574, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 71.316, mean reward: 5.486 [2.883, 9.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.636], loss: 1.073813, mae: 0.982251, mean_q: 4.903522
 36259/100000: episode: 575, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 64.867, mean reward: 5.897 [3.826, 11.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.682], loss: 3.052485, mae: 1.125349, mean_q: 6.342862
 36271/100000: episode: 576, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 54.524, mean reward: 4.544 [3.447, 7.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.555], loss: 0.466834, mae: 0.667253, mean_q: 5.841568
 36286/100000: episode: 577, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 114.941, mean reward: 7.663 [3.899, 12.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.124, 10.673], loss: 11.040834, mae: 1.250678, mean_q: 6.204117
 36301/100000: episode: 578, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 81.383, mean reward: 5.426 [3.561, 7.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.541], loss: 0.682316, mae: 0.778014, mean_q: 5.781218
 36308/100000: episode: 579, duration: 0.053s, episode steps: 7, steps per second: 131, episode reward: 43.196, mean reward: 6.171 [4.006, 14.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.691], loss: 1.113885, mae: 0.638369, mean_q: 5.405538
 36319/100000: episode: 580, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 40.102, mean reward: 3.646 [3.063, 4.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.573], loss: 0.504262, mae: 0.631425, mean_q: 5.949835
 36334/100000: episode: 581, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 98.374, mean reward: 6.558 [4.143, 12.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.763, 10.585], loss: 1.139317, mae: 0.695349, mean_q: 5.896850
 36349/100000: episode: 582, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 70.571, mean reward: 4.705 [2.572, 6.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.374, 10.383], loss: 0.543329, mae: 0.633864, mean_q: 5.885572
 36363/100000: episode: 583, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 96.894, mean reward: 6.921 [5.257, 10.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.227, 10.614], loss: 0.482385, mae: 0.665484, mean_q: 5.870917
 36366/100000: episode: 584, duration: 0.019s, episode steps: 3, steps per second: 157, episode reward: 15.919, mean reward: 5.306 [4.279, 5.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-1.169, 10.625], loss: 3.081609, mae: 0.863957, mean_q: 6.341236
 36372/100000: episode: 585, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 29.918, mean reward: 4.986 [3.616, 6.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.253, 10.590], loss: 0.506206, mae: 0.697010, mean_q: 6.195404
 36375/100000: episode: 586, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: 17.534, mean reward: 5.845 [4.963, 6.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.576], loss: 0.347796, mae: 0.603061, mean_q: 6.047343
 36381/100000: episode: 587, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 30.258, mean reward: 5.043 [3.854, 6.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.490], loss: 0.723166, mae: 0.708964, mean_q: 5.413016
 36387/100000: episode: 588, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 32.771, mean reward: 5.462 [3.933, 7.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.252, 10.487], loss: 0.639040, mae: 0.723802, mean_q: 6.191545
 36394/100000: episode: 589, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 35.171, mean reward: 5.024 [3.802, 6.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.586], loss: 0.624086, mae: 0.716897, mean_q: 5.888796
 36400/100000: episode: 590, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 29.337, mean reward: 4.890 [4.133, 5.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.609], loss: 0.406560, mae: 0.579386, mean_q: 5.823397
 36415/100000: episode: 591, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 66.002, mean reward: 4.400 [3.392, 5.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.314, 10.557], loss: 0.410376, mae: 0.601007, mean_q: 5.847339
 36428/100000: episode: 592, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 69.795, mean reward: 5.369 [4.437, 7.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.035, 10.633], loss: 0.538873, mae: 0.693414, mean_q: 6.103833
 36431/100000: episode: 593, duration: 0.019s, episode steps: 3, steps per second: 158, episode reward: 17.988, mean reward: 5.996 [4.857, 7.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.357, 10.583], loss: 0.594224, mae: 0.642658, mean_q: 5.929288
 36442/100000: episode: 594, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 87.050, mean reward: 7.914 [4.363, 11.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.231, 10.659], loss: 0.964031, mae: 0.740284, mean_q: 6.208833
 36457/100000: episode: 595, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 64.412, mean reward: 4.294 [2.898, 8.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.662, 10.433], loss: 15.154797, mae: 1.518832, mean_q: 6.491241
 36472/100000: episode: 596, duration: 0.096s, episode steps: 15, steps per second: 157, episode reward: 49.821, mean reward: 3.321 [2.787, 4.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.421], loss: 7.101288, mae: 1.123973, mean_q: 6.217114
 36487/100000: episode: 597, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 55.205, mean reward: 3.680 [2.843, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.454], loss: 0.365205, mae: 0.602121, mean_q: 6.132167
 36500/100000: episode: 598, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 70.272, mean reward: 5.406 [3.886, 7.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.175, 10.669], loss: 1.139346, mae: 0.749884, mean_q: 6.400320
 36515/100000: episode: 599, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 53.344, mean reward: 3.556 [2.907, 6.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.572, 10.637], loss: 0.622580, mae: 0.702275, mean_q: 6.164107
 36518/100000: episode: 600, duration: 0.024s, episode steps: 3, steps per second: 125, episode reward: 23.147, mean reward: 7.716 [5.101, 10.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.509], loss: 0.482049, mae: 0.661941, mean_q: 5.953173
 36529/100000: episode: 601, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 36.254, mean reward: 3.296 [2.258, 5.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.443], loss: 0.614288, mae: 0.680922, mean_q: 6.178878
 36543/100000: episode: 602, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 68.366, mean reward: 4.883 [3.187, 8.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.546, 10.569], loss: 0.444766, mae: 0.625642, mean_q: 6.064326
 36550/100000: episode: 603, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 44.390, mean reward: 6.341 [4.425, 9.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.035, 10.687], loss: 0.612104, mae: 0.666780, mean_q: 6.071477
 36565/100000: episode: 604, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 60.486, mean reward: 4.032 [3.081, 5.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.633, 10.486], loss: 9.500342, mae: 1.060181, mean_q: 6.627424
 36571/100000: episode: 605, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 37.802, mean reward: 6.300 [3.956, 8.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.670], loss: 0.870653, mae: 0.953535, mean_q: 5.363734
 36583/100000: episode: 606, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 49.284, mean reward: 4.107 [3.269, 4.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.106, 10.553], loss: 1.394987, mae: 0.910277, mean_q: 6.508614
 36589/100000: episode: 607, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 42.427, mean reward: 7.071 [6.360, 8.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.035, 10.551], loss: 0.774475, mae: 0.746965, mean_q: 5.465941
 36596/100000: episode: 608, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 29.495, mean reward: 4.214 [3.581, 5.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.398, 10.558], loss: 0.753076, mae: 0.833411, mean_q: 6.379374
 36603/100000: episode: 609, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 44.190, mean reward: 6.313 [4.662, 9.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.636], loss: 0.445688, mae: 0.689177, mean_q: 5.886009
 36618/100000: episode: 610, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 73.438, mean reward: 4.896 [4.051, 5.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.042, 10.597], loss: 1.188963, mae: 0.840044, mean_q: 6.461947
 36633/100000: episode: 611, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 55.840, mean reward: 3.723 [2.880, 5.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.527], loss: 7.011256, mae: 0.906766, mean_q: 6.019866
 36639/100000: episode: 612, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 58.764, mean reward: 9.794 [7.314, 10.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.053, 10.662], loss: 2.225631, mae: 1.278239, mean_q: 6.922853
 36650/100000: episode: 613, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 61.813, mean reward: 5.619 [4.683, 8.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.677], loss: 1.001950, mae: 0.909576, mean_q: 5.851835
 36665/100000: episode: 614, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 58.384, mean reward: 3.892 [2.714, 5.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.313, 10.598], loss: 6.675508, mae: 0.976642, mean_q: 6.438234
 36676/100000: episode: 615, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 60.039, mean reward: 5.458 [3.616, 6.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.164, 10.548], loss: 0.578132, mae: 0.730947, mean_q: 6.455857
 36687/100000: episode: 616, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 200.157, mean reward: 18.196 [6.310, 36.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.577], loss: 0.472361, mae: 0.632652, mean_q: 6.027854
 36693/100000: episode: 617, duration: 0.045s, episode steps: 6, steps per second: 134, episode reward: 23.019, mean reward: 3.836 [3.172, 4.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.333 [-0.035, 10.565], loss: 1.855424, mae: 0.914609, mean_q: 6.725233
 36706/100000: episode: 618, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 66.792, mean reward: 5.138 [2.359, 17.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-1.586, 10.370], loss: 8.372542, mae: 1.053309, mean_q: 6.620445
 36721/100000: episode: 619, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 80.869, mean reward: 5.391 [4.057, 7.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.151, 10.594], loss: 0.960070, mae: 0.711965, mean_q: 6.082291
 36736/100000: episode: 620, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 78.061, mean reward: 5.204 [3.346, 9.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.362, 10.579], loss: 10.263207, mae: 1.315905, mean_q: 6.515749
 36742/100000: episode: 621, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 35.185, mean reward: 5.864 [4.477, 6.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.712, 10.553], loss: 1.202886, mae: 0.952435, mean_q: 6.687255
 36756/100000: episode: 622, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 94.115, mean reward: 6.722 [4.006, 18.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.690], loss: 1.306161, mae: 0.820859, mean_q: 6.298968
 36767/100000: episode: 623, duration: 0.073s, episode steps: 11, steps per second: 152, episode reward: 39.843, mean reward: 3.622 [2.517, 5.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.203, 10.391], loss: 1.029967, mae: 0.856639, mean_q: 6.418952
 36770/100000: episode: 624, duration: 0.018s, episode steps: 3, steps per second: 167, episode reward: 21.174, mean reward: 7.058 [5.925, 8.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.573], loss: 0.520567, mae: 0.585922, mean_q: 6.241652
 36784/100000: episode: 625, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 72.436, mean reward: 5.174 [4.215, 7.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.035, 10.598], loss: 1.911893, mae: 0.863644, mean_q: 6.445189
 36797/100000: episode: 626, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 54.258, mean reward: 4.174 [2.179, 6.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.328], loss: 0.492602, mae: 0.680248, mean_q: 6.299129
 36804/100000: episode: 627, duration: 0.048s, episode steps: 7, steps per second: 145, episode reward: 47.650, mean reward: 6.807 [3.998, 12.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.054, 10.446], loss: 13.562432, mae: 1.156883, mean_q: 6.698380
 36810/100000: episode: 628, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 34.454, mean reward: 5.742 [4.057, 8.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.623], loss: 22.985472, mae: 1.971995, mean_q: 7.992875
 36813/100000: episode: 629, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: 18.497, mean reward: 6.166 [6.076, 6.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.487], loss: 1.347804, mae: 0.880067, mean_q: 6.401933
 36828/100000: episode: 630, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 103.641, mean reward: 6.909 [4.363, 17.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.797, 10.548], loss: 0.772779, mae: 0.847100, mean_q: 6.432879
 36839/100000: episode: 631, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 68.894, mean reward: 6.263 [4.094, 9.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.584], loss: 8.634141, mae: 1.258624, mean_q: 6.997619
 36850/100000: episode: 632, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 39.273, mean reward: 3.570 [2.551, 4.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.258, 10.381], loss: 1.081887, mae: 0.988639, mean_q: 5.878964
 36863/100000: episode: 633, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 53.862, mean reward: 4.143 [2.963, 6.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.165, 10.426], loss: 3.640643, mae: 1.176027, mean_q: 6.990127
 36866/100000: episode: 634, duration: 0.022s, episode steps: 3, steps per second: 139, episode reward: 18.150, mean reward: 6.050 [4.164, 7.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.606], loss: 2.050346, mae: 0.868583, mean_q: 6.309225
 36880/100000: episode: 635, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 110.894, mean reward: 7.921 [5.530, 17.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.606, 10.649], loss: 3.218363, mae: 1.061067, mean_q: 7.006691
 36895/100000: episode: 636, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 82.133, mean reward: 5.476 [2.617, 7.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.307, 10.470], loss: 4.568117, mae: 1.205227, mean_q: 6.782539
[Info] FALSIFICATION!
[Info] Levels: [4.709678, 6.4539995, 11.190171, 13.581636]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.12]
[Info] Error Prob: 0.00012000000000000003

 36902/100000: episode: 637, duration: 4.455s, episode steps: 7, steps per second: 2, episode reward: 144.364, mean reward: 20.623 [4.926, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.852], loss: 1.213219, mae: 0.867016, mean_q: 6.278790
 37002/100000: episode: 638, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 204.141, mean reward: 2.041 [1.446, 6.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.458, 10.160], loss: 7.243864, mae: 1.113353, mean_q: 6.722688
 37102/100000: episode: 639, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 185.291, mean reward: 1.853 [1.455, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.120, 10.098], loss: 6.153009, mae: 1.004217, mean_q: 6.642975
 37202/100000: episode: 640, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 187.267, mean reward: 1.873 [1.463, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.952, 10.156], loss: 3.421630, mae: 0.969760, mean_q: 6.626515
 37302/100000: episode: 641, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 202.208, mean reward: 2.022 [1.443, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.624, 10.544], loss: 5.323478, mae: 1.069756, mean_q: 6.705868
 37402/100000: episode: 642, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 208.628, mean reward: 2.086 [1.460, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.885, 10.098], loss: 2.854806, mae: 0.950369, mean_q: 6.560057
 37502/100000: episode: 643, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 187.556, mean reward: 1.876 [1.433, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.743, 10.160], loss: 2.883240, mae: 0.901797, mean_q: 6.656000
 37602/100000: episode: 644, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.402, mean reward: 1.924 [1.442, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.502, 10.255], loss: 4.135210, mae: 0.965464, mean_q: 6.590700
 37702/100000: episode: 645, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 195.808, mean reward: 1.958 [1.477, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.932, 10.216], loss: 4.131333, mae: 0.958650, mean_q: 6.497713
 37802/100000: episode: 646, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 173.971, mean reward: 1.740 [1.438, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.619, 10.120], loss: 6.386755, mae: 1.117809, mean_q: 6.665232
 37902/100000: episode: 647, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.532, mean reward: 1.865 [1.446, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.004, 10.098], loss: 3.401120, mae: 0.924984, mean_q: 6.574829
 38002/100000: episode: 648, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 181.392, mean reward: 1.814 [1.456, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.633, 10.098], loss: 3.357407, mae: 0.929552, mean_q: 6.511946
 38102/100000: episode: 649, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 214.176, mean reward: 2.142 [1.514, 4.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.035, 10.098], loss: 2.218428, mae: 0.843969, mean_q: 6.491135
 38202/100000: episode: 650, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 204.399, mean reward: 2.044 [1.480, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.410 [-0.932, 10.098], loss: 2.549694, mae: 0.838685, mean_q: 6.384887
 38302/100000: episode: 651, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 198.177, mean reward: 1.982 [1.475, 3.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.561, 10.135], loss: 3.648650, mae: 0.948859, mean_q: 6.426259
 38402/100000: episode: 652, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 191.413, mean reward: 1.914 [1.457, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.623, 10.295], loss: 2.733441, mae: 0.906670, mean_q: 6.399633
 38502/100000: episode: 653, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 193.773, mean reward: 1.938 [1.460, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.256, 10.212], loss: 3.629315, mae: 0.913912, mean_q: 6.398907
 38602/100000: episode: 654, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 237.137, mean reward: 2.371 [1.471, 4.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.962, 10.320], loss: 1.242595, mae: 0.768664, mean_q: 6.214778
 38702/100000: episode: 655, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 193.823, mean reward: 1.938 [1.509, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.879, 10.098], loss: 5.059334, mae: 0.995220, mean_q: 6.330291
 38802/100000: episode: 656, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 198.912, mean reward: 1.989 [1.430, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.431, 10.098], loss: 0.962393, mae: 0.736357, mean_q: 6.066236
 38902/100000: episode: 657, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 187.127, mean reward: 1.871 [1.473, 2.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.487, 10.201], loss: 4.551317, mae: 0.961561, mean_q: 6.107239
 39002/100000: episode: 658, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 184.069, mean reward: 1.841 [1.471, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.839, 10.110], loss: 2.085440, mae: 0.761709, mean_q: 6.049881
 39102/100000: episode: 659, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 197.893, mean reward: 1.979 [1.473, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.124, 10.114], loss: 2.963289, mae: 0.840941, mean_q: 5.990530
 39202/100000: episode: 660, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 192.969, mean reward: 1.930 [1.453, 5.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.611, 10.257], loss: 5.394774, mae: 0.967091, mean_q: 5.942998
 39302/100000: episode: 661, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 208.681, mean reward: 2.087 [1.462, 9.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.259, 10.098], loss: 3.692925, mae: 0.872926, mean_q: 5.964084
 39402/100000: episode: 662, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 187.430, mean reward: 1.874 [1.471, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.709, 10.098], loss: 3.515980, mae: 0.885025, mean_q: 5.848346
 39502/100000: episode: 663, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 364.863, mean reward: 3.649 [1.496, 84.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.413, 10.519], loss: 1.152585, mae: 0.716953, mean_q: 5.699489
 39602/100000: episode: 664, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 177.181, mean reward: 1.772 [1.463, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.351, 10.156], loss: 2.503918, mae: 0.762675, mean_q: 5.730188
 39702/100000: episode: 665, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 178.984, mean reward: 1.790 [1.443, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.057, 10.214], loss: 4.267838, mae: 0.827172, mean_q: 5.814261
 39802/100000: episode: 666, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 189.670, mean reward: 1.897 [1.455, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.861, 10.202], loss: 1.532206, mae: 0.747482, mean_q: 5.712771
 39902/100000: episode: 667, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.001, mean reward: 1.920 [1.447, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.140, 10.098], loss: 0.973339, mae: 0.663675, mean_q: 5.562899
 40002/100000: episode: 668, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 221.387, mean reward: 2.214 [1.464, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.838, 10.098], loss: 1.067701, mae: 0.674723, mean_q: 5.534741
 40102/100000: episode: 669, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 199.962, mean reward: 2.000 [1.489, 3.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.904, 10.161], loss: 2.701968, mae: 0.791362, mean_q: 5.483786
 40202/100000: episode: 670, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 190.836, mean reward: 1.908 [1.454, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.968, 10.098], loss: 4.192042, mae: 0.833425, mean_q: 5.569406
 40302/100000: episode: 671, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 186.628, mean reward: 1.866 [1.455, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.871, 10.098], loss: 2.774411, mae: 0.747084, mean_q: 5.455895
 40402/100000: episode: 672, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 197.167, mean reward: 1.972 [1.455, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.832, 10.158], loss: 3.726095, mae: 0.828028, mean_q: 5.454560
 40502/100000: episode: 673, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 192.473, mean reward: 1.925 [1.464, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.213, 10.103], loss: 5.138929, mae: 0.889219, mean_q: 5.520359
 40602/100000: episode: 674, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 191.038, mean reward: 1.910 [1.451, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.982, 10.128], loss: 4.072784, mae: 0.717299, mean_q: 5.188895
 40702/100000: episode: 675, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 190.811, mean reward: 1.908 [1.464, 3.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.847, 10.207], loss: 0.779470, mae: 0.591388, mean_q: 5.096958
 40802/100000: episode: 676, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 203.058, mean reward: 2.031 [1.495, 4.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.161, 10.246], loss: 3.711093, mae: 0.736198, mean_q: 5.185107
 40902/100000: episode: 677, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 190.413, mean reward: 1.904 [1.492, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.660, 10.202], loss: 1.916287, mae: 0.612119, mean_q: 5.058917
 41002/100000: episode: 678, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 195.175, mean reward: 1.952 [1.484, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.164, 10.349], loss: 0.819571, mae: 0.557303, mean_q: 4.967520
 41102/100000: episode: 679, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 180.879, mean reward: 1.809 [1.434, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.456, 10.098], loss: 1.880724, mae: 0.601456, mean_q: 4.892564
 41202/100000: episode: 680, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 239.427, mean reward: 2.394 [1.462, 5.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.080, 10.098], loss: 0.679426, mae: 0.518887, mean_q: 4.754186
 41302/100000: episode: 681, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 198.682, mean reward: 1.987 [1.458, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.463, 10.098], loss: 3.296875, mae: 0.637762, mean_q: 4.708924
 41402/100000: episode: 682, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 209.769, mean reward: 2.098 [1.502, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.000, 10.098], loss: 6.002099, mae: 0.689581, mean_q: 4.635363
 41502/100000: episode: 683, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 188.790, mean reward: 1.888 [1.432, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.134, 10.098], loss: 3.536902, mae: 0.646869, mean_q: 4.519279
 41602/100000: episode: 684, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 181.125, mean reward: 1.811 [1.463, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.312, 10.252], loss: 1.345607, mae: 0.475541, mean_q: 4.378349
 41702/100000: episode: 685, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 200.680, mean reward: 2.007 [1.480, 5.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.806, 10.301], loss: 0.360754, mae: 0.385494, mean_q: 4.194121
 41802/100000: episode: 686, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 209.163, mean reward: 2.092 [1.475, 5.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.726, 10.098], loss: 1.736054, mae: 0.407587, mean_q: 4.098401
 41902/100000: episode: 687, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 193.377, mean reward: 1.934 [1.458, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.775, 10.098], loss: 0.134602, mae: 0.314929, mean_q: 3.923112
 42002/100000: episode: 688, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 199.344, mean reward: 1.993 [1.525, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.915, 10.246], loss: 0.935265, mae: 0.363732, mean_q: 3.943988
 42102/100000: episode: 689, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 187.484, mean reward: 1.875 [1.484, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.300, 10.098], loss: 0.101249, mae: 0.301260, mean_q: 3.894849
 42202/100000: episode: 690, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 180.242, mean reward: 1.802 [1.459, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.768, 10.116], loss: 0.131159, mae: 0.300727, mean_q: 3.889522
 42302/100000: episode: 691, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 207.304, mean reward: 2.073 [1.468, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.529, 10.098], loss: 0.120692, mae: 0.300897, mean_q: 3.902334
 42402/100000: episode: 692, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 182.052, mean reward: 1.821 [1.453, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.317, 10.126], loss: 0.119820, mae: 0.298647, mean_q: 3.890472
 42502/100000: episode: 693, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 185.001, mean reward: 1.850 [1.491, 4.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.094, 10.098], loss: 0.860410, mae: 0.340954, mean_q: 3.934431
 42602/100000: episode: 694, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 179.274, mean reward: 1.793 [1.459, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.826, 10.098], loss: 0.086979, mae: 0.289444, mean_q: 3.882615
 42702/100000: episode: 695, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 189.744, mean reward: 1.897 [1.454, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.912, 10.123], loss: 0.164631, mae: 0.310249, mean_q: 3.870104
 42802/100000: episode: 696, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 221.810, mean reward: 2.218 [1.465, 5.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.822, 10.098], loss: 0.144681, mae: 0.318121, mean_q: 3.894912
 42902/100000: episode: 697, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 184.602, mean reward: 1.846 [1.471, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.825, 10.098], loss: 0.153513, mae: 0.316082, mean_q: 3.924067
 43002/100000: episode: 698, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 190.973, mean reward: 1.910 [1.463, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.174, 10.139], loss: 0.843700, mae: 0.348769, mean_q: 3.945713
 43102/100000: episode: 699, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 183.786, mean reward: 1.838 [1.478, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.494, 10.098], loss: 0.140477, mae: 0.324923, mean_q: 3.898286
 43202/100000: episode: 700, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 198.810, mean reward: 1.988 [1.473, 4.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.920, 10.098], loss: 0.896972, mae: 0.379077, mean_q: 3.927193
 43302/100000: episode: 701, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 181.998, mean reward: 1.820 [1.484, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.670, 10.267], loss: 0.109340, mae: 0.293852, mean_q: 3.876438
 43402/100000: episode: 702, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 189.110, mean reward: 1.891 [1.459, 2.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.761, 10.098], loss: 1.830058, mae: 0.436180, mean_q: 3.931981
 43502/100000: episode: 703, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 201.244, mean reward: 2.012 [1.488, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.674, 10.381], loss: 1.615077, mae: 0.410566, mean_q: 3.941497
 43602/100000: episode: 704, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 199.861, mean reward: 1.999 [1.514, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.797, 10.176], loss: 0.145993, mae: 0.313017, mean_q: 3.920258
 43702/100000: episode: 705, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 187.991, mean reward: 1.880 [1.431, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.628, 10.098], loss: 0.843016, mae: 0.357279, mean_q: 3.918489
 43802/100000: episode: 706, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 184.188, mean reward: 1.842 [1.440, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.362, 10.098], loss: 0.161375, mae: 0.307826, mean_q: 3.894366
 43902/100000: episode: 707, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 193.574, mean reward: 1.936 [1.461, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.141, 10.397], loss: 0.860278, mae: 0.358943, mean_q: 3.886129
 44002/100000: episode: 708, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 198.712, mean reward: 1.987 [1.548, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.121, 10.098], loss: 0.107964, mae: 0.293327, mean_q: 3.846853
 44102/100000: episode: 709, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.202, mean reward: 1.842 [1.448, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.633, 10.098], loss: 0.108701, mae: 0.300432, mean_q: 3.886046
 44202/100000: episode: 710, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 186.393, mean reward: 1.864 [1.473, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.919, 10.217], loss: 0.107362, mae: 0.292525, mean_q: 3.866830
 44302/100000: episode: 711, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 193.951, mean reward: 1.940 [1.488, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.520, 10.110], loss: 0.105470, mae: 0.293777, mean_q: 3.866966
 44402/100000: episode: 712, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 188.010, mean reward: 1.880 [1.498, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.002, 10.360], loss: 0.096329, mae: 0.297615, mean_q: 3.844182
 44502/100000: episode: 713, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 190.868, mean reward: 1.909 [1.448, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.083, 10.203], loss: 0.862430, mae: 0.346720, mean_q: 3.843947
 44602/100000: episode: 714, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 200.110, mean reward: 2.001 [1.523, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.850, 10.280], loss: 0.081755, mae: 0.282766, mean_q: 3.806717
 44702/100000: episode: 715, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 179.162, mean reward: 1.792 [1.444, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.059, 10.280], loss: 0.084908, mae: 0.286539, mean_q: 3.826965
 44802/100000: episode: 716, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 206.690, mean reward: 2.067 [1.506, 6.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.866, 10.098], loss: 0.081551, mae: 0.277650, mean_q: 3.820921
 44902/100000: episode: 717, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 285.361, mean reward: 2.854 [1.504, 12.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.363, 10.345], loss: 0.092238, mae: 0.286127, mean_q: 3.839293
 45002/100000: episode: 718, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 185.116, mean reward: 1.851 [1.487, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.974, 10.128], loss: 0.114470, mae: 0.293882, mean_q: 3.844341
 45102/100000: episode: 719, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 185.609, mean reward: 1.856 [1.467, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.853, 10.225], loss: 0.103840, mae: 0.304229, mean_q: 3.850549
 45202/100000: episode: 720, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 205.380, mean reward: 2.054 [1.458, 4.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.091, 10.255], loss: 0.107161, mae: 0.295627, mean_q: 3.859134
 45302/100000: episode: 721, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 181.549, mean reward: 1.815 [1.466, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.187, 10.098], loss: 0.123755, mae: 0.303992, mean_q: 3.869570
 45402/100000: episode: 722, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 185.884, mean reward: 1.859 [1.441, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.016, 10.110], loss: 0.135261, mae: 0.311390, mean_q: 3.879098
 45502/100000: episode: 723, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 184.175, mean reward: 1.842 [1.455, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.843, 10.098], loss: 0.101686, mae: 0.297231, mean_q: 3.838525
 45602/100000: episode: 724, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 212.500, mean reward: 2.125 [1.497, 5.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.778, 10.497], loss: 0.097613, mae: 0.290589, mean_q: 3.841935
 45702/100000: episode: 725, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 187.979, mean reward: 1.880 [1.457, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.579, 10.152], loss: 0.101115, mae: 0.295830, mean_q: 3.861822
 45802/100000: episode: 726, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 217.095, mean reward: 2.171 [1.478, 6.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.123, 10.436], loss: 0.118128, mae: 0.303353, mean_q: 3.856637
 45902/100000: episode: 727, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 196.099, mean reward: 1.961 [1.459, 4.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.537, 10.369], loss: 0.104194, mae: 0.292889, mean_q: 3.853527
 46002/100000: episode: 728, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 186.016, mean reward: 1.860 [1.466, 2.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.386, 10.126], loss: 0.129471, mae: 0.302537, mean_q: 3.860479
 46102/100000: episode: 729, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 219.959, mean reward: 2.200 [1.484, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.495, 10.098], loss: 0.120073, mae: 0.299986, mean_q: 3.868417
 46202/100000: episode: 730, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 190.401, mean reward: 1.904 [1.487, 7.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.751, 10.156], loss: 0.102485, mae: 0.301752, mean_q: 3.851411
 46302/100000: episode: 731, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 190.950, mean reward: 1.909 [1.491, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.361, 10.098], loss: 0.092740, mae: 0.293914, mean_q: 3.838414
 46402/100000: episode: 732, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 211.229, mean reward: 2.112 [1.457, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.441, 10.270], loss: 0.102711, mae: 0.293947, mean_q: 3.846521
 46502/100000: episode: 733, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 187.063, mean reward: 1.871 [1.443, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.455, 10.380], loss: 0.103095, mae: 0.293067, mean_q: 3.854779
 46602/100000: episode: 734, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 189.071, mean reward: 1.891 [1.443, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.331, 10.098], loss: 0.114003, mae: 0.300782, mean_q: 3.856217
 46702/100000: episode: 735, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 202.244, mean reward: 2.022 [1.451, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.227, 10.098], loss: 0.115184, mae: 0.307403, mean_q: 3.876831
 46802/100000: episode: 736, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.476, mean reward: 1.955 [1.442, 5.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.243, 10.098], loss: 0.120136, mae: 0.300560, mean_q: 3.846889
[Info] 1-TH LEVEL FOUND: 5.438630104064941, Considering 10/90 traces
 46902/100000: episode: 737, duration: 4.674s, episode steps: 100, steps per second: 21, episode reward: 181.515, mean reward: 1.815 [1.456, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.544, 10.210], loss: 0.118954, mae: 0.302866, mean_q: 3.860810
 46911/100000: episode: 738, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 17.335, mean reward: 1.926 [1.553, 2.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.233], loss: 0.165696, mae: 0.339684, mean_q: 3.905762
 46920/100000: episode: 739, duration: 0.063s, episode steps: 9, steps per second: 143, episode reward: 20.590, mean reward: 2.288 [1.886, 2.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.276], loss: 0.081425, mae: 0.281242, mean_q: 3.816267
 46972/100000: episode: 740, duration: 0.309s, episode steps: 52, steps per second: 168, episode reward: 109.982, mean reward: 2.115 [1.487, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.886 [-0.342, 10.242], loss: 0.117482, mae: 0.302811, mean_q: 3.866361
 46988/100000: episode: 741, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 49.036, mean reward: 3.065 [2.376, 6.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-1.247, 10.431], loss: 0.196711, mae: 0.333775, mean_q: 3.928859
 47040/100000: episode: 742, duration: 0.280s, episode steps: 52, steps per second: 186, episode reward: 104.448, mean reward: 2.009 [1.455, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.322, 10.252], loss: 0.107636, mae: 0.293369, mean_q: 3.855081
 47049/100000: episode: 743, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 19.793, mean reward: 2.199 [1.825, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.384, 10.311], loss: 0.084626, mae: 0.290108, mean_q: 3.859345
 47101/100000: episode: 744, duration: 0.279s, episode steps: 52, steps per second: 186, episode reward: 113.209, mean reward: 2.177 [1.470, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.719, 10.174], loss: 0.106617, mae: 0.299900, mean_q: 3.856435
 47129/100000: episode: 745, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 77.560, mean reward: 2.770 [2.026, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.904, 10.346], loss: 0.136418, mae: 0.295250, mean_q: 3.850017
 47157/100000: episode: 746, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 93.725, mean reward: 3.347 [2.310, 7.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.745, 10.467], loss: 0.114702, mae: 0.311236, mean_q: 3.903889
 47173/100000: episode: 747, duration: 0.105s, episode steps: 16, steps per second: 152, episode reward: 36.300, mean reward: 2.269 [1.944, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.276, 10.423], loss: 0.093799, mae: 0.286708, mean_q: 3.893932
 47190/100000: episode: 748, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 41.388, mean reward: 2.435 [1.933, 4.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.275, 10.100], loss: 0.143922, mae: 0.316381, mean_q: 3.869141
 47206/100000: episode: 749, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 49.779, mean reward: 3.111 [2.436, 4.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.506], loss: 0.160799, mae: 0.340287, mean_q: 3.878543
 47258/100000: episode: 750, duration: 0.293s, episode steps: 52, steps per second: 177, episode reward: 121.799, mean reward: 2.342 [1.485, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.571, 10.117], loss: 0.145026, mae: 0.336158, mean_q: 3.918046
 47275/100000: episode: 751, duration: 0.097s, episode steps: 17, steps per second: 174, episode reward: 56.735, mean reward: 3.337 [2.290, 5.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-1.564, 10.381], loss: 0.107555, mae: 0.293995, mean_q: 3.900826
 47303/100000: episode: 752, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 83.718, mean reward: 2.990 [2.260, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.522, 10.426], loss: 0.114245, mae: 0.308295, mean_q: 3.890634
 47355/100000: episode: 753, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 133.733, mean reward: 2.572 [1.789, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.947, 10.295], loss: 0.113321, mae: 0.321753, mean_q: 3.950186
 47370/100000: episode: 754, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 37.164, mean reward: 2.478 [2.090, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.402], loss: 0.113308, mae: 0.328092, mean_q: 3.975999
 47386/100000: episode: 755, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 36.873, mean reward: 2.305 [1.878, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.230], loss: 0.132885, mae: 0.293764, mean_q: 3.861424
 47403/100000: episode: 756, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 38.857, mean reward: 2.286 [1.703, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.163, 10.252], loss: 0.137666, mae: 0.328873, mean_q: 3.980859
 47418/100000: episode: 757, duration: 0.093s, episode steps: 15, steps per second: 162, episode reward: 40.440, mean reward: 2.696 [2.047, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.413, 10.519], loss: 0.083470, mae: 0.288181, mean_q: 3.923465
 47470/100000: episode: 758, duration: 0.301s, episode steps: 52, steps per second: 173, episode reward: 121.293, mean reward: 2.333 [1.707, 6.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.340, 10.461], loss: 0.116849, mae: 0.313727, mean_q: 3.916046
 47496/100000: episode: 759, duration: 0.137s, episode steps: 26, steps per second: 189, episode reward: 68.805, mean reward: 2.646 [1.916, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.092, 10.100], loss: 0.108100, mae: 0.314419, mean_q: 3.973811
 47547/100000: episode: 760, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 114.142, mean reward: 2.238 [1.605, 6.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.728, 10.269], loss: 0.131685, mae: 0.317938, mean_q: 3.960536
 47599/100000: episode: 761, duration: 0.298s, episode steps: 52, steps per second: 175, episode reward: 124.999, mean reward: 2.404 [1.496, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.693, 10.100], loss: 0.149078, mae: 0.333151, mean_q: 3.970906
 47650/100000: episode: 762, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 93.792, mean reward: 1.839 [1.448, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.642, 10.178], loss: 0.114545, mae: 0.312419, mean_q: 3.964232
 47678/100000: episode: 763, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 110.844, mean reward: 3.959 [2.617, 6.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.354, 10.539], loss: 0.130734, mae: 0.324766, mean_q: 4.004311
 47706/100000: episode: 764, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 94.047, mean reward: 3.359 [2.435, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.505, 10.531], loss: 0.135715, mae: 0.327222, mean_q: 3.994761
 47721/100000: episode: 765, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 36.059, mean reward: 2.404 [1.923, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.042, 10.463], loss: 0.118439, mae: 0.318545, mean_q: 4.042893
 47773/100000: episode: 766, duration: 0.304s, episode steps: 52, steps per second: 171, episode reward: 120.724, mean reward: 2.322 [1.712, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.805, 10.268], loss: 0.114202, mae: 0.323193, mean_q: 3.981867
 47790/100000: episode: 767, duration: 0.112s, episode steps: 17, steps per second: 152, episode reward: 76.801, mean reward: 4.518 [2.377, 12.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.696, 10.100], loss: 0.094480, mae: 0.286664, mean_q: 3.972282
 47816/100000: episode: 768, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 88.204, mean reward: 3.392 [2.426, 6.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.472, 10.100], loss: 0.129480, mae: 0.340965, mean_q: 4.093326
 47831/100000: episode: 769, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 44.385, mean reward: 2.959 [2.099, 5.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.672, 10.430], loss: 0.315346, mae: 0.339575, mean_q: 3.969359
 47883/100000: episode: 770, duration: 0.297s, episode steps: 52, steps per second: 175, episode reward: 154.153, mean reward: 2.964 [1.916, 7.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.125, 10.617], loss: 0.161526, mae: 0.357079, mean_q: 4.075454
 47935/100000: episode: 771, duration: 0.263s, episode steps: 52, steps per second: 197, episode reward: 105.671, mean reward: 2.032 [1.522, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.777, 10.100], loss: 0.175537, mae: 0.347770, mean_q: 4.063840
 47952/100000: episode: 772, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 47.244, mean reward: 2.779 [2.204, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.293, 10.100], loss: 0.131998, mae: 0.357873, mean_q: 4.082671
 47978/100000: episode: 773, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 88.586, mean reward: 3.407 [2.668, 5.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.361, 10.100], loss: 0.279107, mae: 0.391373, mean_q: 4.107879
 47987/100000: episode: 774, duration: 0.061s, episode steps: 9, steps per second: 149, episode reward: 21.962, mean reward: 2.440 [2.057, 2.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.334], loss: 0.169625, mae: 0.387978, mean_q: 4.129229
 48004/100000: episode: 775, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 49.275, mean reward: 2.899 [2.476, 4.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.773, 10.445], loss: 0.131391, mae: 0.358951, mean_q: 4.046856
 48030/100000: episode: 776, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 118.731, mean reward: 4.567 [2.400, 7.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.912, 10.100], loss: 0.174255, mae: 0.377023, mean_q: 4.187354
 48058/100000: episode: 777, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 88.233, mean reward: 3.151 [2.254, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.579, 10.554], loss: 0.124641, mae: 0.328050, mean_q: 4.090250
 48073/100000: episode: 778, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 31.646, mean reward: 2.110 [1.902, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.431, 10.293], loss: 0.142123, mae: 0.365262, mean_q: 4.212554
 48089/100000: episode: 779, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 34.339, mean reward: 2.146 [1.674, 2.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.279], loss: 0.172913, mae: 0.358915, mean_q: 4.114994
 48106/100000: episode: 780, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 36.259, mean reward: 2.133 [1.688, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-1.343, 10.255], loss: 0.194252, mae: 0.398652, mean_q: 4.168804
 48115/100000: episode: 781, duration: 0.065s, episode steps: 9, steps per second: 138, episode reward: 17.644, mean reward: 1.960 [1.538, 2.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.090, 10.203], loss: 0.128525, mae: 0.350265, mean_q: 4.239688
 48132/100000: episode: 782, duration: 0.107s, episode steps: 17, steps per second: 158, episode reward: 50.126, mean reward: 2.949 [2.298, 5.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.151, 10.100], loss: 0.121720, mae: 0.336573, mean_q: 4.065088
 48148/100000: episode: 783, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 40.464, mean reward: 2.529 [2.179, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.082, 10.441], loss: 0.158933, mae: 0.344553, mean_q: 4.177364
 48164/100000: episode: 784, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 42.801, mean reward: 2.675 [2.472, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.153, 10.369], loss: 0.143702, mae: 0.337872, mean_q: 4.101068
 48180/100000: episode: 785, duration: 0.093s, episode steps: 16, steps per second: 173, episode reward: 44.321, mean reward: 2.770 [2.258, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.430, 10.390], loss: 0.139718, mae: 0.363733, mean_q: 4.155384
 48197/100000: episode: 786, duration: 0.087s, episode steps: 17, steps per second: 195, episode reward: 44.698, mean reward: 2.629 [1.939, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.614, 10.100], loss: 0.170235, mae: 0.382600, mean_q: 4.185692
 48214/100000: episode: 787, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 57.691, mean reward: 3.394 [2.622, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.495], loss: 0.111200, mae: 0.312681, mean_q: 4.136406
 48266/100000: episode: 788, duration: 0.286s, episode steps: 52, steps per second: 182, episode reward: 145.844, mean reward: 2.805 [1.783, 6.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.356, 10.313], loss: 0.183570, mae: 0.367308, mean_q: 4.205167
 48282/100000: episode: 789, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 54.204, mean reward: 3.388 [2.243, 4.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.037, 10.582], loss: 0.161123, mae: 0.355321, mean_q: 4.133624
 48298/100000: episode: 790, duration: 0.106s, episode steps: 16, steps per second: 151, episode reward: 46.980, mean reward: 2.936 [2.236, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.933, 10.535], loss: 0.178438, mae: 0.378812, mean_q: 4.244416
 48349/100000: episode: 791, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 111.387, mean reward: 2.184 [1.693, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-1.614, 10.378], loss: 0.180215, mae: 0.370098, mean_q: 4.190845
 48366/100000: episode: 792, duration: 0.105s, episode steps: 17, steps per second: 163, episode reward: 51.813, mean reward: 3.048 [2.432, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.216, 10.100], loss: 0.144066, mae: 0.324697, mean_q: 4.152206
 48392/100000: episode: 793, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 79.151, mean reward: 3.044 [2.237, 7.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.420, 10.100], loss: 0.217688, mae: 0.385297, mean_q: 4.243208
 48407/100000: episode: 794, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 35.826, mean reward: 2.388 [1.888, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.652, 10.310], loss: 0.131763, mae: 0.352695, mean_q: 4.279319
 48422/100000: episode: 795, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 35.771, mean reward: 2.385 [1.963, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.371], loss: 0.205947, mae: 0.425584, mean_q: 4.295073
 48448/100000: episode: 796, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 86.762, mean reward: 3.337 [2.469, 5.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.597, 10.100], loss: 0.191346, mae: 0.397057, mean_q: 4.287604
 48463/100000: episode: 797, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 36.997, mean reward: 2.466 [1.845, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.264, 10.371], loss: 0.220242, mae: 0.415535, mean_q: 4.304717
 48472/100000: episode: 798, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 18.922, mean reward: 2.102 [1.734, 2.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.090, 10.269], loss: 0.124020, mae: 0.336620, mean_q: 4.205353
 48488/100000: episode: 799, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 37.439, mean reward: 2.340 [2.052, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.333], loss: 0.127631, mae: 0.350883, mean_q: 4.179849
 48505/100000: episode: 800, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 40.147, mean reward: 2.362 [1.911, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.315], loss: 0.147208, mae: 0.366037, mean_q: 4.227232
 48533/100000: episode: 801, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 77.457, mean reward: 2.766 [2.012, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.133, 10.409], loss: 0.169606, mae: 0.386461, mean_q: 4.229826
 48585/100000: episode: 802, duration: 0.297s, episode steps: 52, steps per second: 175, episode reward: 114.758, mean reward: 2.207 [1.473, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-1.109, 10.156], loss: 0.162685, mae: 0.386409, mean_q: 4.300117
 48637/100000: episode: 803, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 103.475, mean reward: 1.990 [1.460, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.628, 10.209], loss: 0.185913, mae: 0.389868, mean_q: 4.329574
 48689/100000: episode: 804, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 193.752, mean reward: 3.726 [1.812, 17.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-1.507, 10.507], loss: 0.238522, mae: 0.408588, mean_q: 4.325946
 48741/100000: episode: 805, duration: 0.266s, episode steps: 52, steps per second: 195, episode reward: 117.352, mean reward: 2.257 [1.743, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.646, 10.339], loss: 0.168010, mae: 0.376161, mean_q: 4.344818
 48793/100000: episode: 806, duration: 0.298s, episode steps: 52, steps per second: 175, episode reward: 127.821, mean reward: 2.458 [1.481, 4.176], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.877, 10.121], loss: 0.165401, mae: 0.374921, mean_q: 4.311558
 48819/100000: episode: 807, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 108.923, mean reward: 4.189 [3.014, 8.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.565, 10.100], loss: 0.245545, mae: 0.397473, mean_q: 4.376134
 48845/100000: episode: 808, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 85.810, mean reward: 3.300 [1.905, 5.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.053, 10.100], loss: 0.172280, mae: 0.398174, mean_q: 4.422926
 48860/100000: episode: 809, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 35.144, mean reward: 2.343 [1.944, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.663, 10.371], loss: 0.158699, mae: 0.380083, mean_q: 4.302224
 48876/100000: episode: 810, duration: 0.108s, episode steps: 16, steps per second: 148, episode reward: 35.861, mean reward: 2.241 [1.910, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.321], loss: 0.197827, mae: 0.413444, mean_q: 4.213850
 48928/100000: episode: 811, duration: 0.282s, episode steps: 52, steps per second: 184, episode reward: 212.864, mean reward: 4.094 [2.441, 17.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-1.026, 10.379], loss: 0.257278, mae: 0.415380, mean_q: 4.363242
 48979/100000: episode: 812, duration: 0.275s, episode steps: 51, steps per second: 185, episode reward: 104.267, mean reward: 2.044 [1.542, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.326, 10.273], loss: 0.257583, mae: 0.411509, mean_q: 4.388522
 48994/100000: episode: 813, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 33.709, mean reward: 2.247 [1.864, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.783, 10.352], loss: 0.295967, mae: 0.420919, mean_q: 4.453847
 49046/100000: episode: 814, duration: 0.270s, episode steps: 52, steps per second: 192, episode reward: 121.026, mean reward: 2.327 [1.655, 4.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.823, 10.267], loss: 0.217405, mae: 0.407640, mean_q: 4.391040
 49074/100000: episode: 815, duration: 0.164s, episode steps: 28, steps per second: 170, episode reward: 61.052, mean reward: 2.180 [1.541, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.816, 10.295], loss: 0.171862, mae: 0.379504, mean_q: 4.353597
 49100/100000: episode: 816, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 101.217, mean reward: 3.893 [1.844, 6.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.240, 10.100], loss: 0.187667, mae: 0.398547, mean_q: 4.406289
 49152/100000: episode: 817, duration: 0.291s, episode steps: 52, steps per second: 178, episode reward: 168.057, mean reward: 3.232 [1.650, 6.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.678, 10.290], loss: 0.221026, mae: 0.414017, mean_q: 4.422264
 49161/100000: episode: 818, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 26.477, mean reward: 2.942 [2.358, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.102, 10.386], loss: 0.160540, mae: 0.387798, mean_q: 4.380330
 49189/100000: episode: 819, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 82.616, mean reward: 2.951 [2.046, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.475, 10.380], loss: 0.248144, mae: 0.431135, mean_q: 4.404631
 49206/100000: episode: 820, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 38.280, mean reward: 2.252 [1.855, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.066, 10.386], loss: 0.202751, mae: 0.424495, mean_q: 4.517126
 49258/100000: episode: 821, duration: 0.275s, episode steps: 52, steps per second: 189, episode reward: 105.112, mean reward: 2.021 [1.444, 4.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-0.331, 10.132], loss: 0.224346, mae: 0.426911, mean_q: 4.473035
 49286/100000: episode: 822, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 113.391, mean reward: 4.050 [2.489, 7.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.638, 10.522], loss: 0.271706, mae: 0.405851, mean_q: 4.490510
 49301/100000: episode: 823, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 37.081, mean reward: 2.472 [2.041, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.422], loss: 0.230703, mae: 0.439391, mean_q: 4.487144
 49316/100000: episode: 824, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 43.295, mean reward: 2.886 [2.111, 4.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.547], loss: 0.246082, mae: 0.467638, mean_q: 4.567938
 49344/100000: episode: 825, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 108.229, mean reward: 3.865 [2.635, 6.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.662, 10.422], loss: 0.228047, mae: 0.407982, mean_q: 4.530529
 49395/100000: episode: 826, duration: 0.300s, episode steps: 51, steps per second: 170, episode reward: 117.293, mean reward: 2.300 [1.773, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.752, 10.399], loss: 0.354578, mae: 0.476353, mean_q: 4.602636
[Info] 2-TH LEVEL FOUND: 8.029153823852539, Considering 10/90 traces
 49421/100000: episode: 827, duration: 4.359s, episode steps: 26, steps per second: 6, episode reward: 82.814, mean reward: 3.185 [2.268, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.353, 10.100], loss: 0.202784, mae: 0.423337, mean_q: 4.452815
 49443/100000: episode: 828, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 73.363, mean reward: 3.335 [2.423, 4.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.341, 10.444], loss: 0.418861, mae: 0.454832, mean_q: 4.595525
 49454/100000: episode: 829, duration: 0.077s, episode steps: 11, steps per second: 144, episode reward: 73.410, mean reward: 6.674 [5.017, 11.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.691], loss: 0.456057, mae: 0.479620, mean_q: 4.534123
 49478/100000: episode: 830, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 153.120, mean reward: 6.380 [4.197, 14.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.370, 10.516], loss: 0.198869, mae: 0.409569, mean_q: 4.494587
 49495/100000: episode: 831, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 68.930, mean reward: 4.055 [3.390, 5.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.506], loss: 0.230490, mae: 0.424154, mean_q: 4.566654
 49515/100000: episode: 832, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 90.079, mean reward: 4.504 [3.415, 6.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.571], loss: 0.190112, mae: 0.422406, mean_q: 4.632964
 49539/100000: episode: 833, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 92.113, mean reward: 3.838 [1.948, 9.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.248, 10.321], loss: 0.280203, mae: 0.472513, mean_q: 4.677780
 49556/100000: episode: 834, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 100.761, mean reward: 5.927 [3.662, 10.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.688], loss: 0.217762, mae: 0.431207, mean_q: 4.575298
 49567/100000: episode: 835, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 81.458, mean reward: 7.405 [4.569, 11.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.618], loss: 0.243791, mae: 0.503579, mean_q: 4.824058
 49591/100000: episode: 836, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 80.900, mean reward: 3.371 [2.510, 5.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.288, 10.373], loss: 0.416236, mae: 0.506572, mean_q: 4.769198
 49594/100000: episode: 837, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: 23.136, mean reward: 7.712 [5.539, 9.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.545], loss: 0.367954, mae: 0.474538, mean_q: 4.404171
 49614/100000: episode: 838, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 88.707, mean reward: 4.435 [3.656, 5.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.600, 10.583], loss: 0.398893, mae: 0.502685, mean_q: 4.754303
 49630/100000: episode: 839, duration: 0.083s, episode steps: 16, steps per second: 192, episode reward: 66.210, mean reward: 4.138 [3.072, 5.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.572], loss: 0.764856, mae: 0.614095, mean_q: 4.787681
 49650/100000: episode: 840, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 82.136, mean reward: 4.107 [2.252, 7.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.471], loss: 0.328814, mae: 0.542942, mean_q: 4.861923
 49670/100000: episode: 841, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 55.748, mean reward: 2.787 [1.716, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.360], loss: 0.277062, mae: 0.459408, mean_q: 4.781507
 49694/100000: episode: 842, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 110.691, mean reward: 4.612 [2.718, 10.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.905, 10.625], loss: 0.440564, mae: 0.507480, mean_q: 4.751848
 49716/100000: episode: 843, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 104.598, mean reward: 4.754 [2.844, 6.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.095, 10.596], loss: 0.280594, mae: 0.498559, mean_q: 4.836550
 49730/100000: episode: 844, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 67.635, mean reward: 4.831 [2.838, 11.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.545, 10.100], loss: 0.325067, mae: 0.440161, mean_q: 4.705196
 49752/100000: episode: 845, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 226.410, mean reward: 10.291 [2.825, 65.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.177, 10.687], loss: 0.375333, mae: 0.475852, mean_q: 4.919259
 49774/100000: episode: 846, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 144.099, mean reward: 6.550 [2.540, 21.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.576, 10.570], loss: 3.157350, mae: 0.738583, mean_q: 5.003735
 49790/100000: episode: 847, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 83.725, mean reward: 5.233 [3.773, 13.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.305, 10.453], loss: 0.556402, mae: 0.687259, mean_q: 5.061355
 49814/100000: episode: 848, duration: 0.117s, episode steps: 24, steps per second: 206, episode reward: 84.676, mean reward: 3.528 [2.249, 5.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.336], loss: 0.290259, mae: 0.475218, mean_q: 4.904301
 49825/100000: episode: 849, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 45.943, mean reward: 4.177 [3.600, 5.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.055, 10.567], loss: 0.378862, mae: 0.502491, mean_q: 4.996507
 49845/100000: episode: 850, duration: 0.122s, episode steps: 20, steps per second: 163, episode reward: 65.336, mean reward: 3.267 [2.406, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.430, 10.488], loss: 0.436646, mae: 0.532274, mean_q: 4.820025
 49862/100000: episode: 851, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 48.379, mean reward: 2.846 [1.687, 4.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.389], loss: 0.377154, mae: 0.552800, mean_q: 4.848398
 49873/100000: episode: 852, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 54.611, mean reward: 4.965 [3.967, 7.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.452, 10.491], loss: 0.724210, mae: 0.484179, mean_q: 4.845747
 49890/100000: episode: 853, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 73.417, mean reward: 4.319 [3.024, 5.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.459], loss: 0.579021, mae: 0.525211, mean_q: 5.029785
 49893/100000: episode: 854, duration: 0.020s, episode steps: 3, steps per second: 149, episode reward: 34.964, mean reward: 11.655 [6.631, 21.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.035, 10.564], loss: 0.267344, mae: 0.516702, mean_q: 5.206088
 49913/100000: episode: 855, duration: 0.095s, episode steps: 20, steps per second: 210, episode reward: 115.985, mean reward: 5.799 [4.001, 10.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.163, 10.569], loss: 0.284233, mae: 0.475108, mean_q: 4.890162
 49924/100000: episode: 856, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 51.969, mean reward: 4.724 [2.725, 9.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.128, 10.543], loss: 0.382868, mae: 0.537101, mean_q: 4.757684
 49938/100000: episode: 857, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 55.358, mean reward: 3.954 [2.622, 5.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.390, 10.100], loss: 0.388437, mae: 0.582340, mean_q: 5.154679
 49952/100000: episode: 858, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 58.068, mean reward: 4.148 [2.773, 7.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.492, 10.100], loss: 0.387675, mae: 0.499597, mean_q: 5.038033
 49969/100000: episode: 859, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 55.143, mean reward: 3.244 [2.697, 3.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.497, 10.504], loss: 1.215867, mae: 0.651648, mean_q: 5.081462
 49993/100000: episode: 860, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 92.680, mean reward: 3.862 [2.812, 7.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.918, 10.532], loss: 2.826818, mae: 0.826036, mean_q: 5.101903
[Info] FALSIFICATION!
[Info] Levels: [5.43863, 8.029154, 9.955457]
[Info] Cond. Prob: [0.1, 0.1, 0.08]
[Info] Error Prob: 0.0008000000000000001

 49998/100000: episode: 861, duration: 4.339s, episode steps: 5, steps per second: 1, episode reward: 122.338, mean reward: 24.468 [4.949, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.282, 10.020], loss: 1.871534, mae: 1.036746, mean_q: 4.298767
 50098/100000: episode: 862, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 202.455, mean reward: 2.025 [1.446, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.574, 10.098], loss: 1.949433, mae: 0.694229, mean_q: 5.151579
 50198/100000: episode: 863, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 189.220, mean reward: 1.892 [1.468, 3.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.718, 10.098], loss: 0.799804, mae: 0.615894, mean_q: 5.158274
 50298/100000: episode: 864, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 185.737, mean reward: 1.857 [1.445, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.701, 10.154], loss: 1.328442, mae: 0.654497, mean_q: 5.197222
 50398/100000: episode: 865, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 188.157, mean reward: 1.882 [1.435, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.535, 10.315], loss: 4.063496, mae: 0.778984, mean_q: 5.227857
 50498/100000: episode: 866, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 215.191, mean reward: 2.152 [1.518, 7.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.866, 10.201], loss: 0.615794, mae: 0.578156, mean_q: 5.183838
 50598/100000: episode: 867, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 212.729, mean reward: 2.127 [1.491, 5.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.800, 10.098], loss: 1.077062, mae: 0.621526, mean_q: 5.118383
 50698/100000: episode: 868, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 185.357, mean reward: 1.854 [1.445, 2.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.044, 10.200], loss: 2.086427, mae: 0.667453, mean_q: 5.156335
 50798/100000: episode: 869, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 194.512, mean reward: 1.945 [1.469, 4.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.768, 10.098], loss: 0.611491, mae: 0.561889, mean_q: 5.130561
 50898/100000: episode: 870, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 185.970, mean reward: 1.860 [1.469, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.360, 10.189], loss: 2.445688, mae: 0.676555, mean_q: 5.206160
 50998/100000: episode: 871, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 188.557, mean reward: 1.886 [1.441, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.226, 10.098], loss: 0.416481, mae: 0.542115, mean_q: 5.163225
 51098/100000: episode: 872, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 188.615, mean reward: 1.886 [1.452, 4.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.898, 10.107], loss: 3.203310, mae: 0.667642, mean_q: 5.124161
 51198/100000: episode: 873, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 181.773, mean reward: 1.818 [1.463, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.308, 10.143], loss: 3.403428, mae: 0.757572, mean_q: 5.264357
 51298/100000: episode: 874, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 192.953, mean reward: 1.930 [1.474, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.064, 10.301], loss: 1.260325, mae: 0.683044, mean_q: 5.110275
 51398/100000: episode: 875, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 194.227, mean reward: 1.942 [1.530, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.758, 10.262], loss: 1.108243, mae: 0.614538, mean_q: 5.231541
 51498/100000: episode: 876, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 188.384, mean reward: 1.884 [1.459, 4.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.261, 10.098], loss: 1.061504, mae: 0.645864, mean_q: 5.031718
 51598/100000: episode: 877, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 191.788, mean reward: 1.918 [1.459, 4.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.311, 10.188], loss: 2.270618, mae: 0.735501, mean_q: 5.163154
 51698/100000: episode: 878, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 181.330, mean reward: 1.813 [1.472, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.778, 10.192], loss: 1.923170, mae: 0.605423, mean_q: 5.153634
 51798/100000: episode: 879, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 197.248, mean reward: 1.972 [1.497, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.640, 10.256], loss: 2.030730, mae: 0.689979, mean_q: 5.191382
 51898/100000: episode: 880, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 189.974, mean reward: 1.900 [1.472, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.480, 10.098], loss: 0.473012, mae: 0.532776, mean_q: 5.159751
 51998/100000: episode: 881, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 196.734, mean reward: 1.967 [1.484, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.525, 10.098], loss: 2.005430, mae: 0.642343, mean_q: 5.250690
 52098/100000: episode: 882, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 183.722, mean reward: 1.837 [1.451, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.993, 10.115], loss: 1.852119, mae: 0.580778, mean_q: 5.113165
 52198/100000: episode: 883, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.562, mean reward: 1.856 [1.448, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.974, 10.124], loss: 2.433353, mae: 0.671998, mean_q: 5.018875
 52298/100000: episode: 884, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 185.061, mean reward: 1.851 [1.470, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.025, 10.133], loss: 2.463327, mae: 0.673322, mean_q: 5.082968
 52398/100000: episode: 885, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 207.442, mean reward: 2.074 [1.512, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.576, 10.125], loss: 2.607096, mae: 0.795680, mean_q: 5.014737
 52498/100000: episode: 886, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 182.272, mean reward: 1.823 [1.433, 2.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.440, 10.098], loss: 4.501714, mae: 0.747446, mean_q: 5.093563
 52598/100000: episode: 887, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 207.445, mean reward: 2.074 [1.480, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.448, 10.098], loss: 0.549938, mae: 0.537049, mean_q: 5.007015
 52698/100000: episode: 888, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 187.668, mean reward: 1.877 [1.458, 4.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.728, 10.198], loss: 0.662390, mae: 0.522417, mean_q: 4.932798
 52798/100000: episode: 889, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 190.233, mean reward: 1.902 [1.458, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.119, 10.098], loss: 1.889024, mae: 0.577702, mean_q: 4.930605
 52898/100000: episode: 890, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 181.310, mean reward: 1.813 [1.453, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.472, 10.201], loss: 0.488004, mae: 0.511845, mean_q: 4.873861
 52998/100000: episode: 891, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 191.767, mean reward: 1.918 [1.485, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.229, 10.129], loss: 3.736316, mae: 0.659775, mean_q: 4.940328
 53098/100000: episode: 892, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 190.192, mean reward: 1.902 [1.516, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.092, 10.098], loss: 0.335507, mae: 0.449383, mean_q: 4.835590
 53198/100000: episode: 893, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 192.390, mean reward: 1.924 [1.448, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.385, 10.185], loss: 3.145957, mae: 0.606623, mean_q: 4.784971
 53298/100000: episode: 894, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 183.164, mean reward: 1.832 [1.472, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.520, 10.112], loss: 1.091142, mae: 0.547634, mean_q: 4.703254
 53398/100000: episode: 895, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 197.746, mean reward: 1.977 [1.499, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.317, 10.098], loss: 3.577975, mae: 0.624057, mean_q: 4.748615
 53498/100000: episode: 896, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 201.658, mean reward: 2.017 [1.477, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.769, 10.340], loss: 1.597086, mae: 0.625061, mean_q: 4.706910
 53598/100000: episode: 897, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 187.897, mean reward: 1.879 [1.446, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.515, 10.143], loss: 3.282593, mae: 0.647086, mean_q: 4.733149
 53698/100000: episode: 898, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.569, mean reward: 1.886 [1.449, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.473, 10.098], loss: 4.142868, mae: 0.676040, mean_q: 4.674179
 53798/100000: episode: 899, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 193.792, mean reward: 1.938 [1.492, 3.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.166, 10.116], loss: 0.360714, mae: 0.441325, mean_q: 4.557355
 53898/100000: episode: 900, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 202.731, mean reward: 2.027 [1.520, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.775, 10.356], loss: 2.335688, mae: 0.554399, mean_q: 4.574268
 53998/100000: episode: 901, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 186.627, mean reward: 1.866 [1.447, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.874, 10.098], loss: 2.229481, mae: 0.558297, mean_q: 4.601606
 54098/100000: episode: 902, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 183.735, mean reward: 1.837 [1.463, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.669, 10.236], loss: 0.867054, mae: 0.450599, mean_q: 4.446432
 54198/100000: episode: 903, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 195.533, mean reward: 1.955 [1.451, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.452, 10.404], loss: 0.898881, mae: 0.481609, mean_q: 4.464949
 54298/100000: episode: 904, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 196.065, mean reward: 1.961 [1.505, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.464, 10.098], loss: 0.976694, mae: 0.504448, mean_q: 4.433289
 54398/100000: episode: 905, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 198.020, mean reward: 1.980 [1.483, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.759, 10.098], loss: 1.517603, mae: 0.516638, mean_q: 4.406972
 54498/100000: episode: 906, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: 180.536, mean reward: 1.805 [1.464, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.947, 10.154], loss: 0.241950, mae: 0.366059, mean_q: 4.228091
 54598/100000: episode: 907, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.018, mean reward: 1.830 [1.464, 2.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.800, 10.238], loss: 1.656293, mae: 0.429496, mean_q: 4.172782
[Info] FALSIFICATION!
[Info] Levels: [6.3102093]
[Info] Cond. Prob: [0.0425531914893617]
[Info] Error Prob: 0.0425531914893617

 54692/100000: episode: 908, duration: 2.822s, episode steps: 94, steps per second: 33, episode reward: 302.913, mean reward: 3.222 [1.438, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.379 [-1.165, 9.988], loss: 1.293839, mae: 0.443544, mean_q: 4.168570
 54792/100000: episode: 909, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 189.192, mean reward: 1.892 [1.432, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.552, 10.098], loss: 1.519366, mae: 0.382604, mean_q: 3.961996
 54892/100000: episode: 910, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 180.273, mean reward: 1.803 [1.451, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.412, 10.159], loss: 3.939844, mae: 0.416456, mean_q: 3.956778
 54992/100000: episode: 911, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 195.696, mean reward: 1.957 [1.496, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.822, 10.098], loss: 2.802541, mae: 0.419718, mean_q: 3.921405
 55092/100000: episode: 912, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 203.869, mean reward: 2.039 [1.487, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.832, 10.301], loss: 1.459448, mae: 0.363923, mean_q: 3.858735
 55192/100000: episode: 913, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 203.175, mean reward: 2.032 [1.469, 6.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.088, 10.098], loss: 0.091909, mae: 0.285875, mean_q: 3.821900
 55292/100000: episode: 914, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 196.120, mean reward: 1.961 [1.501, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.460, 10.234], loss: 0.086410, mae: 0.286053, mean_q: 3.807035
 55392/100000: episode: 915, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 199.920, mean reward: 1.999 [1.496, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.553, 10.098], loss: 0.074096, mae: 0.266235, mean_q: 3.804912
 55492/100000: episode: 916, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 200.099, mean reward: 2.001 [1.461, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.037, 10.306], loss: 0.081977, mae: 0.284945, mean_q: 3.821897
 55592/100000: episode: 917, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.241, mean reward: 1.962 [1.448, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.863, 10.098], loss: 1.477098, mae: 0.360483, mean_q: 3.844119
 55692/100000: episode: 918, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 181.193, mean reward: 1.812 [1.436, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.443, 10.124], loss: 1.458001, mae: 0.353381, mean_q: 3.857108
 55792/100000: episode: 919, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.257, mean reward: 1.963 [1.478, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.523, 10.098], loss: 0.084245, mae: 0.284121, mean_q: 3.816572
 55892/100000: episode: 920, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 189.529, mean reward: 1.895 [1.452, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.330, 10.098], loss: 0.077335, mae: 0.271461, mean_q: 3.796053
 55992/100000: episode: 921, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 204.044, mean reward: 2.040 [1.483, 5.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.520, 10.098], loss: 1.464432, mae: 0.340867, mean_q: 3.841403
 56092/100000: episode: 922, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 200.938, mean reward: 2.009 [1.487, 4.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.509, 10.184], loss: 0.091413, mae: 0.293414, mean_q: 3.836345
 56192/100000: episode: 923, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 183.342, mean reward: 1.833 [1.455, 4.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.604, 10.247], loss: 0.076416, mae: 0.274727, mean_q: 3.818536
 56292/100000: episode: 924, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 190.944, mean reward: 1.909 [1.459, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.121, 10.098], loss: 0.073563, mae: 0.270108, mean_q: 3.804424
 56392/100000: episode: 925, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 201.707, mean reward: 2.017 [1.470, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.279, 10.149], loss: 0.076646, mae: 0.277004, mean_q: 3.813934
 56492/100000: episode: 926, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 188.884, mean reward: 1.889 [1.449, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.172, 10.122], loss: 2.800961, mae: 0.413649, mean_q: 3.856438
 56592/100000: episode: 927, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 182.578, mean reward: 1.826 [1.454, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.492, 10.166], loss: 0.092636, mae: 0.295469, mean_q: 3.826712
 56692/100000: episode: 928, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 197.881, mean reward: 1.979 [1.485, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.268, 10.269], loss: 0.073456, mae: 0.273294, mean_q: 3.796847
 56792/100000: episode: 929, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 178.797, mean reward: 1.788 [1.449, 4.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.877, 10.108], loss: 0.072791, mae: 0.273912, mean_q: 3.811601
 56892/100000: episode: 930, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 197.614, mean reward: 1.976 [1.488, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.891, 10.240], loss: 0.080163, mae: 0.281479, mean_q: 3.794483
 56992/100000: episode: 931, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 258.780, mean reward: 2.588 [1.471, 11.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.170, 10.098], loss: 0.076767, mae: 0.281392, mean_q: 3.824557
 57092/100000: episode: 932, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 178.667, mean reward: 1.787 [1.468, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.857, 10.149], loss: 0.086512, mae: 0.282199, mean_q: 3.829451
 57192/100000: episode: 933, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 189.430, mean reward: 1.894 [1.469, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.680, 10.117], loss: 0.085265, mae: 0.279177, mean_q: 3.823956
 57292/100000: episode: 934, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 196.307, mean reward: 1.963 [1.504, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.739, 10.251], loss: 0.076927, mae: 0.279671, mean_q: 3.848827
 57392/100000: episode: 935, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 197.776, mean reward: 1.978 [1.445, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.309, 10.254], loss: 0.072329, mae: 0.268980, mean_q: 3.812455
 57492/100000: episode: 936, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.880, mean reward: 1.869 [1.468, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.584, 10.112], loss: 0.098459, mae: 0.285365, mean_q: 3.832270
 57592/100000: episode: 937, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 209.917, mean reward: 2.099 [1.490, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.678, 10.386], loss: 0.074333, mae: 0.277060, mean_q: 3.829403
 57692/100000: episode: 938, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 183.670, mean reward: 1.837 [1.495, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.831, 10.245], loss: 1.501720, mae: 0.402766, mean_q: 3.868024
 57792/100000: episode: 939, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 184.276, mean reward: 1.843 [1.455, 2.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.551, 10.098], loss: 4.179630, mae: 0.532988, mean_q: 3.922085
 57892/100000: episode: 940, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 184.891, mean reward: 1.849 [1.443, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.822, 10.178], loss: 0.101845, mae: 0.305718, mean_q: 3.846567
 57992/100000: episode: 941, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.308, mean reward: 1.913 [1.447, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.929, 10.186], loss: 1.445534, mae: 0.383867, mean_q: 3.897145
 58092/100000: episode: 942, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 193.297, mean reward: 1.933 [1.457, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.279, 10.209], loss: 1.443383, mae: 0.375278, mean_q: 3.901559
 58192/100000: episode: 943, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 201.014, mean reward: 2.010 [1.458, 4.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.105, 10.141], loss: 3.954600, mae: 0.468143, mean_q: 3.937394
 58292/100000: episode: 944, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 188.138, mean reward: 1.881 [1.438, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.150, 10.098], loss: 1.309445, mae: 0.360438, mean_q: 3.912578
 58392/100000: episode: 945, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 201.292, mean reward: 2.013 [1.456, 5.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.828, 10.098], loss: 1.439446, mae: 0.388810, mean_q: 3.925290
 58492/100000: episode: 946, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 181.659, mean reward: 1.817 [1.462, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.735, 10.235], loss: 2.730431, mae: 0.456011, mean_q: 3.968832
 58592/100000: episode: 947, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 199.348, mean reward: 1.993 [1.459, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.397, 10.198], loss: 2.653169, mae: 0.408597, mean_q: 3.863656
 58692/100000: episode: 948, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 209.250, mean reward: 2.092 [1.437, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.732, 10.098], loss: 0.187161, mae: 0.368821, mean_q: 3.876234
 58792/100000: episode: 949, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 200.307, mean reward: 2.003 [1.471, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.040, 10.448], loss: 1.400301, mae: 0.364602, mean_q: 3.867167
 58892/100000: episode: 950, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 199.302, mean reward: 1.993 [1.490, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.281, 10.204], loss: 1.353678, mae: 0.344411, mean_q: 3.884539
 58992/100000: episode: 951, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 183.178, mean reward: 1.832 [1.461, 2.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.066, 10.102], loss: 2.720728, mae: 0.496075, mean_q: 3.954257
 59092/100000: episode: 952, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 189.597, mean reward: 1.896 [1.443, 5.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.910, 10.098], loss: 0.123709, mae: 0.332768, mean_q: 3.822267
 59192/100000: episode: 953, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.496, mean reward: 1.875 [1.451, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.514, 10.261], loss: 2.650769, mae: 0.435145, mean_q: 3.940980
 59292/100000: episode: 954, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 209.889, mean reward: 2.099 [1.509, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.370, 10.324], loss: 0.125369, mae: 0.333820, mean_q: 3.842562
 59392/100000: episode: 955, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 224.136, mean reward: 2.241 [1.463, 5.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.298, 10.357], loss: 0.118864, mae: 0.319656, mean_q: 3.878794
 59492/100000: episode: 956, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 193.258, mean reward: 1.933 [1.487, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.980, 10.098], loss: 2.684693, mae: 0.433926, mean_q: 3.915838
 59592/100000: episode: 957, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 191.679, mean reward: 1.917 [1.535, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.219, 10.098], loss: 0.128403, mae: 0.334141, mean_q: 3.885470
 59692/100000: episode: 958, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 188.780, mean reward: 1.888 [1.464, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.942, 10.207], loss: 0.118365, mae: 0.325627, mean_q: 3.898521
 59792/100000: episode: 959, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 191.767, mean reward: 1.918 [1.472, 4.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.123, 10.182], loss: 0.105341, mae: 0.313699, mean_q: 3.869849
 59892/100000: episode: 960, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.086, mean reward: 1.931 [1.442, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.422, 10.098], loss: 0.127270, mae: 0.320746, mean_q: 3.885316
 59992/100000: episode: 961, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 200.021, mean reward: 2.000 [1.491, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.080, 10.098], loss: 0.097170, mae: 0.306938, mean_q: 3.861707
 60092/100000: episode: 962, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 185.239, mean reward: 1.852 [1.490, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.697, 10.098], loss: 0.106140, mae: 0.309695, mean_q: 3.872322
 60192/100000: episode: 963, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.543, mean reward: 1.965 [1.451, 5.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.580, 10.098], loss: 0.122850, mae: 0.318708, mean_q: 3.885677
 60292/100000: episode: 964, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 205.269, mean reward: 2.053 [1.502, 4.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.931, 10.301], loss: 0.098283, mae: 0.302582, mean_q: 3.852644
 60392/100000: episode: 965, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 183.093, mean reward: 1.831 [1.444, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.327, 10.159], loss: 0.104269, mae: 0.303419, mean_q: 3.865484
 60492/100000: episode: 966, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 201.788, mean reward: 2.018 [1.531, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.188, 10.193], loss: 0.096627, mae: 0.303818, mean_q: 3.847615
 60592/100000: episode: 967, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 189.457, mean reward: 1.895 [1.448, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.607, 10.098], loss: 0.100064, mae: 0.298988, mean_q: 3.852031
 60692/100000: episode: 968, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 186.901, mean reward: 1.869 [1.440, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.012, 10.130], loss: 0.105865, mae: 0.305581, mean_q: 3.879365
 60792/100000: episode: 969, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 187.381, mean reward: 1.874 [1.446, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.219, 10.098], loss: 0.102323, mae: 0.301389, mean_q: 3.845338
 60892/100000: episode: 970, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 187.895, mean reward: 1.879 [1.451, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.522, 10.248], loss: 0.111674, mae: 0.303660, mean_q: 3.874615
 60992/100000: episode: 971, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 207.143, mean reward: 2.071 [1.448, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.351, 10.427], loss: 0.096132, mae: 0.300225, mean_q: 3.854423
 61092/100000: episode: 972, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 199.827, mean reward: 1.998 [1.464, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.038, 10.102], loss: 0.090209, mae: 0.289765, mean_q: 3.838571
 61192/100000: episode: 973, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 213.439, mean reward: 2.134 [1.497, 5.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.469, 10.098], loss: 0.091266, mae: 0.289884, mean_q: 3.856608
 61292/100000: episode: 974, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 198.667, mean reward: 1.987 [1.509, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.549, 10.228], loss: 0.110186, mae: 0.309377, mean_q: 3.873767
 61392/100000: episode: 975, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 192.317, mean reward: 1.923 [1.433, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.196, 10.098], loss: 0.107415, mae: 0.306772, mean_q: 3.862600
 61492/100000: episode: 976, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.288, mean reward: 1.923 [1.497, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.848, 10.157], loss: 0.107450, mae: 0.303413, mean_q: 3.863591
 61592/100000: episode: 977, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 179.448, mean reward: 1.794 [1.459, 2.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.499, 10.098], loss: 0.093538, mae: 0.296447, mean_q: 3.855339
 61692/100000: episode: 978, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 188.536, mean reward: 1.885 [1.519, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.773, 10.136], loss: 0.089869, mae: 0.291816, mean_q: 3.861678
 61792/100000: episode: 979, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 198.468, mean reward: 1.985 [1.498, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-2.040, 10.098], loss: 0.106523, mae: 0.303321, mean_q: 3.860132
 61892/100000: episode: 980, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.835, mean reward: 1.978 [1.466, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.712, 10.254], loss: 0.093787, mae: 0.301152, mean_q: 3.875355
 61992/100000: episode: 981, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 236.481, mean reward: 2.365 [1.488, 7.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.750, 10.098], loss: 0.107893, mae: 0.314824, mean_q: 3.859482
 62092/100000: episode: 982, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 191.881, mean reward: 1.919 [1.476, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.909, 10.098], loss: 0.102507, mae: 0.308480, mean_q: 3.873516
 62192/100000: episode: 983, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 186.409, mean reward: 1.864 [1.463, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.315, 10.098], loss: 0.098211, mae: 0.314236, mean_q: 3.866944
 62292/100000: episode: 984, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 192.878, mean reward: 1.929 [1.453, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.912, 10.132], loss: 0.098965, mae: 0.315865, mean_q: 3.884959
 62392/100000: episode: 985, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 201.055, mean reward: 2.011 [1.491, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.633, 10.331], loss: 0.094362, mae: 0.305273, mean_q: 3.863340
 62492/100000: episode: 986, duration: 0.583s, episode steps: 100, steps per second: 171, episode reward: 187.602, mean reward: 1.876 [1.478, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.564, 10.098], loss: 0.094806, mae: 0.300062, mean_q: 3.867388
 62592/100000: episode: 987, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 240.173, mean reward: 2.402 [1.451, 4.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.517, 10.098], loss: 0.093905, mae: 0.300438, mean_q: 3.874889
 62692/100000: episode: 988, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 179.491, mean reward: 1.795 [1.440, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.703, 10.188], loss: 0.098142, mae: 0.300539, mean_q: 3.860962
 62792/100000: episode: 989, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 186.595, mean reward: 1.866 [1.459, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.732, 10.152], loss: 0.094419, mae: 0.302851, mean_q: 3.866459
 62892/100000: episode: 990, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 193.772, mean reward: 1.938 [1.466, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.682, 10.098], loss: 0.097512, mae: 0.298270, mean_q: 3.884292
 62992/100000: episode: 991, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 205.610, mean reward: 2.056 [1.452, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.006, 10.098], loss: 0.091070, mae: 0.301250, mean_q: 3.888701
 63092/100000: episode: 992, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 216.768, mean reward: 2.168 [1.441, 4.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.562, 10.098], loss: 0.106101, mae: 0.310545, mean_q: 3.893672
 63192/100000: episode: 993, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 186.894, mean reward: 1.869 [1.434, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.650, 10.098], loss: 0.099847, mae: 0.308351, mean_q: 3.901500
 63292/100000: episode: 994, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 191.871, mean reward: 1.919 [1.456, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.342, 10.098], loss: 0.089926, mae: 0.299049, mean_q: 3.890659
 63392/100000: episode: 995, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 191.505, mean reward: 1.915 [1.471, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.688, 10.185], loss: 0.100078, mae: 0.305631, mean_q: 3.882249
 63492/100000: episode: 996, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 177.375, mean reward: 1.774 [1.455, 2.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.240, 10.098], loss: 0.091070, mae: 0.300671, mean_q: 3.881146
 63592/100000: episode: 997, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 205.694, mean reward: 2.057 [1.470, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.722, 10.426], loss: 0.099821, mae: 0.305217, mean_q: 3.900178
 63692/100000: episode: 998, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 183.381, mean reward: 1.834 [1.461, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.107, 10.147], loss: 0.097771, mae: 0.302217, mean_q: 3.909420
 63792/100000: episode: 999, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.568, mean reward: 1.936 [1.464, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.476, 10.330], loss: 0.095507, mae: 0.302350, mean_q: 3.905683
 63892/100000: episode: 1000, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 213.152, mean reward: 2.132 [1.476, 6.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.098, 10.098], loss: 0.100747, mae: 0.307411, mean_q: 3.898480
 63992/100000: episode: 1001, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 203.090, mean reward: 2.031 [1.483, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.430, 10.252], loss: 0.092026, mae: 0.305684, mean_q: 3.866943
 64092/100000: episode: 1002, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 217.478, mean reward: 2.175 [1.511, 4.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.358, 10.333], loss: 0.100425, mae: 0.303087, mean_q: 3.869082
 64192/100000: episode: 1003, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 205.583, mean reward: 2.056 [1.508, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.286, 10.186], loss: 0.101735, mae: 0.310569, mean_q: 3.911901
 64292/100000: episode: 1004, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 196.317, mean reward: 1.963 [1.482, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.658, 10.454], loss: 0.094534, mae: 0.297852, mean_q: 3.877678
 64392/100000: episode: 1005, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.411, mean reward: 1.954 [1.468, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.789, 10.381], loss: 0.088373, mae: 0.293107, mean_q: 3.902135
 64492/100000: episode: 1006, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.843, mean reward: 1.818 [1.467, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.914, 10.127], loss: 0.082084, mae: 0.289175, mean_q: 3.873916
 64592/100000: episode: 1007, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.889, mean reward: 1.849 [1.457, 3.629], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.629, 10.098], loss: 0.095983, mae: 0.301073, mean_q: 3.893146
[Info] 1-TH LEVEL FOUND: 5.732029914855957, Considering 10/90 traces
 64692/100000: episode: 1008, duration: 4.784s, episode steps: 100, steps per second: 21, episode reward: 197.248, mean reward: 1.972 [1.451, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.182, 10.147], loss: 0.093185, mae: 0.295399, mean_q: 3.899453
 64738/100000: episode: 1009, duration: 0.239s, episode steps: 46, steps per second: 193, episode reward: 96.553, mean reward: 2.099 [1.514, 7.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.622, 10.100], loss: 0.080979, mae: 0.276345, mean_q: 3.885690
 64757/100000: episode: 1010, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 52.084, mean reward: 2.741 [2.040, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.123, 10.100], loss: 0.098203, mae: 0.312722, mean_q: 3.880928
 64781/100000: episode: 1011, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 57.894, mean reward: 2.412 [1.838, 3.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.035, 10.100], loss: 0.102245, mae: 0.329094, mean_q: 3.934594
 64805/100000: episode: 1012, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 71.053, mean reward: 2.961 [2.401, 4.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.355, 10.419], loss: 0.092258, mae: 0.298022, mean_q: 3.906678
 64824/100000: episode: 1013, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 38.302, mean reward: 2.016 [1.525, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.173, 10.100], loss: 0.085136, mae: 0.299384, mean_q: 3.868805
 64840/100000: episode: 1014, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 42.304, mean reward: 2.644 [2.048, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.741, 10.100], loss: 0.099331, mae: 0.288547, mean_q: 3.878669
 64854/100000: episode: 1015, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 31.485, mean reward: 2.249 [1.742, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.199, 10.100], loss: 0.075755, mae: 0.279277, mean_q: 3.889887
 64864/100000: episode: 1016, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 24.913, mean reward: 2.491 [2.227, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.253, 10.100], loss: 0.082453, mae: 0.270954, mean_q: 3.895179
 64876/100000: episode: 1017, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 37.673, mean reward: 3.139 [2.280, 4.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.131, 10.100], loss: 0.088227, mae: 0.290304, mean_q: 3.838135
 64923/100000: episode: 1018, duration: 0.242s, episode steps: 47, steps per second: 194, episode reward: 177.930, mean reward: 3.786 [1.835, 7.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-1.471, 10.100], loss: 0.104723, mae: 0.305240, mean_q: 3.945202
 64939/100000: episode: 1019, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 40.835, mean reward: 2.552 [1.749, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.303, 10.100], loss: 0.122230, mae: 0.316346, mean_q: 3.964804
 64985/100000: episode: 1020, duration: 0.232s, episode steps: 46, steps per second: 198, episode reward: 128.753, mean reward: 2.799 [1.762, 7.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.945, 10.100], loss: 0.120445, mae: 0.330549, mean_q: 3.951903
 64995/100000: episode: 1021, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 26.645, mean reward: 2.665 [1.873, 3.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.341, 10.100], loss: 0.124476, mae: 0.324485, mean_q: 3.984247
 65010/100000: episode: 1022, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 30.051, mean reward: 2.003 [1.644, 2.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.387, 10.100], loss: 0.093891, mae: 0.304229, mean_q: 3.964421
 65056/100000: episode: 1023, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 96.885, mean reward: 2.106 [1.565, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-1.013, 10.100], loss: 0.112222, mae: 0.319838, mean_q: 3.982936
 65080/100000: episode: 1024, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 67.177, mean reward: 2.799 [2.064, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.314, 10.100], loss: 0.103892, mae: 0.318299, mean_q: 3.934893
 65099/100000: episode: 1025, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 48.733, mean reward: 2.565 [2.027, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.569, 10.100], loss: 0.086428, mae: 0.298277, mean_q: 3.981158
 65109/100000: episode: 1026, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 25.547, mean reward: 2.555 [2.110, 2.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.202, 10.100], loss: 0.125466, mae: 0.297112, mean_q: 3.975007
 65123/100000: episode: 1027, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 37.183, mean reward: 2.656 [1.855, 4.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.450, 10.100], loss: 0.120345, mae: 0.320911, mean_q: 4.012286
 65139/100000: episode: 1028, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 52.205, mean reward: 3.263 [2.154, 6.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.578, 10.100], loss: 0.123215, mae: 0.327503, mean_q: 3.980712
 65149/100000: episode: 1029, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 19.255, mean reward: 1.926 [1.591, 2.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.195, 10.100], loss: 0.107310, mae: 0.333061, mean_q: 4.011239
 65164/100000: episode: 1030, duration: 0.080s, episode steps: 15, steps per second: 186, episode reward: 49.850, mean reward: 3.323 [2.614, 4.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.337, 10.100], loss: 0.100289, mae: 0.301730, mean_q: 3.859099
 65183/100000: episode: 1031, duration: 0.115s, episode steps: 19, steps per second: 166, episode reward: 69.350, mean reward: 3.650 [2.418, 5.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.244, 10.100], loss: 0.122830, mae: 0.332765, mean_q: 4.014905
 65207/100000: episode: 1032, duration: 0.143s, episode steps: 24, steps per second: 168, episode reward: 60.968, mean reward: 2.540 [2.003, 3.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.458, 10.355], loss: 0.083045, mae: 0.291075, mean_q: 3.972980
 65223/100000: episode: 1033, duration: 0.081s, episode steps: 16, steps per second: 198, episode reward: 36.283, mean reward: 2.268 [1.928, 2.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.336, 10.100], loss: 0.111633, mae: 0.329977, mean_q: 4.003347
 65270/100000: episode: 1034, duration: 0.239s, episode steps: 47, steps per second: 197, episode reward: 129.140, mean reward: 2.748 [1.603, 4.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.631, 10.100], loss: 0.091999, mae: 0.298549, mean_q: 3.999391
 65289/100000: episode: 1035, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 73.994, mean reward: 3.894 [2.833, 8.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.885, 10.100], loss: 0.107537, mae: 0.334453, mean_q: 4.008408
 65304/100000: episode: 1036, duration: 0.104s, episode steps: 15, steps per second: 145, episode reward: 38.082, mean reward: 2.539 [2.167, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.340, 10.100], loss: 0.109118, mae: 0.319539, mean_q: 4.010158
 65351/100000: episode: 1037, duration: 0.248s, episode steps: 47, steps per second: 190, episode reward: 111.932, mean reward: 2.382 [1.730, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.335, 10.100], loss: 0.115184, mae: 0.325956, mean_q: 4.027805
 65397/100000: episode: 1038, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 121.359, mean reward: 2.638 [1.478, 5.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.818, 10.100], loss: 0.126099, mae: 0.334717, mean_q: 4.071207
 65443/100000: episode: 1039, duration: 0.254s, episode steps: 46, steps per second: 181, episode reward: 100.851, mean reward: 2.192 [1.516, 4.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.507, 10.305], loss: 0.118312, mae: 0.323756, mean_q: 4.048448
 65467/100000: episode: 1040, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 55.257, mean reward: 2.302 [1.931, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.207, 10.100], loss: 0.103893, mae: 0.313615, mean_q: 4.045080
 65486/100000: episode: 1041, duration: 0.095s, episode steps: 19, steps per second: 201, episode reward: 51.817, mean reward: 2.727 [2.014, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.493, 10.100], loss: 0.113168, mae: 0.323525, mean_q: 4.041306
 65533/100000: episode: 1042, duration: 0.229s, episode steps: 47, steps per second: 205, episode reward: 181.103, mean reward: 3.853 [2.178, 7.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.184, 10.100], loss: 0.096871, mae: 0.317051, mean_q: 4.048862
 65545/100000: episode: 1043, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 31.205, mean reward: 2.600 [2.205, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.107, 10.100], loss: 0.081843, mae: 0.316679, mean_q: 4.036256
 65561/100000: episode: 1044, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 35.367, mean reward: 2.210 [1.761, 2.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.046, 10.100], loss: 0.136875, mae: 0.342155, mean_q: 4.084986
 65585/100000: episode: 1045, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 66.835, mean reward: 2.785 [2.403, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.399, 10.100], loss: 0.138425, mae: 0.354527, mean_q: 4.141986
 65631/100000: episode: 1046, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 189.181, mean reward: 4.113 [2.514, 8.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.315, 10.100], loss: 0.115572, mae: 0.328216, mean_q: 4.149536
 65655/100000: episode: 1047, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 72.314, mean reward: 3.013 [2.282, 5.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.374, 10.100], loss: 0.153803, mae: 0.364751, mean_q: 4.150334
 65665/100000: episode: 1048, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 28.222, mean reward: 2.822 [2.536, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.542, 10.100], loss: 0.116586, mae: 0.341100, mean_q: 4.164908
 65675/100000: episode: 1049, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 20.574, mean reward: 2.057 [1.837, 2.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.094, 10.100], loss: 0.157922, mae: 0.363390, mean_q: 4.153101
 65689/100000: episode: 1050, duration: 0.071s, episode steps: 14, steps per second: 196, episode reward: 42.923, mean reward: 3.066 [1.860, 6.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.683, 10.100], loss: 0.107317, mae: 0.321408, mean_q: 4.134294
 65705/100000: episode: 1051, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 42.475, mean reward: 2.655 [2.123, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.666, 10.100], loss: 0.132868, mae: 0.348942, mean_q: 4.166620
 65729/100000: episode: 1052, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 84.856, mean reward: 3.536 [2.088, 9.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.406, 10.100], loss: 0.120492, mae: 0.326543, mean_q: 4.191828
 65775/100000: episode: 1053, duration: 0.278s, episode steps: 46, steps per second: 166, episode reward: 129.508, mean reward: 2.815 [1.836, 4.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.485, 10.100], loss: 0.144749, mae: 0.348527, mean_q: 4.181009
 65787/100000: episode: 1054, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 32.024, mean reward: 2.669 [2.247, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.352, 10.100], loss: 0.166184, mae: 0.395341, mean_q: 4.327858
 65803/100000: episode: 1055, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 34.510, mean reward: 2.157 [1.759, 2.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.180, 10.100], loss: 0.192704, mae: 0.383146, mean_q: 4.260261
 65827/100000: episode: 1056, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 93.876, mean reward: 3.912 [2.650, 5.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.901, 10.542], loss: 0.178484, mae: 0.393026, mean_q: 4.216210
 65851/100000: episode: 1057, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 63.594, mean reward: 2.650 [2.022, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.348, 10.100], loss: 0.177409, mae: 0.412874, mean_q: 4.247218
 65861/100000: episode: 1058, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 23.236, mean reward: 2.324 [1.697, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.135, 10.100], loss: 0.117358, mae: 0.342504, mean_q: 4.191863
 65885/100000: episode: 1059, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 71.165, mean reward: 2.965 [2.418, 4.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.495], loss: 0.186866, mae: 0.387194, mean_q: 4.268305
 65932/100000: episode: 1060, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 114.096, mean reward: 2.428 [1.595, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.471, 10.100], loss: 0.160139, mae: 0.376395, mean_q: 4.243402
 65947/100000: episode: 1061, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 41.559, mean reward: 2.771 [2.150, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.326, 10.100], loss: 0.152189, mae: 0.359643, mean_q: 4.216913
 65994/100000: episode: 1062, duration: 0.241s, episode steps: 47, steps per second: 195, episode reward: 149.215, mean reward: 3.175 [2.037, 6.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.962, 10.100], loss: 0.158431, mae: 0.386161, mean_q: 4.306785
 66008/100000: episode: 1063, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 33.880, mean reward: 2.420 [1.909, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.905, 10.100], loss: 0.140360, mae: 0.343004, mean_q: 4.220133
 66054/100000: episode: 1064, duration: 0.249s, episode steps: 46, steps per second: 184, episode reward: 106.265, mean reward: 2.310 [1.555, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.496, 10.208], loss: 0.177098, mae: 0.377621, mean_q: 4.304260
 66069/100000: episode: 1065, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 44.016, mean reward: 2.934 [2.567, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.343, 10.100], loss: 0.133011, mae: 0.348763, mean_q: 4.264617
 66088/100000: episode: 1066, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 53.712, mean reward: 2.827 [2.098, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.388, 10.100], loss: 0.154536, mae: 0.373903, mean_q: 4.284473
 66135/100000: episode: 1067, duration: 0.278s, episode steps: 47, steps per second: 169, episode reward: 153.253, mean reward: 3.261 [1.459, 6.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.653, 10.100], loss: 0.152014, mae: 0.370900, mean_q: 4.235869
 66159/100000: episode: 1068, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 79.754, mean reward: 3.323 [2.215, 6.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.125, 10.384], loss: 0.189323, mae: 0.418891, mean_q: 4.381793
 66206/100000: episode: 1069, duration: 0.234s, episode steps: 47, steps per second: 201, episode reward: 148.835, mean reward: 3.167 [2.123, 6.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.537, 10.100], loss: 0.175018, mae: 0.391618, mean_q: 4.393890
 66230/100000: episode: 1070, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 66.251, mean reward: 2.760 [1.866, 8.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.184, 10.100], loss: 0.208543, mae: 0.402440, mean_q: 4.216519
 66244/100000: episode: 1071, duration: 0.088s, episode steps: 14, steps per second: 158, episode reward: 29.449, mean reward: 2.104 [1.556, 2.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.149, 10.100], loss: 0.199780, mae: 0.419290, mean_q: 4.438131
 66290/100000: episode: 1072, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 127.806, mean reward: 2.778 [2.248, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.197, 10.100], loss: 0.192446, mae: 0.413000, mean_q: 4.467683
 66309/100000: episode: 1073, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 64.782, mean reward: 3.410 [2.642, 4.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.455, 10.100], loss: 0.183374, mae: 0.401265, mean_q: 4.295478
 66319/100000: episode: 1074, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 23.997, mean reward: 2.400 [1.948, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.394, 10.100], loss: 0.146808, mae: 0.363924, mean_q: 4.434965
 66334/100000: episode: 1075, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 34.317, mean reward: 2.288 [1.918, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.570, 10.100], loss: 0.162262, mae: 0.372821, mean_q: 4.283316
 66358/100000: episode: 1076, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 71.514, mean reward: 2.980 [2.331, 4.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.901, 10.100], loss: 0.204928, mae: 0.417357, mean_q: 4.417843
 66372/100000: episode: 1077, duration: 0.075s, episode steps: 14, steps per second: 188, episode reward: 35.520, mean reward: 2.537 [2.063, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.335, 10.100], loss: 0.122773, mae: 0.351514, mean_q: 4.344932
 66391/100000: episode: 1078, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 45.483, mean reward: 2.394 [1.813, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.131, 10.100], loss: 0.153747, mae: 0.380508, mean_q: 4.404250
 66438/100000: episode: 1079, duration: 0.232s, episode steps: 47, steps per second: 203, episode reward: 235.852, mean reward: 5.018 [2.399, 14.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.618, 10.100], loss: 0.233483, mae: 0.414698, mean_q: 4.441308
 66462/100000: episode: 1080, duration: 0.137s, episode steps: 24, steps per second: 175, episode reward: 75.582, mean reward: 3.149 [2.406, 9.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.316, 10.100], loss: 0.214352, mae: 0.425193, mean_q: 4.490614
 66486/100000: episode: 1081, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 64.282, mean reward: 2.678 [1.871, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.453, 10.100], loss: 0.169714, mae: 0.394308, mean_q: 4.525504
 66502/100000: episode: 1082, duration: 0.093s, episode steps: 16, steps per second: 171, episode reward: 41.304, mean reward: 2.581 [2.090, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.310, 10.100], loss: 0.183190, mae: 0.374922, mean_q: 4.446657
 66518/100000: episode: 1083, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 37.141, mean reward: 2.321 [1.841, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.985, 10.100], loss: 0.209009, mae: 0.400434, mean_q: 4.419482
 66530/100000: episode: 1084, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 28.270, mean reward: 2.356 [1.635, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.559, 10.100], loss: 0.179794, mae: 0.403485, mean_q: 4.440475
 66549/100000: episode: 1085, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 57.344, mean reward: 3.018 [2.257, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.132, 10.100], loss: 0.140049, mae: 0.374220, mean_q: 4.419058
 66561/100000: episode: 1086, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 34.315, mean reward: 2.860 [2.258, 3.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.252, 10.100], loss: 0.217030, mae: 0.426016, mean_q: 4.501850
 66585/100000: episode: 1087, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 74.571, mean reward: 3.107 [2.365, 4.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.541], loss: 0.222181, mae: 0.446560, mean_q: 4.537481
 66604/100000: episode: 1088, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 47.293, mean reward: 2.489 [1.777, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.744, 10.100], loss: 0.230033, mae: 0.440682, mean_q: 4.606031
 66628/100000: episode: 1089, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 80.188, mean reward: 3.341 [2.385, 4.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.452, 10.100], loss: 0.165758, mae: 0.391536, mean_q: 4.520573
 66638/100000: episode: 1090, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 22.770, mean reward: 2.277 [1.950, 2.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.938, 10.100], loss: 0.158141, mae: 0.390040, mean_q: 4.415767
 66652/100000: episode: 1091, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 27.158, mean reward: 1.940 [1.676, 2.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.193, 10.100], loss: 0.171780, mae: 0.384592, mean_q: 4.558770
 66676/100000: episode: 1092, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 88.295, mean reward: 3.679 [2.215, 6.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.452, 10.100], loss: 0.142609, mae: 0.378304, mean_q: 4.506475
 66688/100000: episode: 1093, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 34.133, mean reward: 2.844 [2.428, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.250, 10.100], loss: 0.279651, mae: 0.420690, mean_q: 4.593105
 66712/100000: episode: 1094, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 52.387, mean reward: 2.183 [1.831, 2.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.244, 10.100], loss: 0.197089, mae: 0.412395, mean_q: 4.568890
 66736/100000: episode: 1095, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 67.506, mean reward: 2.813 [2.286, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.252, 10.100], loss: 0.210977, mae: 0.428545, mean_q: 4.506720
 66752/100000: episode: 1096, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 41.929, mean reward: 2.621 [2.073, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.141, 10.100], loss: 0.193565, mae: 0.413098, mean_q: 4.607178
 66768/100000: episode: 1097, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 33.261, mean reward: 2.079 [1.544, 2.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.355, 10.100], loss: 0.185020, mae: 0.415578, mean_q: 4.539804
[Info] 2-TH LEVEL FOUND: 8.437468528747559, Considering 10/90 traces
 66787/100000: episode: 1098, duration: 4.331s, episode steps: 19, steps per second: 4, episode reward: 54.182, mean reward: 2.852 [2.068, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.278, 10.100], loss: 0.428863, mae: 0.491839, mean_q: 4.656788
 66827/100000: episode: 1099, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 133.070, mean reward: 3.327 [2.000, 11.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.755, 10.100], loss: 0.228691, mae: 0.418354, mean_q: 4.539250
 66866/100000: episode: 1100, duration: 0.221s, episode steps: 39, steps per second: 177, episode reward: 189.677, mean reward: 4.864 [2.943, 10.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.449, 10.100], loss: 0.257624, mae: 0.431619, mean_q: 4.618436
 66905/100000: episode: 1101, duration: 0.197s, episode steps: 39, steps per second: 198, episode reward: 192.949, mean reward: 4.947 [2.530, 11.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-1.344, 10.100], loss: 0.248199, mae: 0.449294, mean_q: 4.689456
 66928/100000: episode: 1102, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 192.393, mean reward: 8.365 [4.779, 18.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.545, 10.100], loss: 0.214801, mae: 0.409303, mean_q: 4.662140
 66967/100000: episode: 1103, duration: 0.223s, episode steps: 39, steps per second: 175, episode reward: 112.455, mean reward: 2.883 [1.844, 7.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.407, 10.100], loss: 0.282116, mae: 0.450746, mean_q: 4.698079
 67006/100000: episode: 1104, duration: 0.225s, episode steps: 39, steps per second: 173, episode reward: 145.838, mean reward: 3.739 [1.565, 10.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.888, 10.100], loss: 0.331934, mae: 0.445513, mean_q: 4.721772
 67029/100000: episode: 1105, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 88.124, mean reward: 3.831 [2.555, 5.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.177, 10.100], loss: 0.411922, mae: 0.536539, mean_q: 4.790778
 67068/100000: episode: 1106, duration: 0.205s, episode steps: 39, steps per second: 190, episode reward: 152.518, mean reward: 3.911 [2.307, 7.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.884, 10.100], loss: 0.383997, mae: 0.521432, mean_q: 4.808286
 67089/100000: episode: 1107, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 105.854, mean reward: 5.041 [2.780, 8.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.329, 10.100], loss: 0.188075, mae: 0.426798, mean_q: 4.714751
 67129/100000: episode: 1108, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 180.227, mean reward: 4.506 [2.180, 16.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.050, 10.100], loss: 0.398080, mae: 0.521595, mean_q: 4.860896
 67142/100000: episode: 1109, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 53.027, mean reward: 4.079 [3.265, 5.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.690, 10.100], loss: 0.297776, mae: 0.488794, mean_q: 4.949492
 67181/100000: episode: 1110, duration: 0.222s, episode steps: 39, steps per second: 176, episode reward: 92.978, mean reward: 2.384 [1.565, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.725, 10.197], loss: 0.484133, mae: 0.563308, mean_q: 4.960831
 67201/100000: episode: 1111, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 54.807, mean reward: 2.740 [2.018, 5.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.316, 10.100], loss: 0.398499, mae: 0.537045, mean_q: 4.941686
 67240/100000: episode: 1112, duration: 0.203s, episode steps: 39, steps per second: 192, episode reward: 153.515, mean reward: 3.936 [2.590, 8.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.384, 10.100], loss: 0.371734, mae: 0.511611, mean_q: 4.930759
 67279/100000: episode: 1113, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 115.359, mean reward: 2.958 [1.491, 5.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-1.305, 10.100], loss: 0.247865, mae: 0.471580, mean_q: 4.895143
 67302/100000: episode: 1114, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 83.050, mean reward: 3.611 [2.332, 6.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.226, 10.100], loss: 0.447622, mae: 0.533992, mean_q: 4.907609
 67315/100000: episode: 1115, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 63.791, mean reward: 4.907 [2.851, 7.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.066, 10.100], loss: 0.522653, mae: 0.529875, mean_q: 4.995577
 67335/100000: episode: 1116, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 87.465, mean reward: 4.373 [3.072, 7.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.353, 10.100], loss: 0.337790, mae: 0.509964, mean_q: 4.940012
 67374/100000: episode: 1117, duration: 0.223s, episode steps: 39, steps per second: 175, episode reward: 127.332, mean reward: 3.265 [2.283, 7.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.516, 10.100], loss: 0.324245, mae: 0.484640, mean_q: 4.978633
 67413/100000: episode: 1118, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 118.499, mean reward: 3.038 [1.679, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.289, 10.100], loss: 0.327017, mae: 0.516050, mean_q: 4.963142
 67452/100000: episode: 1119, duration: 0.214s, episode steps: 39, steps per second: 182, episode reward: 89.292, mean reward: 2.290 [1.705, 4.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.382, 10.100], loss: 0.344686, mae: 0.520355, mean_q: 5.015878
 67492/100000: episode: 1120, duration: 0.201s, episode steps: 40, steps per second: 199, episode reward: 156.418, mean reward: 3.910 [1.640, 7.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.565, 10.100], loss: 0.312647, mae: 0.510520, mean_q: 5.004382
 67513/100000: episode: 1121, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 104.191, mean reward: 4.961 [3.161, 8.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.563, 10.100], loss: 0.510067, mae: 0.534120, mean_q: 5.094620
 67552/100000: episode: 1122, duration: 0.238s, episode steps: 39, steps per second: 164, episode reward: 104.926, mean reward: 2.690 [1.975, 4.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.010, 10.100], loss: 0.404716, mae: 0.560724, mean_q: 5.147914
 67592/100000: episode: 1123, duration: 0.224s, episode steps: 40, steps per second: 179, episode reward: 164.832, mean reward: 4.121 [2.198, 9.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.499, 10.100], loss: 0.404105, mae: 0.538796, mean_q: 5.116591
 67612/100000: episode: 1124, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 84.730, mean reward: 4.237 [2.969, 6.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.424, 10.100], loss: 0.333154, mae: 0.532516, mean_q: 5.167970
 67651/100000: episode: 1125, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 125.600, mean reward: 3.221 [2.100, 7.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.863, 10.100], loss: 0.330314, mae: 0.513927, mean_q: 5.069531
 67671/100000: episode: 1126, duration: 0.101s, episode steps: 20, steps per second: 198, episode reward: 74.255, mean reward: 3.713 [3.199, 4.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.192, 10.100], loss: 0.403877, mae: 0.547121, mean_q: 5.161300
 67711/100000: episode: 1127, duration: 0.227s, episode steps: 40, steps per second: 176, episode reward: 136.610, mean reward: 3.415 [1.787, 6.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.594, 10.100], loss: 0.353360, mae: 0.548504, mean_q: 5.110926
 67750/100000: episode: 1128, duration: 0.215s, episode steps: 39, steps per second: 182, episode reward: 112.712, mean reward: 2.890 [2.317, 4.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-0.295, 10.100], loss: 0.440220, mae: 0.533871, mean_q: 5.188916
 67789/100000: episode: 1129, duration: 0.207s, episode steps: 39, steps per second: 188, episode reward: 101.981, mean reward: 2.615 [1.687, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.673, 10.100], loss: 0.347485, mae: 0.550579, mean_q: 5.225661
 67809/100000: episode: 1130, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 116.495, mean reward: 5.825 [3.406, 16.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.471, 10.100], loss: 0.407689, mae: 0.577929, mean_q: 5.322315
 67848/100000: episode: 1131, duration: 0.217s, episode steps: 39, steps per second: 180, episode reward: 120.989, mean reward: 3.102 [1.918, 5.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.776, 10.100], loss: 0.579207, mae: 0.598453, mean_q: 5.162809
 67887/100000: episode: 1132, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 129.950, mean reward: 3.332 [1.937, 6.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.310, 10.100], loss: 0.505692, mae: 0.589511, mean_q: 5.389468
 67900/100000: episode: 1133, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 46.592, mean reward: 3.584 [2.980, 4.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.387, 10.100], loss: 0.423361, mae: 0.534756, mean_q: 5.142950
 67939/100000: episode: 1134, duration: 0.204s, episode steps: 39, steps per second: 191, episode reward: 94.919, mean reward: 2.434 [1.500, 7.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.664, 10.199], loss: 0.487024, mae: 0.560203, mean_q: 5.314454
 67979/100000: episode: 1135, duration: 0.206s, episode steps: 40, steps per second: 194, episode reward: 119.911, mean reward: 2.998 [1.519, 5.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-1.052, 10.100], loss: 0.438766, mae: 0.578269, mean_q: 5.295009
 67999/100000: episode: 1136, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 76.480, mean reward: 3.824 [2.735, 5.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.414, 10.100], loss: 0.480002, mae: 0.569806, mean_q: 5.331872
 68038/100000: episode: 1137, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 98.895, mean reward: 2.536 [1.909, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.422, 10.100], loss: 0.464861, mae: 0.602314, mean_q: 5.241674
 68059/100000: episode: 1138, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 70.962, mean reward: 3.379 [2.610, 5.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.403, 10.100], loss: 0.269151, mae: 0.501516, mean_q: 5.170458
 68100/100000: episode: 1139, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 94.221, mean reward: 2.298 [1.777, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.127, 10.100], loss: 0.463335, mae: 0.605559, mean_q: 5.420239
 68113/100000: episode: 1140, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 58.155, mean reward: 4.473 [2.974, 5.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.626, 10.100], loss: 0.468918, mae: 0.619814, mean_q: 5.537127
 68152/100000: episode: 1141, duration: 0.216s, episode steps: 39, steps per second: 180, episode reward: 102.796, mean reward: 2.636 [1.498, 9.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.508, 10.100], loss: 0.617500, mae: 0.622613, mean_q: 5.376180
 68191/100000: episode: 1142, duration: 0.221s, episode steps: 39, steps per second: 177, episode reward: 139.641, mean reward: 3.581 [1.922, 11.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-1.032, 10.100], loss: 0.540129, mae: 0.601691, mean_q: 5.356002
 68231/100000: episode: 1143, duration: 0.205s, episode steps: 40, steps per second: 195, episode reward: 254.709, mean reward: 6.368 [4.053, 10.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.416, 10.100], loss: 0.405762, mae: 0.582133, mean_q: 5.455378
 68244/100000: episode: 1144, duration: 0.095s, episode steps: 13, steps per second: 137, episode reward: 40.291, mean reward: 3.099 [2.522, 4.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.289, 10.100], loss: 0.500685, mae: 0.610126, mean_q: 5.536789
 68285/100000: episode: 1145, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 120.019, mean reward: 2.927 [2.003, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.875, 10.100], loss: 0.646006, mae: 0.629699, mean_q: 5.470763
 68308/100000: episode: 1146, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 82.060, mean reward: 3.568 [2.533, 5.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.573, 10.100], loss: 0.803210, mae: 0.691644, mean_q: 5.600311
 68328/100000: episode: 1147, duration: 0.116s, episode steps: 20, steps per second: 172, episode reward: 113.404, mean reward: 5.670 [3.636, 13.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.656, 10.100], loss: 0.368541, mae: 0.546018, mean_q: 5.518756
 68349/100000: episode: 1148, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 136.324, mean reward: 6.492 [2.864, 16.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.253, 10.100], loss: 0.462571, mae: 0.572549, mean_q: 5.591001
 68389/100000: episode: 1149, duration: 0.225s, episode steps: 40, steps per second: 178, episode reward: 154.997, mean reward: 3.875 [2.323, 8.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.255, 10.100], loss: 0.551418, mae: 0.628426, mean_q: 5.649804
 68429/100000: episode: 1150, duration: 0.218s, episode steps: 40, steps per second: 183, episode reward: 165.393, mean reward: 4.135 [1.608, 11.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.091, 10.100], loss: 0.449004, mae: 0.576320, mean_q: 5.595993
 68442/100000: episode: 1151, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 61.302, mean reward: 4.716 [2.852, 7.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.823, 10.100], loss: 0.620412, mae: 0.660856, mean_q: 5.616037
 68481/100000: episode: 1152, duration: 0.205s, episode steps: 39, steps per second: 191, episode reward: 111.949, mean reward: 2.870 [1.573, 5.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.758, 10.100], loss: 0.595537, mae: 0.666730, mean_q: 5.536700
 68494/100000: episode: 1153, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 41.492, mean reward: 3.192 [2.733, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.438, 10.100], loss: 0.514524, mae: 0.622652, mean_q: 5.918289
 68534/100000: episode: 1154, duration: 0.233s, episode steps: 40, steps per second: 171, episode reward: 222.404, mean reward: 5.560 [2.064, 91.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.606, 10.100], loss: 0.611418, mae: 0.652188, mean_q: 5.691238
 68547/100000: episode: 1155, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 40.803, mean reward: 3.139 [2.407, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.354, 10.100], loss: 10.159393, mae: 1.167696, mean_q: 6.097689
 68586/100000: episode: 1156, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 127.514, mean reward: 3.270 [2.055, 4.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.574, 10.100], loss: 0.662092, mae: 0.734648, mean_q: 5.528242
 68625/100000: episode: 1157, duration: 0.196s, episode steps: 39, steps per second: 199, episode reward: 113.171, mean reward: 2.902 [2.180, 5.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.818, 10.100], loss: 0.486794, mae: 0.625078, mean_q: 5.569214
 68638/100000: episode: 1158, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 43.236, mean reward: 3.326 [2.456, 5.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.520, 10.100], loss: 0.761167, mae: 0.714941, mean_q: 5.870336
 68661/100000: episode: 1159, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 117.507, mean reward: 5.109 [2.879, 17.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.372, 10.100], loss: 0.382750, mae: 0.587903, mean_q: 5.832945
 68701/100000: episode: 1160, duration: 0.219s, episode steps: 40, steps per second: 182, episode reward: 316.655, mean reward: 7.916 [2.239, 30.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.340, 10.100], loss: 0.810302, mae: 0.709865, mean_q: 5.836145
 68722/100000: episode: 1161, duration: 0.110s, episode steps: 21, steps per second: 191, episode reward: 54.573, mean reward: 2.599 [2.343, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.230, 10.100], loss: 0.684010, mae: 0.682796, mean_q: 5.504359
 68743/100000: episode: 1162, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 93.066, mean reward: 4.432 [2.845, 5.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.602, 10.100], loss: 0.717192, mae: 0.677110, mean_q: 5.709316
[Info] FALSIFICATION!
[Info] Levels: [5.73203, 8.437469, 10.847074]
[Info] Cond. Prob: [0.1, 0.1, 0.18]
[Info] Error Prob: 0.0018000000000000004

 68769/100000: episode: 1163, duration: 4.563s, episode steps: 26, steps per second: 6, episode reward: 258.344, mean reward: 9.936 [2.455, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.830 [-0.669, 9.559], loss: 0.921384, mae: 0.757156, mean_q: 5.933869
 68869/100000: episode: 1164, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 190.920, mean reward: 1.909 [1.447, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.235, 10.100], loss: 5.960941, mae: 0.969807, mean_q: 6.067704
 68969/100000: episode: 1165, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.721, mean reward: 1.907 [1.439, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.960, 10.098], loss: 2.343910, mae: 0.901149, mean_q: 5.961494
 69069/100000: episode: 1166, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 199.579, mean reward: 1.996 [1.442, 4.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.960, 10.168], loss: 2.053009, mae: 0.756922, mean_q: 5.925613
 69169/100000: episode: 1167, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 193.081, mean reward: 1.931 [1.467, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.656, 10.120], loss: 4.666890, mae: 0.892940, mean_q: 6.055604
 69269/100000: episode: 1168, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 201.807, mean reward: 2.018 [1.456, 12.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.118, 10.370], loss: 2.177779, mae: 0.825091, mean_q: 5.929889
 69369/100000: episode: 1169, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 230.286, mean reward: 2.303 [1.478, 4.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.372, 10.098], loss: 2.126585, mae: 0.788480, mean_q: 5.986489
 69469/100000: episode: 1170, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 177.025, mean reward: 1.770 [1.450, 2.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.080, 10.202], loss: 2.193722, mae: 0.810483, mean_q: 6.033937
 69569/100000: episode: 1171, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 214.252, mean reward: 2.143 [1.460, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.044, 10.163], loss: 2.172385, mae: 0.820879, mean_q: 6.081770
 69669/100000: episode: 1172, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 203.549, mean reward: 2.035 [1.460, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.582, 10.098], loss: 0.888713, mae: 0.738567, mean_q: 5.941123
 69769/100000: episode: 1173, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 217.696, mean reward: 2.177 [1.508, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.289, 10.098], loss: 4.648841, mae: 0.912888, mean_q: 6.051994
 69869/100000: episode: 1174, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 197.515, mean reward: 1.975 [1.501, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.221, 10.334], loss: 6.010046, mae: 1.007658, mean_q: 5.968430
 69969/100000: episode: 1175, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 185.309, mean reward: 1.853 [1.450, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.535, 10.098], loss: 3.319359, mae: 0.842528, mean_q: 5.948919
 70069/100000: episode: 1176, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 177.997, mean reward: 1.780 [1.458, 2.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.483, 10.098], loss: 3.273208, mae: 0.801526, mean_q: 5.924888
 70169/100000: episode: 1177, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 194.118, mean reward: 1.941 [1.479, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.859, 10.098], loss: 5.858891, mae: 0.954805, mean_q: 5.997874
 70269/100000: episode: 1178, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 191.626, mean reward: 1.916 [1.441, 3.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.255, 10.098], loss: 4.598012, mae: 0.866222, mean_q: 5.871158
 70369/100000: episode: 1179, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 200.784, mean reward: 2.008 [1.449, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.419, 10.234], loss: 3.361130, mae: 0.809572, mean_q: 5.827293
 70469/100000: episode: 1180, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.409, mean reward: 1.954 [1.447, 3.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.766, 10.278], loss: 3.204905, mae: 0.782160, mean_q: 5.811125
 70569/100000: episode: 1181, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 186.547, mean reward: 1.865 [1.436, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.153, 10.408], loss: 4.704843, mae: 0.910436, mean_q: 5.794872
 70669/100000: episode: 1182, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 183.260, mean reward: 1.833 [1.458, 2.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.363, 10.168], loss: 0.818806, mae: 0.692810, mean_q: 5.687847
 70769/100000: episode: 1183, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 193.973, mean reward: 1.940 [1.461, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.195, 10.105], loss: 0.693380, mae: 0.651453, mean_q: 5.577101
 70869/100000: episode: 1184, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 211.733, mean reward: 2.117 [1.517, 5.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.188, 10.098], loss: 3.272139, mae: 0.765751, mean_q: 5.620293
 70969/100000: episode: 1185, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 188.590, mean reward: 1.886 [1.460, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.746, 10.234], loss: 0.772995, mae: 0.651422, mean_q: 5.555862
 71069/100000: episode: 1186, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.726, mean reward: 1.897 [1.452, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.047, 10.098], loss: 3.210919, mae: 0.727010, mean_q: 5.591757
 71169/100000: episode: 1187, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.454, mean reward: 1.955 [1.532, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.266, 10.098], loss: 2.108145, mae: 0.736464, mean_q: 5.533201
 71269/100000: episode: 1188, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 197.799, mean reward: 1.978 [1.467, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.049, 10.098], loss: 3.396606, mae: 0.762360, mean_q: 5.517303
 71369/100000: episode: 1189, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 185.460, mean reward: 1.855 [1.444, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.854, 10.393], loss: 2.096251, mae: 0.699446, mean_q: 5.421080
 71469/100000: episode: 1190, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 182.542, mean reward: 1.825 [1.450, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.203, 10.261], loss: 3.423588, mae: 0.780247, mean_q: 5.542631
 71569/100000: episode: 1191, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 201.021, mean reward: 2.010 [1.449, 3.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.562, 10.098], loss: 3.362604, mae: 0.746039, mean_q: 5.421996
 71669/100000: episode: 1192, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 194.963, mean reward: 1.950 [1.475, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.236, 10.249], loss: 4.486526, mae: 0.786707, mean_q: 5.400402
 71769/100000: episode: 1193, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 216.661, mean reward: 2.167 [1.520, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.728, 10.098], loss: 3.389369, mae: 0.723884, mean_q: 5.259398
 71869/100000: episode: 1194, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 216.110, mean reward: 2.161 [1.445, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.647, 10.175], loss: 0.660416, mae: 0.573172, mean_q: 5.128604
 71969/100000: episode: 1195, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.300, mean reward: 1.873 [1.488, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.836, 10.204], loss: 1.797508, mae: 0.615329, mean_q: 5.061469
 72069/100000: episode: 1196, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 192.906, mean reward: 1.929 [1.450, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.543, 10.098], loss: 1.845061, mae: 0.578267, mean_q: 4.951630
 72169/100000: episode: 1197, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 214.196, mean reward: 2.142 [1.455, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.393, 10.539], loss: 4.134910, mae: 0.682175, mean_q: 4.981045
 72269/100000: episode: 1198, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 196.287, mean reward: 1.963 [1.449, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.365, 10.098], loss: 1.721480, mae: 0.581145, mean_q: 4.964429
 72369/100000: episode: 1199, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 188.873, mean reward: 1.889 [1.458, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.137, 10.098], loss: 3.005069, mae: 0.628585, mean_q: 4.888721
 72469/100000: episode: 1200, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 183.147, mean reward: 1.831 [1.480, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.126, 10.185], loss: 2.898701, mae: 0.589053, mean_q: 4.837623
 72569/100000: episode: 1201, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 189.164, mean reward: 1.892 [1.474, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.985, 10.122], loss: 3.208577, mae: 0.645176, mean_q: 4.836012
 72669/100000: episode: 1202, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 191.302, mean reward: 1.913 [1.450, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.949, 10.098], loss: 4.233411, mae: 0.655593, mean_q: 4.710500
 72769/100000: episode: 1203, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 193.073, mean reward: 1.931 [1.441, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.978, 10.098], loss: 0.482636, mae: 0.460329, mean_q: 4.541785
 72869/100000: episode: 1204, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 176.716, mean reward: 1.767 [1.464, 2.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.611, 10.193], loss: 0.366295, mae: 0.441958, mean_q: 4.481477
 72969/100000: episode: 1205, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.892, mean reward: 1.899 [1.438, 5.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.880, 10.226], loss: 3.053022, mae: 0.607707, mean_q: 4.541367
 73069/100000: episode: 1206, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 193.046, mean reward: 1.930 [1.446, 7.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.999, 10.131], loss: 0.455011, mae: 0.477529, mean_q: 4.482401
 73169/100000: episode: 1207, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 186.405, mean reward: 1.864 [1.454, 4.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.294, 10.098], loss: 0.322002, mae: 0.410945, mean_q: 4.346734
 73269/100000: episode: 1208, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 229.298, mean reward: 2.293 [1.479, 5.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.234, 10.437], loss: 1.523467, mae: 0.435331, mean_q: 4.336859
 73369/100000: episode: 1209, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 227.468, mean reward: 2.275 [1.511, 4.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.457, 10.098], loss: 1.490938, mae: 0.458251, mean_q: 4.235059
 73469/100000: episode: 1210, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 260.242, mean reward: 2.602 [1.457, 6.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.046, 10.098], loss: 1.562192, mae: 0.432140, mean_q: 4.239901
 73569/100000: episode: 1211, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.670, mean reward: 1.897 [1.459, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.765, 10.098], loss: 0.317714, mae: 0.410259, mean_q: 4.089632
 73669/100000: episode: 1212, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 205.979, mean reward: 2.060 [1.466, 4.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.774, 10.098], loss: 1.483932, mae: 0.417382, mean_q: 4.081899
 73769/100000: episode: 1213, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 186.945, mean reward: 1.869 [1.481, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.661, 10.098], loss: 1.378119, mae: 0.394167, mean_q: 3.986180
 73869/100000: episode: 1214, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 236.451, mean reward: 2.365 [1.484, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.684, 10.310], loss: 0.108988, mae: 0.311164, mean_q: 3.944548
 73969/100000: episode: 1215, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 191.228, mean reward: 1.912 [1.480, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.324, 10.098], loss: 0.088121, mae: 0.302105, mean_q: 3.949490
 74069/100000: episode: 1216, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 234.722, mean reward: 2.347 [1.449, 6.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.994, 10.608], loss: 0.100037, mae: 0.303283, mean_q: 3.939935
 74169/100000: episode: 1217, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 194.823, mean reward: 1.948 [1.483, 2.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.743, 10.098], loss: 0.094927, mae: 0.305085, mean_q: 3.940895
 74269/100000: episode: 1218, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 219.262, mean reward: 2.193 [1.500, 8.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.566, 10.153], loss: 0.111966, mae: 0.309802, mean_q: 3.976986
 74369/100000: episode: 1219, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 207.076, mean reward: 2.071 [1.477, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.907, 10.198], loss: 0.099557, mae: 0.295081, mean_q: 3.950247
 74469/100000: episode: 1220, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 190.757, mean reward: 1.908 [1.469, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.149, 10.125], loss: 0.087990, mae: 0.290349, mean_q: 3.948987
 74569/100000: episode: 1221, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 202.417, mean reward: 2.024 [1.477, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.000, 10.135], loss: 0.097535, mae: 0.304702, mean_q: 3.929955
 74669/100000: episode: 1222, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 190.333, mean reward: 1.903 [1.443, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.443, 10.351], loss: 0.105970, mae: 0.313671, mean_q: 3.953264
 74769/100000: episode: 1223, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 229.235, mean reward: 2.292 [1.461, 4.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.092, 10.098], loss: 0.084975, mae: 0.289845, mean_q: 3.934366
 74869/100000: episode: 1224, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.441, mean reward: 1.874 [1.460, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.878, 10.100], loss: 0.107422, mae: 0.307781, mean_q: 3.934594
 74969/100000: episode: 1225, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 191.025, mean reward: 1.910 [1.458, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.836, 10.157], loss: 0.102471, mae: 0.299154, mean_q: 3.951187
 75069/100000: episode: 1226, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 218.769, mean reward: 2.188 [1.505, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.840, 10.098], loss: 0.107543, mae: 0.307264, mean_q: 3.955588
 75169/100000: episode: 1227, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 203.498, mean reward: 2.035 [1.483, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.637, 10.098], loss: 0.095671, mae: 0.307779, mean_q: 3.973839
 75269/100000: episode: 1228, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 244.447, mean reward: 2.444 [1.561, 4.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.721, 10.527], loss: 0.110572, mae: 0.310732, mean_q: 3.961213
 75369/100000: episode: 1229, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 214.179, mean reward: 2.142 [1.485, 8.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.864, 10.098], loss: 0.096235, mae: 0.303462, mean_q: 3.980367
 75469/100000: episode: 1230, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.030, mean reward: 1.880 [1.449, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.851, 10.288], loss: 0.095811, mae: 0.305452, mean_q: 3.974088
 75569/100000: episode: 1231, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 208.144, mean reward: 2.081 [1.530, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.308, 10.179], loss: 0.118010, mae: 0.319884, mean_q: 3.990266
 75669/100000: episode: 1232, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 192.137, mean reward: 1.921 [1.480, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.195, 10.325], loss: 0.110697, mae: 0.317909, mean_q: 3.988431
 75769/100000: episode: 1233, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 200.783, mean reward: 2.008 [1.435, 5.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.447, 10.210], loss: 0.102840, mae: 0.312168, mean_q: 3.997506
 75869/100000: episode: 1234, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 191.299, mean reward: 1.913 [1.468, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.942, 10.152], loss: 0.096204, mae: 0.303761, mean_q: 3.982884
 75969/100000: episode: 1235, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 199.349, mean reward: 1.993 [1.433, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.740, 10.353], loss: 0.105257, mae: 0.318682, mean_q: 3.994616
 76069/100000: episode: 1236, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 204.399, mean reward: 2.044 [1.456, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.418, 10.098], loss: 0.082490, mae: 0.289505, mean_q: 3.945795
 76169/100000: episode: 1237, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 215.814, mean reward: 2.158 [1.495, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.078, 10.098], loss: 0.096819, mae: 0.312488, mean_q: 3.980646
 76269/100000: episode: 1238, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 196.322, mean reward: 1.963 [1.445, 5.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.359, 10.375], loss: 0.123641, mae: 0.328558, mean_q: 4.003589
 76369/100000: episode: 1239, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 197.915, mean reward: 1.979 [1.464, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.054, 10.098], loss: 0.114097, mae: 0.315859, mean_q: 4.010931
 76469/100000: episode: 1240, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 177.005, mean reward: 1.770 [1.440, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.050, 10.098], loss: 0.111575, mae: 0.312643, mean_q: 4.016172
 76569/100000: episode: 1241, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 189.086, mean reward: 1.891 [1.456, 3.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.198, 10.098], loss: 0.114032, mae: 0.320586, mean_q: 3.999997
 76669/100000: episode: 1242, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 188.442, mean reward: 1.884 [1.461, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.704, 10.098], loss: 0.107676, mae: 0.314554, mean_q: 4.000978
 76769/100000: episode: 1243, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 224.041, mean reward: 2.240 [1.448, 9.935], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.597, 10.240], loss: 0.111763, mae: 0.323533, mean_q: 4.008927
 76869/100000: episode: 1244, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 189.122, mean reward: 1.891 [1.452, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.154, 10.098], loss: 0.107281, mae: 0.319179, mean_q: 3.997674
 76969/100000: episode: 1245, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 196.331, mean reward: 1.963 [1.451, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.444, 10.192], loss: 0.122443, mae: 0.320960, mean_q: 4.002645
 77069/100000: episode: 1246, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 222.721, mean reward: 2.227 [1.515, 7.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.650, 10.186], loss: 0.123338, mae: 0.329295, mean_q: 3.996658
 77169/100000: episode: 1247, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.786, mean reward: 1.888 [1.465, 3.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.784, 10.098], loss: 0.100350, mae: 0.315529, mean_q: 3.974392
 77269/100000: episode: 1248, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 271.273, mean reward: 2.713 [1.458, 6.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.441, 10.098], loss: 0.114713, mae: 0.320590, mean_q: 4.007769
 77369/100000: episode: 1249, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 183.671, mean reward: 1.837 [1.470, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.139, 10.222], loss: 0.136530, mae: 0.343251, mean_q: 4.028734
 77469/100000: episode: 1250, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 206.838, mean reward: 2.068 [1.476, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.476, 10.098], loss: 0.123362, mae: 0.333220, mean_q: 4.037034
 77569/100000: episode: 1251, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 198.536, mean reward: 1.985 [1.475, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.313, 10.427], loss: 0.103903, mae: 0.315390, mean_q: 4.035602
 77669/100000: episode: 1252, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 196.474, mean reward: 1.965 [1.457, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.656, 10.098], loss: 0.136261, mae: 0.342563, mean_q: 4.070760
 77769/100000: episode: 1253, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 200.869, mean reward: 2.009 [1.507, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.676, 10.148], loss: 0.142655, mae: 0.347404, mean_q: 4.068269
 77869/100000: episode: 1254, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 208.681, mean reward: 2.087 [1.456, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.683, 10.184], loss: 0.122726, mae: 0.337535, mean_q: 4.037644
 77969/100000: episode: 1255, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 237.461, mean reward: 2.375 [1.448, 13.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.977, 10.356], loss: 0.121620, mae: 0.327311, mean_q: 4.055194
 78069/100000: episode: 1256, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 187.397, mean reward: 1.874 [1.466, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.040, 10.098], loss: 0.179359, mae: 0.356058, mean_q: 4.079426
 78169/100000: episode: 1257, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 210.709, mean reward: 2.107 [1.496, 5.528], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.806, 10.098], loss: 0.125500, mae: 0.334618, mean_q: 4.075399
 78269/100000: episode: 1258, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 232.234, mean reward: 2.322 [1.445, 7.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.903, 10.491], loss: 0.146806, mae: 0.334323, mean_q: 4.077798
 78369/100000: episode: 1259, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 191.314, mean reward: 1.913 [1.515, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.270, 10.098], loss: 0.134213, mae: 0.335966, mean_q: 4.042859
 78469/100000: episode: 1260, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 199.495, mean reward: 1.995 [1.458, 3.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.313, 10.098], loss: 0.103354, mae: 0.318833, mean_q: 4.026551
 78569/100000: episode: 1261, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.860, mean reward: 1.919 [1.443, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.783, 10.109], loss: 0.118148, mae: 0.326092, mean_q: 4.034270
 78669/100000: episode: 1262, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 181.110, mean reward: 1.811 [1.453, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.844, 10.129], loss: 0.143165, mae: 0.334605, mean_q: 4.029707
[Info] 1-TH LEVEL FOUND: 6.798864841461182, Considering 10/90 traces
 78769/100000: episode: 1263, duration: 4.683s, episode steps: 100, steps per second: 21, episode reward: 197.571, mean reward: 1.976 [1.438, 3.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.637, 10.098], loss: 0.105970, mae: 0.328125, mean_q: 4.048244
 78782/100000: episode: 1264, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 36.954, mean reward: 2.843 [1.963, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.381, 10.100], loss: 0.172049, mae: 0.334782, mean_q: 4.074277
 78819/100000: episode: 1265, duration: 0.199s, episode steps: 37, steps per second: 186, episode reward: 113.285, mean reward: 3.062 [2.052, 5.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.883, 10.100], loss: 0.114238, mae: 0.336855, mean_q: 4.055802
 78846/100000: episode: 1266, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 104.952, mean reward: 3.887 [2.380, 8.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-1.163, 10.100], loss: 0.173071, mae: 0.380828, mean_q: 4.096386
 78873/100000: episode: 1267, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 90.427, mean reward: 3.349 [2.595, 7.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.274, 10.100], loss: 0.174922, mae: 0.360182, mean_q: 4.089234
 78885/100000: episode: 1268, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 48.148, mean reward: 4.012 [3.333, 5.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.741, 10.598], loss: 0.179454, mae: 0.353352, mean_q: 4.157917
 78912/100000: episode: 1269, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 62.996, mean reward: 2.333 [1.873, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.706, 10.100], loss: 0.155070, mae: 0.349106, mean_q: 4.096664
 78946/100000: episode: 1270, duration: 0.180s, episode steps: 34, steps per second: 188, episode reward: 175.535, mean reward: 5.163 [2.539, 28.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.479, 10.583], loss: 0.230129, mae: 0.366729, mean_q: 4.149285
 78973/100000: episode: 1271, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 73.870, mean reward: 2.736 [2.017, 4.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.433, 10.100], loss: 0.136278, mae: 0.349234, mean_q: 4.065940
 78992/100000: episode: 1272, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 50.459, mean reward: 2.656 [1.889, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.167, 10.100], loss: 0.702377, mae: 0.414440, mean_q: 4.204357
 79003/100000: episode: 1273, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 26.656, mean reward: 2.423 [2.111, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.153, 10.100], loss: 0.161122, mae: 0.403013, mean_q: 4.195710
 79015/100000: episode: 1274, duration: 0.078s, episode steps: 12, steps per second: 153, episode reward: 40.400, mean reward: 3.367 [2.561, 5.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.475], loss: 0.332054, mae: 0.379470, mean_q: 4.017864
 79048/100000: episode: 1275, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 117.128, mean reward: 3.549 [2.252, 5.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.408, 10.100], loss: 0.188286, mae: 0.371143, mean_q: 4.111000
 79067/100000: episode: 1276, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 87.629, mean reward: 4.612 [3.142, 6.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.310, 10.100], loss: 0.117941, mae: 0.326115, mean_q: 4.089377
 79078/100000: episode: 1277, duration: 0.074s, episode steps: 11, steps per second: 150, episode reward: 37.441, mean reward: 3.404 [2.967, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.877, 10.100], loss: 0.132225, mae: 0.340594, mean_q: 4.109092
 79115/100000: episode: 1278, duration: 0.185s, episode steps: 37, steps per second: 200, episode reward: 98.719, mean reward: 2.668 [1.772, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-1.377, 10.100], loss: 0.159334, mae: 0.359197, mean_q: 4.175323
 79152/100000: episode: 1279, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: 92.170, mean reward: 2.491 [1.641, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.734, 10.100], loss: 0.207884, mae: 0.376091, mean_q: 4.130853
 79163/100000: episode: 1280, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 34.546, mean reward: 3.141 [2.502, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.258, 10.100], loss: 0.135235, mae: 0.382861, mean_q: 4.185348
 79178/100000: episode: 1281, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 40.899, mean reward: 2.727 [2.247, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.235, 10.100], loss: 0.102884, mae: 0.323021, mean_q: 4.144547
 79211/100000: episode: 1282, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 108.310, mean reward: 3.282 [2.390, 4.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.273, 10.100], loss: 0.151692, mae: 0.355218, mean_q: 4.246405
 79221/100000: episode: 1283, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 48.714, mean reward: 4.871 [2.970, 6.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.080, 10.589], loss: 0.170293, mae: 0.322107, mean_q: 4.097314
 79236/100000: episode: 1284, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 47.564, mean reward: 3.171 [2.306, 6.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.279, 10.100], loss: 0.123719, mae: 0.333003, mean_q: 4.206636
 79246/100000: episode: 1285, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 30.290, mean reward: 3.029 [2.039, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.371], loss: 0.114189, mae: 0.324455, mean_q: 4.181408
 79257/100000: episode: 1286, duration: 0.059s, episode steps: 11, steps per second: 188, episode reward: 35.413, mean reward: 3.219 [2.816, 4.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.950, 10.100], loss: 0.178515, mae: 0.394345, mean_q: 4.222260
 79267/100000: episode: 1287, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 32.674, mean reward: 3.267 [2.598, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.623, 10.352], loss: 0.124795, mae: 0.340087, mean_q: 4.238800
 79282/100000: episode: 1288, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 58.994, mean reward: 3.933 [2.616, 5.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.509, 10.100], loss: 0.116034, mae: 0.318554, mean_q: 4.169447
 79319/100000: episode: 1289, duration: 0.183s, episode steps: 37, steps per second: 202, episode reward: 96.123, mean reward: 2.598 [1.776, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.204, 10.100], loss: 0.160810, mae: 0.372294, mean_q: 4.242787
 79334/100000: episode: 1290, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 50.630, mean reward: 3.375 [2.552, 4.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.290, 10.100], loss: 0.316625, mae: 0.414066, mean_q: 4.389737
 79347/100000: episode: 1291, duration: 0.067s, episode steps: 13, steps per second: 193, episode reward: 39.080, mean reward: 3.006 [2.399, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.433, 10.100], loss: 0.405937, mae: 0.434721, mean_q: 4.247271
 79358/100000: episode: 1292, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 46.051, mean reward: 4.186 [3.482, 4.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.446, 10.100], loss: 0.227472, mae: 0.407816, mean_q: 4.199604
 79369/100000: episode: 1293, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 28.711, mean reward: 2.610 [2.160, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.314, 10.100], loss: 0.237929, mae: 0.392177, mean_q: 4.283509
 79381/100000: episode: 1294, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 46.846, mean reward: 3.904 [2.309, 7.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.703, 10.398], loss: 0.158402, mae: 0.375238, mean_q: 4.201935
 79394/100000: episode: 1295, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 30.701, mean reward: 2.362 [1.914, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.318, 10.100], loss: 0.116561, mae: 0.343699, mean_q: 4.283281
 79404/100000: episode: 1296, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 38.344, mean reward: 3.834 [2.946, 5.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.823, 10.600], loss: 0.218439, mae: 0.425881, mean_q: 4.401631
 79437/100000: episode: 1297, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 102.279, mean reward: 3.099 [1.895, 5.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.186, 10.100], loss: 0.196459, mae: 0.382386, mean_q: 4.306055
 79474/100000: episode: 1298, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 85.746, mean reward: 2.317 [1.517, 3.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.091, 10.100], loss: 0.242384, mae: 0.403317, mean_q: 4.269179
 79485/100000: episode: 1299, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 28.958, mean reward: 2.633 [2.215, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.155, 10.100], loss: 1.101241, mae: 0.502005, mean_q: 4.481952
 79495/100000: episode: 1300, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 37.962, mean reward: 3.796 [3.118, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.073, 10.518], loss: 0.229648, mae: 0.436622, mean_q: 4.210268
 79506/100000: episode: 1301, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 25.886, mean reward: 2.353 [2.094, 2.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.262, 10.100], loss: 0.411860, mae: 0.460825, mean_q: 4.455878
 79543/100000: episode: 1302, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 94.145, mean reward: 2.544 [1.703, 5.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.952, 10.100], loss: 0.431730, mae: 0.395827, mean_q: 4.276417
 79555/100000: episode: 1303, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 45.701, mean reward: 3.808 [3.108, 5.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.505], loss: 0.245385, mae: 0.426714, mean_q: 4.338745
 79567/100000: episode: 1304, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 41.500, mean reward: 3.458 [3.048, 3.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.490], loss: 0.161503, mae: 0.381019, mean_q: 4.299630
 79601/100000: episode: 1305, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 86.413, mean reward: 2.542 [1.588, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.050, 10.166], loss: 0.241006, mae: 0.413900, mean_q: 4.358492
 79638/100000: episode: 1306, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 92.125, mean reward: 2.490 [1.565, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.511, 10.290], loss: 0.198903, mae: 0.382629, mean_q: 4.314558
 79651/100000: episode: 1307, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 37.378, mean reward: 2.875 [2.080, 5.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.287, 10.100], loss: 0.472144, mae: 0.478649, mean_q: 4.422325
 79685/100000: episode: 1308, duration: 0.198s, episode steps: 34, steps per second: 172, episode reward: 69.690, mean reward: 2.050 [1.557, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.035, 10.328], loss: 0.328665, mae: 0.438349, mean_q: 4.419782
 79718/100000: episode: 1309, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 92.642, mean reward: 2.807 [1.794, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.606, 10.100], loss: 0.207243, mae: 0.414810, mean_q: 4.351284
 79728/100000: episode: 1310, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 34.376, mean reward: 3.438 [2.533, 4.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.591], loss: 0.347719, mae: 0.437022, mean_q: 4.422025
 79765/100000: episode: 1311, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 96.833, mean reward: 2.617 [1.494, 5.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-1.040, 10.100], loss: 0.229445, mae: 0.409667, mean_q: 4.399145
 79780/100000: episode: 1312, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 53.661, mean reward: 3.577 [1.947, 16.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.355, 10.100], loss: 0.196416, mae: 0.404225, mean_q: 4.454095
 79793/100000: episode: 1313, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 32.492, mean reward: 2.499 [1.898, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.422, 10.100], loss: 0.174516, mae: 0.378354, mean_q: 4.255736
 79812/100000: episode: 1314, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 60.154, mean reward: 3.166 [2.555, 4.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.372, 10.100], loss: 0.788556, mae: 0.476742, mean_q: 4.516964
 79846/100000: episode: 1315, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 106.907, mean reward: 3.144 [2.346, 6.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-1.078, 10.361], loss: 0.544079, mae: 0.481141, mean_q: 4.341650
 79858/100000: episode: 1316, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 50.097, mean reward: 4.175 [2.916, 7.321], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.605], loss: 0.247300, mae: 0.437094, mean_q: 4.299756
 79885/100000: episode: 1317, duration: 0.169s, episode steps: 27, steps per second: 160, episode reward: 75.934, mean reward: 2.812 [2.051, 3.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.459, 10.100], loss: 0.272932, mae: 0.426387, mean_q: 4.478721
 79896/100000: episode: 1318, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 37.221, mean reward: 3.384 [2.895, 4.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.484, 10.100], loss: 0.209234, mae: 0.431470, mean_q: 4.417059
 79911/100000: episode: 1319, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 45.373, mean reward: 3.025 [2.078, 4.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.386, 10.100], loss: 0.272847, mae: 0.416876, mean_q: 4.447810
 79930/100000: episode: 1320, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 50.691, mean reward: 2.668 [1.945, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.137, 10.100], loss: 0.432560, mae: 0.492901, mean_q: 4.481499
 79943/100000: episode: 1321, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 36.364, mean reward: 2.797 [2.009, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.775, 10.100], loss: 0.355693, mae: 0.462530, mean_q: 4.560019
 79970/100000: episode: 1322, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 69.631, mean reward: 2.579 [1.915, 4.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.680, 10.100], loss: 0.183248, mae: 0.397937, mean_q: 4.400405
 79997/100000: episode: 1323, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 98.651, mean reward: 3.654 [2.653, 6.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.444, 10.100], loss: 0.605320, mae: 0.435528, mean_q: 4.463964
 80012/100000: episode: 1324, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 45.766, mean reward: 3.051 [2.293, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.416, 10.100], loss: 0.248939, mae: 0.446572, mean_q: 4.498586
 80046/100000: episode: 1325, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 96.536, mean reward: 2.839 [2.309, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.071, 10.422], loss: 0.163147, mae: 0.393011, mean_q: 4.434521
 80073/100000: episode: 1326, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 81.582, mean reward: 3.022 [2.042, 5.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.303, 10.100], loss: 0.311705, mae: 0.424106, mean_q: 4.446146
 80106/100000: episode: 1327, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 84.350, mean reward: 2.556 [1.747, 4.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.334, 10.100], loss: 0.564648, mae: 0.456221, mean_q: 4.547171
 80143/100000: episode: 1328, duration: 0.205s, episode steps: 37, steps per second: 181, episode reward: 98.991, mean reward: 2.675 [1.782, 6.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.346, 10.100], loss: 0.179390, mae: 0.409024, mean_q: 4.478027
 80154/100000: episode: 1329, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 36.087, mean reward: 3.281 [2.758, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.346, 10.100], loss: 0.434376, mae: 0.471788, mean_q: 4.281457
 80191/100000: episode: 1330, duration: 0.194s, episode steps: 37, steps per second: 191, episode reward: 120.479, mean reward: 3.256 [1.506, 13.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.989, 10.140], loss: 0.511297, mae: 0.485745, mean_q: 4.550918
 80218/100000: episode: 1331, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 92.073, mean reward: 3.410 [2.649, 4.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.320, 10.100], loss: 0.199879, mae: 0.422132, mean_q: 4.501735
 80228/100000: episode: 1332, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 32.767, mean reward: 3.277 [2.707, 4.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.397], loss: 0.553301, mae: 0.496454, mean_q: 4.558686
 80262/100000: episode: 1333, duration: 0.198s, episode steps: 34, steps per second: 172, episode reward: 121.188, mean reward: 3.564 [2.192, 9.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.035, 10.547], loss: 0.178508, mae: 0.411341, mean_q: 4.507555
 80295/100000: episode: 1334, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 93.878, mean reward: 2.845 [2.010, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.068, 10.100], loss: 0.186856, mae: 0.404796, mean_q: 4.465271
 80314/100000: episode: 1335, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 64.622, mean reward: 3.401 [2.687, 4.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.273, 10.100], loss: 0.336744, mae: 0.456852, mean_q: 4.570273
 80326/100000: episode: 1336, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 46.559, mean reward: 3.880 [2.788, 6.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.575], loss: 0.210797, mae: 0.427267, mean_q: 4.414732
 80360/100000: episode: 1337, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 70.779, mean reward: 2.082 [1.628, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.360, 10.292], loss: 0.275546, mae: 0.424816, mean_q: 4.551961
 80387/100000: episode: 1338, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 70.535, mean reward: 2.612 [1.896, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.203, 10.100], loss: 0.766444, mae: 0.509884, mean_q: 4.638135
 80398/100000: episode: 1339, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 34.543, mean reward: 3.140 [2.843, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.475, 10.100], loss: 1.184594, mae: 0.572596, mean_q: 4.719841
 80417/100000: episode: 1340, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 83.844, mean reward: 4.413 [2.510, 7.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.705, 10.100], loss: 0.377494, mae: 0.500264, mean_q: 4.481737
 80430/100000: episode: 1341, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 38.413, mean reward: 2.955 [2.064, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.423, 10.100], loss: 0.166149, mae: 0.394951, mean_q: 4.431744
 80442/100000: episode: 1342, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 45.603, mean reward: 3.800 [2.929, 5.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-1.096, 10.500], loss: 1.143846, mae: 0.586407, mean_q: 4.753388
 80461/100000: episode: 1343, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 46.273, mean reward: 2.435 [1.825, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.199, 10.100], loss: 0.207836, mae: 0.446593, mean_q: 4.640785
 80474/100000: episode: 1344, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 34.023, mean reward: 2.617 [2.050, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.253, 10.100], loss: 0.177622, mae: 0.433564, mean_q: 4.587374
 80511/100000: episode: 1345, duration: 0.206s, episode steps: 37, steps per second: 180, episode reward: 89.883, mean reward: 2.429 [1.643, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.652, 10.100], loss: 0.564695, mae: 0.475323, mean_q: 4.636698
 80523/100000: episode: 1346, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 37.341, mean reward: 3.112 [2.808, 4.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.587, 10.457], loss: 0.180467, mae: 0.414547, mean_q: 4.556772
 80534/100000: episode: 1347, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 35.610, mean reward: 3.237 [2.815, 4.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.244, 10.100], loss: 0.144452, mae: 0.386817, mean_q: 4.643330
 80568/100000: episode: 1348, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 86.384, mean reward: 2.541 [2.083, 3.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.290, 10.380], loss: 0.253565, mae: 0.438168, mean_q: 4.604381
 80581/100000: episode: 1349, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 45.208, mean reward: 3.478 [2.368, 4.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.308, 10.100], loss: 0.185464, mae: 0.416468, mean_q: 4.481004
 80592/100000: episode: 1350, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 26.655, mean reward: 2.423 [2.061, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.213, 10.100], loss: 0.150062, mae: 0.394964, mean_q: 4.585535
 80629/100000: episode: 1351, duration: 0.193s, episode steps: 37, steps per second: 191, episode reward: 124.098, mean reward: 3.354 [1.910, 5.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.691, 10.100], loss: 0.480722, mae: 0.461348, mean_q: 4.703466
 80639/100000: episode: 1352, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 26.958, mean reward: 2.696 [2.382, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.453], loss: 0.221812, mae: 0.439192, mean_q: 4.503446
[Info] 2-TH LEVEL FOUND: 8.171672821044922, Considering 10/90 traces
 80651/100000: episode: 1353, duration: 4.211s, episode steps: 12, steps per second: 3, episode reward: 37.034, mean reward: 3.086 [2.789, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.311, 10.478], loss: 0.208242, mae: 0.428281, mean_q: 4.733531
 80658/100000: episode: 1354, duration: 0.044s, episode steps: 7, steps per second: 157, episode reward: 40.941, mean reward: 5.849 [4.786, 6.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.566], loss: 0.975205, mae: 0.618378, mean_q: 4.758499
 80668/100000: episode: 1355, duration: 0.066s, episode steps: 10, steps per second: 153, episode reward: 51.059, mean reward: 5.106 [3.218, 7.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.638], loss: 0.285373, mae: 0.465489, mean_q: 4.579827
 80678/100000: episode: 1356, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 30.778, mean reward: 3.078 [2.541, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.832, 10.467], loss: 0.321889, mae: 0.444033, mean_q: 4.732100
 80684/100000: episode: 1357, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 25.914, mean reward: 4.319 [3.949, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.555], loss: 0.268291, mae: 0.435267, mean_q: 4.655758
 80692/100000: episode: 1358, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 26.703, mean reward: 3.338 [2.970, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.546, 10.451], loss: 0.433383, mae: 0.493037, mean_q: 4.757425
 80701/100000: episode: 1359, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 47.234, mean reward: 5.248 [4.045, 8.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.513], loss: 0.237851, mae: 0.420750, mean_q: 4.511605
 80708/100000: episode: 1360, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 32.435, mean reward: 4.634 [3.713, 5.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.601], loss: 0.160886, mae: 0.403617, mean_q: 4.792835
 80718/100000: episode: 1361, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 47.767, mean reward: 4.777 [3.008, 9.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.114, 10.542], loss: 0.352972, mae: 0.417419, mean_q: 4.702732
 80726/100000: episode: 1362, duration: 0.040s, episode steps: 8, steps per second: 198, episode reward: 24.916, mean reward: 3.114 [2.634, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.491], loss: 0.221682, mae: 0.417297, mean_q: 4.638544
 80736/100000: episode: 1363, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 30.118, mean reward: 3.012 [2.131, 6.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.212, 10.408], loss: 0.365054, mae: 0.475646, mean_q: 4.934711
 80744/100000: episode: 1364, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 40.641, mean reward: 5.080 [3.097, 8.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.035, 10.644], loss: 1.496151, mae: 0.533185, mean_q: 4.678928
 80754/100000: episode: 1365, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 46.332, mean reward: 4.633 [3.790, 5.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.457], loss: 0.268477, mae: 0.520606, mean_q: 4.840194
 80783/100000: episode: 1366, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 172.875, mean reward: 5.961 [2.002, 35.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.302, 10.100], loss: 0.422299, mae: 0.481139, mean_q: 4.709580
 80790/100000: episode: 1367, duration: 0.038s, episode steps: 7, steps per second: 184, episode reward: 22.316, mean reward: 3.188 [2.605, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.479], loss: 0.261845, mae: 0.452496, mean_q: 4.701850
 80799/100000: episode: 1368, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 31.472, mean reward: 3.497 [2.565, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.615, 10.475], loss: 0.244683, mae: 0.460618, mean_q: 4.831971
 80828/100000: episode: 1369, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 171.267, mean reward: 5.906 [3.214, 10.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.115, 10.100], loss: 0.338887, mae: 0.460320, mean_q: 4.784476
 80836/100000: episode: 1370, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 25.722, mean reward: 3.215 [2.604, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.342, 10.436], loss: 0.260382, mae: 0.493423, mean_q: 4.674446
 80843/100000: episode: 1371, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 31.861, mean reward: 4.552 [3.977, 5.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.385, 10.586], loss: 0.374704, mae: 0.569346, mean_q: 4.830301
 80872/100000: episode: 1372, duration: 0.144s, episode steps: 29, steps per second: 201, episode reward: 134.377, mean reward: 4.634 [2.993, 6.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.568, 10.100], loss: 0.363492, mae: 0.477656, mean_q: 4.851366
 80878/100000: episode: 1373, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 39.116, mean reward: 6.519 [5.381, 8.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.537], loss: 0.183433, mae: 0.409903, mean_q: 4.824015
 80885/100000: episode: 1374, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 35.851, mean reward: 5.122 [4.063, 6.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.646], loss: 1.662507, mae: 0.569075, mean_q: 4.875354
 80892/100000: episode: 1375, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 19.786, mean reward: 2.827 [2.387, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.452], loss: 0.462233, mae: 0.616190, mean_q: 5.068718
 80899/100000: episode: 1376, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 24.565, mean reward: 3.509 [2.943, 4.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.186, 10.513], loss: 4.070230, mae: 0.869643, mean_q: 4.612428
 80907/100000: episode: 1377, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 26.564, mean reward: 3.320 [2.696, 4.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.520], loss: 0.490326, mae: 0.714439, mean_q: 5.466028
 80913/100000: episode: 1378, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 32.038, mean reward: 5.340 [4.895, 5.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.599], loss: 0.803334, mae: 0.591047, mean_q: 4.451940
 80919/100000: episode: 1379, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 37.250, mean reward: 6.208 [4.853, 7.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.553], loss: 2.823046, mae: 0.850935, mean_q: 4.975190
 80928/100000: episode: 1380, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 38.996, mean reward: 4.333 [3.079, 6.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.130, 10.535], loss: 0.570105, mae: 0.692657, mean_q: 5.121931
 80934/100000: episode: 1381, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 30.903, mean reward: 5.151 [4.078, 8.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.302, 10.431], loss: 0.764691, mae: 0.591868, mean_q: 4.625014
 80963/100000: episode: 1382, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 168.888, mean reward: 5.824 [2.777, 13.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-2.167, 10.100], loss: 0.292420, mae: 0.488847, mean_q: 4.944530
 80970/100000: episode: 1383, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 29.282, mean reward: 4.183 [3.076, 6.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.580], loss: 0.777010, mae: 0.586303, mean_q: 5.013669
 80979/100000: episode: 1384, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 51.388, mean reward: 5.710 [3.647, 7.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.087, 10.602], loss: 0.260315, mae: 0.479194, mean_q: 4.935021
 81008/100000: episode: 1385, duration: 0.146s, episode steps: 29, steps per second: 199, episode reward: 87.764, mean reward: 3.026 [1.598, 5.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.682, 10.100], loss: 0.321241, mae: 0.464655, mean_q: 4.883987
 81015/100000: episode: 1386, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 49.214, mean reward: 7.031 [3.559, 10.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.400, 10.653], loss: 0.349315, mae: 0.505051, mean_q: 4.867982
 81024/100000: episode: 1387, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 37.600, mean reward: 4.178 [3.320, 5.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.468, 10.554], loss: 0.313039, mae: 0.518092, mean_q: 5.118029
 81053/100000: episode: 1388, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 262.310, mean reward: 9.045 [2.841, 25.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.893, 10.100], loss: 0.249080, mae: 0.471530, mean_q: 4.996461
 81061/100000: episode: 1389, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 24.959, mean reward: 3.120 [2.801, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.536], loss: 0.279678, mae: 0.484629, mean_q: 5.140060
 81069/100000: episode: 1390, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 38.230, mean reward: 4.779 [3.561, 7.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.569], loss: 0.467008, mae: 0.565992, mean_q: 5.095005
 81078/100000: episode: 1391, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 34.466, mean reward: 3.830 [2.903, 5.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.470], loss: 0.661114, mae: 0.599760, mean_q: 5.036903
 81086/100000: episode: 1392, duration: 0.043s, episode steps: 8, steps per second: 188, episode reward: 31.788, mean reward: 3.974 [3.387, 4.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.561], loss: 0.379237, mae: 0.497083, mean_q: 5.078534
 81094/100000: episode: 1393, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 31.551, mean reward: 3.944 [2.897, 4.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.476], loss: 0.961782, mae: 0.772026, mean_q: 5.335790
[Info] FALSIFICATION!
[Info] Levels: [6.798865, 8.171673, 9.654687]
[Info] Cond. Prob: [0.1, 0.1, 0.11]
[Info] Error Prob: 0.0011000000000000003

 81100/100000: episode: 1394, duration: 4.542s, episode steps: 6, steps per second: 1, episode reward: 149.523, mean reward: 24.921 [3.521, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.390, 10.509], loss: 0.696547, mae: 0.617552, mean_q: 4.597792
 81200/100000: episode: 1395, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 198.447, mean reward: 1.984 [1.468, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.980, 10.108], loss: 1.033734, mae: 0.632401, mean_q: 5.146818
 81300/100000: episode: 1396, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 181.905, mean reward: 1.819 [1.466, 3.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.858, 10.098], loss: 1.106873, mae: 0.627169, mean_q: 5.112134
 81400/100000: episode: 1397, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 234.786, mean reward: 2.348 [1.476, 5.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.098, 10.098], loss: 0.974855, mae: 0.624022, mean_q: 5.132419
 81500/100000: episode: 1398, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 187.674, mean reward: 1.877 [1.447, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.017, 10.200], loss: 0.677463, mae: 0.609061, mean_q: 5.181363
 81600/100000: episode: 1399, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 187.417, mean reward: 1.874 [1.463, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.706, 10.132], loss: 0.895972, mae: 0.586209, mean_q: 5.098186
 81700/100000: episode: 1400, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 187.961, mean reward: 1.880 [1.458, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.643, 10.098], loss: 4.893604, mae: 0.843965, mean_q: 5.269309
 81800/100000: episode: 1401, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.760, mean reward: 1.958 [1.475, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.173, 10.305], loss: 0.811934, mae: 0.618549, mean_q: 5.220050
 81900/100000: episode: 1402, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 210.638, mean reward: 2.106 [1.486, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.796, 10.282], loss: 2.182892, mae: 0.669996, mean_q: 5.147810
 82000/100000: episode: 1403, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 187.069, mean reward: 1.871 [1.479, 2.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.257, 10.109], loss: 0.864957, mae: 0.620567, mean_q: 5.143517
 82100/100000: episode: 1404, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 201.505, mean reward: 2.015 [1.443, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.814, 10.098], loss: 0.964126, mae: 0.588724, mean_q: 5.149222
 82200/100000: episode: 1405, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 188.846, mean reward: 1.888 [1.461, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.646, 10.276], loss: 2.070546, mae: 0.616508, mean_q: 5.155283
 82300/100000: episode: 1406, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 185.072, mean reward: 1.851 [1.437, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.401, 10.098], loss: 0.584568, mae: 0.555045, mean_q: 5.099420
 82400/100000: episode: 1407, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 202.469, mean reward: 2.025 [1.461, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.438, 10.098], loss: 2.205451, mae: 0.633562, mean_q: 5.154552
 82500/100000: episode: 1408, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 203.973, mean reward: 2.040 [1.499, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.818, 10.147], loss: 4.502838, mae: 0.767135, mean_q: 5.212548
 82600/100000: episode: 1409, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 190.676, mean reward: 1.907 [1.477, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.067, 10.098], loss: 3.342527, mae: 0.662071, mean_q: 5.154537
 82700/100000: episode: 1410, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 190.016, mean reward: 1.900 [1.487, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.111, 10.256], loss: 0.905803, mae: 0.583045, mean_q: 5.070839
 82800/100000: episode: 1411, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.422, mean reward: 1.894 [1.449, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.067, 10.208], loss: 1.972827, mae: 0.624739, mean_q: 5.215533
 82900/100000: episode: 1412, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.551, mean reward: 1.836 [1.447, 2.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.565, 10.098], loss: 3.111294, mae: 0.645362, mean_q: 5.184899
 83000/100000: episode: 1413, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 190.609, mean reward: 1.906 [1.469, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.845, 10.263], loss: 0.694586, mae: 0.561393, mean_q: 5.097125
 83100/100000: episode: 1414, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.898, mean reward: 1.859 [1.484, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.647, 10.098], loss: 2.148960, mae: 0.645473, mean_q: 5.172086
 83200/100000: episode: 1415, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 193.137, mean reward: 1.931 [1.476, 4.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.454, 10.098], loss: 4.773053, mae: 0.814665, mean_q: 5.139806
 83300/100000: episode: 1416, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 186.957, mean reward: 1.870 [1.436, 4.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.493, 10.098], loss: 1.816206, mae: 0.606314, mean_q: 4.998972
 83400/100000: episode: 1417, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 181.805, mean reward: 1.818 [1.444, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.626, 10.098], loss: 0.670209, mae: 0.569857, mean_q: 4.998334
 83500/100000: episode: 1418, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 183.089, mean reward: 1.831 [1.460, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.259, 10.151], loss: 0.865981, mae: 0.585664, mean_q: 5.062100
 83600/100000: episode: 1419, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.748, mean reward: 1.897 [1.436, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.697, 10.098], loss: 4.471118, mae: 0.695479, mean_q: 5.136753
 83700/100000: episode: 1420, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 186.745, mean reward: 1.867 [1.447, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.688, 10.246], loss: 0.415491, mae: 0.528038, mean_q: 5.014275
 83800/100000: episode: 1421, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 185.106, mean reward: 1.851 [1.451, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.174, 10.098], loss: 2.067953, mae: 0.608422, mean_q: 5.034973
 83900/100000: episode: 1422, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 179.767, mean reward: 1.798 [1.445, 2.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.834, 10.098], loss: 0.578704, mae: 0.516790, mean_q: 4.863126
 84000/100000: episode: 1423, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 174.903, mean reward: 1.749 [1.445, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.792, 10.109], loss: 0.528405, mae: 0.490901, mean_q: 4.886983
 84100/100000: episode: 1424, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 177.543, mean reward: 1.775 [1.477, 2.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.107, 10.098], loss: 0.612086, mae: 0.486719, mean_q: 4.877919
 84200/100000: episode: 1425, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 191.373, mean reward: 1.914 [1.446, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.719, 10.098], loss: 1.908965, mae: 0.557753, mean_q: 4.859998
 84300/100000: episode: 1426, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 199.266, mean reward: 1.993 [1.458, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.360, 10.098], loss: 2.941164, mae: 0.586625, mean_q: 4.799790
 84400/100000: episode: 1427, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 210.559, mean reward: 2.106 [1.464, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.287, 10.405], loss: 3.221933, mae: 0.649530, mean_q: 4.772943
 84500/100000: episode: 1428, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 184.023, mean reward: 1.840 [1.472, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.592, 10.098], loss: 3.049029, mae: 0.593478, mean_q: 4.675393
 84600/100000: episode: 1429, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 225.254, mean reward: 2.253 [1.496, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.786, 10.098], loss: 3.352884, mae: 0.647754, mean_q: 4.825391
 84700/100000: episode: 1430, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 190.134, mean reward: 1.901 [1.484, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.860, 10.249], loss: 0.663712, mae: 0.478995, mean_q: 4.683333
 84800/100000: episode: 1431, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 180.908, mean reward: 1.809 [1.475, 2.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.489, 10.204], loss: 1.980313, mae: 0.555100, mean_q: 4.671673
 84900/100000: episode: 1432, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 186.900, mean reward: 1.869 [1.468, 2.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.690, 10.100], loss: 2.380555, mae: 0.574333, mean_q: 4.636146
 85000/100000: episode: 1433, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 182.145, mean reward: 1.821 [1.462, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.388, 10.098], loss: 2.861215, mae: 0.528338, mean_q: 4.532413
 85100/100000: episode: 1434, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 200.561, mean reward: 2.006 [1.479, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.400, 10.277], loss: 3.025163, mae: 0.573208, mean_q: 4.616632
 85200/100000: episode: 1435, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 187.537, mean reward: 1.875 [1.455, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.249, 10.098], loss: 0.582637, mae: 0.450898, mean_q: 4.430993
 85300/100000: episode: 1436, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 190.966, mean reward: 1.910 [1.448, 4.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.895, 10.257], loss: 0.652165, mae: 0.447299, mean_q: 4.412534
 85400/100000: episode: 1437, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 185.067, mean reward: 1.851 [1.466, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.260, 10.197], loss: 2.859331, mae: 0.499741, mean_q: 4.420982
 85500/100000: episode: 1438, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.825, mean reward: 1.888 [1.447, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.457, 10.234], loss: 0.524036, mae: 0.404270, mean_q: 4.318457
 85600/100000: episode: 1439, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 186.274, mean reward: 1.863 [1.446, 3.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.945, 10.098], loss: 0.664067, mae: 0.419556, mean_q: 4.289181
 85700/100000: episode: 1440, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 188.189, mean reward: 1.882 [1.438, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.242, 10.330], loss: 4.323132, mae: 0.544009, mean_q: 4.282807
 85800/100000: episode: 1441, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 200.894, mean reward: 2.009 [1.519, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.835, 10.241], loss: 2.724524, mae: 0.448763, mean_q: 4.148006
 85900/100000: episode: 1442, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 177.040, mean reward: 1.770 [1.455, 2.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.194, 10.098], loss: 0.272024, mae: 0.361529, mean_q: 3.987595
 86000/100000: episode: 1443, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 185.819, mean reward: 1.858 [1.464, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.363, 10.098], loss: 1.328329, mae: 0.353676, mean_q: 3.949069
 86100/100000: episode: 1444, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 187.451, mean reward: 1.875 [1.496, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.100, 10.098], loss: 0.226963, mae: 0.317991, mean_q: 3.802786
 86200/100000: episode: 1445, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 249.948, mean reward: 2.499 [1.450, 5.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.739, 10.163], loss: 0.092957, mae: 0.291111, mean_q: 3.806635
 86300/100000: episode: 1446, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 238.743, mean reward: 2.387 [1.463, 7.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.028, 10.098], loss: 0.081918, mae: 0.280707, mean_q: 3.781652
 86400/100000: episode: 1447, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.586, mean reward: 1.906 [1.518, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.535, 10.255], loss: 0.087099, mae: 0.280314, mean_q: 3.798944
 86500/100000: episode: 1448, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 203.843, mean reward: 2.038 [1.449, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.025, 10.248], loss: 0.079710, mae: 0.279308, mean_q: 3.795002
 86600/100000: episode: 1449, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 181.932, mean reward: 1.819 [1.453, 3.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.365, 10.214], loss: 0.072109, mae: 0.267049, mean_q: 3.795662
 86700/100000: episode: 1450, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 201.781, mean reward: 2.018 [1.482, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.322, 10.415], loss: 0.069812, mae: 0.269877, mean_q: 3.798663
 86800/100000: episode: 1451, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.571, mean reward: 1.926 [1.458, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.725, 10.324], loss: 0.075700, mae: 0.275530, mean_q: 3.802525
 86900/100000: episode: 1452, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 178.843, mean reward: 1.788 [1.440, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.677, 10.114], loss: 0.071393, mae: 0.265539, mean_q: 3.791058
 87000/100000: episode: 1453, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 185.013, mean reward: 1.850 [1.453, 4.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.174, 10.125], loss: 0.070993, mae: 0.269868, mean_q: 3.804765
 87100/100000: episode: 1454, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.565, mean reward: 1.886 [1.486, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.453, 10.228], loss: 0.070352, mae: 0.266042, mean_q: 3.780887
 87200/100000: episode: 1455, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 184.602, mean reward: 1.846 [1.433, 4.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.744, 10.098], loss: 0.078665, mae: 0.276661, mean_q: 3.809563
 87300/100000: episode: 1456, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 195.020, mean reward: 1.950 [1.490, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.808, 10.098], loss: 0.070133, mae: 0.268115, mean_q: 3.816177
 87400/100000: episode: 1457, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 189.794, mean reward: 1.898 [1.473, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.720, 10.098], loss: 0.062661, mae: 0.261403, mean_q: 3.779143
 87500/100000: episode: 1458, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 193.172, mean reward: 1.932 [1.483, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.894, 10.268], loss: 0.075535, mae: 0.272275, mean_q: 3.806124
 87600/100000: episode: 1459, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 194.650, mean reward: 1.947 [1.460, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.344, 10.261], loss: 0.071843, mae: 0.273412, mean_q: 3.821476
 87700/100000: episode: 1460, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 181.071, mean reward: 1.811 [1.437, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.324, 10.111], loss: 0.069433, mae: 0.260832, mean_q: 3.791326
 87800/100000: episode: 1461, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 191.417, mean reward: 1.914 [1.471, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.030, 10.108], loss: 0.069883, mae: 0.269032, mean_q: 3.797672
 87900/100000: episode: 1462, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 186.409, mean reward: 1.864 [1.460, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.615, 10.307], loss: 0.068602, mae: 0.265458, mean_q: 3.794246
 88000/100000: episode: 1463, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 185.575, mean reward: 1.856 [1.445, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.634, 10.098], loss: 0.069432, mae: 0.267144, mean_q: 3.809054
 88100/100000: episode: 1464, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 182.271, mean reward: 1.823 [1.470, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.654, 10.144], loss: 0.058993, mae: 0.251583, mean_q: 3.772963
 88200/100000: episode: 1465, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 182.101, mean reward: 1.821 [1.471, 2.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.982, 10.237], loss: 0.058680, mae: 0.249161, mean_q: 3.749177
 88300/100000: episode: 1466, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 209.921, mean reward: 2.099 [1.522, 4.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.294, 10.098], loss: 0.065481, mae: 0.261428, mean_q: 3.790794
 88400/100000: episode: 1467, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 201.575, mean reward: 2.016 [1.442, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.400, 10.335], loss: 0.065321, mae: 0.262605, mean_q: 3.769429
 88500/100000: episode: 1468, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 188.805, mean reward: 1.888 [1.503, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.637, 10.141], loss: 0.062673, mae: 0.260688, mean_q: 3.782663
 88600/100000: episode: 1469, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.423, mean reward: 1.964 [1.446, 4.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.096, 10.225], loss: 0.060393, mae: 0.254911, mean_q: 3.803470
 88700/100000: episode: 1470, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 194.207, mean reward: 1.942 [1.465, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.938, 10.098], loss: 0.072788, mae: 0.267136, mean_q: 3.825814
 88800/100000: episode: 1471, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 197.495, mean reward: 1.975 [1.447, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.193, 10.098], loss: 0.066105, mae: 0.264121, mean_q: 3.811339
 88900/100000: episode: 1472, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 211.810, mean reward: 2.118 [1.449, 5.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.846, 10.308], loss: 0.071121, mae: 0.264631, mean_q: 3.799791
 89000/100000: episode: 1473, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.182, mean reward: 1.892 [1.511, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.555, 10.098], loss: 0.068309, mae: 0.263589, mean_q: 3.824731
 89100/100000: episode: 1474, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.546, mean reward: 1.965 [1.451, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.483, 10.379], loss: 0.071952, mae: 0.268100, mean_q: 3.841130
 89200/100000: episode: 1475, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 190.500, mean reward: 1.905 [1.481, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.233, 10.098], loss: 0.068614, mae: 0.265904, mean_q: 3.826493
 89300/100000: episode: 1476, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 187.584, mean reward: 1.876 [1.469, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.902, 10.125], loss: 0.082359, mae: 0.283348, mean_q: 3.849206
 89400/100000: episode: 1477, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 225.557, mean reward: 2.256 [1.541, 4.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.674, 10.098], loss: 0.074381, mae: 0.276089, mean_q: 3.838477
 89500/100000: episode: 1478, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 207.052, mean reward: 2.071 [1.493, 4.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.321, 10.337], loss: 0.071417, mae: 0.268534, mean_q: 3.844636
 89600/100000: episode: 1479, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 182.475, mean reward: 1.825 [1.469, 2.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.328, 10.172], loss: 0.071050, mae: 0.265245, mean_q: 3.843510
 89700/100000: episode: 1480, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 202.417, mean reward: 2.024 [1.485, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.846, 10.239], loss: 0.070704, mae: 0.268123, mean_q: 3.829329
 89800/100000: episode: 1481, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 197.185, mean reward: 1.972 [1.436, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.622, 10.154], loss: 0.068917, mae: 0.260884, mean_q: 3.842167
 89900/100000: episode: 1482, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 184.801, mean reward: 1.848 [1.453, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.853, 10.233], loss: 0.074339, mae: 0.274377, mean_q: 3.851233
 90000/100000: episode: 1483, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 193.240, mean reward: 1.932 [1.452, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.113, 10.250], loss: 0.068369, mae: 0.266705, mean_q: 3.847330
 90100/100000: episode: 1484, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 195.618, mean reward: 1.956 [1.454, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.856, 10.282], loss: 0.067602, mae: 0.269034, mean_q: 3.847946
 90200/100000: episode: 1485, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 228.824, mean reward: 2.288 [1.521, 6.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.687, 10.324], loss: 0.070879, mae: 0.271363, mean_q: 3.846407
 90300/100000: episode: 1486, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 189.968, mean reward: 1.900 [1.455, 2.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.129, 10.098], loss: 0.076725, mae: 0.277240, mean_q: 3.872228
 90400/100000: episode: 1487, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 174.494, mean reward: 1.745 [1.496, 2.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.298, 10.155], loss: 0.077328, mae: 0.273781, mean_q: 3.865494
 90500/100000: episode: 1488, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 183.763, mean reward: 1.838 [1.491, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.111, 10.098], loss: 0.081663, mae: 0.277616, mean_q: 3.863626
 90600/100000: episode: 1489, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 207.076, mean reward: 2.071 [1.446, 5.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.672, 10.256], loss: 0.074863, mae: 0.269753, mean_q: 3.857610
 90700/100000: episode: 1490, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 200.652, mean reward: 2.007 [1.450, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.265, 10.256], loss: 0.076374, mae: 0.273568, mean_q: 3.861962
 90800/100000: episode: 1491, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 186.330, mean reward: 1.863 [1.467, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.893, 10.098], loss: 0.083713, mae: 0.288612, mean_q: 3.858710
 90900/100000: episode: 1492, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 183.348, mean reward: 1.833 [1.471, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.882, 10.098], loss: 0.077963, mae: 0.281080, mean_q: 3.869022
 91000/100000: episode: 1493, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 189.451, mean reward: 1.895 [1.477, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.680, 10.098], loss: 0.081630, mae: 0.279582, mean_q: 3.867999
[Info] 1-TH LEVEL FOUND: 5.81249475479126, Considering 10/90 traces
 91100/100000: episode: 1494, duration: 4.686s, episode steps: 100, steps per second: 21, episode reward: 184.910, mean reward: 1.849 [1.446, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.982, 10.212], loss: 0.074661, mae: 0.274698, mean_q: 3.854923
 91200/100000: episode: 1495, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 193.356, mean reward: 1.934 [1.458, 4.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.588, 10.100], loss: 0.082759, mae: 0.282436, mean_q: 3.858408
 91226/100000: episode: 1496, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 76.725, mean reward: 2.951 [2.040, 4.560], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.731, 10.498], loss: 0.071068, mae: 0.274327, mean_q: 3.830715
 91326/100000: episode: 1497, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.942, mean reward: 1.949 [1.489, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-0.339, 10.182], loss: 0.077516, mae: 0.276249, mean_q: 3.841690
 91353/100000: episode: 1498, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 75.835, mean reward: 2.809 [2.172, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.259, 10.449], loss: 0.075137, mae: 0.277233, mean_q: 3.844059
 91379/100000: episode: 1499, duration: 0.146s, episode steps: 26, steps per second: 179, episode reward: 62.548, mean reward: 2.406 [1.747, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.420, 10.358], loss: 0.066083, mae: 0.259137, mean_q: 3.855972
 91401/100000: episode: 1500, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 56.294, mean reward: 2.559 [1.699, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.512, 10.100], loss: 0.099449, mae: 0.287306, mean_q: 3.819388
 91417/100000: episode: 1501, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 51.679, mean reward: 3.230 [2.085, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.454, 10.332], loss: 0.056717, mae: 0.242645, mean_q: 3.846660
 91451/100000: episode: 1502, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 102.296, mean reward: 3.009 [2.104, 4.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.221, 10.426], loss: 0.070828, mae: 0.272342, mean_q: 3.844517
 91468/100000: episode: 1503, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 39.784, mean reward: 2.340 [1.861, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.403], loss: 0.089767, mae: 0.293103, mean_q: 3.863689
 91485/100000: episode: 1504, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 46.195, mean reward: 2.717 [2.272, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.388], loss: 0.074398, mae: 0.275852, mean_q: 3.861648
 91512/100000: episode: 1505, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 56.598, mean reward: 2.096 [1.436, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.845, 10.135], loss: 0.082535, mae: 0.274989, mean_q: 3.838430
 91544/100000: episode: 1506, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 93.003, mean reward: 2.906 [2.014, 6.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.919, 10.100], loss: 0.087308, mae: 0.291189, mean_q: 3.868370
 91566/100000: episode: 1507, duration: 0.114s, episode steps: 22, steps per second: 192, episode reward: 56.252, mean reward: 2.557 [1.905, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.881, 10.100], loss: 0.070845, mae: 0.265990, mean_q: 3.893091
 91592/100000: episode: 1508, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 71.182, mean reward: 2.738 [2.082, 3.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.289, 10.385], loss: 0.074641, mae: 0.282004, mean_q: 3.905319
 91690/100000: episode: 1509, duration: 0.526s, episode steps: 98, steps per second: 186, episode reward: 181.061, mean reward: 1.848 [1.466, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-0.782, 10.188], loss: 0.075100, mae: 0.269979, mean_q: 3.905397
 91726/100000: episode: 1510, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 110.962, mean reward: 3.082 [2.116, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.765, 10.100], loss: 0.075078, mae: 0.267906, mean_q: 3.916777
 91748/100000: episode: 1511, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 71.353, mean reward: 3.243 [2.458, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.347, 10.100], loss: 0.079875, mae: 0.281216, mean_q: 3.917214
 91848/100000: episode: 1512, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 199.956, mean reward: 2.000 [1.489, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.040, 10.240], loss: 0.077394, mae: 0.280897, mean_q: 3.898000
 91870/100000: episode: 1513, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 88.762, mean reward: 4.035 [1.995, 7.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.250, 10.100], loss: 0.079115, mae: 0.279438, mean_q: 3.967970
 91886/100000: episode: 1514, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 44.615, mean reward: 2.788 [2.287, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.419], loss: 0.059129, mae: 0.267583, mean_q: 3.922749
 91902/100000: episode: 1515, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 54.058, mean reward: 3.379 [2.200, 10.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.220, 10.346], loss: 0.078926, mae: 0.294293, mean_q: 3.966545
 92002/100000: episode: 1516, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.694, mean reward: 1.897 [1.496, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.984, 10.100], loss: 0.101023, mae: 0.292938, mean_q: 3.954851
 92029/100000: episode: 1517, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 60.713, mean reward: 2.249 [1.662, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.292, 10.430], loss: 0.088383, mae: 0.291033, mean_q: 3.986792
 92129/100000: episode: 1518, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.248, mean reward: 1.862 [1.434, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-0.182, 10.183], loss: 0.077397, mae: 0.275499, mean_q: 3.943391
 92163/100000: episode: 1519, duration: 0.197s, episode steps: 34, steps per second: 173, episode reward: 130.655, mean reward: 3.843 [2.552, 5.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.200, 10.535], loss: 0.122790, mae: 0.314662, mean_q: 4.007306
 92190/100000: episode: 1520, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 58.276, mean reward: 2.158 [1.616, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.298, 10.253], loss: 0.097074, mae: 0.303217, mean_q: 4.015673
 92224/100000: episode: 1521, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 119.350, mean reward: 3.510 [2.067, 5.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.591, 10.355], loss: 0.074046, mae: 0.271017, mean_q: 3.965902
 92256/100000: episode: 1522, duration: 0.170s, episode steps: 32, steps per second: 189, episode reward: 97.631, mean reward: 3.051 [1.945, 4.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.299, 10.100], loss: 0.120184, mae: 0.311316, mean_q: 4.055522
 92356/100000: episode: 1523, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.143, mean reward: 1.881 [1.478, 3.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.448 [-0.183, 10.100], loss: 0.088247, mae: 0.296125, mean_q: 4.034510
 92383/100000: episode: 1524, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 63.515, mean reward: 2.352 [1.721, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.578, 10.317], loss: 0.089269, mae: 0.290650, mean_q: 3.982193
 92409/100000: episode: 1525, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 54.600, mean reward: 2.100 [1.510, 2.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.100, 10.236], loss: 0.088787, mae: 0.299920, mean_q: 4.038549
 92436/100000: episode: 1526, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 58.906, mean reward: 2.182 [1.515, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.421, 10.101], loss: 0.164801, mae: 0.328290, mean_q: 4.059523
 92463/100000: episode: 1527, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 61.779, mean reward: 2.288 [1.523, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.129], loss: 0.095385, mae: 0.302845, mean_q: 3.991375
 92499/100000: episode: 1528, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 76.664, mean reward: 2.130 [1.524, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-1.406, 10.309], loss: 0.117414, mae: 0.297603, mean_q: 3.989559
 92526/100000: episode: 1529, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 70.516, mean reward: 2.612 [1.914, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.773, 10.292], loss: 0.110859, mae: 0.323019, mean_q: 4.098713
 92560/100000: episode: 1530, duration: 0.195s, episode steps: 34, steps per second: 175, episode reward: 111.680, mean reward: 3.285 [1.459, 6.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.211, 10.185], loss: 0.088007, mae: 0.298601, mean_q: 4.037892
 92592/100000: episode: 1531, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 92.906, mean reward: 2.903 [2.003, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.767, 10.100], loss: 0.165377, mae: 0.326574, mean_q: 4.080190
 92614/100000: episode: 1532, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 67.802, mean reward: 3.082 [2.129, 4.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.056, 10.100], loss: 0.084666, mae: 0.293480, mean_q: 4.088637
 92712/100000: episode: 1533, duration: 0.515s, episode steps: 98, steps per second: 190, episode reward: 187.385, mean reward: 1.912 [1.452, 3.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.478 [-0.345, 10.100], loss: 0.115040, mae: 0.316039, mean_q: 4.076482
 92810/100000: episode: 1534, duration: 0.505s, episode steps: 98, steps per second: 194, episode reward: 179.806, mean reward: 1.835 [1.457, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.472 [-1.312, 10.124], loss: 0.128294, mae: 0.317067, mean_q: 4.083351
 92836/100000: episode: 1535, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 79.385, mean reward: 3.053 [2.334, 5.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.904, 10.386], loss: 0.095659, mae: 0.288985, mean_q: 4.086962
 92934/100000: episode: 1536, duration: 0.509s, episode steps: 98, steps per second: 192, episode reward: 195.920, mean reward: 1.999 [1.473, 4.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-0.653, 10.100], loss: 0.101356, mae: 0.311634, mean_q: 4.085062
 93034/100000: episode: 1537, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 185.822, mean reward: 1.858 [1.451, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.452 [-0.715, 10.100], loss: 0.116713, mae: 0.319922, mean_q: 4.125188
 93134/100000: episode: 1538, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 194.512, mean reward: 1.945 [1.430, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.454 [-3.003, 10.152], loss: 0.108641, mae: 0.313197, mean_q: 4.099294
 93151/100000: episode: 1539, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 44.332, mean reward: 2.608 [2.229, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.035, 10.404], loss: 0.127923, mae: 0.341903, mean_q: 4.132594
 93167/100000: episode: 1540, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 51.841, mean reward: 3.240 [2.540, 4.761], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.279, 10.386], loss: 0.101449, mae: 0.307133, mean_q: 4.070150
 93184/100000: episode: 1541, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 49.932, mean reward: 2.937 [2.584, 3.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.485], loss: 0.114732, mae: 0.331203, mean_q: 4.131005
 93218/100000: episode: 1542, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 92.909, mean reward: 2.733 [1.775, 5.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-1.056, 10.287], loss: 0.135115, mae: 0.304352, mean_q: 4.135409
 93316/100000: episode: 1543, duration: 0.485s, episode steps: 98, steps per second: 202, episode reward: 186.489, mean reward: 1.903 [1.435, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.476 [-0.234, 10.208], loss: 0.104412, mae: 0.314103, mean_q: 4.119039
 93348/100000: episode: 1544, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 79.011, mean reward: 2.469 [1.985, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.447, 10.100], loss: 0.111801, mae: 0.326175, mean_q: 4.119724
 93446/100000: episode: 1545, duration: 0.540s, episode steps: 98, steps per second: 182, episode reward: 188.273, mean reward: 1.921 [1.466, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.822, 10.206], loss: 0.118128, mae: 0.330356, mean_q: 4.157841
 93544/100000: episode: 1546, duration: 0.512s, episode steps: 98, steps per second: 191, episode reward: 183.363, mean reward: 1.871 [1.477, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-1.006, 10.333], loss: 0.102799, mae: 0.308915, mean_q: 4.133180
 93644/100000: episode: 1547, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 180.590, mean reward: 1.806 [1.453, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.450 [-0.287, 10.175], loss: 0.119824, mae: 0.318991, mean_q: 4.104603
 93666/100000: episode: 1548, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 54.848, mean reward: 2.493 [1.945, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.296, 10.100], loss: 0.104904, mae: 0.331645, mean_q: 4.110998
 93766/100000: episode: 1549, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.353, mean reward: 1.904 [1.478, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.077, 10.100], loss: 0.167612, mae: 0.332981, mean_q: 4.136351
 93783/100000: episode: 1550, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 41.130, mean reward: 2.419 [2.029, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.401], loss: 0.076049, mae: 0.284909, mean_q: 4.075865
 93799/100000: episode: 1551, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 46.540, mean reward: 2.909 [2.137, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.095, 10.460], loss: 0.171432, mae: 0.331409, mean_q: 4.179601
 93899/100000: episode: 1552, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 198.783, mean reward: 1.988 [1.477, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.570, 10.100], loss: 0.135130, mae: 0.333918, mean_q: 4.114645
 93926/100000: episode: 1553, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 63.886, mean reward: 2.366 [1.639, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.055, 10.265], loss: 0.115238, mae: 0.325135, mean_q: 4.085731
 93943/100000: episode: 1554, duration: 0.085s, episode steps: 17, steps per second: 200, episode reward: 62.670, mean reward: 3.686 [2.366, 4.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.351, 10.553], loss: 0.112105, mae: 0.336478, mean_q: 4.152871
 93965/100000: episode: 1555, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 63.056, mean reward: 2.866 [2.068, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.024, 10.100], loss: 0.106941, mae: 0.323158, mean_q: 4.093974
 93987/100000: episode: 1556, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 65.779, mean reward: 2.990 [1.538, 5.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.642, 10.131], loss: 0.100599, mae: 0.305382, mean_q: 4.148767
 94087/100000: episode: 1557, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 199.259, mean reward: 1.993 [1.469, 4.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-1.315, 10.100], loss: 0.114340, mae: 0.331358, mean_q: 4.136947
 94185/100000: episode: 1558, duration: 0.545s, episode steps: 98, steps per second: 180, episode reward: 196.570, mean reward: 2.006 [1.478, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-2.073, 10.165], loss: 0.105055, mae: 0.316776, mean_q: 4.145680
 94217/100000: episode: 1559, duration: 0.181s, episode steps: 32, steps per second: 176, episode reward: 77.112, mean reward: 2.410 [1.810, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-1.593, 10.100], loss: 0.125188, mae: 0.338368, mean_q: 4.169456
 94234/100000: episode: 1560, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 62.800, mean reward: 3.694 [2.337, 5.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.727, 10.488], loss: 0.117644, mae: 0.333736, mean_q: 4.229593
 94266/100000: episode: 1561, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 92.863, mean reward: 2.902 [2.013, 4.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.530, 10.100], loss: 0.108725, mae: 0.332095, mean_q: 4.199881
 94293/100000: episode: 1562, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 62.023, mean reward: 2.297 [1.886, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.105, 10.349], loss: 0.102813, mae: 0.308984, mean_q: 4.093587
 94329/100000: episode: 1563, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 129.050, mean reward: 3.585 [2.357, 8.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.269, 10.100], loss: 0.130100, mae: 0.334228, mean_q: 4.154739
 94356/100000: episode: 1564, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 78.519, mean reward: 2.908 [2.218, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.789, 10.381], loss: 0.125969, mae: 0.335547, mean_q: 4.202998
 94378/100000: episode: 1565, duration: 0.127s, episode steps: 22, steps per second: 174, episode reward: 74.987, mean reward: 3.408 [2.536, 4.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.354, 10.100], loss: 0.156147, mae: 0.365350, mean_q: 4.256463
 94400/100000: episode: 1566, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 71.027, mean reward: 3.229 [1.873, 7.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.660, 10.100], loss: 0.215368, mae: 0.365987, mean_q: 4.211753
 94427/100000: episode: 1567, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 61.244, mean reward: 2.268 [1.736, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.313, 10.284], loss: 0.117704, mae: 0.344270, mean_q: 4.268812
 94459/100000: episode: 1568, duration: 0.179s, episode steps: 32, steps per second: 178, episode reward: 96.025, mean reward: 3.001 [1.904, 5.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.051, 10.100], loss: 0.149685, mae: 0.357345, mean_q: 4.266467
 94559/100000: episode: 1569, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 204.331, mean reward: 2.043 [1.508, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.453 [-1.086, 10.391], loss: 0.132056, mae: 0.340570, mean_q: 4.244249
 94593/100000: episode: 1570, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 106.899, mean reward: 3.144 [2.267, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.472, 10.340], loss: 0.094348, mae: 0.309189, mean_q: 4.206330
 94629/100000: episode: 1571, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 117.793, mean reward: 3.272 [2.133, 5.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-0.204, 10.100], loss: 0.148066, mae: 0.359476, mean_q: 4.307333
 94663/100000: episode: 1572, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 148.947, mean reward: 4.381 [2.602, 7.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.418, 10.426], loss: 0.115367, mae: 0.336718, mean_q: 4.288671
 94690/100000: episode: 1573, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 60.741, mean reward: 2.250 [1.512, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.284, 10.262], loss: 0.157519, mae: 0.375981, mean_q: 4.291890
 94722/100000: episode: 1574, duration: 0.158s, episode steps: 32, steps per second: 203, episode reward: 106.525, mean reward: 3.329 [2.035, 5.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.706, 10.100], loss: 0.129730, mae: 0.354928, mean_q: 4.372357
 94758/100000: episode: 1575, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 113.736, mean reward: 3.159 [1.959, 6.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.009, 10.100], loss: 0.175113, mae: 0.376319, mean_q: 4.367255
 94856/100000: episode: 1576, duration: 0.531s, episode steps: 98, steps per second: 185, episode reward: 217.286, mean reward: 2.217 [1.462, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.466 [-0.985, 10.100], loss: 0.133003, mae: 0.355501, mean_q: 4.359841
 94954/100000: episode: 1577, duration: 0.518s, episode steps: 98, steps per second: 189, episode reward: 185.627, mean reward: 1.894 [1.472, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.471 [-1.116, 10.283], loss: 0.133422, mae: 0.355974, mean_q: 4.340220
 94976/100000: episode: 1578, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 52.697, mean reward: 2.395 [1.837, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.260, 10.100], loss: 0.166690, mae: 0.389738, mean_q: 4.412623
 95010/100000: episode: 1579, duration: 0.193s, episode steps: 34, steps per second: 177, episode reward: 88.983, mean reward: 2.617 [1.602, 4.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-1.398, 10.141], loss: 0.132356, mae: 0.349341, mean_q: 4.321770
 95037/100000: episode: 1580, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 62.000, mean reward: 2.296 [1.654, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.282, 10.235], loss: 0.138031, mae: 0.365968, mean_q: 4.385795
 95135/100000: episode: 1581, duration: 0.524s, episode steps: 98, steps per second: 187, episode reward: 197.632, mean reward: 2.017 [1.434, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.476 [-0.713, 10.100], loss: 0.167180, mae: 0.378475, mean_q: 4.386230
 95151/100000: episode: 1582, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 45.908, mean reward: 2.869 [1.882, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.279], loss: 0.175667, mae: 0.397355, mean_q: 4.494252
 95187/100000: episode: 1583, duration: 0.191s, episode steps: 36, steps per second: 189, episode reward: 107.633, mean reward: 2.990 [1.988, 4.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.183, 10.100], loss: 0.160316, mae: 0.383109, mean_q: 4.376382
[Info] 2-TH LEVEL FOUND: 7.1963887214660645, Considering 10/90 traces
 95209/100000: episode: 1584, duration: 4.325s, episode steps: 22, steps per second: 5, episode reward: 73.951, mean reward: 3.361 [2.410, 8.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.730, 10.100], loss: 0.174783, mae: 0.382669, mean_q: 4.349407
 95239/100000: episode: 1585, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 99.076, mean reward: 3.303 [2.484, 5.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.976, 10.423], loss: 0.137929, mae: 0.374717, mean_q: 4.453506
 95267/100000: episode: 1586, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 88.254, mean reward: 3.152 [2.378, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.456, 10.304], loss: 0.144951, mae: 0.369917, mean_q: 4.443155
 95283/100000: episode: 1587, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 69.259, mean reward: 4.329 [3.275, 5.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.736, 10.100], loss: 0.284820, mae: 0.425701, mean_q: 4.517410
 95314/100000: episode: 1588, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 99.794, mean reward: 3.219 [2.430, 5.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.663, 10.476], loss: 0.189108, mae: 0.399117, mean_q: 4.528250
 95331/100000: episode: 1589, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 48.765, mean reward: 2.869 [2.111, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.560, 10.100], loss: 0.152225, mae: 0.372123, mean_q: 4.479109
 95348/100000: episode: 1590, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 40.255, mean reward: 2.368 [1.871, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.328, 10.100], loss: 0.211924, mae: 0.436560, mean_q: 4.553622
 95375/100000: episode: 1591, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 116.686, mean reward: 4.322 [2.930, 7.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.510], loss: 0.145355, mae: 0.357619, mean_q: 4.461564
 95404/100000: episode: 1592, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 91.829, mean reward: 3.167 [1.538, 6.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.219, 10.221], loss: 0.211996, mae: 0.409243, mean_q: 4.536710
 95412/100000: episode: 1593, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 29.766, mean reward: 3.721 [2.901, 4.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.441, 10.100], loss: 0.124996, mae: 0.360100, mean_q: 4.508431
 95437/100000: episode: 1594, duration: 0.152s, episode steps: 25, steps per second: 164, episode reward: 87.986, mean reward: 3.519 [2.317, 5.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.299, 10.100], loss: 0.160003, mae: 0.386973, mean_q: 4.521440
 95464/100000: episode: 1595, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 161.555, mean reward: 5.984 [3.818, 13.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.671], loss: 0.179162, mae: 0.420476, mean_q: 4.588154
 95472/100000: episode: 1596, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 29.883, mean reward: 3.735 [3.323, 4.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.484, 10.100], loss: 0.129795, mae: 0.363345, mean_q: 4.531048
 95500/100000: episode: 1597, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 79.308, mean reward: 2.832 [2.068, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.511, 10.345], loss: 0.193781, mae: 0.410410, mean_q: 4.654235
 95525/100000: episode: 1598, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 91.912, mean reward: 3.676 [2.601, 6.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.849, 10.100], loss: 0.291570, mae: 0.429606, mean_q: 4.616582
 95533/100000: episode: 1599, duration: 0.062s, episode steps: 8, steps per second: 128, episode reward: 30.022, mean reward: 3.753 [3.298, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.575, 10.100], loss: 0.173019, mae: 0.417183, mean_q: 4.628716
 95549/100000: episode: 1600, duration: 0.103s, episode steps: 16, steps per second: 155, episode reward: 56.582, mean reward: 3.536 [2.582, 5.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.500, 10.100], loss: 0.204355, mae: 0.421094, mean_q: 4.533982
 95576/100000: episode: 1601, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 98.025, mean reward: 3.631 [2.805, 5.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.508, 10.434], loss: 0.191507, mae: 0.407408, mean_q: 4.575955
 95604/100000: episode: 1602, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 123.258, mean reward: 4.402 [2.745, 6.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.769, 10.629], loss: 0.263736, mae: 0.424651, mean_q: 4.617993
 95632/100000: episode: 1603, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 78.355, mean reward: 2.798 [1.571, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.524, 10.329], loss: 0.174483, mae: 0.405971, mean_q: 4.628795
 95649/100000: episode: 1604, duration: 0.113s, episode steps: 17, steps per second: 151, episode reward: 75.200, mean reward: 4.424 [3.268, 6.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.371, 10.100], loss: 0.155930, mae: 0.385365, mean_q: 4.641332
 95657/100000: episode: 1605, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 32.249, mean reward: 4.031 [3.109, 7.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.515, 10.100], loss: 0.170202, mae: 0.386067, mean_q: 4.418153
 95687/100000: episode: 1606, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 93.386, mean reward: 3.113 [1.844, 6.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.035, 10.348], loss: 0.177569, mae: 0.410756, mean_q: 4.718565
 95703/100000: episode: 1607, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 49.870, mean reward: 3.117 [2.417, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.420, 10.100], loss: 0.142915, mae: 0.376241, mean_q: 4.620151
 95719/100000: episode: 1608, duration: 0.087s, episode steps: 16, steps per second: 183, episode reward: 49.052, mean reward: 3.066 [2.520, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.240, 10.100], loss: 0.176632, mae: 0.398913, mean_q: 4.653880
 95750/100000: episode: 1609, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 102.502, mean reward: 3.307 [2.547, 4.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.035, 10.469], loss: 0.227669, mae: 0.433595, mean_q: 4.696454
 95779/100000: episode: 1610, duration: 0.142s, episode steps: 29, steps per second: 204, episode reward: 80.910, mean reward: 2.790 [1.737, 4.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.759, 10.240], loss: 0.236941, mae: 0.422917, mean_q: 4.709342
 95810/100000: episode: 1611, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 111.089, mean reward: 3.584 [1.818, 8.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.788, 10.333], loss: 0.200745, mae: 0.418702, mean_q: 4.754243
 95826/100000: episode: 1612, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 50.844, mean reward: 3.178 [2.704, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.368, 10.100], loss: 0.177627, mae: 0.422302, mean_q: 4.736920
 95854/100000: episode: 1613, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 117.483, mean reward: 4.196 [2.813, 9.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.082, 10.480], loss: 0.208995, mae: 0.424010, mean_q: 4.784351
 95884/100000: episode: 1614, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 121.499, mean reward: 4.050 [2.409, 6.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.143, 10.471], loss: 0.262313, mae: 0.437198, mean_q: 4.780963
 95912/100000: episode: 1615, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 72.089, mean reward: 2.575 [1.939, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.235, 10.428], loss: 0.171800, mae: 0.416587, mean_q: 4.872134
 95939/100000: episode: 1616, duration: 0.166s, episode steps: 27, steps per second: 162, episode reward: 103.605, mean reward: 3.837 [2.184, 8.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.826, 10.320], loss: 0.250967, mae: 0.445319, mean_q: 4.889599
 95968/100000: episode: 1617, duration: 0.167s, episode steps: 29, steps per second: 173, episode reward: 68.562, mean reward: 2.364 [1.723, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.058, 10.254], loss: 0.282482, mae: 0.463416, mean_q: 4.843254
 95997/100000: episode: 1618, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 133.951, mean reward: 4.619 [2.483, 12.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.224, 10.485], loss: 0.314271, mae: 0.497197, mean_q: 4.943063
 96014/100000: episode: 1619, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 67.015, mean reward: 3.942 [3.032, 5.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.494, 10.100], loss: 0.179356, mae: 0.430342, mean_q: 4.917302
 96039/100000: episode: 1620, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 115.898, mean reward: 4.636 [2.386, 16.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.080, 10.100], loss: 0.216657, mae: 0.432744, mean_q: 4.831083
 96056/100000: episode: 1621, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 50.879, mean reward: 2.993 [2.310, 4.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.281, 10.100], loss: 0.226864, mae: 0.459996, mean_q: 4.980736
 96083/100000: episode: 1622, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 85.873, mean reward: 3.180 [2.502, 5.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.656, 10.484], loss: 0.272749, mae: 0.459150, mean_q: 4.952860
 96112/100000: episode: 1623, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 134.144, mean reward: 4.626 [3.072, 6.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.632, 10.405], loss: 0.293072, mae: 0.458043, mean_q: 4.922622
 96128/100000: episode: 1624, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 50.811, mean reward: 3.176 [2.677, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.370, 10.100], loss: 0.209099, mae: 0.430480, mean_q: 4.947816
 96156/100000: episode: 1625, duration: 0.163s, episode steps: 28, steps per second: 172, episode reward: 77.274, mean reward: 2.760 [2.234, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.391], loss: 0.294403, mae: 0.476375, mean_q: 5.013236
 96185/100000: episode: 1626, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 72.117, mean reward: 2.487 [1.460, 5.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.407, 10.109], loss: 0.334807, mae: 0.467914, mean_q: 4.938449
 96213/100000: episode: 1627, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 74.391, mean reward: 2.657 [1.783, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.911, 10.268], loss: 0.252219, mae: 0.459274, mean_q: 4.988395
 96242/100000: episode: 1628, duration: 0.160s, episode steps: 29, steps per second: 181, episode reward: 73.041, mean reward: 2.519 [1.661, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.124, 10.292], loss: 0.349001, mae: 0.498970, mean_q: 4.934248
 96269/100000: episode: 1629, duration: 0.140s, episode steps: 27, steps per second: 192, episode reward: 80.163, mean reward: 2.969 [1.783, 4.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.265, 10.300], loss: 0.292582, mae: 0.496390, mean_q: 4.986168
 96294/100000: episode: 1630, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 89.840, mean reward: 3.594 [2.364, 5.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.014, 10.100], loss: 0.305383, mae: 0.486975, mean_q: 4.997977
 96322/100000: episode: 1631, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 72.434, mean reward: 2.587 [1.549, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.052, 10.158], loss: 0.241247, mae: 0.457042, mean_q: 4.938382
 96347/100000: episode: 1632, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 109.876, mean reward: 4.395 [2.786, 14.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.204, 10.100], loss: 0.289829, mae: 0.453931, mean_q: 4.999160
 96375/100000: episode: 1633, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 81.754, mean reward: 2.920 [1.987, 4.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.049, 10.278], loss: 0.186878, mae: 0.433650, mean_q: 5.009735
 96400/100000: episode: 1634, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 83.858, mean reward: 3.354 [1.958, 7.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.765, 10.100], loss: 0.225202, mae: 0.462435, mean_q: 5.003250
 96408/100000: episode: 1635, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 27.900, mean reward: 3.487 [3.016, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.461, 10.100], loss: 0.214584, mae: 0.447780, mean_q: 4.906052
 96437/100000: episode: 1636, duration: 0.168s, episode steps: 29, steps per second: 172, episode reward: 112.000, mean reward: 3.862 [2.365, 7.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.441, 10.540], loss: 0.291643, mae: 0.461415, mean_q: 5.024528
 96454/100000: episode: 1637, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 89.685, mean reward: 5.276 [3.077, 10.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.321, 10.100], loss: 0.337116, mae: 0.505461, mean_q: 5.089263
 96483/100000: episode: 1638, duration: 0.171s, episode steps: 29, steps per second: 169, episode reward: 109.527, mean reward: 3.777 [2.230, 6.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.283, 10.305], loss: 0.327611, mae: 0.517667, mean_q: 5.115644
 96491/100000: episode: 1639, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 104.231, mean reward: 13.029 [2.992, 55.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.530, 10.100], loss: 0.494295, mae: 0.523816, mean_q: 5.048344
 96519/100000: episode: 1640, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 112.268, mean reward: 4.010 [2.567, 5.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.187, 10.424], loss: 0.293612, mae: 0.506230, mean_q: 5.009263
 96547/100000: episode: 1641, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 109.254, mean reward: 3.902 [2.759, 6.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.504, 10.492], loss: 0.344074, mae: 0.510430, mean_q: 5.028783
 96564/100000: episode: 1642, duration: 0.084s, episode steps: 17, steps per second: 203, episode reward: 61.078, mean reward: 3.593 [2.523, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.204, 10.100], loss: 0.339294, mae: 0.529791, mean_q: 5.274825
 96592/100000: episode: 1643, duration: 0.146s, episode steps: 28, steps per second: 191, episode reward: 106.514, mean reward: 3.804 [2.353, 8.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.171, 10.405], loss: 0.275714, mae: 0.503979, mean_q: 5.248242
 96620/100000: episode: 1644, duration: 0.149s, episode steps: 28, steps per second: 187, episode reward: 154.486, mean reward: 5.517 [2.520, 11.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.035, 10.448], loss: 1.886420, mae: 0.576311, mean_q: 5.174350
 96647/100000: episode: 1645, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 83.758, mean reward: 3.102 [2.030, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.430, 10.347], loss: 0.266023, mae: 0.518789, mean_q: 5.145641
 96677/100000: episode: 1646, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 185.974, mean reward: 6.199 [3.318, 24.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.154, 10.450], loss: 0.380858, mae: 0.527598, mean_q: 5.191984
 96702/100000: episode: 1647, duration: 0.126s, episode steps: 25, steps per second: 199, episode reward: 72.526, mean reward: 2.901 [2.196, 7.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.436, 10.100], loss: 2.229860, mae: 0.635289, mean_q: 5.172410
 96718/100000: episode: 1648, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 66.060, mean reward: 4.129 [2.759, 8.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.444, 10.100], loss: 0.786857, mae: 0.615856, mean_q: 5.309967
 96747/100000: episode: 1649, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 89.643, mean reward: 3.091 [2.258, 5.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.441], loss: 0.320316, mae: 0.544036, mean_q: 5.257942
 96764/100000: episode: 1650, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 70.242, mean reward: 4.132 [2.904, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.594, 10.100], loss: 0.267425, mae: 0.488591, mean_q: 5.356571
 96794/100000: episode: 1651, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 110.117, mean reward: 3.671 [1.910, 10.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.283], loss: 0.273520, mae: 0.486373, mean_q: 5.242006
 96821/100000: episode: 1652, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 75.447, mean reward: 2.794 [2.094, 5.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.724, 10.315], loss: 0.554903, mae: 0.560248, mean_q: 5.357002
 96829/100000: episode: 1653, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 32.751, mean reward: 4.094 [3.468, 4.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.365, 10.100], loss: 0.939425, mae: 0.555712, mean_q: 5.239398
 96856/100000: episode: 1654, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 69.063, mean reward: 2.558 [1.570, 5.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.261], loss: 0.298365, mae: 0.525079, mean_q: 5.199739
 96884/100000: episode: 1655, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 81.956, mean reward: 2.927 [2.013, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.309, 10.466], loss: 0.656698, mae: 0.592052, mean_q: 5.336988
 96912/100000: episode: 1656, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 76.628, mean reward: 2.737 [2.172, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.088, 10.516], loss: 1.945436, mae: 0.659352, mean_q: 5.283638
 96939/100000: episode: 1657, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 83.476, mean reward: 3.092 [2.615, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.199, 10.435], loss: 0.240605, mae: 0.485641, mean_q: 5.185435
 96970/100000: episode: 1658, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 178.801, mean reward: 5.768 [1.905, 13.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.685, 10.295], loss: 1.904109, mae: 0.611496, mean_q: 5.396421
 97001/100000: episode: 1659, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 90.412, mean reward: 2.917 [1.756, 5.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.090, 10.317], loss: 0.644784, mae: 0.607640, mean_q: 5.346081
 97030/100000: episode: 1660, duration: 0.166s, episode steps: 29, steps per second: 175, episode reward: 79.522, mean reward: 2.742 [1.495, 4.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.320], loss: 0.437221, mae: 0.551954, mean_q: 5.409316
 97038/100000: episode: 1661, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 22.213, mean reward: 2.777 [2.544, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.332, 10.100], loss: 0.966720, mae: 0.690671, mean_q: 5.661928
 97066/100000: episode: 1662, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 77.643, mean reward: 2.773 [2.092, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.349], loss: 0.331763, mae: 0.538117, mean_q: 5.370510
 97074/100000: episode: 1663, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 42.463, mean reward: 5.308 [3.339, 8.740], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.183, 10.100], loss: 0.282812, mae: 0.519640, mean_q: 5.492054
 97104/100000: episode: 1664, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 337.553, mean reward: 11.252 [3.868, 55.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.754, 10.554], loss: 1.943876, mae: 0.646868, mean_q: 5.425869
 97135/100000: episode: 1665, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 92.930, mean reward: 2.998 [1.617, 8.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.273, 10.329], loss: 0.348473, mae: 0.538070, mean_q: 5.451776
 97152/100000: episode: 1666, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 70.812, mean reward: 4.165 [2.737, 9.271], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.121, 10.100], loss: 0.521042, mae: 0.557243, mean_q: 5.300496
 97180/100000: episode: 1667, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 82.889, mean reward: 2.960 [2.066, 5.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.631, 10.266], loss: 0.842509, mae: 0.694126, mean_q: 5.620984
 97209/100000: episode: 1668, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 77.159, mean reward: 2.661 [2.114, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.340, 10.371], loss: 1.933919, mae: 0.710068, mean_q: 5.611896
 97236/100000: episode: 1669, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 113.731, mean reward: 4.212 [3.156, 7.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.493, 10.506], loss: 2.187856, mae: 0.749876, mean_q: 5.671376
 97266/100000: episode: 1670, duration: 0.156s, episode steps: 30, steps per second: 193, episode reward: 82.763, mean reward: 2.759 [1.635, 3.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.046, 10.247], loss: 1.879120, mae: 0.698928, mean_q: 5.528618
 97283/100000: episode: 1671, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 41.168, mean reward: 2.422 [1.598, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.145, 10.100], loss: 0.501032, mae: 0.643841, mean_q: 5.507808
 97300/100000: episode: 1672, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 64.048, mean reward: 3.768 [2.983, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.247, 10.100], loss: 0.602951, mae: 0.646524, mean_q: 5.651769
 97325/100000: episode: 1673, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 81.533, mean reward: 3.261 [2.679, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.584, 10.100], loss: 2.421660, mae: 0.948133, mean_q: 5.755545
[Info] 3-TH LEVEL FOUND: 10.54433822631836, Considering 10/90 traces
 97333/100000: episode: 1674, duration: 4.222s, episode steps: 8, steps per second: 2, episode reward: 35.497, mean reward: 4.437 [3.573, 5.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.326, 10.100], loss: 3.018112, mae: 0.738205, mean_q: 5.462922
 97349/100000: episode: 1675, duration: 0.096s, episode steps: 16, steps per second: 167, episode reward: 78.251, mean reward: 4.891 [3.330, 7.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.346, 10.445], loss: 3.298993, mae: 0.807967, mean_q: 5.878709
 97368/100000: episode: 1676, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 74.465, mean reward: 3.919 [3.005, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.316, 10.522], loss: 0.364926, mae: 0.615250, mean_q: 5.549139
 97389/100000: episode: 1677, duration: 0.140s, episode steps: 21, steps per second: 150, episode reward: 52.703, mean reward: 2.510 [1.637, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.229, 10.216], loss: 0.444462, mae: 0.632290, mean_q: 5.661298
 97406/100000: episode: 1678, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 65.830, mean reward: 3.872 [3.264, 6.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.528], loss: 0.807176, mae: 0.640443, mean_q: 5.575161
 97423/100000: episode: 1679, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 79.951, mean reward: 4.703 [3.756, 6.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.334, 10.531], loss: 0.641207, mae: 0.589619, mean_q: 5.521410
 97444/100000: episode: 1680, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 84.953, mean reward: 4.045 [3.041, 7.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.485], loss: 0.834876, mae: 0.655266, mean_q: 5.743911
 97462/100000: episode: 1681, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 99.035, mean reward: 5.502 [3.887, 7.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.862, 10.567], loss: 4.937790, mae: 0.822784, mean_q: 5.625953
 97477/100000: episode: 1682, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 49.810, mean reward: 3.321 [2.566, 4.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.416], loss: 3.297944, mae: 0.855347, mean_q: 5.778439
 97493/100000: episode: 1683, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 61.605, mean reward: 3.850 [1.934, 7.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.389], loss: 0.833377, mae: 0.624413, mean_q: 5.485301
 97508/100000: episode: 1684, duration: 0.073s, episode steps: 15, steps per second: 206, episode reward: 46.286, mean reward: 3.086 [2.312, 4.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.451, 10.432], loss: 1.872920, mae: 0.685128, mean_q: 5.642953
 97525/100000: episode: 1685, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 62.526, mean reward: 3.678 [2.827, 5.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.624, 10.418], loss: 0.648639, mae: 0.613044, mean_q: 5.567944
 97540/100000: episode: 1686, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 92.394, mean reward: 6.160 [4.061, 16.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.431, 10.583], loss: 0.609804, mae: 0.641059, mean_q: 5.596587
 97559/100000: episode: 1687, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 105.388, mean reward: 5.547 [2.300, 17.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.319], loss: 1.544082, mae: 0.719659, mean_q: 5.825649
 97577/100000: episode: 1688, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 83.704, mean reward: 4.650 [2.770, 7.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-1.192, 10.500], loss: 0.766277, mae: 0.681212, mean_q: 5.737082
 97594/100000: episode: 1689, duration: 0.085s, episode steps: 17, steps per second: 199, episode reward: 65.609, mean reward: 3.859 [2.285, 7.796], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.345], loss: 1.294015, mae: 0.786110, mean_q: 5.810597
 97610/100000: episode: 1690, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 81.674, mean reward: 5.105 [3.164, 7.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.504], loss: 0.264310, mae: 0.505058, mean_q: 5.486876
 97629/100000: episode: 1691, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 68.639, mean reward: 3.613 [2.269, 6.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.400], loss: 0.503761, mae: 0.623879, mean_q: 5.752632
 97644/100000: episode: 1692, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 117.957, mean reward: 7.864 [2.798, 20.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.147, 10.547], loss: 0.539644, mae: 0.591122, mean_q: 5.695435
 97662/100000: episode: 1693, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 105.557, mean reward: 5.864 [3.199, 20.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.497], loss: 1.483271, mae: 0.812957, mean_q: 5.981216
 97677/100000: episode: 1694, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 77.457, mean reward: 5.164 [3.560, 9.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.045, 10.559], loss: 0.808985, mae: 0.688338, mean_q: 5.798302
 97696/100000: episode: 1695, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 80.767, mean reward: 4.251 [3.227, 5.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.481, 10.551], loss: 0.936029, mae: 0.723602, mean_q: 5.910171
 97714/100000: episode: 1696, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 112.592, mean reward: 6.255 [1.856, 24.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.396], loss: 1.174192, mae: 0.713827, mean_q: 5.803189
 97732/100000: episode: 1697, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 69.646, mean reward: 3.869 [2.667, 6.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.701, 10.476], loss: 2.609656, mae: 0.758312, mean_q: 5.999380
 97751/100000: episode: 1698, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 53.608, mean reward: 2.821 [2.222, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.571, 10.380], loss: 0.982400, mae: 0.749761, mean_q: 5.718722
 97768/100000: episode: 1699, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 88.530, mean reward: 5.208 [2.975, 6.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.313, 10.526], loss: 1.102829, mae: 0.752632, mean_q: 6.056835
 97783/100000: episode: 1700, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 67.457, mean reward: 4.497 [2.677, 6.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.035, 10.543], loss: 0.678259, mae: 0.710100, mean_q: 5.947309
 97800/100000: episode: 1701, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 69.780, mean reward: 4.105 [2.175, 5.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.512], loss: 0.524909, mae: 0.651759, mean_q: 5.816022
 97818/100000: episode: 1702, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 87.406, mean reward: 4.856 [2.790, 11.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-1.440, 10.437], loss: 0.782265, mae: 0.738683, mean_q: 6.147157
 97834/100000: episode: 1703, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 83.893, mean reward: 5.243 [4.136, 6.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.631], loss: 0.901794, mae: 0.816719, mean_q: 6.267789
 97852/100000: episode: 1704, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 89.264, mean reward: 4.959 [2.509, 15.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.669, 10.426], loss: 1.615256, mae: 0.738162, mean_q: 6.148190
[Info] FALSIFICATION!
[Info] Levels: [5.8124948, 7.1963887, 10.544338, 12.625724]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.04]
[Info] Error Prob: 4.000000000000001e-05

 97853/100000: episode: 1705, duration: 4.525s, episode steps: 1, steps per second: 0, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.020, 10.146], loss: 0.376789, mae: 0.653152, mean_q: 6.048222
 97953/100000: episode: 1706, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 193.102, mean reward: 1.931 [1.467, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.890, 10.229], loss: 2.967803, mae: 0.866740, mean_q: 6.091664
 98053/100000: episode: 1707, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 177.743, mean reward: 1.777 [1.441, 2.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.224, 10.098], loss: 1.886032, mae: 0.772172, mean_q: 6.063595
 98153/100000: episode: 1708, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 201.693, mean reward: 2.017 [1.452, 3.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.057, 10.245], loss: 4.557598, mae: 0.904251, mean_q: 6.062086
 98253/100000: episode: 1709, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 197.985, mean reward: 1.980 [1.445, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.054, 10.373], loss: 3.373379, mae: 0.916346, mean_q: 6.153007
 98353/100000: episode: 1710, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 197.763, mean reward: 1.978 [1.488, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.589, 10.249], loss: 3.736987, mae: 0.880583, mean_q: 6.105118
 98453/100000: episode: 1711, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 183.356, mean reward: 1.834 [1.464, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.515, 10.163], loss: 2.058763, mae: 0.780015, mean_q: 6.088154
 98553/100000: episode: 1712, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 185.573, mean reward: 1.856 [1.475, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.426, 10.149], loss: 3.547069, mae: 0.801510, mean_q: 6.098608
 98653/100000: episode: 1713, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 187.192, mean reward: 1.872 [1.453, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.554, 10.098], loss: 2.440493, mae: 0.748840, mean_q: 6.021163
 98753/100000: episode: 1714, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 197.319, mean reward: 1.973 [1.441, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.747, 10.238], loss: 1.049942, mae: 0.674338, mean_q: 5.921698
 98853/100000: episode: 1715, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 192.675, mean reward: 1.927 [1.460, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.543, 10.278], loss: 5.749395, mae: 0.875633, mean_q: 6.086545
 98953/100000: episode: 1716, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 187.987, mean reward: 1.880 [1.439, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.492, 10.187], loss: 2.013547, mae: 0.730584, mean_q: 5.904890
 99053/100000: episode: 1717, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 205.200, mean reward: 2.052 [1.487, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.795, 10.161], loss: 3.608871, mae: 0.831039, mean_q: 6.037630
 99153/100000: episode: 1718, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 190.702, mean reward: 1.907 [1.432, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.810, 10.221], loss: 1.111413, mae: 0.685063, mean_q: 5.933969
 99253/100000: episode: 1719, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 185.554, mean reward: 1.856 [1.466, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.811, 10.316], loss: 1.547114, mae: 0.745416, mean_q: 5.879552
 99353/100000: episode: 1720, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 180.317, mean reward: 1.803 [1.439, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.172, 10.254], loss: 0.927369, mae: 0.661511, mean_q: 5.905659
 99453/100000: episode: 1721, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 230.209, mean reward: 2.302 [1.451, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.865, 10.098], loss: 5.498797, mae: 0.897045, mean_q: 6.054699
 99553/100000: episode: 1722, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.852, mean reward: 1.899 [1.480, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.691, 10.098], loss: 3.645155, mae: 0.813509, mean_q: 5.816448
 99653/100000: episode: 1723, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 197.616, mean reward: 1.976 [1.448, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.978, 10.195], loss: 2.202848, mae: 0.789433, mean_q: 5.811523
 99753/100000: episode: 1724, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 185.460, mean reward: 1.855 [1.446, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.062, 10.131], loss: 3.518222, mae: 0.808340, mean_q: 5.931534
 99853/100000: episode: 1725, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 204.061, mean reward: 2.041 [1.453, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.684, 10.101], loss: 1.287984, mae: 0.668477, mean_q: 5.755572
 99953/100000: episode: 1726, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 182.751, mean reward: 1.828 [1.466, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.913, 10.098], loss: 1.927094, mae: 0.774661, mean_q: 5.849138
done, took 603.958 seconds
[Info] End Importance Splitting. Falsification occurred 7 times.
