Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.174s, episode steps: 100, steps per second: 575, episode reward: 197.811, mean reward: 1.978 [1.467, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.888, 10.331], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.076s, episode steps: 100, steps per second: 1312, episode reward: 203.184, mean reward: 2.032 [1.546, 5.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.780, 10.292], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.060s, episode steps: 100, steps per second: 1677, episode reward: 190.318, mean reward: 1.903 [1.481, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.038, 10.098], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.060s, episode steps: 100, steps per second: 1675, episode reward: 193.451, mean reward: 1.935 [1.454, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.786, 10.236], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.060s, episode steps: 100, steps per second: 1661, episode reward: 201.756, mean reward: 2.018 [1.442, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.779, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.060s, episode steps: 100, steps per second: 1671, episode reward: 182.086, mean reward: 1.821 [1.467, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.424, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.062s, episode steps: 100, steps per second: 1613, episode reward: 199.410, mean reward: 1.994 [1.479, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.920, 10.168], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 182.357, mean reward: 1.824 [1.440, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.952, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.060s, episode steps: 100, steps per second: 1676, episode reward: 186.062, mean reward: 1.861 [1.457, 4.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.883, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.062s, episode steps: 100, steps per second: 1605, episode reward: 187.406, mean reward: 1.874 [1.467, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.627, 10.271], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.060s, episode steps: 100, steps per second: 1679, episode reward: 188.735, mean reward: 1.887 [1.445, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.870, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.060s, episode steps: 100, steps per second: 1670, episode reward: 194.525, mean reward: 1.945 [1.481, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.812, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.060s, episode steps: 100, steps per second: 1678, episode reward: 203.920, mean reward: 2.039 [1.521, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.276, 10.353], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.060s, episode steps: 100, steps per second: 1675, episode reward: 214.562, mean reward: 2.146 [1.486, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.866, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.060s, episode steps: 100, steps per second: 1676, episode reward: 203.681, mean reward: 2.037 [1.469, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.959, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.061s, episode steps: 100, steps per second: 1629, episode reward: 183.425, mean reward: 1.834 [1.455, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.317, 10.282], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.060s, episode steps: 100, steps per second: 1669, episode reward: 191.248, mean reward: 1.912 [1.496, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.361, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.060s, episode steps: 100, steps per second: 1667, episode reward: 190.336, mean reward: 1.903 [1.457, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.828, 10.129], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.060s, episode steps: 100, steps per second: 1665, episode reward: 185.458, mean reward: 1.855 [1.465, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.190, 10.098], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.060s, episode steps: 100, steps per second: 1675, episode reward: 189.108, mean reward: 1.891 [1.457, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.919, 10.150], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.060s, episode steps: 100, steps per second: 1678, episode reward: 201.886, mean reward: 2.019 [1.449, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.415, 10.098], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.061s, episode steps: 100, steps per second: 1647, episode reward: 194.153, mean reward: 1.942 [1.447, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.594, 10.365], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.059s, episode steps: 100, steps per second: 1701, episode reward: 179.895, mean reward: 1.799 [1.442, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.659, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.060s, episode steps: 100, steps per second: 1678, episode reward: 183.259, mean reward: 1.833 [1.469, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.451, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.060s, episode steps: 100, steps per second: 1674, episode reward: 184.559, mean reward: 1.846 [1.477, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.748, 10.195], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 211.658, mean reward: 2.117 [1.515, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.599, 10.192], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.081s, episode steps: 100, steps per second: 1228, episode reward: 185.843, mean reward: 1.858 [1.438, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.717, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.060s, episode steps: 100, steps per second: 1678, episode reward: 193.029, mean reward: 1.930 [1.482, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.950, 10.400], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.065s, episode steps: 100, steps per second: 1536, episode reward: 197.409, mean reward: 1.974 [1.486, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.220, 10.098], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.060s, episode steps: 100, steps per second: 1677, episode reward: 187.707, mean reward: 1.877 [1.456, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.371, 10.183], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.059s, episode steps: 100, steps per second: 1683, episode reward: 186.354, mean reward: 1.864 [1.453, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.781, 10.154], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.059s, episode steps: 100, steps per second: 1682, episode reward: 199.973, mean reward: 2.000 [1.512, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.312, 10.236], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.086s, episode steps: 100, steps per second: 1166, episode reward: 191.673, mean reward: 1.917 [1.451, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.781, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.060s, episode steps: 100, steps per second: 1677, episode reward: 193.345, mean reward: 1.933 [1.467, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.513, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.062s, episode steps: 100, steps per second: 1602, episode reward: 189.895, mean reward: 1.899 [1.466, 4.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.270, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.069s, episode steps: 100, steps per second: 1459, episode reward: 185.174, mean reward: 1.852 [1.456, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.440, 10.155], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.060s, episode steps: 100, steps per second: 1672, episode reward: 198.351, mean reward: 1.984 [1.446, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.162, 10.122], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.060s, episode steps: 100, steps per second: 1679, episode reward: 203.502, mean reward: 2.035 [1.509, 4.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.116, 10.333], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.059s, episode steps: 100, steps per second: 1684, episode reward: 194.319, mean reward: 1.943 [1.461, 4.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.211, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.059s, episode steps: 100, steps per second: 1685, episode reward: 189.804, mean reward: 1.898 [1.435, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.828, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.060s, episode steps: 100, steps per second: 1677, episode reward: 214.033, mean reward: 2.140 [1.453, 9.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.919, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.060s, episode steps: 100, steps per second: 1670, episode reward: 201.364, mean reward: 2.014 [1.505, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.901, 10.375], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.059s, episode steps: 100, steps per second: 1683, episode reward: 185.563, mean reward: 1.856 [1.499, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.705, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.060s, episode steps: 100, steps per second: 1680, episode reward: 201.475, mean reward: 2.015 [1.440, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.593, 10.098], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.061s, episode steps: 100, steps per second: 1645, episode reward: 199.362, mean reward: 1.994 [1.454, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.464, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.060s, episode steps: 100, steps per second: 1661, episode reward: 197.441, mean reward: 1.974 [1.480, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.705, 10.117], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.060s, episode steps: 100, steps per second: 1678, episode reward: 185.778, mean reward: 1.858 [1.473, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.968, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.060s, episode steps: 100, steps per second: 1679, episode reward: 184.750, mean reward: 1.848 [1.441, 2.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.419, 10.176], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.059s, episode steps: 100, steps per second: 1683, episode reward: 191.509, mean reward: 1.915 [1.483, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.895, 10.152], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.060s, episode steps: 100, steps per second: 1674, episode reward: 187.396, mean reward: 1.874 [1.488, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.146, 10.098], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.211s, episode steps: 100, steps per second: 83, episode reward: 185.613, mean reward: 1.856 [1.486, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.588, 10.152], loss: 0.619997, mae: 0.823592, mean_q: 1.246132
  5200/100000: episode: 52, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 195.303, mean reward: 1.953 [1.459, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.945, 10.098], loss: 0.114980, mae: 0.315834, mean_q: 2.354208
  5300/100000: episode: 53, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 244.116, mean reward: 2.441 [1.503, 6.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.032, 10.098], loss: 0.087340, mae: 0.282196, mean_q: 2.877696
  5400/100000: episode: 54, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.232, mean reward: 1.952 [1.471, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.585, 10.264], loss: 0.104076, mae: 0.303286, mean_q: 3.255045
  5500/100000: episode: 55, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 186.790, mean reward: 1.868 [1.453, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.803, 10.298], loss: 0.103419, mae: 0.299209, mean_q: 3.466422
  5600/100000: episode: 56, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 194.174, mean reward: 1.942 [1.457, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.783, 10.287], loss: 0.112717, mae: 0.308825, mean_q: 3.620794
  5700/100000: episode: 57, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.145, mean reward: 1.951 [1.534, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.772, 10.098], loss: 0.105273, mae: 0.307350, mean_q: 3.708589
  5800/100000: episode: 58, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 188.148, mean reward: 1.881 [1.447, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.191, 10.134], loss: 0.121665, mae: 0.300763, mean_q: 3.754684
  5900/100000: episode: 59, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.931, mean reward: 1.939 [1.457, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.392, 10.201], loss: 0.101337, mae: 0.300647, mean_q: 3.801842
  6000/100000: episode: 60, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 174.558, mean reward: 1.746 [1.463, 2.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.402, 10.169], loss: 0.101622, mae: 0.298859, mean_q: 3.803601
  6100/100000: episode: 61, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 179.289, mean reward: 1.793 [1.477, 2.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.024, 10.098], loss: 0.115305, mae: 0.314808, mean_q: 3.834143
  6200/100000: episode: 62, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 176.698, mean reward: 1.767 [1.470, 2.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.692, 10.233], loss: 0.098916, mae: 0.298360, mean_q: 3.820191
  6300/100000: episode: 63, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 191.681, mean reward: 1.917 [1.465, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.212, 10.098], loss: 0.120456, mae: 0.309929, mean_q: 3.835109
  6400/100000: episode: 64, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.044, mean reward: 1.840 [1.445, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.725, 10.174], loss: 0.110714, mae: 0.302024, mean_q: 3.809721
  6500/100000: episode: 65, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 186.964, mean reward: 1.870 [1.483, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.368, 10.098], loss: 0.101810, mae: 0.300764, mean_q: 3.804520
  6600/100000: episode: 66, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 199.083, mean reward: 1.991 [1.467, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.583, 10.098], loss: 0.101501, mae: 0.287794, mean_q: 3.795635
  6700/100000: episode: 67, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 230.041, mean reward: 2.300 [1.483, 5.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.030, 10.457], loss: 0.104992, mae: 0.295100, mean_q: 3.785458
  6800/100000: episode: 68, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.335, mean reward: 1.893 [1.470, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.934, 10.337], loss: 0.105843, mae: 0.299407, mean_q: 3.823007
  6900/100000: episode: 69, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 184.749, mean reward: 1.847 [1.463, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.987, 10.098], loss: 0.110195, mae: 0.308098, mean_q: 3.814998
  7000/100000: episode: 70, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 218.480, mean reward: 2.185 [1.451, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.387, 10.387], loss: 0.111260, mae: 0.301704, mean_q: 3.804798
  7100/100000: episode: 71, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 187.368, mean reward: 1.874 [1.478, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.923, 10.240], loss: 0.117940, mae: 0.314242, mean_q: 3.830276
  7200/100000: episode: 72, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 193.534, mean reward: 1.935 [1.495, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.658, 10.125], loss: 0.109914, mae: 0.303155, mean_q: 3.818629
  7300/100000: episode: 73, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 207.224, mean reward: 2.072 [1.530, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.630, 10.098], loss: 0.107510, mae: 0.304123, mean_q: 3.830564
  7400/100000: episode: 74, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 182.301, mean reward: 1.823 [1.451, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.172, 10.098], loss: 0.101095, mae: 0.303187, mean_q: 3.836905
  7500/100000: episode: 75, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.117, mean reward: 1.911 [1.458, 3.243], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.902, 10.098], loss: 0.104592, mae: 0.310465, mean_q: 3.836228
  7600/100000: episode: 76, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 208.422, mean reward: 2.084 [1.504, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.463, 10.405], loss: 0.104018, mae: 0.314841, mean_q: 3.841546
  7700/100000: episode: 77, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 190.403, mean reward: 1.904 [1.457, 3.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.296, 10.098], loss: 0.112957, mae: 0.323163, mean_q: 3.852592
  7800/100000: episode: 78, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 188.411, mean reward: 1.884 [1.466, 4.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.564, 10.124], loss: 0.125721, mae: 0.316527, mean_q: 3.841399
  7900/100000: episode: 79, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 200.522, mean reward: 2.005 [1.454, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.779, 10.187], loss: 0.136331, mae: 0.334389, mean_q: 3.852714
  8000/100000: episode: 80, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 194.140, mean reward: 1.941 [1.478, 4.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.482, 10.176], loss: 0.094584, mae: 0.296458, mean_q: 3.841162
  8100/100000: episode: 81, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.742, mean reward: 1.847 [1.465, 2.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.466, 10.175], loss: 0.141234, mae: 0.335524, mean_q: 3.873473
  8200/100000: episode: 82, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.166, mean reward: 1.932 [1.447, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.626, 10.120], loss: 0.115585, mae: 0.317762, mean_q: 3.851852
  8300/100000: episode: 83, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 184.268, mean reward: 1.843 [1.452, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.754, 10.098], loss: 0.099599, mae: 0.313182, mean_q: 3.850236
  8400/100000: episode: 84, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.735, mean reward: 1.887 [1.437, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.186, 10.135], loss: 0.108130, mae: 0.307017, mean_q: 3.834011
  8500/100000: episode: 85, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 192.249, mean reward: 1.922 [1.459, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.545, 10.098], loss: 0.112509, mae: 0.307817, mean_q: 3.827094
  8600/100000: episode: 86, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 190.857, mean reward: 1.909 [1.489, 4.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.690, 10.098], loss: 0.099567, mae: 0.302408, mean_q: 3.830160
  8700/100000: episode: 87, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 185.834, mean reward: 1.858 [1.443, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.252, 10.219], loss: 0.103515, mae: 0.306172, mean_q: 3.848190
  8800/100000: episode: 88, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 176.842, mean reward: 1.768 [1.454, 2.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.094, 10.296], loss: 0.094343, mae: 0.305727, mean_q: 3.845174
  8900/100000: episode: 89, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 197.905, mean reward: 1.979 [1.498, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.076, 10.098], loss: 0.114243, mae: 0.311161, mean_q: 3.826651
  9000/100000: episode: 90, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 197.181, mean reward: 1.972 [1.467, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.262, 10.192], loss: 0.089124, mae: 0.291085, mean_q: 3.836294
  9100/100000: episode: 91, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 195.736, mean reward: 1.957 [1.476, 6.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.076, 10.366], loss: 0.102259, mae: 0.301005, mean_q: 3.825288
  9200/100000: episode: 92, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 181.048, mean reward: 1.810 [1.476, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.495, 10.204], loss: 0.088260, mae: 0.293576, mean_q: 3.808392
  9300/100000: episode: 93, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 195.576, mean reward: 1.956 [1.514, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.113, 10.098], loss: 0.091819, mae: 0.286126, mean_q: 3.804882
  9400/100000: episode: 94, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 208.457, mean reward: 2.085 [1.477, 9.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.276, 10.104], loss: 0.095496, mae: 0.301006, mean_q: 3.810997
  9500/100000: episode: 95, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 185.803, mean reward: 1.858 [1.469, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.669, 10.098], loss: 0.103453, mae: 0.297321, mean_q: 3.815825
  9600/100000: episode: 96, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 190.818, mean reward: 1.908 [1.468, 4.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.126, 10.098], loss: 0.101093, mae: 0.301256, mean_q: 3.807970
  9700/100000: episode: 97, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 214.752, mean reward: 2.148 [1.457, 7.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.111, 10.288], loss: 0.107802, mae: 0.291174, mean_q: 3.811201
  9800/100000: episode: 98, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.214, mean reward: 1.912 [1.478, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.740, 10.107], loss: 0.108815, mae: 0.303478, mean_q: 3.822251
  9900/100000: episode: 99, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 191.380, mean reward: 1.914 [1.475, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.685, 10.404], loss: 0.096760, mae: 0.295444, mean_q: 3.824293
 10000/100000: episode: 100, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 178.133, mean reward: 1.781 [1.461, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.008, 10.227], loss: 0.097848, mae: 0.296184, mean_q: 3.813518
 10100/100000: episode: 101, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 190.661, mean reward: 1.907 [1.474, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.019, 10.098], loss: 0.108744, mae: 0.306113, mean_q: 3.824913
 10200/100000: episode: 102, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 199.425, mean reward: 1.994 [1.466, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.463, 10.098], loss: 0.093605, mae: 0.295772, mean_q: 3.836635
 10300/100000: episode: 103, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.189, mean reward: 1.962 [1.462, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.735, 10.106], loss: 0.083030, mae: 0.287409, mean_q: 3.820178
 10400/100000: episode: 104, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 198.823, mean reward: 1.988 [1.450, 5.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.423, 10.098], loss: 0.090807, mae: 0.284870, mean_q: 3.800871
 10500/100000: episode: 105, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 207.380, mean reward: 2.074 [1.520, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.760, 10.143], loss: 0.081756, mae: 0.283041, mean_q: 3.793138
 10600/100000: episode: 106, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 187.822, mean reward: 1.878 [1.463, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.865, 10.098], loss: 0.087377, mae: 0.293977, mean_q: 3.813520
 10700/100000: episode: 107, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 195.295, mean reward: 1.953 [1.461, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.413, 10.172], loss: 0.096120, mae: 0.293021, mean_q: 3.811470
 10800/100000: episode: 108, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 213.682, mean reward: 2.137 [1.508, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.394, 10.098], loss: 0.107686, mae: 0.304493, mean_q: 3.827550
 10900/100000: episode: 109, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 241.768, mean reward: 2.418 [1.503, 3.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.896, 10.098], loss: 0.111327, mae: 0.307368, mean_q: 3.829041
 11000/100000: episode: 110, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 184.713, mean reward: 1.847 [1.468, 2.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.439, 10.111], loss: 0.082875, mae: 0.294259, mean_q: 3.841636
 11100/100000: episode: 111, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 205.244, mean reward: 2.052 [1.464, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.427, 10.375], loss: 0.090937, mae: 0.310171, mean_q: 3.842523
 11200/100000: episode: 112, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.673, mean reward: 1.977 [1.492, 5.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.543, 10.404], loss: 0.094061, mae: 0.308683, mean_q: 3.853605
 11300/100000: episode: 113, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 198.556, mean reward: 1.986 [1.451, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.697, 10.098], loss: 0.102199, mae: 0.309714, mean_q: 3.865507
 11400/100000: episode: 114, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 190.141, mean reward: 1.901 [1.446, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.410, 10.120], loss: 0.098747, mae: 0.305755, mean_q: 3.857104
 11500/100000: episode: 115, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 188.044, mean reward: 1.880 [1.447, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.551, 10.152], loss: 0.101637, mae: 0.306551, mean_q: 3.864077
 11600/100000: episode: 116, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 184.399, mean reward: 1.844 [1.449, 3.352], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.569, 10.098], loss: 0.104720, mae: 0.314992, mean_q: 3.861725
 11700/100000: episode: 117, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 185.012, mean reward: 1.850 [1.466, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.937, 10.164], loss: 0.092235, mae: 0.298076, mean_q: 3.846417
 11800/100000: episode: 118, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 216.605, mean reward: 2.166 [1.485, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.911, 10.098], loss: 0.086681, mae: 0.305635, mean_q: 3.849639
 11900/100000: episode: 119, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 193.551, mean reward: 1.936 [1.441, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.791, 10.098], loss: 0.097734, mae: 0.308099, mean_q: 3.851250
 12000/100000: episode: 120, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 198.101, mean reward: 1.981 [1.472, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.750, 10.297], loss: 0.099946, mae: 0.309951, mean_q: 3.852164
 12100/100000: episode: 121, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 191.034, mean reward: 1.910 [1.468, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.150, 10.098], loss: 0.079656, mae: 0.290505, mean_q: 3.851314
 12200/100000: episode: 122, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 195.250, mean reward: 1.952 [1.499, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.768, 10.132], loss: 0.106770, mae: 0.305482, mean_q: 3.849805
 12300/100000: episode: 123, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 183.357, mean reward: 1.834 [1.496, 2.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.922, 10.098], loss: 0.103484, mae: 0.309066, mean_q: 3.861158
 12400/100000: episode: 124, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 190.223, mean reward: 1.902 [1.441, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.949, 10.098], loss: 0.087267, mae: 0.299111, mean_q: 3.862717
 12500/100000: episode: 125, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 203.423, mean reward: 2.034 [1.484, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.869, 10.098], loss: 0.091909, mae: 0.293469, mean_q: 3.859748
 12600/100000: episode: 126, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 184.738, mean reward: 1.847 [1.472, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.497, 10.098], loss: 0.095818, mae: 0.301491, mean_q: 3.850271
 12700/100000: episode: 127, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 183.687, mean reward: 1.837 [1.481, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.669, 10.098], loss: 0.106052, mae: 0.304027, mean_q: 3.859522
 12800/100000: episode: 128, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 196.789, mean reward: 1.968 [1.496, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.287, 10.098], loss: 0.090350, mae: 0.300092, mean_q: 3.841904
 12900/100000: episode: 129, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 180.964, mean reward: 1.810 [1.452, 2.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.474, 10.291], loss: 0.098373, mae: 0.300036, mean_q: 3.852606
 13000/100000: episode: 130, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 193.487, mean reward: 1.935 [1.449, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.903, 10.227], loss: 0.087291, mae: 0.294753, mean_q: 3.833889
 13100/100000: episode: 131, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 210.959, mean reward: 2.110 [1.464, 4.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.946, 10.294], loss: 0.105989, mae: 0.304319, mean_q: 3.832191
 13200/100000: episode: 132, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 203.875, mean reward: 2.039 [1.492, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.985, 10.098], loss: 0.096987, mae: 0.301062, mean_q: 3.847118
 13300/100000: episode: 133, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 185.306, mean reward: 1.853 [1.478, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.863, 10.098], loss: 0.088862, mae: 0.300403, mean_q: 3.852833
 13400/100000: episode: 134, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 225.525, mean reward: 2.255 [1.475, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.025, 10.098], loss: 0.112590, mae: 0.318912, mean_q: 3.856299
 13500/100000: episode: 135, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 211.259, mean reward: 2.113 [1.459, 6.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.787, 10.351], loss: 0.098486, mae: 0.313247, mean_q: 3.860496
 13600/100000: episode: 136, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 187.405, mean reward: 1.874 [1.461, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.255, 10.262], loss: 0.102530, mae: 0.311148, mean_q: 3.862350
 13700/100000: episode: 137, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 207.668, mean reward: 2.077 [1.451, 5.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.197, 10.181], loss: 0.097530, mae: 0.316884, mean_q: 3.879281
 13800/100000: episode: 138, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 205.178, mean reward: 2.052 [1.475, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.903, 10.355], loss: 0.103702, mae: 0.312390, mean_q: 3.870123
 13900/100000: episode: 139, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 207.860, mean reward: 2.079 [1.473, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.762, 10.098], loss: 0.115481, mae: 0.322471, mean_q: 3.890921
 14000/100000: episode: 140, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 181.167, mean reward: 1.812 [1.452, 2.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.064, 10.134], loss: 0.106895, mae: 0.319309, mean_q: 3.881161
 14100/100000: episode: 141, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 184.201, mean reward: 1.842 [1.448, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.347, 10.215], loss: 0.098942, mae: 0.315725, mean_q: 3.884972
 14200/100000: episode: 142, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 201.228, mean reward: 2.012 [1.456, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.973, 10.331], loss: 0.100471, mae: 0.322171, mean_q: 3.892483
 14300/100000: episode: 143, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 188.807, mean reward: 1.888 [1.470, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.862, 10.130], loss: 0.102132, mae: 0.318929, mean_q: 3.899939
 14400/100000: episode: 144, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 189.014, mean reward: 1.890 [1.457, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.809, 10.310], loss: 0.110908, mae: 0.321511, mean_q: 3.884431
 14500/100000: episode: 145, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 205.655, mean reward: 2.057 [1.488, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.007, 10.224], loss: 0.107369, mae: 0.322870, mean_q: 3.889066
 14600/100000: episode: 146, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 180.744, mean reward: 1.807 [1.443, 2.695], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.040, 10.098], loss: 0.109380, mae: 0.317185, mean_q: 3.892960
 14700/100000: episode: 147, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 190.710, mean reward: 1.907 [1.446, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.635, 10.098], loss: 0.089628, mae: 0.300611, mean_q: 3.883564
 14800/100000: episode: 148, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 213.693, mean reward: 2.137 [1.480, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.804, 10.098], loss: 0.092291, mae: 0.312321, mean_q: 3.892720
 14900/100000: episode: 149, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 185.164, mean reward: 1.852 [1.463, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.031, 10.108], loss: 0.091260, mae: 0.307596, mean_q: 3.918776
[Info] 1-TH LEVEL FOUND: 4.690190315246582, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.003s, episode steps: 100, steps per second: 20, episode reward: 196.397, mean reward: 1.964 [1.438, 4.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.047, 10.103], loss: 0.100434, mae: 0.323335, mean_q: 3.897929
 15009/100000: episode: 151, duration: 0.063s, episode steps: 9, steps per second: 142, episode reward: 31.653, mean reward: 3.517 [2.515, 5.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.347, 10.100], loss: 0.064772, mae: 0.281776, mean_q: 3.923663
 15019/100000: episode: 152, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 29.362, mean reward: 2.936 [2.174, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-1.040, 10.100], loss: 0.110974, mae: 0.332872, mean_q: 3.924100
 15037/100000: episode: 153, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 65.515, mean reward: 3.640 [2.520, 6.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.540, 10.100], loss: 0.099183, mae: 0.318369, mean_q: 3.896761
 15047/100000: episode: 154, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 25.169, mean reward: 2.517 [2.269, 2.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.255, 10.100], loss: 0.152346, mae: 0.328371, mean_q: 3.892990
 15075/100000: episode: 155, duration: 0.154s, episode steps: 28, steps per second: 181, episode reward: 52.298, mean reward: 1.868 [1.455, 2.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.568, 10.316], loss: 0.089082, mae: 0.306885, mean_q: 3.922419
 15084/100000: episode: 156, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 23.614, mean reward: 2.624 [2.275, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.344, 10.100], loss: 0.089498, mae: 0.300358, mean_q: 3.915524
 15113/100000: episode: 157, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 60.432, mean reward: 2.084 [1.720, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.449, 10.320], loss: 0.117224, mae: 0.317884, mean_q: 3.899483
 15122/100000: episode: 158, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 21.342, mean reward: 2.371 [2.073, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.640, 10.100], loss: 0.118123, mae: 0.334203, mean_q: 3.938739
 15132/100000: episode: 159, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 24.392, mean reward: 2.439 [2.102, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.346, 10.100], loss: 0.089134, mae: 0.293195, mean_q: 3.824437
 15150/100000: episode: 160, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 59.078, mean reward: 3.282 [2.409, 4.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.371, 10.100], loss: 0.092301, mae: 0.319517, mean_q: 3.940646
 15161/100000: episode: 161, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 32.748, mean reward: 2.977 [2.208, 5.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.142, 10.100], loss: 0.100396, mae: 0.314013, mean_q: 3.912431
 15179/100000: episode: 162, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 66.270, mean reward: 3.682 [2.250, 5.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.719, 10.100], loss: 0.133702, mae: 0.346774, mean_q: 3.920919
 15188/100000: episode: 163, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 22.686, mean reward: 2.521 [1.901, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.226, 10.100], loss: 0.120803, mae: 0.345872, mean_q: 3.962382
 15197/100000: episode: 164, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 20.830, mean reward: 2.314 [2.010, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.151, 10.100], loss: 0.116838, mae: 0.350217, mean_q: 3.975357
 15251/100000: episode: 165, duration: 0.277s, episode steps: 54, steps per second: 195, episode reward: 107.773, mean reward: 1.996 [1.441, 4.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.537, 10.100], loss: 0.122570, mae: 0.339105, mean_q: 3.936264
 15260/100000: episode: 166, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 23.664, mean reward: 2.629 [2.087, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.373, 10.100], loss: 0.081878, mae: 0.285557, mean_q: 3.921668
 15269/100000: episode: 167, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 22.613, mean reward: 2.513 [2.091, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.456, 10.100], loss: 0.129410, mae: 0.347158, mean_q: 3.981044
 15323/100000: episode: 168, duration: 0.292s, episode steps: 54, steps per second: 185, episode reward: 102.425, mean reward: 1.897 [1.487, 2.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.322, 10.185], loss: 0.101102, mae: 0.319962, mean_q: 3.944742
 15377/100000: episode: 169, duration: 0.285s, episode steps: 54, steps per second: 190, episode reward: 103.935, mean reward: 1.925 [1.511, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.861 [-1.495, 10.100], loss: 0.106663, mae: 0.332815, mean_q: 3.920107
 15387/100000: episode: 170, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 23.965, mean reward: 2.397 [2.141, 2.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.246, 10.100], loss: 0.111178, mae: 0.342562, mean_q: 3.953949
 15405/100000: episode: 171, duration: 0.092s, episode steps: 18, steps per second: 195, episode reward: 54.836, mean reward: 3.046 [2.365, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.467, 10.100], loss: 0.095655, mae: 0.302836, mean_q: 3.929510
 15434/100000: episode: 172, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 61.934, mean reward: 2.136 [1.727, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.035, 10.258], loss: 0.108626, mae: 0.324550, mean_q: 3.961069
 15444/100000: episode: 173, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 26.716, mean reward: 2.672 [2.331, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.187, 10.100], loss: 0.142938, mae: 0.330731, mean_q: 3.987653
 15473/100000: episode: 174, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 78.832, mean reward: 2.718 [1.934, 4.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.237, 10.325], loss: 0.107640, mae: 0.329172, mean_q: 3.955779
 15482/100000: episode: 175, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 25.696, mean reward: 2.855 [2.459, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.372, 10.100], loss: 0.135335, mae: 0.362861, mean_q: 3.937269
 15493/100000: episode: 176, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 35.322, mean reward: 3.211 [2.431, 3.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.367, 10.100], loss: 0.114064, mae: 0.347128, mean_q: 3.956897
 15511/100000: episode: 177, duration: 0.107s, episode steps: 18, steps per second: 167, episode reward: 52.235, mean reward: 2.902 [1.935, 4.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.772, 10.100], loss: 0.111667, mae: 0.350628, mean_q: 3.951315
 15520/100000: episode: 178, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 18.856, mean reward: 2.095 [1.868, 2.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.278, 10.100], loss: 0.110590, mae: 0.342851, mean_q: 4.000008
 15574/100000: episode: 179, duration: 0.290s, episode steps: 54, steps per second: 186, episode reward: 97.182, mean reward: 1.800 [1.465, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.875 [-0.437, 10.131], loss: 0.100427, mae: 0.329232, mean_q: 3.970909
 15620/100000: episode: 180, duration: 0.256s, episode steps: 46, steps per second: 180, episode reward: 115.749, mean reward: 2.516 [1.662, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.433, 10.292], loss: 0.127946, mae: 0.342705, mean_q: 3.977545
 15630/100000: episode: 181, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 29.294, mean reward: 2.929 [2.407, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.287, 10.100], loss: 0.090166, mae: 0.315247, mean_q: 3.997279
 15641/100000: episode: 182, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 37.073, mean reward: 3.370 [2.457, 4.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.347, 10.100], loss: 0.134527, mae: 0.364081, mean_q: 4.019034
 15651/100000: episode: 183, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 37.451, mean reward: 3.745 [2.355, 7.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.311, 10.100], loss: 0.113334, mae: 0.333343, mean_q: 3.970861
 15660/100000: episode: 184, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 21.722, mean reward: 2.414 [2.042, 2.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.215, 10.100], loss: 0.128692, mae: 0.373836, mean_q: 3.959858
 15678/100000: episode: 185, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 61.516, mean reward: 3.418 [2.485, 4.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.426, 10.100], loss: 0.104545, mae: 0.323402, mean_q: 3.918625
 15724/100000: episode: 186, duration: 0.252s, episode steps: 46, steps per second: 183, episode reward: 95.001, mean reward: 2.065 [1.496, 5.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.409, 10.123], loss: 0.117824, mae: 0.342916, mean_q: 3.988970
 15735/100000: episode: 187, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 33.904, mean reward: 3.082 [2.427, 4.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.242, 10.100], loss: 0.144583, mae: 0.373300, mean_q: 3.988627
 15764/100000: episode: 188, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 64.738, mean reward: 2.232 [1.602, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.900, 10.100], loss: 0.133260, mae: 0.360865, mean_q: 3.947820
 15773/100000: episode: 189, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 24.625, mean reward: 2.736 [2.468, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.285, 10.100], loss: 0.124073, mae: 0.372947, mean_q: 4.072851
 15819/100000: episode: 190, duration: 0.251s, episode steps: 46, steps per second: 183, episode reward: 93.665, mean reward: 2.036 [1.524, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.815, 10.117], loss: 0.116569, mae: 0.354735, mean_q: 4.007649
 15830/100000: episode: 191, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 41.959, mean reward: 3.814 [2.840, 6.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-1.757, 10.100], loss: 0.104610, mae: 0.335857, mean_q: 3.930847
 15839/100000: episode: 192, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 22.118, mean reward: 2.458 [2.231, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.247, 10.100], loss: 0.111228, mae: 0.355357, mean_q: 3.981446
 15885/100000: episode: 193, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 98.737, mean reward: 2.146 [1.581, 4.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.978, 10.368], loss: 0.112144, mae: 0.335659, mean_q: 3.983338
 15914/100000: episode: 194, duration: 0.151s, episode steps: 29, steps per second: 191, episode reward: 63.531, mean reward: 2.191 [1.483, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.120, 10.292], loss: 0.115033, mae: 0.331416, mean_q: 4.003745
 15960/100000: episode: 195, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 100.419, mean reward: 2.183 [1.532, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.450, 10.234], loss: 0.101131, mae: 0.326031, mean_q: 3.977223
 15970/100000: episode: 196, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 27.425, mean reward: 2.743 [2.459, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.137, 10.100], loss: 0.091853, mae: 0.328227, mean_q: 4.094300
 15979/100000: episode: 197, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 19.746, mean reward: 2.194 [1.917, 2.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.237, 10.100], loss: 0.102399, mae: 0.320923, mean_q: 4.055290
 16007/100000: episode: 198, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 67.384, mean reward: 2.407 [1.495, 5.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-1.303, 10.184], loss: 0.113462, mae: 0.338680, mean_q: 4.014427
 16017/100000: episode: 199, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 35.880, mean reward: 3.588 [3.005, 4.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.466, 10.100], loss: 0.108836, mae: 0.341834, mean_q: 3.989621
 16035/100000: episode: 200, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 53.654, mean reward: 2.981 [2.306, 4.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.258, 10.100], loss: 0.096428, mae: 0.325287, mean_q: 3.966147
 16063/100000: episode: 201, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 55.083, mean reward: 1.967 [1.533, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.223], loss: 0.099578, mae: 0.332964, mean_q: 4.037435
 16072/100000: episode: 202, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 21.599, mean reward: 2.400 [2.080, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.215, 10.100], loss: 0.194481, mae: 0.417664, mean_q: 4.203144
 16118/100000: episode: 203, duration: 0.248s, episode steps: 46, steps per second: 185, episode reward: 101.366, mean reward: 2.204 [1.571, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.269, 10.100], loss: 0.115006, mae: 0.336746, mean_q: 4.000218
 16128/100000: episode: 204, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 36.313, mean reward: 3.631 [2.629, 5.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.235, 10.100], loss: 0.149725, mae: 0.372972, mean_q: 4.087097
 16182/100000: episode: 205, duration: 0.281s, episode steps: 54, steps per second: 192, episode reward: 114.762, mean reward: 2.125 [1.507, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.941, 10.303], loss: 0.112494, mae: 0.337513, mean_q: 3.986233
 16191/100000: episode: 206, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 21.111, mean reward: 2.346 [1.912, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.313, 10.100], loss: 0.118711, mae: 0.337186, mean_q: 3.986390
 16201/100000: episode: 207, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 25.218, mean reward: 2.522 [2.262, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.380, 10.100], loss: 0.122913, mae: 0.371291, mean_q: 3.979235
 16211/100000: episode: 208, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 29.431, mean reward: 2.943 [2.266, 5.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.176, 10.100], loss: 0.122104, mae: 0.361003, mean_q: 4.092336
 16221/100000: episode: 209, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 29.101, mean reward: 2.910 [2.689, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.182, 10.100], loss: 0.133033, mae: 0.373985, mean_q: 3.997615
 16239/100000: episode: 210, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 50.502, mean reward: 2.806 [2.093, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.678, 10.100], loss: 0.160084, mae: 0.377842, mean_q: 4.047201
 16293/100000: episode: 211, duration: 0.302s, episode steps: 54, steps per second: 179, episode reward: 106.164, mean reward: 1.966 [1.517, 3.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-0.307, 10.100], loss: 0.114273, mae: 0.334221, mean_q: 4.006121
 16302/100000: episode: 212, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 23.955, mean reward: 2.662 [2.253, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.348, 10.100], loss: 0.082755, mae: 0.310018, mean_q: 4.042590
 16348/100000: episode: 213, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 93.420, mean reward: 2.031 [1.523, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-0.603, 10.153], loss: 0.110815, mae: 0.345696, mean_q: 4.026771
 16377/100000: episode: 214, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 105.379, mean reward: 3.634 [2.481, 5.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.787, 10.493], loss: 0.116640, mae: 0.332198, mean_q: 4.012237
 16423/100000: episode: 215, duration: 0.258s, episode steps: 46, steps per second: 178, episode reward: 100.671, mean reward: 2.189 [1.582, 5.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.619, 10.227], loss: 0.139922, mae: 0.338493, mean_q: 4.068588
 16433/100000: episode: 216, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 22.676, mean reward: 2.268 [1.998, 2.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.311, 10.100], loss: 0.143605, mae: 0.372818, mean_q: 4.133440
 16443/100000: episode: 217, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 30.102, mean reward: 3.010 [2.487, 3.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.282, 10.100], loss: 0.114636, mae: 0.340257, mean_q: 4.005481
 16489/100000: episode: 218, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 93.449, mean reward: 2.031 [1.479, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.665, 10.289], loss: 0.128373, mae: 0.358103, mean_q: 4.061982
 16518/100000: episode: 219, duration: 0.158s, episode steps: 29, steps per second: 183, episode reward: 86.553, mean reward: 2.985 [2.272, 5.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.178, 10.409], loss: 0.136455, mae: 0.360193, mean_q: 4.050985
 16529/100000: episode: 220, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 34.201, mean reward: 3.109 [2.586, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.457, 10.100], loss: 0.098033, mae: 0.334553, mean_q: 4.056813
 16558/100000: episode: 221, duration: 0.159s, episode steps: 29, steps per second: 182, episode reward: 70.691, mean reward: 2.438 [1.925, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.448, 10.502], loss: 0.136341, mae: 0.357594, mean_q: 4.131490
 16576/100000: episode: 222, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 63.961, mean reward: 3.553 [2.720, 5.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.370, 10.100], loss: 0.132310, mae: 0.358649, mean_q: 4.030851
 16605/100000: episode: 223, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 59.765, mean reward: 2.061 [1.475, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.111, 10.210], loss: 0.138228, mae: 0.368475, mean_q: 4.108477
 16614/100000: episode: 224, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 28.650, mean reward: 3.183 [2.435, 4.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.162, 10.100], loss: 0.257731, mae: 0.447481, mean_q: 4.164461
 16625/100000: episode: 225, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 34.111, mean reward: 3.101 [2.405, 4.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.367, 10.100], loss: 0.093300, mae: 0.316147, mean_q: 4.101014
 16636/100000: episode: 226, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 28.610, mean reward: 2.601 [2.222, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.300, 10.100], loss: 0.147078, mae: 0.363997, mean_q: 4.193415
 16664/100000: episode: 227, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 70.487, mean reward: 2.517 [1.706, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.079, 10.222], loss: 0.168922, mae: 0.406548, mean_q: 4.148727
 16718/100000: episode: 228, duration: 0.289s, episode steps: 54, steps per second: 187, episode reward: 120.659, mean reward: 2.234 [1.438, 5.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.226, 10.315], loss: 0.145508, mae: 0.366595, mean_q: 4.129183
 16728/100000: episode: 229, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 24.369, mean reward: 2.437 [2.074, 2.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.445, 10.100], loss: 0.094050, mae: 0.310811, mean_q: 4.133715
 16774/100000: episode: 230, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 91.659, mean reward: 1.993 [1.548, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.207, 10.197], loss: 0.144263, mae: 0.372924, mean_q: 4.128189
 16792/100000: episode: 231, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 72.395, mean reward: 4.022 [2.828, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.388, 10.100], loss: 0.095025, mae: 0.322556, mean_q: 4.105942
 16821/100000: episode: 232, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 59.016, mean reward: 2.035 [1.532, 2.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.756, 10.150], loss: 0.143192, mae: 0.375302, mean_q: 4.186537
 16839/100000: episode: 233, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 72.864, mean reward: 4.048 [2.549, 5.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.281, 10.100], loss: 0.147290, mae: 0.377699, mean_q: 4.198518
 16868/100000: episode: 234, duration: 0.164s, episode steps: 29, steps per second: 177, episode reward: 85.256, mean reward: 2.940 [2.057, 5.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.888, 10.456], loss: 0.119276, mae: 0.344082, mean_q: 4.136388
 16886/100000: episode: 235, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 56.375, mean reward: 3.132 [2.457, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.315, 10.100], loss: 0.106072, mae: 0.328179, mean_q: 4.174988
 16896/100000: episode: 236, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 24.172, mean reward: 2.417 [1.968, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.170, 10.100], loss: 0.170381, mae: 0.406216, mean_q: 4.268826
 16907/100000: episode: 237, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 41.450, mean reward: 3.768 [2.817, 4.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.618, 10.100], loss: 0.153679, mae: 0.378950, mean_q: 4.273556
 16916/100000: episode: 238, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 19.438, mean reward: 2.160 [1.832, 3.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.260, 10.100], loss: 0.153741, mae: 0.392668, mean_q: 4.186487
 16925/100000: episode: 239, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 18.505, mean reward: 2.056 [1.937, 2.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.297, 10.100], loss: 0.138832, mae: 0.380848, mean_q: 4.163910
[Info] 2-TH LEVEL FOUND: 6.6097517013549805, Considering 10/90 traces
 16979/100000: episode: 240, duration: 4.369s, episode steps: 54, steps per second: 12, episode reward: 103.740, mean reward: 1.921 [1.445, 5.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.614, 10.324], loss: 0.144210, mae: 0.380537, mean_q: 4.194482
 16991/100000: episode: 241, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 44.178, mean reward: 3.681 [3.170, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.291, 10.100], loss: 0.185074, mae: 0.406095, mean_q: 4.225710
 17003/100000: episode: 242, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 36.511, mean reward: 3.043 [2.361, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.471, 10.100], loss: 0.166858, mae: 0.384541, mean_q: 4.136299
 17009/100000: episode: 243, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 18.872, mean reward: 3.145 [2.726, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.338, 10.100], loss: 0.082933, mae: 0.290249, mean_q: 3.995317
 17024/100000: episode: 244, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 61.650, mean reward: 4.110 [2.739, 7.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.301, 10.100], loss: 0.153868, mae: 0.393644, mean_q: 4.240926
 17033/100000: episode: 245, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 34.421, mean reward: 3.825 [3.122, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.444, 10.100], loss: 0.134008, mae: 0.375580, mean_q: 4.147308
 17047/100000: episode: 246, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 68.113, mean reward: 4.865 [3.825, 6.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.425, 10.100], loss: 0.168867, mae: 0.403064, mean_q: 4.213502
 17055/100000: episode: 247, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 33.601, mean reward: 4.200 [3.146, 6.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.364, 10.100], loss: 0.173235, mae: 0.413871, mean_q: 4.145864
 17062/100000: episode: 248, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 27.022, mean reward: 3.860 [2.585, 5.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.479, 10.100], loss: 0.131153, mae: 0.356252, mean_q: 4.159531
 17071/100000: episode: 249, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 32.406, mean reward: 3.601 [2.479, 5.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.358, 10.100], loss: 0.155305, mae: 0.373731, mean_q: 4.157067
 17078/100000: episode: 250, duration: 0.042s, episode steps: 7, steps per second: 165, episode reward: 17.452, mean reward: 2.493 [2.202, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.307, 10.100], loss: 0.169217, mae: 0.399322, mean_q: 4.381291
 17087/100000: episode: 251, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 28.172, mean reward: 3.130 [2.826, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.473, 10.100], loss: 0.126584, mae: 0.351878, mean_q: 4.108146
 17099/100000: episode: 252, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 52.315, mean reward: 4.360 [3.222, 6.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.159, 10.100], loss: 0.180090, mae: 0.386940, mean_q: 4.315657
 17113/100000: episode: 253, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 47.662, mean reward: 3.404 [2.826, 4.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.100, 10.100], loss: 0.142511, mae: 0.377416, mean_q: 4.257005
 17127/100000: episode: 254, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 57.421, mean reward: 4.101 [3.193, 5.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.524, 10.100], loss: 0.191578, mae: 0.427370, mean_q: 4.377694
 17135/100000: episode: 255, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 37.866, mean reward: 4.733 [2.860, 6.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.456, 10.100], loss: 0.164662, mae: 0.418392, mean_q: 4.335866
 17143/100000: episode: 256, duration: 0.051s, episode steps: 8, steps per second: 155, episode reward: 31.038, mean reward: 3.880 [3.073, 5.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.512, 10.100], loss: 0.155381, mae: 0.359307, mean_q: 4.082275
 17152/100000: episode: 257, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 26.750, mean reward: 2.972 [2.589, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.332, 10.100], loss: 0.140824, mae: 0.374641, mean_q: 4.258526
 17160/100000: episode: 258, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 231.325, mean reward: 28.916 [3.819, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.500, 10.100], loss: 0.129560, mae: 0.366260, mean_q: 4.260442
 17174/100000: episode: 259, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 47.351, mean reward: 3.382 [2.893, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.651, 10.100], loss: 0.282874, mae: 0.429843, mean_q: 4.291231
 17186/100000: episode: 260, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 45.450, mean reward: 3.787 [2.931, 4.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.977, 10.100], loss: 0.168568, mae: 0.395950, mean_q: 4.285915
 17192/100000: episode: 261, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 20.517, mean reward: 3.419 [3.022, 3.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.454, 10.100], loss: 0.105722, mae: 0.336683, mean_q: 4.258358
 17199/100000: episode: 262, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 21.660, mean reward: 3.094 [2.719, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.298, 10.100], loss: 0.181309, mae: 0.428917, mean_q: 4.499225
 17205/100000: episode: 263, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 20.905, mean reward: 3.484 [3.051, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.366, 10.100], loss: 0.119816, mae: 0.348139, mean_q: 4.184521
 17213/100000: episode: 264, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 27.041, mean reward: 3.380 [2.791, 4.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.320, 10.100], loss: 0.140162, mae: 0.371355, mean_q: 4.176896
 17227/100000: episode: 265, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 70.098, mean reward: 5.007 [3.453, 7.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.402, 10.100], loss: 0.164429, mae: 0.389137, mean_q: 4.296746
 17241/100000: episode: 266, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 59.779, mean reward: 4.270 [3.355, 5.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.777, 10.100], loss: 0.172929, mae: 0.397606, mean_q: 4.352812
 17256/100000: episode: 267, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 60.812, mean reward: 4.054 [2.937, 6.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.487, 10.100], loss: 0.162014, mae: 0.412005, mean_q: 4.246345
 17264/100000: episode: 268, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 30.985, mean reward: 3.873 [3.175, 4.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.556, 10.100], loss: 0.175356, mae: 0.411144, mean_q: 4.460640
 17272/100000: episode: 269, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 26.808, mean reward: 3.351 [2.704, 4.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.399, 10.100], loss: 0.152183, mae: 0.408427, mean_q: 4.261040
 17286/100000: episode: 270, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 43.937, mean reward: 3.138 [2.704, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.337, 10.100], loss: 0.156879, mae: 0.397015, mean_q: 4.302279
 17298/100000: episode: 271, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 34.053, mean reward: 2.838 [2.635, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.337, 10.100], loss: 0.133445, mae: 0.375660, mean_q: 4.405603
 17313/100000: episode: 272, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 65.970, mean reward: 4.398 [2.970, 6.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.748, 10.100], loss: 0.204007, mae: 0.438941, mean_q: 4.326572
 17321/100000: episode: 273, duration: 0.054s, episode steps: 8, steps per second: 147, episode reward: 33.183, mean reward: 4.148 [3.388, 6.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.560, 10.100], loss: 0.171398, mae: 0.395983, mean_q: 4.365763
 17330/100000: episode: 274, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 53.920, mean reward: 5.991 [4.885, 7.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.437, 10.100], loss: 0.191873, mae: 0.433994, mean_q: 4.172729
 17338/100000: episode: 275, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 29.713, mean reward: 3.714 [2.694, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.357, 10.100], loss: 0.156194, mae: 0.402687, mean_q: 4.450996
 17353/100000: episode: 276, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 44.757, mean reward: 2.984 [2.623, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.025, 10.100], loss: 9.859687, mae: 0.789009, mean_q: 4.589452
 17361/100000: episode: 277, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 32.618, mean reward: 4.077 [3.640, 5.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.831, 10.100], loss: 0.986575, mae: 1.035626, mean_q: 4.637656
 17370/100000: episode: 278, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 38.583, mean reward: 4.287 [3.704, 5.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.486, 10.100], loss: 0.780311, mae: 0.936604, mean_q: 3.842454
 17376/100000: episode: 279, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 18.643, mean reward: 3.107 [2.514, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.376, 10.100], loss: 0.571840, mae: 0.781091, mean_q: 4.703316
 17385/100000: episode: 280, duration: 0.050s, episode steps: 9, steps per second: 182, episode reward: 32.648, mean reward: 3.628 [3.255, 4.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.400, 10.100], loss: 0.582587, mae: 0.742940, mean_q: 4.441984
 17394/100000: episode: 281, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 42.734, mean reward: 4.748 [2.770, 8.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.249, 10.100], loss: 15.638975, mae: 1.016429, mean_q: 4.884421
 17400/100000: episode: 282, duration: 0.040s, episode steps: 6, steps per second: 152, episode reward: 19.829, mean reward: 3.305 [3.056, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.387, 10.100], loss: 23.390821, mae: 1.224027, mean_q: 4.216405
 17408/100000: episode: 283, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 87.100, mean reward: 10.887 [4.775, 30.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.517, 10.100], loss: 15.357450, mae: 1.167554, mean_q: 5.124647
 17416/100000: episode: 284, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 29.839, mean reward: 3.730 [3.076, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.377, 10.100], loss: 0.533663, mae: 0.689530, mean_q: 4.804030
 17423/100000: episode: 285, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 19.994, mean reward: 2.856 [2.552, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.759, 10.100], loss: 0.800474, mae: 0.694344, mean_q: 4.012556
 17435/100000: episode: 286, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 38.626, mean reward: 3.219 [2.664, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.389, 10.100], loss: 1.167840, mae: 0.644327, mean_q: 4.568566
 17444/100000: episode: 287, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 37.358, mean reward: 4.151 [3.307, 4.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.351, 10.100], loss: 0.391297, mae: 0.504469, mean_q: 4.487567
 17456/100000: episode: 288, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 53.733, mean reward: 4.478 [2.878, 6.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.513, 10.100], loss: 1.102255, mae: 0.598351, mean_q: 4.399648
 17464/100000: episode: 289, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 37.035, mean reward: 4.629 [3.285, 8.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.357, 10.100], loss: 0.310274, mae: 0.493896, mean_q: 4.540648
 17472/100000: episode: 290, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 52.680, mean reward: 6.585 [3.987, 11.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.567, 10.100], loss: 0.453627, mae: 0.553014, mean_q: 4.421257
 17480/100000: episode: 291, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 26.010, mean reward: 3.251 [2.962, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.437, 10.100], loss: 0.383969, mae: 0.555458, mean_q: 4.359243
 17492/100000: episode: 292, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 54.228, mean reward: 4.519 [2.225, 7.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.328, 10.100], loss: 0.391207, mae: 0.526798, mean_q: 4.618195
 17498/100000: episode: 293, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 28.986, mean reward: 4.831 [3.366, 8.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.512, 10.100], loss: 0.460359, mae: 0.572287, mean_q: 4.458360
 17506/100000: episode: 294, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 33.092, mean reward: 4.137 [3.774, 4.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.343, 10.100], loss: 15.167253, mae: 0.945713, mean_q: 4.789089
 17518/100000: episode: 295, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 55.431, mean reward: 4.619 [3.369, 6.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.431, 10.100], loss: 0.486276, mae: 0.613535, mean_q: 4.532973
 17525/100000: episode: 296, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 24.368, mean reward: 3.481 [3.300, 3.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.294, 10.100], loss: 0.480985, mae: 0.616256, mean_q: 4.916766
 17532/100000: episode: 297, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 21.231, mean reward: 3.033 [2.503, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.409, 10.100], loss: 0.279308, mae: 0.477603, mean_q: 4.508883
 17538/100000: episode: 298, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 27.312, mean reward: 4.552 [3.669, 5.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.448, 10.100], loss: 0.426797, mae: 0.486362, mean_q: 4.444923
 17547/100000: episode: 299, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 38.886, mean reward: 4.321 [2.943, 5.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.293, 10.100], loss: 0.452464, mae: 0.554106, mean_q: 4.809107
 17556/100000: episode: 300, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 31.636, mean reward: 3.515 [3.121, 4.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.382, 10.100], loss: 0.327152, mae: 0.500187, mean_q: 4.362728
 17564/100000: episode: 301, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 80.642, mean reward: 10.080 [3.885, 36.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.464, 10.100], loss: 0.323510, mae: 0.522769, mean_q: 4.828773
 17571/100000: episode: 302, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 23.169, mean reward: 3.310 [2.835, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.321, 10.100], loss: 0.381983, mae: 0.511687, mean_q: 4.686191
 17577/100000: episode: 303, duration: 0.039s, episode steps: 6, steps per second: 153, episode reward: 21.306, mean reward: 3.551 [3.054, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.390, 10.100], loss: 0.474673, mae: 0.516273, mean_q: 4.632761
 17584/100000: episode: 304, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 19.026, mean reward: 2.718 [2.199, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.246, 10.100], loss: 0.326473, mae: 0.584232, mean_q: 5.209876
 17592/100000: episode: 305, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 28.744, mean reward: 3.593 [2.794, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.277, 10.100], loss: 0.367115, mae: 0.517991, mean_q: 4.450894
 17604/100000: episode: 306, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 45.048, mean reward: 3.754 [2.801, 5.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.515, 10.100], loss: 10.640687, mae: 1.004607, mean_q: 5.197157
 17613/100000: episode: 307, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 45.551, mean reward: 5.061 [3.283, 7.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.473, 10.100], loss: 0.395573, mae: 0.590188, mean_q: 4.246290
 17628/100000: episode: 308, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 50.060, mean reward: 3.337 [2.290, 5.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.381, 10.100], loss: 0.392376, mae: 0.523311, mean_q: 4.776090
 17640/100000: episode: 309, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 51.476, mean reward: 4.290 [3.589, 5.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.537, 10.100], loss: 1.612651, mae: 0.654709, mean_q: 4.619296
 17647/100000: episode: 310, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 29.530, mean reward: 4.219 [3.058, 5.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.358, 10.100], loss: 1.187909, mae: 1.200994, mean_q: 5.632812
 17655/100000: episode: 311, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 30.249, mean reward: 3.781 [3.198, 4.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.253, 10.100], loss: 0.489803, mae: 0.648998, mean_q: 4.702461
 17661/100000: episode: 312, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 19.727, mean reward: 3.288 [2.737, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.378, 10.100], loss: 2.122436, mae: 0.914551, mean_q: 3.920003
 17670/100000: episode: 313, duration: 0.050s, episode steps: 9, steps per second: 182, episode reward: 32.469, mean reward: 3.608 [2.693, 4.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.450, 10.100], loss: 0.431775, mae: 0.572134, mean_q: 4.784661
 17684/100000: episode: 314, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 53.796, mean reward: 3.843 [3.022, 5.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.724, 10.100], loss: 10.005272, mae: 0.748859, mean_q: 4.688870
 17693/100000: episode: 315, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 43.024, mean reward: 4.780 [3.739, 7.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.934, 10.100], loss: 0.957866, mae: 0.877610, mean_q: 5.167214
 17701/100000: episode: 316, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 38.423, mean reward: 4.803 [4.300, 5.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.524, 10.100], loss: 0.558790, mae: 0.702689, mean_q: 4.392882
 17710/100000: episode: 317, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 33.283, mean reward: 3.698 [2.953, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.951, 10.100], loss: 1.829599, mae: 0.614537, mean_q: 4.572645
 17716/100000: episode: 318, duration: 0.038s, episode steps: 6, steps per second: 158, episode reward: 17.649, mean reward: 2.941 [2.577, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.366, 10.100], loss: 0.471718, mae: 0.604453, mean_q: 4.977420
 17728/100000: episode: 319, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 42.574, mean reward: 3.548 [2.676, 4.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.434, 10.100], loss: 0.335688, mae: 0.505641, mean_q: 4.571250
 17742/100000: episode: 320, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 74.542, mean reward: 5.324 [3.281, 9.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.499, 10.100], loss: 10.069847, mae: 0.832106, mean_q: 5.018857
 17750/100000: episode: 321, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 36.682, mean reward: 4.585 [3.530, 6.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.368, 10.100], loss: 16.308317, mae: 1.158939, mean_q: 5.459061
 17762/100000: episode: 322, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 47.669, mean reward: 3.972 [2.915, 5.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.482, 10.100], loss: 0.540058, mae: 0.649802, mean_q: 4.723062
 17770/100000: episode: 323, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 27.200, mean reward: 3.400 [2.804, 3.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.453, 10.100], loss: 15.368660, mae: 1.003136, mean_q: 4.950294
 17779/100000: episode: 324, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 37.768, mean reward: 4.196 [3.260, 5.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.097, 10.100], loss: 0.861178, mae: 0.806887, mean_q: 5.053062
 17785/100000: episode: 325, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 23.113, mean reward: 3.852 [3.272, 4.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.379, 10.100], loss: 0.676727, mae: 0.815325, mean_q: 4.052309
 17794/100000: episode: 326, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 33.879, mean reward: 3.764 [2.669, 5.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.476, 10.100], loss: 0.621305, mae: 0.668620, mean_q: 5.092833
 17808/100000: episode: 327, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 44.813, mean reward: 3.201 [2.756, 4.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.514, 10.100], loss: 1.235049, mae: 0.673100, mean_q: 5.001421
 17823/100000: episode: 328, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 46.485, mean reward: 3.099 [2.509, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.207, 10.100], loss: 0.388128, mae: 0.565131, mean_q: 4.847846
 17831/100000: episode: 329, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 31.400, mean reward: 3.925 [3.367, 4.648], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.787, 10.100], loss: 0.544863, mae: 0.613660, mean_q: 4.923347
[Info] 3-TH LEVEL FOUND: 8.946846961975098, Considering 10/90 traces
 17840/100000: episode: 330, duration: 4.117s, episode steps: 9, steps per second: 2, episode reward: 43.281, mean reward: 4.809 [3.726, 6.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.365, 10.100], loss: 0.715854, mae: 0.791794, mean_q: 5.465964
 17842/100000: episode: 331, duration: 0.015s, episode steps: 2, steps per second: 129, episode reward: 7.687, mean reward: 3.843 [3.561, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.524, 10.100], loss: 0.860031, mae: 0.683860, mean_q: 4.848683
 17843/100000: episode: 332, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 4.941, mean reward: 4.941 [4.941, 4.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.536, 10.100], loss: 0.313709, mae: 0.556449, mean_q: 5.309692
 17844/100000: episode: 333, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 5.819, mean reward: 5.819 [5.819, 5.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.605, 10.100], loss: 0.247714, mae: 0.505241, mean_q: 4.404870
 17846/100000: episode: 334, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: 9.680, mean reward: 4.840 [4.565, 5.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.484, 10.100], loss: 0.304846, mae: 0.544904, mean_q: 4.490266
 17848/100000: episode: 335, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 7.524, mean reward: 3.762 [3.735, 3.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.518, 10.100], loss: 0.402494, mae: 0.621856, mean_q: 4.375447
 17850/100000: episode: 336, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 17.360, mean reward: 8.680 [5.037, 12.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.582, 10.100], loss: 0.184212, mae: 0.417391, mean_q: 4.593318
 17851/100000: episode: 337, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 6.387, mean reward: 6.387 [6.387, 6.387], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.542, 10.200], loss: 0.611195, mae: 0.477023, mean_q: 4.775247
 17853/100000: episode: 338, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 9.185, mean reward: 4.593 [4.462, 4.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.493, 10.100], loss: 0.175438, mae: 0.472191, mean_q: 5.014175
 17854/100000: episode: 339, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 4.377, mean reward: 4.377 [4.377, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.475, 10.100], loss: 1.258550, mae: 0.765644, mean_q: 5.115034
 17856/100000: episode: 340, duration: 0.014s, episode steps: 2, steps per second: 143, episode reward: 8.145, mean reward: 4.072 [4.038, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.446, 10.100], loss: 0.449283, mae: 0.622299, mean_q: 4.933807
 17857/100000: episode: 341, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 5.518, mean reward: 5.518 [5.518, 5.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.543, 10.200], loss: 0.190816, mae: 0.491273, mean_q: 5.056369
 17859/100000: episode: 342, duration: 0.016s, episode steps: 2, steps per second: 124, episode reward: 13.337, mean reward: 6.669 [5.838, 7.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.497, 10.100], loss: 0.279275, mae: 0.506236, mean_q: 5.161592
 17860/100000: episode: 343, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 3.996, mean reward: 3.996 [3.996, 3.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.541, 10.200], loss: 1.129143, mae: 0.742165, mean_q: 4.777563
 17861/100000: episode: 344, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 15.021, mean reward: 15.021 [15.021, 15.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.488, 10.200], loss: 0.199286, mae: 0.358492, mean_q: 4.487172
 17862/100000: episode: 345, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 4.151, mean reward: 4.151 [4.151, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.520, 10.200], loss: 0.116675, mae: 0.338939, mean_q: 4.456130
 17863/100000: episode: 346, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 3.697, mean reward: 3.697 [3.697, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.419, 10.100], loss: 0.101023, mae: 0.349916, mean_q: 4.670145
 17864/100000: episode: 347, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 4.858, mean reward: 4.858 [4.858, 4.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.550, 10.100], loss: 0.196520, mae: 0.429186, mean_q: 5.092311
 17865/100000: episode: 348, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 3.655, mean reward: 3.655 [3.655, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.563, 10.100], loss: 1.012050, mae: 0.757508, mean_q: 5.003165
 17866/100000: episode: 349, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 8.893, mean reward: 8.893 [8.893, 8.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.504, 10.200], loss: 0.433438, mae: 0.467582, mean_q: 4.765974
 17867/100000: episode: 350, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 4.681, mean reward: 4.681 [4.681, 4.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.453, 10.100], loss: 0.267844, mae: 0.497594, mean_q: 4.972329
 17868/100000: episode: 351, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 4.138, mean reward: 4.138 [4.138, 4.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.507, 10.200], loss: 0.816966, mae: 0.698986, mean_q: 4.934825
 17869/100000: episode: 352, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 5.089, mean reward: 5.089 [5.089, 5.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.466, 10.100], loss: 0.227868, mae: 0.462363, mean_q: 4.700042
 17870/100000: episode: 353, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 3.805, mean reward: 3.805 [3.805, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.583, 10.100], loss: 0.266959, mae: 0.522238, mean_q: 4.749810
 17871/100000: episode: 354, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 6.102, mean reward: 6.102 [6.102, 6.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.540, 10.200], loss: 0.174146, mae: 0.465883, mean_q: 4.729530
 17875/100000: episode: 355, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 29.966, mean reward: 7.491 [5.426, 10.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.487, 10.100], loss: 0.502407, mae: 0.575297, mean_q: 5.045100
 17877/100000: episode: 356, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 7.682, mean reward: 3.841 [3.766, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.558, 10.100], loss: 0.305268, mae: 0.470882, mean_q: 4.688664
 17878/100000: episode: 357, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 3.232, mean reward: 3.232 [3.232, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.477, 10.100], loss: 0.108857, mae: 0.356697, mean_q: 4.945908
 17879/100000: episode: 358, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 3.577, mean reward: 3.577 [3.577, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.517, 10.100], loss: 0.279257, mae: 0.398678, mean_q: 4.487743
 17883/100000: episode: 359, duration: 0.023s, episode steps: 4, steps per second: 178, episode reward: 35.967, mean reward: 8.992 [7.427, 10.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.527, 10.100], loss: 0.590602, mae: 0.607922, mean_q: 4.633879
 17887/100000: episode: 360, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: 27.362, mean reward: 6.840 [6.033, 7.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.409, 10.100], loss: 0.682926, mae: 0.668933, mean_q: 4.865820
 17888/100000: episode: 361, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 5.461, mean reward: 5.461 [5.461, 5.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.492, 10.200], loss: 0.486561, mae: 0.600897, mean_q: 4.619017
 17890/100000: episode: 362, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: 9.705, mean reward: 4.852 [4.763, 4.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.517, 10.100], loss: 0.781455, mae: 0.739036, mean_q: 5.072925
 17891/100000: episode: 363, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 5.007, mean reward: 5.007 [5.007, 5.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.500, 10.100], loss: 0.221453, mae: 0.574613, mean_q: 5.143338
 17892/100000: episode: 364, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 3.874, mean reward: 3.874 [3.874, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.557, 10.100], loss: 0.443082, mae: 0.775384, mean_q: 5.520177
 17893/100000: episode: 365, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 9.151, mean reward: 9.151 [9.151, 9.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.492, 10.200], loss: 1.803287, mae: 1.319256, mean_q: 5.787002
 17895/100000: episode: 366, duration: 0.017s, episode steps: 2, steps per second: 116, episode reward: 7.695, mean reward: 3.848 [3.736, 3.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.516, 10.100], loss: 1.062092, mae: 0.960095, mean_q: 5.351920
 17899/100000: episode: 367, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 16.705, mean reward: 4.176 [3.835, 4.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.446, 10.100], loss: 0.519891, mae: 0.587573, mean_q: 4.932103
 17901/100000: episode: 368, duration: 0.018s, episode steps: 2, steps per second: 112, episode reward: 10.964, mean reward: 5.482 [5.077, 5.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.465, 10.100], loss: 0.378240, mae: 0.559357, mean_q: 4.609918
 17902/100000: episode: 369, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 3.768, mean reward: 3.768 [3.768, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.485, 10.100], loss: 0.731192, mae: 0.744591, mean_q: 4.785673
 17904/100000: episode: 370, duration: 0.019s, episode steps: 2, steps per second: 108, episode reward: 8.538, mean reward: 4.269 [4.163, 4.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.484, 10.100], loss: 0.486652, mae: 0.645345, mean_q: 4.514990
 17905/100000: episode: 371, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 7.488, mean reward: 7.488 [7.488, 7.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.516, 10.100], loss: 0.334573, mae: 0.502765, mean_q: 4.689942
 17906/100000: episode: 372, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 5.274, mean reward: 5.274 [5.274, 5.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.574, 10.200], loss: 0.813506, mae: 0.765829, mean_q: 5.284853
 17907/100000: episode: 373, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 3.986, mean reward: 3.986 [3.986, 3.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.482, 10.100], loss: 0.431376, mae: 0.519959, mean_q: 4.548651
 17909/100000: episode: 374, duration: 0.013s, episode steps: 2, steps per second: 150, episode reward: 8.194, mean reward: 4.097 [4.058, 4.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.478, 10.100], loss: 0.696068, mae: 0.704610, mean_q: 5.335155
 17910/100000: episode: 375, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 3.568, mean reward: 3.568 [3.568, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.528, 10.100], loss: 0.260393, mae: 0.513600, mean_q: 5.548769
 17911/100000: episode: 376, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 4.373, mean reward: 4.373 [4.373, 4.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.553, 10.200], loss: 0.261816, mae: 0.586381, mean_q: 5.297575
 17912/100000: episode: 377, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 3.927, mean reward: 3.927 [3.927, 3.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.532, 10.100], loss: 0.157941, mae: 0.451742, mean_q: 5.062322
 17913/100000: episode: 378, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 5.982, mean reward: 5.982 [5.982, 5.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.559, 10.200], loss: 0.150258, mae: 0.408527, mean_q: 5.139498
 17914/100000: episode: 379, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 4.336, mean reward: 4.336 [4.336, 4.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.505, 10.100], loss: 0.339613, mae: 0.490379, mean_q: 4.999611
 17916/100000: episode: 380, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 10.197, mean reward: 5.099 [4.676, 5.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.516, 10.100], loss: 0.924471, mae: 0.779522, mean_q: 4.969339
 17917/100000: episode: 381, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 5.587, mean reward: 5.587 [5.587, 5.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.588, 10.200], loss: 2.082329, mae: 0.912994, mean_q: 4.977383
 17918/100000: episode: 382, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 5.100, mean reward: 5.100 [5.100, 5.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.489, 10.100], loss: 0.534592, mae: 0.678841, mean_q: 4.893157
 17919/100000: episode: 383, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 3.965, mean reward: 3.965 [3.965, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.559, 10.100], loss: 0.533746, mae: 0.528905, mean_q: 4.905605
 17920/100000: episode: 384, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 3.298, mean reward: 3.298 [3.298, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.566, 10.100], loss: 0.388971, mae: 0.558518, mean_q: 5.022794
 17921/100000: episode: 385, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 15.593, mean reward: 15.593 [15.593, 15.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.492, 10.200], loss: 1.651190, mae: 0.857686, mean_q: 5.011519
 17922/100000: episode: 386, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 3.960, mean reward: 3.960 [3.960, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.480, 10.100], loss: 0.426416, mae: 0.466410, mean_q: 4.578205
 17926/100000: episode: 387, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 24.831, mean reward: 6.208 [4.540, 7.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.482, 10.100], loss: 0.511162, mae: 0.614373, mean_q: 5.274475
 17930/100000: episode: 388, duration: 0.026s, episode steps: 4, steps per second: 157, episode reward: 37.754, mean reward: 9.438 [7.169, 12.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.429, 10.100], loss: 0.771501, mae: 0.651849, mean_q: 4.929058
 17932/100000: episode: 389, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 8.225, mean reward: 4.113 [4.103, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.546, 10.100], loss: 0.329864, mae: 0.575535, mean_q: 4.950088
 17933/100000: episode: 390, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 6.991, mean reward: 6.991 [6.991, 6.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.533, 10.200], loss: 13.336594, mae: 1.456079, mean_q: 5.026694
 17934/100000: episode: 391, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 4.260, mean reward: 4.260 [4.260, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.484, 10.100], loss: 0.730335, mae: 0.596270, mean_q: 5.058415
 17935/100000: episode: 392, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 5.319, mean reward: 5.319 [5.319, 5.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.458, 10.100], loss: 0.273377, mae: 0.568872, mean_q: 5.230472
 17936/100000: episode: 393, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 14.781, mean reward: 14.781 [14.781, 14.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.538, 10.100], loss: 1.257670, mae: 0.905308, mean_q: 5.333244
 17938/100000: episode: 394, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 11.465, mean reward: 5.733 [5.011, 6.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.495, 10.100], loss: 0.223684, mae: 0.466467, mean_q: 4.974484
 17940/100000: episode: 395, duration: 0.013s, episode steps: 2, steps per second: 148, episode reward: 9.458, mean reward: 4.729 [4.225, 5.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.669, 10.100], loss: 0.757033, mae: 0.651572, mean_q: 4.955268
 17942/100000: episode: 396, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 15.001, mean reward: 7.501 [6.759, 8.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.530, 10.100], loss: 0.527673, mae: 0.597778, mean_q: 4.516006
 17943/100000: episode: 397, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 6.153, mean reward: 6.153 [6.153, 6.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.515, 10.200], loss: 0.244543, mae: 0.424880, mean_q: 4.402108
 17945/100000: episode: 398, duration: 0.017s, episode steps: 2, steps per second: 121, episode reward: 8.725, mean reward: 4.362 [4.175, 4.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.447, 10.100], loss: 0.481721, mae: 0.629394, mean_q: 4.516057
 17947/100000: episode: 399, duration: 0.016s, episode steps: 2, steps per second: 123, episode reward: 9.213, mean reward: 4.606 [3.866, 5.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.485, 10.100], loss: 0.486840, mae: 0.574921, mean_q: 4.622731
 17949/100000: episode: 400, duration: 0.017s, episode steps: 2, steps per second: 118, episode reward: 9.034, mean reward: 4.517 [4.186, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.492, 10.100], loss: 0.719484, mae: 0.546428, mean_q: 4.910309
 17950/100000: episode: 401, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 4.836, mean reward: 4.836 [4.836, 4.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.441, 10.100], loss: 0.538378, mae: 0.586734, mean_q: 5.003243
 17951/100000: episode: 402, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 4.069, mean reward: 4.069 [4.069, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.564, 10.100], loss: 0.909258, mae: 0.927954, mean_q: 5.424510
 17952/100000: episode: 403, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 3.989, mean reward: 3.989 [3.989, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.546, 10.100], loss: 0.756153, mae: 0.697885, mean_q: 5.077803
 17953/100000: episode: 404, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 4.230, mean reward: 4.230 [4.230, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.545, 10.100], loss: 0.497014, mae: 0.605667, mean_q: 4.752737
 17954/100000: episode: 405, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 15.133, mean reward: 15.133 [15.133, 15.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.511, 10.200], loss: 120.257721, mae: 3.253302, mean_q: 5.223894
 17956/100000: episode: 406, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 6.056, mean reward: 3.028 [2.947, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.460, 10.100], loss: 0.788049, mae: 0.845616, mean_q: 5.580943
 17958/100000: episode: 407, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 9.322, mean reward: 4.661 [4.387, 4.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.437, 10.100], loss: 0.901564, mae: 0.933415, mean_q: 5.519779
 17960/100000: episode: 408, duration: 0.019s, episode steps: 2, steps per second: 106, episode reward: 7.576, mean reward: 3.788 [3.220, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.467, 10.100], loss: 0.767132, mae: 0.816968, mean_q: 5.334361
 17961/100000: episode: 409, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 5.397, mean reward: 5.397 [5.397, 5.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.566, 10.100], loss: 0.959045, mae: 0.794163, mean_q: 4.936522
 17962/100000: episode: 410, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 4.789, mean reward: 4.789 [4.789, 4.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.467, 10.100], loss: 0.813792, mae: 0.800999, mean_q: 4.633912
 17964/100000: episode: 411, duration: 0.015s, episode steps: 2, steps per second: 132, episode reward: 8.980, mean reward: 4.490 [4.460, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.456, 10.100], loss: 0.355680, mae: 0.561394, mean_q: 4.622051
 17966/100000: episode: 412, duration: 0.014s, episode steps: 2, steps per second: 142, episode reward: 7.751, mean reward: 3.875 [3.355, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.445, 10.100], loss: 0.457084, mae: 0.648970, mean_q: 4.255417
 17967/100000: episode: 413, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 4.398, mean reward: 4.398 [4.398, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.444, 10.100], loss: 135.628815, mae: 3.647946, mean_q: 4.845867
 17968/100000: episode: 414, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 4.972, mean reward: 4.972 [4.972, 4.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.516, 10.100], loss: 0.408996, mae: 0.584297, mean_q: 4.858862
 17969/100000: episode: 415, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 3.728, mean reward: 3.728 [3.728, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.472, 10.100], loss: 0.625448, mae: 0.697577, mean_q: 5.022267
 17971/100000: episode: 416, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 15.717, mean reward: 7.858 [6.166, 9.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.509, 10.100], loss: 0.341959, mae: 0.591791, mean_q: 5.395216
 17972/100000: episode: 417, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 11.540, mean reward: 11.540 [11.540, 11.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.467, 10.200], loss: 1.344221, mae: 0.942548, mean_q: 5.234325
 17974/100000: episode: 418, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 8.182, mean reward: 4.091 [3.938, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.563, 10.100], loss: 0.231404, mae: 0.522414, mean_q: 5.242503
 17976/100000: episode: 419, duration: 0.019s, episode steps: 2, steps per second: 105, episode reward: 10.499, mean reward: 5.250 [4.383, 6.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.525, 10.100], loss: 0.932529, mae: 0.704652, mean_q: 5.580456
[Info] NOT FOUND NEW LEVEL, Current Best Level is 8.946846961975098
 17978/100000: episode: 420, duration: 3.890s, episode steps: 2, steps per second: 1, episode reward: 8.724, mean reward: 4.362 [4.091, 4.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.481, 10.100], loss: 0.480420, mae: 0.580669, mean_q: 4.915049
 18078/100000: episode: 421, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 184.757, mean reward: 1.848 [1.451, 2.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.759, 10.098], loss: 0.560014, mae: 0.640097, mean_q: 4.984186
 18178/100000: episode: 422, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 204.837, mean reward: 2.048 [1.478, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.168, 10.098], loss: 2.184953, mae: 0.693124, mean_q: 4.996420
 18278/100000: episode: 423, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 200.317, mean reward: 2.003 [1.440, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.913, 10.162], loss: 0.516414, mae: 0.610876, mean_q: 4.863571
 18378/100000: episode: 424, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 184.596, mean reward: 1.846 [1.444, 2.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.533, 10.098], loss: 0.845497, mae: 0.637718, mean_q: 4.896128
 18478/100000: episode: 425, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.659, mean reward: 1.907 [1.454, 2.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.820, 10.282], loss: 1.946734, mae: 0.685744, mean_q: 4.959756
 18578/100000: episode: 426, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.124, mean reward: 1.951 [1.450, 4.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.386, 10.098], loss: 3.261607, mae: 0.722945, mean_q: 4.988944
 18678/100000: episode: 427, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 188.017, mean reward: 1.880 [1.458, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.897, 10.098], loss: 2.149781, mae: 0.699536, mean_q: 4.926469
 18778/100000: episode: 428, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 214.979, mean reward: 2.150 [1.467, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.451, 10.223], loss: 4.586048, mae: 0.756693, mean_q: 4.994559
 18878/100000: episode: 429, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 197.608, mean reward: 1.976 [1.453, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.529, 10.098], loss: 4.651693, mae: 0.816061, mean_q: 4.957234
 18978/100000: episode: 430, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 184.874, mean reward: 1.849 [1.459, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.407, 10.098], loss: 4.417217, mae: 0.718482, mean_q: 4.960303
 19078/100000: episode: 431, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 188.171, mean reward: 1.882 [1.453, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.636, 10.305], loss: 4.726960, mae: 0.870694, mean_q: 5.024154
 19178/100000: episode: 432, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 202.998, mean reward: 2.030 [1.464, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.796, 10.221], loss: 3.470503, mae: 0.779872, mean_q: 5.021163
 19278/100000: episode: 433, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 180.691, mean reward: 1.807 [1.467, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.989, 10.098], loss: 3.462948, mae: 0.720762, mean_q: 4.983160
 19378/100000: episode: 434, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 176.290, mean reward: 1.763 [1.470, 2.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.536, 10.098], loss: 3.185136, mae: 0.708533, mean_q: 5.024323
 19478/100000: episode: 435, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 181.173, mean reward: 1.812 [1.470, 2.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.260, 10.234], loss: 1.904454, mae: 0.669922, mean_q: 4.929755
 19578/100000: episode: 436, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 192.701, mean reward: 1.927 [1.461, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.341, 10.406], loss: 1.949605, mae: 0.663613, mean_q: 4.946100
 19678/100000: episode: 437, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 200.705, mean reward: 2.007 [1.513, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.518, 10.360], loss: 4.450392, mae: 0.741871, mean_q: 4.966943
 19778/100000: episode: 438, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 194.464, mean reward: 1.945 [1.450, 4.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.064, 10.098], loss: 2.038144, mae: 0.673292, mean_q: 4.941181
 19878/100000: episode: 439, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 190.847, mean reward: 1.908 [1.485, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.902, 10.098], loss: 3.076078, mae: 0.688724, mean_q: 4.930762
 19978/100000: episode: 440, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 177.392, mean reward: 1.774 [1.477, 2.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.961, 10.258], loss: 0.749105, mae: 0.600164, mean_q: 4.852873
 20078/100000: episode: 441, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 182.656, mean reward: 1.827 [1.440, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.033, 10.098], loss: 0.629421, mae: 0.570621, mean_q: 4.810020
 20178/100000: episode: 442, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 206.065, mean reward: 2.061 [1.445, 6.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.305, 10.274], loss: 0.723712, mae: 0.594695, mean_q: 4.802746
 20278/100000: episode: 443, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 197.768, mean reward: 1.978 [1.464, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.725, 10.385], loss: 1.805746, mae: 0.626857, mean_q: 4.826069
 20378/100000: episode: 444, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 181.258, mean reward: 1.813 [1.453, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.263, 10.128], loss: 3.216359, mae: 0.683673, mean_q: 4.770115
 20478/100000: episode: 445, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.925, mean reward: 1.849 [1.511, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.675, 10.098], loss: 3.241637, mae: 0.660532, mean_q: 4.765855
 20578/100000: episode: 446, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 180.695, mean reward: 1.807 [1.435, 2.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.644, 10.098], loss: 2.175720, mae: 0.656302, mean_q: 4.792474
 20678/100000: episode: 447, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.854, mean reward: 1.859 [1.488, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.430, 10.145], loss: 0.597626, mae: 0.572928, mean_q: 4.686966
 20778/100000: episode: 448, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 211.686, mean reward: 2.117 [1.523, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.936, 10.156], loss: 3.147893, mae: 0.621369, mean_q: 4.712927
 20878/100000: episode: 449, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 194.460, mean reward: 1.945 [1.457, 4.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.800, 10.378], loss: 3.340210, mae: 0.662065, mean_q: 4.794458
 20978/100000: episode: 450, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 243.006, mean reward: 2.430 [1.449, 11.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.434, 10.098], loss: 0.645931, mae: 0.575845, mean_q: 4.692088
 21078/100000: episode: 451, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 201.259, mean reward: 2.013 [1.492, 4.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.491, 10.098], loss: 1.928852, mae: 0.620796, mean_q: 4.727926
 21178/100000: episode: 452, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 190.524, mean reward: 1.905 [1.450, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.854, 10.132], loss: 2.374917, mae: 0.639787, mean_q: 4.692591
 21278/100000: episode: 453, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 188.779, mean reward: 1.888 [1.502, 3.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.522, 10.098], loss: 0.602987, mae: 0.539887, mean_q: 4.600669
 21378/100000: episode: 454, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 194.054, mean reward: 1.941 [1.499, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.915, 10.098], loss: 0.693890, mae: 0.551730, mean_q: 4.605791
 21478/100000: episode: 455, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 198.624, mean reward: 1.986 [1.471, 3.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.805, 10.098], loss: 1.966029, mae: 0.584914, mean_q: 4.594748
 21578/100000: episode: 456, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 188.619, mean reward: 1.886 [1.464, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.779, 10.098], loss: 1.911150, mae: 0.592299, mean_q: 4.636996
 21678/100000: episode: 457, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 179.143, mean reward: 1.791 [1.437, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.807, 10.128], loss: 1.729291, mae: 0.576185, mean_q: 4.580619
 21778/100000: episode: 458, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 178.692, mean reward: 1.787 [1.442, 2.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.172, 10.214], loss: 2.001556, mae: 0.590759, mean_q: 4.551523
 21878/100000: episode: 459, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 203.677, mean reward: 2.037 [1.443, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.472, 10.098], loss: 4.520445, mae: 0.665926, mean_q: 4.559236
 21978/100000: episode: 460, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 207.435, mean reward: 2.074 [1.553, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.783, 10.098], loss: 3.988120, mae: 0.595883, mean_q: 4.440063
 22078/100000: episode: 461, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 197.337, mean reward: 1.973 [1.494, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.867, 10.217], loss: 0.489924, mae: 0.463803, mean_q: 4.361050
 22178/100000: episode: 462, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 186.171, mean reward: 1.862 [1.466, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.935, 10.098], loss: 0.603990, mae: 0.494636, mean_q: 4.300584
 22278/100000: episode: 463, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 182.729, mean reward: 1.827 [1.443, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.029, 10.120], loss: 0.862977, mae: 0.478762, mean_q: 4.239437
 22378/100000: episode: 464, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.014, mean reward: 1.830 [1.469, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.903, 10.098], loss: 0.626495, mae: 0.472685, mean_q: 4.183831
 22478/100000: episode: 465, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.328, mean reward: 1.823 [1.464, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.837, 10.098], loss: 0.335372, mae: 0.437707, mean_q: 4.119512
 22578/100000: episode: 466, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 181.391, mean reward: 1.814 [1.451, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.710, 10.230], loss: 0.257294, mae: 0.372795, mean_q: 3.981479
 22678/100000: episode: 467, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 189.329, mean reward: 1.893 [1.436, 6.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.509, 10.098], loss: 0.203622, mae: 0.361172, mean_q: 3.915153
 22778/100000: episode: 468, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 195.332, mean reward: 1.953 [1.465, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.409, 10.098], loss: 0.192512, mae: 0.338821, mean_q: 3.851618
 22878/100000: episode: 469, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 189.439, mean reward: 1.894 [1.450, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.975, 10.098], loss: 0.146900, mae: 0.327231, mean_q: 3.838642
 22978/100000: episode: 470, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.750, mean reward: 1.928 [1.465, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.814, 10.098], loss: 0.096030, mae: 0.290551, mean_q: 3.800730
 23078/100000: episode: 471, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 213.964, mean reward: 2.140 [1.561, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.413, 10.098], loss: 0.102328, mae: 0.298673, mean_q: 3.808353
 23178/100000: episode: 472, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.956, mean reward: 1.940 [1.456, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.612, 10.098], loss: 0.098535, mae: 0.295478, mean_q: 3.792478
 23278/100000: episode: 473, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.315, mean reward: 1.863 [1.465, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.649, 10.161], loss: 0.087868, mae: 0.283924, mean_q: 3.789571
 23378/100000: episode: 474, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 186.383, mean reward: 1.864 [1.477, 2.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.456, 10.098], loss: 0.087673, mae: 0.293803, mean_q: 3.790883
 23478/100000: episode: 475, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.274, mean reward: 1.803 [1.472, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.520, 10.225], loss: 0.104681, mae: 0.296951, mean_q: 3.801075
 23578/100000: episode: 476, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 212.053, mean reward: 2.121 [1.466, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.847, 10.339], loss: 0.077182, mae: 0.280309, mean_q: 3.777262
 23678/100000: episode: 477, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 205.517, mean reward: 2.055 [1.460, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.794, 10.301], loss: 0.099857, mae: 0.305990, mean_q: 3.822335
 23778/100000: episode: 478, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 193.642, mean reward: 1.936 [1.466, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.014, 10.193], loss: 0.115688, mae: 0.308515, mean_q: 3.799361
 23878/100000: episode: 479, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 182.673, mean reward: 1.827 [1.479, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.680, 10.190], loss: 0.099789, mae: 0.295487, mean_q: 3.793287
 23978/100000: episode: 480, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 208.287, mean reward: 2.083 [1.472, 7.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.415, 10.098], loss: 0.086985, mae: 0.294628, mean_q: 3.786720
 24078/100000: episode: 481, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 185.903, mean reward: 1.859 [1.476, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.845, 10.098], loss: 0.097546, mae: 0.311206, mean_q: 3.805199
 24178/100000: episode: 482, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 189.802, mean reward: 1.898 [1.467, 2.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.022, 10.098], loss: 0.164177, mae: 0.309885, mean_q: 3.824355
 24278/100000: episode: 483, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 184.160, mean reward: 1.842 [1.465, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.099, 10.243], loss: 0.098031, mae: 0.299678, mean_q: 3.809324
 24378/100000: episode: 484, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 194.103, mean reward: 1.941 [1.519, 7.486], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.915, 10.217], loss: 0.113234, mae: 0.295357, mean_q: 3.800466
 24478/100000: episode: 485, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 201.502, mean reward: 2.015 [1.478, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.595, 10.252], loss: 0.086530, mae: 0.289194, mean_q: 3.802342
 24578/100000: episode: 486, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.048, mean reward: 1.930 [1.466, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.406, 10.180], loss: 0.093516, mae: 0.300580, mean_q: 3.811601
 24678/100000: episode: 487, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 205.862, mean reward: 2.059 [1.500, 4.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.460, 10.372], loss: 0.109796, mae: 0.309514, mean_q: 3.807134
 24778/100000: episode: 488, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 184.543, mean reward: 1.845 [1.462, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.752, 10.126], loss: 0.095722, mae: 0.292340, mean_q: 3.817042
 24878/100000: episode: 489, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 189.870, mean reward: 1.899 [1.449, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.809, 10.165], loss: 0.098782, mae: 0.296600, mean_q: 3.811263
 24978/100000: episode: 490, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 209.993, mean reward: 2.100 [1.478, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.363, 10.282], loss: 0.080978, mae: 0.291479, mean_q: 3.812696
 25078/100000: episode: 491, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 203.368, mean reward: 2.034 [1.475, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.556, 10.298], loss: 0.094710, mae: 0.301928, mean_q: 3.817258
 25178/100000: episode: 492, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 182.433, mean reward: 1.824 [1.433, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.331, 10.098], loss: 0.105521, mae: 0.307180, mean_q: 3.832749
 25278/100000: episode: 493, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 195.330, mean reward: 1.953 [1.440, 4.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.712, 10.206], loss: 0.103162, mae: 0.298707, mean_q: 3.833024
 25378/100000: episode: 494, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 209.339, mean reward: 2.093 [1.447, 5.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.289, 10.178], loss: 0.107604, mae: 0.307844, mean_q: 3.829496
 25478/100000: episode: 495, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 207.078, mean reward: 2.071 [1.458, 5.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.681, 10.098], loss: 0.114036, mae: 0.307518, mean_q: 3.853289
 25578/100000: episode: 496, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 190.348, mean reward: 1.903 [1.465, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.162, 10.338], loss: 0.112348, mae: 0.306103, mean_q: 3.845783
 25678/100000: episode: 497, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 212.822, mean reward: 2.128 [1.465, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.584, 10.405], loss: 0.128169, mae: 0.326074, mean_q: 3.860660
 25778/100000: episode: 498, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 186.957, mean reward: 1.870 [1.437, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.948, 10.098], loss: 0.109533, mae: 0.306485, mean_q: 3.862351
 25878/100000: episode: 499, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.113, mean reward: 1.891 [1.447, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.856, 10.114], loss: 0.093999, mae: 0.298828, mean_q: 3.849853
 25978/100000: episode: 500, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 179.548, mean reward: 1.795 [1.454, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.942, 10.098], loss: 0.076725, mae: 0.284364, mean_q: 3.829584
 26078/100000: episode: 501, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 192.864, mean reward: 1.929 [1.445, 4.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.757, 10.098], loss: 0.099826, mae: 0.307891, mean_q: 3.838939
 26178/100000: episode: 502, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 201.730, mean reward: 2.017 [1.525, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.012, 10.098], loss: 0.105404, mae: 0.314084, mean_q: 3.839966
 26278/100000: episode: 503, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 200.713, mean reward: 2.007 [1.485, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.298, 10.391], loss: 0.085596, mae: 0.295096, mean_q: 3.828467
 26378/100000: episode: 504, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 203.405, mean reward: 2.034 [1.450, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.114, 10.098], loss: 0.095443, mae: 0.302109, mean_q: 3.821695
 26478/100000: episode: 505, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 195.123, mean reward: 1.951 [1.476, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.149, 10.098], loss: 0.097005, mae: 0.299864, mean_q: 3.831528
 26578/100000: episode: 506, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 189.273, mean reward: 1.893 [1.496, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.865, 10.123], loss: 0.089618, mae: 0.303886, mean_q: 3.851843
 26678/100000: episode: 507, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 185.527, mean reward: 1.855 [1.465, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.930, 10.122], loss: 0.105506, mae: 0.311312, mean_q: 3.845784
 26778/100000: episode: 508, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 196.186, mean reward: 1.962 [1.517, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.492, 10.098], loss: 0.107323, mae: 0.314796, mean_q: 3.851268
 26878/100000: episode: 509, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 186.102, mean reward: 1.861 [1.464, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.106, 10.098], loss: 0.113464, mae: 0.310312, mean_q: 3.855038
 26978/100000: episode: 510, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 193.784, mean reward: 1.938 [1.447, 4.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.137, 10.266], loss: 0.090272, mae: 0.297370, mean_q: 3.838207
 27078/100000: episode: 511, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 194.052, mean reward: 1.941 [1.469, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.468, 10.098], loss: 0.081189, mae: 0.283706, mean_q: 3.801865
 27178/100000: episode: 512, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 233.046, mean reward: 2.330 [1.531, 6.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.789, 10.098], loss: 0.098033, mae: 0.294765, mean_q: 3.816543
 27278/100000: episode: 513, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 189.588, mean reward: 1.896 [1.451, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.120, 10.098], loss: 0.089826, mae: 0.298830, mean_q: 3.833902
 27378/100000: episode: 514, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 187.147, mean reward: 1.871 [1.464, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.445, 10.271], loss: 0.101140, mae: 0.310573, mean_q: 3.844177
 27478/100000: episode: 515, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 192.057, mean reward: 1.921 [1.494, 6.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.692, 10.254], loss: 0.105344, mae: 0.315991, mean_q: 3.870548
 27578/100000: episode: 516, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.321, mean reward: 1.893 [1.476, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.180, 10.246], loss: 0.126479, mae: 0.324384, mean_q: 3.871781
 27678/100000: episode: 517, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 192.000, mean reward: 1.920 [1.448, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.368, 10.447], loss: 0.109196, mae: 0.317657, mean_q: 3.867778
 27778/100000: episode: 518, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 191.227, mean reward: 1.912 [1.471, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.369, 10.333], loss: 0.101140, mae: 0.301351, mean_q: 3.863354
 27878/100000: episode: 519, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 205.113, mean reward: 2.051 [1.434, 3.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.621, 10.260], loss: 0.104569, mae: 0.310188, mean_q: 3.868337
[Info] 1-TH LEVEL FOUND: 4.899550437927246, Considering 10/90 traces
 27978/100000: episode: 520, duration: 4.572s, episode steps: 100, steps per second: 22, episode reward: 189.237, mean reward: 1.892 [1.470, 4.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.795, 10.114], loss: 0.108946, mae: 0.317257, mean_q: 3.867314
 28018/100000: episode: 521, duration: 0.202s, episode steps: 40, steps per second: 198, episode reward: 107.771, mean reward: 2.694 [1.795, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-1.001, 10.523], loss: 0.103487, mae: 0.319086, mean_q: 3.847768
 28031/100000: episode: 522, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 34.289, mean reward: 2.638 [1.993, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.605, 10.332], loss: 0.063202, mae: 0.274037, mean_q: 3.867565
 28082/100000: episode: 523, duration: 0.237s, episode steps: 51, steps per second: 216, episode reward: 150.092, mean reward: 2.943 [1.774, 14.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-1.391, 10.258], loss: 0.102537, mae: 0.313979, mean_q: 3.901156
 28133/100000: episode: 524, duration: 0.240s, episode steps: 51, steps per second: 212, episode reward: 137.491, mean reward: 2.696 [1.609, 9.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.677, 10.216], loss: 0.130802, mae: 0.327551, mean_q: 3.901360
 28144/100000: episode: 525, duration: 0.059s, episode steps: 11, steps per second: 185, episode reward: 24.370, mean reward: 2.215 [1.607, 2.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.065, 10.100], loss: 0.149341, mae: 0.330304, mean_q: 3.895556
 28155/100000: episode: 526, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 26.905, mean reward: 2.446 [2.185, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.208, 10.100], loss: 0.125106, mae: 0.356850, mean_q: 3.922520
 28191/100000: episode: 527, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 104.627, mean reward: 2.906 [2.146, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.788, 10.479], loss: 0.105897, mae: 0.318138, mean_q: 3.893943
 28218/100000: episode: 528, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 93.429, mean reward: 3.460 [2.466, 5.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.271, 10.100], loss: 0.097329, mae: 0.308840, mean_q: 3.914749
 28229/100000: episode: 529, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 28.111, mean reward: 2.556 [2.318, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.379, 10.100], loss: 0.183252, mae: 0.376821, mean_q: 3.970186
 28240/100000: episode: 530, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 22.773, mean reward: 2.070 [1.547, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.096, 10.100], loss: 0.107600, mae: 0.345679, mean_q: 3.929546
 28265/100000: episode: 531, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 55.995, mean reward: 2.240 [1.680, 5.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.272, 10.273], loss: 0.141686, mae: 0.329663, mean_q: 3.894868
 28281/100000: episode: 532, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 50.147, mean reward: 3.134 [2.706, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.373, 10.100], loss: 0.118526, mae: 0.321026, mean_q: 3.882146
 28300/100000: episode: 533, duration: 0.106s, episode steps: 19, steps per second: 180, episode reward: 56.011, mean reward: 2.948 [2.284, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.556], loss: 0.105676, mae: 0.329639, mean_q: 3.903938
 28351/100000: episode: 534, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 135.002, mean reward: 2.647 [1.883, 5.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.081, 10.412], loss: 0.114731, mae: 0.319451, mean_q: 3.940176
 28378/100000: episode: 535, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 55.503, mean reward: 2.056 [1.668, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.633, 10.100], loss: 0.097248, mae: 0.306033, mean_q: 3.903039
 28405/100000: episode: 536, duration: 0.127s, episode steps: 27, steps per second: 213, episode reward: 68.005, mean reward: 2.519 [1.730, 4.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.620, 10.100], loss: 0.145493, mae: 0.341722, mean_q: 4.011637
 28426/100000: episode: 537, duration: 0.103s, episode steps: 21, steps per second: 203, episode reward: 48.159, mean reward: 2.293 [1.911, 2.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.429, 10.100], loss: 0.125245, mae: 0.324986, mean_q: 3.928238
 28442/100000: episode: 538, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 38.268, mean reward: 2.392 [1.834, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-1.200, 10.100], loss: 0.134740, mae: 0.324923, mean_q: 3.965266
 28493/100000: episode: 539, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 134.122, mean reward: 2.630 [2.027, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-1.066, 10.471], loss: 0.142299, mae: 0.347729, mean_q: 3.960852
 28529/100000: episode: 540, duration: 0.174s, episode steps: 36, steps per second: 207, episode reward: 123.183, mean reward: 3.422 [2.212, 7.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.752, 10.445], loss: 0.126660, mae: 0.343100, mean_q: 3.984859
 28556/100000: episode: 541, duration: 0.129s, episode steps: 27, steps per second: 209, episode reward: 73.198, mean reward: 2.711 [2.043, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.498, 10.100], loss: 0.184285, mae: 0.353929, mean_q: 3.996124
 28581/100000: episode: 542, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 71.942, mean reward: 2.878 [2.251, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.849, 10.529], loss: 0.134525, mae: 0.347031, mean_q: 3.988952
 28592/100000: episode: 543, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 49.071, mean reward: 4.461 [2.280, 15.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.330, 10.100], loss: 0.138979, mae: 0.362740, mean_q: 4.011639
 28608/100000: episode: 544, duration: 0.077s, episode steps: 16, steps per second: 208, episode reward: 88.480, mean reward: 5.530 [2.118, 37.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.650, 10.100], loss: 0.077660, mae: 0.283929, mean_q: 3.979714
 28627/100000: episode: 545, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 50.072, mean reward: 2.635 [1.936, 4.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.272], loss: 0.235156, mae: 0.402427, mean_q: 4.103649
 28678/100000: episode: 546, duration: 0.246s, episode steps: 51, steps per second: 207, episode reward: 161.649, mean reward: 3.170 [1.539, 7.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.576, 10.138], loss: 0.152649, mae: 0.353777, mean_q: 4.023564
 28699/100000: episode: 547, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 44.716, mean reward: 2.129 [1.550, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.546, 10.100], loss: 0.193537, mae: 0.378415, mean_q: 4.115054
 28712/100000: episode: 548, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 32.957, mean reward: 2.535 [2.257, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-1.159, 10.464], loss: 0.326230, mae: 0.408361, mean_q: 4.107176
 28737/100000: episode: 549, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 77.805, mean reward: 3.112 [2.192, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.559, 10.350], loss: 0.141203, mae: 0.343438, mean_q: 4.048601
 28777/100000: episode: 550, duration: 0.198s, episode steps: 40, steps per second: 202, episode reward: 78.771, mean reward: 1.969 [1.633, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.519, 10.145], loss: 0.137169, mae: 0.348749, mean_q: 4.071433
 28828/100000: episode: 551, duration: 0.247s, episode steps: 51, steps per second: 206, episode reward: 157.797, mean reward: 3.094 [2.144, 5.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.952, 10.460], loss: 0.256518, mae: 0.395220, mean_q: 4.083520
 28839/100000: episode: 552, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 22.883, mean reward: 2.080 [1.774, 2.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.073, 10.100], loss: 0.184333, mae: 0.372588, mean_q: 4.096685
 28866/100000: episode: 553, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 61.943, mean reward: 2.294 [1.804, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.944, 10.100], loss: 0.184509, mae: 0.396653, mean_q: 4.133598
 28887/100000: episode: 554, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 50.834, mean reward: 2.421 [1.907, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.475, 10.100], loss: 0.216662, mae: 0.378377, mean_q: 4.126021
 28906/100000: episode: 555, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 50.136, mean reward: 2.639 [1.813, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.263, 10.352], loss: 0.290145, mae: 0.404530, mean_q: 4.145501
 28925/100000: episode: 556, duration: 0.110s, episode steps: 19, steps per second: 172, episode reward: 61.047, mean reward: 3.213 [2.323, 5.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.302, 10.449], loss: 0.153924, mae: 0.380756, mean_q: 4.122302
 28965/100000: episode: 557, duration: 0.232s, episode steps: 40, steps per second: 173, episode reward: 80.658, mean reward: 2.016 [1.546, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.647, 10.186], loss: 0.258716, mae: 0.409582, mean_q: 4.136001
 29016/100000: episode: 558, duration: 0.257s, episode steps: 51, steps per second: 199, episode reward: 124.596, mean reward: 2.443 [1.734, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.415, 10.342], loss: 0.154936, mae: 0.367579, mean_q: 4.134842
 29029/100000: episode: 559, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 31.969, mean reward: 2.459 [1.932, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.235, 10.360], loss: 0.306521, mae: 0.421921, mean_q: 4.184594
 29056/100000: episode: 560, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: 78.045, mean reward: 2.891 [2.203, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.550, 10.100], loss: 0.267314, mae: 0.422635, mean_q: 4.148128
 29069/100000: episode: 561, duration: 0.066s, episode steps: 13, steps per second: 197, episode reward: 37.627, mean reward: 2.894 [1.883, 4.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.335], loss: 1.687743, mae: 0.436749, mean_q: 4.092356
 29080/100000: episode: 562, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 25.081, mean reward: 2.280 [1.952, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.093, 10.100], loss: 0.382224, mae: 0.537089, mean_q: 4.328385
 29131/100000: episode: 563, duration: 0.253s, episode steps: 51, steps per second: 202, episode reward: 168.915, mean reward: 3.312 [2.068, 8.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.832, 10.475], loss: 0.235207, mae: 0.421632, mean_q: 4.199309
 29144/100000: episode: 564, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 29.244, mean reward: 2.250 [1.759, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.492, 10.329], loss: 0.194744, mae: 0.390671, mean_q: 4.122178
 29160/100000: episode: 565, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 41.446, mean reward: 2.590 [2.166, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.252, 10.100], loss: 0.175072, mae: 0.398383, mean_q: 4.171048
 29173/100000: episode: 566, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 35.048, mean reward: 2.696 [2.116, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.369], loss: 0.298712, mae: 0.443016, mean_q: 4.321411
 29200/100000: episode: 567, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 77.044, mean reward: 2.853 [1.932, 4.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.753, 10.100], loss: 0.169187, mae: 0.392120, mean_q: 4.193904
 29240/100000: episode: 568, duration: 0.212s, episode steps: 40, steps per second: 189, episode reward: 96.042, mean reward: 2.401 [1.802, 4.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.688, 10.303], loss: 0.200176, mae: 0.397652, mean_q: 4.210053
 29267/100000: episode: 569, duration: 0.133s, episode steps: 27, steps per second: 204, episode reward: 73.473, mean reward: 2.721 [1.733, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.104, 10.100], loss: 0.155125, mae: 0.366180, mean_q: 4.182861
 29280/100000: episode: 570, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 27.810, mean reward: 2.139 [1.550, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.469], loss: 0.395481, mae: 0.466869, mean_q: 4.256859
 29305/100000: episode: 571, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 72.644, mean reward: 2.906 [2.344, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.059, 10.391], loss: 0.176243, mae: 0.399519, mean_q: 4.218367
 29356/100000: episode: 572, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 144.590, mean reward: 2.835 [2.027, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.734, 10.466], loss: 0.139929, mae: 0.361830, mean_q: 4.189021
 29372/100000: episode: 573, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 45.323, mean reward: 2.833 [2.073, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.295, 10.100], loss: 0.177694, mae: 0.381312, mean_q: 4.240459
 29391/100000: episode: 574, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 50.992, mean reward: 2.684 [2.046, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.294], loss: 1.254180, mae: 0.531881, mean_q: 4.298188
 29418/100000: episode: 575, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 69.516, mean reward: 2.575 [1.922, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.149, 10.100], loss: 0.208672, mae: 0.429132, mean_q: 4.255611
 29429/100000: episode: 576, duration: 0.054s, episode steps: 11, steps per second: 205, episode reward: 29.396, mean reward: 2.672 [2.224, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.344, 10.100], loss: 0.116012, mae: 0.321940, mean_q: 4.185537
 29456/100000: episode: 577, duration: 0.142s, episode steps: 27, steps per second: 190, episode reward: 64.977, mean reward: 2.407 [1.796, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.314, 10.100], loss: 0.224836, mae: 0.401634, mean_q: 4.279447
 29496/100000: episode: 578, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 115.081, mean reward: 2.877 [2.009, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.603, 10.439], loss: 0.179677, mae: 0.408493, mean_q: 4.275417
 29547/100000: episode: 579, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 129.120, mean reward: 2.532 [1.677, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.392, 10.261], loss: 0.167125, mae: 0.376090, mean_q: 4.286795
 29560/100000: episode: 580, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 32.201, mean reward: 2.477 [1.796, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.237], loss: 0.161038, mae: 0.385417, mean_q: 4.221605
[Info] FALSIFICATION!
[Info] Levels: [4.8995504, 7.6950026]
[Info] Cond. Prob: [0.1, 0.12]
[Info] Error Prob: 0.012

 29561/100000: episode: 581, duration: 4.358s, episode steps: 1, steps per second: 0, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.413, 5.123], loss: 0.090174, mae: 0.319854, mean_q: 4.062935
 29661/100000: episode: 582, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.898, mean reward: 1.889 [1.457, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.779, 10.347], loss: 4.119867, mae: 0.539140, mean_q: 4.308438
 29761/100000: episode: 583, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 181.414, mean reward: 1.814 [1.461, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.338, 10.098], loss: 0.521753, mae: 0.513315, mean_q: 4.310390
 29861/100000: episode: 584, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 193.177, mean reward: 1.932 [1.436, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.784, 10.143], loss: 1.790091, mae: 0.504314, mean_q: 4.336265
 29961/100000: episode: 585, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 180.170, mean reward: 1.802 [1.470, 2.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.360, 10.183], loss: 0.429907, mae: 0.431277, mean_q: 4.294769
 30061/100000: episode: 586, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 191.929, mean reward: 1.919 [1.467, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.353, 10.098], loss: 0.364278, mae: 0.404945, mean_q: 4.284779
 30161/100000: episode: 587, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 198.619, mean reward: 1.986 [1.496, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.183, 10.364], loss: 1.547937, mae: 0.464785, mean_q: 4.300626
 30261/100000: episode: 588, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 181.205, mean reward: 1.812 [1.475, 2.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.316, 10.142], loss: 2.806983, mae: 0.533986, mean_q: 4.335840
 30361/100000: episode: 589, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.541, mean reward: 1.925 [1.490, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.752, 10.152], loss: 0.586909, mae: 0.448699, mean_q: 4.292487
 30461/100000: episode: 590, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 198.592, mean reward: 1.986 [1.447, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.102, 10.098], loss: 1.471440, mae: 0.447895, mean_q: 4.280324
 30561/100000: episode: 591, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 188.103, mean reward: 1.881 [1.463, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.890, 10.174], loss: 1.488961, mae: 0.452270, mean_q: 4.285737
 30661/100000: episode: 592, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 187.354, mean reward: 1.874 [1.473, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.468, 10.109], loss: 0.177829, mae: 0.375970, mean_q: 4.239053
 30761/100000: episode: 593, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 227.509, mean reward: 2.275 [1.452, 8.142], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.646, 10.420], loss: 1.674400, mae: 0.450563, mean_q: 4.305372
 30861/100000: episode: 594, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 198.112, mean reward: 1.981 [1.474, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.837, 10.098], loss: 1.519011, mae: 0.450502, mean_q: 4.301857
 30961/100000: episode: 595, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 175.147, mean reward: 1.751 [1.435, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.716, 10.237], loss: 0.201005, mae: 0.402301, mean_q: 4.311853
 31061/100000: episode: 596, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.645, mean reward: 1.926 [1.465, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.369, 10.145], loss: 0.611257, mae: 0.433809, mean_q: 4.294715
 31161/100000: episode: 597, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.694, mean reward: 1.837 [1.464, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.491, 10.275], loss: 0.265532, mae: 0.414917, mean_q: 4.291050
 31261/100000: episode: 598, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 198.765, mean reward: 1.988 [1.467, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.526, 10.098], loss: 0.408966, mae: 0.401911, mean_q: 4.266831
 31361/100000: episode: 599, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 186.322, mean reward: 1.863 [1.492, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.834, 10.098], loss: 0.189504, mae: 0.376130, mean_q: 4.284510
 31461/100000: episode: 600, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 183.957, mean reward: 1.840 [1.440, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.867, 10.283], loss: 0.618461, mae: 0.444701, mean_q: 4.257604
 31561/100000: episode: 601, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 182.339, mean reward: 1.823 [1.484, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.228, 10.098], loss: 1.665033, mae: 0.450760, mean_q: 4.274745
 31661/100000: episode: 602, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 182.872, mean reward: 1.829 [1.446, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.751, 10.100], loss: 0.412279, mae: 0.422964, mean_q: 4.251919
 31761/100000: episode: 603, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 192.531, mean reward: 1.925 [1.486, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.550, 10.133], loss: 0.164182, mae: 0.362722, mean_q: 4.240265
 31861/100000: episode: 604, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 195.446, mean reward: 1.954 [1.473, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.527, 10.317], loss: 0.375036, mae: 0.376894, mean_q: 4.227561
 31961/100000: episode: 605, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 206.343, mean reward: 2.063 [1.433, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.793, 10.098], loss: 1.491211, mae: 0.414584, mean_q: 4.241462
 32061/100000: episode: 606, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 284.640, mean reward: 2.846 [1.522, 9.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.673, 10.098], loss: 0.208241, mae: 0.376372, mean_q: 4.279773
 32161/100000: episode: 607, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 202.007, mean reward: 2.020 [1.452, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.089, 10.231], loss: 0.566121, mae: 0.416919, mean_q: 4.295200
 32261/100000: episode: 608, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 182.026, mean reward: 1.820 [1.478, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.711, 10.098], loss: 1.862752, mae: 0.457058, mean_q: 4.333241
 32361/100000: episode: 609, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 190.316, mean reward: 1.903 [1.479, 4.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.721, 10.152], loss: 0.223205, mae: 0.401488, mean_q: 4.281491
 32461/100000: episode: 610, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.424, mean reward: 1.954 [1.451, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.980, 10.098], loss: 0.446241, mae: 0.440824, mean_q: 4.312220
 32561/100000: episode: 611, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 195.259, mean reward: 1.953 [1.479, 4.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.973, 10.266], loss: 0.378710, mae: 0.390145, mean_q: 4.294145
 32661/100000: episode: 612, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 193.455, mean reward: 1.935 [1.488, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.266, 10.098], loss: 1.693136, mae: 0.474265, mean_q: 4.340446
 32761/100000: episode: 613, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.225, mean reward: 1.802 [1.437, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.220, 10.098], loss: 0.198313, mae: 0.382561, mean_q: 4.290297
 32861/100000: episode: 614, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 183.698, mean reward: 1.837 [1.448, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.603, 10.133], loss: 0.430644, mae: 0.426869, mean_q: 4.316858
 32961/100000: episode: 615, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 208.834, mean reward: 2.088 [1.447, 4.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.084, 10.241], loss: 1.708069, mae: 0.451795, mean_q: 4.266893
 33061/100000: episode: 616, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 195.689, mean reward: 1.957 [1.500, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.079, 10.347], loss: 1.719979, mae: 0.482089, mean_q: 4.295950
 33161/100000: episode: 617, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 188.442, mean reward: 1.884 [1.434, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.604, 10.173], loss: 1.664546, mae: 0.443540, mean_q: 4.239989
 33261/100000: episode: 618, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.571, mean reward: 1.906 [1.434, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.273, 10.098], loss: 1.687710, mae: 0.454239, mean_q: 4.243341
 33361/100000: episode: 619, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 186.908, mean reward: 1.869 [1.478, 3.706], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.474, 10.175], loss: 0.643318, mae: 0.436016, mean_q: 4.197641
 33461/100000: episode: 620, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.914, mean reward: 1.979 [1.471, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.564, 10.344], loss: 1.515418, mae: 0.458959, mean_q: 4.190688
 33561/100000: episode: 621, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 204.678, mean reward: 2.047 [1.479, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.724, 10.177], loss: 0.363433, mae: 0.380790, mean_q: 4.091247
 33661/100000: episode: 622, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 193.249, mean reward: 1.932 [1.452, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.417, 10.170], loss: 2.648548, mae: 0.435346, mean_q: 4.090758
 33761/100000: episode: 623, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 182.008, mean reward: 1.820 [1.446, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.535, 10.148], loss: 3.741903, mae: 0.589318, mean_q: 4.172878
 33861/100000: episode: 624, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 203.955, mean reward: 2.040 [1.475, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.990, 10.422], loss: 0.142597, mae: 0.334185, mean_q: 3.994897
 33961/100000: episode: 625, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 198.786, mean reward: 1.988 [1.469, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.251, 10.098], loss: 0.125578, mae: 0.320066, mean_q: 3.976696
 34061/100000: episode: 626, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 195.003, mean reward: 1.950 [1.502, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.822, 10.098], loss: 2.561920, mae: 0.485276, mean_q: 4.003034
 34161/100000: episode: 627, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.410, mean reward: 1.844 [1.466, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.716, 10.098], loss: 1.350719, mae: 0.415524, mean_q: 3.981573
 34261/100000: episode: 628, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 189.053, mean reward: 1.891 [1.463, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.510, 10.098], loss: 0.131355, mae: 0.314629, mean_q: 3.902715
 34361/100000: episode: 629, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 191.327, mean reward: 1.913 [1.440, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.604, 10.356], loss: 0.103350, mae: 0.298920, mean_q: 3.883168
 34461/100000: episode: 630, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 193.626, mean reward: 1.936 [1.477, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.767, 10.151], loss: 1.352488, mae: 0.359807, mean_q: 3.861834
 34561/100000: episode: 631, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 193.312, mean reward: 1.933 [1.470, 3.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.145, 10.098], loss: 0.104267, mae: 0.302465, mean_q: 3.834948
 34661/100000: episode: 632, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 202.670, mean reward: 2.027 [1.448, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.609, 10.098], loss: 0.102065, mae: 0.293537, mean_q: 3.838152
 34761/100000: episode: 633, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 179.348, mean reward: 1.793 [1.458, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.195, 10.098], loss: 0.110773, mae: 0.312893, mean_q: 3.856372
 34861/100000: episode: 634, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 189.129, mean reward: 1.891 [1.459, 2.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.882, 10.098], loss: 0.103126, mae: 0.293774, mean_q: 3.826512
 34961/100000: episode: 635, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 199.351, mean reward: 1.994 [1.453, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.643, 10.099], loss: 0.087563, mae: 0.285130, mean_q: 3.819273
 35061/100000: episode: 636, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 211.148, mean reward: 2.111 [1.477, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.358, 10.098], loss: 0.089428, mae: 0.283760, mean_q: 3.839242
 35161/100000: episode: 637, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 190.916, mean reward: 1.909 [1.441, 4.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.384, 10.113], loss: 0.107044, mae: 0.298961, mean_q: 3.854529
 35261/100000: episode: 638, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 178.270, mean reward: 1.783 [1.440, 2.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.586, 10.176], loss: 0.090694, mae: 0.290614, mean_q: 3.826496
 35361/100000: episode: 639, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 188.538, mean reward: 1.885 [1.442, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.902, 10.310], loss: 0.101676, mae: 0.295110, mean_q: 3.852934
 35461/100000: episode: 640, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.343, mean reward: 1.893 [1.476, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.696, 10.142], loss: 0.116824, mae: 0.307665, mean_q: 3.855258
 35561/100000: episode: 641, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 210.666, mean reward: 2.107 [1.462, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.969, 10.116], loss: 0.094873, mae: 0.299791, mean_q: 3.831694
 35661/100000: episode: 642, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 190.161, mean reward: 1.902 [1.464, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.334, 10.158], loss: 0.111165, mae: 0.306525, mean_q: 3.844645
 35761/100000: episode: 643, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 181.033, mean reward: 1.810 [1.458, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.378, 10.098], loss: 0.097136, mae: 0.295411, mean_q: 3.856890
 35861/100000: episode: 644, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.738, mean reward: 1.847 [1.455, 3.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.964, 10.098], loss: 0.082446, mae: 0.285702, mean_q: 3.842166
 35961/100000: episode: 645, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 219.988, mean reward: 2.200 [1.491, 4.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.795, 10.098], loss: 0.086203, mae: 0.287707, mean_q: 3.834523
 36061/100000: episode: 646, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 184.181, mean reward: 1.842 [1.455, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.726, 10.117], loss: 0.086389, mae: 0.296071, mean_q: 3.854299
 36161/100000: episode: 647, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 195.227, mean reward: 1.952 [1.442, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.409, 10.098], loss: 0.088365, mae: 0.298552, mean_q: 3.837785
 36261/100000: episode: 648, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 190.784, mean reward: 1.908 [1.456, 4.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.045, 10.148], loss: 0.086652, mae: 0.289115, mean_q: 3.837533
 36361/100000: episode: 649, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 195.908, mean reward: 1.959 [1.510, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.358, 10.239], loss: 0.094202, mae: 0.291792, mean_q: 3.841987
 36461/100000: episode: 650, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.544, mean reward: 1.945 [1.434, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.574, 10.212], loss: 0.099519, mae: 0.306310, mean_q: 3.866838
 36561/100000: episode: 651, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 187.900, mean reward: 1.879 [1.449, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.028, 10.146], loss: 0.098131, mae: 0.304413, mean_q: 3.874234
 36661/100000: episode: 652, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.088, mean reward: 1.931 [1.451, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.669, 10.098], loss: 0.097522, mae: 0.306874, mean_q: 3.881052
 36761/100000: episode: 653, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 184.986, mean reward: 1.850 [1.435, 3.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.544, 10.322], loss: 0.092718, mae: 0.297252, mean_q: 3.869159
 36861/100000: episode: 654, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 192.931, mean reward: 1.929 [1.486, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.730, 10.112], loss: 0.080332, mae: 0.282123, mean_q: 3.815562
 36961/100000: episode: 655, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 197.366, mean reward: 1.974 [1.452, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.624, 10.098], loss: 0.088839, mae: 0.297001, mean_q: 3.864359
 37061/100000: episode: 656, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.399, mean reward: 1.904 [1.440, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.754, 10.098], loss: 0.087040, mae: 0.297956, mean_q: 3.825093
 37161/100000: episode: 657, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 240.294, mean reward: 2.403 [1.538, 5.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.172, 10.295], loss: 0.083078, mae: 0.293218, mean_q: 3.812656
 37261/100000: episode: 658, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 199.295, mean reward: 1.993 [1.469, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.792, 10.137], loss: 0.079932, mae: 0.289013, mean_q: 3.842741
 37361/100000: episode: 659, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 191.170, mean reward: 1.912 [1.438, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.943, 10.215], loss: 0.079883, mae: 0.287221, mean_q: 3.839623
 37461/100000: episode: 660, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 200.863, mean reward: 2.009 [1.479, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.711, 10.098], loss: 0.081288, mae: 0.295178, mean_q: 3.836169
 37561/100000: episode: 661, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 195.248, mean reward: 1.952 [1.494, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.241, 10.098], loss: 0.091863, mae: 0.309307, mean_q: 3.819859
 37661/100000: episode: 662, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 185.067, mean reward: 1.851 [1.459, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.917, 10.098], loss: 0.079417, mae: 0.289635, mean_q: 3.839195
 37761/100000: episode: 663, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 203.503, mean reward: 2.035 [1.472, 3.689], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.552, 10.098], loss: 0.081015, mae: 0.285607, mean_q: 3.829997
 37861/100000: episode: 664, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 198.019, mean reward: 1.980 [1.461, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.738, 10.098], loss: 0.087149, mae: 0.302296, mean_q: 3.849133
 37961/100000: episode: 665, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 182.286, mean reward: 1.823 [1.457, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.641, 10.098], loss: 0.080499, mae: 0.287598, mean_q: 3.831651
 38061/100000: episode: 666, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 184.246, mean reward: 1.842 [1.438, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.769, 10.129], loss: 0.086497, mae: 0.300868, mean_q: 3.845095
 38161/100000: episode: 667, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 175.727, mean reward: 1.757 [1.443, 2.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.810, 10.134], loss: 0.077671, mae: 0.286828, mean_q: 3.828650
 38261/100000: episode: 668, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 181.392, mean reward: 1.814 [1.444, 2.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.860, 10.098], loss: 0.078524, mae: 0.286477, mean_q: 3.827433
 38361/100000: episode: 669, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 187.196, mean reward: 1.872 [1.442, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.759, 10.125], loss: 0.079870, mae: 0.288896, mean_q: 3.816138
 38461/100000: episode: 670, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 192.415, mean reward: 1.924 [1.477, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.767, 10.098], loss: 0.079571, mae: 0.288252, mean_q: 3.811618
 38561/100000: episode: 671, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 193.922, mean reward: 1.939 [1.436, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.608, 10.162], loss: 0.085420, mae: 0.294201, mean_q: 3.807682
 38661/100000: episode: 672, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 181.591, mean reward: 1.816 [1.463, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.075, 10.098], loss: 0.072658, mae: 0.279369, mean_q: 3.811039
 38761/100000: episode: 673, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 189.152, mean reward: 1.892 [1.478, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.208, 10.098], loss: 0.080730, mae: 0.291289, mean_q: 3.823915
 38861/100000: episode: 674, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 196.697, mean reward: 1.967 [1.442, 8.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.585, 10.126], loss: 0.080509, mae: 0.288720, mean_q: 3.803992
 38961/100000: episode: 675, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 192.708, mean reward: 1.927 [1.455, 2.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.693, 10.098], loss: 0.080825, mae: 0.284193, mean_q: 3.792555
 39061/100000: episode: 676, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 203.853, mean reward: 2.039 [1.458, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.855, 10.098], loss: 0.086712, mae: 0.286914, mean_q: 3.813774
 39161/100000: episode: 677, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.909, mean reward: 1.899 [1.483, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.609, 10.098], loss: 0.092324, mae: 0.291179, mean_q: 3.825707
 39261/100000: episode: 678, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 186.921, mean reward: 1.869 [1.461, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.896, 10.098], loss: 0.071928, mae: 0.276887, mean_q: 3.813771
 39361/100000: episode: 679, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 192.810, mean reward: 1.928 [1.483, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.604, 10.098], loss: 0.078281, mae: 0.293883, mean_q: 3.845320
 39461/100000: episode: 680, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 191.766, mean reward: 1.918 [1.463, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.156, 10.307], loss: 0.088092, mae: 0.284846, mean_q: 3.788649
[Info] 1-TH LEVEL FOUND: 5.3964996337890625, Considering 10/90 traces
 39561/100000: episode: 681, duration: 4.583s, episode steps: 100, steps per second: 22, episode reward: 183.846, mean reward: 1.838 [1.448, 2.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.955, 10.157], loss: 0.093519, mae: 0.291165, mean_q: 3.804283
 39611/100000: episode: 682, duration: 0.249s, episode steps: 50, steps per second: 201, episode reward: 126.865, mean reward: 2.537 [2.029, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.352, 10.351], loss: 0.071075, mae: 0.272219, mean_q: 3.808471
 39662/100000: episode: 683, duration: 0.250s, episode steps: 51, steps per second: 204, episode reward: 109.855, mean reward: 2.154 [1.474, 4.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-1.183, 10.117], loss: 0.079021, mae: 0.293615, mean_q: 3.810679
 39713/100000: episode: 684, duration: 0.273s, episode steps: 51, steps per second: 186, episode reward: 98.677, mean reward: 1.935 [1.471, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.478, 10.100], loss: 0.075493, mae: 0.281207, mean_q: 3.795146
 39749/100000: episode: 685, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 139.399, mean reward: 3.872 [2.857, 8.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.281, 10.100], loss: 0.077885, mae: 0.285675, mean_q: 3.784160
 39768/100000: episode: 686, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 57.262, mean reward: 3.014 [2.273, 6.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.352, 10.100], loss: 0.072894, mae: 0.281998, mean_q: 3.858642
 39786/100000: episode: 687, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 51.985, mean reward: 2.888 [2.287, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.125, 10.100], loss: 0.109955, mae: 0.295404, mean_q: 3.854632
 39822/100000: episode: 688, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 94.021, mean reward: 2.612 [1.800, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.058, 10.100], loss: 0.073586, mae: 0.284230, mean_q: 3.879804
 39872/100000: episode: 689, duration: 0.269s, episode steps: 50, steps per second: 186, episode reward: 106.826, mean reward: 2.137 [1.541, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.596, 10.345], loss: 0.082563, mae: 0.289197, mean_q: 3.853569
 39897/100000: episode: 690, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 74.971, mean reward: 2.999 [2.419, 5.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.805, 10.100], loss: 0.112574, mae: 0.301774, mean_q: 3.833841
 39948/100000: episode: 691, duration: 0.275s, episode steps: 51, steps per second: 185, episode reward: 127.197, mean reward: 2.494 [1.885, 5.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.044, 10.177], loss: 0.108556, mae: 0.311116, mean_q: 3.903543
 39998/100000: episode: 692, duration: 0.270s, episode steps: 50, steps per second: 186, episode reward: 107.142, mean reward: 2.143 [1.664, 4.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.344, 10.265], loss: 0.094945, mae: 0.302016, mean_q: 3.870805
 40048/100000: episode: 693, duration: 0.273s, episode steps: 50, steps per second: 183, episode reward: 110.380, mean reward: 2.208 [1.594, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-1.524, 10.179], loss: 0.096509, mae: 0.305263, mean_q: 3.893044
 40064/100000: episode: 694, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 38.264, mean reward: 2.391 [2.043, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.260, 10.100], loss: 0.084634, mae: 0.293132, mean_q: 3.900062
 40083/100000: episode: 695, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 44.765, mean reward: 2.356 [1.886, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.165, 10.100], loss: 0.080167, mae: 0.291739, mean_q: 3.861689
 40133/100000: episode: 696, duration: 0.260s, episode steps: 50, steps per second: 192, episode reward: 103.269, mean reward: 2.065 [1.453, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.369, 10.100], loss: 0.091162, mae: 0.300934, mean_q: 3.887230
 40152/100000: episode: 697, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 66.678, mean reward: 3.509 [2.601, 6.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.290, 10.100], loss: 0.098981, mae: 0.316730, mean_q: 3.910062
 40177/100000: episode: 698, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 66.452, mean reward: 2.658 [1.595, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.314, 10.100], loss: 0.106927, mae: 0.289997, mean_q: 3.885064
 40227/100000: episode: 699, duration: 0.251s, episode steps: 50, steps per second: 200, episode reward: 119.636, mean reward: 2.393 [1.555, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.967, 10.100], loss: 0.084635, mae: 0.287918, mean_q: 3.911480
 40243/100000: episode: 700, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 34.885, mean reward: 2.180 [1.745, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.888, 10.100], loss: 0.078981, mae: 0.294626, mean_q: 3.976068
 40293/100000: episode: 701, duration: 0.277s, episode steps: 50, steps per second: 181, episode reward: 101.806, mean reward: 2.036 [1.568, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.266, 10.143], loss: 0.092430, mae: 0.308126, mean_q: 3.892535
 40318/100000: episode: 702, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 87.868, mean reward: 3.515 [2.473, 5.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.494, 10.100], loss: 0.110502, mae: 0.321801, mean_q: 3.965702
 40357/100000: episode: 703, duration: 0.189s, episode steps: 39, steps per second: 207, episode reward: 76.451, mean reward: 1.960 [1.452, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.416, 10.103], loss: 0.121021, mae: 0.324939, mean_q: 3.993326
 40373/100000: episode: 704, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 42.910, mean reward: 2.682 [2.281, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.263, 10.100], loss: 0.098331, mae: 0.313885, mean_q: 3.935383
 40424/100000: episode: 705, duration: 0.250s, episode steps: 51, steps per second: 204, episode reward: 122.650, mean reward: 2.405 [1.579, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.280, 10.327], loss: 0.098999, mae: 0.301042, mean_q: 3.953648
 40474/100000: episode: 706, duration: 0.271s, episode steps: 50, steps per second: 184, episode reward: 91.977, mean reward: 1.840 [1.458, 2.613], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.344, 10.293], loss: 0.117059, mae: 0.320656, mean_q: 3.987390
 40524/100000: episode: 707, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 103.628, mean reward: 2.073 [1.535, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.909, 10.100], loss: 0.090925, mae: 0.299487, mean_q: 3.947867
 40563/100000: episode: 708, duration: 0.213s, episode steps: 39, steps per second: 183, episode reward: 103.558, mean reward: 2.655 [2.030, 4.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.596, 10.366], loss: 0.085194, mae: 0.287385, mean_q: 3.967304
 40579/100000: episode: 709, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 36.683, mean reward: 2.293 [1.935, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.371, 10.100], loss: 0.104643, mae: 0.316282, mean_q: 4.024234
 40615/100000: episode: 710, duration: 0.181s, episode steps: 36, steps per second: 198, episode reward: 150.734, mean reward: 4.187 [2.704, 8.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.417, 10.100], loss: 0.089240, mae: 0.303710, mean_q: 3.958173
 40654/100000: episode: 711, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 83.843, mean reward: 2.150 [1.496, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.988 [-0.532, 10.100], loss: 0.117545, mae: 0.343197, mean_q: 4.036349
 40704/100000: episode: 712, duration: 0.251s, episode steps: 50, steps per second: 199, episode reward: 99.251, mean reward: 1.985 [1.532, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-2.013, 10.194], loss: 0.100690, mae: 0.313179, mean_q: 3.974057
 40740/100000: episode: 713, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 128.190, mean reward: 3.561 [1.904, 6.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.158, 10.100], loss: 0.103757, mae: 0.320880, mean_q: 4.028686
 40756/100000: episode: 714, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 33.607, mean reward: 2.100 [1.726, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.472, 10.100], loss: 0.166239, mae: 0.354322, mean_q: 4.052305
 40762/100000: episode: 715, duration: 0.034s, episode steps: 6, steps per second: 179, episode reward: 18.719, mean reward: 3.120 [2.602, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.309, 10.100], loss: 0.096658, mae: 0.326694, mean_q: 4.051234
 40781/100000: episode: 716, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 53.694, mean reward: 2.826 [1.825, 3.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.242, 10.100], loss: 0.105708, mae: 0.317494, mean_q: 4.008558
 40820/100000: episode: 717, duration: 0.194s, episode steps: 39, steps per second: 201, episode reward: 89.841, mean reward: 2.304 [1.591, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.840, 10.291], loss: 0.109168, mae: 0.332958, mean_q: 4.015628
 40839/100000: episode: 718, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 41.590, mean reward: 2.189 [1.833, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.654, 10.100], loss: 0.174778, mae: 0.345196, mean_q: 4.075923
 40855/100000: episode: 719, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 37.921, mean reward: 2.370 [2.042, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.324, 10.100], loss: 0.102458, mae: 0.314631, mean_q: 4.012745
 40873/100000: episode: 720, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 55.311, mean reward: 3.073 [2.369, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.222, 10.100], loss: 0.125835, mae: 0.357921, mean_q: 4.096217
 40923/100000: episode: 721, duration: 0.271s, episode steps: 50, steps per second: 185, episode reward: 155.567, mean reward: 3.111 [2.117, 5.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-1.122, 10.503], loss: 0.111474, mae: 0.326732, mean_q: 4.092955
 40948/100000: episode: 722, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 70.261, mean reward: 2.810 [2.064, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.448, 10.100], loss: 0.110546, mae: 0.335464, mean_q: 4.067142
 40987/100000: episode: 723, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 84.825, mean reward: 2.175 [1.797, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.863, 10.405], loss: 0.104038, mae: 0.318566, mean_q: 4.114464
 41038/100000: episode: 724, duration: 0.278s, episode steps: 51, steps per second: 184, episode reward: 111.748, mean reward: 2.191 [1.444, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.310, 10.100], loss: 0.124392, mae: 0.348533, mean_q: 4.118244
 41089/100000: episode: 725, duration: 0.255s, episode steps: 51, steps per second: 200, episode reward: 104.057, mean reward: 2.040 [1.497, 4.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.693, 10.139], loss: 0.126620, mae: 0.345747, mean_q: 4.115710
 41107/100000: episode: 726, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 38.362, mean reward: 2.131 [1.715, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.106, 10.100], loss: 0.100385, mae: 0.305579, mean_q: 4.013537
 41125/100000: episode: 727, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 48.733, mean reward: 2.707 [1.824, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.738, 10.100], loss: 0.120945, mae: 0.322440, mean_q: 4.089052
 41161/100000: episode: 728, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 126.100, mean reward: 3.503 [1.885, 6.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-1.397, 10.100], loss: 0.115433, mae: 0.320605, mean_q: 4.148008
 41211/100000: episode: 729, duration: 0.256s, episode steps: 50, steps per second: 195, episode reward: 106.759, mean reward: 2.135 [1.530, 4.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.566, 10.110], loss: 0.132518, mae: 0.332049, mean_q: 4.116150
 41262/100000: episode: 730, duration: 0.261s, episode steps: 51, steps per second: 196, episode reward: 110.161, mean reward: 2.160 [1.476, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.373, 10.209], loss: 0.110618, mae: 0.329904, mean_q: 4.114312
 41278/100000: episode: 731, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 34.525, mean reward: 2.158 [1.496, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.091, 10.100], loss: 0.092397, mae: 0.289930, mean_q: 4.132534
 41294/100000: episode: 732, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 31.076, mean reward: 1.942 [1.691, 2.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.266, 10.100], loss: 0.177408, mae: 0.347410, mean_q: 4.036293
 41344/100000: episode: 733, duration: 0.256s, episode steps: 50, steps per second: 196, episode reward: 177.309, mean reward: 3.546 [2.475, 6.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-0.364, 10.480], loss: 0.110094, mae: 0.330928, mean_q: 4.147611
 41380/100000: episode: 734, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 95.766, mean reward: 2.660 [1.813, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.664, 10.100], loss: 0.149440, mae: 0.362547, mean_q: 4.163996
 41398/100000: episode: 735, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 46.718, mean reward: 2.595 [1.745, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.100], loss: 0.087974, mae: 0.306490, mean_q: 4.106632
 41404/100000: episode: 736, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 18.188, mean reward: 3.031 [2.382, 4.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-1.002, 10.100], loss: 0.145280, mae: 0.377573, mean_q: 4.336385
 41454/100000: episode: 737, duration: 0.238s, episode steps: 50, steps per second: 210, episode reward: 136.589, mean reward: 2.732 [2.011, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.368, 10.301], loss: 0.133126, mae: 0.357091, mean_q: 4.124417
 41473/100000: episode: 738, duration: 0.093s, episode steps: 19, steps per second: 203, episode reward: 54.281, mean reward: 2.857 [1.998, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.580, 10.100], loss: 0.118468, mae: 0.345907, mean_q: 4.219393
 41489/100000: episode: 739, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 38.970, mean reward: 2.436 [1.977, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.339, 10.100], loss: 0.151210, mae: 0.368758, mean_q: 4.205354
 41507/100000: episode: 740, duration: 0.087s, episode steps: 18, steps per second: 207, episode reward: 42.199, mean reward: 2.344 [1.858, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.129, 10.100], loss: 0.114453, mae: 0.343402, mean_q: 4.158412
 41513/100000: episode: 741, duration: 0.031s, episode steps: 6, steps per second: 191, episode reward: 20.724, mean reward: 3.454 [2.625, 4.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.344, 10.100], loss: 0.156029, mae: 0.371311, mean_q: 4.154542
 41564/100000: episode: 742, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 98.475, mean reward: 1.931 [1.493, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.702, 10.100], loss: 0.129582, mae: 0.357472, mean_q: 4.223454
 41580/100000: episode: 743, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 37.085, mean reward: 2.318 [1.954, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.324, 10.100], loss: 0.131931, mae: 0.339542, mean_q: 4.087626
 41616/100000: episode: 744, duration: 0.198s, episode steps: 36, steps per second: 181, episode reward: 169.392, mean reward: 4.705 [2.128, 12.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.929, 10.100], loss: 0.119245, mae: 0.337743, mean_q: 4.222898
 41635/100000: episode: 745, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 62.589, mean reward: 3.294 [2.353, 4.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.203, 10.100], loss: 0.150425, mae: 0.389936, mean_q: 4.279105
 41686/100000: episode: 746, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 119.001, mean reward: 2.333 [1.503, 6.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.956, 10.296], loss: 0.164527, mae: 0.367770, mean_q: 4.241816
 41692/100000: episode: 747, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 16.047, mean reward: 2.674 [2.457, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.292, 10.100], loss: 0.155265, mae: 0.383650, mean_q: 4.099942
 41711/100000: episode: 748, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 54.441, mean reward: 2.865 [2.124, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.713, 10.100], loss: 0.251092, mae: 0.430124, mean_q: 4.266229
 41761/100000: episode: 749, duration: 0.256s, episode steps: 50, steps per second: 195, episode reward: 120.684, mean reward: 2.414 [1.589, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.304, 10.199], loss: 0.169977, mae: 0.387543, mean_q: 4.255358
 41812/100000: episode: 750, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 136.957, mean reward: 2.685 [1.902, 4.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.790, 10.301], loss: 0.156871, mae: 0.362026, mean_q: 4.270663
 41830/100000: episode: 751, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 53.595, mean reward: 2.978 [2.339, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.550, 10.100], loss: 0.146407, mae: 0.370036, mean_q: 4.352101
 41846/100000: episode: 752, duration: 0.080s, episode steps: 16, steps per second: 201, episode reward: 28.659, mean reward: 1.791 [1.597, 2.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.159, 10.100], loss: 0.141155, mae: 0.362152, mean_q: 4.234138
 41897/100000: episode: 753, duration: 0.258s, episode steps: 51, steps per second: 197, episode reward: 106.779, mean reward: 2.094 [1.625, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.991, 10.100], loss: 0.156195, mae: 0.383067, mean_q: 4.328290
 41922/100000: episode: 754, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 86.261, mean reward: 3.450 [2.386, 5.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.516, 10.100], loss: 0.162929, mae: 0.377096, mean_q: 4.267511
 41958/100000: episode: 755, duration: 0.187s, episode steps: 36, steps per second: 193, episode reward: 86.504, mean reward: 2.403 [1.861, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.035, 10.100], loss: 0.135928, mae: 0.372361, mean_q: 4.321712
 42008/100000: episode: 756, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 109.644, mean reward: 2.193 [1.619, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.367, 10.349], loss: 0.189436, mae: 0.383370, mean_q: 4.344843
 42058/100000: episode: 757, duration: 0.261s, episode steps: 50, steps per second: 192, episode reward: 118.204, mean reward: 2.364 [1.782, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.608, 10.422], loss: 0.129433, mae: 0.360626, mean_q: 4.282371
 42076/100000: episode: 758, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 47.852, mean reward: 2.658 [2.189, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.289, 10.100], loss: 0.155056, mae: 0.356864, mean_q: 4.274904
 42094/100000: episode: 759, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 46.069, mean reward: 2.559 [2.018, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.303, 10.100], loss: 0.165700, mae: 0.356924, mean_q: 4.269855
 42110/100000: episode: 760, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 40.114, mean reward: 2.507 [1.711, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.827, 10.100], loss: 0.139617, mae: 0.365752, mean_q: 4.271768
 42116/100000: episode: 761, duration: 0.039s, episode steps: 6, steps per second: 154, episode reward: 17.097, mean reward: 2.849 [2.438, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-1.070, 10.100], loss: 0.169798, mae: 0.402527, mean_q: 4.513270
 42152/100000: episode: 762, duration: 0.209s, episode steps: 36, steps per second: 172, episode reward: 82.687, mean reward: 2.297 [1.536, 3.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.507, 10.171], loss: 0.146393, mae: 0.367112, mean_q: 4.307314
 42158/100000: episode: 763, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 18.895, mean reward: 3.149 [2.634, 3.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.412, 10.100], loss: 0.234149, mae: 0.399522, mean_q: 4.389479
 42183/100000: episode: 764, duration: 0.133s, episode steps: 25, steps per second: 189, episode reward: 57.896, mean reward: 2.316 [1.612, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.447, 10.108], loss: 0.157680, mae: 0.381778, mean_q: 4.383608
 42199/100000: episode: 765, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 33.350, mean reward: 2.084 [1.697, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.641, 10.100], loss: 0.202251, mae: 0.397333, mean_q: 4.375462
 42235/100000: episode: 766, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 326.025, mean reward: 9.056 [2.318, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.284, 10.100], loss: 4.101851, mae: 0.483963, mean_q: 4.396049
 42260/100000: episode: 767, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 56.576, mean reward: 2.263 [1.752, 2.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.324, 10.100], loss: 0.517971, mae: 0.707095, mean_q: 4.325491
 42299/100000: episode: 768, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 76.293, mean reward: 1.956 [1.524, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.236, 10.248], loss: 0.350783, mae: 0.490214, mean_q: 4.403773
 42338/100000: episode: 769, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 103.609, mean reward: 2.657 [1.778, 5.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.644, 10.340], loss: 0.346600, mae: 0.413628, mean_q: 4.341339
 42377/100000: episode: 770, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 76.312, mean reward: 1.957 [1.482, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.965, 10.310], loss: 0.187437, mae: 0.398101, mean_q: 4.366912
[Info] 2-TH LEVEL FOUND: 7.572872161865234, Considering 10/90 traces
 42395/100000: episode: 771, duration: 4.161s, episode steps: 18, steps per second: 4, episode reward: 46.291, mean reward: 2.572 [1.828, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.149, 10.100], loss: 0.199050, mae: 0.390861, mean_q: 4.351459
 42414/100000: episode: 772, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 50.656, mean reward: 2.666 [1.979, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.288, 10.100], loss: 0.210498, mae: 0.429317, mean_q: 4.512653
 42447/100000: episode: 773, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 132.783, mean reward: 4.024 [2.551, 10.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.910, 10.100], loss: 0.165213, mae: 0.380811, mean_q: 4.406071
 42480/100000: episode: 774, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 111.309, mean reward: 3.373 [1.689, 7.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-2.209, 10.100], loss: 0.177212, mae: 0.389639, mean_q: 4.449854
 42499/100000: episode: 775, duration: 0.102s, episode steps: 19, steps per second: 187, episode reward: 61.075, mean reward: 3.214 [2.307, 4.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.177, 10.100], loss: 7.319317, mae: 0.543646, mean_q: 4.560632
 42524/100000: episode: 776, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 108.022, mean reward: 4.321 [3.212, 7.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.776, 10.100], loss: 0.705192, mae: 0.802568, mean_q: 4.563784
 42536/100000: episode: 777, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 36.214, mean reward: 3.018 [2.528, 4.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.356, 10.100], loss: 0.287367, mae: 0.416790, mean_q: 4.441106
 42562/100000: episode: 778, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 92.167, mean reward: 3.545 [2.604, 4.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.297, 10.100], loss: 0.216433, mae: 0.430735, mean_q: 4.500256
 42581/100000: episode: 779, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 91.881, mean reward: 4.836 [2.684, 10.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.539, 10.100], loss: 0.234791, mae: 0.408905, mean_q: 4.577162
 42607/100000: episode: 780, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 82.271, mean reward: 3.164 [1.592, 5.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.877, 10.100], loss: 0.226870, mae: 0.427276, mean_q: 4.546441
 42633/100000: episode: 781, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 80.233, mean reward: 3.086 [2.234, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.389, 10.100], loss: 0.231543, mae: 0.429172, mean_q: 4.517375
 42645/100000: episode: 782, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 40.019, mean reward: 3.335 [2.692, 4.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.226, 10.100], loss: 0.352433, mae: 0.451762, mean_q: 4.520865
 42675/100000: episode: 783, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 109.354, mean reward: 3.645 [2.459, 7.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.882, 10.100], loss: 0.223795, mae: 0.427447, mean_q: 4.667819
 42705/100000: episode: 784, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 125.356, mean reward: 4.179 [3.097, 7.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.699, 10.100], loss: 0.199629, mae: 0.402768, mean_q: 4.513443
 42717/100000: episode: 785, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 44.503, mean reward: 3.709 [3.031, 4.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.422, 10.100], loss: 12.020215, mae: 0.999255, mean_q: 4.980646
 42729/100000: episode: 786, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 43.806, mean reward: 3.650 [3.104, 5.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.291, 10.100], loss: 0.492913, mae: 0.675973, mean_q: 4.430281
 42761/100000: episode: 787, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 238.854, mean reward: 7.464 [3.495, 19.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.543, 10.100], loss: 0.333795, mae: 0.492576, mean_q: 4.705003
 42795/100000: episode: 788, duration: 0.197s, episode steps: 34, steps per second: 173, episode reward: 166.186, mean reward: 4.888 [2.584, 10.698], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.237, 10.100], loss: 4.523529, mae: 0.698343, mean_q: 4.734727
 42827/100000: episode: 789, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 136.600, mean reward: 4.269 [2.411, 6.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-0.625, 10.100], loss: 0.298515, mae: 0.485291, mean_q: 4.673363
 42861/100000: episode: 790, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 143.657, mean reward: 4.225 [2.239, 8.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-1.501, 10.100], loss: 0.363958, mae: 0.467105, mean_q: 4.718916
 42873/100000: episode: 791, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 53.947, mean reward: 4.496 [3.438, 6.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.139, 10.100], loss: 11.618927, mae: 0.749954, mean_q: 4.949619
 42892/100000: episode: 792, duration: 0.096s, episode steps: 19, steps per second: 197, episode reward: 62.435, mean reward: 3.286 [1.962, 4.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.199, 10.100], loss: 0.550187, mae: 0.659288, mean_q: 4.642605
 42904/100000: episode: 793, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 49.434, mean reward: 4.119 [3.127, 6.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.311, 10.100], loss: 0.441230, mae: 0.526948, mean_q: 4.994913
 42937/100000: episode: 794, duration: 0.159s, episode steps: 33, steps per second: 207, episode reward: 121.125, mean reward: 3.670 [2.134, 5.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.554, 10.100], loss: 4.584944, mae: 0.744802, mean_q: 4.879604
 42966/100000: episode: 795, duration: 0.147s, episode steps: 29, steps per second: 198, episode reward: 108.937, mean reward: 3.756 [2.516, 9.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.479, 10.100], loss: 0.575619, mae: 0.563139, mean_q: 4.996480
 42998/100000: episode: 796, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 203.877, mean reward: 6.371 [2.764, 27.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.567, 10.100], loss: 4.735635, mae: 0.762511, mean_q: 4.911112
 43017/100000: episode: 797, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 80.614, mean reward: 4.243 [2.618, 10.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.541, 10.100], loss: 0.260647, mae: 0.491247, mean_q: 4.768553
[Info] FALSIFICATION!
[Info] Levels: [5.3964996, 7.572872, 9.865554]
[Info] Cond. Prob: [0.1, 0.1, 0.13]
[Info] Error Prob: 0.0013000000000000004

 43023/100000: episode: 798, duration: 4.392s, episode steps: 6, steps per second: 1, episode reward: 145.395, mean reward: 24.233 [4.362, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.864 [-0.010, 8.729], loss: 0.308061, mae: 0.546996, mean_q: 5.244085
 43123/100000: episode: 799, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 186.795, mean reward: 1.868 [1.478, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.927, 10.133], loss: 1.788280, mae: 0.571856, mean_q: 4.974187
 43223/100000: episode: 800, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 190.738, mean reward: 1.907 [1.446, 4.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.172, 10.098], loss: 3.282375, mae: 0.696590, mean_q: 5.084039
 43323/100000: episode: 801, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 190.982, mean reward: 1.910 [1.478, 3.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.819, 10.142], loss: 0.420138, mae: 0.513352, mean_q: 4.975081
 43423/100000: episode: 802, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 219.483, mean reward: 2.195 [1.453, 4.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.564, 10.338], loss: 1.738481, mae: 0.560492, mean_q: 5.028149
 43523/100000: episode: 803, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 199.434, mean reward: 1.994 [1.481, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.453, 10.318], loss: 1.801573, mae: 0.611547, mean_q: 5.067000
 43623/100000: episode: 804, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 180.956, mean reward: 1.810 [1.452, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.770, 10.240], loss: 0.622523, mae: 0.562389, mean_q: 5.065856
 43723/100000: episode: 805, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 180.712, mean reward: 1.807 [1.482, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.220, 10.208], loss: 1.736318, mae: 0.562130, mean_q: 5.080521
 43823/100000: episode: 806, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 183.111, mean reward: 1.831 [1.495, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.196, 10.206], loss: 1.818604, mae: 0.618977, mean_q: 5.058587
 43923/100000: episode: 807, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 195.068, mean reward: 1.951 [1.470, 3.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.194, 10.227], loss: 3.312985, mae: 0.723166, mean_q: 5.139674
 44023/100000: episode: 808, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 203.272, mean reward: 2.033 [1.473, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.774, 10.104], loss: 3.205150, mae: 0.645014, mean_q: 5.060194
 44123/100000: episode: 809, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 205.237, mean reward: 2.052 [1.508, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.685, 10.098], loss: 3.060937, mae: 0.654183, mean_q: 5.059953
 44223/100000: episode: 810, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.919, mean reward: 1.849 [1.439, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.223, 10.362], loss: 0.395572, mae: 0.517482, mean_q: 4.996395
 44323/100000: episode: 811, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 184.195, mean reward: 1.842 [1.453, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.296, 10.098], loss: 0.450185, mae: 0.518562, mean_q: 5.056384
 44423/100000: episode: 812, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 202.601, mean reward: 2.026 [1.467, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.925, 10.184], loss: 0.365086, mae: 0.513791, mean_q: 5.053851
 44523/100000: episode: 813, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 192.129, mean reward: 1.921 [1.459, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.509, 10.202], loss: 3.194172, mae: 0.669555, mean_q: 5.055799
 44623/100000: episode: 814, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 217.099, mean reward: 2.171 [1.490, 5.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.373, 10.324], loss: 4.285696, mae: 0.699139, mean_q: 5.064238
 44723/100000: episode: 815, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 212.862, mean reward: 2.129 [1.500, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.589, 10.098], loss: 3.311234, mae: 0.699846, mean_q: 5.002092
 44823/100000: episode: 816, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.615, mean reward: 1.906 [1.459, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.257, 10.098], loss: 1.781817, mae: 0.593681, mean_q: 5.013638
 44923/100000: episode: 817, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 191.901, mean reward: 1.919 [1.448, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.109, 10.123], loss: 0.444785, mae: 0.501124, mean_q: 4.928243
 45023/100000: episode: 818, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 233.439, mean reward: 2.334 [1.623, 4.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.429, 10.294], loss: 1.691049, mae: 0.556840, mean_q: 4.874081
 45123/100000: episode: 819, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 200.335, mean reward: 2.003 [1.496, 5.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.774, 10.098], loss: 0.494911, mae: 0.507122, mean_q: 4.896478
 45223/100000: episode: 820, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.994, mean reward: 1.830 [1.433, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.386, 10.185], loss: 4.747767, mae: 0.771222, mean_q: 4.976529
 45323/100000: episode: 821, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 184.703, mean reward: 1.847 [1.500, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.359, 10.203], loss: 1.785245, mae: 0.582377, mean_q: 4.878977
 45423/100000: episode: 822, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 224.467, mean reward: 2.245 [1.501, 6.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.159, 10.098], loss: 3.058743, mae: 0.647579, mean_q: 4.956289
 45523/100000: episode: 823, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 188.968, mean reward: 1.890 [1.457, 2.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.939, 10.267], loss: 2.882637, mae: 0.609389, mean_q: 4.923501
 45623/100000: episode: 824, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 182.695, mean reward: 1.827 [1.466, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.670, 10.130], loss: 1.785452, mae: 0.566299, mean_q: 4.851752
 45723/100000: episode: 825, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 187.861, mean reward: 1.879 [1.455, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.553, 10.122], loss: 1.764214, mae: 0.583525, mean_q: 4.876322
 45823/100000: episode: 826, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 191.917, mean reward: 1.919 [1.476, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.704, 10.098], loss: 1.670352, mae: 0.540419, mean_q: 4.780076
 45923/100000: episode: 827, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 192.376, mean reward: 1.924 [1.436, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.668, 10.098], loss: 1.988974, mae: 0.602277, mean_q: 4.809895
 46023/100000: episode: 828, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 267.982, mean reward: 2.680 [1.579, 5.538], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.519, 10.098], loss: 5.529445, mae: 0.712519, mean_q: 4.840407
 46123/100000: episode: 829, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 205.716, mean reward: 2.057 [1.487, 4.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.737, 10.098], loss: 0.376649, mae: 0.490412, mean_q: 4.680041
 46223/100000: episode: 830, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 181.836, mean reward: 1.818 [1.445, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.970, 10.098], loss: 4.348135, mae: 0.701592, mean_q: 4.885334
 46323/100000: episode: 831, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 220.146, mean reward: 2.201 [1.443, 4.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.970, 10.279], loss: 0.443012, mae: 0.476092, mean_q: 4.670349
 46423/100000: episode: 832, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 183.037, mean reward: 1.830 [1.457, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.140, 10.145], loss: 1.686376, mae: 0.499263, mean_q: 4.687929
 46523/100000: episode: 833, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 181.514, mean reward: 1.815 [1.461, 2.669], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.876, 10.321], loss: 3.059258, mae: 0.633863, mean_q: 4.696552
 46623/100000: episode: 834, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 191.004, mean reward: 1.910 [1.463, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-2.038, 10.268], loss: 2.991009, mae: 0.580523, mean_q: 4.673614
 46723/100000: episode: 835, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 197.201, mean reward: 1.972 [1.450, 6.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.834, 10.156], loss: 4.196910, mae: 0.645823, mean_q: 4.660941
 46823/100000: episode: 836, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 206.791, mean reward: 2.068 [1.459, 4.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.884, 10.098], loss: 1.794613, mae: 0.543432, mean_q: 4.548123
 46923/100000: episode: 837, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 190.908, mean reward: 1.909 [1.461, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.449, 10.098], loss: 1.556648, mae: 0.497444, mean_q: 4.597515
 47023/100000: episode: 838, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 184.858, mean reward: 1.849 [1.471, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.230, 10.126], loss: 4.398073, mae: 0.622120, mean_q: 4.595501
 47123/100000: episode: 839, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 205.962, mean reward: 2.060 [1.473, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.536, 10.337], loss: 1.746322, mae: 0.546868, mean_q: 4.532696
 47223/100000: episode: 840, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 180.503, mean reward: 1.805 [1.460, 2.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.012, 10.098], loss: 0.308474, mae: 0.422143, mean_q: 4.441491
 47323/100000: episode: 841, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 200.290, mean reward: 2.003 [1.489, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.301, 10.200], loss: 0.404242, mae: 0.430239, mean_q: 4.471551
 47423/100000: episode: 842, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.195, mean reward: 1.922 [1.510, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.140, 10.098], loss: 0.313522, mae: 0.402319, mean_q: 4.384155
 47523/100000: episode: 843, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.046, mean reward: 1.830 [1.443, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.548, 10.098], loss: 0.369243, mae: 0.415725, mean_q: 4.357743
 47623/100000: episode: 844, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.205, mean reward: 1.822 [1.474, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.719, 10.315], loss: 1.468904, mae: 0.436971, mean_q: 4.286574
 47723/100000: episode: 845, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 198.906, mean reward: 1.989 [1.511, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.579, 10.211], loss: 1.456962, mae: 0.430108, mean_q: 4.213771
 47823/100000: episode: 846, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 197.945, mean reward: 1.979 [1.534, 4.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.400, 10.098], loss: 0.166727, mae: 0.350828, mean_q: 4.078256
 47923/100000: episode: 847, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 208.186, mean reward: 2.082 [1.447, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.449, 10.098], loss: 1.568978, mae: 0.407827, mean_q: 4.036541
 48023/100000: episode: 848, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 223.255, mean reward: 2.233 [1.504, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.136, 10.380], loss: 0.159963, mae: 0.326766, mean_q: 3.918550
 48123/100000: episode: 849, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.465, mean reward: 1.885 [1.458, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.812, 10.417], loss: 0.098670, mae: 0.317199, mean_q: 3.937353
 48223/100000: episode: 850, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 205.014, mean reward: 2.050 [1.439, 5.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.788, 10.116], loss: 0.102703, mae: 0.312139, mean_q: 3.925839
 48323/100000: episode: 851, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 186.455, mean reward: 1.865 [1.437, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.049, 10.098], loss: 0.086753, mae: 0.299246, mean_q: 3.893912
 48423/100000: episode: 852, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 197.328, mean reward: 1.973 [1.530, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.166, 10.320], loss: 0.091901, mae: 0.305175, mean_q: 3.891339
 48523/100000: episode: 853, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 225.535, mean reward: 2.255 [1.439, 4.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.322, 10.098], loss: 0.095536, mae: 0.303322, mean_q: 3.903523
 48623/100000: episode: 854, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.100, mean reward: 1.861 [1.467, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.430, 10.098], loss: 0.098234, mae: 0.304186, mean_q: 3.901925
 48723/100000: episode: 855, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 236.817, mean reward: 2.368 [1.464, 8.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.964, 10.098], loss: 0.097334, mae: 0.309695, mean_q: 3.920035
 48823/100000: episode: 856, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 198.050, mean reward: 1.980 [1.436, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.987, 10.098], loss: 0.118288, mae: 0.316272, mean_q: 3.940368
 48923/100000: episode: 857, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 194.833, mean reward: 1.948 [1.476, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.355, 10.278], loss: 0.104255, mae: 0.312135, mean_q: 3.938055
 49023/100000: episode: 858, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 196.895, mean reward: 1.969 [1.448, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.723, 10.098], loss: 0.112942, mae: 0.314760, mean_q: 3.950077
 49123/100000: episode: 859, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 190.713, mean reward: 1.907 [1.437, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.266, 10.098], loss: 0.089681, mae: 0.303766, mean_q: 3.949525
 49223/100000: episode: 860, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 229.541, mean reward: 2.295 [1.494, 31.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.392, 10.098], loss: 0.247652, mae: 0.328294, mean_q: 3.932397
 49323/100000: episode: 861, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 203.733, mean reward: 2.037 [1.469, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.693, 10.256], loss: 0.115763, mae: 0.313275, mean_q: 3.951196
 49423/100000: episode: 862, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 238.521, mean reward: 2.385 [1.456, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.521, 10.339], loss: 0.248603, mae: 0.333374, mean_q: 3.956646
 49523/100000: episode: 863, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 181.487, mean reward: 1.815 [1.451, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.754, 10.177], loss: 0.377820, mae: 0.341316, mean_q: 3.978916
 49623/100000: episode: 864, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 182.180, mean reward: 1.822 [1.451, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.200, 10.155], loss: 0.101430, mae: 0.305329, mean_q: 3.943711
 49723/100000: episode: 865, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.550, mean reward: 1.925 [1.457, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.756, 10.098], loss: 0.103584, mae: 0.312777, mean_q: 3.957565
 49823/100000: episode: 866, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 184.102, mean reward: 1.841 [1.459, 2.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.262, 10.098], loss: 0.109290, mae: 0.306529, mean_q: 3.928632
 49923/100000: episode: 867, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 183.518, mean reward: 1.835 [1.443, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.206, 10.126], loss: 0.105537, mae: 0.312669, mean_q: 3.930924
 50023/100000: episode: 868, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 199.334, mean reward: 1.993 [1.453, 6.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.135, 10.168], loss: 0.237635, mae: 0.328356, mean_q: 3.936050
 50123/100000: episode: 869, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 190.310, mean reward: 1.903 [1.454, 4.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.408, 10.163], loss: 0.250382, mae: 0.327959, mean_q: 3.935078
 50223/100000: episode: 870, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.372, mean reward: 1.894 [1.448, 3.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.619, 10.098], loss: 0.231902, mae: 0.316832, mean_q: 3.918670
 50323/100000: episode: 871, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 194.940, mean reward: 1.949 [1.437, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.012, 10.336], loss: 0.235454, mae: 0.322687, mean_q: 3.919298
 50423/100000: episode: 872, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 200.807, mean reward: 2.008 [1.508, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.742, 10.098], loss: 0.106576, mae: 0.312112, mean_q: 3.909630
 50523/100000: episode: 873, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.811, mean reward: 1.918 [1.435, 6.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.846, 10.175], loss: 0.249418, mae: 0.328193, mean_q: 3.925392
 50623/100000: episode: 874, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 183.442, mean reward: 1.834 [1.486, 2.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.904, 10.315], loss: 0.100283, mae: 0.310372, mean_q: 3.918024
 50723/100000: episode: 875, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 202.282, mean reward: 2.023 [1.467, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.822, 10.374], loss: 0.100112, mae: 0.314930, mean_q: 3.930974
 50823/100000: episode: 876, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 209.390, mean reward: 2.094 [1.536, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.277, 10.098], loss: 0.098724, mae: 0.303759, mean_q: 3.936699
 50923/100000: episode: 877, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 201.872, mean reward: 2.019 [1.505, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.449 [-1.212, 10.241], loss: 0.516706, mae: 0.358236, mean_q: 3.962691
 51023/100000: episode: 878, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 183.154, mean reward: 1.832 [1.436, 2.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.982, 10.248], loss: 0.230673, mae: 0.319240, mean_q: 3.918804
 51123/100000: episode: 879, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 210.717, mean reward: 2.107 [1.507, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.774, 10.098], loss: 0.238138, mae: 0.332359, mean_q: 3.914222
 51223/100000: episode: 880, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 185.780, mean reward: 1.858 [1.444, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.914, 10.100], loss: 0.230256, mae: 0.310828, mean_q: 3.908347
 51323/100000: episode: 881, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 186.370, mean reward: 1.864 [1.464, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.277, 10.401], loss: 0.222546, mae: 0.311175, mean_q: 3.903693
 51423/100000: episode: 882, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 184.433, mean reward: 1.844 [1.433, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.272, 10.174], loss: 0.482644, mae: 0.336596, mean_q: 3.929646
 51523/100000: episode: 883, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 188.411, mean reward: 1.884 [1.437, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.809, 10.108], loss: 0.356349, mae: 0.322685, mean_q: 3.898136
 51623/100000: episode: 884, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 183.017, mean reward: 1.830 [1.454, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.798, 10.229], loss: 0.236304, mae: 0.326747, mean_q: 3.904055
 51723/100000: episode: 885, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 205.760, mean reward: 2.058 [1.475, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.626, 10.098], loss: 0.109513, mae: 0.307071, mean_q: 3.903639
 51823/100000: episode: 886, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 187.737, mean reward: 1.877 [1.476, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.321, 10.098], loss: 0.221879, mae: 0.308782, mean_q: 3.882278
 51923/100000: episode: 887, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.583, mean reward: 1.816 [1.485, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.892, 10.130], loss: 0.089834, mae: 0.296038, mean_q: 3.880673
 52023/100000: episode: 888, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 192.987, mean reward: 1.930 [1.436, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-2.196, 10.098], loss: 0.234470, mae: 0.321406, mean_q: 3.901299
 52123/100000: episode: 889, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 188.541, mean reward: 1.885 [1.497, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.904, 10.098], loss: 0.094522, mae: 0.294187, mean_q: 3.877424
 52223/100000: episode: 890, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 198.912, mean reward: 1.989 [1.458, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.463, 10.374], loss: 0.093360, mae: 0.291204, mean_q: 3.864542
 52323/100000: episode: 891, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 180.263, mean reward: 1.803 [1.453, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.832, 10.128], loss: 0.097908, mae: 0.296976, mean_q: 3.880607
 52423/100000: episode: 892, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 191.575, mean reward: 1.916 [1.443, 4.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.609, 10.098], loss: 0.238400, mae: 0.317600, mean_q: 3.900702
 52523/100000: episode: 893, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 183.559, mean reward: 1.836 [1.491, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.817, 10.098], loss: 0.363573, mae: 0.323110, mean_q: 3.894407
 52623/100000: episode: 894, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 184.861, mean reward: 1.849 [1.456, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.703, 10.098], loss: 0.353858, mae: 0.342568, mean_q: 3.908373
 52723/100000: episode: 895, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 186.123, mean reward: 1.861 [1.439, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.441, 10.098], loss: 0.228558, mae: 0.321495, mean_q: 3.922489
 52823/100000: episode: 896, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 193.425, mean reward: 1.934 [1.506, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.369, 10.278], loss: 0.237951, mae: 0.322703, mean_q: 3.892384
 52923/100000: episode: 897, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.700, mean reward: 1.957 [1.459, 4.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.269, 10.098], loss: 0.097165, mae: 0.295073, mean_q: 3.868400
[Info] 1-TH LEVEL FOUND: 5.879594802856445, Considering 10/90 traces
 53023/100000: episode: 898, duration: 4.654s, episode steps: 100, steps per second: 21, episode reward: 192.064, mean reward: 1.921 [1.482, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.653, 10.098], loss: 0.095131, mae: 0.296358, mean_q: 3.866638
 53038/100000: episode: 899, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 42.536, mean reward: 2.836 [2.345, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.381, 10.100], loss: 0.079896, mae: 0.286868, mean_q: 3.846076
 53071/100000: episode: 900, duration: 0.172s, episode steps: 33, steps per second: 191, episode reward: 90.912, mean reward: 2.755 [1.564, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.326, 10.100], loss: 0.085743, mae: 0.295934, mean_q: 3.862160
 53121/100000: episode: 901, duration: 0.262s, episode steps: 50, steps per second: 191, episode reward: 103.879, mean reward: 2.078 [1.490, 3.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.953, 10.100], loss: 0.083270, mae: 0.288597, mean_q: 3.828571
 53170/100000: episode: 902, duration: 0.233s, episode steps: 49, steps per second: 210, episode reward: 107.342, mean reward: 2.191 [1.561, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.589, 10.237], loss: 0.088441, mae: 0.288736, mean_q: 3.876817
 53187/100000: episode: 903, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 57.861, mean reward: 3.404 [2.424, 4.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.356, 10.100], loss: 0.126678, mae: 0.304658, mean_q: 3.854403
 53236/100000: episode: 904, duration: 0.284s, episode steps: 49, steps per second: 172, episode reward: 112.420, mean reward: 2.294 [1.587, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.512, 10.373], loss: 0.085650, mae: 0.287918, mean_q: 3.838060
 53286/100000: episode: 905, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 98.522, mean reward: 1.970 [1.451, 2.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.037, 10.235], loss: 0.080207, mae: 0.288129, mean_q: 3.872901
 53319/100000: episode: 906, duration: 0.192s, episode steps: 33, steps per second: 172, episode reward: 120.582, mean reward: 3.654 [2.216, 5.155], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.182, 10.100], loss: 0.497716, mae: 0.343115, mean_q: 3.910080
 53369/100000: episode: 907, duration: 0.287s, episode steps: 50, steps per second: 174, episode reward: 132.366, mean reward: 2.647 [1.479, 5.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.327, 10.168], loss: 0.353058, mae: 0.329859, mean_q: 3.922765
 53389/100000: episode: 908, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 54.851, mean reward: 2.743 [2.128, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.852, 10.100], loss: 0.119338, mae: 0.331412, mean_q: 3.959331
 53439/100000: episode: 909, duration: 0.255s, episode steps: 50, steps per second: 196, episode reward: 104.998, mean reward: 2.100 [1.533, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.321, 10.205], loss: 0.361975, mae: 0.334114, mean_q: 3.905315
 53510/100000: episode: 910, duration: 0.361s, episode steps: 71, steps per second: 197, episode reward: 136.482, mean reward: 1.922 [1.507, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.727 [-0.474, 10.100], loss: 0.288826, mae: 0.334938, mean_q: 3.941642
 53560/100000: episode: 911, duration: 0.246s, episode steps: 50, steps per second: 203, episode reward: 91.669, mean reward: 1.833 [1.443, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.909 [-0.368, 10.160], loss: 0.394175, mae: 0.361585, mean_q: 3.938054
 53593/100000: episode: 912, duration: 0.159s, episode steps: 33, steps per second: 207, episode reward: 94.771, mean reward: 2.872 [2.055, 5.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-1.073, 10.100], loss: 0.497744, mae: 0.365497, mean_q: 3.952533
 53664/100000: episode: 913, duration: 0.343s, episode steps: 71, steps per second: 207, episode reward: 144.750, mean reward: 2.039 [1.446, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.729 [-0.405, 10.109], loss: 0.103996, mae: 0.315093, mean_q: 3.946485
 53735/100000: episode: 914, duration: 0.387s, episode steps: 71, steps per second: 184, episode reward: 127.416, mean reward: 1.795 [1.493, 2.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.735 [-0.887, 10.181], loss: 0.093990, mae: 0.301597, mean_q: 3.903832
 53761/100000: episode: 915, duration: 0.128s, episode steps: 26, steps per second: 202, episode reward: 64.917, mean reward: 2.497 [1.838, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.097, 10.100], loss: 0.109140, mae: 0.319339, mean_q: 3.886338
 53810/100000: episode: 916, duration: 0.250s, episode steps: 49, steps per second: 196, episode reward: 110.778, mean reward: 2.261 [1.467, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.630, 10.166], loss: 0.100049, mae: 0.317744, mean_q: 3.905375
 53827/100000: episode: 917, duration: 0.082s, episode steps: 17, steps per second: 208, episode reward: 52.415, mean reward: 3.083 [2.146, 4.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.781, 10.100], loss: 0.103047, mae: 0.323118, mean_q: 3.940008
 53876/100000: episode: 918, duration: 0.257s, episode steps: 49, steps per second: 191, episode reward: 106.101, mean reward: 2.165 [1.637, 2.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.819, 10.279], loss: 0.076344, mae: 0.285615, mean_q: 3.893235
 53902/100000: episode: 919, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 70.983, mean reward: 2.730 [1.578, 7.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.383, 10.118], loss: 0.094875, mae: 0.297309, mean_q: 3.952583
 53935/100000: episode: 920, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 90.707, mean reward: 2.749 [1.665, 7.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.256, 10.100], loss: 0.105584, mae: 0.309778, mean_q: 3.941907
 53950/100000: episode: 921, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 34.115, mean reward: 2.274 [1.933, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.158, 10.100], loss: 0.083666, mae: 0.285010, mean_q: 3.862364
 54000/100000: episode: 922, duration: 0.256s, episode steps: 50, steps per second: 195, episode reward: 117.415, mean reward: 2.348 [1.524, 4.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.358, 10.100], loss: 0.355027, mae: 0.326348, mean_q: 3.949321
 54015/100000: episode: 923, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 44.180, mean reward: 2.945 [2.465, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.435, 10.100], loss: 0.126979, mae: 0.354612, mean_q: 3.987893
 54035/100000: episode: 924, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 49.023, mean reward: 2.451 [1.936, 3.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.348, 10.100], loss: 0.718087, mae: 0.378012, mean_q: 4.004820
 54055/100000: episode: 925, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 51.020, mean reward: 2.551 [1.937, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.100], loss: 0.080602, mae: 0.288972, mean_q: 3.899110
 54126/100000: episode: 926, duration: 0.392s, episode steps: 71, steps per second: 181, episode reward: 146.512, mean reward: 2.064 [1.508, 4.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.725 [-0.870, 10.125], loss: 0.111687, mae: 0.325944, mean_q: 3.946866
 54159/100000: episode: 927, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 94.928, mean reward: 2.877 [1.660, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.230, 10.100], loss: 0.097517, mae: 0.306152, mean_q: 3.949182
 54230/100000: episode: 928, duration: 0.340s, episode steps: 71, steps per second: 209, episode reward: 135.007, mean reward: 1.902 [1.474, 3.851], mean action: 0.000 [0.000, 0.000], mean observation: 1.722 [-0.183, 10.100], loss: 0.126620, mae: 0.334171, mean_q: 3.949576
 54245/100000: episode: 929, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 37.764, mean reward: 2.518 [1.730, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.117, 10.100], loss: 0.077219, mae: 0.285554, mean_q: 3.885595
 54260/100000: episode: 930, duration: 0.076s, episode steps: 15, steps per second: 198, episode reward: 35.991, mean reward: 2.399 [1.855, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.765, 10.100], loss: 0.097281, mae: 0.294265, mean_q: 3.967355
 54280/100000: episode: 931, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 55.848, mean reward: 2.792 [1.921, 4.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.107, 10.100], loss: 0.112264, mae: 0.318095, mean_q: 3.969767
 54329/100000: episode: 932, duration: 0.261s, episode steps: 49, steps per second: 188, episode reward: 109.872, mean reward: 2.242 [1.547, 3.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.566, 10.151], loss: 0.098214, mae: 0.300222, mean_q: 3.939044
 54400/100000: episode: 933, duration: 0.341s, episode steps: 71, steps per second: 208, episode reward: 139.700, mean reward: 1.968 [1.477, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.725 [-1.028, 10.147], loss: 0.095429, mae: 0.308104, mean_q: 3.934359
 54450/100000: episode: 934, duration: 0.256s, episode steps: 50, steps per second: 195, episode reward: 104.821, mean reward: 2.096 [1.463, 4.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.760, 10.100], loss: 0.100669, mae: 0.305782, mean_q: 3.962328
 54500/100000: episode: 935, duration: 0.268s, episode steps: 50, steps per second: 187, episode reward: 126.189, mean reward: 2.524 [1.491, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.357, 10.100], loss: 0.112907, mae: 0.318763, mean_q: 4.000412
 54520/100000: episode: 936, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 67.349, mean reward: 3.367 [2.076, 6.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.537, 10.100], loss: 0.134403, mae: 0.343986, mean_q: 4.061335
 54570/100000: episode: 937, duration: 0.257s, episode steps: 50, steps per second: 195, episode reward: 141.078, mean reward: 2.822 [1.536, 4.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.680, 10.155], loss: 0.122790, mae: 0.341615, mean_q: 4.016089
 54590/100000: episode: 938, duration: 0.106s, episode steps: 20, steps per second: 189, episode reward: 65.516, mean reward: 3.276 [2.220, 6.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.281, 10.100], loss: 0.106341, mae: 0.319926, mean_q: 3.996212
 54616/100000: episode: 939, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 88.734, mean reward: 3.413 [2.460, 6.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.423, 10.100], loss: 0.099286, mae: 0.310329, mean_q: 4.025907
 54633/100000: episode: 940, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 43.242, mean reward: 2.544 [2.169, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.010, 10.100], loss: 0.100442, mae: 0.309258, mean_q: 4.042871
 54648/100000: episode: 941, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 45.849, mean reward: 3.057 [2.399, 4.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.608, 10.100], loss: 0.118303, mae: 0.322660, mean_q: 4.025965
 54668/100000: episode: 942, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 49.761, mean reward: 2.488 [1.630, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.504, 10.100], loss: 0.118793, mae: 0.327016, mean_q: 3.998476
 54739/100000: episode: 943, duration: 0.356s, episode steps: 71, steps per second: 200, episode reward: 147.566, mean reward: 2.078 [1.446, 3.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.732 [-1.430, 10.151], loss: 0.107406, mae: 0.324437, mean_q: 4.027011
 54810/100000: episode: 944, duration: 0.380s, episode steps: 71, steps per second: 187, episode reward: 133.100, mean reward: 1.875 [1.458, 4.159], mean action: 0.000 [0.000, 0.000], mean observation: 1.731 [-1.637, 10.150], loss: 0.117355, mae: 0.335314, mean_q: 4.058280
 54830/100000: episode: 945, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 42.507, mean reward: 2.125 [1.622, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.104, 10.100], loss: 0.091218, mae: 0.309118, mean_q: 4.023190
 54845/100000: episode: 946, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 36.461, mean reward: 2.431 [2.155, 2.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.249, 10.100], loss: 0.121870, mae: 0.359348, mean_q: 4.050432
 54860/100000: episode: 947, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 47.377, mean reward: 3.158 [2.484, 4.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.583, 10.100], loss: 0.123812, mae: 0.371065, mean_q: 4.068855
 54880/100000: episode: 948, duration: 0.105s, episode steps: 20, steps per second: 190, episode reward: 44.250, mean reward: 2.212 [1.838, 2.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.199, 10.100], loss: 0.148778, mae: 0.346639, mean_q: 4.083747
 54913/100000: episode: 949, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 90.454, mean reward: 2.741 [1.530, 4.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.728, 10.100], loss: 0.118329, mae: 0.332914, mean_q: 4.089045
 54962/100000: episode: 950, duration: 0.274s, episode steps: 49, steps per second: 179, episode reward: 110.343, mean reward: 2.252 [1.491, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.342, 10.247], loss: 0.114583, mae: 0.331971, mean_q: 4.131135
 54977/100000: episode: 951, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 39.217, mean reward: 2.614 [2.040, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.215, 10.100], loss: 0.087816, mae: 0.292165, mean_q: 4.097419
 55010/100000: episode: 952, duration: 0.160s, episode steps: 33, steps per second: 206, episode reward: 125.611, mean reward: 3.806 [2.306, 7.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.471, 10.100], loss: 0.118910, mae: 0.319102, mean_q: 4.112047
 55025/100000: episode: 953, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 48.004, mean reward: 3.200 [2.497, 4.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.374, 10.100], loss: 0.110586, mae: 0.317319, mean_q: 4.055520
 55040/100000: episode: 954, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 45.861, mean reward: 3.057 [2.351, 4.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.415, 10.100], loss: 0.160500, mae: 0.359085, mean_q: 4.126641
 55055/100000: episode: 955, duration: 0.072s, episode steps: 15, steps per second: 208, episode reward: 46.310, mean reward: 3.087 [2.654, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.274, 10.100], loss: 0.136153, mae: 0.336259, mean_q: 4.033343
 55072/100000: episode: 956, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 46.451, mean reward: 2.732 [2.133, 4.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.708, 10.100], loss: 0.126545, mae: 0.337093, mean_q: 4.125551
 55143/100000: episode: 957, duration: 0.370s, episode steps: 71, steps per second: 192, episode reward: 141.797, mean reward: 1.997 [1.458, 4.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.733 [-0.364, 10.119], loss: 0.122681, mae: 0.344429, mean_q: 4.120533
 55169/100000: episode: 958, duration: 0.130s, episode steps: 26, steps per second: 199, episode reward: 110.257, mean reward: 4.241 [2.570, 8.573], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.283, 10.100], loss: 0.101069, mae: 0.325740, mean_q: 4.086968
 55186/100000: episode: 959, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 49.063, mean reward: 2.886 [1.808, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.165, 10.100], loss: 0.115348, mae: 0.334805, mean_q: 4.091830
 55212/100000: episode: 960, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 69.382, mean reward: 2.669 [2.215, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.723, 10.100], loss: 0.130282, mae: 0.359203, mean_q: 4.189631
 55232/100000: episode: 961, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 43.093, mean reward: 2.155 [1.779, 2.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.317, 10.100], loss: 0.137000, mae: 0.358658, mean_q: 4.163485
 55247/100000: episode: 962, duration: 0.095s, episode steps: 15, steps per second: 158, episode reward: 43.081, mean reward: 2.872 [2.174, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.240, 10.100], loss: 0.164366, mae: 0.342996, mean_q: 4.092856
 55262/100000: episode: 963, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 39.848, mean reward: 2.657 [2.231, 3.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.492, 10.100], loss: 0.115221, mae: 0.348312, mean_q: 4.128160
 55311/100000: episode: 964, duration: 0.272s, episode steps: 49, steps per second: 180, episode reward: 103.584, mean reward: 2.114 [1.556, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.342, 10.100], loss: 0.130998, mae: 0.363778, mean_q: 4.208663
 55328/100000: episode: 965, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 43.712, mean reward: 2.571 [2.099, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.093, 10.100], loss: 0.151772, mae: 0.380146, mean_q: 4.171146
 55343/100000: episode: 966, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 38.850, mean reward: 2.590 [2.195, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.270, 10.100], loss: 0.124903, mae: 0.336131, mean_q: 4.118678
 55414/100000: episode: 967, duration: 0.350s, episode steps: 71, steps per second: 203, episode reward: 153.431, mean reward: 2.161 [1.473, 4.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.715 [-0.667, 10.100], loss: 0.117232, mae: 0.340285, mean_q: 4.189742
 55440/100000: episode: 968, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 70.002, mean reward: 2.692 [1.544, 4.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.100], loss: 0.135589, mae: 0.355405, mean_q: 4.196441
 55455/100000: episode: 969, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 34.620, mean reward: 2.308 [1.734, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.148, 10.100], loss: 0.125815, mae: 0.341973, mean_q: 4.205482
 55475/100000: episode: 970, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 62.611, mean reward: 3.131 [2.157, 6.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.403, 10.100], loss: 0.130514, mae: 0.365986, mean_q: 4.224484
 55546/100000: episode: 971, duration: 0.370s, episode steps: 71, steps per second: 192, episode reward: 141.617, mean reward: 1.995 [1.449, 11.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.731 [-0.758, 10.124], loss: 0.136814, mae: 0.352913, mean_q: 4.192320
 55566/100000: episode: 972, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 52.345, mean reward: 2.617 [2.366, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.479, 10.100], loss: 0.144621, mae: 0.364041, mean_q: 4.239631
 55581/100000: episode: 973, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 45.202, mean reward: 3.013 [2.559, 4.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.340, 10.100], loss: 0.120074, mae: 0.356141, mean_q: 4.321563
 55596/100000: episode: 974, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 50.578, mean reward: 3.372 [2.470, 4.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.400, 10.100], loss: 0.161205, mae: 0.357401, mean_q: 4.199403
 55646/100000: episode: 975, duration: 0.251s, episode steps: 50, steps per second: 199, episode reward: 112.433, mean reward: 2.249 [1.514, 3.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.136, 10.161], loss: 0.128594, mae: 0.348708, mean_q: 4.256591
 55666/100000: episode: 976, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 62.455, mean reward: 3.123 [2.310, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.240, 10.100], loss: 0.152137, mae: 0.374712, mean_q: 4.240099
 55681/100000: episode: 977, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 37.402, mean reward: 2.493 [2.038, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.364, 10.100], loss: 0.106623, mae: 0.318247, mean_q: 4.235681
 55752/100000: episode: 978, duration: 0.361s, episode steps: 71, steps per second: 197, episode reward: 131.540, mean reward: 1.853 [1.441, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.729 [-0.244, 10.100], loss: 0.159136, mae: 0.365866, mean_q: 4.238376
 55769/100000: episode: 979, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 38.544, mean reward: 2.267 [1.753, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.149, 10.100], loss: 0.162083, mae: 0.361030, mean_q: 4.251584
 55784/100000: episode: 980, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 43.178, mean reward: 2.879 [2.028, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.161, 10.100], loss: 0.135158, mae: 0.375659, mean_q: 4.378922
 55817/100000: episode: 981, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 98.774, mean reward: 2.993 [1.491, 6.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.873, 10.100], loss: 0.150175, mae: 0.361533, mean_q: 4.257537
 55832/100000: episode: 982, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 48.872, mean reward: 3.258 [2.358, 5.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.357, 10.100], loss: 0.111947, mae: 0.331468, mean_q: 4.265382
 55847/100000: episode: 983, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 38.753, mean reward: 2.584 [2.304, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.076, 10.100], loss: 0.193506, mae: 0.392965, mean_q: 4.334890
 55864/100000: episode: 984, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 49.399, mean reward: 2.906 [1.891, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.149, 10.100], loss: 0.137217, mae: 0.365254, mean_q: 4.245780
 55881/100000: episode: 985, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 45.983, mean reward: 2.705 [2.275, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.408, 10.100], loss: 0.157710, mae: 0.379092, mean_q: 4.270918
 55952/100000: episode: 986, duration: 0.360s, episode steps: 71, steps per second: 197, episode reward: 151.093, mean reward: 2.128 [1.519, 4.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.736 [-0.544, 10.427], loss: 0.133589, mae: 0.355283, mean_q: 4.302952
 56001/100000: episode: 987, duration: 0.266s, episode steps: 49, steps per second: 184, episode reward: 131.283, mean reward: 2.679 [1.702, 4.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.840, 10.294], loss: 0.127828, mae: 0.347330, mean_q: 4.324936
[Info] 2-TH LEVEL FOUND: 7.999200820922852, Considering 10/90 traces
 56050/100000: episode: 988, duration: 4.340s, episode steps: 49, steps per second: 11, episode reward: 153.253, mean reward: 3.128 [2.011, 5.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.954, 10.318], loss: 0.151441, mae: 0.365651, mean_q: 4.309155
 56083/100000: episode: 989, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 90.496, mean reward: 2.742 [1.516, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.136, 10.141], loss: 0.147801, mae: 0.381297, mean_q: 4.359668
 56116/100000: episode: 990, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 109.259, mean reward: 3.311 [1.737, 7.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-1.322, 10.100], loss: 0.147567, mae: 0.380030, mean_q: 4.359984
 56149/100000: episode: 991, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 148.497, mean reward: 4.500 [1.571, 17.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.804, 10.100], loss: 0.126766, mae: 0.347107, mean_q: 4.314549
 56182/100000: episode: 992, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 76.878, mean reward: 2.330 [1.586, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.716, 10.100], loss: 0.118838, mae: 0.358364, mean_q: 4.347927
 56215/100000: episode: 993, duration: 0.164s, episode steps: 33, steps per second: 202, episode reward: 112.608, mean reward: 3.412 [2.076, 9.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.384, 10.100], loss: 0.183535, mae: 0.388705, mean_q: 4.373686
 56248/100000: episode: 994, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 122.057, mean reward: 3.699 [2.691, 6.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.994, 10.100], loss: 0.123984, mae: 0.355588, mean_q: 4.397723
 56281/100000: episode: 995, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 91.265, mean reward: 2.766 [1.525, 8.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.116, 10.100], loss: 0.148632, mae: 0.362589, mean_q: 4.351099
 56314/100000: episode: 996, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 84.758, mean reward: 2.568 [1.901, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.414, 10.100], loss: 0.253869, mae: 0.426204, mean_q: 4.484152
 56318/100000: episode: 997, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 21.357, mean reward: 5.339 [5.073, 5.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.555, 10.100], loss: 0.296810, mae: 0.448208, mean_q: 4.448033
 56351/100000: episode: 998, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 98.658, mean reward: 2.990 [1.987, 6.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.768, 10.100], loss: 0.185771, mae: 0.394816, mean_q: 4.393794
 56384/100000: episode: 999, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 101.589, mean reward: 3.078 [2.290, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.204, 10.100], loss: 0.277970, mae: 0.411474, mean_q: 4.509942
 56417/100000: episode: 1000, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 106.937, mean reward: 3.241 [2.220, 4.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.369, 10.100], loss: 0.191546, mae: 0.417522, mean_q: 4.450761
 56450/100000: episode: 1001, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 100.093, mean reward: 3.033 [1.664, 7.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.293, 10.100], loss: 0.253244, mae: 0.441901, mean_q: 4.409063
 56483/100000: episode: 1002, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 85.273, mean reward: 2.584 [1.729, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.129, 10.100], loss: 0.300028, mae: 0.419504, mean_q: 4.503872
 56516/100000: episode: 1003, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 76.828, mean reward: 2.328 [1.482, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.977, 10.104], loss: 0.498835, mae: 0.480558, mean_q: 4.548552
 56549/100000: episode: 1004, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 132.809, mean reward: 4.025 [2.457, 6.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.372, 10.100], loss: 0.311852, mae: 0.463491, mean_q: 4.633695
 56582/100000: episode: 1005, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 140.920, mean reward: 4.270 [2.207, 7.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.394, 10.100], loss: 0.202886, mae: 0.415422, mean_q: 4.502849
 56615/100000: episode: 1006, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 83.628, mean reward: 2.534 [1.492, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-1.123, 10.100], loss: 0.192927, mae: 0.411034, mean_q: 4.575648
 56648/100000: episode: 1007, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 94.270, mean reward: 2.857 [1.805, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.574, 10.100], loss: 0.306722, mae: 0.448004, mean_q: 4.562778
 56681/100000: episode: 1008, duration: 0.183s, episode steps: 33, steps per second: 181, episode reward: 90.629, mean reward: 2.746 [1.690, 5.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.220, 10.100], loss: 0.184307, mae: 0.386682, mean_q: 4.461174
 56714/100000: episode: 1009, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 88.304, mean reward: 2.676 [1.636, 8.165], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.681, 10.100], loss: 0.339851, mae: 0.454978, mean_q: 4.567803
 56747/100000: episode: 1010, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 101.775, mean reward: 3.084 [1.982, 5.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.762, 10.100], loss: 0.334694, mae: 0.455638, mean_q: 4.653349
 56780/100000: episode: 1011, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 105.023, mean reward: 3.183 [2.294, 4.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.165, 10.100], loss: 0.325309, mae: 0.443214, mean_q: 4.651383
 56813/100000: episode: 1012, duration: 0.163s, episode steps: 33, steps per second: 203, episode reward: 168.186, mean reward: 5.097 [2.835, 11.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.348, 10.100], loss: 0.189014, mae: 0.406702, mean_q: 4.650879
 56846/100000: episode: 1013, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 82.320, mean reward: 2.495 [1.653, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.506, 10.100], loss: 0.144980, mae: 0.385150, mean_q: 4.628817
 56879/100000: episode: 1014, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 84.822, mean reward: 2.570 [1.547, 4.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.709, 10.100], loss: 0.223565, mae: 0.432901, mean_q: 4.675098
 56912/100000: episode: 1015, duration: 0.157s, episode steps: 33, steps per second: 210, episode reward: 87.854, mean reward: 2.662 [1.862, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.556, 10.100], loss: 0.201026, mae: 0.409990, mean_q: 4.664358
 56945/100000: episode: 1016, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 102.674, mean reward: 3.111 [2.005, 4.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.785, 10.100], loss: 0.192337, mae: 0.407481, mean_q: 4.701137
 56978/100000: episode: 1017, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 131.684, mean reward: 3.990 [2.247, 19.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.324, 10.100], loss: 0.280124, mae: 0.438091, mean_q: 4.752294
 56982/100000: episode: 1018, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 41.888, mean reward: 10.472 [7.339, 18.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.498, 10.100], loss: 0.128062, mae: 0.346040, mean_q: 4.727246
 57015/100000: episode: 1019, duration: 0.181s, episode steps: 33, steps per second: 183, episode reward: 108.371, mean reward: 3.284 [1.838, 5.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.067, 10.100], loss: 0.306226, mae: 0.471131, mean_q: 4.766077
 57048/100000: episode: 1020, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 88.915, mean reward: 2.694 [1.968, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-1.343, 10.100], loss: 0.267534, mae: 0.424051, mean_q: 4.781766
 57081/100000: episode: 1021, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 92.888, mean reward: 2.815 [1.755, 4.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.412, 10.100], loss: 0.197122, mae: 0.406572, mean_q: 4.715564
 57114/100000: episode: 1022, duration: 0.165s, episode steps: 33, steps per second: 199, episode reward: 115.910, mean reward: 3.512 [2.710, 4.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.672, 10.100], loss: 0.418270, mae: 0.488421, mean_q: 4.778014
 57147/100000: episode: 1023, duration: 0.169s, episode steps: 33, steps per second: 195, episode reward: 89.080, mean reward: 2.699 [1.504, 4.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.527, 10.250], loss: 0.300614, mae: 0.462847, mean_q: 4.889383
 57180/100000: episode: 1024, duration: 0.166s, episode steps: 33, steps per second: 198, episode reward: 106.557, mean reward: 3.229 [1.976, 6.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.179, 10.100], loss: 0.213388, mae: 0.451739, mean_q: 4.773216
 57213/100000: episode: 1025, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 81.786, mean reward: 2.478 [1.611, 4.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-1.365, 10.100], loss: 0.444030, mae: 0.510742, mean_q: 4.852785
 57246/100000: episode: 1026, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 94.362, mean reward: 2.859 [1.474, 9.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.510, 10.105], loss: 0.236870, mae: 0.472503, mean_q: 4.937878
 57279/100000: episode: 1027, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 96.793, mean reward: 2.933 [1.882, 4.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.039, 10.100], loss: 0.515692, mae: 0.518551, mean_q: 4.885399
 57283/100000: episode: 1028, duration: 0.022s, episode steps: 4, steps per second: 178, episode reward: 32.678, mean reward: 8.170 [4.861, 10.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.532, 10.100], loss: 0.245727, mae: 0.468706, mean_q: 4.733544
 57316/100000: episode: 1029, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 159.361, mean reward: 4.829 [2.626, 30.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.272, 10.100], loss: 0.205447, mae: 0.446259, mean_q: 4.848353
 57349/100000: episode: 1030, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 96.847, mean reward: 2.935 [1.923, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.915, 10.100], loss: 0.620441, mae: 0.509818, mean_q: 4.923013
 57382/100000: episode: 1031, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 91.865, mean reward: 2.784 [1.668, 6.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.444, 10.100], loss: 0.525825, mae: 0.533340, mean_q: 4.884415
 57415/100000: episode: 1032, duration: 0.162s, episode steps: 33, steps per second: 204, episode reward: 110.060, mean reward: 3.335 [1.622, 7.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.923, 10.100], loss: 0.359609, mae: 0.474538, mean_q: 4.938572
 57448/100000: episode: 1033, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 133.995, mean reward: 4.060 [2.618, 6.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.533, 10.100], loss: 0.358263, mae: 0.475285, mean_q: 5.077291
 57481/100000: episode: 1034, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 85.898, mean reward: 2.603 [1.748, 5.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.211, 10.100], loss: 0.793207, mae: 0.522061, mean_q: 5.035316
 57514/100000: episode: 1035, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 88.183, mean reward: 2.672 [1.508, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.139], loss: 0.377561, mae: 0.498993, mean_q: 4.969082
 57547/100000: episode: 1036, duration: 0.163s, episode steps: 33, steps per second: 203, episode reward: 109.919, mean reward: 3.331 [1.741, 8.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.498, 10.100], loss: 0.609438, mae: 0.540336, mean_q: 5.012049
 57580/100000: episode: 1037, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 93.602, mean reward: 2.836 [2.002, 4.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.216, 10.100], loss: 0.466090, mae: 0.560005, mean_q: 4.974794
 57613/100000: episode: 1038, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 120.596, mean reward: 3.654 [2.240, 10.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.410, 10.100], loss: 0.323656, mae: 0.507497, mean_q: 5.014316
[Info] FALSIFICATION!
[Info] Levels: [5.879595, 7.999201, 11.427491]
[Info] Cond. Prob: [0.1, 0.1, 0.03]
[Info] Error Prob: 0.00030000000000000003

 57617/100000: episode: 1039, duration: 4.430s, episode steps: 4, steps per second: 1, episode reward: 114.424, mean reward: 28.606 [4.286, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.499, 10.100], loss: 0.258012, mae: 0.485716, mean_q: 5.190382
 57717/100000: episode: 1040, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 187.492, mean reward: 1.875 [1.448, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.200, 10.184], loss: 1.937365, mae: 0.696244, mean_q: 5.076063
 57817/100000: episode: 1041, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 224.031, mean reward: 2.240 [1.450, 6.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.386, 10.098], loss: 1.727439, mae: 0.568244, mean_q: 5.079966
 57917/100000: episode: 1042, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 178.892, mean reward: 1.789 [1.454, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.408, 10.099], loss: 1.674334, mae: 0.622223, mean_q: 4.959687
 58017/100000: episode: 1043, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 181.391, mean reward: 1.814 [1.472, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.728, 10.156], loss: 1.658857, mae: 0.537723, mean_q: 5.099850
 58117/100000: episode: 1044, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 184.743, mean reward: 1.847 [1.462, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.207, 10.210], loss: 0.607445, mae: 0.601442, mean_q: 5.028387
 58217/100000: episode: 1045, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 192.146, mean reward: 1.921 [1.457, 2.939], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.977, 10.235], loss: 1.828643, mae: 0.628850, mean_q: 5.046677
 58317/100000: episode: 1046, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 188.717, mean reward: 1.887 [1.451, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.468, 10.098], loss: 0.379854, mae: 0.510419, mean_q: 4.996227
 58417/100000: episode: 1047, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 187.385, mean reward: 1.874 [1.454, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.760, 10.107], loss: 0.462072, mae: 0.500151, mean_q: 4.957301
 58517/100000: episode: 1048, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 184.052, mean reward: 1.841 [1.451, 2.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.700, 10.147], loss: 0.520507, mae: 0.515735, mean_q: 4.992082
 58617/100000: episode: 1049, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 205.551, mean reward: 2.056 [1.512, 6.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.032, 10.098], loss: 0.369919, mae: 0.493420, mean_q: 4.986215
 58717/100000: episode: 1050, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 186.782, mean reward: 1.868 [1.439, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.545, 10.098], loss: 1.676775, mae: 0.575610, mean_q: 4.976099
 58817/100000: episode: 1051, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 204.508, mean reward: 2.045 [1.456, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.979, 10.312], loss: 0.359481, mae: 0.467922, mean_q: 4.861554
 58917/100000: episode: 1052, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 200.421, mean reward: 2.004 [1.484, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.841, 10.098], loss: 0.394300, mae: 0.485847, mean_q: 4.881651
 59017/100000: episode: 1053, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 225.189, mean reward: 2.252 [1.500, 4.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.239, 10.209], loss: 0.323017, mae: 0.468246, mean_q: 4.873199
 59117/100000: episode: 1054, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 197.188, mean reward: 1.972 [1.470, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.942, 10.098], loss: 1.656902, mae: 0.567854, mean_q: 4.888648
 59217/100000: episode: 1055, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 198.817, mean reward: 1.988 [1.480, 4.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.891, 10.098], loss: 0.321900, mae: 0.474786, mean_q: 4.879558
 59317/100000: episode: 1056, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 209.231, mean reward: 2.092 [1.452, 4.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.468, 10.291], loss: 2.107562, mae: 0.649707, mean_q: 4.918825
 59417/100000: episode: 1057, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 238.948, mean reward: 2.389 [1.468, 6.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.278, 10.526], loss: 1.733808, mae: 0.564534, mean_q: 4.905731
 59517/100000: episode: 1058, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 185.520, mean reward: 1.855 [1.513, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.650, 10.212], loss: 0.375729, mae: 0.453735, mean_q: 4.839239
 59617/100000: episode: 1059, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 183.847, mean reward: 1.838 [1.494, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.516, 10.333], loss: 3.117867, mae: 0.661361, mean_q: 4.815715
 59717/100000: episode: 1060, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.137, mean reward: 1.871 [1.452, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.974, 10.098], loss: 1.813137, mae: 0.593690, mean_q: 4.873055
 59817/100000: episode: 1061, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 209.123, mean reward: 2.091 [1.448, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.585, 10.098], loss: 0.428654, mae: 0.481686, mean_q: 4.871122
 59917/100000: episode: 1062, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 201.384, mean reward: 2.014 [1.477, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.170, 10.098], loss: 1.565312, mae: 0.529267, mean_q: 4.794159
 60017/100000: episode: 1063, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.075, mean reward: 1.821 [1.476, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.074, 10.098], loss: 1.951981, mae: 0.640967, mean_q: 4.864172
 60117/100000: episode: 1064, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 180.755, mean reward: 1.808 [1.467, 2.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.841, 10.098], loss: 1.607068, mae: 0.549298, mean_q: 4.724361
 60217/100000: episode: 1065, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 207.902, mean reward: 2.079 [1.432, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.369, 10.098], loss: 3.807828, mae: 0.593478, mean_q: 4.722026
 60317/100000: episode: 1066, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 183.159, mean reward: 1.832 [1.451, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.746, 10.098], loss: 0.446846, mae: 0.543900, mean_q: 4.732753
 60417/100000: episode: 1067, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 184.605, mean reward: 1.846 [1.459, 2.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.888, 10.098], loss: 0.418968, mae: 0.453652, mean_q: 4.688997
 60517/100000: episode: 1068, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 193.866, mean reward: 1.939 [1.454, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.375, 10.098], loss: 0.353759, mae: 0.452192, mean_q: 4.688934
 60617/100000: episode: 1069, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 188.556, mean reward: 1.886 [1.475, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.322, 10.136], loss: 0.296542, mae: 0.437766, mean_q: 4.675393
 60717/100000: episode: 1070, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 192.636, mean reward: 1.926 [1.495, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.540, 10.098], loss: 0.379641, mae: 0.461602, mean_q: 4.637091
 60817/100000: episode: 1071, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 182.404, mean reward: 1.824 [1.451, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.503, 10.133], loss: 3.800719, mae: 0.669332, mean_q: 4.680342
 60917/100000: episode: 1072, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 186.572, mean reward: 1.866 [1.445, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.455, 10.329], loss: 1.407463, mae: 0.444715, mean_q: 4.566385
 61017/100000: episode: 1073, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 185.452, mean reward: 1.855 [1.466, 2.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.025, 10.307], loss: 0.491342, mae: 0.461780, mean_q: 4.546601
 61117/100000: episode: 1074, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 201.954, mean reward: 2.020 [1.486, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.911, 10.098], loss: 0.371362, mae: 0.418617, mean_q: 4.551129
 61217/100000: episode: 1075, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 185.191, mean reward: 1.852 [1.440, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.957, 10.105], loss: 0.277396, mae: 0.390580, mean_q: 4.434725
 61317/100000: episode: 1076, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 188.833, mean reward: 1.888 [1.452, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.378, 10.323], loss: 0.234439, mae: 0.384888, mean_q: 4.390979
 61417/100000: episode: 1077, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 189.813, mean reward: 1.898 [1.465, 4.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-2.012, 10.098], loss: 0.260090, mae: 0.390860, mean_q: 4.398184
 61517/100000: episode: 1078, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 195.929, mean reward: 1.959 [1.462, 10.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.679, 10.098], loss: 0.231108, mae: 0.362411, mean_q: 4.276079
 61617/100000: episode: 1079, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 180.061, mean reward: 1.801 [1.486, 2.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.508, 10.098], loss: 2.773785, mae: 0.544055, mean_q: 4.330521
 61717/100000: episode: 1080, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.058, mean reward: 1.891 [1.464, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.303, 10.136], loss: 0.316702, mae: 0.375931, mean_q: 4.253609
 61817/100000: episode: 1081, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 188.042, mean reward: 1.880 [1.462, 2.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.406, 10.228], loss: 0.384940, mae: 0.368936, mean_q: 4.212811
 61917/100000: episode: 1082, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 190.330, mean reward: 1.903 [1.468, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.861, 10.098], loss: 0.157630, mae: 0.327598, mean_q: 4.154316
 62017/100000: episode: 1083, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 195.363, mean reward: 1.954 [1.440, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.042, 10.098], loss: 1.292125, mae: 0.370064, mean_q: 4.113239
 62117/100000: episode: 1084, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 189.748, mean reward: 1.897 [1.457, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.278, 10.438], loss: 1.430541, mae: 0.454000, mean_q: 4.087728
 62217/100000: episode: 1085, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 176.775, mean reward: 1.768 [1.448, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.166, 10.182], loss: 0.280497, mae: 0.352650, mean_q: 4.028014
 62317/100000: episode: 1086, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 206.998, mean reward: 2.070 [1.464, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.769, 10.098], loss: 0.170657, mae: 0.322883, mean_q: 3.978032
 62417/100000: episode: 1087, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 191.882, mean reward: 1.919 [1.450, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.052, 10.098], loss: 1.371420, mae: 0.429649, mean_q: 3.992720
 62517/100000: episode: 1088, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.868, mean reward: 1.929 [1.441, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.452, 10.098], loss: 0.111958, mae: 0.298955, mean_q: 3.894125
 62617/100000: episode: 1089, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.463, mean reward: 1.925 [1.487, 4.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.675, 10.098], loss: 0.142123, mae: 0.308017, mean_q: 3.871831
 62717/100000: episode: 1090, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 210.285, mean reward: 2.103 [1.437, 5.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.441, 10.265], loss: 0.102701, mae: 0.289536, mean_q: 3.853716
 62817/100000: episode: 1091, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 198.414, mean reward: 1.984 [1.465, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.166, 10.203], loss: 0.101287, mae: 0.285360, mean_q: 3.833749
 62917/100000: episode: 1092, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 183.799, mean reward: 1.838 [1.472, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.709, 10.231], loss: 0.088580, mae: 0.284134, mean_q: 3.837823
 63017/100000: episode: 1093, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 216.849, mean reward: 2.168 [1.461, 5.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.209, 10.098], loss: 0.101988, mae: 0.287894, mean_q: 3.826399
 63117/100000: episode: 1094, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 192.423, mean reward: 1.924 [1.443, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.468, 10.209], loss: 0.111043, mae: 0.291073, mean_q: 3.852544
 63217/100000: episode: 1095, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 194.463, mean reward: 1.945 [1.440, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.720, 10.098], loss: 0.094222, mae: 0.285922, mean_q: 3.843040
 63317/100000: episode: 1096, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.048, mean reward: 1.880 [1.468, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.926, 10.531], loss: 0.103676, mae: 0.291559, mean_q: 3.848501
 63417/100000: episode: 1097, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 189.151, mean reward: 1.892 [1.444, 4.532], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.820, 10.148], loss: 0.087768, mae: 0.283776, mean_q: 3.845193
 63517/100000: episode: 1098, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 190.243, mean reward: 1.902 [1.449, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.414, 10.098], loss: 0.085247, mae: 0.281461, mean_q: 3.823721
 63617/100000: episode: 1099, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 217.542, mean reward: 2.175 [1.476, 4.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.243, 10.260], loss: 0.087935, mae: 0.294325, mean_q: 3.850360
 63717/100000: episode: 1100, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 196.630, mean reward: 1.966 [1.441, 4.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.628, 10.184], loss: 0.076567, mae: 0.281756, mean_q: 3.846364
 63817/100000: episode: 1101, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 196.302, mean reward: 1.963 [1.476, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.145, 10.286], loss: 0.099253, mae: 0.288776, mean_q: 3.841218
 63917/100000: episode: 1102, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 203.266, mean reward: 2.033 [1.506, 6.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.770, 10.101], loss: 0.089053, mae: 0.281337, mean_q: 3.854702
 64017/100000: episode: 1103, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.639, mean reward: 1.876 [1.467, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.111, 10.154], loss: 0.086884, mae: 0.288023, mean_q: 3.844113
 64117/100000: episode: 1104, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.528, mean reward: 1.895 [1.476, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.679, 10.098], loss: 0.096489, mae: 0.282639, mean_q: 3.844202
 64217/100000: episode: 1105, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.390, mean reward: 1.904 [1.501, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.722, 10.160], loss: 0.093972, mae: 0.282162, mean_q: 3.839419
 64317/100000: episode: 1106, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 219.471, mean reward: 2.195 [1.438, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.439, 10.098], loss: 0.114753, mae: 0.291177, mean_q: 3.839745
 64417/100000: episode: 1107, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 200.328, mean reward: 2.003 [1.498, 4.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.603, 10.098], loss: 0.086196, mae: 0.277908, mean_q: 3.836074
 64517/100000: episode: 1108, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 197.454, mean reward: 1.975 [1.452, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.884, 10.098], loss: 0.086229, mae: 0.291284, mean_q: 3.833131
 64617/100000: episode: 1109, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 191.232, mean reward: 1.912 [1.465, 4.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.585, 10.243], loss: 0.088648, mae: 0.275576, mean_q: 3.807268
 64717/100000: episode: 1110, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 183.833, mean reward: 1.838 [1.449, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.076, 10.098], loss: 0.079008, mae: 0.285529, mean_q: 3.836211
 64817/100000: episode: 1111, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 198.073, mean reward: 1.981 [1.504, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.497, 10.098], loss: 0.079957, mae: 0.275969, mean_q: 3.826439
 64917/100000: episode: 1112, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 204.870, mean reward: 2.049 [1.447, 4.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.203, 10.098], loss: 0.084774, mae: 0.276698, mean_q: 3.801381
 65017/100000: episode: 1113, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 204.167, mean reward: 2.042 [1.471, 6.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.834, 10.357], loss: 0.099109, mae: 0.291821, mean_q: 3.835508
 65117/100000: episode: 1114, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 183.424, mean reward: 1.834 [1.445, 2.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.818, 10.317], loss: 0.079239, mae: 0.279760, mean_q: 3.811478
 65217/100000: episode: 1115, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 189.271, mean reward: 1.893 [1.492, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.220, 10.101], loss: 0.106516, mae: 0.289426, mean_q: 3.826943
 65317/100000: episode: 1116, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 179.647, mean reward: 1.796 [1.457, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.737, 10.217], loss: 0.089371, mae: 0.291226, mean_q: 3.822036
 65417/100000: episode: 1117, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 220.587, mean reward: 2.206 [1.449, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.713, 10.414], loss: 0.098517, mae: 0.290188, mean_q: 3.838932
 65517/100000: episode: 1118, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 194.523, mean reward: 1.945 [1.487, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.861, 10.203], loss: 0.095819, mae: 0.288739, mean_q: 3.845293
 65617/100000: episode: 1119, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 206.513, mean reward: 2.065 [1.478, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.377, 10.098], loss: 0.099583, mae: 0.288936, mean_q: 3.853355
 65717/100000: episode: 1120, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 206.108, mean reward: 2.061 [1.438, 4.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.557, 10.177], loss: 0.096735, mae: 0.288133, mean_q: 3.848698
 65817/100000: episode: 1121, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 198.397, mean reward: 1.984 [1.448, 6.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.059, 10.208], loss: 0.102062, mae: 0.298173, mean_q: 3.857939
 65917/100000: episode: 1122, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 204.394, mean reward: 2.044 [1.514, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.353, 10.305], loss: 0.113106, mae: 0.308488, mean_q: 3.868455
 66017/100000: episode: 1123, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 196.550, mean reward: 1.966 [1.489, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.797, 10.140], loss: 0.097792, mae: 0.301115, mean_q: 3.867497
 66117/100000: episode: 1124, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.501, mean reward: 1.885 [1.461, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.591, 10.116], loss: 0.099528, mae: 0.302115, mean_q: 3.870057
 66217/100000: episode: 1125, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 192.504, mean reward: 1.925 [1.456, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.950, 10.098], loss: 0.087520, mae: 0.288625, mean_q: 3.836512
 66317/100000: episode: 1126, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 184.703, mean reward: 1.847 [1.457, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.359, 10.098], loss: 0.095071, mae: 0.302555, mean_q: 3.853936
 66417/100000: episode: 1127, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 227.754, mean reward: 2.278 [1.548, 4.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.963, 10.098], loss: 0.111006, mae: 0.304339, mean_q: 3.857677
 66517/100000: episode: 1128, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 190.451, mean reward: 1.905 [1.490, 4.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.124, 10.098], loss: 0.107837, mae: 0.304912, mean_q: 3.876783
 66617/100000: episode: 1129, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.845, mean reward: 1.858 [1.458, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.487, 10.098], loss: 0.093759, mae: 0.302048, mean_q: 3.876770
 66717/100000: episode: 1130, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 201.522, mean reward: 2.015 [1.478, 3.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.806, 10.098], loss: 0.083842, mae: 0.294928, mean_q: 3.854191
 66817/100000: episode: 1131, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 191.217, mean reward: 1.912 [1.500, 3.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.863, 10.098], loss: 0.097647, mae: 0.304063, mean_q: 3.877714
 66917/100000: episode: 1132, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.920, mean reward: 1.879 [1.523, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.686, 10.173], loss: 0.095340, mae: 0.296688, mean_q: 3.872022
 67017/100000: episode: 1133, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 212.602, mean reward: 2.126 [1.478, 4.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.794, 10.508], loss: 0.104162, mae: 0.306583, mean_q: 3.886889
 67117/100000: episode: 1134, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.235, mean reward: 1.972 [1.442, 4.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.589, 10.119], loss: 0.106227, mae: 0.313402, mean_q: 3.886132
 67217/100000: episode: 1135, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 187.122, mean reward: 1.871 [1.460, 4.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.638, 10.270], loss: 0.114578, mae: 0.322875, mean_q: 3.884575
 67317/100000: episode: 1136, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 197.298, mean reward: 1.973 [1.461, 4.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.809, 10.098], loss: 0.097295, mae: 0.306660, mean_q: 3.889042
 67417/100000: episode: 1137, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 195.982, mean reward: 1.960 [1.495, 4.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.528, 10.098], loss: 0.100604, mae: 0.309762, mean_q: 3.878491
 67517/100000: episode: 1138, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 184.879, mean reward: 1.849 [1.468, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.462, 10.263], loss: 0.121617, mae: 0.318609, mean_q: 3.899240
[Info] 1-TH LEVEL FOUND: 5.226287364959717, Considering 10/90 traces
 67617/100000: episode: 1139, duration: 4.626s, episode steps: 100, steps per second: 22, episode reward: 213.514, mean reward: 2.135 [1.469, 6.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.661, 10.098], loss: 0.096300, mae: 0.306607, mean_q: 3.879713
 67657/100000: episode: 1140, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 85.967, mean reward: 2.149 [1.509, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-0.769, 10.124], loss: 0.109008, mae: 0.326835, mean_q: 3.909192
 67703/100000: episode: 1141, duration: 0.226s, episode steps: 46, steps per second: 203, episode reward: 116.084, mean reward: 2.524 [1.497, 4.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.362, 10.173], loss: 0.102971, mae: 0.315560, mean_q: 3.901409
 67737/100000: episode: 1142, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 81.302, mean reward: 2.391 [1.857, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.423, 10.100], loss: 0.115169, mae: 0.325024, mean_q: 3.937666
 67759/100000: episode: 1143, duration: 0.127s, episode steps: 22, steps per second: 173, episode reward: 89.190, mean reward: 4.054 [2.611, 6.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.495, 10.652], loss: 0.141873, mae: 0.347171, mean_q: 3.921131
 67805/100000: episode: 1144, duration: 0.223s, episode steps: 46, steps per second: 206, episode reward: 103.751, mean reward: 2.255 [1.688, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-1.038, 10.100], loss: 0.105695, mae: 0.318505, mean_q: 3.896196
 67842/100000: episode: 1145, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 84.995, mean reward: 2.297 [1.712, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.074, 10.100], loss: 0.110126, mae: 0.317579, mean_q: 3.916441
 67888/100000: episode: 1146, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 112.353, mean reward: 2.442 [1.714, 5.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.588, 10.100], loss: 0.114046, mae: 0.326064, mean_q: 3.956056
 67903/100000: episode: 1147, duration: 0.073s, episode steps: 15, steps per second: 207, episode reward: 37.540, mean reward: 2.503 [2.096, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.162, 10.100], loss: 0.109819, mae: 0.332185, mean_q: 3.969953
 67949/100000: episode: 1148, duration: 0.245s, episode steps: 46, steps per second: 187, episode reward: 121.228, mean reward: 2.635 [1.809, 3.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.312, 10.100], loss: 0.102702, mae: 0.325042, mean_q: 3.937093
 67989/100000: episode: 1149, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 90.008, mean reward: 2.250 [1.664, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.391, 10.146], loss: 0.092434, mae: 0.307887, mean_q: 3.922006
 68004/100000: episode: 1150, duration: 0.082s, episode steps: 15, steps per second: 183, episode reward: 40.196, mean reward: 2.680 [2.264, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.396, 10.100], loss: 0.105164, mae: 0.338831, mean_q: 3.928509
 68026/100000: episode: 1151, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 59.085, mean reward: 2.686 [2.259, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.278, 10.425], loss: 0.147632, mae: 0.332874, mean_q: 3.972105
 68041/100000: episode: 1152, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 49.731, mean reward: 3.315 [2.585, 5.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-1.465, 10.100], loss: 0.111731, mae: 0.306940, mean_q: 3.911915
 68075/100000: episode: 1153, duration: 0.187s, episode steps: 34, steps per second: 182, episode reward: 113.103, mean reward: 3.327 [2.264, 5.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.011 [-0.316, 10.100], loss: 0.133234, mae: 0.337577, mean_q: 3.978750
 68109/100000: episode: 1154, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 118.100, mean reward: 3.474 [2.246, 7.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.458, 10.100], loss: 0.141469, mae: 0.355558, mean_q: 4.003536
 68151/100000: episode: 1155, duration: 0.206s, episode steps: 42, steps per second: 204, episode reward: 109.959, mean reward: 2.618 [2.106, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.695, 10.377], loss: 0.134155, mae: 0.345032, mean_q: 4.023822
 68169/100000: episode: 1156, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 40.891, mean reward: 2.272 [1.921, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.467, 10.100], loss: 0.109757, mae: 0.338606, mean_q: 4.002298
 68215/100000: episode: 1157, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 101.646, mean reward: 2.210 [1.486, 3.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.706, 10.100], loss: 0.128363, mae: 0.340474, mean_q: 4.005815
 68257/100000: episode: 1158, duration: 0.214s, episode steps: 42, steps per second: 197, episode reward: 84.853, mean reward: 2.020 [1.439, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.515, 10.100], loss: 0.122085, mae: 0.336017, mean_q: 4.045185
 68294/100000: episode: 1159, duration: 0.204s, episode steps: 37, steps per second: 181, episode reward: 74.398, mean reward: 2.011 [1.479, 3.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.046, 10.121], loss: 0.128115, mae: 0.333252, mean_q: 4.025077
 68312/100000: episode: 1160, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 49.215, mean reward: 2.734 [2.134, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.579, 10.100], loss: 0.122705, mae: 0.344870, mean_q: 4.072098
 68358/100000: episode: 1161, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 102.463, mean reward: 2.227 [1.511, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.369, 10.236], loss: 0.110948, mae: 0.325743, mean_q: 4.018117
 68395/100000: episode: 1162, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 74.550, mean reward: 2.015 [1.493, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.035, 10.100], loss: 0.129323, mae: 0.356450, mean_q: 4.078178
 68417/100000: episode: 1163, duration: 0.107s, episode steps: 22, steps per second: 205, episode reward: 58.068, mean reward: 2.639 [2.006, 5.717], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.207, 10.317], loss: 0.134370, mae: 0.364701, mean_q: 4.017324
 68463/100000: episode: 1164, duration: 0.239s, episode steps: 46, steps per second: 193, episode reward: 108.826, mean reward: 2.366 [1.818, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.318, 10.100], loss: 0.144771, mae: 0.352850, mean_q: 4.060586
 68505/100000: episode: 1165, duration: 0.218s, episode steps: 42, steps per second: 192, episode reward: 132.607, mean reward: 3.157 [1.935, 5.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.453, 10.100], loss: 0.123566, mae: 0.339548, mean_q: 4.026781
 68551/100000: episode: 1166, duration: 0.257s, episode steps: 46, steps per second: 179, episode reward: 121.478, mean reward: 2.641 [1.707, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-1.020, 10.100], loss: 0.120146, mae: 0.329386, mean_q: 4.081557
 68597/100000: episode: 1167, duration: 0.256s, episode steps: 46, steps per second: 180, episode reward: 124.357, mean reward: 2.703 [1.719, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.449, 10.100], loss: 0.119015, mae: 0.340671, mean_q: 4.106746
 68639/100000: episode: 1168, duration: 0.239s, episode steps: 42, steps per second: 176, episode reward: 94.527, mean reward: 2.251 [1.506, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.306, 10.215], loss: 0.140370, mae: 0.361557, mean_q: 4.106243
 68657/100000: episode: 1169, duration: 0.103s, episode steps: 18, steps per second: 176, episode reward: 55.717, mean reward: 3.095 [1.854, 5.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.200, 10.100], loss: 0.114470, mae: 0.328059, mean_q: 4.051221
 68694/100000: episode: 1170, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 92.746, mean reward: 2.507 [1.926, 6.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-1.052, 10.100], loss: 0.120674, mae: 0.327080, mean_q: 4.076808
 68712/100000: episode: 1171, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 41.699, mean reward: 2.317 [1.859, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.165, 10.100], loss: 0.124091, mae: 0.335658, mean_q: 4.078656
 68758/100000: episode: 1172, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 96.251, mean reward: 2.092 [1.530, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.733, 10.174], loss: 0.119715, mae: 0.330400, mean_q: 4.099994
 68804/100000: episode: 1173, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 102.769, mean reward: 2.234 [1.660, 4.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.546, 10.100], loss: 0.140308, mae: 0.357286, mean_q: 4.114862
 68838/100000: episode: 1174, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 119.301, mean reward: 3.509 [2.462, 4.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.519, 10.100], loss: 0.155839, mae: 0.358596, mean_q: 4.120277
 68872/100000: episode: 1175, duration: 0.167s, episode steps: 34, steps per second: 203, episode reward: 83.874, mean reward: 2.467 [1.663, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.595, 10.100], loss: 0.166197, mae: 0.383154, mean_q: 4.210638
 68918/100000: episode: 1176, duration: 0.250s, episode steps: 46, steps per second: 184, episode reward: 105.127, mean reward: 2.285 [1.605, 3.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.177, 10.100], loss: 0.127296, mae: 0.343425, mean_q: 4.137050
 68933/100000: episode: 1177, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 43.696, mean reward: 2.913 [2.334, 4.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.200, 10.100], loss: 0.098785, mae: 0.325206, mean_q: 4.151805
 68979/100000: episode: 1178, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 98.702, mean reward: 2.146 [1.475, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.315, 10.101], loss: 0.141971, mae: 0.366682, mean_q: 4.179456
 69001/100000: episode: 1179, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 62.517, mean reward: 2.842 [2.280, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.395, 10.377], loss: 0.153594, mae: 0.359098, mean_q: 4.187133
 69016/100000: episode: 1180, duration: 0.094s, episode steps: 15, steps per second: 159, episode reward: 38.691, mean reward: 2.579 [1.602, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.354, 10.100], loss: 0.181893, mae: 0.422608, mean_q: 4.211322
 69034/100000: episode: 1181, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 50.256, mean reward: 2.792 [2.090, 5.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.423, 10.100], loss: 0.156016, mae: 0.381607, mean_q: 4.172823
 69068/100000: episode: 1182, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 87.439, mean reward: 2.572 [1.909, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.841, 10.100], loss: 0.145875, mae: 0.359607, mean_q: 4.142415
 69102/100000: episode: 1183, duration: 0.167s, episode steps: 34, steps per second: 204, episode reward: 106.199, mean reward: 3.123 [2.121, 4.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.015 [-0.976, 10.100], loss: 0.126254, mae: 0.352145, mean_q: 4.173968
 69142/100000: episode: 1184, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 84.094, mean reward: 2.102 [1.488, 4.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.483, 10.142], loss: 0.130759, mae: 0.347108, mean_q: 4.222853
 69188/100000: episode: 1185, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 115.416, mean reward: 2.509 [1.834, 4.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.399, 10.100], loss: 0.153419, mae: 0.370683, mean_q: 4.216743
 69206/100000: episode: 1186, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 49.396, mean reward: 2.744 [1.937, 5.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.325, 10.100], loss: 0.138876, mae: 0.350634, mean_q: 4.182140
 69243/100000: episode: 1187, duration: 0.180s, episode steps: 37, steps per second: 205, episode reward: 80.065, mean reward: 2.164 [1.692, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.972, 10.100], loss: 0.138480, mae: 0.362644, mean_q: 4.244786
 69285/100000: episode: 1188, duration: 0.211s, episode steps: 42, steps per second: 199, episode reward: 105.848, mean reward: 2.520 [1.866, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.938, 10.422], loss: 0.148292, mae: 0.364322, mean_q: 4.239645
 69319/100000: episode: 1189, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 66.807, mean reward: 1.965 [1.605, 2.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.321, 10.269], loss: 0.137135, mae: 0.360820, mean_q: 4.208170
 69334/100000: episode: 1190, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 38.256, mean reward: 2.550 [2.048, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.236, 10.100], loss: 0.127174, mae: 0.335260, mean_q: 4.170603
 69352/100000: episode: 1191, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 48.687, mean reward: 2.705 [1.965, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.874, 10.100], loss: 0.124496, mae: 0.359113, mean_q: 4.169010
 69392/100000: episode: 1192, duration: 0.203s, episode steps: 40, steps per second: 197, episode reward: 91.511, mean reward: 2.288 [1.560, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.665, 10.101], loss: 0.130916, mae: 0.361008, mean_q: 4.247794
 69426/100000: episode: 1193, duration: 0.177s, episode steps: 34, steps per second: 193, episode reward: 100.021, mean reward: 2.942 [2.163, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-1.314, 10.100], loss: 0.128076, mae: 0.351195, mean_q: 4.207918
 69466/100000: episode: 1194, duration: 0.195s, episode steps: 40, steps per second: 205, episode reward: 94.288, mean reward: 2.357 [1.589, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.261, 10.110], loss: 0.139994, mae: 0.366676, mean_q: 4.270394
 69508/100000: episode: 1195, duration: 0.215s, episode steps: 42, steps per second: 196, episode reward: 84.021, mean reward: 2.001 [1.458, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.522, 10.176], loss: 0.151156, mae: 0.377365, mean_q: 4.256964
 69530/100000: episode: 1196, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 69.621, mean reward: 3.165 [2.255, 6.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.595, 10.415], loss: 0.167288, mae: 0.384428, mean_q: 4.337250
 69548/100000: episode: 1197, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 46.450, mean reward: 2.581 [2.082, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-1.580, 10.100], loss: 0.140546, mae: 0.355646, mean_q: 4.227620
 69566/100000: episode: 1198, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 42.122, mean reward: 2.340 [1.461, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.138, 10.100], loss: 0.146514, mae: 0.379993, mean_q: 4.330760
 69600/100000: episode: 1199, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 77.144, mean reward: 2.269 [1.758, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.237, 10.100], loss: 0.144850, mae: 0.368521, mean_q: 4.257271
 69615/100000: episode: 1200, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 45.937, mean reward: 3.062 [2.490, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.651, 10.100], loss: 0.152699, mae: 0.348514, mean_q: 4.244392
 69652/100000: episode: 1201, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 82.033, mean reward: 2.217 [1.654, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.551, 10.181], loss: 0.130276, mae: 0.366240, mean_q: 4.243926
 69698/100000: episode: 1202, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 102.801, mean reward: 2.235 [1.595, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.164, 10.100], loss: 0.138495, mae: 0.372125, mean_q: 4.289837
 69735/100000: episode: 1203, duration: 0.209s, episode steps: 37, steps per second: 177, episode reward: 84.911, mean reward: 2.295 [1.608, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.269, 10.100], loss: 0.121045, mae: 0.339359, mean_q: 4.265180
 69777/100000: episode: 1204, duration: 0.213s, episode steps: 42, steps per second: 197, episode reward: 115.134, mean reward: 2.741 [1.703, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.371, 10.100], loss: 0.137925, mae: 0.362389, mean_q: 4.221993
 69799/100000: episode: 1205, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 86.971, mean reward: 3.953 [2.536, 6.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.637], loss: 0.140985, mae: 0.378183, mean_q: 4.332143
 69845/100000: episode: 1206, duration: 0.243s, episode steps: 46, steps per second: 190, episode reward: 107.482, mean reward: 2.337 [1.556, 4.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.614, 10.100], loss: 0.146693, mae: 0.371414, mean_q: 4.357380
 69887/100000: episode: 1207, duration: 0.222s, episode steps: 42, steps per second: 189, episode reward: 114.921, mean reward: 2.736 [2.046, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-0.682, 10.100], loss: 0.149923, mae: 0.378562, mean_q: 4.329910
 69921/100000: episode: 1208, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 120.332, mean reward: 3.539 [2.199, 5.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.414, 10.100], loss: 0.141919, mae: 0.355723, mean_q: 4.368906
 69963/100000: episode: 1209, duration: 0.221s, episode steps: 42, steps per second: 190, episode reward: 103.486, mean reward: 2.464 [1.849, 3.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.630, 10.100], loss: 0.169402, mae: 0.388791, mean_q: 4.402215
 70009/100000: episode: 1210, duration: 0.256s, episode steps: 46, steps per second: 180, episode reward: 133.367, mean reward: 2.899 [2.062, 4.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.498, 10.100], loss: 0.177704, mae: 0.403234, mean_q: 4.386401
 70055/100000: episode: 1211, duration: 0.250s, episode steps: 46, steps per second: 184, episode reward: 137.976, mean reward: 2.999 [1.717, 9.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.721, 10.100], loss: 0.145964, mae: 0.380237, mean_q: 4.387792
 70097/100000: episode: 1212, duration: 0.215s, episode steps: 42, steps per second: 196, episode reward: 96.344, mean reward: 2.294 [1.755, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.060, 10.100], loss: 0.154693, mae: 0.384414, mean_q: 4.374874
 70143/100000: episode: 1213, duration: 0.244s, episode steps: 46, steps per second: 188, episode reward: 96.415, mean reward: 2.096 [1.544, 3.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.424, 10.100], loss: 0.172749, mae: 0.402833, mean_q: 4.447137
 70161/100000: episode: 1214, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 36.925, mean reward: 2.051 [1.799, 2.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.294, 10.100], loss: 0.157247, mae: 0.386523, mean_q: 4.422316
 70176/100000: episode: 1215, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 39.324, mean reward: 2.622 [2.079, 3.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.428, 10.100], loss: 0.152399, mae: 0.390786, mean_q: 4.369113
 70210/100000: episode: 1216, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 97.923, mean reward: 2.880 [2.057, 4.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.635, 10.100], loss: 0.144690, mae: 0.372756, mean_q: 4.398108
 70232/100000: episode: 1217, duration: 0.119s, episode steps: 22, steps per second: 184, episode reward: 83.230, mean reward: 3.783 [2.647, 5.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.035, 10.529], loss: 0.155243, mae: 0.390203, mean_q: 4.446281
 70274/100000: episode: 1218, duration: 0.226s, episode steps: 42, steps per second: 186, episode reward: 128.099, mean reward: 3.050 [2.062, 7.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-1.002, 10.368], loss: 0.165309, mae: 0.398935, mean_q: 4.480810
 70289/100000: episode: 1219, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 34.989, mean reward: 2.333 [2.048, 2.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.399, 10.100], loss: 0.168240, mae: 0.389891, mean_q: 4.362181
 70331/100000: episode: 1220, duration: 0.211s, episode steps: 42, steps per second: 199, episode reward: 106.233, mean reward: 2.529 [1.838, 4.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.763, 10.100], loss: 0.213148, mae: 0.427855, mean_q: 4.517973
 70353/100000: episode: 1221, duration: 0.117s, episode steps: 22, steps per second: 187, episode reward: 115.044, mean reward: 5.229 [2.773, 8.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.602], loss: 0.180486, mae: 0.417763, mean_q: 4.463973
 70393/100000: episode: 1222, duration: 0.199s, episode steps: 40, steps per second: 201, episode reward: 92.295, mean reward: 2.307 [1.847, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.077, 10.100], loss: 0.178833, mae: 0.403542, mean_q: 4.508016
 70439/100000: episode: 1223, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 107.830, mean reward: 2.344 [1.530, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.048, 10.228], loss: 0.219416, mae: 0.455439, mean_q: 4.489895
 70473/100000: episode: 1224, duration: 0.173s, episode steps: 34, steps per second: 196, episode reward: 77.284, mean reward: 2.273 [1.792, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.873, 10.100], loss: 0.164031, mae: 0.397181, mean_q: 4.469961
 70515/100000: episode: 1225, duration: 0.232s, episode steps: 42, steps per second: 181, episode reward: 137.929, mean reward: 3.284 [1.763, 5.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.116, 10.100], loss: 0.171881, mae: 0.404765, mean_q: 4.488982
 70561/100000: episode: 1226, duration: 0.234s, episode steps: 46, steps per second: 197, episode reward: 98.443, mean reward: 2.140 [1.515, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.572, 10.100], loss: 0.171269, mae: 0.399416, mean_q: 4.553617
 70576/100000: episode: 1227, duration: 0.072s, episode steps: 15, steps per second: 209, episode reward: 40.986, mean reward: 2.732 [2.471, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.598, 10.100], loss: 0.203841, mae: 0.424781, mean_q: 4.487966
 70594/100000: episode: 1228, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 53.480, mean reward: 2.971 [2.222, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.414, 10.100], loss: 0.153108, mae: 0.366350, mean_q: 4.518813
[Info] 2-TH LEVEL FOUND: 6.785133361816406, Considering 10/90 traces
 70634/100000: episode: 1229, duration: 4.287s, episode steps: 40, steps per second: 9, episode reward: 78.819, mean reward: 1.970 [1.555, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.837, 10.100], loss: 0.185654, mae: 0.409215, mean_q: 4.559760
 70662/100000: episode: 1230, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 70.315, mean reward: 2.511 [1.608, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.035, 10.100], loss: 0.140550, mae: 0.378472, mean_q: 4.597083
 70671/100000: episode: 1231, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 29.691, mean reward: 3.299 [2.584, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.901, 10.489], loss: 0.175210, mae: 0.381988, mean_q: 4.375420
 70680/100000: episode: 1232, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 35.842, mean reward: 3.982 [3.351, 5.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.554], loss: 0.198613, mae: 0.432641, mean_q: 4.647239
 70700/100000: episode: 1233, duration: 0.098s, episode steps: 20, steps per second: 204, episode reward: 57.766, mean reward: 2.888 [2.080, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.418], loss: 0.170402, mae: 0.395252, mean_q: 4.549866
 70728/100000: episode: 1234, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 97.046, mean reward: 3.466 [2.296, 9.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.324, 10.100], loss: 0.193458, mae: 0.418402, mean_q: 4.576374
 70748/100000: episode: 1235, duration: 0.115s, episode steps: 20, steps per second: 173, episode reward: 58.682, mean reward: 2.934 [2.148, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.481, 10.474], loss: 0.202937, mae: 0.410649, mean_q: 4.553617
 70776/100000: episode: 1236, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 93.341, mean reward: 3.334 [1.789, 9.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.150, 10.100], loss: 0.227032, mae: 0.434631, mean_q: 4.537201
 70793/100000: episode: 1237, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 51.062, mean reward: 3.004 [2.175, 4.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.463, 10.100], loss: 0.216665, mae: 0.432880, mean_q: 4.594164
 70821/100000: episode: 1238, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 86.332, mean reward: 3.083 [1.930, 5.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.350, 10.100], loss: 0.174182, mae: 0.391344, mean_q: 4.516384
 70852/100000: episode: 1239, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 85.332, mean reward: 2.753 [1.706, 5.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.634, 10.100], loss: 0.199159, mae: 0.411602, mean_q: 4.650630
 70861/100000: episode: 1240, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 35.053, mean reward: 3.895 [3.240, 4.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.409, 10.517], loss: 0.163519, mae: 0.389161, mean_q: 4.583859
 70881/100000: episode: 1241, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 54.326, mean reward: 2.716 [2.155, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.349, 10.363], loss: 0.162336, mae: 0.387396, mean_q: 4.461581
 70910/100000: episode: 1242, duration: 0.181s, episode steps: 29, steps per second: 160, episode reward: 86.466, mean reward: 2.982 [2.045, 5.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.230, 10.100], loss: 0.214507, mae: 0.434493, mean_q: 4.691229
 70939/100000: episode: 1243, duration: 0.164s, episode steps: 29, steps per second: 176, episode reward: 100.906, mean reward: 3.480 [2.463, 8.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.549, 10.100], loss: 0.233898, mae: 0.444791, mean_q: 4.758515
 70970/100000: episode: 1244, duration: 0.169s, episode steps: 31, steps per second: 183, episode reward: 86.043, mean reward: 2.776 [2.035, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.308, 10.100], loss: 0.216600, mae: 0.433099, mean_q: 4.631142
 70999/100000: episode: 1245, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 96.906, mean reward: 3.342 [2.026, 5.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.136, 10.100], loss: 0.186589, mae: 0.406342, mean_q: 4.697564
 71016/100000: episode: 1246, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 49.798, mean reward: 2.929 [2.301, 4.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.413, 10.100], loss: 0.223879, mae: 0.442775, mean_q: 4.630112
 71044/100000: episode: 1247, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 120.780, mean reward: 4.314 [2.787, 6.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.817, 10.100], loss: 0.255203, mae: 0.467399, mean_q: 4.657829
 71073/100000: episode: 1248, duration: 0.150s, episode steps: 29, steps per second: 194, episode reward: 88.469, mean reward: 3.051 [2.103, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.217, 10.100], loss: 0.224185, mae: 0.420035, mean_q: 4.663528
 71101/100000: episode: 1249, duration: 0.142s, episode steps: 28, steps per second: 198, episode reward: 86.769, mean reward: 3.099 [1.807, 5.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.158, 10.100], loss: 0.216137, mae: 0.422273, mean_q: 4.745676
 71110/100000: episode: 1250, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 53.701, mean reward: 5.967 [4.312, 7.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.272, 10.604], loss: 0.174250, mae: 0.391022, mean_q: 4.674815
 71139/100000: episode: 1251, duration: 0.139s, episode steps: 29, steps per second: 208, episode reward: 93.165, mean reward: 3.213 [2.048, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.328, 10.100], loss: 0.267709, mae: 0.458134, mean_q: 4.769661
 71167/100000: episode: 1252, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 87.077, mean reward: 3.110 [2.134, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.593, 10.100], loss: 0.244287, mae: 0.466870, mean_q: 4.834777
 71187/100000: episode: 1253, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 60.351, mean reward: 3.018 [2.278, 4.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.035, 10.430], loss: 0.197415, mae: 0.428105, mean_q: 4.775725
 71218/100000: episode: 1254, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 88.179, mean reward: 2.844 [2.177, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.157, 10.100], loss: 0.247653, mae: 0.445736, mean_q: 4.750833
 71246/100000: episode: 1255, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 119.281, mean reward: 4.260 [2.372, 9.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.320, 10.100], loss: 0.238315, mae: 0.450783, mean_q: 4.824622
 71275/100000: episode: 1256, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 341.554, mean reward: 11.778 [2.835, 62.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.464, 10.100], loss: 0.252565, mae: 0.443302, mean_q: 4.816435
 71303/100000: episode: 1257, duration: 0.138s, episode steps: 28, steps per second: 202, episode reward: 108.375, mean reward: 3.871 [2.521, 7.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.547, 10.100], loss: 0.433175, mae: 0.528082, mean_q: 4.962820
 71331/100000: episode: 1258, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 71.829, mean reward: 2.565 [1.621, 3.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.205, 10.100], loss: 0.348987, mae: 0.507861, mean_q: 4.928711
 71348/100000: episode: 1259, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 70.947, mean reward: 4.173 [2.772, 15.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.338, 10.100], loss: 0.193507, mae: 0.419104, mean_q: 4.849603
 71379/100000: episode: 1260, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 120.867, mean reward: 3.899 [2.441, 7.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-1.744, 10.100], loss: 0.301871, mae: 0.473501, mean_q: 4.917635
 71410/100000: episode: 1261, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 99.173, mean reward: 3.199 [2.052, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.627, 10.100], loss: 0.507194, mae: 0.511010, mean_q: 4.901737
 71438/100000: episode: 1262, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 87.340, mean reward: 3.119 [2.398, 4.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.890, 10.100], loss: 0.705777, mae: 0.572152, mean_q: 4.898301
 71466/100000: episode: 1263, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 82.895, mean reward: 2.961 [1.863, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.944, 10.100], loss: 2.248114, mae: 0.645461, mean_q: 5.015895
 71495/100000: episode: 1264, duration: 0.144s, episode steps: 29, steps per second: 202, episode reward: 123.077, mean reward: 4.244 [2.843, 7.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.416, 10.100], loss: 0.340025, mae: 0.496017, mean_q: 5.000835
 71515/100000: episode: 1265, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 53.224, mean reward: 2.661 [1.854, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.277], loss: 0.653130, mae: 0.566341, mean_q: 5.036458
 71524/100000: episode: 1266, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 36.454, mean reward: 4.050 [3.357, 5.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.582], loss: 0.191107, mae: 0.432685, mean_q: 4.880559
 71552/100000: episode: 1267, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 86.532, mean reward: 3.090 [2.181, 5.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.866, 10.100], loss: 0.301470, mae: 0.513961, mean_q: 5.049389
 71581/100000: episode: 1268, duration: 0.152s, episode steps: 29, steps per second: 191, episode reward: 238.150, mean reward: 8.212 [2.787, 62.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.477, 10.100], loss: 0.322047, mae: 0.488468, mean_q: 5.025307
 71610/100000: episode: 1269, duration: 0.175s, episode steps: 29, steps per second: 166, episode reward: 99.187, mean reward: 3.420 [2.310, 5.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-1.829, 10.100], loss: 0.635713, mae: 0.582615, mean_q: 5.149751
 71641/100000: episode: 1270, duration: 0.171s, episode steps: 31, steps per second: 182, episode reward: 105.942, mean reward: 3.417 [2.347, 4.676], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-1.405, 10.100], loss: 2.602053, mae: 0.671188, mean_q: 5.169801
 71672/100000: episode: 1271, duration: 0.189s, episode steps: 31, steps per second: 164, episode reward: 157.200, mean reward: 5.071 [2.890, 21.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.281, 10.100], loss: 2.477878, mae: 0.713784, mean_q: 5.403110
 71703/100000: episode: 1272, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 79.042, mean reward: 2.550 [1.693, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.237, 10.100], loss: 4.331230, mae: 0.869123, mean_q: 5.339889
 71731/100000: episode: 1273, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 83.458, mean reward: 2.981 [2.304, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.208, 10.100], loss: 0.371008, mae: 0.560986, mean_q: 5.176173
[Info] FALSIFICATION!
[Info] Levels: [5.2262874, 6.7851334, 9.631709]
[Info] Cond. Prob: [0.1, 0.1, 0.15]
[Info] Error Prob: 0.0015000000000000002

 71746/100000: episode: 1274, duration: 4.309s, episode steps: 15, steps per second: 3, episode reward: 143.354, mean reward: 9.557 [2.505, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.895, 9.695], loss: 5.164458, mae: 0.908932, mean_q: 5.491021
 71846/100000: episode: 1275, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 201.256, mean reward: 2.013 [1.459, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.021, 10.098], loss: 2.373924, mae: 0.669055, mean_q: 5.259377
 71946/100000: episode: 1276, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 197.080, mean reward: 1.971 [1.468, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.220, 10.214], loss: 0.324393, mae: 0.535959, mean_q: 5.161533
 72046/100000: episode: 1277, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 208.232, mean reward: 2.082 [1.447, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.110, 10.098], loss: 3.193206, mae: 0.772958, mean_q: 5.283385
 72146/100000: episode: 1278, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.296, mean reward: 1.893 [1.459, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.436, 10.098], loss: 4.443340, mae: 0.730222, mean_q: 5.223030
 72246/100000: episode: 1279, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.521, mean reward: 1.885 [1.455, 4.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.000, 10.098], loss: 1.034544, mae: 0.614674, mean_q: 5.212548
 72346/100000: episode: 1280, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 193.833, mean reward: 1.938 [1.476, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.207, 10.236], loss: 1.916422, mae: 0.706051, mean_q: 5.266015
 72446/100000: episode: 1281, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 203.617, mean reward: 2.036 [1.436, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.118, 10.166], loss: 1.971364, mae: 0.634516, mean_q: 5.219690
 72546/100000: episode: 1282, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 194.338, mean reward: 1.943 [1.487, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.840, 10.195], loss: 1.142657, mae: 0.614861, mean_q: 5.267274
 72646/100000: episode: 1283, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 192.167, mean reward: 1.922 [1.504, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.902, 10.108], loss: 2.831177, mae: 0.701032, mean_q: 5.244497
 72746/100000: episode: 1284, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 203.582, mean reward: 2.036 [1.487, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.500, 10.416], loss: 4.286670, mae: 0.757222, mean_q: 5.258244
 72846/100000: episode: 1285, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 194.332, mean reward: 1.943 [1.498, 4.513], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.235, 10.098], loss: 3.379704, mae: 0.682045, mean_q: 5.265998
 72946/100000: episode: 1286, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 205.666, mean reward: 2.057 [1.462, 6.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.979, 10.098], loss: 1.145219, mae: 0.576676, mean_q: 5.143090
 73046/100000: episode: 1287, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 193.134, mean reward: 1.931 [1.472, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.446, 10.290], loss: 0.405477, mae: 0.510020, mean_q: 5.029040
 73146/100000: episode: 1288, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.793, mean reward: 1.888 [1.471, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.456, 10.098], loss: 1.266154, mae: 0.580788, mean_q: 5.092957
 73246/100000: episode: 1289, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 195.082, mean reward: 1.951 [1.457, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.930, 10.171], loss: 0.695078, mae: 0.575983, mean_q: 5.071402
 73346/100000: episode: 1290, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 186.594, mean reward: 1.866 [1.455, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.943, 10.098], loss: 4.142034, mae: 0.804643, mean_q: 5.203369
 73446/100000: episode: 1291, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 180.605, mean reward: 1.806 [1.470, 2.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.359, 10.298], loss: 3.102842, mae: 0.686850, mean_q: 5.082396
 73546/100000: episode: 1292, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 197.119, mean reward: 1.971 [1.478, 4.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.145, 10.178], loss: 1.044809, mae: 0.586117, mean_q: 5.034657
 73646/100000: episode: 1293, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 180.611, mean reward: 1.806 [1.439, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.367, 10.098], loss: 1.742782, mae: 0.608956, mean_q: 5.071665
 73746/100000: episode: 1294, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 184.544, mean reward: 1.845 [1.467, 3.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.148, 10.250], loss: 4.399172, mae: 0.804549, mean_q: 5.088634
 73846/100000: episode: 1295, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 183.406, mean reward: 1.834 [1.444, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.142, 10.164], loss: 2.106847, mae: 0.691027, mean_q: 5.021495
 73946/100000: episode: 1296, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 193.765, mean reward: 1.938 [1.451, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.191, 10.154], loss: 4.139060, mae: 0.758194, mean_q: 5.092689
 74046/100000: episode: 1297, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 194.428, mean reward: 1.944 [1.461, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.248, 10.098], loss: 3.460679, mae: 0.713707, mean_q: 5.034721
 74146/100000: episode: 1298, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 205.815, mean reward: 2.058 [1.467, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-2.157, 10.098], loss: 1.203209, mae: 0.594408, mean_q: 4.936934
 74246/100000: episode: 1299, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 183.580, mean reward: 1.836 [1.451, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.851, 10.098], loss: 1.806920, mae: 0.635201, mean_q: 4.923502
 74346/100000: episode: 1300, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 195.373, mean reward: 1.954 [1.476, 4.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.321, 10.098], loss: 2.974099, mae: 0.621106, mean_q: 4.892432
 74446/100000: episode: 1301, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 197.276, mean reward: 1.973 [1.458, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.678, 10.098], loss: 1.428823, mae: 0.545706, mean_q: 4.833234
 74546/100000: episode: 1302, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 188.984, mean reward: 1.890 [1.462, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.021, 10.120], loss: 1.386695, mae: 0.589427, mean_q: 4.854134
 74646/100000: episode: 1303, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 189.993, mean reward: 1.900 [1.461, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.751, 10.098], loss: 2.383741, mae: 0.651683, mean_q: 4.860752
 74746/100000: episode: 1304, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 192.473, mean reward: 1.925 [1.453, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.646, 10.149], loss: 2.423580, mae: 0.630165, mean_q: 4.771155
 74846/100000: episode: 1305, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.743, mean reward: 1.897 [1.457, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.782, 10.222], loss: 1.831721, mae: 0.553415, mean_q: 4.753647
 74946/100000: episode: 1306, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 240.013, mean reward: 2.400 [1.516, 8.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.981, 10.335], loss: 1.248470, mae: 0.574096, mean_q: 4.751057
 75046/100000: episode: 1307, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 191.227, mean reward: 1.912 [1.469, 4.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.861, 10.099], loss: 2.697512, mae: 0.621444, mean_q: 4.776946
 75146/100000: episode: 1308, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 215.968, mean reward: 2.160 [1.466, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.257, 10.325], loss: 1.385839, mae: 0.539707, mean_q: 4.654486
 75246/100000: episode: 1309, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 184.079, mean reward: 1.841 [1.490, 2.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.577, 10.234], loss: 2.112348, mae: 0.549498, mean_q: 4.561157
 75346/100000: episode: 1310, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 191.764, mean reward: 1.918 [1.440, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.582, 10.139], loss: 1.556275, mae: 0.567592, mean_q: 4.631848
 75446/100000: episode: 1311, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.101, mean reward: 1.961 [1.491, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.360, 10.372], loss: 2.929828, mae: 0.580218, mean_q: 4.603117
 75546/100000: episode: 1312, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 181.832, mean reward: 1.818 [1.456, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.287, 10.166], loss: 1.032059, mae: 0.482500, mean_q: 4.541114
 75646/100000: episode: 1313, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 194.401, mean reward: 1.944 [1.499, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.959, 10.304], loss: 1.210858, mae: 0.531901, mean_q: 4.586214
 75746/100000: episode: 1314, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.117, mean reward: 1.971 [1.440, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.828, 10.224], loss: 0.659854, mae: 0.493435, mean_q: 4.561508
 75846/100000: episode: 1315, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 191.964, mean reward: 1.920 [1.452, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.757, 10.098], loss: 1.140023, mae: 0.481749, mean_q: 4.464809
 75946/100000: episode: 1316, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 189.356, mean reward: 1.894 [1.466, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.725, 10.205], loss: 2.236773, mae: 0.509414, mean_q: 4.401489
 76046/100000: episode: 1317, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 196.135, mean reward: 1.961 [1.458, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.871, 10.098], loss: 0.865979, mae: 0.447573, mean_q: 4.353293
 76146/100000: episode: 1318, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.352, mean reward: 1.974 [1.493, 5.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.052, 10.315], loss: 1.032297, mae: 0.438638, mean_q: 4.295477
 76246/100000: episode: 1319, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 204.727, mean reward: 2.047 [1.489, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.847, 10.098], loss: 0.405578, mae: 0.400753, mean_q: 4.249685
 76346/100000: episode: 1320, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 181.536, mean reward: 1.815 [1.446, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.487, 10.098], loss: 0.299629, mae: 0.351220, mean_q: 4.077775
 76446/100000: episode: 1321, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 236.495, mean reward: 2.365 [1.507, 4.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.268, 10.098], loss: 0.118842, mae: 0.329911, mean_q: 4.013077
 76546/100000: episode: 1322, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 198.156, mean reward: 1.982 [1.459, 5.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.303, 10.121], loss: 0.324482, mae: 0.355725, mean_q: 4.001479
 76646/100000: episode: 1323, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 193.933, mean reward: 1.939 [1.491, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.777, 10.098], loss: 1.370791, mae: 0.338745, mean_q: 3.921754
 76746/100000: episode: 1324, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.837, mean reward: 1.888 [1.467, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.918, 10.098], loss: 0.176208, mae: 0.375563, mean_q: 3.886523
 76846/100000: episode: 1325, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.920, mean reward: 1.889 [1.463, 4.587], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.224, 10.111], loss: 0.095940, mae: 0.309253, mean_q: 3.864852
 76946/100000: episode: 1326, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 180.321, mean reward: 1.803 [1.453, 2.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.983, 10.149], loss: 0.091326, mae: 0.304892, mean_q: 3.852547
 77046/100000: episode: 1327, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 224.835, mean reward: 2.248 [1.567, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.115, 10.098], loss: 0.103269, mae: 0.313226, mean_q: 3.852625
 77146/100000: episode: 1328, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 182.567, mean reward: 1.826 [1.476, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.518, 10.216], loss: 0.108513, mae: 0.309465, mean_q: 3.863636
 77246/100000: episode: 1329, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 184.816, mean reward: 1.848 [1.476, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.417, 10.137], loss: 0.094135, mae: 0.300560, mean_q: 3.845378
 77346/100000: episode: 1330, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.903, mean reward: 1.899 [1.445, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.058, 10.098], loss: 0.094326, mae: 0.308032, mean_q: 3.846400
 77446/100000: episode: 1331, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 200.533, mean reward: 2.005 [1.470, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.071, 10.228], loss: 0.093004, mae: 0.304170, mean_q: 3.857579
 77546/100000: episode: 1332, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 185.155, mean reward: 1.852 [1.462, 5.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.885, 10.222], loss: 0.109774, mae: 0.323194, mean_q: 3.863688
 77646/100000: episode: 1333, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.809, mean reward: 1.918 [1.480, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.472, 10.098], loss: 0.089246, mae: 0.294675, mean_q: 3.835648
 77746/100000: episode: 1334, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 181.354, mean reward: 1.814 [1.450, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.774, 10.098], loss: 0.103010, mae: 0.310745, mean_q: 3.846973
 77846/100000: episode: 1335, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 200.887, mean reward: 2.009 [1.453, 4.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.926, 10.131], loss: 0.094320, mae: 0.308377, mean_q: 3.840192
 77946/100000: episode: 1336, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 182.145, mean reward: 1.821 [1.437, 2.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.955, 10.128], loss: 0.089424, mae: 0.290654, mean_q: 3.816072
 78046/100000: episode: 1337, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 209.503, mean reward: 2.095 [1.476, 3.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.296, 10.345], loss: 0.100638, mae: 0.301537, mean_q: 3.835569
 78146/100000: episode: 1338, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 202.051, mean reward: 2.021 [1.495, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.226, 10.098], loss: 0.097858, mae: 0.309460, mean_q: 3.846243
 78246/100000: episode: 1339, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 193.505, mean reward: 1.935 [1.464, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.104, 10.242], loss: 0.089948, mae: 0.302499, mean_q: 3.849195
 78346/100000: episode: 1340, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.061, mean reward: 1.961 [1.478, 7.818], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.757, 10.098], loss: 0.092882, mae: 0.306418, mean_q: 3.853364
 78446/100000: episode: 1341, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 191.782, mean reward: 1.918 [1.494, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.496, 10.256], loss: 0.110879, mae: 0.318151, mean_q: 3.861881
 78546/100000: episode: 1342, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: 196.535, mean reward: 1.965 [1.438, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.486, 10.098], loss: 0.113358, mae: 0.310383, mean_q: 3.845090
 78646/100000: episode: 1343, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.726, mean reward: 1.827 [1.434, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.909, 10.098], loss: 0.100396, mae: 0.307200, mean_q: 3.853566
 78746/100000: episode: 1344, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 196.330, mean reward: 1.963 [1.463, 3.705], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.993, 10.098], loss: 0.099754, mae: 0.306816, mean_q: 3.847212
 78846/100000: episode: 1345, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 200.702, mean reward: 2.007 [1.488, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.351, 10.299], loss: 0.106538, mae: 0.311381, mean_q: 3.869696
 78946/100000: episode: 1346, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 213.651, mean reward: 2.137 [1.474, 4.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.841, 10.098], loss: 0.111650, mae: 0.314444, mean_q: 3.859865
 79046/100000: episode: 1347, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 177.818, mean reward: 1.778 [1.460, 2.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.365, 10.098], loss: 0.097798, mae: 0.304791, mean_q: 3.867620
 79146/100000: episode: 1348, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 192.705, mean reward: 1.927 [1.463, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.628, 10.295], loss: 0.114031, mae: 0.312812, mean_q: 3.880297
 79246/100000: episode: 1349, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 212.863, mean reward: 2.129 [1.548, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.616, 10.098], loss: 0.118715, mae: 0.322564, mean_q: 3.876770
 79346/100000: episode: 1350, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 289.809, mean reward: 2.898 [1.509, 6.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.237, 10.488], loss: 0.097657, mae: 0.304756, mean_q: 3.871168
 79446/100000: episode: 1351, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 187.757, mean reward: 1.878 [1.456, 3.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.168, 10.230], loss: 0.091906, mae: 0.303816, mean_q: 3.885963
 79546/100000: episode: 1352, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 207.279, mean reward: 2.073 [1.452, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.094, 10.419], loss: 0.101383, mae: 0.313979, mean_q: 3.901520
 79646/100000: episode: 1353, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 193.093, mean reward: 1.931 [1.442, 3.624], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.343, 10.098], loss: 0.107989, mae: 0.323739, mean_q: 3.904721
 79746/100000: episode: 1354, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 182.497, mean reward: 1.825 [1.452, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.570, 10.098], loss: 0.084708, mae: 0.299624, mean_q: 3.902010
 79846/100000: episode: 1355, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 206.415, mean reward: 2.064 [1.481, 2.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.114, 10.098], loss: 0.107569, mae: 0.319347, mean_q: 3.898627
 79946/100000: episode: 1356, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 206.928, mean reward: 2.069 [1.480, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.537, 10.098], loss: 0.096456, mae: 0.306971, mean_q: 3.910068
 80046/100000: episode: 1357, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.864, mean reward: 1.889 [1.513, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.150, 10.147], loss: 0.088017, mae: 0.297136, mean_q: 3.878078
 80146/100000: episode: 1358, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 195.597, mean reward: 1.956 [1.452, 4.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.730, 10.098], loss: 0.098252, mae: 0.307851, mean_q: 3.881781
 80246/100000: episode: 1359, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 179.953, mean reward: 1.800 [1.433, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.793, 10.131], loss: 0.093609, mae: 0.308138, mean_q: 3.883044
 80346/100000: episode: 1360, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 200.319, mean reward: 2.003 [1.487, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.762, 10.098], loss: 0.093359, mae: 0.306206, mean_q: 3.890895
 80446/100000: episode: 1361, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 217.662, mean reward: 2.177 [1.505, 5.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.757, 10.473], loss: 0.093580, mae: 0.301582, mean_q: 3.894369
 80546/100000: episode: 1362, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 214.657, mean reward: 2.147 [1.547, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.153, 10.098], loss: 0.090934, mae: 0.293754, mean_q: 3.880623
 80646/100000: episode: 1363, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 236.573, mean reward: 2.366 [1.491, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.737, 10.098], loss: 0.086279, mae: 0.291757, mean_q: 3.914414
 80746/100000: episode: 1364, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 195.947, mean reward: 1.959 [1.436, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.512, 10.158], loss: 0.089134, mae: 0.300521, mean_q: 3.922832
 80846/100000: episode: 1365, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 188.264, mean reward: 1.883 [1.439, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.776, 10.098], loss: 0.087661, mae: 0.297251, mean_q: 3.933546
 80946/100000: episode: 1366, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 180.444, mean reward: 1.804 [1.448, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.050, 10.186], loss: 0.107496, mae: 0.312263, mean_q: 3.953147
 81046/100000: episode: 1367, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 190.497, mean reward: 1.905 [1.480, 4.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.093, 10.098], loss: 0.086198, mae: 0.297315, mean_q: 3.925817
 81146/100000: episode: 1368, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 204.075, mean reward: 2.041 [1.484, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.012, 10.098], loss: 0.095499, mae: 0.302491, mean_q: 3.923523
 81246/100000: episode: 1369, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 188.337, mean reward: 1.883 [1.477, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.496, 10.211], loss: 0.088184, mae: 0.297546, mean_q: 3.899240
 81346/100000: episode: 1370, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.467, mean reward: 1.825 [1.444, 2.690], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.819, 10.116], loss: 0.098535, mae: 0.305350, mean_q: 3.916744
 81446/100000: episode: 1371, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 190.361, mean reward: 1.904 [1.461, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.443, 10.098], loss: 0.087441, mae: 0.296915, mean_q: 3.898566
 81546/100000: episode: 1372, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 210.420, mean reward: 2.104 [1.476, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.890, 10.098], loss: 0.089491, mae: 0.297237, mean_q: 3.913013
 81646/100000: episode: 1373, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 186.289, mean reward: 1.863 [1.442, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.221, 10.098], loss: 0.092644, mae: 0.300001, mean_q: 3.879618
[Info] 1-TH LEVEL FOUND: 5.953490734100342, Considering 10/90 traces
 81746/100000: episode: 1374, duration: 4.585s, episode steps: 100, steps per second: 22, episode reward: 193.250, mean reward: 1.932 [1.463, 4.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.759, 10.322], loss: 0.089373, mae: 0.304212, mean_q: 3.912426
 81760/100000: episode: 1375, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 33.611, mean reward: 2.401 [2.015, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.168, 10.100], loss: 0.074561, mae: 0.266926, mean_q: 3.848366
 81801/100000: episode: 1376, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 148.696, mean reward: 3.627 [2.717, 5.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.224, 10.100], loss: 0.081212, mae: 0.286881, mean_q: 3.900604
 81807/100000: episode: 1377, duration: 0.031s, episode steps: 6, steps per second: 193, episode reward: 15.201, mean reward: 2.533 [2.220, 2.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.307, 10.100], loss: 0.082841, mae: 0.293451, mean_q: 3.938981
 81826/100000: episode: 1378, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 42.555, mean reward: 2.240 [1.869, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.127, 10.100], loss: 0.095665, mae: 0.303857, mean_q: 3.935214
 81916/100000: episode: 1379, duration: 0.459s, episode steps: 90, steps per second: 196, episode reward: 173.117, mean reward: 1.924 [1.466, 4.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.555 [-0.579, 10.134], loss: 0.100985, mae: 0.305785, mean_q: 3.932275
 81957/100000: episode: 1380, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 138.862, mean reward: 3.387 [1.965, 6.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.287, 10.100], loss: 0.090578, mae: 0.296432, mean_q: 3.959959
 81971/100000: episode: 1381, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 43.934, mean reward: 3.138 [2.487, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.469], loss: 0.083132, mae: 0.303147, mean_q: 3.933534
 82000/100000: episode: 1382, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 110.556, mean reward: 3.812 [2.531, 5.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.666, 10.100], loss: 0.102875, mae: 0.312591, mean_q: 3.985361
 82006/100000: episode: 1383, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 20.180, mean reward: 3.363 [2.956, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.268, 10.100], loss: 0.056558, mae: 0.243671, mean_q: 3.992800
 82020/100000: episode: 1384, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 49.781, mean reward: 3.556 [2.409, 4.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.232, 10.445], loss: 0.121336, mae: 0.346646, mean_q: 3.983572
 82034/100000: episode: 1385, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 36.223, mean reward: 2.587 [2.309, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.541, 10.100], loss: 0.084894, mae: 0.286598, mean_q: 3.863432
 82053/100000: episode: 1386, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 45.345, mean reward: 2.387 [1.945, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.241, 10.100], loss: 0.117011, mae: 0.350110, mean_q: 4.157240
 82094/100000: episode: 1387, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 89.704, mean reward: 2.188 [1.597, 6.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.415, 10.129], loss: 0.091652, mae: 0.311666, mean_q: 3.988144
 82113/100000: episode: 1388, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 55.026, mean reward: 2.896 [2.270, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.962, 10.100], loss: 0.101598, mae: 0.304528, mean_q: 3.993285
 82127/100000: episode: 1389, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 31.493, mean reward: 2.249 [1.660, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.075, 10.100], loss: 0.086872, mae: 0.296501, mean_q: 4.041567
 82141/100000: episode: 1390, duration: 0.081s, episode steps: 14, steps per second: 174, episode reward: 36.306, mean reward: 2.593 [2.022, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.153, 10.100], loss: 0.104550, mae: 0.312970, mean_q: 3.932121
 82155/100000: episode: 1391, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 27.971, mean reward: 1.998 [1.671, 2.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.819, 10.307], loss: 0.112533, mae: 0.326770, mean_q: 4.090996
 82196/100000: episode: 1392, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 160.708, mean reward: 3.920 [2.049, 9.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.389, 10.100], loss: 0.104576, mae: 0.321459, mean_q: 4.007795
 82286/100000: episode: 1393, duration: 0.475s, episode steps: 90, steps per second: 189, episode reward: 168.321, mean reward: 1.870 [1.466, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.557 [-0.807, 10.100], loss: 0.108117, mae: 0.320396, mean_q: 4.026030
 82300/100000: episode: 1394, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 38.486, mean reward: 2.749 [2.428, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.121, 10.100], loss: 0.107137, mae: 0.327364, mean_q: 4.086151
 82329/100000: episode: 1395, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 89.684, mean reward: 3.093 [2.010, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.456, 10.100], loss: 0.107852, mae: 0.325991, mean_q: 4.025568
 82348/100000: episode: 1396, duration: 0.095s, episode steps: 19, steps per second: 200, episode reward: 46.664, mean reward: 2.456 [1.738, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.411, 10.100], loss: 0.112120, mae: 0.332291, mean_q: 4.075449
 82362/100000: episode: 1397, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 35.821, mean reward: 2.559 [1.975, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.168, 10.100], loss: 0.094661, mae: 0.315420, mean_q: 4.034760
 82403/100000: episode: 1398, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 148.236, mean reward: 3.616 [2.392, 7.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.776, 10.100], loss: 0.131143, mae: 0.347924, mean_q: 4.104908
 82430/100000: episode: 1399, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 63.195, mean reward: 2.341 [1.984, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.509, 10.100], loss: 0.129623, mae: 0.324765, mean_q: 4.087645
 82457/100000: episode: 1400, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 62.556, mean reward: 2.317 [1.554, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.100, 10.125], loss: 0.126615, mae: 0.336281, mean_q: 4.127501
 82547/100000: episode: 1401, duration: 0.454s, episode steps: 90, steps per second: 198, episode reward: 182.116, mean reward: 2.024 [1.461, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.555 [-1.702, 10.100], loss: 0.107714, mae: 0.326268, mean_q: 4.144684
 82588/100000: episode: 1402, duration: 0.214s, episode steps: 41, steps per second: 191, episode reward: 248.776, mean reward: 6.068 [2.938, 13.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.829, 10.100], loss: 0.129515, mae: 0.325438, mean_q: 4.118487
 82678/100000: episode: 1403, duration: 0.454s, episode steps: 90, steps per second: 198, episode reward: 194.252, mean reward: 2.158 [1.475, 5.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.559 [-0.738, 10.118], loss: 0.152448, mae: 0.359098, mean_q: 4.143072
 82692/100000: episode: 1404, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 55.725, mean reward: 3.980 [2.305, 9.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.185, 10.100], loss: 0.157892, mae: 0.361420, mean_q: 4.168715
 82733/100000: episode: 1405, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 111.018, mean reward: 2.708 [1.520, 4.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.883, 10.162], loss: 0.231629, mae: 0.408672, mean_q: 4.232068
 82747/100000: episode: 1406, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 37.662, mean reward: 2.690 [1.947, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.633, 10.100], loss: 0.192719, mae: 0.364600, mean_q: 4.111215
 82766/100000: episode: 1407, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 45.066, mean reward: 2.372 [1.737, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.106, 10.100], loss: 0.173429, mae: 0.379689, mean_q: 4.245016
 82795/100000: episode: 1408, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 75.905, mean reward: 2.617 [1.809, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.398, 10.100], loss: 0.240893, mae: 0.428717, mean_q: 4.230222
 82809/100000: episode: 1409, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 37.570, mean reward: 2.684 [1.951, 3.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.390], loss: 0.221193, mae: 0.443774, mean_q: 4.353748
 82836/100000: episode: 1410, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 66.772, mean reward: 2.473 [1.688, 4.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.100, 10.100], loss: 0.196949, mae: 0.376832, mean_q: 4.176366
 82850/100000: episode: 1411, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 38.652, mean reward: 2.761 [2.268, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.382, 10.354], loss: 0.263553, mae: 0.392860, mean_q: 4.254722
 82856/100000: episode: 1412, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 18.468, mean reward: 3.078 [2.562, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.482, 10.100], loss: 0.145062, mae: 0.370377, mean_q: 4.209092
 82875/100000: episode: 1413, duration: 0.100s, episode steps: 19, steps per second: 191, episode reward: 55.548, mean reward: 2.924 [2.440, 3.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.633, 10.100], loss: 0.175451, mae: 0.370308, mean_q: 4.189918
 82902/100000: episode: 1414, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 86.973, mean reward: 3.221 [2.445, 5.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.207, 10.100], loss: 0.138657, mae: 0.341384, mean_q: 4.197546
 82943/100000: episode: 1415, duration: 0.213s, episode steps: 41, steps per second: 193, episode reward: 129.050, mean reward: 3.148 [1.566, 6.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.523, 10.169], loss: 0.190116, mae: 0.385861, mean_q: 4.300067
 82970/100000: episode: 1416, duration: 0.131s, episode steps: 27, steps per second: 206, episode reward: 74.564, mean reward: 2.762 [2.009, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.427, 10.100], loss: 0.156785, mae: 0.358456, mean_q: 4.203086
 82997/100000: episode: 1417, duration: 0.128s, episode steps: 27, steps per second: 211, episode reward: 88.950, mean reward: 3.294 [2.375, 4.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.318, 10.100], loss: 0.208993, mae: 0.406289, mean_q: 4.296014
 83024/100000: episode: 1418, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 58.877, mean reward: 2.181 [1.765, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.218, 10.100], loss: 0.152618, mae: 0.356460, mean_q: 4.181309
 83030/100000: episode: 1419, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 20.163, mean reward: 3.360 [2.895, 4.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.618, 10.100], loss: 0.149636, mae: 0.364309, mean_q: 4.357004
 83071/100000: episode: 1420, duration: 0.215s, episode steps: 41, steps per second: 191, episode reward: 120.875, mean reward: 2.948 [1.773, 7.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.260, 10.100], loss: 0.200945, mae: 0.399087, mean_q: 4.322752
 83085/100000: episode: 1421, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 40.932, mean reward: 2.924 [1.919, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.449], loss: 0.178657, mae: 0.393078, mean_q: 4.382009
 83126/100000: episode: 1422, duration: 0.227s, episode steps: 41, steps per second: 181, episode reward: 156.233, mean reward: 3.811 [2.453, 10.333], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.545, 10.100], loss: 0.236235, mae: 0.407178, mean_q: 4.374193
 83167/100000: episode: 1423, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 86.359, mean reward: 2.106 [1.660, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.568, 10.291], loss: 0.150777, mae: 0.386085, mean_q: 4.297681
 83194/100000: episode: 1424, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 65.982, mean reward: 2.444 [1.818, 3.725], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.853, 10.100], loss: 0.166090, mae: 0.385456, mean_q: 4.410062
 83208/100000: episode: 1425, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 37.636, mean reward: 2.688 [2.198, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.429], loss: 0.237257, mae: 0.448055, mean_q: 4.416930
 83298/100000: episode: 1426, duration: 0.444s, episode steps: 90, steps per second: 203, episode reward: 195.663, mean reward: 2.174 [1.523, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.542 [-0.799, 10.363], loss: 0.185592, mae: 0.396585, mean_q: 4.351833
 83304/100000: episode: 1427, duration: 0.050s, episode steps: 6, steps per second: 121, episode reward: 19.820, mean reward: 3.303 [2.788, 4.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.727, 10.100], loss: 0.144182, mae: 0.381525, mean_q: 4.225459
 83323/100000: episode: 1428, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 49.599, mean reward: 2.610 [2.182, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.902, 10.100], loss: 0.130194, mae: 0.353063, mean_q: 4.295565
 83364/100000: episode: 1429, duration: 0.214s, episode steps: 41, steps per second: 191, episode reward: 201.226, mean reward: 4.908 [2.682, 8.945], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.609, 10.100], loss: 0.229475, mae: 0.412711, mean_q: 4.391345
 83391/100000: episode: 1430, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 73.528, mean reward: 2.723 [1.560, 5.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.409, 10.150], loss: 0.161144, mae: 0.384179, mean_q: 4.377232
 83481/100000: episode: 1431, duration: 0.475s, episode steps: 90, steps per second: 189, episode reward: 194.540, mean reward: 2.162 [1.465, 4.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.551 [-1.310, 10.181], loss: 0.242332, mae: 0.427542, mean_q: 4.459396
 83508/100000: episode: 1432, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 117.687, mean reward: 4.359 [2.630, 7.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.424, 10.100], loss: 0.196393, mae: 0.396337, mean_q: 4.399208
 83514/100000: episode: 1433, duration: 0.031s, episode steps: 6, steps per second: 193, episode reward: 17.007, mean reward: 2.834 [2.561, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.394, 10.100], loss: 0.231574, mae: 0.444420, mean_q: 4.451578
 83604/100000: episode: 1434, duration: 0.447s, episode steps: 90, steps per second: 201, episode reward: 183.789, mean reward: 2.042 [1.447, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.552 [-0.978, 10.100], loss: 0.190152, mae: 0.408731, mean_q: 4.470282
 83656/100000: episode: 1435, duration: 0.254s, episode steps: 52, steps per second: 204, episode reward: 178.588, mean reward: 3.434 [2.269, 6.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-0.842, 10.464], loss: 0.220541, mae: 0.417910, mean_q: 4.436140
 83697/100000: episode: 1436, duration: 0.205s, episode steps: 41, steps per second: 200, episode reward: 124.059, mean reward: 3.026 [1.959, 5.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.905, 10.100], loss: 0.200432, mae: 0.398259, mean_q: 4.500806
 83716/100000: episode: 1437, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 53.806, mean reward: 2.832 [2.279, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.005, 10.100], loss: 0.157392, mae: 0.396780, mean_q: 4.423660
 83722/100000: episode: 1438, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 20.329, mean reward: 3.388 [2.615, 4.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.415, 10.100], loss: 0.106182, mae: 0.334158, mean_q: 4.212422
 83741/100000: episode: 1439, duration: 0.093s, episode steps: 19, steps per second: 204, episode reward: 40.900, mean reward: 2.153 [1.760, 2.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.376, 10.100], loss: 0.183649, mae: 0.414761, mean_q: 4.510553
 83747/100000: episode: 1440, duration: 0.040s, episode steps: 6, steps per second: 148, episode reward: 18.985, mean reward: 3.164 [2.700, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.406, 10.100], loss: 0.185524, mae: 0.404114, mean_q: 4.521286
 83753/100000: episode: 1441, duration: 0.032s, episode steps: 6, steps per second: 190, episode reward: 16.550, mean reward: 2.758 [2.589, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.441, 10.100], loss: 0.163662, mae: 0.374397, mean_q: 4.492376
 83805/100000: episode: 1442, duration: 0.276s, episode steps: 52, steps per second: 189, episode reward: 140.755, mean reward: 2.707 [1.797, 4.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-1.125, 10.283], loss: 0.166951, mae: 0.392096, mean_q: 4.433205
 83846/100000: episode: 1443, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 118.529, mean reward: 2.891 [1.979, 9.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.258, 10.412], loss: 0.186696, mae: 0.414520, mean_q: 4.493678
 83852/100000: episode: 1444, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 18.316, mean reward: 3.053 [2.792, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.801, 10.100], loss: 0.166628, mae: 0.407573, mean_q: 4.579663
 83942/100000: episode: 1445, duration: 0.452s, episode steps: 90, steps per second: 199, episode reward: 175.821, mean reward: 1.954 [1.439, 4.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.550 [-0.298, 10.211], loss: 0.242964, mae: 0.449820, mean_q: 4.498184
 83994/100000: episode: 1446, duration: 0.268s, episode steps: 52, steps per second: 194, episode reward: 191.919, mean reward: 3.691 [1.835, 8.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.492, 10.289], loss: 0.256941, mae: 0.457210, mean_q: 4.601334
 84035/100000: episode: 1447, duration: 0.219s, episode steps: 41, steps per second: 188, episode reward: 126.849, mean reward: 3.094 [1.742, 6.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.653, 10.100], loss: 0.258781, mae: 0.470337, mean_q: 4.596491
 84041/100000: episode: 1448, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 20.072, mean reward: 3.345 [2.912, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.450, 10.100], loss: 0.370421, mae: 0.472589, mean_q: 4.772420
 84055/100000: episode: 1449, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 42.794, mean reward: 3.057 [2.540, 4.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.712, 10.570], loss: 0.196739, mae: 0.422561, mean_q: 4.407172
 84145/100000: episode: 1450, duration: 0.470s, episode steps: 90, steps per second: 191, episode reward: 179.511, mean reward: 1.995 [1.452, 4.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-1.894, 10.105], loss: 0.225204, mae: 0.442488, mean_q: 4.565688
 84186/100000: episode: 1451, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 96.829, mean reward: 2.362 [1.517, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.484, 10.122], loss: 0.223561, mae: 0.447194, mean_q: 4.645658
 84238/100000: episode: 1452, duration: 0.258s, episode steps: 52, steps per second: 201, episode reward: 126.872, mean reward: 2.440 [1.628, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.882 [-0.431, 10.361], loss: 0.273227, mae: 0.440886, mean_q: 4.614533
 84328/100000: episode: 1453, duration: 0.476s, episode steps: 90, steps per second: 189, episode reward: 172.923, mean reward: 1.921 [1.472, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.549 [-0.677, 10.100], loss: 0.257876, mae: 0.452881, mean_q: 4.657066
 84342/100000: episode: 1454, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 53.832, mean reward: 3.845 [2.803, 7.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.266, 10.410], loss: 0.203324, mae: 0.390543, mean_q: 4.638771
 84356/100000: episode: 1455, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 35.324, mean reward: 2.523 [1.839, 3.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.165, 10.100], loss: 0.399587, mae: 0.527214, mean_q: 4.733944
 84408/100000: episode: 1456, duration: 0.264s, episode steps: 52, steps per second: 197, episode reward: 140.842, mean reward: 2.708 [1.775, 17.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.428, 10.390], loss: 0.250953, mae: 0.447394, mean_q: 4.589065
 84435/100000: episode: 1457, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 78.082, mean reward: 2.892 [2.073, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.166, 10.100], loss: 0.406546, mae: 0.444069, mean_q: 4.612130
 84464/100000: episode: 1458, duration: 0.167s, episode steps: 29, steps per second: 174, episode reward: 89.868, mean reward: 3.099 [2.178, 4.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-1.067, 10.100], loss: 0.331125, mae: 0.517137, mean_q: 4.677899
 84554/100000: episode: 1459, duration: 0.460s, episode steps: 90, steps per second: 196, episode reward: 162.643, mean reward: 1.807 [1.450, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.552 [-1.017, 10.100], loss: 0.245778, mae: 0.446229, mean_q: 4.618320
 84568/100000: episode: 1460, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 41.409, mean reward: 2.958 [2.155, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.640, 10.100], loss: 0.304064, mae: 0.475848, mean_q: 4.722111
 84587/100000: episode: 1461, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 53.622, mean reward: 2.822 [2.339, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.366, 10.100], loss: 0.229557, mae: 0.455954, mean_q: 4.683722
 84601/100000: episode: 1462, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 38.794, mean reward: 2.771 [2.034, 4.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.211, 10.100], loss: 0.188011, mae: 0.419806, mean_q: 4.623736
 84642/100000: episode: 1463, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 131.182, mean reward: 3.200 [1.604, 4.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.175, 10.236], loss: 0.211087, mae: 0.427608, mean_q: 4.618228
[Info] 2-TH LEVEL FOUND: 9.189452171325684, Considering 10/90 traces
 84661/100000: episode: 1464, duration: 4.180s, episode steps: 19, steps per second: 5, episode reward: 44.259, mean reward: 2.329 [1.855, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.298, 10.100], loss: 0.229217, mae: 0.433418, mean_q: 4.650422
 84699/100000: episode: 1465, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 137.300, mean reward: 3.613 [2.498, 6.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.312, 10.100], loss: 0.261506, mae: 0.454864, mean_q: 4.700154
 84734/100000: episode: 1466, duration: 0.180s, episode steps: 35, steps per second: 195, episode reward: 144.662, mean reward: 4.133 [2.292, 6.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.525, 10.100], loss: 0.258462, mae: 0.447212, mean_q: 4.717976
 84771/100000: episode: 1467, duration: 0.189s, episode steps: 37, steps per second: 196, episode reward: 138.622, mean reward: 3.747 [1.863, 7.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.816, 10.100], loss: 0.222996, mae: 0.430674, mean_q: 4.717030
 84806/100000: episode: 1468, duration: 0.174s, episode steps: 35, steps per second: 202, episode reward: 88.649, mean reward: 2.533 [1.481, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.598, 10.167], loss: 0.261246, mae: 0.466915, mean_q: 4.740937
 84841/100000: episode: 1469, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 169.012, mean reward: 4.829 [2.784, 15.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.205, 10.100], loss: 0.312499, mae: 0.465902, mean_q: 4.720482
 84871/100000: episode: 1470, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 120.249, mean reward: 4.008 [2.817, 6.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.381, 10.100], loss: 0.261947, mae: 0.477232, mean_q: 4.725940
 84890/100000: episode: 1471, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 68.809, mean reward: 3.622 [2.598, 4.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.591, 10.100], loss: 0.320507, mae: 0.488891, mean_q: 4.905256
 84925/100000: episode: 1472, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 517.779, mean reward: 14.794 [3.577, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.448, 10.100], loss: 0.349923, mae: 0.469455, mean_q: 4.826186
 84949/100000: episode: 1473, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 84.111, mean reward: 3.505 [2.713, 6.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-1.215, 10.100], loss: 0.532460, mae: 0.502216, mean_q: 4.810128
 84979/100000: episode: 1474, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 119.877, mean reward: 3.996 [2.421, 7.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.723, 10.100], loss: 1.446006, mae: 0.722100, mean_q: 5.042809
 84997/100000: episode: 1475, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 55.710, mean reward: 3.095 [2.013, 5.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.172, 10.100], loss: 1.258465, mae: 0.661173, mean_q: 5.113279
 85032/100000: episode: 1476, duration: 0.179s, episode steps: 35, steps per second: 196, episode reward: 100.989, mean reward: 2.885 [1.775, 5.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.737, 10.100], loss: 0.400541, mae: 0.520726, mean_q: 4.962759
 85056/100000: episode: 1477, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 90.856, mean reward: 3.786 [2.985, 4.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.364, 10.100], loss: 6.189672, mae: 0.825060, mean_q: 4.980336
 85093/100000: episode: 1478, duration: 0.182s, episode steps: 37, steps per second: 203, episode reward: 136.938, mean reward: 3.701 [2.536, 5.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.548, 10.100], loss: 4.122595, mae: 0.639730, mean_q: 5.085723
 85130/100000: episode: 1479, duration: 0.176s, episode steps: 37, steps per second: 211, episode reward: 137.012, mean reward: 3.703 [2.584, 6.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.474, 10.100], loss: 4.379807, mae: 0.806243, mean_q: 5.062448
 85165/100000: episode: 1480, duration: 0.171s, episode steps: 35, steps per second: 204, episode reward: 123.310, mean reward: 3.523 [2.361, 6.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.381, 10.100], loss: 0.289862, mae: 0.492289, mean_q: 5.064608
 85203/100000: episode: 1481, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 124.703, mean reward: 3.282 [1.924, 11.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.226, 10.100], loss: 1.234748, mae: 0.579736, mean_q: 5.125144
 85242/100000: episode: 1482, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 291.486, mean reward: 7.474 [2.665, 43.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.515, 10.100], loss: 0.250106, mae: 0.462839, mean_q: 4.898366
 85272/100000: episode: 1483, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 92.959, mean reward: 3.099 [2.218, 5.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.656, 10.100], loss: 0.817050, mae: 0.530299, mean_q: 5.124870
 85302/100000: episode: 1484, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 114.230, mean reward: 3.808 [2.479, 5.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.676, 10.100], loss: 0.833253, mae: 0.570307, mean_q: 5.174047
 85332/100000: episode: 1485, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 146.319, mean reward: 4.877 [3.239, 9.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.442, 10.100], loss: 0.589673, mae: 0.566274, mean_q: 5.195298
[Info] FALSIFICATION!
[Info] Levels: [5.9534907, 9.189452, 21.747086]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 85353/100000: episode: 1486, duration: 4.552s, episode steps: 21, steps per second: 5, episode reward: 349.630, mean reward: 16.649 [4.134, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.338, 9.911], loss: 2.098720, mae: 0.694253, mean_q: 5.390858
 85453/100000: episode: 1487, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.681, mean reward: 1.887 [1.460, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.723, 10.292], loss: 2.632964, mae: 0.749008, mean_q: 5.330134
 85553/100000: episode: 1488, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 176.878, mean reward: 1.769 [1.446, 2.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.107, 10.132], loss: 3.321847, mae: 0.808982, mean_q: 5.348175
 85653/100000: episode: 1489, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 201.172, mean reward: 2.012 [1.467, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.764, 10.098], loss: 3.175285, mae: 0.739992, mean_q: 5.335845
 85753/100000: episode: 1490, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 197.691, mean reward: 1.977 [1.494, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.970, 10.098], loss: 5.010554, mae: 0.849841, mean_q: 5.307034
 85853/100000: episode: 1491, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 192.528, mean reward: 1.925 [1.464, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.372, 10.098], loss: 2.656144, mae: 0.735529, mean_q: 5.428927
 85953/100000: episode: 1492, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 203.121, mean reward: 2.031 [1.487, 5.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.667, 10.340], loss: 4.659930, mae: 0.789910, mean_q: 5.453548
 86053/100000: episode: 1493, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 191.201, mean reward: 1.912 [1.458, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.678, 10.219], loss: 3.387464, mae: 0.753289, mean_q: 5.410464
 86153/100000: episode: 1494, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 196.937, mean reward: 1.969 [1.472, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.486, 10.118], loss: 3.056703, mae: 0.782894, mean_q: 5.388519
 86253/100000: episode: 1495, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 191.827, mean reward: 1.918 [1.530, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.649, 10.098], loss: 3.262349, mae: 0.754620, mean_q: 5.605796
 86353/100000: episode: 1496, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 177.124, mean reward: 1.771 [1.439, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.521, 10.160], loss: 4.635767, mae: 0.807235, mean_q: 5.472290
 86453/100000: episode: 1497, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 188.911, mean reward: 1.889 [1.461, 2.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.509, 10.098], loss: 2.324914, mae: 0.670125, mean_q: 5.358778
 86553/100000: episode: 1498, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 190.245, mean reward: 1.902 [1.468, 3.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.077, 10.171], loss: 1.268497, mae: 0.633174, mean_q: 5.338861
 86653/100000: episode: 1499, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 219.454, mean reward: 2.195 [1.465, 6.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.643, 10.323], loss: 3.807160, mae: 0.754593, mean_q: 5.470215
 86753/100000: episode: 1500, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 203.106, mean reward: 2.031 [1.456, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.639, 10.322], loss: 5.866037, mae: 0.799991, mean_q: 5.391797
 86853/100000: episode: 1501, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 182.780, mean reward: 1.828 [1.458, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.807, 10.098], loss: 3.669226, mae: 0.765575, mean_q: 5.529840
 86953/100000: episode: 1502, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 202.999, mean reward: 2.030 [1.548, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.625, 10.235], loss: 6.075487, mae: 0.843686, mean_q: 5.334451
 87053/100000: episode: 1503, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 246.154, mean reward: 2.462 [1.541, 5.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.570, 10.315], loss: 3.236862, mae: 0.745100, mean_q: 5.332827
 87153/100000: episode: 1504, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 210.979, mean reward: 2.110 [1.480, 3.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.201, 10.098], loss: 1.840851, mae: 0.637653, mean_q: 5.232505
 87253/100000: episode: 1505, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 196.229, mean reward: 1.962 [1.516, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.899, 10.098], loss: 1.833715, mae: 0.600652, mean_q: 5.160232
 87353/100000: episode: 1506, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 198.667, mean reward: 1.987 [1.445, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.298, 10.098], loss: 1.455484, mae: 0.599383, mean_q: 5.026083
 87453/100000: episode: 1507, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 198.783, mean reward: 1.988 [1.439, 5.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.267, 10.098], loss: 2.892530, mae: 0.725970, mean_q: 5.235175
 87553/100000: episode: 1508, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 198.582, mean reward: 1.986 [1.442, 8.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.286, 10.098], loss: 2.070601, mae: 0.589114, mean_q: 5.058178
 87653/100000: episode: 1509, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 226.213, mean reward: 2.262 [1.454, 4.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.514, 10.098], loss: 4.901079, mae: 0.752645, mean_q: 5.130881
 87753/100000: episode: 1510, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 182.832, mean reward: 1.828 [1.435, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.387, 10.106], loss: 5.902732, mae: 0.833213, mean_q: 5.226353
 87853/100000: episode: 1511, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 195.800, mean reward: 1.958 [1.444, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.265, 10.238], loss: 0.799655, mae: 0.539990, mean_q: 4.989151
 87953/100000: episode: 1512, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 203.308, mean reward: 2.033 [1.520, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.892, 10.194], loss: 5.088636, mae: 0.812255, mean_q: 5.177918
 88053/100000: episode: 1513, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 179.151, mean reward: 1.792 [1.459, 2.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.966, 10.194], loss: 3.144420, mae: 0.643068, mean_q: 5.004345
 88153/100000: episode: 1514, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 203.300, mean reward: 2.033 [1.489, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.869, 10.340], loss: 2.983742, mae: 0.687741, mean_q: 5.068042
 88253/100000: episode: 1515, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 181.655, mean reward: 1.817 [1.436, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.572, 10.175], loss: 0.992927, mae: 0.572211, mean_q: 4.996826
 88353/100000: episode: 1516, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 183.488, mean reward: 1.835 [1.442, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.718, 10.138], loss: 0.597054, mae: 0.499079, mean_q: 4.870870
 88453/100000: episode: 1517, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 215.384, mean reward: 2.154 [1.453, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.836, 10.149], loss: 1.186428, mae: 0.574597, mean_q: 4.861738
 88553/100000: episode: 1518, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 202.065, mean reward: 2.021 [1.464, 7.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.724, 10.378], loss: 4.080113, mae: 0.689859, mean_q: 4.926023
 88653/100000: episode: 1519, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 179.404, mean reward: 1.794 [1.461, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.610, 10.319], loss: 5.058241, mae: 0.724155, mean_q: 4.952140
 88753/100000: episode: 1520, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 187.698, mean reward: 1.877 [1.459, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.540, 10.098], loss: 3.052778, mae: 0.586616, mean_q: 4.803201
 88853/100000: episode: 1521, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 209.084, mean reward: 2.091 [1.470, 5.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.496, 10.098], loss: 4.251597, mae: 0.622248, mean_q: 4.808202
 88953/100000: episode: 1522, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 206.852, mean reward: 2.069 [1.448, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.027, 10.098], loss: 2.407322, mae: 0.561724, mean_q: 4.693433
 89053/100000: episode: 1523, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.665, mean reward: 1.967 [1.478, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.204, 10.157], loss: 2.942398, mae: 0.604113, mean_q: 4.746087
 89153/100000: episode: 1524, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 195.087, mean reward: 1.951 [1.449, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.041, 10.098], loss: 2.499905, mae: 0.554079, mean_q: 4.646093
 89253/100000: episode: 1525, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 189.969, mean reward: 1.900 [1.445, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.657, 10.098], loss: 5.448846, mae: 0.791042, mean_q: 4.861847
 89353/100000: episode: 1526, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 215.154, mean reward: 2.152 [1.524, 4.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.024, 10.381], loss: 3.560499, mae: 0.590542, mean_q: 4.647883
 89453/100000: episode: 1527, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 217.485, mean reward: 2.175 [1.496, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.063, 10.141], loss: 1.141548, mae: 0.538538, mean_q: 4.733185
 89553/100000: episode: 1528, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 198.352, mean reward: 1.984 [1.497, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.591, 10.098], loss: 4.906709, mae: 0.681795, mean_q: 4.760772
 89653/100000: episode: 1529, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 196.034, mean reward: 1.960 [1.450, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.833, 10.098], loss: 4.846814, mae: 0.644288, mean_q: 4.740545
 89753/100000: episode: 1530, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 204.306, mean reward: 2.043 [1.441, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.413 [-1.160, 10.098], loss: 5.051634, mae: 0.674443, mean_q: 4.731823
 89853/100000: episode: 1531, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 183.749, mean reward: 1.837 [1.476, 2.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.144, 10.198], loss: 3.643094, mae: 0.571822, mean_q: 4.679779
 89953/100000: episode: 1532, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 179.537, mean reward: 1.795 [1.483, 2.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.665, 10.098], loss: 0.361463, mae: 0.372624, mean_q: 4.244604
 90053/100000: episode: 1533, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 187.327, mean reward: 1.873 [1.462, 2.822], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.722, 10.165], loss: 2.747964, mae: 0.525663, mean_q: 4.383510
 90153/100000: episode: 1534, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 184.745, mean reward: 1.847 [1.475, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.589, 10.119], loss: 1.138851, mae: 0.422079, mean_q: 4.196509
 90253/100000: episode: 1535, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 190.835, mean reward: 1.908 [1.501, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.224, 10.202], loss: 1.475537, mae: 0.399305, mean_q: 4.111925
 90353/100000: episode: 1536, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 192.639, mean reward: 1.926 [1.472, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.609, 10.098], loss: 0.756668, mae: 0.334736, mean_q: 3.937480
 90453/100000: episode: 1537, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 189.941, mean reward: 1.899 [1.471, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.261, 10.098], loss: 0.095259, mae: 0.302337, mean_q: 3.916186
 90553/100000: episode: 1538, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 185.626, mean reward: 1.856 [1.451, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.857, 10.184], loss: 0.092276, mae: 0.296853, mean_q: 3.891340
 90653/100000: episode: 1539, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 199.694, mean reward: 1.997 [1.468, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.803, 10.107], loss: 0.082959, mae: 0.287636, mean_q: 3.891727
 90753/100000: episode: 1540, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 191.543, mean reward: 1.915 [1.441, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.683, 10.098], loss: 0.094481, mae: 0.295722, mean_q: 3.890312
 90853/100000: episode: 1541, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 185.580, mean reward: 1.856 [1.439, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.646, 10.098], loss: 0.103778, mae: 0.301802, mean_q: 3.909824
 90953/100000: episode: 1542, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 178.298, mean reward: 1.783 [1.449, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.648, 10.196], loss: 0.082062, mae: 0.281835, mean_q: 3.871834
 91053/100000: episode: 1543, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 193.377, mean reward: 1.934 [1.490, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.350, 10.302], loss: 0.099981, mae: 0.292607, mean_q: 3.874013
 91153/100000: episode: 1544, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 177.433, mean reward: 1.774 [1.443, 2.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.618, 10.119], loss: 0.086229, mae: 0.288576, mean_q: 3.868580
 91253/100000: episode: 1545, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.568, mean reward: 1.926 [1.467, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.219, 10.167], loss: 0.103620, mae: 0.290938, mean_q: 3.905063
 91353/100000: episode: 1546, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 196.241, mean reward: 1.962 [1.491, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.176, 10.188], loss: 0.094390, mae: 0.289992, mean_q: 3.881416
 91453/100000: episode: 1547, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 195.400, mean reward: 1.954 [1.456, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.438, 10.247], loss: 0.086304, mae: 0.288553, mean_q: 3.897871
 91553/100000: episode: 1548, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.260, mean reward: 1.933 [1.462, 4.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.269, 10.098], loss: 0.078262, mae: 0.276946, mean_q: 3.882126
 91653/100000: episode: 1549, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 193.642, mean reward: 1.936 [1.448, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.626, 10.098], loss: 0.095832, mae: 0.292464, mean_q: 3.880022
 91753/100000: episode: 1550, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 202.281, mean reward: 2.023 [1.464, 5.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.469, 10.098], loss: 0.084744, mae: 0.288262, mean_q: 3.872087
 91853/100000: episode: 1551, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 182.799, mean reward: 1.828 [1.456, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.236, 10.114], loss: 0.099479, mae: 0.293612, mean_q: 3.878990
 91953/100000: episode: 1552, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 177.406, mean reward: 1.774 [1.469, 2.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.162, 10.370], loss: 0.079257, mae: 0.276281, mean_q: 3.865076
 92053/100000: episode: 1553, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 186.099, mean reward: 1.861 [1.461, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.319, 10.348], loss: 0.075089, mae: 0.271185, mean_q: 3.831547
 92153/100000: episode: 1554, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 209.464, mean reward: 2.095 [1.510, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.714, 10.098], loss: 0.080659, mae: 0.282583, mean_q: 3.859955
 92253/100000: episode: 1555, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 196.231, mean reward: 1.962 [1.458, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.911, 10.232], loss: 0.077538, mae: 0.275420, mean_q: 3.858240
 92353/100000: episode: 1556, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 191.866, mean reward: 1.919 [1.480, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.951, 10.098], loss: 0.100022, mae: 0.292508, mean_q: 3.840815
 92453/100000: episode: 1557, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 181.327, mean reward: 1.813 [1.452, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.602, 10.239], loss: 0.106328, mae: 0.290395, mean_q: 3.827302
 92553/100000: episode: 1558, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 192.972, mean reward: 1.930 [1.456, 3.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.191, 10.126], loss: 0.073318, mae: 0.275445, mean_q: 3.829710
 92653/100000: episode: 1559, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 181.119, mean reward: 1.811 [1.444, 2.588], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.431, 10.100], loss: 0.080494, mae: 0.281076, mean_q: 3.824086
 92753/100000: episode: 1560, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 201.685, mean reward: 2.017 [1.465, 3.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.471, 10.167], loss: 0.074952, mae: 0.275617, mean_q: 3.824650
 92853/100000: episode: 1561, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 215.316, mean reward: 2.153 [1.463, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.412, 10.098], loss: 0.078231, mae: 0.277699, mean_q: 3.821159
 92953/100000: episode: 1562, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 207.856, mean reward: 2.079 [1.470, 4.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.311, 10.149], loss: 0.069647, mae: 0.265566, mean_q: 3.816087
 93053/100000: episode: 1563, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 192.432, mean reward: 1.924 [1.445, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.262, 10.098], loss: 0.074624, mae: 0.276546, mean_q: 3.834134
 93153/100000: episode: 1564, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 215.884, mean reward: 2.159 [1.465, 3.298], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.834, 10.098], loss: 0.075905, mae: 0.279771, mean_q: 3.811033
 93253/100000: episode: 1565, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 216.924, mean reward: 2.169 [1.498, 5.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.387, 10.329], loss: 0.072722, mae: 0.280333, mean_q: 3.830706
 93353/100000: episode: 1566, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 193.568, mean reward: 1.936 [1.447, 4.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.807, 10.098], loss: 0.082817, mae: 0.287890, mean_q: 3.854442
 93453/100000: episode: 1567, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 188.213, mean reward: 1.882 [1.486, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.917, 10.098], loss: 0.084251, mae: 0.292368, mean_q: 3.841324
 93553/100000: episode: 1568, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 184.870, mean reward: 1.849 [1.449, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.529, 10.167], loss: 0.081915, mae: 0.287111, mean_q: 3.849226
 93653/100000: episode: 1569, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 196.258, mean reward: 1.963 [1.461, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.745, 10.098], loss: 0.071432, mae: 0.270288, mean_q: 3.836011
 93753/100000: episode: 1570, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 185.875, mean reward: 1.859 [1.439, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.243, 10.251], loss: 0.074239, mae: 0.276285, mean_q: 3.833585
 93853/100000: episode: 1571, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 188.839, mean reward: 1.888 [1.452, 4.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.210, 10.098], loss: 0.078382, mae: 0.276462, mean_q: 3.833633
 93953/100000: episode: 1572, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 200.780, mean reward: 2.008 [1.476, 4.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.133, 10.099], loss: 0.073706, mae: 0.281678, mean_q: 3.852272
 94053/100000: episode: 1573, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 194.280, mean reward: 1.943 [1.480, 2.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.142, 10.098], loss: 0.075822, mae: 0.283495, mean_q: 3.844579
 94153/100000: episode: 1574, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 201.971, mean reward: 2.020 [1.458, 5.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.182, 10.098], loss: 0.083310, mae: 0.289710, mean_q: 3.858008
 94253/100000: episode: 1575, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 211.334, mean reward: 2.113 [1.519, 4.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.453, 10.313], loss: 0.082758, mae: 0.292063, mean_q: 3.854401
 94353/100000: episode: 1576, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 192.446, mean reward: 1.924 [1.449, 8.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.255, 10.106], loss: 0.098975, mae: 0.296900, mean_q: 3.853139
 94453/100000: episode: 1577, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 195.407, mean reward: 1.954 [1.479, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.461, 10.169], loss: 0.087108, mae: 0.288161, mean_q: 3.841352
 94553/100000: episode: 1578, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 212.102, mean reward: 2.121 [1.531, 3.653], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.103, 10.098], loss: 0.072846, mae: 0.278079, mean_q: 3.823661
 94653/100000: episode: 1579, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 208.321, mean reward: 2.083 [1.447, 4.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.818, 10.098], loss: 0.078047, mae: 0.284152, mean_q: 3.844683
 94753/100000: episode: 1580, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.353, mean reward: 1.904 [1.471, 3.261], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.467, 10.186], loss: 0.083839, mae: 0.287364, mean_q: 3.831664
 94853/100000: episode: 1581, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 193.602, mean reward: 1.936 [1.480, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.211, 10.362], loss: 0.084000, mae: 0.286275, mean_q: 3.829315
 94953/100000: episode: 1582, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 199.966, mean reward: 2.000 [1.468, 7.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.506, 10.300], loss: 0.083481, mae: 0.291875, mean_q: 3.842101
 95053/100000: episode: 1583, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 182.846, mean reward: 1.828 [1.465, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.725, 10.158], loss: 0.087504, mae: 0.290310, mean_q: 3.835805
 95153/100000: episode: 1584, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 210.421, mean reward: 2.104 [1.465, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.480, 10.098], loss: 0.090986, mae: 0.296544, mean_q: 3.845593
 95253/100000: episode: 1585, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 191.614, mean reward: 1.916 [1.466, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.938, 10.147], loss: 0.085368, mae: 0.294617, mean_q: 3.851704
[Info] 1-TH LEVEL FOUND: 5.632089138031006, Considering 10/90 traces
 95353/100000: episode: 1586, duration: 4.592s, episode steps: 100, steps per second: 22, episode reward: 180.903, mean reward: 1.809 [1.469, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.342, 10.112], loss: 0.084960, mae: 0.292136, mean_q: 3.856040
 95378/100000: episode: 1587, duration: 0.132s, episode steps: 25, steps per second: 190, episode reward: 62.491, mean reward: 2.500 [1.733, 4.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.420, 10.164], loss: 0.098337, mae: 0.288410, mean_q: 3.809470
 95392/100000: episode: 1588, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 38.446, mean reward: 2.746 [1.818, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.241, 10.100], loss: 0.062071, mae: 0.264576, mean_q: 3.849360
 95459/100000: episode: 1589, duration: 0.362s, episode steps: 67, steps per second: 185, episode reward: 173.932, mean reward: 2.596 [1.598, 4.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.756 [-1.578, 10.100], loss: 0.099027, mae: 0.304096, mean_q: 3.835360
 95526/100000: episode: 1590, duration: 0.341s, episode steps: 67, steps per second: 197, episode reward: 131.347, mean reward: 1.960 [1.480, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.770 [-0.978, 10.216], loss: 0.100635, mae: 0.308548, mean_q: 3.880661
 95589/100000: episode: 1591, duration: 0.319s, episode steps: 63, steps per second: 198, episode reward: 219.588, mean reward: 3.486 [2.343, 8.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.779 [-1.128, 10.100], loss: 0.098824, mae: 0.308594, mean_q: 3.867829
 95601/100000: episode: 1592, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 32.133, mean reward: 2.678 [2.163, 4.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.005, 10.100], loss: 0.113109, mae: 0.322236, mean_q: 3.881872
 95626/100000: episode: 1593, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 63.165, mean reward: 2.527 [1.734, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.536, 10.242], loss: 0.089827, mae: 0.297552, mean_q: 3.859301
 95641/100000: episode: 1594, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 47.103, mean reward: 3.140 [2.400, 5.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.285, 10.100], loss: 0.163682, mae: 0.343733, mean_q: 3.983846
 95656/100000: episode: 1595, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 50.229, mean reward: 3.349 [2.384, 4.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.460, 10.100], loss: 0.112533, mae: 0.321516, mean_q: 3.879355
 95671/100000: episode: 1596, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 42.362, mean reward: 2.824 [2.296, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-1.153, 10.100], loss: 0.122575, mae: 0.322062, mean_q: 3.917868
 95734/100000: episode: 1597, duration: 0.326s, episode steps: 63, steps per second: 193, episode reward: 133.479, mean reward: 2.119 [1.462, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.806 [-0.813, 10.123], loss: 0.108967, mae: 0.322257, mean_q: 3.928924
 95746/100000: episode: 1598, duration: 0.062s, episode steps: 12, steps per second: 195, episode reward: 25.103, mean reward: 2.092 [1.786, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.162, 10.100], loss: 0.129503, mae: 0.307274, mean_q: 3.917029
 95771/100000: episode: 1599, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 75.686, mean reward: 3.027 [2.164, 4.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.102, 10.349], loss: 0.110768, mae: 0.329527, mean_q: 3.941540
 95796/100000: episode: 1600, duration: 0.128s, episode steps: 25, steps per second: 195, episode reward: 66.597, mean reward: 2.664 [2.067, 5.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.764, 10.365], loss: 0.121028, mae: 0.328986, mean_q: 3.974738
 95810/100000: episode: 1601, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 46.816, mean reward: 3.344 [2.262, 5.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.408, 10.100], loss: 0.091489, mae: 0.306350, mean_q: 3.902433
 95824/100000: episode: 1602, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 31.176, mean reward: 2.227 [1.734, 2.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.116, 10.100], loss: 0.095115, mae: 0.306866, mean_q: 3.963475
 95836/100000: episode: 1603, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 38.323, mean reward: 3.194 [2.097, 6.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.151, 10.100], loss: 0.104375, mae: 0.326455, mean_q: 3.953820
 95855/100000: episode: 1604, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 53.722, mean reward: 2.827 [2.051, 4.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.047, 10.100], loss: 0.127985, mae: 0.333938, mean_q: 4.019166
 95867/100000: episode: 1605, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 35.140, mean reward: 2.928 [2.419, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.417, 10.100], loss: 0.092810, mae: 0.304311, mean_q: 3.877647
 95879/100000: episode: 1606, duration: 0.058s, episode steps: 12, steps per second: 205, episode reward: 29.393, mean reward: 2.449 [1.946, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.534, 10.100], loss: 0.148421, mae: 0.377666, mean_q: 3.948914
 95942/100000: episode: 1607, duration: 0.317s, episode steps: 63, steps per second: 199, episode reward: 139.011, mean reward: 2.207 [1.465, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.808 [-1.331, 10.125], loss: 0.125847, mae: 0.331952, mean_q: 3.986665
 95957/100000: episode: 1608, duration: 0.078s, episode steps: 15, steps per second: 191, episode reward: 43.788, mean reward: 2.919 [2.091, 4.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.751, 10.100], loss: 0.118470, mae: 0.332776, mean_q: 3.960635
 95982/100000: episode: 1609, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 71.702, mean reward: 2.868 [2.361, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.406, 10.395], loss: 0.114520, mae: 0.337992, mean_q: 4.016764
 96007/100000: episode: 1610, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 85.601, mean reward: 3.424 [2.764, 5.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.236, 10.501], loss: 0.128364, mae: 0.350177, mean_q: 4.022804
 96021/100000: episode: 1611, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 39.528, mean reward: 2.823 [2.144, 3.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.291, 10.100], loss: 0.097197, mae: 0.317837, mean_q: 4.095196
 96035/100000: episode: 1612, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 31.803, mean reward: 2.272 [1.893, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.803, 10.100], loss: 0.129997, mae: 0.343313, mean_q: 3.962012
 96049/100000: episode: 1613, duration: 0.084s, episode steps: 14, steps per second: 167, episode reward: 38.811, mean reward: 2.772 [2.289, 3.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.199, 10.100], loss: 0.101114, mae: 0.332542, mean_q: 4.006828
 96063/100000: episode: 1614, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 36.812, mean reward: 2.629 [1.910, 5.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.268, 10.100], loss: 0.110264, mae: 0.324416, mean_q: 3.961217
 96077/100000: episode: 1615, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 39.260, mean reward: 2.804 [2.196, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.328, 10.100], loss: 0.146944, mae: 0.364760, mean_q: 4.071722
 96092/100000: episode: 1616, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 42.936, mean reward: 2.862 [2.321, 4.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.249, 10.100], loss: 0.094761, mae: 0.307598, mean_q: 4.018435
 96169/100000: episode: 1617, duration: 0.396s, episode steps: 77, steps per second: 194, episode reward: 165.829, mean reward: 2.154 [1.571, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.671 [-1.267, 10.365], loss: 0.123179, mae: 0.336957, mean_q: 4.043230
 96194/100000: episode: 1618, duration: 0.123s, episode steps: 25, steps per second: 204, episode reward: 75.340, mean reward: 3.014 [2.164, 4.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.708, 10.410], loss: 0.115026, mae: 0.344302, mean_q: 4.084311
 96208/100000: episode: 1619, duration: 0.086s, episode steps: 14, steps per second: 162, episode reward: 31.751, mean reward: 2.268 [1.920, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.099, 10.100], loss: 0.103795, mae: 0.341302, mean_q: 4.009694
 96227/100000: episode: 1620, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 53.835, mean reward: 2.833 [1.832, 5.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.262, 10.100], loss: 0.108957, mae: 0.325211, mean_q: 4.024906
 96304/100000: episode: 1621, duration: 0.402s, episode steps: 77, steps per second: 192, episode reward: 150.265, mean reward: 1.951 [1.454, 4.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.671 [-0.949, 10.125], loss: 0.131076, mae: 0.340037, mean_q: 4.067860
 96318/100000: episode: 1622, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 31.572, mean reward: 2.255 [1.976, 2.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.177, 10.100], loss: 0.096754, mae: 0.318085, mean_q: 4.056719
 96337/100000: episode: 1623, duration: 0.093s, episode steps: 19, steps per second: 205, episode reward: 59.118, mean reward: 3.111 [2.170, 4.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.410, 10.100], loss: 0.167830, mae: 0.375359, mean_q: 4.095063
 96414/100000: episode: 1624, duration: 0.396s, episode steps: 77, steps per second: 195, episode reward: 143.069, mean reward: 1.858 [1.476, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.676 [-0.772, 10.100], loss: 0.130955, mae: 0.342650, mean_q: 4.075059
 96433/100000: episode: 1625, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 47.046, mean reward: 2.476 [2.009, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.363, 10.100], loss: 0.116383, mae: 0.344392, mean_q: 4.062338
 96447/100000: episode: 1626, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 42.922, mean reward: 3.066 [2.583, 4.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.157, 10.100], loss: 0.112339, mae: 0.330212, mean_q: 4.019156
 96462/100000: episode: 1627, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 41.683, mean reward: 2.779 [2.202, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.989, 10.100], loss: 0.123611, mae: 0.349583, mean_q: 4.099401
 96476/100000: episode: 1628, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 32.730, mean reward: 2.338 [1.795, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.354, 10.100], loss: 0.133135, mae: 0.334603, mean_q: 4.075246
 96491/100000: episode: 1629, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 47.315, mean reward: 3.154 [2.707, 3.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.535, 10.100], loss: 0.190187, mae: 0.387663, mean_q: 4.152204
 96558/100000: episode: 1630, duration: 0.329s, episode steps: 67, steps per second: 204, episode reward: 136.757, mean reward: 2.041 [1.453, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.772 [-0.035, 10.257], loss: 0.117611, mae: 0.340594, mean_q: 4.094806
 96621/100000: episode: 1631, duration: 0.313s, episode steps: 63, steps per second: 201, episode reward: 135.789, mean reward: 2.155 [1.485, 3.412], mean action: 0.000 [0.000, 0.000], mean observation: 1.802 [-1.794, 10.333], loss: 0.142388, mae: 0.353840, mean_q: 4.104355
 96698/100000: episode: 1632, duration: 0.413s, episode steps: 77, steps per second: 186, episode reward: 153.996, mean reward: 2.000 [1.462, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.663 [-0.251, 10.100], loss: 0.130162, mae: 0.346806, mean_q: 4.109970
 96723/100000: episode: 1633, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 60.429, mean reward: 2.417 [1.788, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-1.708, 10.344], loss: 0.185913, mae: 0.386738, mean_q: 4.141036
 96735/100000: episode: 1634, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 33.522, mean reward: 2.793 [2.263, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.232, 10.100], loss: 0.190710, mae: 0.377104, mean_q: 4.090584
 96747/100000: episode: 1635, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 36.937, mean reward: 3.078 [2.212, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.377, 10.100], loss: 0.146932, mae: 0.367383, mean_q: 4.020928
 96762/100000: episode: 1636, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 32.173, mean reward: 2.145 [1.721, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.492, 10.100], loss: 0.138375, mae: 0.354590, mean_q: 4.166730
 96825/100000: episode: 1637, duration: 0.303s, episode steps: 63, steps per second: 208, episode reward: 207.297, mean reward: 3.290 [1.977, 8.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.794 [-0.270, 10.100], loss: 0.137176, mae: 0.358084, mean_q: 4.135182
 96892/100000: episode: 1638, duration: 0.372s, episode steps: 67, steps per second: 180, episode reward: 209.765, mean reward: 3.131 [1.665, 6.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.767 [-1.224, 10.100], loss: 0.139515, mae: 0.357340, mean_q: 4.141587
 96904/100000: episode: 1639, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 25.922, mean reward: 2.160 [1.926, 2.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.305, 10.100], loss: 0.148813, mae: 0.332874, mean_q: 4.161919
 96971/100000: episode: 1640, duration: 0.339s, episode steps: 67, steps per second: 198, episode reward: 238.054, mean reward: 3.553 [1.606, 13.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.746 [-0.731, 10.100], loss: 0.143800, mae: 0.354944, mean_q: 4.172022
 96983/100000: episode: 1641, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 35.479, mean reward: 2.957 [2.214, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.281, 10.100], loss: 0.285443, mae: 0.434632, mean_q: 4.376720
 97050/100000: episode: 1642, duration: 0.371s, episode steps: 67, steps per second: 180, episode reward: 140.908, mean reward: 2.103 [1.509, 4.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.758 [-0.754, 10.100], loss: 0.192603, mae: 0.388280, mean_q: 4.252081
 97062/100000: episode: 1643, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 26.522, mean reward: 2.210 [1.845, 2.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.119, 10.100], loss: 0.167486, mae: 0.375776, mean_q: 4.259223
 97076/100000: episode: 1644, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 26.568, mean reward: 1.898 [1.598, 2.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.567, 10.100], loss: 0.220931, mae: 0.392945, mean_q: 4.238403
 97101/100000: episode: 1645, duration: 0.127s, episode steps: 25, steps per second: 198, episode reward: 77.412, mean reward: 3.096 [2.267, 4.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.618, 10.411], loss: 0.167907, mae: 0.376309, mean_q: 4.300707
 97115/100000: episode: 1646, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 50.196, mean reward: 3.585 [2.210, 5.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.532, 10.100], loss: 0.167698, mae: 0.346060, mean_q: 4.184301
 97192/100000: episode: 1647, duration: 0.382s, episode steps: 77, steps per second: 202, episode reward: 177.045, mean reward: 2.299 [1.478, 6.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.663 [-1.057, 10.100], loss: 0.186320, mae: 0.377756, mean_q: 4.237951
 97217/100000: episode: 1648, duration: 0.118s, episode steps: 25, steps per second: 211, episode reward: 96.462, mean reward: 3.858 [2.603, 6.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.570], loss: 0.192808, mae: 0.392191, mean_q: 4.239592
 97231/100000: episode: 1649, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 32.037, mean reward: 2.288 [1.928, 2.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.297, 10.100], loss: 0.335030, mae: 0.450795, mean_q: 4.385449
 97245/100000: episode: 1650, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 30.409, mean reward: 2.172 [1.914, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.224, 10.100], loss: 0.202775, mae: 0.410612, mean_q: 4.264781
 97259/100000: episode: 1651, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 35.615, mean reward: 2.544 [1.962, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.577, 10.100], loss: 0.146476, mae: 0.382397, mean_q: 4.256706
 97326/100000: episode: 1652, duration: 0.341s, episode steps: 67, steps per second: 196, episode reward: 177.953, mean reward: 2.656 [1.890, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.758 [-0.351, 10.100], loss: 0.188865, mae: 0.389006, mean_q: 4.277330
 97393/100000: episode: 1653, duration: 0.347s, episode steps: 67, steps per second: 193, episode reward: 201.284, mean reward: 3.004 [1.864, 7.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.770 [-0.813, 10.100], loss: 0.213213, mae: 0.413808, mean_q: 4.316589
 97418/100000: episode: 1654, duration: 0.117s, episode steps: 25, steps per second: 214, episode reward: 92.375, mean reward: 3.695 [2.621, 5.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.316, 10.401], loss: 0.197181, mae: 0.413240, mean_q: 4.379488
 97485/100000: episode: 1655, duration: 0.344s, episode steps: 67, steps per second: 195, episode reward: 139.228, mean reward: 2.078 [1.507, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.770 [-0.258, 10.207], loss: 0.219271, mae: 0.415572, mean_q: 4.321550
 97500/100000: episode: 1656, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 39.314, mean reward: 2.621 [2.196, 3.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.240, 10.100], loss: 0.176558, mae: 0.398239, mean_q: 4.264972
 97519/100000: episode: 1657, duration: 0.109s, episode steps: 19, steps per second: 174, episode reward: 45.860, mean reward: 2.414 [1.996, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.128, 10.100], loss: 0.189590, mae: 0.395426, mean_q: 4.383579
 97582/100000: episode: 1658, duration: 0.320s, episode steps: 63, steps per second: 197, episode reward: 123.843, mean reward: 1.966 [1.537, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.799 [-0.660, 10.100], loss: 0.195823, mae: 0.394540, mean_q: 4.364113
 97649/100000: episode: 1659, duration: 0.345s, episode steps: 67, steps per second: 194, episode reward: 147.891, mean reward: 2.207 [1.481, 4.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.761 [-0.310, 10.100], loss: 0.194654, mae: 0.409257, mean_q: 4.373590
 97716/100000: episode: 1660, duration: 0.345s, episode steps: 67, steps per second: 194, episode reward: 167.946, mean reward: 2.507 [1.859, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.757 [-0.810, 10.100], loss: 0.270530, mae: 0.432455, mean_q: 4.395396
 97730/100000: episode: 1661, duration: 0.068s, episode steps: 14, steps per second: 204, episode reward: 34.467, mean reward: 2.462 [2.183, 2.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.524, 10.100], loss: 0.164643, mae: 0.401409, mean_q: 4.239896
 97745/100000: episode: 1662, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 55.974, mean reward: 3.732 [1.948, 6.612], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.802, 10.100], loss: 0.150983, mae: 0.381618, mean_q: 4.289863
 97757/100000: episode: 1663, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 29.951, mean reward: 2.496 [1.932, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.184, 10.100], loss: 0.132234, mae: 0.358669, mean_q: 4.302773
 97771/100000: episode: 1664, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 42.888, mean reward: 3.063 [2.009, 7.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.067, 10.100], loss: 0.174484, mae: 0.403567, mean_q: 4.360722
 97848/100000: episode: 1665, duration: 0.394s, episode steps: 77, steps per second: 195, episode reward: 147.223, mean reward: 1.912 [1.463, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.670 [-0.807, 10.100], loss: 0.202738, mae: 0.394825, mean_q: 4.398464
 97862/100000: episode: 1666, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 46.721, mean reward: 3.337 [2.277, 5.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.698, 10.100], loss: 0.162100, mae: 0.382670, mean_q: 4.244828
 97874/100000: episode: 1667, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 22.861, mean reward: 1.905 [1.677, 2.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.365, 10.100], loss: 0.153747, mae: 0.404247, mean_q: 4.475081
 97899/100000: episode: 1668, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 67.016, mean reward: 2.681 [1.990, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.395], loss: 0.218910, mae: 0.435125, mean_q: 4.369411
 97966/100000: episode: 1669, duration: 0.342s, episode steps: 67, steps per second: 196, episode reward: 193.340, mean reward: 2.886 [1.510, 4.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.760 [-1.410, 10.100], loss: 0.207451, mae: 0.414438, mean_q: 4.390997
 98029/100000: episode: 1670, duration: 0.342s, episode steps: 63, steps per second: 184, episode reward: 161.286, mean reward: 2.560 [1.497, 4.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.807 [-0.250, 10.100], loss: 0.176727, mae: 0.388427, mean_q: 4.397577
 98043/100000: episode: 1671, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 32.604, mean reward: 2.329 [1.775, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-1.088, 10.100], loss: 0.235482, mae: 0.417513, mean_q: 4.412549
 98057/100000: episode: 1672, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 36.861, mean reward: 2.633 [1.949, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.143, 10.100], loss: 0.189259, mae: 0.399937, mean_q: 4.356750
 98071/100000: episode: 1673, duration: 0.082s, episode steps: 14, steps per second: 172, episode reward: 38.470, mean reward: 2.748 [2.369, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.365, 10.100], loss: 0.224207, mae: 0.429942, mean_q: 4.486797
 98096/100000: episode: 1674, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 74.518, mean reward: 2.981 [2.217, 5.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.369, 10.409], loss: 0.222710, mae: 0.394635, mean_q: 4.442195
 98159/100000: episode: 1675, duration: 0.319s, episode steps: 63, steps per second: 197, episode reward: 152.407, mean reward: 2.419 [1.503, 6.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.801 [-0.585, 10.100], loss: 0.223534, mae: 0.423479, mean_q: 4.455148
[Info] 2-TH LEVEL FOUND: 7.270254611968994, Considering 10/90 traces
 98173/100000: episode: 1676, duration: 4.126s, episode steps: 14, steps per second: 3, episode reward: 33.181, mean reward: 2.370 [1.939, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.233, 10.100], loss: 0.218439, mae: 0.420367, mean_q: 4.507966
 98220/100000: episode: 1677, duration: 0.237s, episode steps: 47, steps per second: 198, episode reward: 105.482, mean reward: 2.244 [1.501, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.473, 10.100], loss: 0.190192, mae: 0.408602, mean_q: 4.388284
 98249/100000: episode: 1678, duration: 0.159s, episode steps: 29, steps per second: 183, episode reward: 86.347, mean reward: 2.977 [2.209, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.610, 10.100], loss: 0.151448, mae: 0.378062, mean_q: 4.404629
 98266/100000: episode: 1679, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 54.620, mean reward: 3.213 [2.420, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.253, 10.100], loss: 0.304724, mae: 0.487979, mean_q: 4.582645
 98313/100000: episode: 1680, duration: 0.244s, episode steps: 47, steps per second: 193, episode reward: 129.156, mean reward: 2.748 [2.010, 4.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.243, 10.100], loss: 0.187567, mae: 0.401228, mean_q: 4.493719
 98367/100000: episode: 1681, duration: 0.272s, episode steps: 54, steps per second: 199, episode reward: 166.670, mean reward: 3.086 [2.339, 5.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.851 [-1.254, 10.100], loss: 0.192292, mae: 0.406274, mean_q: 4.472946
 98384/100000: episode: 1682, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 39.871, mean reward: 2.345 [1.929, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.122, 10.100], loss: 0.346801, mae: 0.452660, mean_q: 4.561437
 98441/100000: episode: 1683, duration: 0.297s, episode steps: 57, steps per second: 192, episode reward: 255.421, mean reward: 4.481 [2.400, 11.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.837 [-0.719, 10.100], loss: 0.253364, mae: 0.454543, mean_q: 4.588280
 98480/100000: episode: 1684, duration: 0.195s, episode steps: 39, steps per second: 200, episode reward: 98.000, mean reward: 2.513 [2.079, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.241, 10.100], loss: 0.271798, mae: 0.460972, mean_q: 4.640070
 98487/100000: episode: 1685, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 23.714, mean reward: 3.388 [2.779, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.408, 10.100], loss: 0.131746, mae: 0.363971, mean_q: 4.577690
 98494/100000: episode: 1686, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 24.616, mean reward: 3.517 [2.664, 4.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-1.399, 10.100], loss: 0.385763, mae: 0.504862, mean_q: 4.646020
 98541/100000: episode: 1687, duration: 0.238s, episode steps: 47, steps per second: 197, episode reward: 111.890, mean reward: 2.381 [1.789, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.145, 10.100], loss: 0.245759, mae: 0.441673, mean_q: 4.579316
 98551/100000: episode: 1688, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 31.552, mean reward: 3.155 [2.771, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.998, 10.100], loss: 0.269264, mae: 0.473636, mean_q: 4.552214
 98561/100000: episode: 1689, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 32.469, mean reward: 3.247 [2.745, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.626, 10.100], loss: 0.213706, mae: 0.470996, mean_q: 4.702290
 98568/100000: episode: 1690, duration: 0.036s, episode steps: 7, steps per second: 194, episode reward: 19.223, mean reward: 2.746 [2.500, 2.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.381, 10.100], loss: 0.319956, mae: 0.519436, mean_q: 4.581394
 98585/100000: episode: 1691, duration: 0.081s, episode steps: 17, steps per second: 209, episode reward: 61.311, mean reward: 3.607 [3.067, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.472, 10.100], loss: 0.270145, mae: 0.424405, mean_q: 4.673439
 98614/100000: episode: 1692, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 80.909, mean reward: 2.790 [1.649, 4.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.261, 10.100], loss: 0.287056, mae: 0.471441, mean_q: 4.640774
 98625/100000: episode: 1693, duration: 0.054s, episode steps: 11, steps per second: 203, episode reward: 36.037, mean reward: 3.276 [2.641, 4.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.449, 10.100], loss: 0.281402, mae: 0.490760, mean_q: 4.730027
 98654/100000: episode: 1694, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 74.183, mean reward: 2.558 [1.612, 4.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.111, 10.109], loss: 0.205937, mae: 0.434244, mean_q: 4.645472
 98701/100000: episode: 1695, duration: 0.247s, episode steps: 47, steps per second: 191, episode reward: 148.993, mean reward: 3.170 [1.649, 5.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.923 [-1.601, 10.100], loss: 0.349112, mae: 0.497216, mean_q: 4.735613
 98740/100000: episode: 1696, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 211.314, mean reward: 5.418 [2.444, 13.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.393, 10.100], loss: 0.267862, mae: 0.483395, mean_q: 4.745790
 98751/100000: episode: 1697, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 54.803, mean reward: 4.982 [2.633, 10.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.608, 10.100], loss: 0.201343, mae: 0.419906, mean_q: 4.570031
 98758/100000: episode: 1698, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 25.865, mean reward: 3.695 [3.157, 4.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.356, 10.100], loss: 0.169545, mae: 0.393910, mean_q: 4.763106
 98775/100000: episode: 1699, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 53.412, mean reward: 3.142 [2.733, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.292, 10.100], loss: 0.304632, mae: 0.481968, mean_q: 4.716221
 98786/100000: episode: 1700, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 42.133, mean reward: 3.830 [2.364, 4.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.340, 10.100], loss: 0.260699, mae: 0.484456, mean_q: 4.830467
 98793/100000: episode: 1701, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 28.293, mean reward: 4.042 [2.443, 5.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.512, 10.100], loss: 0.232357, mae: 0.421956, mean_q: 4.725373
 98803/100000: episode: 1702, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 28.495, mean reward: 2.850 [2.067, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.329, 10.100], loss: 0.805800, mae: 0.660893, mean_q: 4.792804
 98813/100000: episode: 1703, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 31.688, mean reward: 3.169 [2.856, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.618, 10.100], loss: 0.761232, mae: 0.819473, mean_q: 4.809835
 98867/100000: episode: 1704, duration: 0.292s, episode steps: 54, steps per second: 185, episode reward: 191.737, mean reward: 3.551 [1.711, 9.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.853 [-0.812, 10.100], loss: 0.431775, mae: 0.572145, mean_q: 4.864318
 98921/100000: episode: 1705, duration: 0.279s, episode steps: 54, steps per second: 193, episode reward: 148.956, mean reward: 2.758 [1.497, 5.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.411, 10.100], loss: 0.367169, mae: 0.494740, mean_q: 4.838476
 98968/100000: episode: 1706, duration: 0.265s, episode steps: 47, steps per second: 178, episode reward: 125.315, mean reward: 2.666 [1.937, 4.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.105, 10.100], loss: 0.281231, mae: 0.491432, mean_q: 4.817121
 99007/100000: episode: 1707, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 120.405, mean reward: 3.087 [2.263, 6.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-1.232, 10.100], loss: 0.310032, mae: 0.483312, mean_q: 4.856802
 99046/100000: episode: 1708, duration: 0.215s, episode steps: 39, steps per second: 181, episode reward: 136.677, mean reward: 3.505 [2.588, 7.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.463, 10.100], loss: 0.261329, mae: 0.472045, mean_q: 4.879646
 99075/100000: episode: 1709, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 69.706, mean reward: 2.404 [1.660, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.089, 10.100], loss: 0.230782, mae: 0.461071, mean_q: 4.868237
 99129/100000: episode: 1710, duration: 0.282s, episode steps: 54, steps per second: 191, episode reward: 189.523, mean reward: 3.510 [2.239, 5.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.797, 10.100], loss: 0.388138, mae: 0.522618, mean_q: 4.972552
 99158/100000: episode: 1711, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 67.100, mean reward: 2.314 [1.541, 3.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.442, 10.100], loss: 0.335394, mae: 0.546917, mean_q: 4.951918
 99212/100000: episode: 1712, duration: 0.255s, episode steps: 54, steps per second: 212, episode reward: 162.684, mean reward: 3.013 [1.542, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.323, 10.129], loss: 0.361549, mae: 0.532474, mean_q: 5.023640
 99223/100000: episode: 1713, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 33.891, mean reward: 3.081 [2.353, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.271, 10.100], loss: 0.265499, mae: 0.475164, mean_q: 4.799459
 99252/100000: episode: 1714, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 87.501, mean reward: 3.017 [1.769, 5.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.035, 10.100], loss: 0.396578, mae: 0.536541, mean_q: 5.134317
 99269/100000: episode: 1715, duration: 0.088s, episode steps: 17, steps per second: 193, episode reward: 41.179, mean reward: 2.422 [1.833, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.342, 10.100], loss: 0.341839, mae: 0.511769, mean_q: 5.011635
 99298/100000: episode: 1716, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 72.389, mean reward: 2.496 [1.474, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.488, 10.185], loss: 0.319926, mae: 0.496873, mean_q: 5.075682
 99337/100000: episode: 1717, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 105.212, mean reward: 2.698 [1.507, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.429, 10.100], loss: 0.296483, mae: 0.485788, mean_q: 5.010172
 99394/100000: episode: 1718, duration: 0.315s, episode steps: 57, steps per second: 181, episode reward: 226.485, mean reward: 3.973 [2.256, 9.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.838 [-0.684, 10.100], loss: 0.302875, mae: 0.490607, mean_q: 4.980235
 99441/100000: episode: 1719, duration: 0.237s, episode steps: 47, steps per second: 198, episode reward: 144.179, mean reward: 3.068 [1.961, 4.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.799, 10.100], loss: 0.364546, mae: 0.511960, mean_q: 5.011451
 99498/100000: episode: 1720, duration: 0.290s, episode steps: 57, steps per second: 197, episode reward: 150.010, mean reward: 2.632 [1.831, 6.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.848 [-0.232, 10.100], loss: 0.323792, mae: 0.511887, mean_q: 5.039646
 99508/100000: episode: 1721, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 28.838, mean reward: 2.884 [2.390, 3.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.630, 10.100], loss: 0.308124, mae: 0.517796, mean_q: 5.044597
 99555/100000: episode: 1722, duration: 0.244s, episode steps: 47, steps per second: 193, episode reward: 154.425, mean reward: 3.286 [1.722, 7.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-1.420, 10.100], loss: 0.344927, mae: 0.524915, mean_q: 5.050940
 99565/100000: episode: 1723, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 28.615, mean reward: 2.861 [2.349, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.316, 10.100], loss: 0.225746, mae: 0.484456, mean_q: 5.118390
 99572/100000: episode: 1724, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 24.828, mean reward: 3.547 [2.750, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.530, 10.100], loss: 0.296592, mae: 0.484543, mean_q: 4.976543
 99601/100000: episode: 1725, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 82.701, mean reward: 2.852 [1.632, 5.834], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.471, 10.100], loss: 0.257927, mae: 0.461766, mean_q: 4.990279
 99630/100000: episode: 1726, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 74.101, mean reward: 2.555 [2.003, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.928, 10.100], loss: 0.335542, mae: 0.532973, mean_q: 5.001204
 99637/100000: episode: 1727, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 22.238, mean reward: 3.177 [2.744, 4.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.358, 10.100], loss: 0.241849, mae: 0.501533, mean_q: 5.145680
 99684/100000: episode: 1728, duration: 0.265s, episode steps: 47, steps per second: 177, episode reward: 123.486, mean reward: 2.627 [2.212, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.466, 10.100], loss: 0.365281, mae: 0.560792, mean_q: 5.086390
 99694/100000: episode: 1729, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 30.432, mean reward: 3.043 [2.596, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.725, 10.100], loss: 0.277602, mae: 0.524877, mean_q: 5.056964
 99723/100000: episode: 1730, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 84.815, mean reward: 2.925 [2.103, 4.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.281, 10.100], loss: 0.408736, mae: 0.550961, mean_q: 5.110022
 99762/100000: episode: 1731, duration: 0.202s, episode steps: 39, steps per second: 193, episode reward: 120.103, mean reward: 3.080 [2.107, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.346, 10.100], loss: 0.350623, mae: 0.537428, mean_q: 5.110127
 99779/100000: episode: 1732, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 63.699, mean reward: 3.747 [2.666, 7.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.849, 10.100], loss: 0.337892, mae: 0.501861, mean_q: 5.098240
 99790/100000: episode: 1733, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 33.469, mean reward: 3.043 [2.710, 3.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.376, 10.100], loss: 0.392772, mae: 0.575790, mean_q: 5.099205
 99847/100000: episode: 1734, duration: 0.299s, episode steps: 57, steps per second: 191, episode reward: 198.972, mean reward: 3.491 [2.044, 10.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.829 [-0.446, 10.100], loss: 0.358990, mae: 0.546913, mean_q: 5.133943
 99857/100000: episode: 1735, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 27.782, mean reward: 2.778 [2.442, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.307, 10.100], loss: 0.335997, mae: 0.516625, mean_q: 5.221713
 99904/100000: episode: 1736, duration: 0.231s, episode steps: 47, steps per second: 204, episode reward: 133.589, mean reward: 2.842 [1.854, 5.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.851, 10.100], loss: 0.333960, mae: 0.523292, mean_q: 5.186379
 99958/100000: episode: 1737, duration: 0.280s, episode steps: 54, steps per second: 193, episode reward: 133.410, mean reward: 2.471 [1.466, 6.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.267, 10.100], loss: 0.361769, mae: 0.537121, mean_q: 5.215920
done, took 581.469 seconds
[Info] End Importance Splitting. Falsification occurred 5 times.
