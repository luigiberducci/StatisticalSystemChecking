Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.170s, episode steps: 100, steps per second: 588, episode reward: 185.334, mean reward: 1.853 [1.445, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.989, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.067s, episode steps: 100, steps per second: 1495, episode reward: 223.293, mean reward: 2.233 [1.457, 4.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.640, 10.423], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.061s, episode steps: 100, steps per second: 1646, episode reward: 175.446, mean reward: 1.754 [1.447, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.421, 10.209], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 186.999, mean reward: 1.870 [1.469, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.780, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.069s, episode steps: 100, steps per second: 1446, episode reward: 186.902, mean reward: 1.869 [1.466, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.965, 10.112], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.061s, episode steps: 100, steps per second: 1648, episode reward: 181.263, mean reward: 1.813 [1.477, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.294, 10.107], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.060s, episode steps: 100, steps per second: 1678, episode reward: 210.013, mean reward: 2.100 [1.471, 8.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-0.933, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.060s, episode steps: 100, steps per second: 1663, episode reward: 201.450, mean reward: 2.015 [1.454, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.164, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.059s, episode steps: 100, steps per second: 1683, episode reward: 196.038, mean reward: 1.960 [1.466, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.403, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.068s, episode steps: 100, steps per second: 1477, episode reward: 192.826, mean reward: 1.928 [1.490, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.632, 10.098], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.063s, episode steps: 100, steps per second: 1599, episode reward: 198.022, mean reward: 1.980 [1.468, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.092, 10.098], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.069s, episode steps: 100, steps per second: 1443, episode reward: 206.539, mean reward: 2.065 [1.476, 4.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.418, 10.154], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.060s, episode steps: 100, steps per second: 1672, episode reward: 200.125, mean reward: 2.001 [1.486, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.327, 10.140], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.068s, episode steps: 100, steps per second: 1477, episode reward: 195.084, mean reward: 1.951 [1.492, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.101, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.071s, episode steps: 100, steps per second: 1409, episode reward: 190.921, mean reward: 1.909 [1.461, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.418, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.072s, episode steps: 100, steps per second: 1393, episode reward: 186.909, mean reward: 1.869 [1.481, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.718, 10.102], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.068s, episode steps: 100, steps per second: 1461, episode reward: 206.688, mean reward: 2.067 [1.471, 4.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.515, 10.180], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.060s, episode steps: 100, steps per second: 1656, episode reward: 211.401, mean reward: 2.114 [1.459, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.503, 10.098], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.067s, episode steps: 100, steps per second: 1488, episode reward: 192.150, mean reward: 1.921 [1.483, 4.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.204, 10.184], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.061s, episode steps: 100, steps per second: 1640, episode reward: 213.692, mean reward: 2.137 [1.493, 3.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.428, 10.098], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.075s, episode steps: 100, steps per second: 1325, episode reward: 211.826, mean reward: 2.118 [1.459, 5.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.809, 10.113], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.066s, episode steps: 100, steps per second: 1523, episode reward: 218.409, mean reward: 2.184 [1.545, 4.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.981, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.060s, episode steps: 100, steps per second: 1665, episode reward: 197.351, mean reward: 1.974 [1.441, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.136, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.066s, episode steps: 100, steps per second: 1515, episode reward: 202.322, mean reward: 2.023 [1.476, 4.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.802, 10.223], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.060s, episode steps: 100, steps per second: 1658, episode reward: 224.404, mean reward: 2.244 [1.489, 4.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.401, 10.444], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.061s, episode steps: 100, steps per second: 1642, episode reward: 213.399, mean reward: 2.134 [1.492, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.678, 10.098], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.060s, episode steps: 100, steps per second: 1675, episode reward: 203.324, mean reward: 2.033 [1.460, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.816, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.060s, episode steps: 100, steps per second: 1678, episode reward: 187.821, mean reward: 1.878 [1.439, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.191, 10.132], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.060s, episode steps: 100, steps per second: 1653, episode reward: 191.762, mean reward: 1.918 [1.484, 4.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.737, 10.131], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.061s, episode steps: 100, steps per second: 1648, episode reward: 184.095, mean reward: 1.841 [1.466, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.185, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.060s, episode steps: 100, steps per second: 1660, episode reward: 195.435, mean reward: 1.954 [1.518, 4.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.554, 10.356], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.067s, episode steps: 100, steps per second: 1502, episode reward: 238.510, mean reward: 2.385 [1.501, 6.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.295, 10.227], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.061s, episode steps: 100, steps per second: 1648, episode reward: 229.066, mean reward: 2.291 [1.648, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.198, 10.242], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.071s, episode steps: 100, steps per second: 1408, episode reward: 198.323, mean reward: 1.983 [1.478, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.389, 10.232], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.060s, episode steps: 100, steps per second: 1666, episode reward: 198.649, mean reward: 1.986 [1.514, 5.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.592, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.060s, episode steps: 100, steps per second: 1678, episode reward: 184.577, mean reward: 1.846 [1.507, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.050, 10.155], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.060s, episode steps: 100, steps per second: 1670, episode reward: 196.564, mean reward: 1.966 [1.456, 3.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.209, 10.144], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.060s, episode steps: 100, steps per second: 1667, episode reward: 207.480, mean reward: 2.075 [1.480, 4.553], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.676, 10.528], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.070s, episode steps: 100, steps per second: 1419, episode reward: 225.094, mean reward: 2.251 [1.508, 4.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.647, 10.098], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.077s, episode steps: 100, steps per second: 1306, episode reward: 204.099, mean reward: 2.041 [1.457, 6.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.710, 10.278], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.060s, episode steps: 100, steps per second: 1666, episode reward: 197.282, mean reward: 1.973 [1.541, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.573, 10.194], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.060s, episode steps: 100, steps per second: 1667, episode reward: 186.039, mean reward: 1.860 [1.455, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.792, 10.099], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.060s, episode steps: 100, steps per second: 1676, episode reward: 191.907, mean reward: 1.919 [1.503, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.383, 10.098], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.060s, episode steps: 100, steps per second: 1661, episode reward: 223.492, mean reward: 2.235 [1.478, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.934, 10.270], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.068s, episode steps: 100, steps per second: 1478, episode reward: 183.232, mean reward: 1.832 [1.465, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.939, 10.100], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.060s, episode steps: 100, steps per second: 1666, episode reward: 200.534, mean reward: 2.005 [1.476, 4.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.179, 10.130], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.060s, episode steps: 100, steps per second: 1679, episode reward: 227.881, mean reward: 2.279 [1.471, 4.299], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.481, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.060s, episode steps: 100, steps per second: 1675, episode reward: 188.553, mean reward: 1.886 [1.436, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.745, 10.191], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.061s, episode steps: 100, steps per second: 1647, episode reward: 179.833, mean reward: 1.798 [1.439, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.643, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.060s, episode steps: 100, steps per second: 1662, episode reward: 222.849, mean reward: 2.228 [1.545, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.516, 10.406], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 181.307, mean reward: 1.813 [1.453, 2.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.783, 10.161], loss: 0.283262, mae: 0.543245, mean_q: 2.184828
  5200/100000: episode: 52, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 213.705, mean reward: 2.137 [1.489, 5.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.591, 10.354], loss: 0.132705, mae: 0.361108, mean_q: 2.959980
  5300/100000: episode: 53, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 187.778, mean reward: 1.878 [1.510, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.993, 10.165], loss: 0.136997, mae: 0.360852, mean_q: 3.311320
  5400/100000: episode: 54, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.112, mean reward: 1.881 [1.477, 4.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.030, 10.144], loss: 0.134136, mae: 0.357441, mean_q: 3.570912
  5500/100000: episode: 55, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 186.763, mean reward: 1.868 [1.485, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.524, 10.199], loss: 0.134057, mae: 0.347642, mean_q: 3.729452
  5600/100000: episode: 56, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 207.686, mean reward: 2.077 [1.439, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.217, 10.433], loss: 0.122285, mae: 0.344790, mean_q: 3.805796
  5700/100000: episode: 57, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 189.626, mean reward: 1.896 [1.468, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.653, 10.205], loss: 0.134349, mae: 0.359334, mean_q: 3.893614
  5800/100000: episode: 58, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 190.255, mean reward: 1.903 [1.477, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.727, 10.098], loss: 0.132937, mae: 0.348604, mean_q: 3.905731
  5900/100000: episode: 59, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.208, mean reward: 1.892 [1.463, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.837, 10.190], loss: 0.120208, mae: 0.339722, mean_q: 3.908847
  6000/100000: episode: 60, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 200.566, mean reward: 2.006 [1.461, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.035, 10.283], loss: 0.120853, mae: 0.339473, mean_q: 3.917027
  6100/100000: episode: 61, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 208.177, mean reward: 2.082 [1.504, 6.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.874, 10.098], loss: 0.120164, mae: 0.342718, mean_q: 3.948911
  6200/100000: episode: 62, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 189.528, mean reward: 1.895 [1.440, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.973, 10.098], loss: 0.110429, mae: 0.334530, mean_q: 3.944277
  6300/100000: episode: 63, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 177.213, mean reward: 1.772 [1.481, 2.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.650, 10.098], loss: 0.118793, mae: 0.334166, mean_q: 3.938371
  6400/100000: episode: 64, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 188.360, mean reward: 1.884 [1.504, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.239, 10.098], loss: 0.131936, mae: 0.351027, mean_q: 3.961544
  6500/100000: episode: 65, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 232.473, mean reward: 2.325 [1.502, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.695, 10.323], loss: 0.120680, mae: 0.330033, mean_q: 3.958928
  6600/100000: episode: 66, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 197.404, mean reward: 1.974 [1.472, 6.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.994, 10.098], loss: 0.117759, mae: 0.337286, mean_q: 3.971658
  6700/100000: episode: 67, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 183.826, mean reward: 1.838 [1.461, 2.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.590, 10.098], loss: 0.127030, mae: 0.347158, mean_q: 3.976827
  6800/100000: episode: 68, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 236.369, mean reward: 2.364 [1.465, 5.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.130, 10.291], loss: 0.131036, mae: 0.339700, mean_q: 3.965847
  6900/100000: episode: 69, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 177.140, mean reward: 1.771 [1.458, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.327, 10.260], loss: 0.129021, mae: 0.349066, mean_q: 3.972340
  7000/100000: episode: 70, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 183.924, mean reward: 1.839 [1.438, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.921, 10.098], loss: 0.123966, mae: 0.342504, mean_q: 3.949694
  7100/100000: episode: 71, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 199.242, mean reward: 1.992 [1.464, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.167, 10.098], loss: 0.142152, mae: 0.357910, mean_q: 3.976447
  7200/100000: episode: 72, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 178.478, mean reward: 1.785 [1.437, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.160, 10.098], loss: 0.117427, mae: 0.335780, mean_q: 3.941330
  7300/100000: episode: 73, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 185.488, mean reward: 1.855 [1.463, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.795, 10.261], loss: 0.111820, mae: 0.330037, mean_q: 3.946208
  7400/100000: episode: 74, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 215.153, mean reward: 2.152 [1.447, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.102, 10.430], loss: 0.116634, mae: 0.330985, mean_q: 3.930104
  7500/100000: episode: 75, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 183.051, mean reward: 1.831 [1.497, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.145, 10.143], loss: 0.125120, mae: 0.339745, mean_q: 3.935084
  7600/100000: episode: 76, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 192.595, mean reward: 1.926 [1.460, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.898, 10.277], loss: 0.113308, mae: 0.332308, mean_q: 3.938627
  7700/100000: episode: 77, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 190.340, mean reward: 1.903 [1.469, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.107, 10.098], loss: 0.122746, mae: 0.338533, mean_q: 3.935373
  7800/100000: episode: 78, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 239.519, mean reward: 2.395 [1.492, 5.507], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.790, 10.098], loss: 0.109766, mae: 0.322702, mean_q: 3.914584
  7900/100000: episode: 79, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 201.305, mean reward: 2.013 [1.429, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.844, 10.098], loss: 0.143302, mae: 0.354794, mean_q: 3.946481
  8000/100000: episode: 80, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 215.362, mean reward: 2.154 [1.451, 5.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.446, 10.219], loss: 0.133866, mae: 0.348873, mean_q: 3.938695
  8100/100000: episode: 81, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 205.047, mean reward: 2.050 [1.462, 4.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.616, 10.235], loss: 0.122254, mae: 0.342633, mean_q: 3.947544
  8200/100000: episode: 82, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 195.587, mean reward: 1.956 [1.486, 4.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.563, 10.315], loss: 0.121091, mae: 0.338660, mean_q: 3.942458
  8300/100000: episode: 83, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.339, mean reward: 1.883 [1.459, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.106, 10.098], loss: 0.118271, mae: 0.330601, mean_q: 3.929407
  8400/100000: episode: 84, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 187.332, mean reward: 1.873 [1.458, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.984, 10.234], loss: 0.127222, mae: 0.341124, mean_q: 3.932741
  8500/100000: episode: 85, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 196.685, mean reward: 1.967 [1.449, 3.765], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.570, 10.098], loss: 0.123249, mae: 0.333620, mean_q: 3.919858
  8600/100000: episode: 86, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 184.046, mean reward: 1.840 [1.466, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.639, 10.232], loss: 0.138355, mae: 0.351272, mean_q: 3.942561
  8700/100000: episode: 87, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 198.265, mean reward: 1.983 [1.470, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.923, 10.175], loss: 0.122857, mae: 0.338146, mean_q: 3.921177
  8800/100000: episode: 88, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: 195.684, mean reward: 1.957 [1.452, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.199, 10.211], loss: 0.123171, mae: 0.336282, mean_q: 3.928867
  8900/100000: episode: 89, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 192.364, mean reward: 1.924 [1.498, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.520, 10.142], loss: 0.119912, mae: 0.331559, mean_q: 3.907362
  9000/100000: episode: 90, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.660, mean reward: 1.897 [1.466, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.713, 10.189], loss: 0.106288, mae: 0.320151, mean_q: 3.900669
  9100/100000: episode: 91, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 184.673, mean reward: 1.847 [1.442, 3.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.127, 10.183], loss: 0.110448, mae: 0.326965, mean_q: 3.888325
  9200/100000: episode: 92, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 185.988, mean reward: 1.860 [1.453, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.712, 10.098], loss: 0.116201, mae: 0.338547, mean_q: 3.912440
  9300/100000: episode: 93, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 184.847, mean reward: 1.848 [1.478, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.328, 10.302], loss: 0.100762, mae: 0.319540, mean_q: 3.904876
  9400/100000: episode: 94, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 180.847, mean reward: 1.808 [1.488, 2.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.777, 10.098], loss: 0.112836, mae: 0.328411, mean_q: 3.884787
  9500/100000: episode: 95, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 190.350, mean reward: 1.903 [1.447, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.383, 10.098], loss: 0.107700, mae: 0.325471, mean_q: 3.876511
  9600/100000: episode: 96, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 179.008, mean reward: 1.790 [1.444, 2.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.389, 10.106], loss: 0.105966, mae: 0.327283, mean_q: 3.889865
  9700/100000: episode: 97, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 211.531, mean reward: 2.115 [1.509, 3.308], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.904, 10.272], loss: 0.103134, mae: 0.317832, mean_q: 3.879754
  9800/100000: episode: 98, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 198.888, mean reward: 1.989 [1.464, 3.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.791, 10.098], loss: 0.106095, mae: 0.315826, mean_q: 3.876531
  9900/100000: episode: 99, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 198.183, mean reward: 1.982 [1.467, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.569, 10.098], loss: 0.114809, mae: 0.327737, mean_q: 3.881696
 10000/100000: episode: 100, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.467, mean reward: 1.905 [1.477, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.859, 10.268], loss: 0.106704, mae: 0.316030, mean_q: 3.858025
 10100/100000: episode: 101, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.856, mean reward: 1.889 [1.480, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.374, 10.108], loss: 0.119613, mae: 0.333285, mean_q: 3.891962
 10200/100000: episode: 102, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 196.684, mean reward: 1.967 [1.452, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.961, 10.162], loss: 0.097544, mae: 0.313051, mean_q: 3.849366
 10300/100000: episode: 103, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 193.171, mean reward: 1.932 [1.504, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.968, 10.098], loss: 0.098757, mae: 0.301798, mean_q: 3.841533
 10400/100000: episode: 104, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 183.448, mean reward: 1.834 [1.470, 3.342], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.274, 10.098], loss: 0.094508, mae: 0.303228, mean_q: 3.855887
 10500/100000: episode: 105, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 215.928, mean reward: 2.159 [1.500, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.704, 10.114], loss: 0.096526, mae: 0.313905, mean_q: 3.856052
 10600/100000: episode: 106, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.881, mean reward: 1.929 [1.487, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.292, 10.098], loss: 0.102114, mae: 0.312734, mean_q: 3.862877
 10700/100000: episode: 107, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 184.590, mean reward: 1.846 [1.490, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.470, 10.274], loss: 0.108301, mae: 0.320261, mean_q: 3.877052
 10800/100000: episode: 108, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 198.172, mean reward: 1.982 [1.466, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.736, 10.098], loss: 0.102998, mae: 0.314437, mean_q: 3.863609
 10900/100000: episode: 109, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 189.185, mean reward: 1.892 [1.447, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.626, 10.221], loss: 0.100181, mae: 0.312807, mean_q: 3.872148
 11000/100000: episode: 110, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 194.094, mean reward: 1.941 [1.467, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.626, 10.147], loss: 0.103250, mae: 0.321536, mean_q: 3.877465
 11100/100000: episode: 111, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 206.684, mean reward: 2.067 [1.450, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.030, 10.262], loss: 0.094609, mae: 0.303970, mean_q: 3.858235
 11200/100000: episode: 112, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 216.729, mean reward: 2.167 [1.501, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.144, 10.388], loss: 0.095058, mae: 0.302813, mean_q: 3.850378
 11300/100000: episode: 113, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 218.375, mean reward: 2.184 [1.481, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.733, 10.261], loss: 0.095612, mae: 0.312326, mean_q: 3.874156
 11400/100000: episode: 114, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 186.587, mean reward: 1.866 [1.438, 4.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.322, 10.223], loss: 0.115239, mae: 0.335401, mean_q: 3.887908
 11500/100000: episode: 115, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 191.137, mean reward: 1.911 [1.498, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.808, 10.098], loss: 0.093903, mae: 0.308720, mean_q: 3.868505
 11600/100000: episode: 116, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 190.380, mean reward: 1.904 [1.446, 5.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.116, 10.198], loss: 0.100548, mae: 0.314708, mean_q: 3.854981
 11700/100000: episode: 117, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 188.150, mean reward: 1.881 [1.458, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.833, 10.141], loss: 0.100197, mae: 0.315614, mean_q: 3.878181
 11800/100000: episode: 118, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 192.863, mean reward: 1.929 [1.449, 4.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.050, 10.098], loss: 0.105396, mae: 0.327438, mean_q: 3.870851
 11900/100000: episode: 119, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 186.017, mean reward: 1.860 [1.459, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.260, 10.098], loss: 0.097161, mae: 0.307627, mean_q: 3.862880
 12000/100000: episode: 120, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 179.482, mean reward: 1.795 [1.477, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.270, 10.173], loss: 0.097321, mae: 0.313682, mean_q: 3.871029
 12100/100000: episode: 121, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 220.347, mean reward: 2.203 [1.464, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.727, 10.098], loss: 0.093837, mae: 0.307688, mean_q: 3.858418
 12200/100000: episode: 122, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 194.123, mean reward: 1.941 [1.450, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.632, 10.098], loss: 0.087159, mae: 0.297888, mean_q: 3.855148
 12300/100000: episode: 123, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 190.945, mean reward: 1.909 [1.439, 2.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.655, 10.098], loss: 0.093315, mae: 0.311945, mean_q: 3.858483
 12400/100000: episode: 124, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 183.209, mean reward: 1.832 [1.458, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.571, 10.188], loss: 0.090379, mae: 0.299704, mean_q: 3.852829
 12500/100000: episode: 125, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 192.156, mean reward: 1.922 [1.447, 7.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.086, 10.098], loss: 0.094602, mae: 0.310896, mean_q: 3.858561
 12600/100000: episode: 126, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 201.820, mean reward: 2.018 [1.465, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.370, 10.139], loss: 0.109595, mae: 0.322100, mean_q: 3.859718
 12700/100000: episode: 127, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.006, mean reward: 1.950 [1.443, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.765, 10.392], loss: 0.102039, mae: 0.313749, mean_q: 3.863545
 12800/100000: episode: 128, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 193.468, mean reward: 1.935 [1.437, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.457, 10.098], loss: 0.095290, mae: 0.307792, mean_q: 3.836958
 12900/100000: episode: 129, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 186.722, mean reward: 1.867 [1.450, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.828, 10.098], loss: 0.085643, mae: 0.295371, mean_q: 3.843063
 13000/100000: episode: 130, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 205.596, mean reward: 2.056 [1.479, 2.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.150, 10.275], loss: 0.090855, mae: 0.303183, mean_q: 3.832378
 13100/100000: episode: 131, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 205.930, mean reward: 2.059 [1.466, 5.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.184, 10.098], loss: 0.100739, mae: 0.311466, mean_q: 3.828387
 13200/100000: episode: 132, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 193.790, mean reward: 1.938 [1.475, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.721, 10.098], loss: 0.095082, mae: 0.297282, mean_q: 3.820447
 13300/100000: episode: 133, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 198.163, mean reward: 1.982 [1.450, 4.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.808, 10.098], loss: 0.080360, mae: 0.290391, mean_q: 3.831573
 13400/100000: episode: 134, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 186.196, mean reward: 1.862 [1.471, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.525, 10.098], loss: 0.099082, mae: 0.298407, mean_q: 3.826375
 13500/100000: episode: 135, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 195.837, mean reward: 1.958 [1.523, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.089, 10.098], loss: 0.094358, mae: 0.306957, mean_q: 3.834020
 13600/100000: episode: 136, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 192.508, mean reward: 1.925 [1.492, 4.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.454, 10.098], loss: 0.091570, mae: 0.301976, mean_q: 3.833352
 13700/100000: episode: 137, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 204.449, mean reward: 2.044 [1.472, 4.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.704, 10.098], loss: 0.100539, mae: 0.311449, mean_q: 3.840359
 13800/100000: episode: 138, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 186.690, mean reward: 1.867 [1.457, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.186, 10.098], loss: 0.095919, mae: 0.313141, mean_q: 3.838148
 13900/100000: episode: 139, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.894, mean reward: 1.889 [1.445, 4.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.786, 10.098], loss: 0.096669, mae: 0.306331, mean_q: 3.844732
 14000/100000: episode: 140, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.038, mean reward: 1.890 [1.462, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.455, 10.163], loss: 0.098994, mae: 0.307764, mean_q: 3.835060
 14100/100000: episode: 141, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 202.895, mean reward: 2.029 [1.508, 3.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.216, 10.373], loss: 0.097369, mae: 0.309802, mean_q: 3.827008
 14200/100000: episode: 142, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 185.935, mean reward: 1.859 [1.467, 3.031], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.669, 10.098], loss: 0.098540, mae: 0.307623, mean_q: 3.836956
 14300/100000: episode: 143, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 181.108, mean reward: 1.811 [1.458, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.188, 10.098], loss: 0.082492, mae: 0.290506, mean_q: 3.828245
 14400/100000: episode: 144, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.160, mean reward: 1.972 [1.441, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.284, 10.098], loss: 0.091835, mae: 0.297632, mean_q: 3.854389
 14500/100000: episode: 145, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 190.231, mean reward: 1.902 [1.521, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.603, 10.208], loss: 0.095834, mae: 0.312986, mean_q: 3.835232
 14600/100000: episode: 146, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 195.474, mean reward: 1.955 [1.477, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.281, 10.099], loss: 0.099814, mae: 0.308178, mean_q: 3.820051
 14700/100000: episode: 147, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 191.136, mean reward: 1.911 [1.495, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.737, 10.098], loss: 0.093842, mae: 0.301051, mean_q: 3.836160
 14800/100000: episode: 148, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 197.861, mean reward: 1.979 [1.470, 5.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.482, 10.178], loss: 0.109581, mae: 0.316587, mean_q: 3.848912
 14900/100000: episode: 149, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 184.704, mean reward: 1.847 [1.444, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.486, 10.241], loss: 0.092589, mae: 0.300616, mean_q: 3.839896
[Info] 1-TH LEVEL FOUND: 4.707712650299072, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.133s, episode steps: 100, steps per second: 19, episode reward: 202.234, mean reward: 2.022 [1.481, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.719, 10.098], loss: 0.109360, mae: 0.318638, mean_q: 3.842112
 15052/100000: episode: 151, duration: 0.284s, episode steps: 52, steps per second: 183, episode reward: 125.371, mean reward: 2.411 [1.450, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.874 [-1.484, 10.100], loss: 0.090728, mae: 0.302026, mean_q: 3.827411
 15103/100000: episode: 152, duration: 0.275s, episode steps: 51, steps per second: 185, episode reward: 111.897, mean reward: 2.194 [1.510, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.299, 10.100], loss: 0.120769, mae: 0.336654, mean_q: 3.881409
 15154/100000: episode: 153, duration: 0.286s, episode steps: 51, steps per second: 178, episode reward: 107.793, mean reward: 2.114 [1.574, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.749, 10.350], loss: 0.086842, mae: 0.296099, mean_q: 3.833835
 15209/100000: episode: 154, duration: 0.290s, episode steps: 55, steps per second: 190, episode reward: 141.771, mean reward: 2.578 [1.512, 4.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.379, 10.206], loss: 0.090213, mae: 0.308835, mean_q: 3.867440
 15260/100000: episode: 155, duration: 0.288s, episode steps: 51, steps per second: 177, episode reward: 121.150, mean reward: 2.375 [1.447, 4.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-1.020, 10.173], loss: 0.114818, mae: 0.334987, mean_q: 3.874187
 15312/100000: episode: 156, duration: 0.289s, episode steps: 52, steps per second: 180, episode reward: 119.591, mean reward: 2.300 [1.576, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.863 [-0.282, 10.100], loss: 0.100423, mae: 0.316919, mean_q: 3.864119
 15367/100000: episode: 157, duration: 0.298s, episode steps: 55, steps per second: 185, episode reward: 134.446, mean reward: 2.444 [1.732, 4.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.355, 10.285], loss: 0.127040, mae: 0.337990, mean_q: 3.880157
 15418/100000: episode: 158, duration: 0.278s, episode steps: 51, steps per second: 183, episode reward: 114.441, mean reward: 2.244 [1.470, 4.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.426, 10.101], loss: 0.104698, mae: 0.329043, mean_q: 3.905751
 15469/100000: episode: 159, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 94.695, mean reward: 1.857 [1.467, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.887, 10.173], loss: 0.115554, mae: 0.334962, mean_q: 3.898620
 15521/100000: episode: 160, duration: 0.283s, episode steps: 52, steps per second: 184, episode reward: 119.472, mean reward: 2.298 [1.582, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 1.860 [-0.546, 10.100], loss: 0.098558, mae: 0.315673, mean_q: 3.885849
 15572/100000: episode: 161, duration: 0.293s, episode steps: 51, steps per second: 174, episode reward: 100.831, mean reward: 1.977 [1.593, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.678, 10.100], loss: 0.090586, mae: 0.311001, mean_q: 3.899899
 15623/100000: episode: 162, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 95.877, mean reward: 1.880 [1.461, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.846, 10.137], loss: 0.102250, mae: 0.335080, mean_q: 3.909062
 15674/100000: episode: 163, duration: 0.275s, episode steps: 51, steps per second: 186, episode reward: 101.125, mean reward: 1.983 [1.462, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.617, 10.168], loss: 0.095563, mae: 0.314701, mean_q: 3.874830
 15729/100000: episode: 164, duration: 0.313s, episode steps: 55, steps per second: 176, episode reward: 123.587, mean reward: 2.247 [1.443, 3.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.859 [-0.336, 10.181], loss: 0.093738, mae: 0.313168, mean_q: 3.904039
 15780/100000: episode: 165, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 127.493, mean reward: 2.500 [1.837, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.470, 10.278], loss: 0.108663, mae: 0.326157, mean_q: 3.924613
 15832/100000: episode: 166, duration: 0.271s, episode steps: 52, steps per second: 192, episode reward: 101.428, mean reward: 1.951 [1.511, 2.642], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-0.508, 10.100], loss: 0.115078, mae: 0.341410, mean_q: 3.948926
 15883/100000: episode: 167, duration: 0.273s, episode steps: 51, steps per second: 187, episode reward: 143.540, mean reward: 2.815 [1.806, 6.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.486, 10.278], loss: 0.106109, mae: 0.336813, mean_q: 3.932631
[Info] FALSIFICATION!
[Info] Levels: [4.7077127, 4.4656773]
[Info] Cond. Prob: [0.1, 0.2]
[Info] Error Prob: 0.020000000000000004

 15887/100000: episode: 168, duration: 4.648s, episode steps: 4, steps per second: 1, episode reward: 122.331, mean reward: 30.583 [2.202, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.256, 5.977], loss: 0.066270, mae: 0.252915, mean_q: 3.805161
 15987/100000: episode: 169, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: 182.808, mean reward: 1.828 [1.460, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.955, 10.098], loss: 1.621909, mae: 0.442264, mean_q: 3.965122
 16087/100000: episode: 170, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 199.366, mean reward: 1.994 [1.475, 5.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.763, 10.098], loss: 0.177150, mae: 0.359281, mean_q: 3.950016
 16187/100000: episode: 171, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 237.678, mean reward: 2.377 [1.446, 6.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.883, 10.481], loss: 1.558587, mae: 0.412664, mean_q: 3.974596
 16287/100000: episode: 172, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 184.510, mean reward: 1.845 [1.441, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.026, 10.287], loss: 0.152262, mae: 0.370782, mean_q: 3.941021
 16387/100000: episode: 173, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 195.604, mean reward: 1.956 [1.452, 2.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.204, 10.284], loss: 1.631595, mae: 0.457309, mean_q: 4.009146
 16487/100000: episode: 174, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 191.634, mean reward: 1.916 [1.482, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.543, 10.098], loss: 0.169765, mae: 0.395420, mean_q: 3.982668
 16587/100000: episode: 175, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 194.327, mean reward: 1.943 [1.447, 2.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.314, 10.098], loss: 4.326397, mae: 0.566877, mean_q: 4.053012
 16687/100000: episode: 176, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 191.905, mean reward: 1.919 [1.460, 4.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.714, 10.098], loss: 0.187615, mae: 0.358901, mean_q: 3.964239
 16787/100000: episode: 177, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 190.941, mean reward: 1.909 [1.463, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.265, 10.173], loss: 1.583297, mae: 0.434499, mean_q: 3.963837
 16887/100000: episode: 178, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 195.414, mean reward: 1.954 [1.512, 5.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.258, 10.098], loss: 1.524421, mae: 0.411531, mean_q: 3.997621
 16987/100000: episode: 179, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 193.656, mean reward: 1.937 [1.445, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.329, 10.099], loss: 0.140864, mae: 0.366987, mean_q: 3.905958
 17087/100000: episode: 180, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 206.116, mean reward: 2.061 [1.438, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.160, 10.098], loss: 0.123968, mae: 0.343595, mean_q: 3.941354
 17187/100000: episode: 181, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 205.817, mean reward: 2.058 [1.452, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.711, 10.098], loss: 1.532572, mae: 0.421058, mean_q: 3.980121
 17287/100000: episode: 182, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 213.326, mean reward: 2.133 [1.491, 4.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.513, 10.098], loss: 0.136545, mae: 0.351119, mean_q: 3.947114
 17387/100000: episode: 183, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 212.105, mean reward: 2.121 [1.458, 4.233], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.642, 10.294], loss: 0.268327, mae: 0.382374, mean_q: 3.998266
 17487/100000: episode: 184, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 205.573, mean reward: 2.056 [1.485, 6.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.779, 10.342], loss: 2.881455, mae: 0.500123, mean_q: 4.053521
 17587/100000: episode: 185, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 202.168, mean reward: 2.022 [1.465, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.447, 10.098], loss: 0.205446, mae: 0.391282, mean_q: 3.945097
 17687/100000: episode: 186, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 182.537, mean reward: 1.825 [1.448, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.884, 10.134], loss: 1.526164, mae: 0.419280, mean_q: 3.945552
 17787/100000: episode: 187, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 224.656, mean reward: 2.247 [1.448, 4.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.069, 10.110], loss: 0.147982, mae: 0.368140, mean_q: 3.935728
 17887/100000: episode: 188, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 230.463, mean reward: 2.305 [1.446, 5.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.281, 10.098], loss: 0.146577, mae: 0.365942, mean_q: 3.948146
 17987/100000: episode: 189, duration: 0.576s, episode steps: 100, steps per second: 174, episode reward: 191.254, mean reward: 1.913 [1.459, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.778, 10.147], loss: 1.518412, mae: 0.416497, mean_q: 3.984044
 18087/100000: episode: 190, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 196.130, mean reward: 1.961 [1.468, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.612, 10.126], loss: 0.241928, mae: 0.424213, mean_q: 4.039808
 18187/100000: episode: 191, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 230.990, mean reward: 2.310 [1.481, 5.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.235, 10.399], loss: 1.521819, mae: 0.429939, mean_q: 4.002748
 18287/100000: episode: 192, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 197.128, mean reward: 1.971 [1.451, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.730, 10.098], loss: 1.489529, mae: 0.409447, mean_q: 4.011641
 18387/100000: episode: 193, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 186.645, mean reward: 1.866 [1.438, 4.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.709, 10.098], loss: 0.274012, mae: 0.417934, mean_q: 4.040474
 18487/100000: episode: 194, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 180.734, mean reward: 1.807 [1.461, 3.560], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.333, 10.098], loss: 2.793067, mae: 0.432249, mean_q: 4.030948
 18587/100000: episode: 195, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 198.770, mean reward: 1.988 [1.441, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.353, 10.098], loss: 1.667080, mae: 0.519528, mean_q: 4.052412
 18687/100000: episode: 196, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 238.476, mean reward: 2.385 [1.432, 6.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.557, 10.307], loss: 1.560899, mae: 0.440413, mean_q: 4.042028
 18787/100000: episode: 197, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 189.571, mean reward: 1.896 [1.430, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.238, 10.098], loss: 2.798745, mae: 0.509806, mean_q: 4.102450
 18887/100000: episode: 198, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 186.370, mean reward: 1.864 [1.443, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.223, 10.154], loss: 0.169983, mae: 0.400963, mean_q: 4.059695
 18987/100000: episode: 199, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 194.278, mean reward: 1.943 [1.439, 3.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.256, 10.123], loss: 0.201454, mae: 0.386847, mean_q: 4.031549
 19087/100000: episode: 200, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 199.851, mean reward: 1.999 [1.441, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.389, 10.098], loss: 2.815832, mae: 0.487272, mean_q: 4.097882
 19187/100000: episode: 201, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 183.950, mean reward: 1.840 [1.445, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.745, 10.098], loss: 1.499366, mae: 0.450358, mean_q: 4.096519
 19287/100000: episode: 202, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 203.818, mean reward: 2.038 [1.481, 3.903], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.003, 10.098], loss: 0.150313, mae: 0.363733, mean_q: 4.022532
 19387/100000: episode: 203, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 187.860, mean reward: 1.879 [1.491, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.618, 10.098], loss: 1.478303, mae: 0.420523, mean_q: 4.061353
 19487/100000: episode: 204, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.556, mean reward: 1.886 [1.458, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.097, 10.098], loss: 0.207252, mae: 0.381510, mean_q: 4.031356
 19587/100000: episode: 205, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 208.337, mean reward: 2.083 [1.455, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.829, 10.098], loss: 4.015903, mae: 0.569813, mean_q: 4.107720
 19687/100000: episode: 206, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 179.129, mean reward: 1.791 [1.445, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.545, 10.098], loss: 2.917264, mae: 0.493816, mean_q: 4.090829
 19787/100000: episode: 207, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 181.987, mean reward: 1.820 [1.481, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.807, 10.165], loss: 2.708802, mae: 0.482710, mean_q: 4.085756
 19887/100000: episode: 208, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 227.876, mean reward: 2.279 [1.483, 6.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.338, 10.486], loss: 0.182045, mae: 0.392103, mean_q: 4.023067
 19987/100000: episode: 209, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 200.165, mean reward: 2.002 [1.434, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.597, 10.120], loss: 0.196955, mae: 0.382517, mean_q: 4.032677
 20087/100000: episode: 210, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 194.119, mean reward: 1.941 [1.479, 3.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.156, 10.101], loss: 0.270194, mae: 0.404471, mean_q: 4.053802
 20187/100000: episode: 211, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 198.895, mean reward: 1.989 [1.441, 3.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.817, 10.282], loss: 1.510743, mae: 0.426743, mean_q: 4.066784
 20287/100000: episode: 212, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 188.652, mean reward: 1.887 [1.446, 4.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.497, 10.098], loss: 0.187031, mae: 0.362309, mean_q: 3.991824
 20387/100000: episode: 213, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 189.823, mean reward: 1.898 [1.459, 3.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.942, 10.098], loss: 0.140562, mae: 0.353157, mean_q: 3.975428
 20487/100000: episode: 214, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 195.489, mean reward: 1.955 [1.474, 4.445], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.747, 10.186], loss: 0.122711, mae: 0.342481, mean_q: 3.960861
 20587/100000: episode: 215, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 195.731, mean reward: 1.957 [1.444, 5.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.957, 10.098], loss: 1.509982, mae: 0.415962, mean_q: 4.020697
 20687/100000: episode: 216, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 199.341, mean reward: 1.993 [1.476, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.642, 10.098], loss: 1.503170, mae: 0.438038, mean_q: 4.000988
 20787/100000: episode: 217, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 212.951, mean reward: 2.130 [1.475, 4.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.276, 10.098], loss: 0.215889, mae: 0.381796, mean_q: 3.972977
 20887/100000: episode: 218, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 184.876, mean reward: 1.849 [1.440, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.474, 10.177], loss: 0.144065, mae: 0.363718, mean_q: 3.940488
 20987/100000: episode: 219, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 193.148, mean reward: 1.931 [1.500, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.573, 10.098], loss: 0.126802, mae: 0.346298, mean_q: 3.949377
 21087/100000: episode: 220, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 179.688, mean reward: 1.797 [1.454, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.894, 10.250], loss: 0.134102, mae: 0.350793, mean_q: 3.945856
 21187/100000: episode: 221, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 218.518, mean reward: 2.185 [1.496, 6.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.094, 10.098], loss: 0.117752, mae: 0.336170, mean_q: 3.915792
 21287/100000: episode: 222, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 187.061, mean reward: 1.871 [1.459, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.028, 10.098], loss: 0.127588, mae: 0.349990, mean_q: 3.939470
 21387/100000: episode: 223, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 183.214, mean reward: 1.832 [1.485, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.431, 10.098], loss: 0.119692, mae: 0.343594, mean_q: 3.917891
 21487/100000: episode: 224, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 189.793, mean reward: 1.898 [1.434, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.681, 10.324], loss: 0.114681, mae: 0.337354, mean_q: 3.909261
 21587/100000: episode: 225, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 210.090, mean reward: 2.101 [1.526, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.901, 10.374], loss: 0.114742, mae: 0.337585, mean_q: 3.921649
 21687/100000: episode: 226, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 188.590, mean reward: 1.886 [1.451, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.473, 10.155], loss: 0.120732, mae: 0.336747, mean_q: 3.924434
 21787/100000: episode: 227, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 196.208, mean reward: 1.962 [1.509, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.541, 10.098], loss: 0.123276, mae: 0.341048, mean_q: 3.932044
 21887/100000: episode: 228, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 228.964, mean reward: 2.290 [1.486, 5.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.823, 10.433], loss: 0.126164, mae: 0.346704, mean_q: 3.948935
 21987/100000: episode: 229, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 188.733, mean reward: 1.887 [1.474, 4.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.888, 10.279], loss: 0.117334, mae: 0.333859, mean_q: 3.932917
 22087/100000: episode: 230, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.130, mean reward: 1.821 [1.451, 2.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.099, 10.162], loss: 0.133332, mae: 0.351360, mean_q: 3.945248
 22187/100000: episode: 231, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 191.267, mean reward: 1.913 [1.468, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.546, 10.109], loss: 0.124441, mae: 0.338035, mean_q: 3.928046
 22287/100000: episode: 232, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 189.405, mean reward: 1.894 [1.491, 4.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.549, 10.098], loss: 0.115403, mae: 0.337114, mean_q: 3.925889
 22387/100000: episode: 233, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 186.692, mean reward: 1.867 [1.451, 2.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.316, 10.098], loss: 0.109730, mae: 0.326931, mean_q: 3.900467
 22487/100000: episode: 234, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 202.710, mean reward: 2.027 [1.487, 3.930], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.286, 10.098], loss: 0.112153, mae: 0.330889, mean_q: 3.899194
 22587/100000: episode: 235, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 187.590, mean reward: 1.876 [1.456, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.044, 10.098], loss: 0.110981, mae: 0.333794, mean_q: 3.893643
 22687/100000: episode: 236, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 199.591, mean reward: 1.996 [1.461, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.694, 10.180], loss: 0.104357, mae: 0.323115, mean_q: 3.895592
 22787/100000: episode: 237, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 205.575, mean reward: 2.056 [1.473, 8.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.049, 10.258], loss: 0.134293, mae: 0.344307, mean_q: 3.908992
 22887/100000: episode: 238, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 202.100, mean reward: 2.021 [1.447, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.381, 10.178], loss: 0.109438, mae: 0.324219, mean_q: 3.887470
 22987/100000: episode: 239, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 242.926, mean reward: 2.429 [1.476, 4.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.557, 10.521], loss: 0.118666, mae: 0.333874, mean_q: 3.895925
 23087/100000: episode: 240, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 188.517, mean reward: 1.885 [1.459, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.171, 10.160], loss: 0.118840, mae: 0.341226, mean_q: 3.923919
 23187/100000: episode: 241, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 195.269, mean reward: 1.953 [1.487, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.409, 10.098], loss: 0.128963, mae: 0.344239, mean_q: 3.905539
 23287/100000: episode: 242, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 199.542, mean reward: 1.995 [1.510, 4.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.840, 10.202], loss: 0.102723, mae: 0.323237, mean_q: 3.896077
 23387/100000: episode: 243, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 180.741, mean reward: 1.807 [1.453, 5.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.422, 10.267], loss: 0.108167, mae: 0.324555, mean_q: 3.893199
 23487/100000: episode: 244, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 180.927, mean reward: 1.809 [1.464, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.132, 10.098], loss: 0.114723, mae: 0.335033, mean_q: 3.913903
 23587/100000: episode: 245, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 179.020, mean reward: 1.790 [1.463, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.442, 10.098], loss: 0.106341, mae: 0.324521, mean_q: 3.897714
 23687/100000: episode: 246, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 187.781, mean reward: 1.878 [1.446, 4.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.642, 10.169], loss: 0.110814, mae: 0.319962, mean_q: 3.895475
 23787/100000: episode: 247, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 195.767, mean reward: 1.958 [1.455, 5.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.245, 10.187], loss: 0.105533, mae: 0.316471, mean_q: 3.844115
 23887/100000: episode: 248, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 197.615, mean reward: 1.976 [1.449, 4.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.136, 10.449], loss: 0.119057, mae: 0.319377, mean_q: 3.849723
 23987/100000: episode: 249, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 224.463, mean reward: 2.245 [1.472, 5.488], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.736, 10.598], loss: 0.093867, mae: 0.308107, mean_q: 3.842431
 24087/100000: episode: 250, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 190.585, mean reward: 1.906 [1.441, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.228, 10.264], loss: 0.116607, mae: 0.330369, mean_q: 3.856816
 24187/100000: episode: 251, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 180.025, mean reward: 1.800 [1.458, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.570, 10.119], loss: 0.111715, mae: 0.324123, mean_q: 3.871305
 24287/100000: episode: 252, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 189.206, mean reward: 1.892 [1.464, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.658, 10.098], loss: 0.110023, mae: 0.320242, mean_q: 3.848428
 24387/100000: episode: 253, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 197.473, mean reward: 1.975 [1.443, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.631, 10.098], loss: 0.115095, mae: 0.324078, mean_q: 3.866719
 24487/100000: episode: 254, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 201.020, mean reward: 2.010 [1.447, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.082, 10.098], loss: 0.105580, mae: 0.327346, mean_q: 3.857411
 24587/100000: episode: 255, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 219.417, mean reward: 2.194 [1.469, 5.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.797, 10.116], loss: 0.115682, mae: 0.326907, mean_q: 3.881419
 24687/100000: episode: 256, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 199.887, mean reward: 1.999 [1.530, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.908, 10.139], loss: 0.108852, mae: 0.322463, mean_q: 3.887756
 24787/100000: episode: 257, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.010, mean reward: 1.900 [1.472, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.647, 10.115], loss: 0.124092, mae: 0.338031, mean_q: 3.875815
 24887/100000: episode: 258, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 199.379, mean reward: 1.994 [1.524, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.590, 10.162], loss: 0.124109, mae: 0.338388, mean_q: 3.908843
 24987/100000: episode: 259, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 210.569, mean reward: 2.106 [1.461, 4.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.640, 10.098], loss: 0.100780, mae: 0.310287, mean_q: 3.861123
 25087/100000: episode: 260, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 196.244, mean reward: 1.962 [1.456, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.130, 10.387], loss: 0.101031, mae: 0.314579, mean_q: 3.879547
 25187/100000: episode: 261, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.797, mean reward: 1.968 [1.456, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.187, 10.098], loss: 0.141660, mae: 0.346782, mean_q: 3.889981
 25287/100000: episode: 262, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 189.328, mean reward: 1.893 [1.451, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.186, 10.213], loss: 0.110192, mae: 0.321644, mean_q: 3.896059
 25387/100000: episode: 263, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.559, mean reward: 1.876 [1.465, 3.028], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.613, 10.098], loss: 0.120002, mae: 0.326633, mean_q: 3.878683
 25487/100000: episode: 264, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 184.030, mean reward: 1.840 [1.457, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.651, 10.098], loss: 0.118235, mae: 0.328430, mean_q: 3.886638
 25587/100000: episode: 265, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 198.025, mean reward: 1.980 [1.487, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.664, 10.098], loss: 0.106226, mae: 0.319672, mean_q: 3.883333
 25687/100000: episode: 266, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 197.447, mean reward: 1.974 [1.473, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.322, 10.098], loss: 0.100455, mae: 0.311126, mean_q: 3.893728
 25787/100000: episode: 267, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 188.788, mean reward: 1.888 [1.443, 4.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.023, 10.264], loss: 0.101564, mae: 0.317976, mean_q: 3.876145
[Info] 1-TH LEVEL FOUND: 4.9384541511535645, Considering 10/90 traces
 25887/100000: episode: 268, duration: 4.759s, episode steps: 100, steps per second: 21, episode reward: 199.861, mean reward: 1.999 [1.493, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.609, 10.098], loss: 0.109698, mae: 0.317071, mean_q: 3.861503
 25914/100000: episode: 269, duration: 0.155s, episode steps: 27, steps per second: 174, episode reward: 70.094, mean reward: 2.596 [1.835, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.604, 10.270], loss: 0.161461, mae: 0.368028, mean_q: 3.921574
 25998/100000: episode: 270, duration: 0.478s, episode steps: 84, steps per second: 176, episode reward: 165.179, mean reward: 1.966 [1.476, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.603 [-1.312, 10.100], loss: 0.103768, mae: 0.315962, mean_q: 3.888733
 26093/100000: episode: 271, duration: 0.493s, episode steps: 95, steps per second: 193, episode reward: 183.468, mean reward: 1.931 [1.482, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.454, 10.100], loss: 0.100292, mae: 0.308458, mean_q: 3.870125
 26177/100000: episode: 272, duration: 0.456s, episode steps: 84, steps per second: 184, episode reward: 162.198, mean reward: 1.931 [1.481, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 1.604 [-0.889, 10.100], loss: 0.119087, mae: 0.328513, mean_q: 3.881513
 26274/100000: episode: 273, duration: 0.536s, episode steps: 97, steps per second: 181, episode reward: 176.841, mean reward: 1.823 [1.477, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-0.602, 10.264], loss: 0.115142, mae: 0.323956, mean_q: 3.884276
 26323/100000: episode: 274, duration: 0.258s, episode steps: 49, steps per second: 190, episode reward: 101.306, mean reward: 2.067 [1.462, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.605, 10.100], loss: 0.105373, mae: 0.319945, mean_q: 3.897577
 26350/100000: episode: 275, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 58.751, mean reward: 2.176 [1.713, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.701, 10.219], loss: 0.091162, mae: 0.306062, mean_q: 3.897380
 26449/100000: episode: 276, duration: 0.549s, episode steps: 99, steps per second: 180, episode reward: 179.215, mean reward: 1.810 [1.434, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.819, 10.150], loss: 0.120308, mae: 0.325742, mean_q: 3.889134
 26498/100000: episode: 277, duration: 0.272s, episode steps: 49, steps per second: 180, episode reward: 208.198, mean reward: 4.249 [2.395, 9.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.908 [-0.369, 10.593], loss: 0.097417, mae: 0.312036, mean_q: 3.895890
 26525/100000: episode: 278, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 86.396, mean reward: 3.200 [2.431, 4.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.557, 10.415], loss: 0.171297, mae: 0.358517, mean_q: 3.919386
 26622/100000: episode: 279, duration: 0.548s, episode steps: 97, steps per second: 177, episode reward: 198.609, mean reward: 2.048 [1.450, 4.850], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-1.269, 10.100], loss: 0.134293, mae: 0.340800, mean_q: 3.920451
 26706/100000: episode: 280, duration: 0.462s, episode steps: 84, steps per second: 182, episode reward: 170.115, mean reward: 2.025 [1.458, 4.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.602 [-0.269, 10.121], loss: 0.138660, mae: 0.337060, mean_q: 3.908412
 26755/100000: episode: 281, duration: 0.265s, episode steps: 49, steps per second: 185, episode reward: 127.121, mean reward: 2.594 [1.930, 4.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.427, 10.259], loss: 0.128776, mae: 0.342609, mean_q: 3.941139
 26850/100000: episode: 282, duration: 0.548s, episode steps: 95, steps per second: 173, episode reward: 189.344, mean reward: 1.993 [1.501, 4.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.490, 10.342], loss: 0.119882, mae: 0.332429, mean_q: 3.916201
 26888/100000: episode: 283, duration: 0.203s, episode steps: 38, steps per second: 187, episode reward: 179.003, mean reward: 4.711 [2.891, 12.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.851, 10.480], loss: 0.123743, mae: 0.345252, mean_q: 3.954866
 26983/100000: episode: 284, duration: 0.530s, episode steps: 95, steps per second: 179, episode reward: 178.678, mean reward: 1.881 [1.480, 2.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.409, 10.100], loss: 0.168447, mae: 0.371812, mean_q: 3.954700
 27082/100000: episode: 285, duration: 0.509s, episode steps: 99, steps per second: 194, episode reward: 199.815, mean reward: 2.018 [1.477, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-1.156, 10.278], loss: 0.168007, mae: 0.369480, mean_q: 3.974567
 27119/100000: episode: 286, duration: 0.188s, episode steps: 37, steps per second: 197, episode reward: 107.172, mean reward: 2.897 [1.583, 8.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.169, 10.115], loss: 0.175861, mae: 0.364750, mean_q: 3.980895
 27214/100000: episode: 287, duration: 0.514s, episode steps: 95, steps per second: 185, episode reward: 174.345, mean reward: 1.835 [1.448, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.409, 10.189], loss: 0.198038, mae: 0.377410, mean_q: 3.986505
 27309/100000: episode: 288, duration: 0.515s, episode steps: 95, steps per second: 185, episode reward: 172.815, mean reward: 1.819 [1.463, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.929, 10.133], loss: 0.151745, mae: 0.356272, mean_q: 4.004493
 27346/100000: episode: 289, duration: 0.197s, episode steps: 37, steps per second: 188, episode reward: 87.404, mean reward: 2.362 [1.514, 4.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.639, 10.141], loss: 0.154342, mae: 0.356600, mean_q: 4.017239
 27430/100000: episode: 290, duration: 0.459s, episode steps: 84, steps per second: 183, episode reward: 171.274, mean reward: 2.039 [1.484, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 1.598 [-0.650, 10.100], loss: 0.145794, mae: 0.348275, mean_q: 3.989313
 27529/100000: episode: 291, duration: 0.535s, episode steps: 99, steps per second: 185, episode reward: 192.518, mean reward: 1.945 [1.448, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-1.448, 10.278], loss: 0.147416, mae: 0.356453, mean_q: 3.990176
 27566/100000: episode: 292, duration: 0.213s, episode steps: 37, steps per second: 174, episode reward: 85.469, mean reward: 2.310 [1.554, 5.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-1.458, 10.325], loss: 0.227706, mae: 0.419827, mean_q: 4.028521
 27661/100000: episode: 293, duration: 0.500s, episode steps: 95, steps per second: 190, episode reward: 183.580, mean reward: 1.932 [1.495, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.324, 10.100], loss: 0.137750, mae: 0.353253, mean_q: 4.010283
 27760/100000: episode: 294, duration: 0.521s, episode steps: 99, steps per second: 190, episode reward: 189.373, mean reward: 1.913 [1.461, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-1.047, 10.327], loss: 0.132957, mae: 0.343287, mean_q: 4.007333
 27798/100000: episode: 295, duration: 0.207s, episode steps: 38, steps per second: 183, episode reward: 154.779, mean reward: 4.073 [1.575, 8.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.315, 10.328], loss: 0.162721, mae: 0.360791, mean_q: 4.030580
 27895/100000: episode: 296, duration: 0.519s, episode steps: 97, steps per second: 187, episode reward: 174.733, mean reward: 1.801 [1.458, 2.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.702, 10.213], loss: 0.195612, mae: 0.379544, mean_q: 4.035414
 27994/100000: episode: 297, duration: 0.536s, episode steps: 99, steps per second: 185, episode reward: 192.176, mean reward: 1.941 [1.456, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-1.668, 10.100], loss: 0.143623, mae: 0.366415, mean_q: 4.005219
 28078/100000: episode: 298, duration: 0.451s, episode steps: 84, steps per second: 186, episode reward: 160.322, mean reward: 1.909 [1.442, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.605 [-0.601, 10.100], loss: 0.134934, mae: 0.333537, mean_q: 4.006221
 28177/100000: episode: 299, duration: 0.534s, episode steps: 99, steps per second: 186, episode reward: 191.396, mean reward: 1.933 [1.458, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-1.219, 10.123], loss: 0.153826, mae: 0.343169, mean_q: 4.017096
 28261/100000: episode: 300, duration: 0.479s, episode steps: 84, steps per second: 176, episode reward: 162.194, mean reward: 1.931 [1.489, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.603 [-0.912, 10.100], loss: 0.161734, mae: 0.357501, mean_q: 3.990594
 28360/100000: episode: 301, duration: 0.526s, episode steps: 99, steps per second: 188, episode reward: 207.116, mean reward: 2.092 [1.463, 4.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.463 [-1.018, 10.113], loss: 0.189636, mae: 0.381675, mean_q: 4.055099
 28385/100000: episode: 302, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 64.140, mean reward: 2.566 [2.030, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.035, 10.430], loss: 0.165130, mae: 0.372412, mean_q: 4.045668
 28423/100000: episode: 303, duration: 0.213s, episode steps: 38, steps per second: 178, episode reward: 110.546, mean reward: 2.909 [1.459, 5.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.183, 10.152], loss: 0.126177, mae: 0.350524, mean_q: 4.006578
 28472/100000: episode: 304, duration: 0.260s, episode steps: 49, steps per second: 188, episode reward: 135.632, mean reward: 2.768 [1.959, 6.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.359, 10.388], loss: 0.141629, mae: 0.354763, mean_q: 4.027572
 28556/100000: episode: 305, duration: 0.456s, episode steps: 84, steps per second: 184, episode reward: 171.829, mean reward: 2.046 [1.471, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.610 [-1.129, 10.131], loss: 0.161836, mae: 0.366649, mean_q: 4.061696
 28593/100000: episode: 306, duration: 0.194s, episode steps: 37, steps per second: 190, episode reward: 98.930, mean reward: 2.674 [1.712, 4.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.035, 10.429], loss: 0.164237, mae: 0.367576, mean_q: 4.079979
 28620/100000: episode: 307, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 59.884, mean reward: 2.218 [1.589, 2.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.076, 10.267], loss: 0.247843, mae: 0.415258, mean_q: 4.079967
 28715/100000: episode: 308, duration: 0.535s, episode steps: 95, steps per second: 178, episode reward: 185.452, mean reward: 1.952 [1.489, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.496 [-0.760, 10.221], loss: 0.160802, mae: 0.383705, mean_q: 4.081132
 28753/100000: episode: 309, duration: 0.212s, episode steps: 38, steps per second: 179, episode reward: 115.212, mean reward: 3.032 [2.036, 5.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.200, 10.393], loss: 0.231242, mae: 0.407966, mean_q: 4.095430
 28852/100000: episode: 310, duration: 0.528s, episode steps: 99, steps per second: 187, episode reward: 183.383, mean reward: 1.852 [1.476, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.462 [-0.604, 10.235], loss: 0.157030, mae: 0.366084, mean_q: 4.079459
 28949/100000: episode: 311, duration: 0.530s, episode steps: 97, steps per second: 183, episode reward: 181.846, mean reward: 1.875 [1.478, 5.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-1.407, 10.100], loss: 0.151398, mae: 0.361716, mean_q: 4.067664
 29048/100000: episode: 312, duration: 0.546s, episode steps: 99, steps per second: 181, episode reward: 187.154, mean reward: 1.890 [1.454, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-0.613, 10.277], loss: 0.151419, mae: 0.364477, mean_q: 4.079026
 29073/100000: episode: 313, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 62.981, mean reward: 2.519 [2.089, 3.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-1.147, 10.380], loss: 0.172749, mae: 0.367375, mean_q: 4.059615
 29122/100000: episode: 314, duration: 0.265s, episode steps: 49, steps per second: 185, episode reward: 110.427, mean reward: 2.254 [1.633, 4.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-1.600, 10.265], loss: 0.174958, mae: 0.388974, mean_q: 4.147102
 29160/100000: episode: 315, duration: 0.196s, episode steps: 38, steps per second: 194, episode reward: 118.375, mean reward: 3.115 [1.488, 7.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-0.724, 10.100], loss: 0.175353, mae: 0.369104, mean_q: 4.122471
 29185/100000: episode: 316, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 62.786, mean reward: 2.511 [2.129, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.379, 10.461], loss: 0.167426, mae: 0.367331, mean_q: 4.062883
 29269/100000: episode: 317, duration: 0.460s, episode steps: 84, steps per second: 183, episode reward: 161.728, mean reward: 1.925 [1.463, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.612 [-1.098, 10.229], loss: 0.167914, mae: 0.373162, mean_q: 4.122939
 29296/100000: episode: 318, duration: 0.128s, episode steps: 27, steps per second: 210, episode reward: 75.215, mean reward: 2.786 [2.230, 4.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.255, 10.496], loss: 0.182923, mae: 0.378011, mean_q: 4.102827
 29380/100000: episode: 319, duration: 0.467s, episode steps: 84, steps per second: 180, episode reward: 170.105, mean reward: 2.025 [1.438, 4.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.609 [-0.940, 10.158], loss: 0.166534, mae: 0.382512, mean_q: 4.110154
 29407/100000: episode: 320, duration: 0.155s, episode steps: 27, steps per second: 175, episode reward: 70.919, mean reward: 2.627 [1.693, 4.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.059, 10.278], loss: 0.130294, mae: 0.353396, mean_q: 4.072819
 29432/100000: episode: 321, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 56.825, mean reward: 2.273 [1.805, 4.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.403], loss: 0.227108, mae: 0.404636, mean_q: 4.143886
 29529/100000: episode: 322, duration: 0.535s, episode steps: 97, steps per second: 181, episode reward: 173.416, mean reward: 1.788 [1.442, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.916, 10.116], loss: 0.153253, mae: 0.360845, mean_q: 4.122997
 29624/100000: episode: 323, duration: 0.539s, episode steps: 95, steps per second: 176, episode reward: 167.126, mean reward: 1.759 [1.438, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-0.873, 10.100], loss: 0.174154, mae: 0.360511, mean_q: 4.104833
 29662/100000: episode: 324, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 101.553, mean reward: 2.672 [1.888, 4.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.363, 10.370], loss: 0.122909, mae: 0.343007, mean_q: 4.073508
 29757/100000: episode: 325, duration: 0.526s, episode steps: 95, steps per second: 181, episode reward: 195.765, mean reward: 2.061 [1.559, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.507 [-0.249, 10.504], loss: 0.138828, mae: 0.349126, mean_q: 4.081548
 29856/100000: episode: 326, duration: 0.514s, episode steps: 99, steps per second: 193, episode reward: 205.593, mean reward: 2.077 [1.487, 4.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-1.040, 10.206], loss: 0.163794, mae: 0.369593, mean_q: 4.108459
 29893/100000: episode: 327, duration: 0.187s, episode steps: 37, steps per second: 198, episode reward: 122.230, mean reward: 3.304 [1.988, 5.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.051, 10.364], loss: 0.173094, mae: 0.366990, mean_q: 4.074911
 29918/100000: episode: 328, duration: 0.146s, episode steps: 25, steps per second: 172, episode reward: 62.774, mean reward: 2.511 [1.849, 4.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.276, 10.353], loss: 0.134214, mae: 0.345760, mean_q: 4.126741
 29956/100000: episode: 329, duration: 0.193s, episode steps: 38, steps per second: 197, episode reward: 104.312, mean reward: 2.745 [1.764, 6.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.365, 10.389], loss: 0.157518, mae: 0.374209, mean_q: 4.126086
 30051/100000: episode: 330, duration: 0.494s, episode steps: 95, steps per second: 192, episode reward: 189.011, mean reward: 1.990 [1.491, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-1.054, 10.244], loss: 0.168462, mae: 0.372130, mean_q: 4.156352
 30146/100000: episode: 331, duration: 0.482s, episode steps: 95, steps per second: 197, episode reward: 170.009, mean reward: 1.790 [1.456, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.368, 10.100], loss: 0.149093, mae: 0.357848, mean_q: 4.165748
 30195/100000: episode: 332, duration: 0.296s, episode steps: 49, steps per second: 165, episode reward: 145.635, mean reward: 2.972 [1.755, 6.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-1.280, 10.298], loss: 0.165646, mae: 0.374460, mean_q: 4.120784
 30244/100000: episode: 333, duration: 0.268s, episode steps: 49, steps per second: 183, episode reward: 170.921, mean reward: 3.488 [2.394, 7.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.102, 10.490], loss: 0.164126, mae: 0.383028, mean_q: 4.153522
 30282/100000: episode: 334, duration: 0.206s, episode steps: 38, steps per second: 184, episode reward: 101.965, mean reward: 2.683 [1.444, 6.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.284, 10.146], loss: 0.227094, mae: 0.412996, mean_q: 4.236773
 30320/100000: episode: 335, duration: 0.212s, episode steps: 38, steps per second: 179, episode reward: 115.081, mean reward: 3.028 [2.087, 5.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.669, 10.402], loss: 0.191103, mae: 0.402511, mean_q: 4.242879
 30415/100000: episode: 336, duration: 0.521s, episode steps: 95, steps per second: 182, episode reward: 189.998, mean reward: 2.000 [1.449, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.506 [-0.666, 10.359], loss: 0.188982, mae: 0.387292, mean_q: 4.167098
 30499/100000: episode: 337, duration: 0.439s, episode steps: 84, steps per second: 192, episode reward: 160.896, mean reward: 1.915 [1.447, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.606 [-0.413, 10.220], loss: 0.157333, mae: 0.361410, mean_q: 4.155199
 30526/100000: episode: 338, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 98.471, mean reward: 3.647 [2.426, 6.508], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.210, 10.362], loss: 0.189044, mae: 0.391238, mean_q: 4.192822
 30623/100000: episode: 339, duration: 0.538s, episode steps: 97, steps per second: 180, episode reward: 189.420, mean reward: 1.953 [1.447, 4.145], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.266, 10.100], loss: 0.165307, mae: 0.383241, mean_q: 4.220381
 30707/100000: episode: 340, duration: 0.454s, episode steps: 84, steps per second: 185, episode reward: 148.565, mean reward: 1.769 [1.478, 2.563], mean action: 0.000 [0.000, 0.000], mean observation: 1.606 [-1.006, 10.100], loss: 0.167654, mae: 0.375972, mean_q: 4.198136
 30745/100000: episode: 341, duration: 0.192s, episode steps: 38, steps per second: 198, episode reward: 126.696, mean reward: 3.334 [2.517, 7.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.339, 10.448], loss: 0.218656, mae: 0.426060, mean_q: 4.250144
 30840/100000: episode: 342, duration: 0.500s, episode steps: 95, steps per second: 190, episode reward: 183.760, mean reward: 1.934 [1.481, 4.873], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-1.550, 10.100], loss: 0.188808, mae: 0.388944, mean_q: 4.238790
 30865/100000: episode: 343, duration: 0.122s, episode steps: 25, steps per second: 204, episode reward: 65.020, mean reward: 2.601 [1.844, 4.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.258, 10.261], loss: 0.146691, mae: 0.362781, mean_q: 4.240757
 30902/100000: episode: 344, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 110.501, mean reward: 2.987 [1.910, 4.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.203, 10.320], loss: 0.211372, mae: 0.391239, mean_q: 4.263023
 30997/100000: episode: 345, duration: 0.539s, episode steps: 95, steps per second: 176, episode reward: 177.307, mean reward: 1.866 [1.455, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.968, 10.100], loss: 0.153351, mae: 0.365307, mean_q: 4.232111
 31092/100000: episode: 346, duration: 0.489s, episode steps: 95, steps per second: 194, episode reward: 199.605, mean reward: 2.101 [1.482, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.503 [-0.892, 10.100], loss: 0.184737, mae: 0.382293, mean_q: 4.296392
 31176/100000: episode: 347, duration: 0.435s, episode steps: 84, steps per second: 193, episode reward: 162.395, mean reward: 1.933 [1.464, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.610 [-0.184, 10.100], loss: 0.207741, mae: 0.410613, mean_q: 4.285865
 31201/100000: episode: 348, duration: 0.131s, episode steps: 25, steps per second: 192, episode reward: 80.366, mean reward: 3.215 [2.554, 5.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.761, 10.506], loss: 0.177863, mae: 0.402936, mean_q: 4.318017
 31238/100000: episode: 349, duration: 0.208s, episode steps: 37, steps per second: 178, episode reward: 100.713, mean reward: 2.722 [1.475, 4.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.541, 10.158], loss: 0.186381, mae: 0.380162, mean_q: 4.220567
 31333/100000: episode: 350, duration: 0.495s, episode steps: 95, steps per second: 192, episode reward: 185.362, mean reward: 1.951 [1.452, 3.634], mean action: 0.000 [0.000, 0.000], mean observation: 1.508 [-0.494, 10.172], loss: 0.192129, mae: 0.391739, mean_q: 4.294108
 31370/100000: episode: 351, duration: 0.196s, episode steps: 37, steps per second: 189, episode reward: 117.555, mean reward: 3.177 [2.262, 5.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.559, 10.413], loss: 0.187359, mae: 0.396885, mean_q: 4.328669
 31469/100000: episode: 352, duration: 0.554s, episode steps: 99, steps per second: 179, episode reward: 199.773, mean reward: 2.018 [1.514, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-0.610, 10.313], loss: 0.171911, mae: 0.384477, mean_q: 4.264240
 31494/100000: episode: 353, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 65.436, mean reward: 2.617 [1.639, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.360, 10.252], loss: 0.144797, mae: 0.366409, mean_q: 4.277327
 31532/100000: episode: 354, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 130.298, mean reward: 3.429 [1.987, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.650, 10.362], loss: 0.222815, mae: 0.412283, mean_q: 4.267934
 31559/100000: episode: 355, duration: 0.144s, episode steps: 27, steps per second: 188, episode reward: 60.302, mean reward: 2.233 [1.786, 2.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.224, 10.289], loss: 0.138692, mae: 0.370376, mean_q: 4.296484
 31656/100000: episode: 356, duration: 0.525s, episode steps: 97, steps per second: 185, episode reward: 180.284, mean reward: 1.859 [1.447, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.150, 10.101], loss: 0.160105, mae: 0.371172, mean_q: 4.264307
 31751/100000: episode: 357, duration: 0.525s, episode steps: 95, steps per second: 181, episode reward: 190.000, mean reward: 2.000 [1.470, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.492 [-1.044, 10.100], loss: 0.162626, mae: 0.387354, mean_q: 4.243513
[Info] 2-TH LEVEL FOUND: 8.205958366394043, Considering 10/90 traces
 31788/100000: episode: 358, duration: 4.438s, episode steps: 37, steps per second: 8, episode reward: 103.278, mean reward: 2.791 [1.893, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.356, 10.310], loss: 0.130044, mae: 0.351859, mean_q: 4.186886
 31824/100000: episode: 359, duration: 0.203s, episode steps: 36, steps per second: 178, episode reward: 106.506, mean reward: 2.958 [1.995, 7.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.059, 10.398], loss: 0.137583, mae: 0.361356, mean_q: 4.257273
 31859/100000: episode: 360, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 119.909, mean reward: 3.426 [1.999, 28.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.277, 10.404], loss: 0.156745, mae: 0.372324, mean_q: 4.245141
 31895/100000: episode: 361, duration: 0.191s, episode steps: 36, steps per second: 188, episode reward: 95.278, mean reward: 2.647 [1.810, 3.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.609, 10.452], loss: 0.150447, mae: 0.366769, mean_q: 4.219434
 31928/100000: episode: 362, duration: 0.172s, episode steps: 33, steps per second: 191, episode reward: 115.162, mean reward: 3.490 [2.197, 6.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.294, 10.455], loss: 0.142168, mae: 0.362704, mean_q: 4.256415
 31963/100000: episode: 363, duration: 0.174s, episode steps: 35, steps per second: 201, episode reward: 108.307, mean reward: 3.094 [2.319, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.329, 10.543], loss: 0.160832, mae: 0.378810, mean_q: 4.323481
 31991/100000: episode: 364, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 119.429, mean reward: 4.265 [3.034, 7.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.169, 10.583], loss: 0.165222, mae: 0.393802, mean_q: 4.371938
 32019/100000: episode: 365, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 80.698, mean reward: 2.882 [2.074, 4.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.092, 10.387], loss: 0.157419, mae: 0.391766, mean_q: 4.316757
 32055/100000: episode: 366, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 94.420, mean reward: 2.623 [1.764, 6.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.065, 10.292], loss: 0.150952, mae: 0.380110, mean_q: 4.315853
 32103/100000: episode: 367, duration: 0.260s, episode steps: 48, steps per second: 185, episode reward: 126.296, mean reward: 2.631 [1.854, 5.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-1.620, 10.419], loss: 0.349986, mae: 0.398633, mean_q: 4.353634
 32138/100000: episode: 368, duration: 0.191s, episode steps: 35, steps per second: 184, episode reward: 103.417, mean reward: 2.955 [1.691, 6.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.149, 10.285], loss: 0.436662, mae: 0.426053, mean_q: 4.282258
 32172/100000: episode: 369, duration: 0.194s, episode steps: 34, steps per second: 175, episode reward: 94.089, mean reward: 2.767 [2.022, 4.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.291], loss: 0.158235, mae: 0.382766, mean_q: 4.341271
 32200/100000: episode: 370, duration: 0.158s, episode steps: 28, steps per second: 178, episode reward: 104.276, mean reward: 3.724 [2.635, 5.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.174, 10.471], loss: 0.178121, mae: 0.376951, mean_q: 4.405665
 32228/100000: episode: 371, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 116.691, mean reward: 4.168 [2.984, 6.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.320, 10.591], loss: 0.185144, mae: 0.415736, mean_q: 4.428457
 32261/100000: episode: 372, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 157.993, mean reward: 4.788 [3.084, 10.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.118, 10.487], loss: 0.168121, mae: 0.391535, mean_q: 4.363439
 32294/100000: episode: 373, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 80.122, mean reward: 2.428 [1.723, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.283], loss: 0.463483, mae: 0.425387, mean_q: 4.457291
 32328/100000: episode: 374, duration: 0.209s, episode steps: 34, steps per second: 163, episode reward: 117.485, mean reward: 3.455 [1.688, 6.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.737, 10.230], loss: 0.185753, mae: 0.413021, mean_q: 4.408844
 32354/100000: episode: 375, duration: 0.148s, episode steps: 26, steps per second: 175, episode reward: 91.034, mean reward: 3.501 [2.057, 7.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.249, 10.323], loss: 0.185413, mae: 0.410501, mean_q: 4.536884
 32390/100000: episode: 376, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 129.645, mean reward: 3.601 [1.518, 7.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-1.084, 10.100], loss: 0.217777, mae: 0.427397, mean_q: 4.513861
 32424/100000: episode: 377, duration: 0.185s, episode steps: 34, steps per second: 183, episode reward: 131.649, mean reward: 3.872 [1.708, 6.413], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.271, 10.329], loss: 0.467137, mae: 0.451902, mean_q: 4.509716
 32459/100000: episode: 378, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 117.974, mean reward: 3.371 [1.992, 6.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.469, 10.388], loss: 0.219997, mae: 0.428344, mean_q: 4.587494
 32492/100000: episode: 379, duration: 0.189s, episode steps: 33, steps per second: 174, episode reward: 110.866, mean reward: 3.360 [1.625, 7.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.370, 10.191], loss: 0.189260, mae: 0.406444, mean_q: 4.552037
 32528/100000: episode: 380, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 89.457, mean reward: 2.485 [1.441, 3.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.829, 10.255], loss: 0.188356, mae: 0.429192, mean_q: 4.520058
 32564/100000: episode: 381, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 89.229, mean reward: 2.479 [1.726, 4.798], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.575, 10.292], loss: 0.249505, mae: 0.463070, mean_q: 4.582166
 32599/100000: episode: 382, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 109.522, mean reward: 3.129 [2.380, 5.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.432], loss: 0.237972, mae: 0.439505, mean_q: 4.557163
 32633/100000: episode: 383, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 99.184, mean reward: 2.917 [1.754, 6.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.985, 10.384], loss: 0.237026, mae: 0.440382, mean_q: 4.598457
 32667/100000: episode: 384, duration: 0.196s, episode steps: 34, steps per second: 173, episode reward: 92.842, mean reward: 2.731 [1.763, 6.712], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.478, 10.267], loss: 0.220924, mae: 0.435588, mean_q: 4.663813
 32703/100000: episode: 385, duration: 0.199s, episode steps: 36, steps per second: 181, episode reward: 94.571, mean reward: 2.627 [1.773, 7.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.140, 10.394], loss: 0.204671, mae: 0.432512, mean_q: 4.533385
 32737/100000: episode: 386, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 90.786, mean reward: 2.670 [1.533, 5.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.085, 10.141], loss: 0.518368, mae: 0.494798, mean_q: 4.696177
 32773/100000: episode: 387, duration: 0.203s, episode steps: 36, steps per second: 178, episode reward: 118.412, mean reward: 3.289 [1.864, 7.570], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.823, 10.220], loss: 0.225753, mae: 0.431619, mean_q: 4.634679
 32806/100000: episode: 388, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 99.805, mean reward: 3.024 [1.736, 12.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.454, 10.238], loss: 0.307751, mae: 0.487030, mean_q: 4.636574
 32839/100000: episode: 389, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 121.752, mean reward: 3.689 [2.603, 6.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.798, 10.584], loss: 0.266610, mae: 0.479225, mean_q: 4.790668
 32887/100000: episode: 390, duration: 0.279s, episode steps: 48, steps per second: 172, episode reward: 129.681, mean reward: 2.702 [2.056, 4.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.347, 10.418], loss: 0.236093, mae: 0.440153, mean_q: 4.647401
 32913/100000: episode: 391, duration: 0.151s, episode steps: 26, steps per second: 173, episode reward: 106.418, mean reward: 4.093 [2.752, 7.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.035, 10.516], loss: 0.222395, mae: 0.442687, mean_q: 4.743641
 32949/100000: episode: 392, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 110.369, mean reward: 3.066 [2.044, 5.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.333, 10.356], loss: 0.260836, mae: 0.475933, mean_q: 4.779493
 32977/100000: episode: 393, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 169.963, mean reward: 6.070 [2.542, 11.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.535, 10.566], loss: 0.247111, mae: 0.461012, mean_q: 4.732337
 33010/100000: episode: 394, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 94.864, mean reward: 2.875 [1.451, 6.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.844, 10.131], loss: 0.273715, mae: 0.481156, mean_q: 4.721861
 33043/100000: episode: 395, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 94.107, mean reward: 2.852 [2.091, 5.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.418, 10.435], loss: 0.324677, mae: 0.489271, mean_q: 4.783545
 33071/100000: episode: 396, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 103.951, mean reward: 3.713 [2.258, 7.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.452, 10.365], loss: 0.289208, mae: 0.467060, mean_q: 4.715602
 33105/100000: episode: 397, duration: 0.196s, episode steps: 34, steps per second: 174, episode reward: 115.091, mean reward: 3.385 [1.954, 5.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.969, 10.321], loss: 0.227177, mae: 0.450280, mean_q: 4.769732
 33131/100000: episode: 398, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 173.056, mean reward: 6.656 [4.382, 10.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.372, 10.532], loss: 0.257529, mae: 0.441680, mean_q: 4.845149
 33159/100000: episode: 399, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 101.337, mean reward: 3.619 [2.400, 8.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-1.123, 10.493], loss: 0.575077, mae: 0.500280, mean_q: 4.830211
 33195/100000: episode: 400, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 121.036, mean reward: 3.362 [1.453, 11.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.070, 10.103], loss: 0.281877, mae: 0.471852, mean_q: 4.821566
 33230/100000: episode: 401, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 126.699, mean reward: 3.620 [1.782, 10.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-1.200, 10.401], loss: 0.304004, mae: 0.495096, mean_q: 4.918406
 33265/100000: episode: 402, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 95.977, mean reward: 2.742 [1.945, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.035, 10.258], loss: 0.256655, mae: 0.480666, mean_q: 4.953298
 33313/100000: episode: 403, duration: 0.256s, episode steps: 48, steps per second: 187, episode reward: 181.318, mean reward: 3.777 [2.324, 6.009], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.786, 10.359], loss: 0.281174, mae: 0.483622, mean_q: 4.919787
 33361/100000: episode: 404, duration: 0.272s, episode steps: 48, steps per second: 176, episode reward: 109.545, mean reward: 2.282 [1.493, 4.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.633, 10.100], loss: 0.545897, mae: 0.551138, mean_q: 5.018572
 33396/100000: episode: 405, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 100.792, mean reward: 2.880 [1.822, 4.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.554, 10.257], loss: 0.306235, mae: 0.480699, mean_q: 4.866502
 33429/100000: episode: 406, duration: 0.184s, episode steps: 33, steps per second: 179, episode reward: 103.962, mean reward: 3.150 [2.366, 5.467], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.209, 10.363], loss: 0.322000, mae: 0.545396, mean_q: 5.016637
 33465/100000: episode: 407, duration: 0.205s, episode steps: 36, steps per second: 176, episode reward: 107.649, mean reward: 2.990 [2.042, 5.447], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.481, 10.308], loss: 0.329284, mae: 0.536492, mean_q: 4.970939
 33513/100000: episode: 408, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 123.158, mean reward: 2.566 [1.550, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.739, 10.141], loss: 0.294860, mae: 0.518441, mean_q: 4.960616
 33539/100000: episode: 409, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 108.437, mean reward: 4.171 [2.486, 8.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.271, 10.531], loss: 0.626911, mae: 0.540779, mean_q: 4.949337
 33572/100000: episode: 410, duration: 0.193s, episode steps: 33, steps per second: 171, episode reward: 102.131, mean reward: 3.095 [1.708, 7.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.650, 10.322], loss: 0.344392, mae: 0.534050, mean_q: 4.969235
 33605/100000: episode: 411, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 104.101, mean reward: 3.155 [1.766, 5.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.035, 10.274], loss: 0.326382, mae: 0.522232, mean_q: 5.089744
 33638/100000: episode: 412, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 91.769, mean reward: 2.781 [2.080, 4.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.275, 10.430], loss: 0.297320, mae: 0.514748, mean_q: 5.064229
 33686/100000: episode: 413, duration: 0.264s, episode steps: 48, steps per second: 182, episode reward: 121.121, mean reward: 2.523 [1.745, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.340, 10.355], loss: 0.304204, mae: 0.512636, mean_q: 4.981541
 33720/100000: episode: 414, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 147.532, mean reward: 4.339 [2.712, 9.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.878, 10.468], loss: 0.325022, mae: 0.518618, mean_q: 5.071116
 33756/100000: episode: 415, duration: 0.210s, episode steps: 36, steps per second: 171, episode reward: 114.476, mean reward: 3.180 [1.589, 7.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.420, 10.173], loss: 0.297690, mae: 0.512601, mean_q: 5.081544
 33790/100000: episode: 416, duration: 0.195s, episode steps: 34, steps per second: 175, episode reward: 101.140, mean reward: 2.975 [2.226, 5.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.834, 10.381], loss: 0.281991, mae: 0.471224, mean_q: 5.145748
 33824/100000: episode: 417, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 109.197, mean reward: 3.212 [1.505, 7.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.150, 10.179], loss: 0.332656, mae: 0.514862, mean_q: 5.125703
 33859/100000: episode: 418, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 85.641, mean reward: 2.447 [1.598, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.524, 10.242], loss: 0.881880, mae: 0.585871, mean_q: 5.174350
 33892/100000: episode: 419, duration: 0.187s, episode steps: 33, steps per second: 177, episode reward: 110.012, mean reward: 3.334 [1.780, 7.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.661, 10.346], loss: 0.382808, mae: 0.555580, mean_q: 5.073620
 33940/100000: episode: 420, duration: 0.258s, episode steps: 48, steps per second: 186, episode reward: 129.339, mean reward: 2.695 [1.957, 5.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.607, 10.307], loss: 0.306750, mae: 0.518525, mean_q: 5.091978
 33973/100000: episode: 421, duration: 0.183s, episode steps: 33, steps per second: 180, episode reward: 109.938, mean reward: 3.331 [2.121, 7.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.294, 10.437], loss: 0.319388, mae: 0.515505, mean_q: 5.050478
 34006/100000: episode: 422, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 144.054, mean reward: 4.365 [1.763, 34.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-1.058, 10.202], loss: 0.393811, mae: 0.555662, mean_q: 5.127568
 34039/100000: episode: 423, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 95.431, mean reward: 2.892 [2.116, 4.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-1.901, 10.487], loss: 0.415272, mae: 0.584948, mean_q: 5.290858
 34065/100000: episode: 424, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 97.911, mean reward: 3.766 [2.529, 7.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.087, 10.440], loss: 0.402737, mae: 0.565974, mean_q: 5.169476
 34099/100000: episode: 425, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 95.049, mean reward: 2.796 [1.915, 5.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.501, 10.462], loss: 0.778986, mae: 0.579384, mean_q: 5.196990
 34132/100000: episode: 426, duration: 0.174s, episode steps: 33, steps per second: 189, episode reward: 83.063, mean reward: 2.517 [1.973, 4.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.574, 10.304], loss: 0.347173, mae: 0.552936, mean_q: 5.243049
 34165/100000: episode: 427, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 99.022, mean reward: 3.001 [2.025, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.035, 10.316], loss: 0.349875, mae: 0.534648, mean_q: 5.187610
 34200/100000: episode: 428, duration: 0.182s, episode steps: 35, steps per second: 193, episode reward: 84.678, mean reward: 2.419 [1.596, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-2.039, 10.248], loss: 0.327864, mae: 0.534163, mean_q: 5.175356
 34234/100000: episode: 429, duration: 0.183s, episode steps: 34, steps per second: 185, episode reward: 132.473, mean reward: 3.896 [2.744, 8.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.165, 10.531], loss: 0.356081, mae: 0.538017, mean_q: 5.272170
 34262/100000: episode: 430, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 93.711, mean reward: 3.347 [2.397, 5.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.163, 10.546], loss: 0.344998, mae: 0.542330, mean_q: 5.337135
 34298/100000: episode: 431, duration: 0.216s, episode steps: 36, steps per second: 167, episode reward: 142.337, mean reward: 3.954 [2.321, 6.142], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.915, 10.388], loss: 0.800579, mae: 0.591848, mean_q: 5.234887
 34333/100000: episode: 432, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 173.495, mean reward: 4.957 [3.056, 17.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.620, 10.518], loss: 1.190679, mae: 0.599593, mean_q: 5.311233
 34381/100000: episode: 433, duration: 0.267s, episode steps: 48, steps per second: 180, episode reward: 111.112, mean reward: 2.315 [1.447, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.369, 10.124], loss: 0.681278, mae: 0.570658, mean_q: 5.393623
 34407/100000: episode: 434, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 156.386, mean reward: 6.015 [3.853, 10.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.530], loss: 0.689412, mae: 0.580733, mean_q: 5.413765
 34442/100000: episode: 435, duration: 0.202s, episode steps: 35, steps per second: 173, episode reward: 151.808, mean reward: 4.337 [2.942, 5.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.033 [-0.517, 10.481], loss: 0.481221, mae: 0.609853, mean_q: 5.511955
 34470/100000: episode: 436, duration: 0.156s, episode steps: 28, steps per second: 179, episode reward: 119.499, mean reward: 4.268 [2.847, 10.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.566, 10.447], loss: 0.458075, mae: 0.595373, mean_q: 5.493056
 34496/100000: episode: 437, duration: 0.146s, episode steps: 26, steps per second: 178, episode reward: 142.681, mean reward: 5.488 [1.930, 37.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.129, 10.318], loss: 0.370793, mae: 0.576866, mean_q: 5.420553
 34531/100000: episode: 438, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 83.502, mean reward: 2.386 [1.472, 4.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.149, 10.134], loss: 1.262317, mae: 0.726912, mean_q: 5.447641
 34564/100000: episode: 439, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 114.014, mean reward: 3.455 [2.266, 12.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-1.156, 10.508], loss: 0.925927, mae: 0.704321, mean_q: 5.484884
 34598/100000: episode: 440, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 105.586, mean reward: 3.105 [2.021, 4.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-1.588, 10.343], loss: 0.830903, mae: 0.618813, mean_q: 5.483717
 34632/100000: episode: 441, duration: 0.204s, episode steps: 34, steps per second: 167, episode reward: 128.860, mean reward: 3.790 [1.901, 7.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.246, 10.352], loss: 0.403615, mae: 0.557300, mean_q: 5.560401
 34668/100000: episode: 442, duration: 0.195s, episode steps: 36, steps per second: 184, episode reward: 89.404, mean reward: 2.483 [1.475, 7.356], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.264, 10.100], loss: 0.443204, mae: 0.578000, mean_q: 5.523021
 34701/100000: episode: 443, duration: 0.172s, episode steps: 33, steps per second: 191, episode reward: 105.643, mean reward: 3.201 [1.707, 5.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.392, 10.183], loss: 0.943107, mae: 0.660563, mean_q: 5.605220
 34736/100000: episode: 444, duration: 0.207s, episode steps: 35, steps per second: 169, episode reward: 114.375, mean reward: 3.268 [2.400, 5.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.427, 10.541], loss: 0.347941, mae: 0.544895, mean_q: 5.412097
 34762/100000: episode: 445, duration: 0.144s, episode steps: 26, steps per second: 181, episode reward: 116.335, mean reward: 4.474 [2.524, 11.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.445], loss: 0.362962, mae: 0.576955, mean_q: 5.543174
 34798/100000: episode: 446, duration: 0.188s, episode steps: 36, steps per second: 191, episode reward: 85.546, mean reward: 2.376 [1.900, 3.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.218, 10.318], loss: 1.206929, mae: 0.736952, mean_q: 5.550113
 34832/100000: episode: 447, duration: 0.196s, episode steps: 34, steps per second: 173, episode reward: 161.556, mean reward: 4.752 [2.239, 17.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.098, 10.362], loss: 0.590133, mae: 0.775391, mean_q: 5.744976
[Info] 3-TH LEVEL FOUND: 10.45025634765625, Considering 10/90 traces
 34880/100000: episode: 448, duration: 4.433s, episode steps: 48, steps per second: 11, episode reward: 151.674, mean reward: 3.160 [2.402, 4.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.918 [-0.338, 10.421], loss: 1.094755, mae: 0.677292, mean_q: 5.652483
 34912/100000: episode: 449, duration: 0.195s, episode steps: 32, steps per second: 164, episode reward: 96.512, mean reward: 3.016 [1.861, 6.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.302, 10.378], loss: 1.457762, mae: 0.740016, mean_q: 5.630799
 34945/100000: episode: 450, duration: 0.178s, episode steps: 33, steps per second: 185, episode reward: 173.134, mean reward: 5.246 [2.539, 11.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.624, 10.461], loss: 0.689188, mae: 0.649936, mean_q: 5.780437
[Info] FALSIFICATION!
[Info] Levels: [4.938454, 8.205958, 10.450256, 11.844616]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.01]
[Info] Error Prob: 1.0000000000000003e-05

 34953/100000: episode: 451, duration: 4.528s, episode steps: 8, steps per second: 2, episode reward: 250.205, mean reward: 31.276 [4.522, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.018, 9.914], loss: 0.437913, mae: 0.622161, mean_q: 5.553779
 35053/100000: episode: 452, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.930, mean reward: 1.919 [1.454, 3.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.960, 10.345], loss: 4.490245, mae: 0.864297, mean_q: 5.806351
 35153/100000: episode: 453, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 189.086, mean reward: 1.891 [1.535, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.731, 10.098], loss: 2.435138, mae: 0.806284, mean_q: 5.816029
 35253/100000: episode: 454, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 196.045, mean reward: 1.960 [1.467, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.057, 10.098], loss: 0.840069, mae: 0.650727, mean_q: 5.651958
 35353/100000: episode: 455, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 181.890, mean reward: 1.819 [1.462, 2.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.392, 10.098], loss: 1.943655, mae: 0.650894, mean_q: 5.605134
 35453/100000: episode: 456, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 181.479, mean reward: 1.815 [1.444, 2.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.484, 10.098], loss: 4.543853, mae: 0.865832, mean_q: 5.767811
 35553/100000: episode: 457, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 184.096, mean reward: 1.841 [1.464, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.643, 10.228], loss: 0.598224, mae: 0.624281, mean_q: 5.672706
 35653/100000: episode: 458, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 179.364, mean reward: 1.794 [1.460, 3.023], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.588, 10.098], loss: 3.220946, mae: 0.731000, mean_q: 5.669601
 35753/100000: episode: 459, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 209.653, mean reward: 2.097 [1.441, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.524, 10.103], loss: 2.351312, mae: 0.802422, mean_q: 5.799556
 35853/100000: episode: 460, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 205.488, mean reward: 2.055 [1.453, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.929, 10.391], loss: 1.018563, mae: 0.652468, mean_q: 5.613889
 35953/100000: episode: 461, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 189.856, mean reward: 1.899 [1.460, 3.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.290, 10.098], loss: 0.887433, mae: 0.635821, mean_q: 5.552486
 36053/100000: episode: 462, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 206.067, mean reward: 2.061 [1.462, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.028, 10.098], loss: 1.784386, mae: 0.684788, mean_q: 5.585817
 36153/100000: episode: 463, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 205.793, mean reward: 2.058 [1.462, 7.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.635, 10.397], loss: 0.777997, mae: 0.645848, mean_q: 5.578338
 36253/100000: episode: 464, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.864, mean reward: 1.829 [1.457, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.138, 10.098], loss: 3.158164, mae: 0.738546, mean_q: 5.614713
 36353/100000: episode: 465, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 198.675, mean reward: 1.987 [1.473, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.020, 10.098], loss: 0.753548, mae: 0.651009, mean_q: 5.536892
 36453/100000: episode: 466, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 194.473, mean reward: 1.945 [1.492, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.036, 10.098], loss: 0.942022, mae: 0.649675, mean_q: 5.573668
 36553/100000: episode: 467, duration: 0.577s, episode steps: 100, steps per second: 173, episode reward: 194.284, mean reward: 1.943 [1.520, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.487, 10.253], loss: 4.372967, mae: 0.850019, mean_q: 5.573049
 36653/100000: episode: 468, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 191.414, mean reward: 1.914 [1.497, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.830, 10.157], loss: 0.808166, mae: 0.633136, mean_q: 5.579300
 36753/100000: episode: 469, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.474, mean reward: 1.955 [1.466, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.732, 10.263], loss: 1.921428, mae: 0.675501, mean_q: 5.526945
 36853/100000: episode: 470, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.721, mean reward: 1.917 [1.478, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.492, 10.098], loss: 0.984856, mae: 0.635812, mean_q: 5.514622
 36953/100000: episode: 471, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 186.057, mean reward: 1.861 [1.453, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.086, 10.135], loss: 1.793775, mae: 0.639072, mean_q: 5.454952
 37053/100000: episode: 472, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 183.241, mean reward: 1.832 [1.441, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.325, 10.098], loss: 0.664597, mae: 0.566934, mean_q: 5.292120
 37153/100000: episode: 473, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 207.758, mean reward: 2.078 [1.488, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.380, 10.225], loss: 1.714771, mae: 0.647612, mean_q: 5.316747
 37253/100000: episode: 474, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 213.730, mean reward: 2.137 [1.497, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.014, 10.098], loss: 1.844135, mae: 0.633630, mean_q: 5.238729
 37353/100000: episode: 475, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 212.894, mean reward: 2.129 [1.483, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.820, 10.098], loss: 3.048347, mae: 0.679851, mean_q: 5.287533
 37453/100000: episode: 476, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 193.055, mean reward: 1.931 [1.472, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.638, 10.098], loss: 2.917089, mae: 0.728034, mean_q: 5.230429
 37553/100000: episode: 477, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 192.130, mean reward: 1.921 [1.471, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.181, 10.098], loss: 2.951742, mae: 0.713624, mean_q: 5.222946
 37653/100000: episode: 478, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 184.392, mean reward: 1.844 [1.441, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.233, 10.350], loss: 1.835775, mae: 0.598436, mean_q: 5.040084
 37753/100000: episode: 479, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 197.697, mean reward: 1.977 [1.488, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.852, 10.098], loss: 0.453415, mae: 0.523220, mean_q: 5.022906
 37853/100000: episode: 480, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 195.051, mean reward: 1.951 [1.459, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.821, 10.098], loss: 6.632462, mae: 0.790222, mean_q: 5.147419
 37953/100000: episode: 481, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 179.934, mean reward: 1.799 [1.477, 2.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.463, 10.098], loss: 1.710664, mae: 0.633537, mean_q: 5.025872
 38053/100000: episode: 482, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 203.025, mean reward: 2.030 [1.458, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.774, 10.338], loss: 3.012139, mae: 0.672442, mean_q: 5.009826
 38153/100000: episode: 483, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 181.929, mean reward: 1.819 [1.436, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.316, 10.098], loss: 1.921419, mae: 0.594478, mean_q: 4.882178
 38253/100000: episode: 484, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 197.161, mean reward: 1.972 [1.444, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.153, 10.098], loss: 0.491535, mae: 0.477532, mean_q: 4.679744
 38353/100000: episode: 485, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 183.853, mean reward: 1.839 [1.474, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.841, 10.098], loss: 0.749413, mae: 0.497727, mean_q: 4.669060
 38453/100000: episode: 486, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 197.389, mean reward: 1.974 [1.483, 2.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.940, 10.307], loss: 4.140429, mae: 0.696248, mean_q: 4.754765
 38553/100000: episode: 487, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 182.009, mean reward: 1.820 [1.452, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.502, 10.103], loss: 2.239117, mae: 0.627159, mean_q: 4.664671
 38653/100000: episode: 488, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 195.793, mean reward: 1.958 [1.451, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.536, 10.098], loss: 3.811439, mae: 0.704881, mean_q: 4.645256
 38753/100000: episode: 489, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 203.878, mean reward: 2.039 [1.497, 6.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.876, 10.338], loss: 1.763787, mae: 0.550934, mean_q: 4.530750
 38853/100000: episode: 490, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 222.196, mean reward: 2.222 [1.506, 6.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.191, 10.139], loss: 1.756346, mae: 0.546121, mean_q: 4.533168
 38953/100000: episode: 491, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 193.945, mean reward: 1.939 [1.475, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.249, 10.115], loss: 1.700944, mae: 0.528968, mean_q: 4.464660
 39053/100000: episode: 492, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 203.672, mean reward: 2.037 [1.539, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.280, 10.098], loss: 0.292583, mae: 0.418470, mean_q: 4.356092
 39153/100000: episode: 493, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 191.420, mean reward: 1.914 [1.464, 4.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.907, 10.126], loss: 2.662750, mae: 0.592729, mean_q: 4.407351
 39253/100000: episode: 494, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 187.790, mean reward: 1.878 [1.477, 2.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.580, 10.098], loss: 0.357132, mae: 0.414259, mean_q: 4.308682
 39353/100000: episode: 495, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 185.787, mean reward: 1.858 [1.488, 2.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.230, 10.098], loss: 4.099132, mae: 0.684557, mean_q: 4.321240
 39453/100000: episode: 496, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 204.584, mean reward: 2.046 [1.463, 5.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.174, 10.164], loss: 0.223418, mae: 0.423703, mean_q: 4.149055
 39553/100000: episode: 497, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 200.254, mean reward: 2.003 [1.455, 4.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.909, 10.270], loss: 1.416981, mae: 0.441950, mean_q: 4.123741
 39653/100000: episode: 498, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 182.520, mean reward: 1.825 [1.448, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.448, 10.130], loss: 2.440528, mae: 0.528524, mean_q: 4.092241
 39753/100000: episode: 499, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 186.149, mean reward: 1.861 [1.468, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.973, 10.231], loss: 1.216851, mae: 0.399714, mean_q: 4.003766
 39853/100000: episode: 500, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 222.366, mean reward: 2.224 [1.494, 5.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-2.176, 10.098], loss: 2.327508, mae: 0.477481, mean_q: 3.988661
 39953/100000: episode: 501, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 187.645, mean reward: 1.876 [1.464, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.841, 10.178], loss: 1.226895, mae: 0.437685, mean_q: 3.906611
 40053/100000: episode: 502, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 187.678, mean reward: 1.877 [1.453, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.896, 10.242], loss: 0.093947, mae: 0.308299, mean_q: 3.858800
 40153/100000: episode: 503, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 214.661, mean reward: 2.147 [1.473, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.387, 10.420], loss: 0.088287, mae: 0.299565, mean_q: 3.850474
 40253/100000: episode: 504, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 177.318, mean reward: 1.773 [1.457, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.657, 10.098], loss: 0.087995, mae: 0.302408, mean_q: 3.854287
 40353/100000: episode: 505, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 206.463, mean reward: 2.065 [1.510, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.776, 10.098], loss: 0.095136, mae: 0.308047, mean_q: 3.851010
 40453/100000: episode: 506, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.384, mean reward: 1.874 [1.478, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.948, 10.205], loss: 0.096063, mae: 0.307032, mean_q: 3.854668
 40553/100000: episode: 507, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 187.580, mean reward: 1.876 [1.441, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.275, 10.098], loss: 0.095525, mae: 0.305136, mean_q: 3.857766
 40653/100000: episode: 508, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 236.206, mean reward: 2.362 [1.481, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.103, 10.458], loss: 0.105328, mae: 0.316093, mean_q: 3.865073
 40753/100000: episode: 509, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 194.626, mean reward: 1.946 [1.493, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.515, 10.127], loss: 0.099003, mae: 0.311953, mean_q: 3.878168
 40853/100000: episode: 510, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 252.540, mean reward: 2.525 [1.524, 5.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.554, 10.098], loss: 0.099750, mae: 0.308320, mean_q: 3.888577
 40953/100000: episode: 511, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 184.420, mean reward: 1.844 [1.457, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.839, 10.098], loss: 0.096183, mae: 0.310577, mean_q: 3.887614
 41053/100000: episode: 512, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 202.571, mean reward: 2.026 [1.459, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.913, 10.310], loss: 0.091443, mae: 0.303051, mean_q: 3.882146
 41153/100000: episode: 513, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.200, mean reward: 1.902 [1.508, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.433, 10.182], loss: 0.107294, mae: 0.321012, mean_q: 3.897208
 41253/100000: episode: 514, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 189.313, mean reward: 1.893 [1.505, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.892, 10.098], loss: 0.091931, mae: 0.301840, mean_q: 3.883874
 41353/100000: episode: 515, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 197.425, mean reward: 1.974 [1.484, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.703, 10.098], loss: 0.102148, mae: 0.313905, mean_q: 3.883752
 41453/100000: episode: 516, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 196.217, mean reward: 1.962 [1.459, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.674, 10.098], loss: 0.092208, mae: 0.300589, mean_q: 3.879334
 41553/100000: episode: 517, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 188.777, mean reward: 1.888 [1.508, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.562, 10.098], loss: 0.091942, mae: 0.306905, mean_q: 3.880428
 41653/100000: episode: 518, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 188.289, mean reward: 1.883 [1.461, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.685, 10.275], loss: 0.109827, mae: 0.318281, mean_q: 3.885458
 41753/100000: episode: 519, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 203.668, mean reward: 2.037 [1.474, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.615, 10.098], loss: 0.098779, mae: 0.307587, mean_q: 3.883729
 41853/100000: episode: 520, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 230.229, mean reward: 2.302 [1.466, 6.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.453, 10.098], loss: 0.094587, mae: 0.305595, mean_q: 3.882075
 41953/100000: episode: 521, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 192.100, mean reward: 1.921 [1.460, 3.465], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.825, 10.098], loss: 0.112566, mae: 0.320374, mean_q: 3.902418
 42053/100000: episode: 522, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 217.287, mean reward: 2.173 [1.452, 5.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.774, 10.098], loss: 0.123453, mae: 0.331609, mean_q: 3.929973
 42153/100000: episode: 523, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 245.143, mean reward: 2.451 [1.507, 23.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.049, 10.192], loss: 0.107238, mae: 0.319577, mean_q: 3.927251
 42253/100000: episode: 524, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.307, mean reward: 1.903 [1.464, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.034, 10.262], loss: 0.186322, mae: 0.343466, mean_q: 3.935043
 42353/100000: episode: 525, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 187.101, mean reward: 1.871 [1.451, 2.854], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.884, 10.307], loss: 0.330714, mae: 0.378922, mean_q: 3.955941
 42453/100000: episode: 526, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 199.103, mean reward: 1.991 [1.458, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.575, 10.098], loss: 0.112217, mae: 0.326108, mean_q: 3.928205
 42553/100000: episode: 527, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 206.675, mean reward: 2.067 [1.495, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.160, 10.098], loss: 0.182240, mae: 0.338215, mean_q: 3.925596
 42653/100000: episode: 528, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 190.387, mean reward: 1.904 [1.463, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.588, 10.212], loss: 0.173323, mae: 0.329007, mean_q: 3.915379
 42753/100000: episode: 529, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 186.069, mean reward: 1.861 [1.473, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.696, 10.144], loss: 0.099806, mae: 0.315532, mean_q: 3.905074
 42853/100000: episode: 530, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 188.530, mean reward: 1.885 [1.432, 2.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.148, 10.098], loss: 0.108349, mae: 0.315331, mean_q: 3.912145
 42953/100000: episode: 531, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 177.386, mean reward: 1.774 [1.455, 2.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.629, 10.263], loss: 0.096936, mae: 0.308959, mean_q: 3.899276
 43053/100000: episode: 532, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 207.819, mean reward: 2.078 [1.449, 4.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.800, 10.140], loss: 0.166772, mae: 0.317676, mean_q: 3.911633
 43153/100000: episode: 533, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 192.439, mean reward: 1.924 [1.462, 2.866], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.098, 10.098], loss: 0.175867, mae: 0.330205, mean_q: 3.910632
 43253/100000: episode: 534, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 202.836, mean reward: 2.028 [1.492, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.960, 10.098], loss: 0.182996, mae: 0.340221, mean_q: 3.925316
 43353/100000: episode: 535, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 188.375, mean reward: 1.884 [1.444, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.222, 10.098], loss: 0.173550, mae: 0.327815, mean_q: 3.929894
 43453/100000: episode: 536, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.274, mean reward: 1.863 [1.456, 3.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.392, 10.148], loss: 0.195873, mae: 0.351616, mean_q: 3.928208
 43553/100000: episode: 537, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 205.340, mean reward: 2.053 [1.484, 4.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.798, 10.266], loss: 0.111723, mae: 0.325056, mean_q: 3.927028
 43653/100000: episode: 538, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 201.969, mean reward: 2.020 [1.470, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.265, 10.098], loss: 0.174871, mae: 0.324109, mean_q: 3.930753
 43753/100000: episode: 539, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 207.198, mean reward: 2.072 [1.498, 3.740], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.296, 10.154], loss: 0.113315, mae: 0.319972, mean_q: 3.921313
 43853/100000: episode: 540, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 194.686, mean reward: 1.947 [1.468, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.104, 10.098], loss: 0.111127, mae: 0.321173, mean_q: 3.911041
 43953/100000: episode: 541, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 188.959, mean reward: 1.890 [1.471, 4.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.923, 10.142], loss: 0.176079, mae: 0.333309, mean_q: 3.935235
 44053/100000: episode: 542, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 201.914, mean reward: 2.019 [1.454, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.753, 10.098], loss: 0.240474, mae: 0.336197, mean_q: 3.921535
 44153/100000: episode: 543, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 209.746, mean reward: 2.097 [1.452, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.133, 10.098], loss: 0.196527, mae: 0.353742, mean_q: 3.930146
 44253/100000: episode: 544, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 186.337, mean reward: 1.863 [1.442, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.328, 10.268], loss: 0.103613, mae: 0.319555, mean_q: 3.938167
 44353/100000: episode: 545, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 190.882, mean reward: 1.909 [1.452, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.228, 10.199], loss: 0.109157, mae: 0.322313, mean_q: 3.928823
 44453/100000: episode: 546, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 183.322, mean reward: 1.833 [1.458, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.177, 10.151], loss: 0.184291, mae: 0.340361, mean_q: 3.937370
 44553/100000: episode: 547, duration: 0.560s, episode steps: 100, steps per second: 178, episode reward: 190.438, mean reward: 1.904 [1.500, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.098, 10.174], loss: 0.112670, mae: 0.329685, mean_q: 3.937627
 44653/100000: episode: 548, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 191.181, mean reward: 1.912 [1.434, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.733, 10.117], loss: 0.099673, mae: 0.311956, mean_q: 3.908714
 44753/100000: episode: 549, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 189.651, mean reward: 1.897 [1.435, 4.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.772, 10.098], loss: 0.109030, mae: 0.328227, mean_q: 3.921594
 44853/100000: episode: 550, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 191.454, mean reward: 1.915 [1.457, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.072, 10.172], loss: 0.105902, mae: 0.321849, mean_q: 3.927012
[Info] 1-TH LEVEL FOUND: 4.968017578125, Considering 10/90 traces
 44953/100000: episode: 551, duration: 4.726s, episode steps: 100, steps per second: 21, episode reward: 202.345, mean reward: 2.023 [1.448, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.022, 10.181], loss: 0.097656, mae: 0.309860, mean_q: 3.905585
 45003/100000: episode: 552, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 95.371, mean reward: 1.907 [1.440, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.475, 10.100], loss: 0.112946, mae: 0.327327, mean_q: 3.919381
 45098/100000: episode: 553, duration: 0.511s, episode steps: 95, steps per second: 186, episode reward: 180.711, mean reward: 1.902 [1.477, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.502 [-1.153, 10.100], loss: 0.177245, mae: 0.333379, mean_q: 3.932971
 45193/100000: episode: 554, duration: 0.495s, episode steps: 95, steps per second: 192, episode reward: 187.184, mean reward: 1.970 [1.443, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.704, 10.360], loss: 0.185914, mae: 0.345037, mean_q: 3.940069
 45236/100000: episode: 555, duration: 0.240s, episode steps: 43, steps per second: 179, episode reward: 105.671, mean reward: 2.457 [1.936, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.321, 10.471], loss: 0.105498, mae: 0.311757, mean_q: 3.920904
 45241/100000: episode: 556, duration: 0.033s, episode steps: 5, steps per second: 149, episode reward: 13.604, mean reward: 2.721 [2.198, 4.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.394, 10.100], loss: 0.099587, mae: 0.313937, mean_q: 3.933320
 45277/100000: episode: 557, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 130.314, mean reward: 3.620 [2.449, 6.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.446, 10.420], loss: 0.101754, mae: 0.323742, mean_q: 3.936675
 45286/100000: episode: 558, duration: 0.046s, episode steps: 9, steps per second: 197, episode reward: 31.074, mean reward: 3.453 [2.420, 5.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.931, 10.100], loss: 0.867462, mae: 0.450072, mean_q: 3.995063
 45295/100000: episode: 559, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 22.818, mean reward: 2.535 [2.048, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.295, 10.100], loss: 0.131097, mae: 0.356609, mean_q: 3.935234
 45304/100000: episode: 560, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 26.153, mean reward: 2.906 [2.525, 4.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.264, 10.100], loss: 0.137926, mae: 0.363509, mean_q: 3.860895
 45313/100000: episode: 561, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 25.424, mean reward: 2.825 [2.523, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.183, 10.100], loss: 0.872979, mae: 0.448947, mean_q: 4.094549
 45356/100000: episode: 562, duration: 0.236s, episode steps: 43, steps per second: 182, episode reward: 126.242, mean reward: 2.936 [2.111, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.604, 10.493], loss: 0.138215, mae: 0.366735, mean_q: 3.955695
 45365/100000: episode: 563, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 24.473, mean reward: 2.719 [2.058, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.466, 10.100], loss: 0.091013, mae: 0.322022, mean_q: 3.905279
 45374/100000: episode: 564, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 28.799, mean reward: 3.200 [2.524, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.281, 10.100], loss: 0.844337, mae: 0.406670, mean_q: 4.000909
 45420/100000: episode: 565, duration: 0.244s, episode steps: 46, steps per second: 188, episode reward: 89.154, mean reward: 1.938 [1.457, 2.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.207, 10.242], loss: 0.116096, mae: 0.341324, mean_q: 3.992178
 45456/100000: episode: 566, duration: 0.206s, episode steps: 36, steps per second: 175, episode reward: 102.294, mean reward: 2.842 [1.923, 8.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.806, 10.301], loss: 0.125215, mae: 0.336021, mean_q: 3.978644
 45551/100000: episode: 567, duration: 0.499s, episode steps: 95, steps per second: 190, episode reward: 186.334, mean reward: 1.961 [1.503, 3.379], mean action: 0.000 [0.000, 0.000], mean observation: 1.499 [-0.653, 10.100], loss: 0.181392, mae: 0.339396, mean_q: 3.956628
 45646/100000: episode: 568, duration: 0.540s, episode steps: 95, steps per second: 176, episode reward: 181.727, mean reward: 1.913 [1.441, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.980, 10.100], loss: 0.207037, mae: 0.368167, mean_q: 3.991381
 45654/100000: episode: 569, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 21.680, mean reward: 2.710 [2.325, 3.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.271, 10.100], loss: 0.114350, mae: 0.342238, mean_q: 3.995181
 45703/100000: episode: 570, duration: 0.290s, episode steps: 49, steps per second: 169, episode reward: 116.325, mean reward: 2.374 [1.486, 4.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.347, 10.146], loss: 0.139725, mae: 0.363958, mean_q: 3.962937
 45708/100000: episode: 571, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 13.997, mean reward: 2.799 [2.298, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.635, 10.100], loss: 0.105725, mae: 0.312871, mean_q: 3.871180
 45757/100000: episode: 572, duration: 0.248s, episode steps: 49, steps per second: 197, episode reward: 119.862, mean reward: 2.446 [1.607, 4.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.428, 10.422], loss: 0.125804, mae: 0.344577, mean_q: 3.972122
 45793/100000: episode: 573, duration: 0.195s, episode steps: 36, steps per second: 184, episode reward: 155.786, mean reward: 4.327 [2.572, 9.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.545, 10.513], loss: 0.130227, mae: 0.346284, mean_q: 3.970989
 45839/100000: episode: 574, duration: 0.259s, episode steps: 46, steps per second: 178, episode reward: 111.681, mean reward: 2.428 [1.707, 3.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.282, 10.401], loss: 0.127045, mae: 0.340970, mean_q: 4.000019
 45848/100000: episode: 575, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 27.367, mean reward: 3.041 [2.707, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.272, 10.100], loss: 0.147864, mae: 0.374354, mean_q: 4.000486
 45853/100000: episode: 576, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 14.986, mean reward: 2.997 [2.396, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.337, 10.100], loss: 0.205056, mae: 0.326407, mean_q: 3.900464
 45896/100000: episode: 577, duration: 0.232s, episode steps: 43, steps per second: 185, episode reward: 93.678, mean reward: 2.179 [1.528, 4.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.580, 10.198], loss: 0.279702, mae: 0.357800, mean_q: 4.013477
 45904/100000: episode: 578, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 19.541, mean reward: 2.443 [2.133, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.483, 10.100], loss: 0.119112, mae: 0.329467, mean_q: 3.934101
 45999/100000: episode: 579, duration: 0.522s, episode steps: 95, steps per second: 182, episode reward: 198.580, mean reward: 2.090 [1.493, 5.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.504 [-0.712, 10.394], loss: 0.216689, mae: 0.370046, mean_q: 4.048813
 46004/100000: episode: 580, duration: 0.028s, episode steps: 5, steps per second: 181, episode reward: 15.671, mean reward: 3.134 [2.703, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.397, 10.100], loss: 0.156157, mae: 0.384459, mean_q: 3.950351
 46054/100000: episode: 581, duration: 0.255s, episode steps: 50, steps per second: 196, episode reward: 96.438, mean reward: 1.929 [1.516, 2.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.380, 10.100], loss: 0.147444, mae: 0.366853, mean_q: 4.034868
 46097/100000: episode: 582, duration: 0.241s, episode steps: 43, steps per second: 179, episode reward: 88.923, mean reward: 2.068 [1.698, 3.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-1.087, 10.276], loss: 0.147807, mae: 0.358466, mean_q: 4.043241
 46106/100000: episode: 583, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 29.782, mean reward: 3.309 [2.892, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.242, 10.100], loss: 0.113347, mae: 0.347979, mean_q: 4.084795
 46155/100000: episode: 584, duration: 0.261s, episode steps: 49, steps per second: 188, episode reward: 119.102, mean reward: 2.431 [1.861, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.399, 10.337], loss: 0.264922, mae: 0.356751, mean_q: 4.040285
 46163/100000: episode: 585, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 19.336, mean reward: 2.417 [2.281, 2.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.315, 10.100], loss: 0.154174, mae: 0.372547, mean_q: 4.089692
 46212/100000: episode: 586, duration: 0.263s, episode steps: 49, steps per second: 186, episode reward: 107.947, mean reward: 2.203 [1.473, 5.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.670, 10.100], loss: 0.172966, mae: 0.384884, mean_q: 4.067846
 46307/100000: episode: 587, duration: 0.512s, episode steps: 95, steps per second: 186, episode reward: 188.725, mean reward: 1.987 [1.483, 4.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.497 [-0.171, 10.209], loss: 0.127003, mae: 0.347030, mean_q: 4.055473
 46356/100000: episode: 588, duration: 0.256s, episode steps: 49, steps per second: 191, episode reward: 102.004, mean reward: 2.082 [1.579, 2.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.683, 10.245], loss: 0.130722, mae: 0.349299, mean_q: 4.071572
 46361/100000: episode: 589, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 12.166, mean reward: 2.433 [2.094, 2.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.261, 10.100], loss: 0.188539, mae: 0.359031, mean_q: 4.059079
 46366/100000: episode: 590, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 14.757, mean reward: 2.951 [2.310, 3.351], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.316, 10.100], loss: 0.136904, mae: 0.370123, mean_q: 4.098623
 46374/100000: episode: 591, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 20.946, mean reward: 2.618 [2.448, 2.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.354, 10.100], loss: 0.163223, mae: 0.340443, mean_q: 4.049005
 46382/100000: episode: 592, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 19.356, mean reward: 2.420 [2.224, 2.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.234, 10.100], loss: 0.136421, mae: 0.343592, mean_q: 4.118574
 46390/100000: episode: 593, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 20.814, mean reward: 2.602 [2.203, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.234, 10.100], loss: 0.105347, mae: 0.340820, mean_q: 4.041162
 46436/100000: episode: 594, duration: 0.248s, episode steps: 46, steps per second: 185, episode reward: 106.689, mean reward: 2.319 [1.698, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-0.516, 10.312], loss: 0.444940, mae: 0.406115, mean_q: 4.114555
 46445/100000: episode: 595, duration: 0.066s, episode steps: 9, steps per second: 137, episode reward: 28.630, mean reward: 3.181 [2.584, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.341, 10.100], loss: 0.922099, mae: 0.529198, mean_q: 4.104329
 46453/100000: episode: 596, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 19.613, mean reward: 2.452 [2.285, 2.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.269, 10.100], loss: 0.195786, mae: 0.450559, mean_q: 3.965884
 46496/100000: episode: 597, duration: 0.240s, episode steps: 43, steps per second: 179, episode reward: 89.889, mean reward: 2.090 [1.682, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.786, 10.465], loss: 0.134149, mae: 0.356078, mean_q: 4.049200
 46545/100000: episode: 598, duration: 0.281s, episode steps: 49, steps per second: 174, episode reward: 100.860, mean reward: 2.058 [1.526, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.521, 10.292], loss: 0.128647, mae: 0.344824, mean_q: 4.054816
 46581/100000: episode: 599, duration: 0.201s, episode steps: 36, steps per second: 179, episode reward: 74.893, mean reward: 2.080 [1.589, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.149, 10.153], loss: 0.115386, mae: 0.327597, mean_q: 4.060843
 46589/100000: episode: 600, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 20.880, mean reward: 2.610 [2.223, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.439, 10.100], loss: 0.116778, mae: 0.327032, mean_q: 4.000118
 46597/100000: episode: 601, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 19.216, mean reward: 2.402 [2.093, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.256, 10.100], loss: 0.099378, mae: 0.316423, mean_q: 4.057133
 46606/100000: episode: 602, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 36.472, mean reward: 4.052 [3.067, 5.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.373, 10.100], loss: 0.096450, mae: 0.310525, mean_q: 4.051765
 46642/100000: episode: 603, duration: 0.181s, episode steps: 36, steps per second: 198, episode reward: 107.869, mean reward: 2.996 [1.848, 7.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.738, 10.224], loss: 0.123955, mae: 0.337845, mean_q: 4.068548
 46737/100000: episode: 604, duration: 0.500s, episode steps: 95, steps per second: 190, episode reward: 184.843, mean reward: 1.946 [1.479, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.505 [-0.602, 10.199], loss: 0.224867, mae: 0.371468, mean_q: 4.069879
 46832/100000: episode: 605, duration: 0.522s, episode steps: 95, steps per second: 182, episode reward: 182.643, mean reward: 1.923 [1.454, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.984, 10.100], loss: 0.133507, mae: 0.357311, mean_q: 4.083175
 46837/100000: episode: 606, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 16.302, mean reward: 3.260 [2.962, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.359, 10.100], loss: 0.202442, mae: 0.344969, mean_q: 4.089627
 46845/100000: episode: 607, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 20.298, mean reward: 2.537 [2.350, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.342, 10.100], loss: 0.154224, mae: 0.397052, mean_q: 4.177278
 46853/100000: episode: 608, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 18.619, mean reward: 2.327 [2.136, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.334, 10.100], loss: 0.130711, mae: 0.364173, mean_q: 4.136852
 46889/100000: episode: 609, duration: 0.217s, episode steps: 36, steps per second: 166, episode reward: 125.676, mean reward: 3.491 [2.108, 5.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.665, 10.476], loss: 0.147617, mae: 0.364740, mean_q: 4.066416
 46894/100000: episode: 610, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 14.113, mean reward: 2.823 [2.422, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.355, 10.100], loss: 0.096184, mae: 0.334152, mean_q: 4.179675
 46902/100000: episode: 611, duration: 0.059s, episode steps: 8, steps per second: 136, episode reward: 18.963, mean reward: 2.370 [2.078, 2.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.262, 10.100], loss: 0.126546, mae: 0.349501, mean_q: 4.075687
 46938/100000: episode: 612, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 78.400, mean reward: 2.178 [1.442, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.464, 10.175], loss: 0.145890, mae: 0.363263, mean_q: 4.093495
 46974/100000: episode: 613, duration: 0.194s, episode steps: 36, steps per second: 186, episode reward: 81.325, mean reward: 2.259 [1.469, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-0.465, 10.112], loss: 0.172051, mae: 0.393724, mean_q: 4.114702
 47069/100000: episode: 614, duration: 0.538s, episode steps: 95, steps per second: 177, episode reward: 190.057, mean reward: 2.001 [1.483, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 1.500 [-0.938, 10.183], loss: 0.354751, mae: 0.392492, mean_q: 4.107326
 47164/100000: episode: 615, duration: 0.510s, episode steps: 95, steps per second: 186, episode reward: 194.779, mean reward: 2.050 [1.448, 6.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.493 [-0.895, 10.100], loss: 0.149762, mae: 0.368710, mean_q: 4.097826
 47200/100000: episode: 616, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 84.152, mean reward: 2.338 [1.763, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-1.692, 10.312], loss: 0.143726, mae: 0.368900, mean_q: 4.110333
 47295/100000: episode: 617, duration: 0.531s, episode steps: 95, steps per second: 179, episode reward: 194.535, mean reward: 2.048 [1.466, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.534, 10.100], loss: 0.161601, mae: 0.377570, mean_q: 4.124792
 47304/100000: episode: 618, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 26.003, mean reward: 2.889 [2.282, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.263, 10.100], loss: 0.120445, mae: 0.336999, mean_q: 3.919915
 47353/100000: episode: 619, duration: 0.265s, episode steps: 49, steps per second: 185, episode reward: 114.162, mean reward: 2.330 [1.599, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.499, 10.162], loss: 0.126893, mae: 0.333128, mean_q: 4.081384
 47362/100000: episode: 620, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 23.210, mean reward: 2.579 [2.167, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.301, 10.100], loss: 0.125748, mae: 0.338014, mean_q: 4.143347
 47412/100000: episode: 621, duration: 0.271s, episode steps: 50, steps per second: 185, episode reward: 108.305, mean reward: 2.166 [1.468, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.286, 10.100], loss: 0.143416, mae: 0.356165, mean_q: 4.104411
 47421/100000: episode: 622, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 21.078, mean reward: 2.342 [2.068, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.224, 10.100], loss: 0.163615, mae: 0.353230, mean_q: 4.090482
 47430/100000: episode: 623, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 27.220, mean reward: 3.024 [2.591, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.377, 10.100], loss: 0.115458, mae: 0.358018, mean_q: 4.212608
 47525/100000: episode: 624, duration: 0.528s, episode steps: 95, steps per second: 180, episode reward: 197.137, mean reward: 2.075 [1.474, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.510 [-0.497, 10.428], loss: 0.137824, mae: 0.356530, mean_q: 4.115455
 47530/100000: episode: 625, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 12.238, mean reward: 2.448 [2.324, 2.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.359, 10.100], loss: 0.128024, mae: 0.358085, mean_q: 4.100648
 47573/100000: episode: 626, duration: 0.258s, episode steps: 43, steps per second: 167, episode reward: 118.215, mean reward: 2.749 [1.545, 6.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.920, 10.294], loss: 0.137329, mae: 0.352583, mean_q: 4.102133
 47578/100000: episode: 627, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 14.959, mean reward: 2.992 [2.627, 3.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.255, 10.100], loss: 0.109658, mae: 0.358191, mean_q: 4.264188
 47628/100000: episode: 628, duration: 0.276s, episode steps: 50, steps per second: 181, episode reward: 95.047, mean reward: 1.901 [1.460, 2.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.614, 10.117], loss: 0.124748, mae: 0.342508, mean_q: 4.182245
 47674/100000: episode: 629, duration: 0.263s, episode steps: 46, steps per second: 175, episode reward: 100.526, mean reward: 2.185 [1.511, 4.054], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.589, 10.200], loss: 0.160841, mae: 0.395798, mean_q: 4.202475
 47710/100000: episode: 630, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 86.816, mean reward: 2.412 [1.513, 4.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.716, 10.250], loss: 0.130014, mae: 0.351039, mean_q: 4.112612
 47756/100000: episode: 631, duration: 0.265s, episode steps: 46, steps per second: 174, episode reward: 105.044, mean reward: 2.284 [1.783, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.530, 10.332], loss: 0.171409, mae: 0.389410, mean_q: 4.156519
 47765/100000: episode: 632, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 29.991, mean reward: 3.332 [2.893, 4.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.364, 10.100], loss: 0.176379, mae: 0.397960, mean_q: 4.145992
 47773/100000: episode: 633, duration: 0.053s, episode steps: 8, steps per second: 151, episode reward: 21.619, mean reward: 2.702 [2.405, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.332, 10.100], loss: 0.107295, mae: 0.372361, mean_q: 4.175554
 47782/100000: episode: 634, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 26.645, mean reward: 2.961 [2.594, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.261, 10.100], loss: 0.189866, mae: 0.382865, mean_q: 4.099861
 47791/100000: episode: 635, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 27.116, mean reward: 3.013 [2.496, 3.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.197, 10.100], loss: 0.133778, mae: 0.382259, mean_q: 4.160929
 47834/100000: episode: 636, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 97.304, mean reward: 2.263 [1.526, 4.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-1.137, 10.233], loss: 0.163900, mae: 0.405575, mean_q: 4.154047
 47880/100000: episode: 637, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 101.720, mean reward: 2.211 [1.611, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.216, 10.293], loss: 0.162026, mae: 0.381000, mean_q: 4.227513
 47888/100000: episode: 638, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 30.084, mean reward: 3.760 [2.797, 4.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.342, 10.100], loss: 0.217008, mae: 0.439877, mean_q: 4.205371
 47897/100000: episode: 639, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 22.240, mean reward: 2.471 [2.114, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.270, 10.100], loss: 0.136376, mae: 0.383501, mean_q: 4.251954
 47943/100000: episode: 640, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 147.669, mean reward: 3.210 [1.818, 5.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.347, 10.521], loss: 0.137038, mae: 0.372503, mean_q: 4.172106
[Info] 2-TH LEVEL FOUND: 7.025991439819336, Considering 10/90 traces
 47948/100000: episode: 641, duration: 4.257s, episode steps: 5, steps per second: 1, episode reward: 12.317, mean reward: 2.463 [2.250, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.482, 10.100], loss: 0.144705, mae: 0.387863, mean_q: 4.357872
 47980/100000: episode: 642, duration: 0.199s, episode steps: 32, steps per second: 161, episode reward: 118.148, mean reward: 3.692 [2.725, 6.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.819, 10.380], loss: 0.160028, mae: 0.389131, mean_q: 4.286293
 48003/100000: episode: 643, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 60.475, mean reward: 2.629 [2.130, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.412], loss: 0.149085, mae: 0.376778, mean_q: 4.211024
 48034/100000: episode: 644, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 83.908, mean reward: 2.707 [1.992, 4.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.035, 10.450], loss: 0.146626, mae: 0.377763, mean_q: 4.268946
 48060/100000: episode: 645, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 91.071, mean reward: 3.503 [2.391, 6.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.119, 10.561], loss: 0.132040, mae: 0.355694, mean_q: 4.254377
 48090/100000: episode: 646, duration: 0.168s, episode steps: 30, steps per second: 178, episode reward: 104.230, mean reward: 3.474 [2.696, 5.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.652, 10.453], loss: 0.141223, mae: 0.371348, mean_q: 4.232518
 48120/100000: episode: 647, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 79.374, mean reward: 2.646 [1.752, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.035, 10.323], loss: 0.121462, mae: 0.352613, mean_q: 4.312886
 48143/100000: episode: 648, duration: 0.133s, episode steps: 23, steps per second: 172, episode reward: 70.060, mean reward: 3.046 [2.098, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.084, 10.376], loss: 0.140597, mae: 0.359109, mean_q: 4.248488
 48173/100000: episode: 649, duration: 0.182s, episode steps: 30, steps per second: 165, episode reward: 93.470, mean reward: 3.116 [2.061, 5.140], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.663, 10.514], loss: 0.179825, mae: 0.405401, mean_q: 4.247766
 48203/100000: episode: 650, duration: 0.185s, episode steps: 30, steps per second: 163, episode reward: 99.370, mean reward: 3.312 [2.516, 4.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.035, 10.435], loss: 0.153368, mae: 0.382480, mean_q: 4.290695
 48222/100000: episode: 651, duration: 0.097s, episode steps: 19, steps per second: 196, episode reward: 64.754, mean reward: 3.408 [2.548, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.432, 10.562], loss: 0.168335, mae: 0.389741, mean_q: 4.308343
 48254/100000: episode: 652, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 117.799, mean reward: 3.681 [1.786, 5.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.077, 10.279], loss: 0.150086, mae: 0.396788, mean_q: 4.365592
 48285/100000: episode: 653, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 91.420, mean reward: 2.949 [1.742, 7.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.372, 10.261], loss: 0.167126, mae: 0.408936, mean_q: 4.348779
 48304/100000: episode: 654, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 50.732, mean reward: 2.670 [2.255, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.323], loss: 0.144519, mae: 0.386147, mean_q: 4.268949
 48330/100000: episode: 655, duration: 0.149s, episode steps: 26, steps per second: 175, episode reward: 120.595, mean reward: 4.638 [2.744, 8.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.109, 10.505], loss: 0.191434, mae: 0.426778, mean_q: 4.385516
 48360/100000: episode: 656, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 105.770, mean reward: 3.526 [1.894, 7.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.818, 10.491], loss: 0.146233, mae: 0.382411, mean_q: 4.388623
 48391/100000: episode: 657, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 64.953, mean reward: 2.095 [1.454, 4.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.244, 10.100], loss: 0.161981, mae: 0.390957, mean_q: 4.386214
 48421/100000: episode: 658, duration: 0.148s, episode steps: 30, steps per second: 203, episode reward: 165.521, mean reward: 5.517 [3.157, 8.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.180, 10.500], loss: 0.229839, mae: 0.440295, mean_q: 4.436289
 48451/100000: episode: 659, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 96.902, mean reward: 3.230 [2.169, 5.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.204, 10.378], loss: 0.188252, mae: 0.416710, mean_q: 4.501455
 48482/100000: episode: 660, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 77.835, mean reward: 2.511 [1.508, 5.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.578, 10.238], loss: 0.158440, mae: 0.400448, mean_q: 4.389383
 48493/100000: episode: 661, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 29.412, mean reward: 2.674 [2.229, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.382], loss: 0.163749, mae: 0.411137, mean_q: 4.485109
 48524/100000: episode: 662, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 105.674, mean reward: 3.409 [2.402, 6.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.170, 10.487], loss: 0.178515, mae: 0.417635, mean_q: 4.438476
 48543/100000: episode: 663, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 59.068, mean reward: 3.109 [2.119, 5.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.070, 10.350], loss: 0.200149, mae: 0.428249, mean_q: 4.571703
 48574/100000: episode: 664, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 85.358, mean reward: 2.753 [2.227, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.526, 10.384], loss: 0.199341, mae: 0.426181, mean_q: 4.498534
 48606/100000: episode: 665, duration: 0.191s, episode steps: 32, steps per second: 168, episode reward: 114.018, mean reward: 3.563 [1.653, 5.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.035, 10.260], loss: 0.204566, mae: 0.452909, mean_q: 4.579149
 48617/100000: episode: 666, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 31.783, mean reward: 2.889 [2.457, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.391], loss: 0.285519, mae: 0.465991, mean_q: 4.564864
 48628/100000: episode: 667, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 30.998, mean reward: 2.818 [2.428, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.622, 10.321], loss: 0.210652, mae: 0.449229, mean_q: 4.599298
 48647/100000: episode: 668, duration: 0.112s, episode steps: 19, steps per second: 170, episode reward: 63.587, mean reward: 3.347 [2.271, 4.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.857, 10.405], loss: 0.194529, mae: 0.428411, mean_q: 4.650906
 48679/100000: episode: 669, duration: 0.181s, episode steps: 32, steps per second: 176, episode reward: 91.669, mean reward: 2.865 [1.829, 4.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-1.071, 10.366], loss: 0.216412, mae: 0.446109, mean_q: 4.656293
 48698/100000: episode: 670, duration: 0.109s, episode steps: 19, steps per second: 175, episode reward: 52.080, mean reward: 2.741 [2.304, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.084, 10.307], loss: 0.173034, mae: 0.397706, mean_q: 4.628620
 48728/100000: episode: 671, duration: 0.151s, episode steps: 30, steps per second: 199, episode reward: 195.800, mean reward: 6.527 [3.870, 11.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-1.068, 10.567], loss: 0.192321, mae: 0.439608, mean_q: 4.596426
 48759/100000: episode: 672, duration: 0.175s, episode steps: 31, steps per second: 178, episode reward: 69.844, mean reward: 2.253 [1.913, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.632, 10.384], loss: 0.227408, mae: 0.454267, mean_q: 4.633405
 48789/100000: episode: 673, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 113.064, mean reward: 3.769 [1.639, 6.642], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.428, 10.282], loss: 0.267086, mae: 0.504691, mean_q: 4.696166
 48819/100000: episode: 674, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 93.756, mean reward: 3.125 [2.128, 4.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.189, 10.346], loss: 0.199377, mae: 0.440933, mean_q: 4.558184
 48850/100000: episode: 675, duration: 0.167s, episode steps: 31, steps per second: 185, episode reward: 81.068, mean reward: 2.615 [1.514, 4.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.374, 10.117], loss: 0.186498, mae: 0.430523, mean_q: 4.652414
 48880/100000: episode: 676, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 73.934, mean reward: 2.464 [1.540, 4.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.236, 10.191], loss: 0.177077, mae: 0.408228, mean_q: 4.619819
 48912/100000: episode: 677, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 110.334, mean reward: 3.448 [2.587, 5.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-1.343, 10.559], loss: 0.232774, mae: 0.428636, mean_q: 4.656831
 48923/100000: episode: 678, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 31.548, mean reward: 2.868 [1.917, 4.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.316], loss: 0.220877, mae: 0.450064, mean_q: 4.815423
 48953/100000: episode: 679, duration: 0.154s, episode steps: 30, steps per second: 194, episode reward: 111.400, mean reward: 3.713 [2.360, 11.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.900, 10.524], loss: 0.218847, mae: 0.464278, mean_q: 4.808766
 48976/100000: episode: 680, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 60.727, mean reward: 2.640 [2.007, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.458, 10.336], loss: 0.212138, mae: 0.441897, mean_q: 4.608451
 48987/100000: episode: 681, duration: 0.063s, episode steps: 11, steps per second: 173, episode reward: 41.132, mean reward: 3.739 [3.190, 5.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.179, 10.514], loss: 0.196104, mae: 0.451777, mean_q: 4.693945
 49013/100000: episode: 682, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 106.243, mean reward: 4.086 [2.707, 5.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.035, 10.506], loss: 0.228053, mae: 0.435975, mean_q: 4.646162
 49036/100000: episode: 683, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 63.122, mean reward: 2.744 [1.890, 4.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.412], loss: 0.211285, mae: 0.425329, mean_q: 4.696101
 49056/100000: episode: 684, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 66.105, mean reward: 3.305 [1.759, 11.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.892, 10.276], loss: 0.215143, mae: 0.445176, mean_q: 4.816067
 49067/100000: episode: 685, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 30.794, mean reward: 2.799 [2.328, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.350, 10.351], loss: 0.453527, mae: 0.470621, mean_q: 4.715178
 49090/100000: episode: 686, duration: 0.147s, episode steps: 23, steps per second: 156, episode reward: 81.546, mean reward: 3.545 [2.491, 9.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.725, 10.400], loss: 0.276002, mae: 0.476849, mean_q: 4.831326
 49113/100000: episode: 687, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 57.987, mean reward: 2.521 [2.107, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.730, 10.408], loss: 0.288656, mae: 0.478778, mean_q: 4.694450
 49132/100000: episode: 688, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 49.727, mean reward: 2.617 [2.031, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.845, 10.345], loss: 0.237317, mae: 0.468364, mean_q: 4.814655
 49163/100000: episode: 689, duration: 0.182s, episode steps: 31, steps per second: 170, episode reward: 120.473, mean reward: 3.886 [2.487, 6.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.402, 10.557], loss: 0.277945, mae: 0.472618, mean_q: 4.881847
 49194/100000: episode: 690, duration: 0.181s, episode steps: 31, steps per second: 172, episode reward: 104.042, mean reward: 3.356 [2.696, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.119, 10.509], loss: 0.253829, mae: 0.465623, mean_q: 4.772975
 49224/100000: episode: 691, duration: 0.164s, episode steps: 30, steps per second: 183, episode reward: 132.293, mean reward: 4.410 [2.594, 7.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.035, 10.439], loss: 0.292200, mae: 0.489914, mean_q: 4.850482
 49254/100000: episode: 692, duration: 0.170s, episode steps: 30, steps per second: 177, episode reward: 111.280, mean reward: 3.709 [2.356, 9.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.165, 10.325], loss: 0.215573, mae: 0.444326, mean_q: 4.824309
 49280/100000: episode: 693, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 108.936, mean reward: 4.190 [2.506, 6.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.346, 10.450], loss: 0.317384, mae: 0.487322, mean_q: 4.868978
 49303/100000: episode: 694, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 72.248, mean reward: 3.141 [2.447, 4.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.497, 10.456], loss: 0.241974, mae: 0.457971, mean_q: 4.789397
 49334/100000: episode: 695, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 103.909, mean reward: 3.352 [2.522, 5.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.264, 10.507], loss: 0.261144, mae: 0.482615, mean_q: 4.942272
 49364/100000: episode: 696, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 91.272, mean reward: 3.042 [2.635, 4.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.296, 10.498], loss: 0.251015, mae: 0.498601, mean_q: 4.929808
 49384/100000: episode: 697, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 57.447, mean reward: 2.872 [2.195, 5.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.478], loss: 0.220295, mae: 0.454350, mean_q: 4.955499
 49395/100000: episode: 698, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 32.810, mean reward: 2.983 [2.404, 3.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.807, 10.399], loss: 0.195470, mae: 0.433788, mean_q: 4.829267
 49415/100000: episode: 699, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 79.422, mean reward: 3.971 [2.646, 6.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.035, 10.468], loss: 0.338526, mae: 0.492702, mean_q: 4.955917
 49435/100000: episode: 700, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 56.990, mean reward: 2.849 [2.100, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.329, 10.367], loss: 0.181598, mae: 0.429322, mean_q: 4.946034
 49461/100000: episode: 701, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 71.685, mean reward: 2.757 [2.058, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.035, 10.468], loss: 0.281870, mae: 0.492641, mean_q: 5.041860
 49493/100000: episode: 702, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 90.189, mean reward: 2.818 [1.790, 4.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.251, 10.270], loss: 0.323507, mae: 0.528361, mean_q: 5.008953
 49504/100000: episode: 703, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 33.294, mean reward: 3.027 [2.546, 3.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.429], loss: 0.190652, mae: 0.434832, mean_q: 4.890804
 49534/100000: episode: 704, duration: 0.172s, episode steps: 30, steps per second: 174, episode reward: 95.635, mean reward: 3.188 [1.916, 5.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.267, 10.356], loss: 0.230332, mae: 0.468064, mean_q: 4.989184
 49565/100000: episode: 705, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 105.585, mean reward: 3.406 [1.599, 7.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.615, 10.107], loss: 0.303299, mae: 0.478490, mean_q: 5.086350
 49591/100000: episode: 706, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 65.609, mean reward: 2.523 [1.784, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.243], loss: 0.338105, mae: 0.502710, mean_q: 5.010190
 49610/100000: episode: 707, duration: 0.108s, episode steps: 19, steps per second: 177, episode reward: 52.658, mean reward: 2.771 [2.243, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.394], loss: 0.202965, mae: 0.452966, mean_q: 4.960315
 49630/100000: episode: 708, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 52.002, mean reward: 2.600 [2.204, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.466, 10.393], loss: 0.279174, mae: 0.504921, mean_q: 5.016065
 49660/100000: episode: 709, duration: 0.173s, episode steps: 30, steps per second: 173, episode reward: 209.402, mean reward: 6.980 [2.701, 22.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.831, 10.480], loss: 0.333918, mae: 0.515323, mean_q: 5.005500
 49690/100000: episode: 710, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 78.931, mean reward: 2.631 [1.772, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.360, 10.280], loss: 0.384680, mae: 0.528536, mean_q: 5.073869
 49701/100000: episode: 711, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 34.988, mean reward: 3.181 [2.449, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.082, 10.461], loss: 0.784017, mae: 0.589948, mean_q: 5.060971
 49720/100000: episode: 712, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 67.827, mean reward: 3.570 [2.521, 5.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.240, 10.556], loss: 0.337744, mae: 0.526049, mean_q: 5.097446
 49739/100000: episode: 713, duration: 0.112s, episode steps: 19, steps per second: 169, episode reward: 56.399, mean reward: 2.968 [2.471, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.511], loss: 0.386352, mae: 0.532370, mean_q: 5.188898
 49762/100000: episode: 714, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 58.145, mean reward: 2.528 [1.619, 4.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.659, 10.231], loss: 0.289376, mae: 0.477104, mean_q: 5.064536
 49773/100000: episode: 715, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 28.041, mean reward: 2.549 [2.041, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.639, 10.312], loss: 0.230057, mae: 0.481075, mean_q: 5.198084
 49803/100000: episode: 716, duration: 0.170s, episode steps: 30, steps per second: 176, episode reward: 77.118, mean reward: 2.571 [1.805, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.376, 10.283], loss: 0.240171, mae: 0.462877, mean_q: 5.018773
 49823/100000: episode: 717, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 63.801, mean reward: 3.190 [1.957, 15.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.401, 10.379], loss: 0.355083, mae: 0.522895, mean_q: 5.266368
 49834/100000: episode: 718, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 30.116, mean reward: 2.738 [2.252, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.035, 10.349], loss: 0.372375, mae: 0.539257, mean_q: 5.167252
 49864/100000: episode: 719, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 230.087, mean reward: 7.670 [3.457, 29.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.186, 10.652], loss: 0.245310, mae: 0.488216, mean_q: 5.190429
 49894/100000: episode: 720, duration: 0.175s, episode steps: 30, steps per second: 172, episode reward: 76.394, mean reward: 2.546 [1.702, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.065, 10.356], loss: 0.283808, mae: 0.487953, mean_q: 5.166388
 49913/100000: episode: 721, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 48.401, mean reward: 2.547 [2.113, 3.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.513], loss: 0.461711, mae: 0.514675, mean_q: 5.123318
 49944/100000: episode: 722, duration: 0.173s, episode steps: 31, steps per second: 180, episode reward: 77.215, mean reward: 2.491 [1.852, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.514, 10.377], loss: 0.335620, mae: 0.518240, mean_q: 5.131553
 49975/100000: episode: 723, duration: 0.189s, episode steps: 31, steps per second: 164, episode reward: 116.162, mean reward: 3.747 [2.812, 4.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.035, 10.555], loss: 0.630955, mae: 0.570943, mean_q: 5.257740
 50005/100000: episode: 724, duration: 0.171s, episode steps: 30, steps per second: 175, episode reward: 83.489, mean reward: 2.783 [1.763, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.898, 10.391], loss: 0.329718, mae: 0.562465, mean_q: 5.338950
 50035/100000: episode: 725, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 83.519, mean reward: 2.784 [1.821, 7.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.427, 10.367], loss: 0.365882, mae: 0.553067, mean_q: 5.287536
 50066/100000: episode: 726, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 78.841, mean reward: 2.543 [1.973, 4.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.499, 10.514], loss: 0.705411, mae: 0.605105, mean_q: 5.335338
 50089/100000: episode: 727, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 61.664, mean reward: 2.681 [1.981, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.035, 10.458], loss: 0.351923, mae: 0.558018, mean_q: 5.229852
 50100/100000: episode: 728, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 38.000, mean reward: 3.455 [3.115, 4.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.491], loss: 0.755719, mae: 0.679097, mean_q: 5.507088
 50119/100000: episode: 729, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 58.046, mean reward: 3.055 [2.528, 4.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.196, 10.475], loss: 0.551344, mae: 0.556670, mean_q: 5.366909
 50151/100000: episode: 730, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 89.085, mean reward: 2.784 [2.084, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.432, 10.382], loss: 0.377973, mae: 0.529529, mean_q: 5.244941
[Info] 3-TH LEVEL FOUND: 10.248340606689453, Considering 10/90 traces
 50162/100000: episode: 731, duration: 4.281s, episode steps: 11, steps per second: 3, episode reward: 34.699, mean reward: 3.154 [2.608, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.518, 10.533], loss: 0.310637, mae: 0.559643, mean_q: 5.057168
 50189/100000: episode: 732, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 114.108, mean reward: 4.226 [2.753, 5.531], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.454, 10.549], loss: 0.297593, mae: 0.519840, mean_q: 5.256376
 50216/100000: episode: 733, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 142.729, mean reward: 5.286 [2.256, 14.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.368, 10.412], loss: 0.586865, mae: 0.588947, mean_q: 5.316003
 50244/100000: episode: 734, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 233.368, mean reward: 8.335 [2.500, 52.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.321, 10.403], loss: 0.277442, mae: 0.505154, mean_q: 5.287539
 50273/100000: episode: 735, duration: 0.163s, episode steps: 29, steps per second: 178, episode reward: 130.438, mean reward: 4.498 [2.251, 9.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.191, 10.317], loss: 0.817454, mae: 0.594572, mean_q: 5.342056
 50286/100000: episode: 736, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 77.847, mean reward: 5.988 [4.120, 8.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.544], loss: 1.011217, mae: 0.707039, mean_q: 5.597723
 50313/100000: episode: 737, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 117.950, mean reward: 4.369 [2.449, 8.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.243, 10.455], loss: 1.795318, mae: 0.806190, mean_q: 5.472345
 50340/100000: episode: 738, duration: 0.138s, episode steps: 27, steps per second: 196, episode reward: 107.237, mean reward: 3.972 [1.999, 10.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.274, 10.418], loss: 0.310377, mae: 0.565406, mean_q: 5.462185
 50368/100000: episode: 739, duration: 0.165s, episode steps: 28, steps per second: 170, episode reward: 133.923, mean reward: 4.783 [2.734, 11.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.035, 10.493], loss: 1.143027, mae: 0.610175, mean_q: 5.637731
 50397/100000: episode: 740, duration: 0.167s, episode steps: 29, steps per second: 173, episode reward: 85.748, mean reward: 2.957 [1.687, 4.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.121, 10.331], loss: 0.475390, mae: 0.577642, mean_q: 5.478507
 50424/100000: episode: 741, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 120.267, mean reward: 4.454 [2.911, 6.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.509, 10.502], loss: 0.465919, mae: 0.562136, mean_q: 5.393234
 50453/100000: episode: 742, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 110.264, mean reward: 3.802 [2.226, 6.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.355], loss: 0.731928, mae: 0.595436, mean_q: 5.584361
 50476/100000: episode: 743, duration: 0.134s, episode steps: 23, steps per second: 172, episode reward: 81.747, mean reward: 3.554 [2.054, 5.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.377], loss: 0.452836, mae: 0.565257, mean_q: 5.484815
 50503/100000: episode: 744, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 179.439, mean reward: 6.646 [2.898, 16.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.395, 10.426], loss: 0.411408, mae: 0.558209, mean_q: 5.550610
 50531/100000: episode: 745, duration: 0.158s, episode steps: 28, steps per second: 178, episode reward: 147.475, mean reward: 5.267 [2.449, 12.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.652, 10.465], loss: 1.079126, mae: 0.678272, mean_q: 5.582330
 50560/100000: episode: 746, duration: 0.162s, episode steps: 29, steps per second: 180, episode reward: 96.734, mean reward: 3.336 [2.467, 7.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.518, 10.414], loss: 2.471134, mae: 0.935096, mean_q: 5.510459
[Info] FALSIFICATION!
[Info] Levels: [4.9680176, 7.0259914, 10.248341, 13.029345]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.12]
[Info] Error Prob: 0.00012000000000000003

 50563/100000: episode: 747, duration: 4.390s, episode steps: 3, steps per second: 1, episode reward: 129.173, mean reward: 43.058 [10.609, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.850 [-0.329, 8.729], loss: 0.585077, mae: 0.873721, mean_q: 5.880390
 50663/100000: episode: 748, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 207.949, mean reward: 2.079 [1.481, 5.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.186, 10.192], loss: 0.942615, mae: 0.636969, mean_q: 5.605665
 50763/100000: episode: 749, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 185.325, mean reward: 1.853 [1.452, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.944, 10.228], loss: 3.677314, mae: 0.850010, mean_q: 5.596978
 50863/100000: episode: 750, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 179.712, mean reward: 1.797 [1.457, 2.616], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.861, 10.347], loss: 2.129958, mae: 0.789055, mean_q: 5.694169
 50963/100000: episode: 751, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 186.289, mean reward: 1.863 [1.464, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.454, 10.177], loss: 4.649576, mae: 0.912207, mean_q: 5.703552
 51063/100000: episode: 752, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 194.260, mean reward: 1.943 [1.505, 5.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.142, 10.142], loss: 3.734944, mae: 0.789183, mean_q: 5.738022
 51163/100000: episode: 753, duration: 0.563s, episode steps: 100, steps per second: 177, episode reward: 184.570, mean reward: 1.846 [1.474, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.549, 10.110], loss: 1.028197, mae: 0.719089, mean_q: 5.625819
 51263/100000: episode: 754, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 191.357, mean reward: 1.914 [1.452, 4.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.523, 10.149], loss: 0.657093, mae: 0.610112, mean_q: 5.621713
 51363/100000: episode: 755, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 178.637, mean reward: 1.786 [1.450, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.767, 10.098], loss: 0.553095, mae: 0.601961, mean_q: 5.475026
 51463/100000: episode: 756, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 200.264, mean reward: 2.003 [1.486, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.718, 10.118], loss: 0.758514, mae: 0.596463, mean_q: 5.478704
 51563/100000: episode: 757, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 185.851, mean reward: 1.859 [1.453, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.872, 10.164], loss: 0.596515, mae: 0.593065, mean_q: 5.467834
 51663/100000: episode: 758, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.932, mean reward: 1.899 [1.480, 2.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.000, 10.239], loss: 1.515653, mae: 0.721224, mean_q: 5.575868
 51763/100000: episode: 759, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 185.167, mean reward: 1.852 [1.456, 4.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.182, 10.168], loss: 0.886095, mae: 0.609493, mean_q: 5.505653
 51863/100000: episode: 760, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 198.960, mean reward: 1.990 [1.546, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.477, 10.258], loss: 1.872751, mae: 0.690294, mean_q: 5.525348
 51963/100000: episode: 761, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 193.767, mean reward: 1.938 [1.445, 5.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.232, 10.098], loss: 2.547247, mae: 0.727314, mean_q: 5.454146
 52063/100000: episode: 762, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 188.616, mean reward: 1.886 [1.464, 2.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.922, 10.098], loss: 0.650345, mae: 0.633102, mean_q: 5.541121
 52163/100000: episode: 763, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 183.239, mean reward: 1.832 [1.446, 6.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.933, 10.228], loss: 1.744327, mae: 0.650876, mean_q: 5.532370
 52263/100000: episode: 764, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.470, mean reward: 1.955 [1.482, 4.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.611, 10.098], loss: 2.822544, mae: 0.698001, mean_q: 5.461226
 52363/100000: episode: 765, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 213.770, mean reward: 2.138 [1.474, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.558, 10.416], loss: 1.673330, mae: 0.731679, mean_q: 5.577100
 52463/100000: episode: 766, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 197.807, mean reward: 1.978 [1.446, 3.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.509, 10.225], loss: 1.734570, mae: 0.677403, mean_q: 5.494981
 52563/100000: episode: 767, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 242.115, mean reward: 2.421 [1.469, 6.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.997, 10.142], loss: 0.604091, mae: 0.617078, mean_q: 5.478760
 52663/100000: episode: 768, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 201.027, mean reward: 2.010 [1.501, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.172, 10.262], loss: 2.199620, mae: 0.707296, mean_q: 5.429272
 52763/100000: episode: 769, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 194.173, mean reward: 1.942 [1.465, 4.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.239, 10.105], loss: 2.893058, mae: 0.795439, mean_q: 5.495072
 52863/100000: episode: 770, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.288, mean reward: 1.933 [1.450, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.926, 10.220], loss: 1.633060, mae: 0.634889, mean_q: 5.410825
 52963/100000: episode: 771, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 201.276, mean reward: 2.013 [1.520, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.642, 10.201], loss: 0.499993, mae: 0.589973, mean_q: 5.284756
 53063/100000: episode: 772, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 184.311, mean reward: 1.843 [1.464, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.345, 10.131], loss: 1.171790, mae: 0.611613, mean_q: 5.350824
 53163/100000: episode: 773, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 185.653, mean reward: 1.857 [1.459, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.564, 10.221], loss: 2.159391, mae: 0.696741, mean_q: 5.285648
 53263/100000: episode: 774, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 190.029, mean reward: 1.900 [1.483, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.873, 10.098], loss: 2.233325, mae: 0.701155, mean_q: 5.253699
 53363/100000: episode: 775, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 175.250, mean reward: 1.752 [1.456, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.798, 10.098], loss: 0.688646, mae: 0.578151, mean_q: 5.187665
 53463/100000: episode: 776, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 180.510, mean reward: 1.805 [1.478, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.337, 10.117], loss: 3.570956, mae: 0.731724, mean_q: 5.088001
 53563/100000: episode: 777, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 201.581, mean reward: 2.016 [1.479, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.206, 10.098], loss: 1.611734, mae: 0.608341, mean_q: 5.017519
 53663/100000: episode: 778, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 205.835, mean reward: 2.058 [1.438, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.122, 10.098], loss: 3.139867, mae: 0.661892, mean_q: 4.995493
 53763/100000: episode: 779, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 226.430, mean reward: 2.264 [1.505, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.171, 10.271], loss: 1.608161, mae: 0.613122, mean_q: 4.959899
 53863/100000: episode: 780, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.614, mean reward: 1.856 [1.454, 2.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.097, 10.098], loss: 1.026312, mae: 0.565732, mean_q: 5.011985
 53963/100000: episode: 781, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 192.763, mean reward: 1.928 [1.497, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.510, 10.098], loss: 3.063978, mae: 0.710700, mean_q: 4.999897
 54063/100000: episode: 782, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.999, mean reward: 1.970 [1.512, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.709, 10.288], loss: 0.338005, mae: 0.462113, mean_q: 4.676483
 54163/100000: episode: 783, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 191.103, mean reward: 1.911 [1.487, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.260], loss: 1.238994, mae: 0.593097, mean_q: 4.742966
 54263/100000: episode: 784, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 179.646, mean reward: 1.796 [1.436, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.723, 10.144], loss: 3.148159, mae: 0.665946, mean_q: 4.714415
 54363/100000: episode: 785, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 192.488, mean reward: 1.925 [1.432, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.187, 10.098], loss: 0.442964, mae: 0.452917, mean_q: 4.523486
 54463/100000: episode: 786, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 194.540, mean reward: 1.945 [1.452, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.410, 10.225], loss: 0.738016, mae: 0.466482, mean_q: 4.568170
 54563/100000: episode: 787, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.458, mean reward: 1.875 [1.462, 4.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.067, 10.098], loss: 2.688249, mae: 0.581642, mean_q: 4.559526
 54663/100000: episode: 788, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 199.156, mean reward: 1.992 [1.435, 4.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.247, 10.098], loss: 0.685470, mae: 0.493463, mean_q: 4.449517
 54763/100000: episode: 789, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.081, mean reward: 1.961 [1.467, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.886, 10.331], loss: 0.532549, mae: 0.438772, mean_q: 4.365114
 54863/100000: episode: 790, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 189.013, mean reward: 1.890 [1.446, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.528, 10.241], loss: 0.203248, mae: 0.383149, mean_q: 4.329670
 54963/100000: episode: 791, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 178.426, mean reward: 1.784 [1.460, 2.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.538, 10.233], loss: 1.531844, mae: 0.466540, mean_q: 4.377172
 55063/100000: episode: 792, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 213.053, mean reward: 2.131 [1.487, 4.250], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.982, 10.427], loss: 1.720546, mae: 0.471103, mean_q: 4.270947
 55163/100000: episode: 793, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 208.493, mean reward: 2.085 [1.468, 4.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.418, 10.098], loss: 0.667053, mae: 0.475002, mean_q: 4.233129
 55263/100000: episode: 794, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 183.360, mean reward: 1.834 [1.458, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.314, 10.098], loss: 1.318325, mae: 0.410733, mean_q: 4.086001
 55363/100000: episode: 795, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 196.038, mean reward: 1.960 [1.451, 4.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.333, 10.360], loss: 0.128366, mae: 0.337593, mean_q: 4.009663
 55463/100000: episode: 796, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 185.006, mean reward: 1.850 [1.512, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.613, 10.260], loss: 0.184510, mae: 0.346312, mean_q: 3.971979
 55563/100000: episode: 797, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 185.071, mean reward: 1.851 [1.457, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.527, 10.098], loss: 0.119482, mae: 0.303059, mean_q: 3.853013
 55663/100000: episode: 798, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.378, mean reward: 1.874 [1.472, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.141, 10.098], loss: 0.083241, mae: 0.296106, mean_q: 3.815876
 55763/100000: episode: 799, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 193.074, mean reward: 1.931 [1.480, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.341, 10.098], loss: 0.076806, mae: 0.283039, mean_q: 3.796610
 55863/100000: episode: 800, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 178.476, mean reward: 1.785 [1.449, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.630, 10.098], loss: 0.081030, mae: 0.284879, mean_q: 3.818358
 55963/100000: episode: 801, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 186.167, mean reward: 1.862 [1.475, 2.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.530, 10.098], loss: 0.096459, mae: 0.308468, mean_q: 3.834699
 56063/100000: episode: 802, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 179.859, mean reward: 1.799 [1.463, 2.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.357, 10.098], loss: 0.088956, mae: 0.290909, mean_q: 3.831801
 56163/100000: episode: 803, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 182.462, mean reward: 1.825 [1.495, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.294, 10.328], loss: 0.076724, mae: 0.282721, mean_q: 3.806544
 56263/100000: episode: 804, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 192.475, mean reward: 1.925 [1.482, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.974, 10.228], loss: 0.079815, mae: 0.285055, mean_q: 3.813566
 56363/100000: episode: 805, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 200.152, mean reward: 2.002 [1.455, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.571, 10.131], loss: 0.084497, mae: 0.288027, mean_q: 3.805301
 56463/100000: episode: 806, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 195.377, mean reward: 1.954 [1.450, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.802, 10.103], loss: 0.075317, mae: 0.280677, mean_q: 3.807055
 56563/100000: episode: 807, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 211.273, mean reward: 2.113 [1.456, 3.420], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.996, 10.098], loss: 0.086830, mae: 0.297294, mean_q: 3.830052
 56663/100000: episode: 808, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 192.234, mean reward: 1.922 [1.480, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.777, 10.098], loss: 0.083497, mae: 0.283384, mean_q: 3.819454
 56763/100000: episode: 809, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 194.855, mean reward: 1.949 [1.475, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.560, 10.098], loss: 0.084695, mae: 0.293261, mean_q: 3.830227
 56863/100000: episode: 810, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 186.083, mean reward: 1.861 [1.472, 3.350], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.983, 10.098], loss: 0.077850, mae: 0.278476, mean_q: 3.839024
 56963/100000: episode: 811, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 187.060, mean reward: 1.871 [1.455, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.938, 10.261], loss: 0.080849, mae: 0.283807, mean_q: 3.840762
 57063/100000: episode: 812, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 190.050, mean reward: 1.900 [1.480, 4.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.065, 10.098], loss: 0.079855, mae: 0.282980, mean_q: 3.832011
 57163/100000: episode: 813, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 205.093, mean reward: 2.051 [1.481, 6.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.990, 10.098], loss: 0.077079, mae: 0.278786, mean_q: 3.845238
 57263/100000: episode: 814, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 204.254, mean reward: 2.043 [1.492, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.623, 10.098], loss: 0.088038, mae: 0.293816, mean_q: 3.856644
 57363/100000: episode: 815, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 183.168, mean reward: 1.832 [1.457, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.838, 10.199], loss: 0.089480, mae: 0.296229, mean_q: 3.836775
 57463/100000: episode: 816, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 179.622, mean reward: 1.796 [1.449, 2.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.660, 10.098], loss: 0.075395, mae: 0.277597, mean_q: 3.826477
 57563/100000: episode: 817, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 192.036, mean reward: 1.920 [1.456, 3.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.694, 10.098], loss: 0.081493, mae: 0.281102, mean_q: 3.805209
 57663/100000: episode: 818, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 204.111, mean reward: 2.041 [1.497, 4.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-1.151, 10.098], loss: 0.080640, mae: 0.285191, mean_q: 3.808855
 57763/100000: episode: 819, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 186.994, mean reward: 1.870 [1.479, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.581, 10.201], loss: 0.081992, mae: 0.284161, mean_q: 3.804093
 57863/100000: episode: 820, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 187.465, mean reward: 1.875 [1.489, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.240, 10.216], loss: 0.077480, mae: 0.280214, mean_q: 3.809932
 57963/100000: episode: 821, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 202.504, mean reward: 2.025 [1.484, 6.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.801, 10.104], loss: 0.076824, mae: 0.272716, mean_q: 3.790236
 58063/100000: episode: 822, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 207.830, mean reward: 2.078 [1.464, 7.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.159, 10.271], loss: 0.078426, mae: 0.272570, mean_q: 3.790163
 58163/100000: episode: 823, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 185.790, mean reward: 1.858 [1.440, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.900, 10.188], loss: 0.074805, mae: 0.274196, mean_q: 3.813460
 58263/100000: episode: 824, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 185.756, mean reward: 1.858 [1.471, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.335, 10.211], loss: 0.091228, mae: 0.284492, mean_q: 3.806218
 58363/100000: episode: 825, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 192.950, mean reward: 1.930 [1.463, 3.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.201, 10.354], loss: 0.100548, mae: 0.298486, mean_q: 3.838442
 58463/100000: episode: 826, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 183.137, mean reward: 1.831 [1.459, 3.593], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.134, 10.098], loss: 0.094315, mae: 0.289575, mean_q: 3.832417
 58563/100000: episode: 827, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 183.889, mean reward: 1.839 [1.443, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.832, 10.098], loss: 0.090726, mae: 0.277577, mean_q: 3.812280
 58663/100000: episode: 828, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 185.051, mean reward: 1.851 [1.456, 3.010], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.273, 10.098], loss: 0.079141, mae: 0.280737, mean_q: 3.803545
 58763/100000: episode: 829, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: 212.597, mean reward: 2.126 [1.505, 5.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.005, 10.325], loss: 0.088728, mae: 0.270733, mean_q: 3.787740
 58863/100000: episode: 830, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 206.224, mean reward: 2.062 [1.500, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.404, 10.428], loss: 0.095307, mae: 0.286213, mean_q: 3.808291
 58963/100000: episode: 831, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: 204.192, mean reward: 2.042 [1.473, 5.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.613, 10.276], loss: 0.091912, mae: 0.279823, mean_q: 3.787858
 59063/100000: episode: 832, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 181.131, mean reward: 1.811 [1.466, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.236, 10.098], loss: 0.090688, mae: 0.280432, mean_q: 3.794144
 59163/100000: episode: 833, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 188.760, mean reward: 1.888 [1.472, 2.989], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.546, 10.165], loss: 0.089038, mae: 0.282764, mean_q: 3.803701
 59263/100000: episode: 834, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 259.654, mean reward: 2.597 [1.474, 5.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.944, 10.098], loss: 0.087249, mae: 0.284348, mean_q: 3.797133
 59363/100000: episode: 835, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 188.476, mean reward: 1.885 [1.478, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.758, 10.104], loss: 0.094308, mae: 0.297417, mean_q: 3.803115
 59463/100000: episode: 836, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 183.881, mean reward: 1.839 [1.444, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.260, 10.275], loss: 0.099498, mae: 0.305173, mean_q: 3.823273
 59563/100000: episode: 837, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 191.593, mean reward: 1.916 [1.443, 5.124], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.592, 10.098], loss: 0.099592, mae: 0.297667, mean_q: 3.816756
 59663/100000: episode: 838, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 193.428, mean reward: 1.934 [1.434, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.807, 10.098], loss: 0.093637, mae: 0.299715, mean_q: 3.824098
 59763/100000: episode: 839, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.661, mean reward: 1.857 [1.466, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.857, 10.098], loss: 0.089337, mae: 0.297681, mean_q: 3.827104
 59863/100000: episode: 840, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 210.035, mean reward: 2.100 [1.474, 4.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.388, 10.098], loss: 0.110670, mae: 0.305692, mean_q: 3.832051
 59963/100000: episode: 841, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 221.885, mean reward: 2.219 [1.481, 5.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.949, 10.098], loss: 0.137857, mae: 0.331065, mean_q: 3.843810
 60063/100000: episode: 842, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 216.860, mean reward: 2.169 [1.452, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.421, 10.196], loss: 0.095357, mae: 0.300353, mean_q: 3.853206
 60163/100000: episode: 843, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 201.660, mean reward: 2.017 [1.483, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.025, 10.271], loss: 0.103236, mae: 0.306117, mean_q: 3.856780
 60263/100000: episode: 844, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 176.141, mean reward: 1.761 [1.468, 3.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.879, 10.145], loss: 0.094119, mae: 0.297562, mean_q: 3.843272
 60363/100000: episode: 845, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.212, mean reward: 1.862 [1.456, 4.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.277, 10.217], loss: 0.090310, mae: 0.292403, mean_q: 3.829311
 60463/100000: episode: 846, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 183.698, mean reward: 1.837 [1.438, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.207, 10.098], loss: 0.092699, mae: 0.293381, mean_q: 3.825933
[Info] 1-TH LEVEL FOUND: 5.645441055297852, Considering 10/90 traces
 60563/100000: episode: 847, duration: 4.723s, episode steps: 100, steps per second: 21, episode reward: 196.369, mean reward: 1.964 [1.461, 4.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.837, 10.098], loss: 0.093885, mae: 0.297122, mean_q: 3.833074
 60643/100000: episode: 848, duration: 0.431s, episode steps: 80, steps per second: 186, episode reward: 171.986, mean reward: 2.150 [1.463, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 1.638 [-0.952, 10.100], loss: 0.092657, mae: 0.296933, mean_q: 3.851663
 60692/100000: episode: 849, duration: 0.267s, episode steps: 49, steps per second: 183, episode reward: 105.416, mean reward: 2.151 [1.446, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.337, 10.100], loss: 0.107263, mae: 0.307793, mean_q: 3.847336
 60716/100000: episode: 850, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 72.693, mean reward: 3.029 [2.114, 5.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.125, 10.420], loss: 0.070021, mae: 0.269524, mean_q: 3.803154
 60801/100000: episode: 851, duration: 0.478s, episode steps: 85, steps per second: 178, episode reward: 167.438, mean reward: 1.970 [1.449, 4.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.600 [-1.394, 10.120], loss: 0.124260, mae: 0.332013, mean_q: 3.896394
 60846/100000: episode: 852, duration: 0.234s, episode steps: 45, steps per second: 193, episode reward: 99.346, mean reward: 2.208 [1.718, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-1.075, 10.214], loss: 0.118670, mae: 0.326107, mean_q: 3.879059
 60895/100000: episode: 853, duration: 0.273s, episode steps: 49, steps per second: 179, episode reward: 250.811, mean reward: 5.119 [2.049, 61.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.389, 10.366], loss: 0.108147, mae: 0.311817, mean_q: 3.868427
 60980/100000: episode: 854, duration: 0.446s, episode steps: 85, steps per second: 191, episode reward: 175.134, mean reward: 2.060 [1.458, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.585 [-1.597, 10.100], loss: 1.453882, mae: 0.474260, mean_q: 3.923229
 60995/100000: episode: 855, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 53.381, mean reward: 3.559 [2.614, 4.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.393, 10.100], loss: 0.280414, mae: 0.484420, mean_q: 3.930828
 61019/100000: episode: 856, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 79.706, mean reward: 3.321 [2.226, 5.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.172, 10.590], loss: 0.131033, mae: 0.356523, mean_q: 3.914304
 61068/100000: episode: 857, duration: 0.270s, episode steps: 49, steps per second: 182, episode reward: 120.511, mean reward: 2.459 [1.661, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.357, 10.374], loss: 0.132163, mae: 0.305486, mean_q: 3.929106
 61081/100000: episode: 858, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 36.697, mean reward: 2.823 [2.222, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.470, 10.478], loss: 0.207368, mae: 0.345667, mean_q: 3.947409
 61169/100000: episode: 859, duration: 0.488s, episode steps: 88, steps per second: 180, episode reward: 160.255, mean reward: 1.821 [1.482, 2.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.568 [-1.182, 10.192], loss: 0.193836, mae: 0.341866, mean_q: 3.934882
 61184/100000: episode: 860, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 43.299, mean reward: 2.887 [2.367, 3.909], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.348, 10.100], loss: 3.701174, mae: 0.556201, mean_q: 4.078263
 61272/100000: episode: 861, duration: 0.506s, episode steps: 88, steps per second: 174, episode reward: 165.638, mean reward: 1.882 [1.472, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.575 [-0.410, 10.272], loss: 0.770284, mae: 0.413660, mean_q: 3.975933
 61283/100000: episode: 862, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 28.723, mean reward: 2.611 [2.149, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.376, 10.100], loss: 0.108417, mae: 0.326897, mean_q: 3.865432
 61363/100000: episode: 863, duration: 0.441s, episode steps: 80, steps per second: 181, episode reward: 156.260, mean reward: 1.953 [1.481, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.652 [-0.813, 10.378], loss: 0.173110, mae: 0.355469, mean_q: 3.957431
 61451/100000: episode: 864, duration: 0.475s, episode steps: 88, steps per second: 185, episode reward: 198.927, mean reward: 2.261 [1.470, 6.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.565 [-0.947, 10.100], loss: 0.200557, mae: 0.341630, mean_q: 3.969143
 61466/100000: episode: 865, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 46.022, mean reward: 3.068 [2.405, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.502, 10.100], loss: 0.254160, mae: 0.383721, mean_q: 4.018366
 61490/100000: episode: 866, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 71.888, mean reward: 2.995 [1.970, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.205, 10.443], loss: 0.288984, mae: 0.348698, mean_q: 4.050349
 61501/100000: episode: 867, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 32.543, mean reward: 2.958 [2.187, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.407, 10.100], loss: 0.107436, mae: 0.330898, mean_q: 3.998880
 61581/100000: episode: 868, duration: 0.421s, episode steps: 80, steps per second: 190, episode reward: 194.790, mean reward: 2.435 [1.923, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.650 [-0.243, 10.100], loss: 1.549108, mae: 0.486368, mean_q: 4.076035
 61594/100000: episode: 869, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 38.910, mean reward: 2.993 [2.386, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.337, 10.500], loss: 4.238145, mae: 0.708633, mean_q: 4.184205
 61609/100000: episode: 870, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 44.436, mean reward: 2.962 [1.961, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.131, 10.100], loss: 0.241680, mae: 0.432909, mean_q: 3.915437
 61620/100000: episode: 871, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 28.450, mean reward: 2.586 [2.302, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.258, 10.100], loss: 0.297839, mae: 0.408770, mean_q: 4.066641
 61700/100000: episode: 872, duration: 0.439s, episode steps: 80, steps per second: 182, episode reward: 159.378, mean reward: 1.992 [1.526, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.654 [-0.211, 10.274], loss: 0.186730, mae: 0.354819, mean_q: 4.002399
 61713/100000: episode: 873, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 30.164, mean reward: 2.320 [1.808, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.237, 10.338], loss: 0.169727, mae: 0.365511, mean_q: 4.046021
 61730/100000: episode: 874, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 49.983, mean reward: 2.940 [1.861, 4.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.366], loss: 0.108078, mae: 0.329047, mean_q: 3.946039
 61810/100000: episode: 875, duration: 0.447s, episode steps: 80, steps per second: 179, episode reward: 165.159, mean reward: 2.064 [1.503, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.645 [-0.417, 10.100], loss: 0.100502, mae: 0.314125, mean_q: 3.976629
 61827/100000: episode: 876, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 34.447, mean reward: 2.026 [1.583, 2.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.257], loss: 0.120514, mae: 0.332648, mean_q: 3.956694
 61912/100000: episode: 877, duration: 0.436s, episode steps: 85, steps per second: 195, episode reward: 162.518, mean reward: 1.912 [1.468, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.601 [-0.285, 10.239], loss: 0.781978, mae: 0.406356, mean_q: 4.032754
 62000/100000: episode: 878, duration: 0.470s, episode steps: 88, steps per second: 187, episode reward: 170.231, mean reward: 1.934 [1.447, 3.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.566 [-0.503, 10.129], loss: 2.001698, mae: 0.502500, mean_q: 4.102066
 62085/100000: episode: 879, duration: 0.489s, episode steps: 85, steps per second: 174, episode reward: 167.407, mean reward: 1.969 [1.469, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.597 [-0.936, 10.187], loss: 0.228487, mae: 0.370532, mean_q: 4.047580
 62096/100000: episode: 880, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 27.350, mean reward: 2.486 [2.182, 2.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.332, 10.100], loss: 0.124385, mae: 0.293962, mean_q: 3.985669
 62113/100000: episode: 881, duration: 0.103s, episode steps: 17, steps per second: 165, episode reward: 39.118, mean reward: 2.301 [1.721, 3.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.651, 10.273], loss: 0.217627, mae: 0.369156, mean_q: 4.061435
 62137/100000: episode: 882, duration: 0.144s, episode steps: 24, steps per second: 167, episode reward: 145.441, mean reward: 6.060 [2.623, 14.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.445], loss: 0.191313, mae: 0.355371, mean_q: 4.071360
 62217/100000: episode: 883, duration: 0.437s, episode steps: 80, steps per second: 183, episode reward: 163.619, mean reward: 2.045 [1.443, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.640 [-0.742, 10.100], loss: 0.974707, mae: 0.448757, mean_q: 4.108827
 62297/100000: episode: 884, duration: 0.429s, episode steps: 80, steps per second: 186, episode reward: 158.446, mean reward: 1.981 [1.489, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.661 [-0.552, 10.100], loss: 0.298040, mae: 0.384745, mean_q: 4.069164
 62346/100000: episode: 885, duration: 0.278s, episode steps: 49, steps per second: 176, episode reward: 118.621, mean reward: 2.421 [1.491, 5.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.461, 10.276], loss: 0.221226, mae: 0.370369, mean_q: 4.088427
 62361/100000: episode: 886, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 61.794, mean reward: 4.120 [2.400, 8.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.527, 10.100], loss: 0.233767, mae: 0.369177, mean_q: 4.125658
 62372/100000: episode: 887, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 26.016, mean reward: 2.365 [2.016, 2.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.162, 10.100], loss: 0.379303, mae: 0.454748, mean_q: 4.170695
 62460/100000: episode: 888, duration: 0.480s, episode steps: 88, steps per second: 183, episode reward: 169.346, mean reward: 1.924 [1.442, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.565 [-0.371, 10.100], loss: 0.152953, mae: 0.364418, mean_q: 4.109918
 62548/100000: episode: 889, duration: 0.465s, episode steps: 88, steps per second: 189, episode reward: 165.281, mean reward: 1.878 [1.451, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.567 [-1.651, 10.110], loss: 0.176416, mae: 0.363074, mean_q: 4.064244
 62565/100000: episode: 890, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 50.586, mean reward: 2.976 [2.236, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.438], loss: 0.120540, mae: 0.336485, mean_q: 4.038785
 62580/100000: episode: 891, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 49.784, mean reward: 3.319 [2.900, 4.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.325, 10.100], loss: 0.138743, mae: 0.367618, mean_q: 4.082386
 62591/100000: episode: 892, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 32.855, mean reward: 2.987 [2.396, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.320, 10.100], loss: 0.177014, mae: 0.363617, mean_q: 4.114610
 62608/100000: episode: 893, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 53.490, mean reward: 3.146 [2.272, 4.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.152, 10.449], loss: 0.246180, mae: 0.422630, mean_q: 4.205474
 62693/100000: episode: 894, duration: 0.462s, episode steps: 85, steps per second: 184, episode reward: 158.066, mean reward: 1.860 [1.497, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.588 [-0.776, 10.216], loss: 0.807370, mae: 0.422699, mean_q: 4.141657
 62710/100000: episode: 895, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 47.349, mean reward: 2.785 [1.920, 4.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.313, 10.380], loss: 0.383576, mae: 0.382902, mean_q: 4.130005
 62725/100000: episode: 896, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 46.320, mean reward: 3.088 [2.481, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.450, 10.100], loss: 0.280847, mae: 0.425603, mean_q: 4.141104
 62738/100000: episode: 897, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 39.793, mean reward: 3.061 [2.531, 4.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.515], loss: 4.112432, mae: 0.525652, mean_q: 4.210105
 62749/100000: episode: 898, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 28.236, mean reward: 2.567 [2.214, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.349, 10.100], loss: 0.504347, mae: 0.517273, mean_q: 4.108438
 62837/100000: episode: 899, duration: 0.480s, episode steps: 88, steps per second: 183, episode reward: 183.355, mean reward: 2.084 [1.536, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.574 [-0.562, 10.280], loss: 0.177408, mae: 0.394516, mean_q: 4.125707
 62922/100000: episode: 900, duration: 0.460s, episode steps: 85, steps per second: 185, episode reward: 193.454, mean reward: 2.276 [1.455, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 1.582 [-0.677, 10.100], loss: 0.846079, mae: 0.444131, mean_q: 4.161372
 62937/100000: episode: 901, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 39.589, mean reward: 2.639 [2.171, 3.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.250, 10.100], loss: 3.602671, mae: 0.511687, mean_q: 4.184107
 63025/100000: episode: 902, duration: 0.485s, episode steps: 88, steps per second: 182, episode reward: 169.760, mean reward: 1.929 [1.477, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.566 [-0.586, 10.100], loss: 0.808860, mae: 0.453880, mean_q: 4.170237
 63036/100000: episode: 903, duration: 0.062s, episode steps: 11, steps per second: 176, episode reward: 33.934, mean reward: 3.085 [2.154, 4.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.315, 10.100], loss: 0.133369, mae: 0.359205, mean_q: 4.196164
 63051/100000: episode: 904, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 68.498, mean reward: 4.567 [2.881, 7.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.372, 10.100], loss: 0.190949, mae: 0.345251, mean_q: 4.041524
 63068/100000: episode: 905, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 54.882, mean reward: 3.228 [2.617, 6.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.161, 10.358], loss: 0.137775, mae: 0.345954, mean_q: 4.142000
 63153/100000: episode: 906, duration: 0.456s, episode steps: 85, steps per second: 187, episode reward: 172.038, mean reward: 2.024 [1.475, 4.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.594 [-0.991, 10.229], loss: 0.260000, mae: 0.405297, mean_q: 4.152080
 63166/100000: episode: 907, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 37.079, mean reward: 2.852 [2.395, 3.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.508], loss: 0.169545, mae: 0.391761, mean_q: 4.158086
 63254/100000: episode: 908, duration: 0.461s, episode steps: 88, steps per second: 191, episode reward: 185.218, mean reward: 2.105 [1.443, 4.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.559 [-0.656, 10.106], loss: 0.233675, mae: 0.388895, mean_q: 4.186853
 63267/100000: episode: 909, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 37.532, mean reward: 2.887 [2.486, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.336], loss: 0.149150, mae: 0.369424, mean_q: 4.094543
 63278/100000: episode: 910, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 24.838, mean reward: 2.258 [2.047, 2.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.168, 10.100], loss: 5.013660, mae: 0.665176, mean_q: 4.453953
 63289/100000: episode: 911, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 32.916, mean reward: 2.992 [2.364, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.400, 10.100], loss: 0.291359, mae: 0.491188, mean_q: 4.201665
 63304/100000: episode: 912, duration: 0.091s, episode steps: 15, steps per second: 166, episode reward: 55.823, mean reward: 3.722 [3.110, 5.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.391, 10.100], loss: 0.186552, mae: 0.401943, mean_q: 4.158314
 63392/100000: episode: 913, duration: 0.457s, episode steps: 88, steps per second: 193, episode reward: 160.553, mean reward: 1.824 [1.455, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.567 [-0.503, 10.154], loss: 0.197834, mae: 0.386174, mean_q: 4.183912
 63437/100000: episode: 914, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 130.664, mean reward: 2.904 [2.349, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.539, 10.482], loss: 0.144910, mae: 0.356624, mean_q: 4.231605
 63522/100000: episode: 915, duration: 0.462s, episode steps: 85, steps per second: 184, episode reward: 160.345, mean reward: 1.886 [1.443, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.600 [-0.476, 10.222], loss: 0.280118, mae: 0.407050, mean_q: 4.195629
 63607/100000: episode: 916, duration: 0.469s, episode steps: 85, steps per second: 181, episode reward: 171.059, mean reward: 2.012 [1.477, 6.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.587 [-1.957, 10.100], loss: 0.338304, mae: 0.437716, mean_q: 4.309385
 63622/100000: episode: 917, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 52.003, mean reward: 3.467 [2.846, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.403, 10.100], loss: 0.208212, mae: 0.369033, mean_q: 4.199748
 63635/100000: episode: 918, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 38.884, mean reward: 2.991 [2.554, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.452], loss: 0.207976, mae: 0.382624, mean_q: 4.235352
 63646/100000: episode: 919, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 42.480, mean reward: 3.862 [2.735, 6.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.510, 10.100], loss: 0.309611, mae: 0.401509, mean_q: 4.233545
 63659/100000: episode: 920, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 31.971, mean reward: 2.459 [2.084, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.637, 10.422], loss: 0.155183, mae: 0.387992, mean_q: 4.249743
 63676/100000: episode: 921, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 45.274, mean reward: 2.663 [1.595, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.331], loss: 0.127247, mae: 0.341861, mean_q: 4.209671
 63721/100000: episode: 922, duration: 0.249s, episode steps: 45, steps per second: 181, episode reward: 93.756, mean reward: 2.083 [1.458, 4.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-1.381, 10.100], loss: 0.232353, mae: 0.397413, mean_q: 4.244432
 63734/100000: episode: 923, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 30.410, mean reward: 2.339 [2.129, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.402], loss: 0.171194, mae: 0.371434, mean_q: 4.276651
 63758/100000: episode: 924, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 52.886, mean reward: 2.204 [1.626, 2.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.035, 10.238], loss: 2.398843, mae: 0.577604, mean_q: 4.258909
 63771/100000: episode: 925, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 39.938, mean reward: 3.072 [2.165, 5.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.207, 10.478], loss: 0.453754, mae: 0.468775, mean_q: 4.289015
 63856/100000: episode: 926, duration: 0.463s, episode steps: 85, steps per second: 184, episode reward: 170.103, mean reward: 2.001 [1.439, 4.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.606 [-0.229, 10.257], loss: 0.199459, mae: 0.377464, mean_q: 4.214180
 63869/100000: episode: 927, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 39.363, mean reward: 3.028 [2.250, 4.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.395], loss: 4.109444, mae: 0.543085, mean_q: 4.398543
 63886/100000: episode: 928, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 42.759, mean reward: 2.515 [2.268, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.396], loss: 0.324027, mae: 0.527337, mean_q: 4.304332
 63966/100000: episode: 929, duration: 0.417s, episode steps: 80, steps per second: 192, episode reward: 155.460, mean reward: 1.943 [1.436, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.652 [-0.716, 10.157], loss: 0.152687, mae: 0.378947, mean_q: 4.270509
 64015/100000: episode: 930, duration: 0.264s, episode steps: 49, steps per second: 186, episode reward: 168.380, mean reward: 3.436 [2.554, 5.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.108, 10.460], loss: 1.416072, mae: 0.523795, mean_q: 4.335640
 64103/100000: episode: 931, duration: 0.462s, episode steps: 88, steps per second: 190, episode reward: 176.981, mean reward: 2.011 [1.517, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.567 [-0.314, 10.151], loss: 1.357767, mae: 0.480087, mean_q: 4.328044
 64191/100000: episode: 932, duration: 0.461s, episode steps: 88, steps per second: 191, episode reward: 168.733, mean reward: 1.917 [1.501, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.573 [-0.552, 10.116], loss: 0.200303, mae: 0.402819, mean_q: 4.272274
 64271/100000: episode: 933, duration: 0.423s, episode steps: 80, steps per second: 189, episode reward: 160.464, mean reward: 2.006 [1.458, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.649 [-0.720, 10.100], loss: 0.875741, mae: 0.451440, mean_q: 4.315272
 64286/100000: episode: 934, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 45.127, mean reward: 3.008 [2.492, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.252, 10.100], loss: 0.209758, mae: 0.367091, mean_q: 4.169194
 64374/100000: episode: 935, duration: 0.465s, episode steps: 88, steps per second: 189, episode reward: 174.995, mean reward: 1.989 [1.472, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 1.572 [-0.834, 10.175], loss: 0.827094, mae: 0.432756, mean_q: 4.305368
 64423/100000: episode: 936, duration: 0.264s, episode steps: 49, steps per second: 186, episode reward: 107.671, mean reward: 2.197 [1.599, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-1.462, 10.206], loss: 0.175979, mae: 0.379798, mean_q: 4.267540
[Info] 2-TH LEVEL FOUND: 9.080354690551758, Considering 10/90 traces
 64511/100000: episode: 937, duration: 4.657s, episode steps: 88, steps per second: 19, episode reward: 163.259, mean reward: 1.855 [1.449, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.569 [-0.434, 10.185], loss: 0.822934, mae: 0.468985, mean_q: 4.362093
 64557/100000: episode: 938, duration: 0.255s, episode steps: 46, steps per second: 180, episode reward: 106.152, mean reward: 2.308 [1.550, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.476, 10.224], loss: 0.205846, mae: 0.394450, mean_q: 4.294009
 64604/100000: episode: 939, duration: 0.258s, episode steps: 47, steps per second: 182, episode reward: 124.532, mean reward: 2.650 [1.490, 6.478], mean action: 0.000 [0.000, 0.000], mean observation: 1.912 [-0.588, 10.100], loss: 0.261087, mae: 0.433503, mean_q: 4.314704
 64651/100000: episode: 940, duration: 0.241s, episode steps: 47, steps per second: 195, episode reward: 112.863, mean reward: 2.401 [1.814, 4.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.926 [-0.926, 10.285], loss: 0.254124, mae: 0.434320, mean_q: 4.333075
 64658/100000: episode: 941, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 30.637, mean reward: 4.377 [3.461, 5.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.598], loss: 0.183176, mae: 0.417104, mean_q: 4.464357
 64701/100000: episode: 942, duration: 0.244s, episode steps: 43, steps per second: 176, episode reward: 143.273, mean reward: 3.332 [2.116, 5.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.529, 10.327], loss: 0.146631, mae: 0.364983, mean_q: 4.297629
 64710/100000: episode: 943, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 49.762, mean reward: 5.529 [2.977, 9.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.522, 10.100], loss: 0.477833, mae: 0.439085, mean_q: 4.285002
 64731/100000: episode: 944, duration: 0.121s, episode steps: 21, steps per second: 174, episode reward: 72.480, mean reward: 3.451 [2.625, 5.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.095, 10.447], loss: 0.263726, mae: 0.481119, mean_q: 4.341621
 64775/100000: episode: 945, duration: 0.248s, episode steps: 44, steps per second: 178, episode reward: 133.866, mean reward: 3.042 [1.928, 4.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.512, 10.323], loss: 0.181122, mae: 0.392526, mean_q: 4.342690
 64820/100000: episode: 946, duration: 0.245s, episode steps: 45, steps per second: 184, episode reward: 309.098, mean reward: 6.869 [2.487, 50.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.914, 10.607], loss: 1.404352, mae: 0.490679, mean_q: 4.487735
 64866/100000: episode: 947, duration: 0.270s, episode steps: 46, steps per second: 170, episode reward: 89.919, mean reward: 1.955 [1.484, 3.707], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.242, 10.195], loss: 0.376478, mae: 0.477392, mean_q: 4.426074
 64912/100000: episode: 948, duration: 0.244s, episode steps: 46, steps per second: 188, episode reward: 95.005, mean reward: 2.065 [1.495, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.306, 10.292], loss: 0.398629, mae: 0.512840, mean_q: 4.500069
 64921/100000: episode: 949, duration: 0.063s, episode steps: 9, steps per second: 144, episode reward: 38.719, mean reward: 4.302 [3.846, 5.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.329, 10.100], loss: 4.042313, mae: 0.723988, mean_q: 4.382830
 64942/100000: episode: 950, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 78.290, mean reward: 3.728 [2.883, 4.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.492], loss: 0.602581, mae: 0.655744, mean_q: 4.373486
 64987/100000: episode: 951, duration: 0.234s, episode steps: 45, steps per second: 192, episode reward: 105.692, mean reward: 2.349 [1.553, 5.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.664, 10.261], loss: 0.277310, mae: 0.488505, mean_q: 4.438720
 65033/100000: episode: 952, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 96.361, mean reward: 2.095 [1.546, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.924, 10.140], loss: 0.218717, mae: 0.387060, mean_q: 4.407786
 65078/100000: episode: 953, duration: 0.252s, episode steps: 45, steps per second: 178, episode reward: 126.144, mean reward: 2.803 [1.524, 7.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-1.117, 10.133], loss: 1.373418, mae: 0.458892, mean_q: 4.431316
 65087/100000: episode: 954, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 39.295, mean reward: 4.366 [2.763, 5.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.952, 10.100], loss: 0.219079, mae: 0.434604, mean_q: 4.358909
 65132/100000: episode: 955, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 110.455, mean reward: 2.455 [1.491, 3.679], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.228, 10.100], loss: 0.380235, mae: 0.492149, mean_q: 4.554700
 65176/100000: episode: 956, duration: 0.249s, episode steps: 44, steps per second: 177, episode reward: 143.864, mean reward: 3.270 [1.801, 14.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.208, 10.247], loss: 0.196809, mae: 0.427109, mean_q: 4.463058
 65185/100000: episode: 957, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 32.608, mean reward: 3.623 [3.343, 4.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.339, 10.100], loss: 3.632674, mae: 0.822776, mean_q: 4.955648
 65206/100000: episode: 958, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 91.674, mean reward: 4.365 [2.002, 11.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.378], loss: 0.469683, mae: 0.633225, mean_q: 4.284314
 65252/100000: episode: 959, duration: 0.245s, episode steps: 46, steps per second: 187, episode reward: 125.750, mean reward: 2.734 [1.699, 7.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.797, 10.544], loss: 1.455006, mae: 0.568479, mean_q: 4.507401
 65295/100000: episode: 960, duration: 0.226s, episode steps: 43, steps per second: 190, episode reward: 199.503, mean reward: 4.640 [2.983, 7.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.966 [-0.286, 10.502], loss: 0.362763, mae: 0.449685, mean_q: 4.517477
 65341/100000: episode: 961, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 124.638, mean reward: 2.710 [1.512, 5.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-0.505, 10.233], loss: 2.517763, mae: 0.594743, mean_q: 4.611782
 65387/100000: episode: 962, duration: 0.253s, episode steps: 46, steps per second: 182, episode reward: 158.746, mean reward: 3.451 [2.125, 6.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-0.255, 10.369], loss: 1.183016, mae: 0.634137, mean_q: 4.659653
 65394/100000: episode: 963, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 28.940, mean reward: 4.134 [3.400, 5.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.253, 10.531], loss: 0.172413, mae: 0.422938, mean_q: 4.672729
 65441/100000: episode: 964, duration: 0.267s, episode steps: 47, steps per second: 176, episode reward: 226.986, mean reward: 4.829 [3.093, 8.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.905, 10.554], loss: 1.439718, mae: 0.544217, mean_q: 4.673392
 65485/100000: episode: 965, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 107.365, mean reward: 2.440 [1.468, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.750, 10.216], loss: 1.442677, mae: 0.504566, mean_q: 4.724028
 65531/100000: episode: 966, duration: 0.268s, episode steps: 46, steps per second: 172, episode reward: 110.004, mean reward: 2.391 [1.511, 4.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.336, 10.100], loss: 0.245402, mae: 0.438852, mean_q: 4.637035
 65538/100000: episode: 967, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 23.333, mean reward: 3.333 [2.828, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.425], loss: 0.449387, mae: 0.547313, mean_q: 5.068301
 65547/100000: episode: 968, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 56.392, mean reward: 6.266 [4.754, 7.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.417, 10.100], loss: 0.595088, mae: 0.579700, mean_q: 4.982760
 65593/100000: episode: 969, duration: 0.268s, episode steps: 46, steps per second: 171, episode reward: 106.905, mean reward: 2.324 [1.517, 5.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.451, 10.100], loss: 0.453058, mae: 0.491981, mean_q: 4.723885
 65639/100000: episode: 970, duration: 0.257s, episode steps: 46, steps per second: 179, episode reward: 137.306, mean reward: 2.985 [1.891, 4.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.245, 10.305], loss: 1.572612, mae: 0.576971, mean_q: 4.744567
 65646/100000: episode: 971, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 31.014, mean reward: 4.431 [3.410, 5.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.531], loss: 0.273670, mae: 0.442927, mean_q: 4.613265
 65690/100000: episode: 972, duration: 0.250s, episode steps: 44, steps per second: 176, episode reward: 115.973, mean reward: 2.636 [1.546, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-1.030, 10.315], loss: 1.267025, mae: 0.733322, mean_q: 4.763393
 65737/100000: episode: 973, duration: 0.243s, episode steps: 47, steps per second: 194, episode reward: 135.048, mean reward: 2.873 [1.925, 4.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.332, 10.552], loss: 0.240428, mae: 0.460952, mean_q: 4.744064
 65781/100000: episode: 974, duration: 0.257s, episode steps: 44, steps per second: 171, episode reward: 106.020, mean reward: 2.410 [1.518, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.293, 10.135], loss: 0.372669, mae: 0.485118, mean_q: 4.715671
 65788/100000: episode: 975, duration: 0.047s, episode steps: 7, steps per second: 150, episode reward: 60.281, mean reward: 8.612 [6.282, 13.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.720], loss: 0.279898, mae: 0.511377, mean_q: 4.776439
 65809/100000: episode: 976, duration: 0.120s, episode steps: 21, steps per second: 176, episode reward: 73.544, mean reward: 3.502 [2.801, 4.487], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.522], loss: 0.216950, mae: 0.428675, mean_q: 4.743357
 65856/100000: episode: 977, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 124.555, mean reward: 2.650 [1.538, 5.959], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.481, 10.344], loss: 0.216763, mae: 0.433044, mean_q: 4.783007
 65902/100000: episode: 978, duration: 0.257s, episode steps: 46, steps per second: 179, episode reward: 113.834, mean reward: 2.475 [1.603, 5.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.928 [-0.432, 10.189], loss: 0.464696, mae: 0.553180, mean_q: 4.867283
 65946/100000: episode: 979, duration: 0.241s, episode steps: 44, steps per second: 182, episode reward: 154.858, mean reward: 3.520 [2.001, 8.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-1.097, 10.327], loss: 0.263108, mae: 0.466686, mean_q: 4.769838
 65992/100000: episode: 980, duration: 0.249s, episode steps: 46, steps per second: 185, episode reward: 94.448, mean reward: 2.053 [1.503, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.263, 10.100], loss: 0.374983, mae: 0.513449, mean_q: 4.805112
 66038/100000: episode: 981, duration: 0.233s, episode steps: 46, steps per second: 197, episode reward: 119.564, mean reward: 2.599 [1.639, 3.888], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.651, 10.230], loss: 0.338413, mae: 0.486407, mean_q: 4.800555
 66081/100000: episode: 982, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 144.446, mean reward: 3.359 [2.044, 5.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-1.198, 10.379], loss: 0.326521, mae: 0.434087, mean_q: 4.759472
 66090/100000: episode: 983, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 36.029, mean reward: 4.003 [3.486, 4.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.348, 10.100], loss: 0.811842, mae: 0.609459, mean_q: 5.202391
 66099/100000: episode: 984, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 32.788, mean reward: 3.643 [3.036, 4.634], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.453, 10.100], loss: 0.603473, mae: 0.505635, mean_q: 4.888914
 66108/100000: episode: 985, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 49.422, mean reward: 5.491 [4.358, 6.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.772, 10.100], loss: 3.390821, mae: 0.797495, mean_q: 5.154226
 66115/100000: episode: 986, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 30.501, mean reward: 4.357 [3.446, 5.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.568], loss: 0.770752, mae: 0.582472, mean_q: 4.650478
 66159/100000: episode: 987, duration: 0.229s, episode steps: 44, steps per second: 192, episode reward: 121.847, mean reward: 2.769 [1.880, 4.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.339, 10.284], loss: 0.216489, mae: 0.442971, mean_q: 4.818397
 66203/100000: episode: 988, duration: 0.233s, episode steps: 44, steps per second: 189, episode reward: 112.874, mean reward: 2.565 [1.828, 7.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.428, 10.264], loss: 0.250883, mae: 0.468723, mean_q: 4.839827
 66210/100000: episode: 989, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 41.176, mean reward: 5.882 [3.125, 11.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.520, 10.464], loss: 0.250493, mae: 0.464036, mean_q: 4.855628
 66254/100000: episode: 990, duration: 0.241s, episode steps: 44, steps per second: 183, episode reward: 104.102, mean reward: 2.366 [1.520, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.445, 10.233], loss: 0.328142, mae: 0.500502, mean_q: 4.908412
 66263/100000: episode: 991, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 36.964, mean reward: 4.107 [2.806, 5.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.280, 10.100], loss: 0.302083, mae: 0.437961, mean_q: 4.849911
 66284/100000: episode: 992, duration: 0.131s, episode steps: 21, steps per second: 160, episode reward: 157.663, mean reward: 7.508 [3.388, 21.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.751], loss: 0.328160, mae: 0.520124, mean_q: 4.795331
 66328/100000: episode: 993, duration: 0.260s, episode steps: 44, steps per second: 169, episode reward: 112.784, mean reward: 2.563 [1.666, 5.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.190, 10.351], loss: 1.205234, mae: 0.671278, mean_q: 5.014248
 66375/100000: episode: 994, duration: 0.265s, episode steps: 47, steps per second: 177, episode reward: 135.702, mean reward: 2.887 [1.721, 6.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.452, 10.287], loss: 0.292020, mae: 0.461964, mean_q: 4.897965
 66384/100000: episode: 995, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 44.396, mean reward: 4.933 [3.732, 6.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.477, 10.100], loss: 0.395680, mae: 0.527270, mean_q: 4.981264
 66393/100000: episode: 996, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 34.326, mean reward: 3.814 [2.688, 7.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.325, 10.100], loss: 0.456023, mae: 0.543150, mean_q: 5.059949
 66402/100000: episode: 997, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 40.563, mean reward: 4.507 [2.997, 8.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.461, 10.100], loss: 0.534812, mae: 0.555795, mean_q: 4.979244
 66409/100000: episode: 998, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 26.427, mean reward: 3.775 [3.311, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.504], loss: 4.801026, mae: 0.731769, mean_q: 5.079691
 66453/100000: episode: 999, duration: 0.239s, episode steps: 44, steps per second: 184, episode reward: 156.646, mean reward: 3.560 [2.360, 9.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.961, 10.478], loss: 0.408896, mae: 0.593915, mean_q: 4.869438
 66498/100000: episode: 1000, duration: 0.263s, episode steps: 45, steps per second: 171, episode reward: 112.616, mean reward: 2.503 [1.834, 3.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-1.004, 10.344], loss: 1.088791, mae: 0.596541, mean_q: 5.050055
 66519/100000: episode: 1001, duration: 0.118s, episode steps: 21, steps per second: 179, episode reward: 63.092, mean reward: 3.004 [2.473, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.443], loss: 0.432547, mae: 0.582043, mean_q: 4.873106
 66528/100000: episode: 1002, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 38.024, mean reward: 4.225 [3.405, 5.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.576, 10.100], loss: 0.262554, mae: 0.500937, mean_q: 4.708059
 66574/100000: episode: 1003, duration: 0.257s, episode steps: 46, steps per second: 179, episode reward: 103.145, mean reward: 2.242 [1.464, 4.042], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.257, 10.100], loss: 0.344976, mae: 0.510923, mean_q: 4.955723
 66621/100000: episode: 1004, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 150.644, mean reward: 3.205 [1.865, 4.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.346, 10.264], loss: 2.159273, mae: 0.671339, mean_q: 5.101464
 66668/100000: episode: 1005, duration: 0.266s, episode steps: 47, steps per second: 177, episode reward: 169.410, mean reward: 3.604 [2.528, 5.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.931 [-0.382, 10.471], loss: 1.087063, mae: 0.627677, mean_q: 5.185162
 66714/100000: episode: 1006, duration: 0.271s, episode steps: 46, steps per second: 170, episode reward: 101.620, mean reward: 2.209 [1.563, 4.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.253, 10.150], loss: 0.324263, mae: 0.535431, mean_q: 4.977431
 66760/100000: episode: 1007, duration: 0.239s, episode steps: 46, steps per second: 192, episode reward: 116.950, mean reward: 2.542 [1.788, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.591, 10.242], loss: 0.503060, mae: 0.547069, mean_q: 5.085571
 66806/100000: episode: 1008, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 102.992, mean reward: 2.239 [1.443, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.229, 10.176], loss: 0.967602, mae: 0.572893, mean_q: 5.112395
 66815/100000: episode: 1009, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 47.079, mean reward: 5.231 [3.555, 7.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.280, 10.100], loss: 3.371540, mae: 0.683721, mean_q: 4.813192
 66859/100000: episode: 1010, duration: 0.252s, episode steps: 44, steps per second: 175, episode reward: 108.223, mean reward: 2.460 [1.898, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.819, 10.386], loss: 0.507794, mae: 0.654834, mean_q: 5.114218
 66868/100000: episode: 1011, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 31.082, mean reward: 3.454 [2.921, 4.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.391, 10.100], loss: 0.256808, mae: 0.509409, mean_q: 4.885757
 66877/100000: episode: 1012, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 52.892, mean reward: 5.877 [4.668, 7.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.351, 10.100], loss: 0.490957, mae: 0.543234, mean_q: 5.110055
 66884/100000: episode: 1013, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 46.887, mean reward: 6.698 [4.201, 9.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-1.384, 10.616], loss: 0.173183, mae: 0.433002, mean_q: 4.842124
 66891/100000: episode: 1014, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 36.996, mean reward: 5.285 [4.187, 7.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.414, 10.666], loss: 0.359383, mae: 0.540761, mean_q: 5.025879
 66900/100000: episode: 1015, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 36.136, mean reward: 4.015 [3.652, 4.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-1.369, 10.100], loss: 0.222775, mae: 0.472255, mean_q: 4.955648
 66946/100000: episode: 1016, duration: 0.258s, episode steps: 46, steps per second: 178, episode reward: 110.080, mean reward: 2.393 [1.700, 4.351], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.265, 10.223], loss: 0.483692, mae: 0.539805, mean_q: 5.154506
 66955/100000: episode: 1017, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 37.759, mean reward: 4.195 [3.089, 5.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.385, 10.100], loss: 0.386344, mae: 0.572708, mean_q: 5.289236
 66962/100000: episode: 1018, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 52.420, mean reward: 7.489 [6.771, 8.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.485, 10.570], loss: 4.333989, mae: 0.809604, mean_q: 5.442297
 67005/100000: episode: 1019, duration: 0.241s, episode steps: 43, steps per second: 179, episode reward: 156.026, mean reward: 3.629 [2.111, 10.011], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.240, 10.386], loss: 0.429703, mae: 0.574927, mean_q: 5.062312
 67026/100000: episode: 1020, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 76.745, mean reward: 3.655 [2.604, 5.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.431], loss: 0.924941, mae: 0.777604, mean_q: 5.106295
 67070/100000: episode: 1021, duration: 0.231s, episode steps: 44, steps per second: 190, episode reward: 91.266, mean reward: 2.074 [1.472, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.488, 10.186], loss: 0.397940, mae: 0.556092, mean_q: 5.049735
 67115/100000: episode: 1022, duration: 0.251s, episode steps: 45, steps per second: 179, episode reward: 96.376, mean reward: 2.142 [1.654, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.244, 10.279], loss: 0.493685, mae: 0.564085, mean_q: 5.094130
 67124/100000: episode: 1023, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 53.263, mean reward: 5.918 [3.839, 10.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.332, 10.100], loss: 0.644719, mae: 0.690279, mean_q: 5.403668
 67133/100000: episode: 1024, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 33.535, mean reward: 3.726 [2.759, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.242, 10.100], loss: 0.212576, mae: 0.448296, mean_q: 4.874719
 67142/100000: episode: 1025, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 47.878, mean reward: 5.320 [4.168, 6.908], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.368, 10.100], loss: 0.338031, mae: 0.548928, mean_q: 5.202754
 67151/100000: episode: 1026, duration: 0.049s, episode steps: 9, steps per second: 182, episode reward: 32.922, mean reward: 3.658 [3.363, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.454, 10.100], loss: 0.370080, mae: 0.496614, mean_q: 4.966336
[Info] 3-TH LEVEL FOUND: 11.341561317443848, Considering 10/90 traces
 67195/100000: episode: 1027, duration: 4.424s, episode steps: 44, steps per second: 10, episode reward: 157.611, mean reward: 3.582 [1.896, 6.590], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.208, 10.270], loss: 0.455239, mae: 0.519239, mean_q: 5.183462
 67213/100000: episode: 1028, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 64.849, mean reward: 3.603 [2.743, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.507], loss: 0.391004, mae: 0.538661, mean_q: 5.197548
 67221/100000: episode: 1029, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 36.116, mean reward: 4.514 [3.534, 6.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.546], loss: 3.958552, mae: 0.924652, mean_q: 5.433599
 67226/100000: episode: 1030, duration: 0.027s, episode steps: 5, steps per second: 182, episode reward: 22.926, mean reward: 4.585 [3.537, 5.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.567], loss: 0.420347, mae: 0.648652, mean_q: 4.802001
 67245/100000: episode: 1031, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 124.902, mean reward: 6.574 [3.658, 12.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.537], loss: 0.841339, mae: 0.737130, mean_q: 5.154480
 67251/100000: episode: 1032, duration: 0.037s, episode steps: 6, steps per second: 162, episode reward: 30.531, mean reward: 5.088 [3.267, 10.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.035, 10.648], loss: 0.318420, mae: 0.574557, mean_q: 5.324173
 67268/100000: episode: 1033, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 56.064, mean reward: 3.298 [2.019, 5.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.361], loss: 0.348819, mae: 0.540857, mean_q: 5.269515
 67287/100000: episode: 1034, duration: 0.101s, episode steps: 19, steps per second: 187, episode reward: 87.761, mean reward: 4.619 [2.847, 8.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.424, 10.643], loss: 0.529452, mae: 0.630470, mean_q: 5.252407
 67292/100000: episode: 1035, duration: 0.039s, episode steps: 5, steps per second: 129, episode reward: 30.210, mean reward: 6.042 [5.005, 7.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.334 [-0.035, 10.630], loss: 0.442085, mae: 0.558395, mean_q: 5.089551
 67309/100000: episode: 1036, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 84.025, mean reward: 4.943 [2.627, 7.605], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.505, 10.492], loss: 0.446402, mae: 0.587014, mean_q: 5.382414
 67325/100000: episode: 1037, duration: 0.106s, episode steps: 16, steps per second: 151, episode reward: 97.006, mean reward: 6.063 [3.906, 10.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.105, 10.597], loss: 0.270662, mae: 0.503592, mean_q: 5.111486
 67344/100000: episode: 1038, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 87.557, mean reward: 4.608 [3.679, 6.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.636, 10.616], loss: 0.594465, mae: 0.615255, mean_q: 5.398481
 67350/100000: episode: 1039, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 72.112, mean reward: 12.019 [6.390, 28.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.762], loss: 0.664594, mae: 0.638355, mean_q: 5.370730
 67366/100000: episode: 1040, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 75.187, mean reward: 4.699 [2.697, 8.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.388, 10.531], loss: 0.721605, mae: 0.626627, mean_q: 5.378083
 67394/100000: episode: 1041, duration: 0.158s, episode steps: 28, steps per second: 178, episode reward: 133.191, mean reward: 4.757 [2.564, 12.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.035, 10.464], loss: 0.425966, mae: 0.567760, mean_q: 5.360205
 67420/100000: episode: 1042, duration: 0.147s, episode steps: 26, steps per second: 176, episode reward: 79.907, mean reward: 3.073 [1.994, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.393, 10.446], loss: 0.479588, mae: 0.562200, mean_q: 5.444049
 67439/100000: episode: 1043, duration: 0.114s, episode steps: 19, steps per second: 167, episode reward: 96.657, mean reward: 5.087 [3.507, 7.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.581], loss: 2.138217, mae: 0.716572, mean_q: 5.542914
 67465/100000: episode: 1044, duration: 0.140s, episode steps: 26, steps per second: 185, episode reward: 174.484, mean reward: 6.711 [4.028, 12.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.115, 10.589], loss: 0.490951, mae: 0.592767, mean_q: 5.369768
 67481/100000: episode: 1045, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 101.097, mean reward: 6.319 [3.262, 14.965], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.059, 10.567], loss: 2.274827, mae: 0.717246, mean_q: 5.692583
 67503/100000: episode: 1046, duration: 0.121s, episode steps: 22, steps per second: 181, episode reward: 83.524, mean reward: 3.797 [2.062, 5.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.191, 10.372], loss: 0.820756, mae: 0.868605, mean_q: 5.663713
 67511/100000: episode: 1047, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 31.362, mean reward: 3.920 [3.172, 5.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.559], loss: 0.823080, mae: 0.729643, mean_q: 5.026961
 67519/100000: episode: 1048, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 116.507, mean reward: 14.563 [3.633, 79.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.618, 10.597], loss: 0.737565, mae: 0.791886, mean_q: 5.770610
 67547/100000: episode: 1049, duration: 0.142s, episode steps: 28, steps per second: 197, episode reward: 82.104, mean reward: 2.932 [2.140, 4.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.597, 10.334], loss: 3.683048, mae: 0.791157, mean_q: 5.573348
 67563/100000: episode: 1050, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 142.271, mean reward: 8.892 [4.668, 25.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.516], loss: 0.610707, mae: 0.631579, mean_q: 5.394044
 67581/100000: episode: 1051, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 60.873, mean reward: 3.382 [1.862, 7.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.402, 10.328], loss: 0.734195, mae: 0.703866, mean_q: 5.652752
 67586/100000: episode: 1052, duration: 0.044s, episode steps: 5, steps per second: 114, episode reward: 27.400, mean reward: 5.480 [5.002, 5.729], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.565], loss: 0.317838, mae: 0.548828, mean_q: 5.324877
 67603/100000: episode: 1053, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 83.073, mean reward: 4.887 [3.987, 6.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.479], loss: 0.743105, mae: 0.685951, mean_q: 5.614035
 67625/100000: episode: 1054, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 96.321, mean reward: 4.378 [2.354, 10.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.311, 10.445], loss: 0.507962, mae: 0.605558, mean_q: 5.614864
 67651/100000: episode: 1055, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 94.733, mean reward: 3.644 [2.168, 7.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.517, 10.521], loss: 0.920251, mae: 0.697663, mean_q: 5.476509
 67679/100000: episode: 1056, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 145.336, mean reward: 5.191 [2.814, 11.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.762, 10.399], loss: 3.904304, mae: 0.858565, mean_q: 5.804399
 67707/100000: episode: 1057, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 105.953, mean reward: 3.784 [1.994, 8.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.343, 10.401], loss: 2.045793, mae: 0.831819, mean_q: 5.798150
 67725/100000: episode: 1058, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 71.719, mean reward: 3.984 [3.259, 5.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.455, 10.596], loss: 0.628697, mae: 0.684717, mean_q: 5.742473
 67744/100000: episode: 1059, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 66.472, mean reward: 3.499 [2.399, 4.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.087, 10.437], loss: 0.575379, mae: 0.641266, mean_q: 5.757748
 67766/100000: episode: 1060, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 101.834, mean reward: 4.629 [3.030, 17.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-1.037, 10.516], loss: 0.876678, mae: 0.670317, mean_q: 5.742671
 67771/100000: episode: 1061, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 52.043, mean reward: 10.409 [5.525, 21.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.328 [-0.035, 10.720], loss: 6.220844, mae: 1.440993, mean_q: 6.923470
 67797/100000: episode: 1062, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 102.955, mean reward: 3.960 [2.862, 5.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.334, 10.473], loss: 1.083404, mae: 0.866848, mean_q: 5.700217
 67823/100000: episode: 1063, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 88.647, mean reward: 3.409 [2.139, 4.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.401], loss: 0.615492, mae: 0.675032, mean_q: 5.682301
 67841/100000: episode: 1064, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 63.401, mean reward: 3.522 [2.639, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.447, 10.522], loss: 0.836192, mae: 0.690176, mean_q: 5.891641
 67867/100000: episode: 1065, duration: 0.151s, episode steps: 26, steps per second: 172, episode reward: 158.095, mean reward: 6.081 [3.311, 10.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.620, 10.681], loss: 0.723693, mae: 0.710133, mean_q: 5.917285
 67889/100000: episode: 1066, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 86.274, mean reward: 3.922 [2.934, 5.329], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.993, 10.569], loss: 1.159169, mae: 0.779433, mean_q: 5.943203
 67895/100000: episode: 1067, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 37.037, mean reward: 6.173 [4.787, 8.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.676], loss: 0.964321, mae: 0.730006, mean_q: 6.400520
 67911/100000: episode: 1068, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 55.823, mean reward: 3.489 [2.805, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.462], loss: 6.353735, mae: 0.939584, mean_q: 5.927010
 67933/100000: episode: 1069, duration: 0.137s, episode steps: 22, steps per second: 160, episode reward: 118.644, mean reward: 5.393 [3.514, 9.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.265, 10.566], loss: 2.317290, mae: 0.833865, mean_q: 5.961333
 67959/100000: episode: 1070, duration: 0.152s, episode steps: 26, steps per second: 172, episode reward: 76.053, mean reward: 2.925 [2.075, 4.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.406], loss: 0.771272, mae: 0.703229, mean_q: 5.773882
 67978/100000: episode: 1071, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 88.110, mean reward: 4.637 [2.872, 7.383], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.059, 10.547], loss: 2.003494, mae: 0.720589, mean_q: 5.823938
 67994/100000: episode: 1072, duration: 0.103s, episode steps: 16, steps per second: 156, episode reward: 71.191, mean reward: 4.449 [3.468, 6.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.753, 10.526], loss: 0.852127, mae: 0.754345, mean_q: 6.095986
 68000/100000: episode: 1073, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 32.430, mean reward: 5.405 [3.955, 7.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.334, 10.551], loss: 0.510819, mae: 0.605855, mean_q: 5.631849
 68008/100000: episode: 1074, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 91.336, mean reward: 11.417 [3.527, 54.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.380, 10.483], loss: 0.575503, mae: 0.616839, mean_q: 5.896109
 68025/100000: episode: 1075, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 89.822, mean reward: 5.284 [3.632, 13.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.106, 10.613], loss: 2.593680, mae: 0.865789, mean_q: 6.010113
[Info] FALSIFICATION!
[Info] Levels: [5.645441, 9.080355, 11.341561, 19.928505]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.03]
[Info] Error Prob: 3.0000000000000008e-05

 68046/100000: episode: 1076, duration: 4.452s, episode steps: 21, steps per second: 5, episode reward: 372.863, mean reward: 17.755 [3.238, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.528, 10.697], loss: 0.809955, mae: 0.770752, mean_q: 6.035794
 68146/100000: episode: 1077, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 217.584, mean reward: 2.176 [1.467, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.356, 10.347], loss: 2.914859, mae: 0.871835, mean_q: 6.096362
 68246/100000: episode: 1078, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 190.614, mean reward: 1.906 [1.450, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.501, 10.415], loss: 5.175735, mae: 1.018577, mean_q: 6.156358
 68346/100000: episode: 1079, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 194.925, mean reward: 1.949 [1.466, 3.520], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.353, 10.098], loss: 4.198342, mae: 0.973500, mean_q: 6.138516
 68446/100000: episode: 1080, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 202.758, mean reward: 2.028 [1.454, 6.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.053, 10.098], loss: 4.292278, mae: 0.951341, mean_q: 6.070993
 68546/100000: episode: 1081, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 190.408, mean reward: 1.904 [1.467, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.660, 10.098], loss: 5.522666, mae: 1.071061, mean_q: 6.223325
 68646/100000: episode: 1082, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 186.193, mean reward: 1.862 [1.437, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.714, 10.098], loss: 5.342983, mae: 1.016026, mean_q: 6.187424
 68746/100000: episode: 1083, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 196.148, mean reward: 1.961 [1.486, 2.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.637, 10.098], loss: 1.153032, mae: 0.793059, mean_q: 6.048726
 68846/100000: episode: 1084, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 182.829, mean reward: 1.828 [1.454, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.921, 10.158], loss: 2.222902, mae: 0.797542, mean_q: 6.020262
 68946/100000: episode: 1085, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 188.828, mean reward: 1.888 [1.468, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.722, 10.098], loss: 3.568066, mae: 0.860340, mean_q: 6.050951
 69046/100000: episode: 1086, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 181.928, mean reward: 1.819 [1.433, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.234, 10.098], loss: 4.473533, mae: 1.011881, mean_q: 5.931058
 69146/100000: episode: 1087, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 198.224, mean reward: 1.982 [1.477, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.621, 10.098], loss: 3.758847, mae: 0.974610, mean_q: 6.040647
 69246/100000: episode: 1088, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 186.953, mean reward: 1.870 [1.442, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.255, 10.278], loss: 2.954503, mae: 0.853495, mean_q: 5.923822
 69346/100000: episode: 1089, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 177.329, mean reward: 1.773 [1.449, 2.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.871, 10.098], loss: 3.967134, mae: 0.879600, mean_q: 6.021399
 69446/100000: episode: 1090, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 189.555, mean reward: 1.896 [1.453, 3.729], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.601, 10.098], loss: 4.734238, mae: 0.991893, mean_q: 6.093235
 69546/100000: episode: 1091, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 201.009, mean reward: 2.010 [1.482, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.517, 10.291], loss: 2.650803, mae: 0.819405, mean_q: 5.891398
 69646/100000: episode: 1092, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 185.127, mean reward: 1.851 [1.445, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.887, 10.215], loss: 6.689029, mae: 1.092128, mean_q: 6.079997
 69746/100000: episode: 1093, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 183.199, mean reward: 1.832 [1.463, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.742, 10.098], loss: 3.619452, mae: 0.892892, mean_q: 5.852976
 69846/100000: episode: 1094, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 192.580, mean reward: 1.926 [1.509, 3.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.083, 10.098], loss: 2.273237, mae: 0.842068, mean_q: 5.754650
 69946/100000: episode: 1095, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 189.403, mean reward: 1.894 [1.464, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.192, 10.274], loss: 0.816213, mae: 0.673937, mean_q: 5.723630
 70046/100000: episode: 1096, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 198.901, mean reward: 1.989 [1.449, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.201, 10.098], loss: 1.737682, mae: 0.742535, mean_q: 5.615921
 70146/100000: episode: 1097, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 183.884, mean reward: 1.839 [1.464, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.692, 10.176], loss: 1.979950, mae: 0.696248, mean_q: 5.641439
 70246/100000: episode: 1098, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 184.521, mean reward: 1.845 [1.481, 2.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.807, 10.130], loss: 2.674819, mae: 0.730535, mean_q: 5.638395
 70346/100000: episode: 1099, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 181.900, mean reward: 1.819 [1.450, 2.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.631, 10.290], loss: 2.156079, mae: 0.749464, mean_q: 5.479426
 70446/100000: episode: 1100, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 191.386, mean reward: 1.914 [1.487, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.539, 10.098], loss: 0.952322, mae: 0.667625, mean_q: 5.460218
 70546/100000: episode: 1101, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 202.576, mean reward: 2.026 [1.516, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.458, 10.125], loss: 2.341872, mae: 0.726115, mean_q: 5.369321
 70646/100000: episode: 1102, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 194.466, mean reward: 1.945 [1.453, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.664, 10.366], loss: 1.673838, mae: 0.667460, mean_q: 5.369520
 70746/100000: episode: 1103, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 181.291, mean reward: 1.813 [1.458, 3.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.717, 10.258], loss: 2.059905, mae: 0.664218, mean_q: 5.340675
 70846/100000: episode: 1104, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 214.756, mean reward: 2.148 [1.466, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.411 [-1.767, 10.098], loss: 1.500288, mae: 0.629735, mean_q: 5.340077
 70946/100000: episode: 1105, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.561, mean reward: 1.926 [1.536, 2.783], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.724, 10.098], loss: 3.135725, mae: 0.741281, mean_q: 5.291957
 71046/100000: episode: 1106, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 192.496, mean reward: 1.925 [1.498, 4.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.363, 10.098], loss: 1.577126, mae: 0.631285, mean_q: 5.242903
 71146/100000: episode: 1107, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 170.290, mean reward: 1.703 [1.451, 2.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.054, 10.098], loss: 0.501508, mae: 0.544294, mean_q: 5.115225
 71246/100000: episode: 1108, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 181.851, mean reward: 1.819 [1.485, 2.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.968, 10.211], loss: 4.325502, mae: 0.928204, mean_q: 5.195027
 71346/100000: episode: 1109, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 196.056, mean reward: 1.961 [1.477, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.957, 10.098], loss: 3.883502, mae: 0.821422, mean_q: 5.173144
 71446/100000: episode: 1110, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 177.173, mean reward: 1.772 [1.446, 2.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.191, 10.098], loss: 4.952748, mae: 0.809717, mean_q: 5.183821
 71546/100000: episode: 1111, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 201.273, mean reward: 2.013 [1.515, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.819, 10.249], loss: 0.603882, mae: 0.568437, mean_q: 4.943894
 71646/100000: episode: 1112, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 178.832, mean reward: 1.788 [1.451, 2.788], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.669, 10.107], loss: 1.518563, mae: 0.517569, mean_q: 4.879909
 71746/100000: episode: 1113, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.936, mean reward: 1.929 [1.435, 4.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.790, 10.098], loss: 4.692329, mae: 0.949714, mean_q: 4.978940
 71846/100000: episode: 1114, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 184.324, mean reward: 1.843 [1.450, 4.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.057, 10.098], loss: 3.611012, mae: 0.906563, mean_q: 4.921619
 71946/100000: episode: 1115, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 220.310, mean reward: 2.203 [1.533, 4.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.712, 10.098], loss: 0.508440, mae: 0.517470, mean_q: 4.722738
 72046/100000: episode: 1116, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 187.158, mean reward: 1.872 [1.436, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.150, 10.101], loss: 1.666390, mae: 0.651989, mean_q: 4.742883
 72146/100000: episode: 1117, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 217.742, mean reward: 2.177 [1.481, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.497, 10.098], loss: 1.606747, mae: 0.602418, mean_q: 4.676730
 72246/100000: episode: 1118, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 196.605, mean reward: 1.966 [1.436, 4.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.894, 10.205], loss: 1.647404, mae: 0.514693, mean_q: 4.610462
 72346/100000: episode: 1119, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 180.957, mean reward: 1.810 [1.484, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.740, 10.152], loss: 2.322536, mae: 0.583698, mean_q: 4.488549
 72446/100000: episode: 1120, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 223.672, mean reward: 2.237 [1.459, 3.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.750, 10.418], loss: 6.549855, mae: 0.808038, mean_q: 4.617660
 72546/100000: episode: 1121, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 176.774, mean reward: 1.768 [1.449, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.774, 10.174], loss: 2.373869, mae: 0.526809, mean_q: 4.412068
 72646/100000: episode: 1122, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 185.716, mean reward: 1.857 [1.514, 2.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.342, 10.226], loss: 1.567030, mae: 0.480303, mean_q: 4.325390
 72746/100000: episode: 1123, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 174.541, mean reward: 1.745 [1.435, 2.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.435, 10.098], loss: 1.625492, mae: 0.483079, mean_q: 4.137820
 72846/100000: episode: 1124, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 194.643, mean reward: 1.946 [1.435, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.023, 10.261], loss: 2.624099, mae: 0.465417, mean_q: 4.095178
 72946/100000: episode: 1125, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 183.843, mean reward: 1.838 [1.462, 2.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.622, 10.133], loss: 0.182262, mae: 0.303883, mean_q: 3.890041
 73046/100000: episode: 1126, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 184.492, mean reward: 1.845 [1.455, 2.702], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.706, 10.098], loss: 2.483946, mae: 0.453567, mean_q: 3.870493
 73146/100000: episode: 1127, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.146, mean reward: 1.811 [1.473, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.222, 10.203], loss: 0.071444, mae: 0.275425, mean_q: 3.776615
 73246/100000: episode: 1128, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 190.712, mean reward: 1.907 [1.454, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.696, 10.098], loss: 0.073216, mae: 0.272700, mean_q: 3.777337
 73346/100000: episode: 1129, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 199.641, mean reward: 1.996 [1.459, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.304, 10.098], loss: 0.072922, mae: 0.275272, mean_q: 3.797523
 73446/100000: episode: 1130, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 178.921, mean reward: 1.789 [1.460, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.723, 10.098], loss: 0.073370, mae: 0.273684, mean_q: 3.765529
 73546/100000: episode: 1131, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 192.580, mean reward: 1.926 [1.489, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.143, 10.309], loss: 0.070590, mae: 0.270214, mean_q: 3.773553
 73646/100000: episode: 1132, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 190.468, mean reward: 1.905 [1.460, 5.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.188, 10.207], loss: 0.066853, mae: 0.266891, mean_q: 3.780512
 73746/100000: episode: 1133, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 190.942, mean reward: 1.909 [1.465, 3.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.595, 10.188], loss: 0.071906, mae: 0.277439, mean_q: 3.756320
 73846/100000: episode: 1134, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 188.125, mean reward: 1.881 [1.486, 3.018], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.627, 10.148], loss: 0.067373, mae: 0.264123, mean_q: 3.761965
 73946/100000: episode: 1135, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 183.478, mean reward: 1.835 [1.474, 2.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.279, 10.246], loss: 0.067779, mae: 0.269577, mean_q: 3.774000
 74046/100000: episode: 1136, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 208.189, mean reward: 2.082 [1.505, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.400, 10.098], loss: 0.062401, mae: 0.258724, mean_q: 3.764544
 74146/100000: episode: 1137, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 186.914, mean reward: 1.869 [1.457, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.706, 10.159], loss: 0.066098, mae: 0.264601, mean_q: 3.779961
 74246/100000: episode: 1138, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 180.508, mean reward: 1.805 [1.452, 2.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.967, 10.138], loss: 0.065909, mae: 0.262551, mean_q: 3.765528
 74346/100000: episode: 1139, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 189.926, mean reward: 1.899 [1.468, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.591, 10.221], loss: 0.065317, mae: 0.268295, mean_q: 3.770691
 74446/100000: episode: 1140, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 186.067, mean reward: 1.861 [1.452, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.151, 10.138], loss: 0.059696, mae: 0.258140, mean_q: 3.767385
 74546/100000: episode: 1141, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 191.969, mean reward: 1.920 [1.474, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.531, 10.304], loss: 0.068771, mae: 0.266120, mean_q: 3.772014
 74646/100000: episode: 1142, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 186.393, mean reward: 1.864 [1.455, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.569, 10.221], loss: 0.064662, mae: 0.262950, mean_q: 3.755109
 74746/100000: episode: 1143, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 245.704, mean reward: 2.457 [1.473, 5.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.917, 10.328], loss: 0.068359, mae: 0.273598, mean_q: 3.768667
 74846/100000: episode: 1144, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.375, mean reward: 1.864 [1.457, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.298, 10.098], loss: 0.064461, mae: 0.263865, mean_q: 3.771050
 74946/100000: episode: 1145, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 187.875, mean reward: 1.879 [1.498, 4.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.519, 10.114], loss: 0.067056, mae: 0.263263, mean_q: 3.761397
 75046/100000: episode: 1146, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 185.887, mean reward: 1.859 [1.454, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.298, 10.098], loss: 0.064881, mae: 0.268822, mean_q: 3.781654
 75146/100000: episode: 1147, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 211.568, mean reward: 2.116 [1.456, 4.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.240, 10.464], loss: 0.075706, mae: 0.278046, mean_q: 3.785887
 75246/100000: episode: 1148, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 207.251, mean reward: 2.073 [1.471, 3.498], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.790, 10.098], loss: 0.069331, mae: 0.264553, mean_q: 3.791324
 75346/100000: episode: 1149, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 186.569, mean reward: 1.866 [1.440, 2.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.400, 10.110], loss: 0.074895, mae: 0.275117, mean_q: 3.810029
 75446/100000: episode: 1150, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 200.501, mean reward: 2.005 [1.448, 4.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.613, 10.098], loss: 0.080944, mae: 0.282269, mean_q: 3.816723
 75546/100000: episode: 1151, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 172.896, mean reward: 1.729 [1.442, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.420, 10.098], loss: 0.082958, mae: 0.287507, mean_q: 3.812404
 75646/100000: episode: 1152, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 245.514, mean reward: 2.455 [1.461, 4.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.971, 10.493], loss: 0.078955, mae: 0.282975, mean_q: 3.809926
 75746/100000: episode: 1153, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 192.226, mean reward: 1.922 [1.445, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.200, 10.170], loss: 0.079508, mae: 0.280522, mean_q: 3.824557
 75846/100000: episode: 1154, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 177.710, mean reward: 1.777 [1.444, 2.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.814, 10.314], loss: 0.072728, mae: 0.277320, mean_q: 3.800011
 75946/100000: episode: 1155, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 198.431, mean reward: 1.984 [1.447, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.489, 10.106], loss: 0.072100, mae: 0.278431, mean_q: 3.806873
 76046/100000: episode: 1156, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 201.516, mean reward: 2.015 [1.445, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.684, 10.098], loss: 0.071297, mae: 0.271190, mean_q: 3.799381
 76146/100000: episode: 1157, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 204.534, mean reward: 2.045 [1.459, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.830, 10.286], loss: 0.075142, mae: 0.272797, mean_q: 3.795574
 76246/100000: episode: 1158, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 196.391, mean reward: 1.964 [1.460, 5.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.063, 10.098], loss: 0.074173, mae: 0.278931, mean_q: 3.827450
 76346/100000: episode: 1159, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 191.333, mean reward: 1.913 [1.467, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.506, 10.098], loss: 0.071619, mae: 0.273083, mean_q: 3.805835
 76446/100000: episode: 1160, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 207.024, mean reward: 2.070 [1.507, 3.510], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.004, 10.402], loss: 0.074741, mae: 0.286017, mean_q: 3.825342
 76546/100000: episode: 1161, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 208.698, mean reward: 2.087 [1.441, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.976, 10.204], loss: 0.076626, mae: 0.287576, mean_q: 3.821355
 76646/100000: episode: 1162, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 209.052, mean reward: 2.091 [1.462, 4.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.198, 10.098], loss: 0.076819, mae: 0.286767, mean_q: 3.834458
 76746/100000: episode: 1163, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 188.978, mean reward: 1.890 [1.470, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.852, 10.098], loss: 0.087015, mae: 0.295523, mean_q: 3.853941
 76846/100000: episode: 1164, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.775, mean reward: 1.918 [1.445, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.300, 10.098], loss: 0.083292, mae: 0.294108, mean_q: 3.837625
 76946/100000: episode: 1165, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 195.083, mean reward: 1.951 [1.451, 6.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.067, 10.098], loss: 0.085526, mae: 0.296290, mean_q: 3.864421
 77046/100000: episode: 1166, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 203.478, mean reward: 2.035 [1.469, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.548, 10.098], loss: 0.080773, mae: 0.287180, mean_q: 3.842956
 77146/100000: episode: 1167, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.071, mean reward: 1.861 [1.505, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.612, 10.257], loss: 0.082685, mae: 0.291574, mean_q: 3.845213
 77246/100000: episode: 1168, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 202.357, mean reward: 2.024 [1.463, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.988, 10.283], loss: 0.083298, mae: 0.288123, mean_q: 3.851621
 77346/100000: episode: 1169, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 191.134, mean reward: 1.911 [1.480, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.889, 10.356], loss: 0.081620, mae: 0.283872, mean_q: 3.839858
 77446/100000: episode: 1170, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 223.276, mean reward: 2.233 [1.475, 5.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.513, 10.098], loss: 0.093043, mae: 0.298148, mean_q: 3.854946
 77546/100000: episode: 1171, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 189.551, mean reward: 1.896 [1.461, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.402, 10.188], loss: 0.080609, mae: 0.283520, mean_q: 3.854565
 77646/100000: episode: 1172, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.296, mean reward: 1.823 [1.453, 3.371], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.588, 10.142], loss: 0.073571, mae: 0.280944, mean_q: 3.823270
 77746/100000: episode: 1173, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 192.816, mean reward: 1.928 [1.472, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.849, 10.098], loss: 0.081996, mae: 0.293066, mean_q: 3.856649
 77846/100000: episode: 1174, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 208.389, mean reward: 2.084 [1.477, 4.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.138, 10.098], loss: 0.089876, mae: 0.295881, mean_q: 3.848232
 77946/100000: episode: 1175, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 199.970, mean reward: 2.000 [1.470, 7.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.635, 10.098], loss: 0.088514, mae: 0.303018, mean_q: 3.880498
[Info] 1-TH LEVEL FOUND: 5.497958183288574, Considering 10/90 traces
 78046/100000: episode: 1176, duration: 4.696s, episode steps: 100, steps per second: 21, episode reward: 184.974, mean reward: 1.850 [1.497, 2.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.004, 10.098], loss: 0.095124, mae: 0.303585, mean_q: 3.871989
 78065/100000: episode: 1177, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 54.291, mean reward: 2.857 [2.138, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.367, 10.100], loss: 0.072157, mae: 0.275792, mean_q: 3.880398
 78082/100000: episode: 1178, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 37.806, mean reward: 2.224 [1.763, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.123, 10.100], loss: 0.096037, mae: 0.308745, mean_q: 3.922517
 78131/100000: episode: 1179, duration: 0.273s, episode steps: 49, steps per second: 179, episode reward: 145.943, mean reward: 2.978 [1.800, 4.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-0.327, 10.397], loss: 0.104127, mae: 0.314721, mean_q: 3.858312
 78230/100000: episode: 1180, duration: 0.543s, episode steps: 99, steps per second: 182, episode reward: 199.747, mean reward: 2.018 [1.455, 4.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.461 [-0.855, 10.412], loss: 0.098469, mae: 0.311525, mean_q: 3.908186
 78329/100000: episode: 1181, duration: 0.525s, episode steps: 99, steps per second: 188, episode reward: 192.488, mean reward: 1.944 [1.454, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.465 [-0.397, 10.100], loss: 0.100977, mae: 0.310901, mean_q: 3.914795
 78346/100000: episode: 1182, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 61.981, mean reward: 3.646 [2.770, 5.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.387, 10.100], loss: 0.107341, mae: 0.310069, mean_q: 3.887337
 78381/100000: episode: 1183, duration: 0.181s, episode steps: 35, steps per second: 194, episode reward: 71.747, mean reward: 2.050 [1.552, 3.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.174, 10.177], loss: 0.071334, mae: 0.278482, mean_q: 3.875603
 78408/100000: episode: 1184, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 107.728, mean reward: 3.990 [2.567, 6.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.296, 10.490], loss: 0.087303, mae: 0.290779, mean_q: 3.908455
 78443/100000: episode: 1185, duration: 0.200s, episode steps: 35, steps per second: 175, episode reward: 65.751, mean reward: 1.879 [1.469, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.240, 10.100], loss: 0.077376, mae: 0.293436, mean_q: 3.911837
 78470/100000: episode: 1186, duration: 0.158s, episode steps: 27, steps per second: 171, episode reward: 68.645, mean reward: 2.542 [1.839, 4.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.432, 10.250], loss: 0.076364, mae: 0.290300, mean_q: 3.929259
 78569/100000: episode: 1187, duration: 0.527s, episode steps: 99, steps per second: 188, episode reward: 183.802, mean reward: 1.857 [1.438, 4.122], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-1.588, 10.100], loss: 0.099026, mae: 0.304971, mean_q: 3.917464
 78592/100000: episode: 1188, duration: 0.131s, episode steps: 23, steps per second: 175, episode reward: 63.649, mean reward: 2.767 [1.753, 5.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.791, 10.280], loss: 0.088749, mae: 0.304659, mean_q: 3.929349
 78691/100000: episode: 1189, duration: 0.518s, episode steps: 99, steps per second: 191, episode reward: 213.082, mean reward: 2.152 [1.456, 6.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-1.225, 10.100], loss: 0.113397, mae: 0.315108, mean_q: 3.930986
 78726/100000: episode: 1190, duration: 0.194s, episode steps: 35, steps per second: 180, episode reward: 85.412, mean reward: 2.440 [1.565, 4.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.972, 10.216], loss: 0.099629, mae: 0.309134, mean_q: 3.916917
 78743/100000: episode: 1191, duration: 0.106s, episode steps: 17, steps per second: 161, episode reward: 40.901, mean reward: 2.406 [2.053, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.310, 10.100], loss: 0.107701, mae: 0.317056, mean_q: 3.919439
 78770/100000: episode: 1192, duration: 0.149s, episode steps: 27, steps per second: 182, episode reward: 65.324, mean reward: 2.419 [1.968, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.437], loss: 0.084466, mae: 0.307413, mean_q: 3.937256
 78869/100000: episode: 1193, duration: 0.546s, episode steps: 99, steps per second: 181, episode reward: 198.118, mean reward: 2.001 [1.463, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.464 [-1.447, 10.279], loss: 0.111742, mae: 0.326717, mean_q: 3.965465
 78888/100000: episode: 1194, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 97.589, mean reward: 5.136 [2.828, 11.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.661, 10.100], loss: 0.104916, mae: 0.327234, mean_q: 3.998774
 78923/100000: episode: 1195, duration: 0.196s, episode steps: 35, steps per second: 179, episode reward: 70.441, mean reward: 2.013 [1.465, 2.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.150, 10.100], loss: 0.146068, mae: 0.336771, mean_q: 4.011826
 78950/100000: episode: 1196, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 66.060, mean reward: 2.447 [1.588, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.035, 10.358], loss: 0.132045, mae: 0.356647, mean_q: 4.001446
 79049/100000: episode: 1197, duration: 0.558s, episode steps: 99, steps per second: 178, episode reward: 197.706, mean reward: 1.997 [1.491, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-0.887, 10.228], loss: 0.125598, mae: 0.333198, mean_q: 4.011915
 79068/100000: episode: 1198, duration: 0.124s, episode steps: 19, steps per second: 153, episode reward: 65.334, mean reward: 3.439 [2.347, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.348, 10.100], loss: 0.096445, mae: 0.305142, mean_q: 4.047991
 79103/100000: episode: 1199, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 77.595, mean reward: 2.217 [1.550, 3.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.405, 10.170], loss: 0.156562, mae: 0.349423, mean_q: 4.024396
 79107/100000: episode: 1200, duration: 0.023s, episode steps: 4, steps per second: 176, episode reward: 11.180, mean reward: 2.795 [2.560, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.450], loss: 0.254363, mae: 0.320214, mean_q: 3.897984
 79124/100000: episode: 1201, duration: 0.112s, episode steps: 17, steps per second: 152, episode reward: 50.878, mean reward: 2.993 [2.397, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.367, 10.100], loss: 0.139031, mae: 0.373041, mean_q: 4.085917
 79223/100000: episode: 1202, duration: 0.526s, episode steps: 99, steps per second: 188, episode reward: 189.483, mean reward: 1.914 [1.456, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.457 [-1.032, 10.100], loss: 0.120660, mae: 0.327461, mean_q: 4.041031
 79227/100000: episode: 1203, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: 12.122, mean reward: 3.030 [2.676, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.490], loss: 0.105276, mae: 0.358859, mean_q: 3.948065
 79254/100000: episode: 1204, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 73.854, mean reward: 2.735 [2.251, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-1.209, 10.382], loss: 0.119867, mae: 0.335379, mean_q: 4.015215
 79281/100000: episode: 1205, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 71.111, mean reward: 2.634 [1.689, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.740, 10.325], loss: 0.106620, mae: 0.328016, mean_q: 4.052309
 79321/100000: episode: 1206, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 109.582, mean reward: 2.740 [1.510, 8.698], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.635, 10.137], loss: 0.144978, mae: 0.342016, mean_q: 4.041553
 79361/100000: episode: 1207, duration: 0.227s, episode steps: 40, steps per second: 177, episode reward: 110.906, mean reward: 2.773 [2.027, 6.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.995, 10.335], loss: 0.121159, mae: 0.333837, mean_q: 4.026656
 79384/100000: episode: 1208, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 53.680, mean reward: 2.334 [1.493, 3.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.140, 10.253], loss: 0.156127, mae: 0.377803, mean_q: 4.103474
 79403/100000: episode: 1209, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 70.959, mean reward: 3.735 [2.365, 10.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-1.361, 10.100], loss: 0.153224, mae: 0.370751, mean_q: 4.191110
 79438/100000: episode: 1210, duration: 0.212s, episode steps: 35, steps per second: 165, episode reward: 83.312, mean reward: 2.380 [1.829, 3.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.035, 10.290], loss: 0.162753, mae: 0.347634, mean_q: 4.092537
 79465/100000: episode: 1211, duration: 0.156s, episode steps: 27, steps per second: 173, episode reward: 71.664, mean reward: 2.654 [1.628, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.035, 10.295], loss: 0.155062, mae: 0.362635, mean_q: 4.034441
 79500/100000: episode: 1212, duration: 0.191s, episode steps: 35, steps per second: 183, episode reward: 106.704, mean reward: 3.049 [1.966, 5.423], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.035, 10.316], loss: 0.151027, mae: 0.352841, mean_q: 4.143081
 79535/100000: episode: 1213, duration: 0.199s, episode steps: 35, steps per second: 176, episode reward: 91.132, mean reward: 2.604 [1.915, 3.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.343, 10.293], loss: 0.116367, mae: 0.334908, mean_q: 4.058947
 79634/100000: episode: 1214, duration: 0.518s, episode steps: 99, steps per second: 191, episode reward: 217.584, mean reward: 2.198 [1.469, 4.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-0.479, 10.270], loss: 0.185020, mae: 0.379222, mean_q: 4.132266
 79669/100000: episode: 1215, duration: 0.194s, episode steps: 35, steps per second: 181, episode reward: 111.298, mean reward: 3.180 [1.674, 6.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.490, 10.259], loss: 0.144039, mae: 0.347036, mean_q: 4.152105
 79673/100000: episode: 1216, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 13.696, mean reward: 3.424 [3.253, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.816, 10.489], loss: 0.082170, mae: 0.313824, mean_q: 4.115449
 79690/100000: episode: 1217, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 44.603, mean reward: 2.624 [2.011, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.258, 10.100], loss: 0.133578, mae: 0.337442, mean_q: 4.092340
 79707/100000: episode: 1218, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 40.329, mean reward: 2.372 [1.882, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.290, 10.100], loss: 0.182675, mae: 0.390784, mean_q: 4.111939
 79730/100000: episode: 1219, duration: 0.137s, episode steps: 23, steps per second: 167, episode reward: 57.036, mean reward: 2.480 [2.149, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.035, 10.437], loss: 0.171741, mae: 0.382702, mean_q: 4.199516
 79829/100000: episode: 1220, duration: 0.520s, episode steps: 99, steps per second: 190, episode reward: 195.643, mean reward: 1.976 [1.470, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-0.736, 10.108], loss: 0.165703, mae: 0.356841, mean_q: 4.117208
 79848/100000: episode: 1221, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 54.249, mean reward: 2.855 [2.011, 6.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.355, 10.100], loss: 0.172502, mae: 0.365923, mean_q: 4.163806
 79875/100000: episode: 1222, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 76.403, mean reward: 2.830 [2.133, 3.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.441, 10.424], loss: 0.199420, mae: 0.389216, mean_q: 4.129814
 79879/100000: episode: 1223, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 13.459, mean reward: 3.365 [3.071, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.494], loss: 0.093941, mae: 0.353203, mean_q: 4.244820
 79898/100000: episode: 1224, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 70.220, mean reward: 3.696 [2.705, 10.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.412, 10.100], loss: 0.240030, mae: 0.406066, mean_q: 4.180381
 79915/100000: episode: 1225, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 47.671, mean reward: 2.804 [2.135, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.981, 10.100], loss: 0.178881, mae: 0.376162, mean_q: 4.194288
 79942/100000: episode: 1226, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 71.882, mean reward: 2.662 [1.984, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.999, 10.368], loss: 0.191299, mae: 0.391545, mean_q: 4.148722
 79959/100000: episode: 1227, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 46.777, mean reward: 2.752 [2.379, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.183, 10.100], loss: 0.138150, mae: 0.388793, mean_q: 4.131744
 79963/100000: episode: 1228, duration: 0.033s, episode steps: 4, steps per second: 123, episode reward: 11.899, mean reward: 2.975 [2.633, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.384], loss: 0.153720, mae: 0.428377, mean_q: 4.371063
 79967/100000: episode: 1229, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 14.009, mean reward: 3.502 [2.901, 4.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.349 [-0.035, 10.559], loss: 0.142529, mae: 0.396230, mean_q: 4.276345
 80007/100000: episode: 1230, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 88.066, mean reward: 2.202 [1.474, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.715, 10.152], loss: 0.163799, mae: 0.372368, mean_q: 4.196866
 80034/100000: episode: 1231, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 59.091, mean reward: 2.189 [1.661, 4.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.309, 10.230], loss: 0.148997, mae: 0.364268, mean_q: 4.200807
 80069/100000: episode: 1232, duration: 0.205s, episode steps: 35, steps per second: 170, episode reward: 85.899, mean reward: 2.454 [1.821, 3.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.019 [-0.400, 10.210], loss: 0.171301, mae: 0.367942, mean_q: 4.219230
 80118/100000: episode: 1233, duration: 0.294s, episode steps: 49, steps per second: 166, episode reward: 99.968, mean reward: 2.040 [1.524, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.413, 10.100], loss: 0.210258, mae: 0.405300, mean_q: 4.202370
 80217/100000: episode: 1234, duration: 0.519s, episode steps: 99, steps per second: 191, episode reward: 179.900, mean reward: 1.817 [1.441, 2.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.264, 10.206], loss: 0.186184, mae: 0.393409, mean_q: 4.203963
 80252/100000: episode: 1235, duration: 0.193s, episode steps: 35, steps per second: 181, episode reward: 73.071, mean reward: 2.088 [1.700, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.399, 10.341], loss: 0.180160, mae: 0.374249, mean_q: 4.193926
 80271/100000: episode: 1236, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 47.542, mean reward: 2.502 [1.773, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.127, 10.100], loss: 0.165747, mae: 0.382167, mean_q: 4.147484
 80288/100000: episode: 1237, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 67.496, mean reward: 3.970 [1.999, 7.715], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.117, 10.100], loss: 0.130640, mae: 0.369906, mean_q: 4.264656
 80305/100000: episode: 1238, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 51.652, mean reward: 3.038 [2.329, 5.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.226, 10.100], loss: 0.107026, mae: 0.321926, mean_q: 4.091866
 80332/100000: episode: 1239, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 67.177, mean reward: 2.488 [1.931, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.058, 10.282], loss: 0.178954, mae: 0.371801, mean_q: 4.174113
 80372/100000: episode: 1240, duration: 0.231s, episode steps: 40, steps per second: 173, episode reward: 100.768, mean reward: 2.519 [1.471, 4.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.983 [-0.576, 10.109], loss: 0.136962, mae: 0.343108, mean_q: 4.187809
 80421/100000: episode: 1241, duration: 0.271s, episode steps: 49, steps per second: 181, episode reward: 94.499, mean reward: 1.929 [1.466, 2.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.914 [-0.645, 10.100], loss: 0.185510, mae: 0.389813, mean_q: 4.218125
 80438/100000: episode: 1242, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 38.672, mean reward: 2.275 [1.831, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.247, 10.100], loss: 0.138035, mae: 0.343974, mean_q: 4.161146
 80461/100000: episode: 1243, duration: 0.132s, episode steps: 23, steps per second: 175, episode reward: 59.674, mean reward: 2.595 [2.147, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.777, 10.421], loss: 0.155799, mae: 0.364424, mean_q: 4.226898
 80488/100000: episode: 1244, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 63.240, mean reward: 2.342 [1.808, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.312, 10.318], loss: 0.155069, mae: 0.378781, mean_q: 4.311983
 80505/100000: episode: 1245, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 42.509, mean reward: 2.501 [1.964, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.157, 10.100], loss: 0.187922, mae: 0.382495, mean_q: 4.274103
 80604/100000: episode: 1246, duration: 0.519s, episode steps: 99, steps per second: 191, episode reward: 189.575, mean reward: 1.915 [1.456, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.460 [-0.849, 10.126], loss: 0.194545, mae: 0.394697, mean_q: 4.195956
 80621/100000: episode: 1247, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 58.827, mean reward: 3.460 [2.566, 4.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.376, 10.100], loss: 0.121924, mae: 0.358141, mean_q: 4.216493
 80640/100000: episode: 1248, duration: 0.098s, episode steps: 19, steps per second: 193, episode reward: 63.815, mean reward: 3.359 [2.425, 5.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.489, 10.100], loss: 0.184662, mae: 0.400225, mean_q: 4.289433
 80689/100000: episode: 1249, duration: 0.253s, episode steps: 49, steps per second: 194, episode reward: 136.751, mean reward: 2.791 [1.728, 4.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.334, 10.295], loss: 0.134476, mae: 0.357446, mean_q: 4.241378
 80708/100000: episode: 1250, duration: 0.093s, episode steps: 19, steps per second: 203, episode reward: 67.748, mean reward: 3.566 [2.799, 4.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.502, 10.100], loss: 0.169467, mae: 0.385829, mean_q: 4.230589
 80807/100000: episode: 1251, duration: 0.526s, episode steps: 99, steps per second: 188, episode reward: 196.121, mean reward: 1.981 [1.487, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.455 [-0.908, 10.100], loss: 0.175700, mae: 0.385059, mean_q: 4.298663
 80824/100000: episode: 1252, duration: 0.103s, episode steps: 17, steps per second: 166, episode reward: 105.363, mean reward: 6.198 [2.906, 11.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.381, 10.100], loss: 0.269268, mae: 0.429961, mean_q: 4.361598
 80843/100000: episode: 1253, duration: 0.107s, episode steps: 19, steps per second: 177, episode reward: 61.057, mean reward: 3.214 [2.536, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.165, 10.100], loss: 0.237629, mae: 0.431791, mean_q: 4.412776
 80870/100000: episode: 1254, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 71.045, mean reward: 2.631 [1.745, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.386, 10.299], loss: 0.190089, mae: 0.387137, mean_q: 4.337385
 80887/100000: episode: 1255, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 55.497, mean reward: 3.265 [2.120, 11.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.215, 10.100], loss: 0.207023, mae: 0.411260, mean_q: 4.347149
 80936/100000: episode: 1256, duration: 0.282s, episode steps: 49, steps per second: 174, episode reward: 113.868, mean reward: 2.324 [1.628, 3.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.514, 10.325], loss: 0.213765, mae: 0.406309, mean_q: 4.339565
 81035/100000: episode: 1257, duration: 0.530s, episode steps: 99, steps per second: 187, episode reward: 210.876, mean reward: 2.130 [1.441, 6.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.456 [-1.966, 10.143], loss: 0.192114, mae: 0.384015, mean_q: 4.327151
 81075/100000: episode: 1258, duration: 0.211s, episode steps: 40, steps per second: 190, episode reward: 93.461, mean reward: 2.337 [1.645, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.970, 10.278], loss: 0.197608, mae: 0.410678, mean_q: 4.417676
 81174/100000: episode: 1259, duration: 0.545s, episode steps: 99, steps per second: 182, episode reward: 181.869, mean reward: 1.837 [1.468, 2.909], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-0.923, 10.146], loss: 0.218521, mae: 0.415994, mean_q: 4.330771
 81214/100000: episode: 1260, duration: 0.227s, episode steps: 40, steps per second: 176, episode reward: 110.567, mean reward: 2.764 [1.605, 6.095], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.855, 10.103], loss: 0.175878, mae: 0.401498, mean_q: 4.371968
 81313/100000: episode: 1261, duration: 0.522s, episode steps: 99, steps per second: 190, episode reward: 204.436, mean reward: 2.065 [1.461, 7.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.467 [-1.154, 10.191], loss: 0.216891, mae: 0.403056, mean_q: 4.372369
 81362/100000: episode: 1262, duration: 0.274s, episode steps: 49, steps per second: 179, episode reward: 123.904, mean reward: 2.529 [1.962, 4.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.881, 10.345], loss: 0.287059, mae: 0.436084, mean_q: 4.314498
 81389/100000: episode: 1263, duration: 0.142s, episode steps: 27, steps per second: 191, episode reward: 71.211, mean reward: 2.637 [1.881, 3.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.334], loss: 0.332555, mae: 0.457682, mean_q: 4.457671
 81412/100000: episode: 1264, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 66.431, mean reward: 2.888 [2.381, 4.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.230, 10.543], loss: 0.220872, mae: 0.408324, mean_q: 4.348684
 81431/100000: episode: 1265, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 48.631, mean reward: 2.560 [1.888, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.268, 10.100], loss: 0.193311, mae: 0.377500, mean_q: 4.349402
[Info] 2-TH LEVEL FOUND: 8.093034744262695, Considering 10/90 traces
 81454/100000: episode: 1266, duration: 4.297s, episode steps: 23, steps per second: 5, episode reward: 63.659, mean reward: 2.768 [2.306, 3.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.414], loss: 0.205656, mae: 0.392374, mean_q: 4.370145
 81459/100000: episode: 1267, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 16.724, mean reward: 3.345 [2.870, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.399, 10.100], loss: 0.169639, mae: 0.360019, mean_q: 4.223491
 81464/100000: episode: 1268, duration: 0.038s, episode steps: 5, steps per second: 131, episode reward: 21.492, mean reward: 4.298 [3.832, 4.603], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.420, 10.100], loss: 0.167780, mae: 0.376150, mean_q: 4.261392
 81469/100000: episode: 1269, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 19.944, mean reward: 3.989 [3.596, 5.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.361, 10.100], loss: 0.194352, mae: 0.404679, mean_q: 4.500876
 81474/100000: episode: 1270, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 20.362, mean reward: 4.072 [3.441, 4.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.494, 10.100], loss: 0.180843, mae: 0.337946, mean_q: 4.374217
 81485/100000: episode: 1271, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 39.872, mean reward: 3.625 [2.822, 4.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.457], loss: 0.236556, mae: 0.374737, mean_q: 4.382163
 81498/100000: episode: 1272, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 49.776, mean reward: 3.829 [2.266, 5.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.427], loss: 0.163016, mae: 0.388779, mean_q: 4.349643
 81511/100000: episode: 1273, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 70.130, mean reward: 5.395 [3.763, 10.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.683, 10.393], loss: 0.245891, mae: 0.430484, mean_q: 4.419061
 81516/100000: episode: 1274, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 20.888, mean reward: 4.178 [3.851, 4.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.531, 10.100], loss: 0.186396, mae: 0.463728, mean_q: 4.518172
 81527/100000: episode: 1275, duration: 0.072s, episode steps: 11, steps per second: 153, episode reward: 41.458, mean reward: 3.769 [2.676, 6.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.335, 10.100], loss: 0.166176, mae: 0.381949, mean_q: 4.297388
 81532/100000: episode: 1276, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 15.830, mean reward: 3.166 [2.738, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.470, 10.100], loss: 0.322824, mae: 0.494817, mean_q: 4.537610
 81537/100000: episode: 1277, duration: 0.042s, episode steps: 5, steps per second: 119, episode reward: 19.042, mean reward: 3.808 [3.319, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.353, 10.100], loss: 0.201944, mae: 0.478312, mean_q: 4.514109
 81547/100000: episode: 1278, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 34.103, mean reward: 3.410 [2.690, 4.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.270, 10.100], loss: 0.195705, mae: 0.399405, mean_q: 4.449475
 81558/100000: episode: 1279, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 39.748, mean reward: 3.613 [2.902, 5.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.261, 10.100], loss: 0.281430, mae: 0.469283, mean_q: 4.632322
 81563/100000: episode: 1280, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 17.001, mean reward: 3.400 [2.882, 4.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.324, 10.100], loss: 0.188769, mae: 0.400409, mean_q: 4.222575
 81577/100000: episode: 1281, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 96.221, mean reward: 6.873 [3.832, 15.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.522, 10.100], loss: 0.226144, mae: 0.417511, mean_q: 4.476228
 81582/100000: episode: 1282, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 22.185, mean reward: 4.437 [3.900, 4.902], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.333, 10.100], loss: 0.425428, mae: 0.491441, mean_q: 4.419645
 81593/100000: episode: 1283, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 33.843, mean reward: 3.077 [2.507, 4.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.425], loss: 0.348185, mae: 0.459328, mean_q: 4.474967
 81598/100000: episode: 1284, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 16.498, mean reward: 3.300 [3.110, 3.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.571, 10.100], loss: 0.388825, mae: 0.517273, mean_q: 4.635880
 81607/100000: episode: 1285, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 48.270, mean reward: 5.363 [3.731, 7.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.380, 10.100], loss: 0.202892, mae: 0.431798, mean_q: 4.497707
 81612/100000: episode: 1286, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 26.832, mean reward: 5.366 [4.078, 7.195], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.409, 10.100], loss: 0.307372, mae: 0.514673, mean_q: 4.726643
 81622/100000: episode: 1287, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 48.688, mean reward: 4.869 [4.061, 6.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.516, 10.100], loss: 0.262567, mae: 0.426681, mean_q: 4.418979
 81627/100000: episode: 1288, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 23.090, mean reward: 4.618 [3.336, 6.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.297, 10.100], loss: 0.222460, mae: 0.407634, mean_q: 4.616745
 81632/100000: episode: 1289, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 17.684, mean reward: 3.537 [2.669, 4.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.546, 10.100], loss: 0.089735, mae: 0.312776, mean_q: 4.379809
 81646/100000: episode: 1290, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 60.388, mean reward: 4.313 [3.162, 6.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.532, 10.100], loss: 0.190647, mae: 0.419427, mean_q: 4.470690
 81656/100000: episode: 1291, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 56.619, mean reward: 5.662 [3.827, 9.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.427, 10.100], loss: 0.475092, mae: 0.549946, mean_q: 4.591662
 81661/100000: episode: 1292, duration: 0.039s, episode steps: 5, steps per second: 129, episode reward: 17.615, mean reward: 3.523 [3.305, 4.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.570, 10.100], loss: 0.251885, mae: 0.393012, mean_q: 4.172643
 81666/100000: episode: 1293, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 22.197, mean reward: 4.439 [3.428, 5.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.641, 10.100], loss: 0.816514, mae: 0.669829, mean_q: 4.714775
 81676/100000: episode: 1294, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 47.797, mean reward: 4.780 [3.386, 8.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.344, 10.100], loss: 0.332018, mae: 0.510759, mean_q: 4.412035
 81693/100000: episode: 1295, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 43.443, mean reward: 2.555 [1.945, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.303, 10.100], loss: 0.265893, mae: 0.502406, mean_q: 4.608842
 81704/100000: episode: 1296, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 130.603, mean reward: 11.873 [2.484, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.339, 10.100], loss: 0.247929, mae: 0.455846, mean_q: 4.567808
 81713/100000: episode: 1297, duration: 0.065s, episode steps: 9, steps per second: 138, episode reward: 33.485, mean reward: 3.721 [2.973, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.343, 10.100], loss: 0.205415, mae: 0.405670, mean_q: 4.529470
 81718/100000: episode: 1298, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 17.986, mean reward: 3.597 [2.822, 4.468], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.483, 10.100], loss: 0.131491, mae: 0.364996, mean_q: 4.451765
 81723/100000: episode: 1299, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 26.144, mean reward: 5.229 [4.631, 5.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.411, 10.100], loss: 0.131175, mae: 0.380984, mean_q: 4.614096
 81740/100000: episode: 1300, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 49.313, mean reward: 2.901 [2.198, 4.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.215, 10.100], loss: 0.354961, mae: 0.507353, mean_q: 4.564152
 81750/100000: episode: 1301, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 37.905, mean reward: 3.790 [2.456, 5.163], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.355, 10.100], loss: 0.387056, mae: 0.457883, mean_q: 4.538241
 81767/100000: episode: 1302, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 47.548, mean reward: 2.797 [1.905, 4.424], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.200, 10.100], loss: 0.237157, mae: 0.446877, mean_q: 4.509438
 81772/100000: episode: 1303, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 20.999, mean reward: 4.200 [3.751, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.568, 10.100], loss: 0.183773, mae: 0.425743, mean_q: 4.453777
 81783/100000: episode: 1304, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 76.912, mean reward: 6.992 [3.038, 34.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.372, 10.100], loss: 1.527677, mae: 0.540854, mean_q: 4.639980
 81788/100000: episode: 1305, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 20.035, mean reward: 4.007 [3.105, 5.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.457, 10.100], loss: 0.316199, mae: 0.518027, mean_q: 4.640458
 81799/100000: episode: 1306, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 41.232, mean reward: 3.748 [3.336, 4.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.569], loss: 13.908324, mae: 1.008381, mean_q: 4.947122
 81810/100000: episode: 1307, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 46.470, mean reward: 4.225 [2.744, 9.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.515, 10.100], loss: 0.410951, mae: 0.625198, mean_q: 4.526324
 81821/100000: episode: 1308, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 46.005, mean reward: 4.182 [2.677, 9.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.309, 10.100], loss: 0.265261, mae: 0.493268, mean_q: 4.639846
 81832/100000: episode: 1309, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 31.139, mean reward: 2.831 [2.257, 4.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.486], loss: 0.233529, mae: 0.465967, mean_q: 4.554211
 81845/100000: episode: 1310, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 44.489, mean reward: 3.422 [2.750, 4.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.518], loss: 0.207830, mae: 0.429084, mean_q: 4.531000
 81858/100000: episode: 1311, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 44.793, mean reward: 3.446 [2.462, 4.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.412, 10.498], loss: 0.294816, mae: 0.470100, mean_q: 4.789502
 81871/100000: episode: 1312, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 48.262, mean reward: 3.712 [2.625, 4.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.595, 10.475], loss: 1.389406, mae: 0.507838, mean_q: 4.531330
 81888/100000: episode: 1313, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 54.695, mean reward: 3.217 [2.528, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.393, 10.100], loss: 0.281502, mae: 0.466309, mean_q: 4.616052
 81893/100000: episode: 1314, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 25.577, mean reward: 5.115 [3.475, 6.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.553, 10.100], loss: 0.246578, mae: 0.428269, mean_q: 4.893051
 81903/100000: episode: 1315, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 46.995, mean reward: 4.700 [2.891, 6.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.321, 10.100], loss: 14.778313, mae: 0.921213, mean_q: 5.080894
 81914/100000: episode: 1316, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 28.423, mean reward: 2.584 [1.993, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.296], loss: 0.397098, mae: 0.601193, mean_q: 4.307862
 81931/100000: episode: 1317, duration: 0.107s, episode steps: 17, steps per second: 159, episode reward: 38.275, mean reward: 2.251 [1.758, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.123, 10.100], loss: 0.323691, mae: 0.540348, mean_q: 4.758329
 81945/100000: episode: 1318, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 63.807, mean reward: 4.558 [3.417, 6.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.521, 10.100], loss: 0.331243, mae: 0.550323, mean_q: 4.972770
 81954/100000: episode: 1319, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 32.803, mean reward: 3.645 [3.030, 4.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.344, 10.100], loss: 0.135633, mae: 0.385317, mean_q: 4.574255
 81959/100000: episode: 1320, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 19.661, mean reward: 3.932 [3.532, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.392, 10.100], loss: 0.457234, mae: 0.540152, mean_q: 4.866897
 81964/100000: episode: 1321, duration: 0.036s, episode steps: 5, steps per second: 138, episode reward: 16.640, mean reward: 3.328 [2.854, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-1.074, 10.100], loss: 0.367746, mae: 0.508806, mean_q: 4.602028
 81969/100000: episode: 1322, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 25.080, mean reward: 5.016 [4.476, 5.625], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.363, 10.100], loss: 0.498163, mae: 0.513261, mean_q: 4.800784
 81983/100000: episode: 1323, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 54.787, mean reward: 3.913 [2.619, 6.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.293, 10.100], loss: 0.320383, mae: 0.487025, mean_q: 4.803989
 81988/100000: episode: 1324, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 29.446, mean reward: 5.889 [4.440, 7.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.408, 10.100], loss: 0.209404, mae: 0.435706, mean_q: 4.912513
 81997/100000: episode: 1325, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 41.283, mean reward: 4.587 [3.159, 5.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.378, 10.100], loss: 0.348274, mae: 0.475709, mean_q: 4.936180
 82008/100000: episode: 1326, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 29.178, mean reward: 2.653 [1.818, 5.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.431, 10.100], loss: 0.212271, mae: 0.457239, mean_q: 4.802527
 82019/100000: episode: 1327, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 37.383, mean reward: 3.398 [2.753, 4.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.488], loss: 1.617028, mae: 0.608935, mean_q: 4.836972
 82030/100000: episode: 1328, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 40.971, mean reward: 3.725 [3.311, 4.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.384, 10.100], loss: 0.321975, mae: 0.462099, mean_q: 4.495159
 82041/100000: episode: 1329, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 41.622, mean reward: 3.784 [2.578, 5.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-1.234, 10.452], loss: 0.228077, mae: 0.462601, mean_q: 4.805886
 82052/100000: episode: 1330, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 38.567, mean reward: 3.506 [2.940, 5.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.347, 10.100], loss: 0.526125, mae: 0.605654, mean_q: 4.937888
 82061/100000: episode: 1331, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 36.133, mean reward: 4.015 [3.303, 5.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.357, 10.100], loss: 0.311265, mae: 0.500554, mean_q: 4.803616
 82066/100000: episode: 1332, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 18.547, mean reward: 3.709 [3.272, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.408, 10.100], loss: 0.276998, mae: 0.519136, mean_q: 4.994541
 82077/100000: episode: 1333, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 89.894, mean reward: 8.172 [3.937, 30.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.317, 10.100], loss: 0.389495, mae: 0.499517, mean_q: 4.720388
 82088/100000: episode: 1334, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 39.733, mean reward: 3.612 [2.536, 4.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.204, 10.403], loss: 0.382296, mae: 0.508009, mean_q: 4.863997
 82105/100000: episode: 1335, duration: 0.110s, episode steps: 17, steps per second: 154, episode reward: 41.177, mean reward: 2.422 [1.979, 2.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.206, 10.100], loss: 0.308846, mae: 0.476666, mean_q: 4.898157
 82116/100000: episode: 1336, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 38.247, mean reward: 3.477 [2.806, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.285, 10.100], loss: 0.327851, mae: 0.502404, mean_q: 4.859839
 82121/100000: episode: 1337, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 20.997, mean reward: 4.199 [3.762, 5.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.382, 10.100], loss: 0.421665, mae: 0.527021, mean_q: 4.847552
 82138/100000: episode: 1338, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 48.132, mean reward: 2.831 [2.312, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.148, 10.100], loss: 1.751595, mae: 0.607296, mean_q: 4.911511
 82143/100000: episode: 1339, duration: 0.029s, episode steps: 5, steps per second: 170, episode reward: 27.972, mean reward: 5.594 [3.483, 8.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.560, 10.100], loss: 0.165112, mae: 0.422382, mean_q: 4.690009
 82156/100000: episode: 1340, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 68.632, mean reward: 5.279 [3.006, 13.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-1.052, 10.488], loss: 0.275477, mae: 0.483112, mean_q: 4.723895
 82173/100000: episode: 1341, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 42.502, mean reward: 2.500 [2.133, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.322, 10.100], loss: 0.273279, mae: 0.477676, mean_q: 4.843366
 82178/100000: episode: 1342, duration: 0.039s, episode steps: 5, steps per second: 127, episode reward: 19.276, mean reward: 3.855 [3.139, 5.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.449, 10.100], loss: 0.471902, mae: 0.558015, mean_q: 4.895099
 82188/100000: episode: 1343, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 47.106, mean reward: 4.711 [2.884, 9.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.226, 10.100], loss: 0.319169, mae: 0.511339, mean_q: 4.855611
 82199/100000: episode: 1344, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 41.160, mean reward: 3.742 [2.608, 6.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.195, 10.100], loss: 0.412714, mae: 0.534125, mean_q: 4.930924
 82216/100000: episode: 1345, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 57.635, mean reward: 3.390 [2.888, 4.882], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.528, 10.100], loss: 9.000227, mae: 0.895416, mean_q: 4.961407
 82221/100000: episode: 1346, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 20.265, mean reward: 4.053 [3.233, 4.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.471, 10.100], loss: 0.380036, mae: 0.610883, mean_q: 5.202993
 82226/100000: episode: 1347, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 17.409, mean reward: 3.482 [3.190, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.441, 10.100], loss: 2.786207, mae: 0.828105, mean_q: 5.250655
 82231/100000: episode: 1348, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 20.573, mean reward: 4.115 [3.910, 4.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.438, 10.100], loss: 0.510742, mae: 0.704733, mean_q: 5.302115
 82242/100000: episode: 1349, duration: 0.057s, episode steps: 11, steps per second: 195, episode reward: 35.067, mean reward: 3.188 [2.572, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.428], loss: 0.398580, mae: 0.523212, mean_q: 4.826690
 82259/100000: episode: 1350, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 41.147, mean reward: 2.420 [2.015, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.517, 10.100], loss: 0.381329, mae: 0.556883, mean_q: 4.971133
 82269/100000: episode: 1351, duration: 0.067s, episode steps: 10, steps per second: 148, episode reward: 50.123, mean reward: 5.012 [3.138, 11.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.319, 10.100], loss: 0.239317, mae: 0.491285, mean_q: 4.873484
 82278/100000: episode: 1352, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 35.068, mean reward: 3.896 [3.054, 5.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.477, 10.100], loss: 0.443256, mae: 0.527529, mean_q: 5.039760
 82292/100000: episode: 1353, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 61.016, mean reward: 4.358 [2.610, 6.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.854, 10.100], loss: 0.300054, mae: 0.480830, mean_q: 4.887098
 82309/100000: episode: 1354, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 42.911, mean reward: 2.524 [1.876, 4.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.243, 10.100], loss: 1.182291, mae: 0.615963, mean_q: 5.057255
 82318/100000: episode: 1355, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 39.620, mean reward: 4.402 [3.874, 5.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.389, 10.100], loss: 0.485421, mae: 0.505993, mean_q: 4.574094
[Info] 3-TH LEVEL FOUND: 9.8482666015625, Considering 10/90 traces
 82328/100000: episode: 1356, duration: 4.228s, episode steps: 10, steps per second: 2, episode reward: 43.970, mean reward: 4.397 [3.710, 6.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.412, 10.100], loss: 1.598460, mae: 0.549672, mean_q: 4.829130
 82337/100000: episode: 1357, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 42.815, mean reward: 4.757 [3.018, 6.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.462, 10.100], loss: 0.295886, mae: 0.466644, mean_q: 4.987348
 82346/100000: episode: 1358, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 41.308, mean reward: 4.590 [2.721, 6.189], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.251, 10.100], loss: 0.646031, mae: 0.613896, mean_q: 5.115509
 82356/100000: episode: 1359, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 33.203, mean reward: 3.320 [2.561, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.214, 10.100], loss: 0.586733, mae: 0.572177, mean_q: 4.980920
 82365/100000: episode: 1360, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 35.668, mean reward: 3.963 [3.354, 5.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.402, 10.100], loss: 0.479657, mae: 0.565789, mean_q: 5.013258
 82373/100000: episode: 1361, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 38.081, mean reward: 4.760 [3.871, 5.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.481, 10.100], loss: 0.517726, mae: 0.570041, mean_q: 5.019864
 82382/100000: episode: 1362, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 50.601, mean reward: 5.622 [4.196, 8.462], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.314, 10.100], loss: 0.340744, mae: 0.502523, mean_q: 4.948655
 82392/100000: episode: 1363, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 42.276, mean reward: 4.228 [3.743, 4.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.426, 10.100], loss: 0.473223, mae: 0.576614, mean_q: 5.027164
 82401/100000: episode: 1364, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 32.276, mean reward: 3.586 [2.676, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.332, 10.100], loss: 16.599918, mae: 1.163320, mean_q: 5.577488
 82410/100000: episode: 1365, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 36.414, mean reward: 4.046 [3.360, 5.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.387, 10.100], loss: 0.552034, mae: 0.612803, mean_q: 4.636262
 82420/100000: episode: 1366, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 87.400, mean reward: 8.740 [4.950, 21.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.318, 10.100], loss: 0.370700, mae: 0.527595, mean_q: 5.175136
 82429/100000: episode: 1367, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 40.097, mean reward: 4.455 [3.785, 5.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.383, 10.100], loss: 0.301806, mae: 0.495132, mean_q: 5.023817
 82437/100000: episode: 1368, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 34.803, mean reward: 4.350 [3.187, 5.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.243, 10.100], loss: 0.513105, mae: 0.620801, mean_q: 5.231242
 82445/100000: episode: 1369, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 32.257, mean reward: 4.032 [3.248, 4.975], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.356, 10.100], loss: 1.263065, mae: 0.655388, mean_q: 5.091960
 82454/100000: episode: 1370, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 35.500, mean reward: 3.944 [3.313, 5.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.280, 10.100], loss: 0.256904, mae: 0.492935, mean_q: 4.987129
 82463/100000: episode: 1371, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 40.644, mean reward: 4.516 [4.136, 5.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.418, 10.100], loss: 0.201280, mae: 0.462199, mean_q: 5.048662
 82471/100000: episode: 1372, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 43.808, mean reward: 5.476 [4.500, 6.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.381, 10.100], loss: 0.368284, mae: 0.515288, mean_q: 5.165437
 82479/100000: episode: 1373, duration: 0.055s, episode steps: 8, steps per second: 146, episode reward: 32.260, mean reward: 4.032 [2.533, 6.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.396, 10.100], loss: 0.250115, mae: 0.476642, mean_q: 4.929293
 82487/100000: episode: 1374, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 33.697, mean reward: 4.212 [3.573, 4.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.320, 10.100], loss: 0.465174, mae: 0.540050, mean_q: 5.206737
 82496/100000: episode: 1375, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 38.030, mean reward: 4.226 [3.635, 5.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.350, 10.100], loss: 0.367719, mae: 0.532806, mean_q: 5.076184
 82505/100000: episode: 1376, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 48.089, mean reward: 5.343 [3.923, 8.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.304, 10.100], loss: 0.618397, mae: 0.642828, mean_q: 5.266366
 82513/100000: episode: 1377, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 36.238, mean reward: 4.530 [3.754, 5.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.358, 10.100], loss: 0.471956, mae: 0.553530, mean_q: 5.059749
 82521/100000: episode: 1378, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 49.591, mean reward: 6.199 [4.178, 8.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.372, 10.100], loss: 0.297065, mae: 0.538649, mean_q: 5.191391
 82530/100000: episode: 1379, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 33.924, mean reward: 3.769 [3.154, 5.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.332, 10.100], loss: 0.911707, mae: 0.641870, mean_q: 5.294120
 82539/100000: episode: 1380, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 29.055, mean reward: 3.228 [2.840, 4.027], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.315, 10.100], loss: 1.412355, mae: 0.576187, mean_q: 4.913078
 82547/100000: episode: 1381, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 37.305, mean reward: 4.663 [3.373, 6.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.363, 10.100], loss: 0.239765, mae: 0.464943, mean_q: 4.993658
 82556/100000: episode: 1382, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 53.524, mean reward: 5.947 [3.985, 8.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.509, 10.100], loss: 1.446568, mae: 0.622798, mean_q: 5.403143
 82563/100000: episode: 1383, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 33.628, mean reward: 4.804 [3.492, 6.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.393, 10.100], loss: 22.011560, mae: 1.216559, mean_q: 5.579867
 82572/100000: episode: 1384, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 38.776, mean reward: 4.308 [3.850, 5.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.420, 10.100], loss: 0.590731, mae: 0.712458, mean_q: 4.986845
 82580/100000: episode: 1385, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 40.774, mean reward: 5.097 [3.683, 7.352], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.535, 10.100], loss: 2.180295, mae: 0.734058, mean_q: 5.194589
 82587/100000: episode: 1386, duration: 0.049s, episode steps: 7, steps per second: 144, episode reward: 45.042, mean reward: 6.435 [4.306, 10.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.334, 10.100], loss: 0.389567, mae: 0.633420, mean_q: 5.256867
 82596/100000: episode: 1387, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 41.621, mean reward: 4.625 [3.530, 5.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.601, 10.100], loss: 0.430087, mae: 0.563132, mean_q: 5.227651
 82604/100000: episode: 1388, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 43.599, mean reward: 5.450 [3.901, 6.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.433, 10.100], loss: 0.361715, mae: 0.521348, mean_q: 5.245181
 82612/100000: episode: 1389, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 40.676, mean reward: 5.085 [3.823, 6.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.337, 10.100], loss: 0.615387, mae: 0.555974, mean_q: 5.125131
 82620/100000: episode: 1390, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 46.599, mean reward: 5.825 [4.539, 7.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.310, 10.100], loss: 18.227261, mae: 0.984266, mean_q: 5.373747
 82629/100000: episode: 1391, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 31.565, mean reward: 3.507 [2.805, 5.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.240, 10.100], loss: 0.747614, mae: 0.812575, mean_q: 5.458431
 82638/100000: episode: 1392, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 69.254, mean reward: 7.695 [4.487, 11.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.390, 10.100], loss: 1.485753, mae: 0.704602, mean_q: 5.369073
 82646/100000: episode: 1393, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 35.138, mean reward: 4.392 [3.592, 4.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.213, 10.100], loss: 0.301143, mae: 0.506310, mean_q: 5.087740
 82655/100000: episode: 1394, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 46.876, mean reward: 5.208 [3.930, 7.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.556, 10.100], loss: 15.952720, mae: 0.944420, mean_q: 5.247184
 82663/100000: episode: 1395, duration: 0.058s, episode steps: 8, steps per second: 137, episode reward: 37.626, mean reward: 4.703 [4.178, 5.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.616, 10.100], loss: 0.732159, mae: 0.854946, mean_q: 5.493377
 82673/100000: episode: 1396, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 58.261, mean reward: 5.826 [3.365, 9.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.321, 10.100], loss: 0.898373, mae: 0.838920, mean_q: 4.901721
 82680/100000: episode: 1397, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 49.441, mean reward: 7.063 [5.387, 11.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.510, 10.100], loss: 0.555854, mae: 0.694035, mean_q: 5.318292
 82687/100000: episode: 1398, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 31.376, mean reward: 4.482 [2.608, 6.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.358, 10.100], loss: 0.901069, mae: 0.685002, mean_q: 5.171431
 82696/100000: episode: 1399, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 62.163, mean reward: 6.907 [4.504, 18.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.565, 10.100], loss: 0.394090, mae: 0.574754, mean_q: 5.127962
 82705/100000: episode: 1400, duration: 0.059s, episode steps: 9, steps per second: 154, episode reward: 45.989, mean reward: 5.110 [2.352, 10.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.258, 10.100], loss: 0.530782, mae: 0.638197, mean_q: 5.336858
 82714/100000: episode: 1401, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 34.515, mean reward: 3.835 [3.129, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.403, 10.100], loss: 2.122694, mae: 0.715096, mean_q: 5.495025
 82723/100000: episode: 1402, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 44.188, mean reward: 4.910 [4.154, 6.409], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.440, 10.100], loss: 0.527472, mae: 0.687100, mean_q: 5.641261
 82732/100000: episode: 1403, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 41.446, mean reward: 4.605 [3.454, 5.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.373, 10.100], loss: 0.998887, mae: 0.664771, mean_q: 5.330933
 82741/100000: episode: 1404, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 78.749, mean reward: 8.750 [3.767, 24.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.281, 10.100], loss: 0.289091, mae: 0.503381, mean_q: 5.034525
 82749/100000: episode: 1405, duration: 0.054s, episode steps: 8, steps per second: 150, episode reward: 37.570, mean reward: 4.696 [3.944, 6.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.301, 10.100], loss: 0.236249, mae: 0.511219, mean_q: 5.523052
 82758/100000: episode: 1406, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 49.921, mean reward: 5.547 [4.548, 6.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.521, 10.100], loss: 0.856811, mae: 0.578202, mean_q: 5.500090
 82766/100000: episode: 1407, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 30.155, mean reward: 3.769 [3.313, 4.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.376, 10.100], loss: 0.315398, mae: 0.547671, mean_q: 5.558917
 82774/100000: episode: 1408, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 33.292, mean reward: 4.162 [3.709, 5.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.493, 10.100], loss: 0.481548, mae: 0.584358, mean_q: 5.537701
 82782/100000: episode: 1409, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 39.962, mean reward: 4.995 [4.266, 5.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.471, 10.100], loss: 0.318276, mae: 0.530607, mean_q: 5.356194
 82790/100000: episode: 1410, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 34.699, mean reward: 4.337 [3.294, 6.776], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.396, 10.100], loss: 2.213513, mae: 0.798027, mean_q: 5.843996
 82798/100000: episode: 1411, duration: 0.044s, episode steps: 8, steps per second: 181, episode reward: 42.374, mean reward: 5.297 [3.883, 6.556], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.375, 10.100], loss: 0.391919, mae: 0.553291, mean_q: 5.311407
 82807/100000: episode: 1412, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 39.377, mean reward: 4.375 [3.474, 5.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.502, 10.100], loss: 0.660055, mae: 0.632032, mean_q: 5.554022
[Info] FALSIFICATION!
[Info] Levels: [5.497958, 8.093035, 9.848267, 11.954864]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.01]
[Info] Error Prob: 1.0000000000000003e-05

 82809/100000: episode: 1413, duration: 4.368s, episode steps: 2, steps per second: 0, episode reward: 120.686, mean reward: 60.343 [20.686, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.250, 9.988], loss: 0.804824, mae: 0.674318, mean_q: 5.750435
 82909/100000: episode: 1414, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 184.932, mean reward: 1.849 [1.473, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.947, 10.118], loss: 8.937788, mae: 0.948977, mean_q: 5.599957
 83009/100000: episode: 1415, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 193.342, mean reward: 1.933 [1.448, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.910, 10.098], loss: 0.821329, mae: 0.645634, mean_q: 5.473448
 83109/100000: episode: 1416, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 199.264, mean reward: 1.993 [1.477, 4.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.693, 10.098], loss: 0.657704, mae: 0.598560, mean_q: 5.438482
 83209/100000: episode: 1417, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 220.940, mean reward: 2.209 [1.492, 6.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-2.193, 10.098], loss: 2.226872, mae: 0.698438, mean_q: 5.405497
 83309/100000: episode: 1418, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 198.897, mean reward: 1.989 [1.506, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.143, 10.250], loss: 3.313505, mae: 0.776888, mean_q: 5.541550
 83409/100000: episode: 1419, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 188.689, mean reward: 1.887 [1.454, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.312, 10.098], loss: 0.658152, mae: 0.605853, mean_q: 5.383898
 83509/100000: episode: 1420, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 180.203, mean reward: 1.802 [1.452, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.031, 10.098], loss: 2.018848, mae: 0.679010, mean_q: 5.516120
 83609/100000: episode: 1421, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 192.364, mean reward: 1.924 [1.448, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.069, 10.098], loss: 2.281671, mae: 0.670513, mean_q: 5.441265
 83709/100000: episode: 1422, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 187.401, mean reward: 1.874 [1.468, 5.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.607, 10.098], loss: 4.524362, mae: 0.727609, mean_q: 5.393527
 83809/100000: episode: 1423, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 207.801, mean reward: 2.078 [1.452, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.477, 10.098], loss: 2.656527, mae: 0.706343, mean_q: 5.289389
 83909/100000: episode: 1424, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 191.993, mean reward: 1.920 [1.448, 3.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.885, 10.228], loss: 0.615149, mae: 0.607503, mean_q: 5.330371
 84009/100000: episode: 1425, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 219.985, mean reward: 2.200 [1.549, 5.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.506, 10.098], loss: 7.208654, mae: 0.858386, mean_q: 5.379289
 84109/100000: episode: 1426, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 200.029, mean reward: 2.000 [1.489, 3.017], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.529, 10.390], loss: 0.757217, mae: 0.617324, mean_q: 5.359778
 84209/100000: episode: 1427, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 208.785, mean reward: 2.088 [1.476, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.809, 10.098], loss: 2.234497, mae: 0.698863, mean_q: 5.393106
 84309/100000: episode: 1428, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 196.949, mean reward: 1.969 [1.460, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.474, 10.098], loss: 0.535459, mae: 0.557333, mean_q: 5.214176
 84409/100000: episode: 1429, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 206.618, mean reward: 2.066 [1.531, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.807, 10.098], loss: 6.034935, mae: 0.837707, mean_q: 5.380205
 84509/100000: episode: 1430, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 192.758, mean reward: 1.928 [1.485, 5.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.386, 10.151], loss: 2.423884, mae: 0.691932, mean_q: 5.279303
 84609/100000: episode: 1431, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 188.961, mean reward: 1.890 [1.444, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.849, 10.098], loss: 1.756156, mae: 0.613615, mean_q: 5.212559
 84709/100000: episode: 1432, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 192.240, mean reward: 1.922 [1.461, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.635, 10.138], loss: 4.852589, mae: 0.754558, mean_q: 5.259727
 84809/100000: episode: 1433, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 194.866, mean reward: 1.949 [1.452, 4.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.390, 10.168], loss: 0.681899, mae: 0.604713, mean_q: 5.073140
 84909/100000: episode: 1434, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 182.799, mean reward: 1.828 [1.466, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.926, 10.121], loss: 0.672748, mae: 0.564115, mean_q: 5.134329
 85009/100000: episode: 1435, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 225.125, mean reward: 2.251 [1.499, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.371, 10.354], loss: 4.581785, mae: 0.734178, mean_q: 5.227616
 85109/100000: episode: 1436, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 178.602, mean reward: 1.786 [1.469, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.543, 10.098], loss: 2.290751, mae: 0.668313, mean_q: 5.211140
 85209/100000: episode: 1437, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 188.030, mean reward: 1.880 [1.455, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.616, 10.098], loss: 4.431858, mae: 0.707931, mean_q: 5.251637
 85309/100000: episode: 1438, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.839, mean reward: 1.878 [1.502, 2.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.801, 10.098], loss: 3.355141, mae: 0.649311, mean_q: 5.073398
 85409/100000: episode: 1439, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 190.426, mean reward: 1.904 [1.459, 2.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.347, 10.098], loss: 3.096066, mae: 0.671414, mean_q: 5.122179
 85509/100000: episode: 1440, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 189.010, mean reward: 1.890 [1.448, 2.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.588, 10.098], loss: 1.937221, mae: 0.614374, mean_q: 5.133390
 85609/100000: episode: 1441, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 242.210, mean reward: 2.422 [1.494, 6.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.168, 10.098], loss: 1.872984, mae: 0.602747, mean_q: 5.065373
 85709/100000: episode: 1442, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 214.094, mean reward: 2.141 [1.480, 5.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.846, 10.136], loss: 3.431906, mae: 0.637244, mean_q: 5.050750
 85809/100000: episode: 1443, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 192.481, mean reward: 1.925 [1.459, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.458, 10.331], loss: 1.837607, mae: 0.567987, mean_q: 4.944084
 85909/100000: episode: 1444, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 185.492, mean reward: 1.855 [1.467, 6.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.180, 10.131], loss: 7.680141, mae: 0.870450, mean_q: 5.201395
 86009/100000: episode: 1445, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 192.792, mean reward: 1.928 [1.443, 3.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.197, 10.165], loss: 1.913206, mae: 0.647673, mean_q: 5.040220
 86109/100000: episode: 1446, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 182.812, mean reward: 1.828 [1.446, 2.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.166, 10.098], loss: 2.101619, mae: 0.607036, mean_q: 5.041161
 86209/100000: episode: 1447, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 189.289, mean reward: 1.893 [1.460, 2.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.649, 10.098], loss: 3.307007, mae: 0.613326, mean_q: 5.012619
 86309/100000: episode: 1448, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 192.496, mean reward: 1.925 [1.495, 4.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.168, 10.098], loss: 5.637336, mae: 0.762518, mean_q: 5.077175
 86409/100000: episode: 1449, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 193.486, mean reward: 1.935 [1.450, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.029, 10.098], loss: 0.362001, mae: 0.478676, mean_q: 4.856866
 86509/100000: episode: 1450, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 191.668, mean reward: 1.917 [1.440, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.777, 10.332], loss: 4.547608, mae: 0.668355, mean_q: 4.822409
 86609/100000: episode: 1451, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 245.254, mean reward: 2.453 [1.445, 4.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.587, 10.098], loss: 1.902810, mae: 0.538467, mean_q: 4.721312
 86709/100000: episode: 1452, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 179.684, mean reward: 1.797 [1.475, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.934, 10.098], loss: 0.401607, mae: 0.455426, mean_q: 4.673190
 86809/100000: episode: 1453, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 190.456, mean reward: 1.905 [1.446, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.952, 10.275], loss: 1.610533, mae: 0.499712, mean_q: 4.602967
 86909/100000: episode: 1454, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 227.030, mean reward: 2.270 [1.584, 3.414], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.960, 10.098], loss: 5.447931, mae: 0.703569, mean_q: 4.706243
 87009/100000: episode: 1455, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.181, mean reward: 1.892 [1.478, 3.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.650, 10.098], loss: 0.514151, mae: 0.497950, mean_q: 4.489502
 87109/100000: episode: 1456, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 200.394, mean reward: 2.004 [1.488, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.077, 10.098], loss: 0.449475, mae: 0.445725, mean_q: 4.528842
 87209/100000: episode: 1457, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 183.253, mean reward: 1.833 [1.477, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.166, 10.098], loss: 0.317466, mae: 0.402950, mean_q: 4.428011
 87309/100000: episode: 1458, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 204.709, mean reward: 2.047 [1.450, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.771, 10.219], loss: 1.494937, mae: 0.423475, mean_q: 4.320633
 87409/100000: episode: 1459, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 206.278, mean reward: 2.063 [1.449, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.730, 10.098], loss: 0.275517, mae: 0.398801, mean_q: 4.292947
 87509/100000: episode: 1460, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 187.962, mean reward: 1.880 [1.449, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.582, 10.216], loss: 0.179717, mae: 0.368041, mean_q: 4.182970
 87609/100000: episode: 1461, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 185.378, mean reward: 1.854 [1.450, 2.671], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.745, 10.134], loss: 0.249759, mae: 0.352936, mean_q: 4.105812
 87709/100000: episode: 1462, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 182.340, mean reward: 1.823 [1.446, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.965, 10.196], loss: 0.169721, mae: 0.319173, mean_q: 3.986757
 87809/100000: episode: 1463, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 204.712, mean reward: 2.047 [1.446, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.380, 10.217], loss: 0.103565, mae: 0.307610, mean_q: 3.938729
 87909/100000: episode: 1464, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 194.208, mean reward: 1.942 [1.468, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.915, 10.098], loss: 0.080134, mae: 0.282838, mean_q: 3.891895
 88009/100000: episode: 1465, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 218.575, mean reward: 2.186 [1.449, 3.748], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.641, 10.098], loss: 0.088406, mae: 0.289107, mean_q: 3.910067
 88109/100000: episode: 1466, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 185.914, mean reward: 1.859 [1.438, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.110, 10.098], loss: 0.086182, mae: 0.284303, mean_q: 3.885534
 88209/100000: episode: 1467, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 185.280, mean reward: 1.853 [1.514, 2.934], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.570, 10.098], loss: 0.096673, mae: 0.294466, mean_q: 3.915949
 88309/100000: episode: 1468, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 187.638, mean reward: 1.876 [1.472, 3.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.938, 10.098], loss: 0.087256, mae: 0.288021, mean_q: 3.878570
 88409/100000: episode: 1469, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 201.366, mean reward: 2.014 [1.492, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.258, 10.098], loss: 0.081316, mae: 0.280677, mean_q: 3.861668
 88509/100000: episode: 1470, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 187.700, mean reward: 1.877 [1.449, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.345, 10.098], loss: 0.081874, mae: 0.276270, mean_q: 3.869365
 88609/100000: episode: 1471, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 180.929, mean reward: 1.809 [1.474, 3.138], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.309, 10.098], loss: 0.088136, mae: 0.281451, mean_q: 3.883728
 88709/100000: episode: 1472, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 213.224, mean reward: 2.132 [1.442, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.767, 10.302], loss: 0.080442, mae: 0.279880, mean_q: 3.882468
 88809/100000: episode: 1473, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 200.027, mean reward: 2.000 [1.487, 5.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.041, 10.409], loss: 0.085507, mae: 0.285025, mean_q: 3.894531
 88909/100000: episode: 1474, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 198.893, mean reward: 1.989 [1.484, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.644, 10.098], loss: 0.089766, mae: 0.285806, mean_q: 3.900311
 89009/100000: episode: 1475, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.837, mean reward: 1.878 [1.477, 2.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.863, 10.098], loss: 0.085376, mae: 0.289157, mean_q: 3.898454
 89109/100000: episode: 1476, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.620, mean reward: 1.926 [1.468, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.607, 10.098], loss: 0.073884, mae: 0.273309, mean_q: 3.871565
 89209/100000: episode: 1477, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 193.587, mean reward: 1.936 [1.454, 4.399], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.802, 10.098], loss: 0.093807, mae: 0.284612, mean_q: 3.874620
 89309/100000: episode: 1478, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 206.626, mean reward: 2.066 [1.458, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.498, 10.098], loss: 0.090696, mae: 0.283769, mean_q: 3.884141
 89409/100000: episode: 1479, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 215.285, mean reward: 2.153 [1.486, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.758, 10.518], loss: 0.091802, mae: 0.290920, mean_q: 3.884673
 89509/100000: episode: 1480, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 184.382, mean reward: 1.844 [1.499, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.827, 10.098], loss: 0.090763, mae: 0.288933, mean_q: 3.886797
 89609/100000: episode: 1481, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 200.117, mean reward: 2.001 [1.444, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.263, 10.183], loss: 0.080952, mae: 0.286712, mean_q: 3.900458
 89709/100000: episode: 1482, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 213.937, mean reward: 2.139 [1.477, 4.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.087, 10.340], loss: 0.077913, mae: 0.275821, mean_q: 3.885767
 89809/100000: episode: 1483, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 184.850, mean reward: 1.849 [1.437, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.093, 10.240], loss: 0.076794, mae: 0.274909, mean_q: 3.873817
 89909/100000: episode: 1484, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 183.581, mean reward: 1.836 [1.445, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.677, 10.160], loss: 0.085363, mae: 0.282861, mean_q: 3.881308
 90009/100000: episode: 1485, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 191.547, mean reward: 1.915 [1.460, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.610, 10.098], loss: 0.083334, mae: 0.284654, mean_q: 3.882683
 90109/100000: episode: 1486, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 229.139, mean reward: 2.291 [1.464, 6.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.760, 10.098], loss: 0.081018, mae: 0.281930, mean_q: 3.894023
 90209/100000: episode: 1487, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.857, mean reward: 1.939 [1.439, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.137, 10.258], loss: 0.091966, mae: 0.288330, mean_q: 3.897756
 90309/100000: episode: 1488, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 181.418, mean reward: 1.814 [1.449, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.324, 10.098], loss: 0.082134, mae: 0.280888, mean_q: 3.900081
 90409/100000: episode: 1489, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 212.528, mean reward: 2.125 [1.497, 4.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.626, 10.207], loss: 0.090700, mae: 0.292712, mean_q: 3.939246
 90509/100000: episode: 1490, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 201.255, mean reward: 2.013 [1.459, 3.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.045, 10.313], loss: 0.103258, mae: 0.297503, mean_q: 3.917957
 90609/100000: episode: 1491, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 196.947, mean reward: 1.969 [1.465, 3.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.332, 10.098], loss: 0.077729, mae: 0.280399, mean_q: 3.904570
 90709/100000: episode: 1492, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 201.190, mean reward: 2.012 [1.456, 4.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.714, 10.338], loss: 0.074903, mae: 0.273518, mean_q: 3.895959
 90809/100000: episode: 1493, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 189.179, mean reward: 1.892 [1.481, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.939, 10.243], loss: 0.081917, mae: 0.284639, mean_q: 3.892538
 90909/100000: episode: 1494, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 188.248, mean reward: 1.882 [1.453, 3.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.846, 10.150], loss: 0.094331, mae: 0.291127, mean_q: 3.882786
 91009/100000: episode: 1495, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 186.861, mean reward: 1.869 [1.499, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.771, 10.146], loss: 0.073002, mae: 0.273766, mean_q: 3.874883
 91109/100000: episode: 1496, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 198.778, mean reward: 1.988 [1.459, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.393, 10.268], loss: 0.083000, mae: 0.280081, mean_q: 3.888886
 91209/100000: episode: 1497, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 189.025, mean reward: 1.890 [1.462, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.287, 10.324], loss: 0.084224, mae: 0.286854, mean_q: 3.885585
 91309/100000: episode: 1498, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 193.887, mean reward: 1.939 [1.493, 3.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.139, 10.311], loss: 0.090971, mae: 0.296937, mean_q: 3.906839
 91409/100000: episode: 1499, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 194.942, mean reward: 1.949 [1.463, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.294, 10.317], loss: 0.077841, mae: 0.281479, mean_q: 3.899441
 91509/100000: episode: 1500, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 197.780, mean reward: 1.978 [1.444, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.602, 10.284], loss: 0.078726, mae: 0.283845, mean_q: 3.890852
 91609/100000: episode: 1501, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 193.152, mean reward: 1.932 [1.445, 3.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.955, 10.234], loss: 0.083162, mae: 0.289216, mean_q: 3.893109
 91709/100000: episode: 1502, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 189.794, mean reward: 1.898 [1.439, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.717, 10.098], loss: 0.074500, mae: 0.278243, mean_q: 3.878459
 91809/100000: episode: 1503, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 212.459, mean reward: 2.125 [1.453, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.032, 10.455], loss: 0.090561, mae: 0.299786, mean_q: 3.908642
 91909/100000: episode: 1504, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 188.028, mean reward: 1.880 [1.447, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.134, 10.281], loss: 0.087188, mae: 0.295796, mean_q: 3.890176
 92009/100000: episode: 1505, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 187.918, mean reward: 1.879 [1.463, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.319, 10.188], loss: 0.089505, mae: 0.295559, mean_q: 3.904614
 92109/100000: episode: 1506, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 212.882, mean reward: 2.129 [1.433, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.105, 10.098], loss: 0.082803, mae: 0.290291, mean_q: 3.883250
 92209/100000: episode: 1507, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 232.593, mean reward: 2.326 [1.488, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.414 [-0.611, 10.284], loss: 0.085676, mae: 0.293597, mean_q: 3.905358
 92309/100000: episode: 1508, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 179.806, mean reward: 1.798 [1.447, 2.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.118, 10.254], loss: 0.084639, mae: 0.286313, mean_q: 3.899970
 92409/100000: episode: 1509, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 206.773, mean reward: 2.068 [1.474, 4.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.943, 10.098], loss: 0.089859, mae: 0.297328, mean_q: 3.889241
 92509/100000: episode: 1510, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 194.083, mean reward: 1.941 [1.480, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-2.215, 10.160], loss: 0.079047, mae: 0.286171, mean_q: 3.902409
 92609/100000: episode: 1511, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 203.333, mean reward: 2.033 [1.461, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.146, 10.240], loss: 0.084786, mae: 0.291466, mean_q: 3.898759
 92709/100000: episode: 1512, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.538, mean reward: 1.935 [1.485, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.524, 10.210], loss: 0.083551, mae: 0.292066, mean_q: 3.911435
[Info] 1-TH LEVEL FOUND: 5.933531761169434, Considering 10/90 traces
 92809/100000: episode: 1513, duration: 4.836s, episode steps: 100, steps per second: 21, episode reward: 199.935, mean reward: 1.999 [1.472, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.471, 10.178], loss: 0.081808, mae: 0.286886, mean_q: 3.911428
 92828/100000: episode: 1514, duration: 0.113s, episode steps: 19, steps per second: 168, episode reward: 45.966, mean reward: 2.419 [1.954, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.399, 10.100], loss: 0.095358, mae: 0.304479, mean_q: 3.946642
 92851/100000: episode: 1515, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 50.591, mean reward: 2.200 [1.615, 3.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.035, 10.100], loss: 0.086219, mae: 0.286751, mean_q: 3.890718
 92869/100000: episode: 1516, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 39.272, mean reward: 2.182 [1.475, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.651, 10.133], loss: 0.069838, mae: 0.283058, mean_q: 3.928894
 92900/100000: episode: 1517, duration: 0.176s, episode steps: 31, steps per second: 177, episode reward: 115.922, mean reward: 3.739 [2.148, 5.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.464, 10.100], loss: 0.085335, mae: 0.290917, mean_q: 3.901469
 92910/100000: episode: 1518, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 25.576, mean reward: 2.558 [2.289, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.324, 10.100], loss: 0.081808, mae: 0.292626, mean_q: 3.918845
 92941/100000: episode: 1519, duration: 0.175s, episode steps: 31, steps per second: 177, episode reward: 88.951, mean reward: 2.869 [1.874, 4.345], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.195, 10.100], loss: 0.110410, mae: 0.320642, mean_q: 3.969473
 92960/100000: episode: 1520, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 50.533, mean reward: 2.660 [2.174, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.392, 10.100], loss: 0.112984, mae: 0.353376, mean_q: 3.991212
 92970/100000: episode: 1521, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 29.050, mean reward: 2.905 [2.566, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.827, 10.100], loss: 0.111536, mae: 0.317319, mean_q: 3.892611
 93013/100000: episode: 1522, duration: 0.234s, episode steps: 43, steps per second: 184, episode reward: 116.444, mean reward: 2.708 [1.517, 5.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.391, 10.201], loss: 0.098450, mae: 0.302133, mean_q: 3.918339
 93056/100000: episode: 1523, duration: 0.238s, episode steps: 43, steps per second: 181, episode reward: 100.777, mean reward: 2.344 [1.504, 4.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.589, 10.116], loss: 0.097453, mae: 0.314011, mean_q: 3.928571
 93079/100000: episode: 1524, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 96.126, mean reward: 4.179 [2.848, 11.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.396, 10.100], loss: 0.101481, mae: 0.318330, mean_q: 3.982701
 93102/100000: episode: 1525, duration: 0.133s, episode steps: 23, steps per second: 172, episode reward: 67.156, mean reward: 2.920 [2.211, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.401, 10.100], loss: 0.114152, mae: 0.318283, mean_q: 3.964893
 93133/100000: episode: 1526, duration: 0.183s, episode steps: 31, steps per second: 170, episode reward: 83.667, mean reward: 2.699 [2.127, 4.003], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.228, 10.100], loss: 0.156617, mae: 0.352224, mean_q: 3.959132
 93176/100000: episode: 1527, duration: 0.217s, episode steps: 43, steps per second: 198, episode reward: 124.955, mean reward: 2.906 [2.017, 6.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.541, 10.100], loss: 0.125573, mae: 0.324280, mean_q: 4.011075
 93194/100000: episode: 1528, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 58.701, mean reward: 3.261 [2.498, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.744, 10.425], loss: 0.112672, mae: 0.325078, mean_q: 3.990848
 93225/100000: episode: 1529, duration: 0.168s, episode steps: 31, steps per second: 184, episode reward: 82.551, mean reward: 2.663 [2.086, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-1.008, 10.100], loss: 0.106095, mae: 0.322394, mean_q: 3.965352
 93243/100000: episode: 1530, duration: 0.110s, episode steps: 18, steps per second: 164, episode reward: 49.153, mean reward: 2.731 [2.111, 5.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.331], loss: 0.092056, mae: 0.319499, mean_q: 4.004869
 93286/100000: episode: 1531, duration: 0.237s, episode steps: 43, steps per second: 182, episode reward: 164.899, mean reward: 3.835 [2.719, 6.674], mean action: 0.000 [0.000, 0.000], mean observation: 1.934 [-1.225, 10.100], loss: 0.101222, mae: 0.321432, mean_q: 4.055027
 93304/100000: episode: 1532, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 49.428, mean reward: 2.746 [1.902, 4.756], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.352], loss: 0.119059, mae: 0.328856, mean_q: 4.070976
 93347/100000: episode: 1533, duration: 0.219s, episode steps: 43, steps per second: 196, episode reward: 113.265, mean reward: 2.634 [1.744, 7.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.916, 10.100], loss: 0.083844, mae: 0.291703, mean_q: 4.023293
 93357/100000: episode: 1534, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 28.781, mean reward: 2.878 [2.284, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.125, 10.100], loss: 0.261952, mae: 0.422607, mean_q: 4.165706
 93380/100000: episode: 1535, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 62.226, mean reward: 2.705 [1.776, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.461, 10.100], loss: 0.115113, mae: 0.342048, mean_q: 4.071402
 93388/100000: episode: 1536, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 21.851, mean reward: 2.731 [2.197, 3.173], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.035, 10.415], loss: 0.099367, mae: 0.314042, mean_q: 3.998138
 93406/100000: episode: 1537, duration: 0.105s, episode steps: 18, steps per second: 172, episode reward: 34.981, mean reward: 1.943 [1.488, 2.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.168], loss: 0.098475, mae: 0.312061, mean_q: 4.075771
 93449/100000: episode: 1538, duration: 0.255s, episode steps: 43, steps per second: 169, episode reward: 112.235, mean reward: 2.610 [1.857, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.952 [-0.401, 10.100], loss: 0.106249, mae: 0.328353, mean_q: 4.091596
 93467/100000: episode: 1539, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 41.593, mean reward: 2.311 [1.790, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.284], loss: 0.165053, mae: 0.326980, mean_q: 4.103829
 93475/100000: episode: 1540, duration: 0.045s, episode steps: 8, steps per second: 180, episode reward: 21.091, mean reward: 2.636 [2.193, 3.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.424], loss: 0.118383, mae: 0.317739, mean_q: 4.112747
 93494/100000: episode: 1541, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 61.902, mean reward: 3.258 [2.340, 4.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.324, 10.100], loss: 0.114596, mae: 0.334608, mean_q: 4.106710
 93502/100000: episode: 1542, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 34.782, mean reward: 4.348 [3.105, 6.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.566], loss: 0.093771, mae: 0.327362, mean_q: 4.163497
 93510/100000: episode: 1543, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 50.664, mean reward: 6.333 [2.510, 29.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-1.059, 10.433], loss: 0.097036, mae: 0.331125, mean_q: 4.064095
 93541/100000: episode: 1544, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 105.255, mean reward: 3.395 [2.521, 5.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.263, 10.100], loss: 0.132010, mae: 0.333668, mean_q: 4.149158
 93564/100000: episode: 1545, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 74.439, mean reward: 3.236 [2.064, 4.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.381, 10.100], loss: 0.111607, mae: 0.331678, mean_q: 4.107810
 93572/100000: episode: 1546, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 25.018, mean reward: 3.127 [2.598, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.289, 10.418], loss: 0.081266, mae: 0.290063, mean_q: 4.054747
 93615/100000: episode: 1547, duration: 0.233s, episode steps: 43, steps per second: 185, episode reward: 177.557, mean reward: 4.129 [1.949, 38.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-0.710, 10.100], loss: 0.607095, mae: 0.394740, mean_q: 4.176907
 93623/100000: episode: 1548, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 28.960, mean reward: 3.620 [3.273, 4.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.042, 10.496], loss: 0.169682, mae: 0.439530, mean_q: 4.342754
 93641/100000: episode: 1549, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 49.983, mean reward: 2.777 [2.033, 3.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.103, 10.381], loss: 0.130754, mae: 0.346443, mean_q: 4.144674
 93672/100000: episode: 1550, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 88.226, mean reward: 2.846 [1.792, 16.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.617, 10.100], loss: 0.169115, mae: 0.360011, mean_q: 4.169069
 93715/100000: episode: 1551, duration: 0.237s, episode steps: 43, steps per second: 181, episode reward: 124.077, mean reward: 2.886 [2.066, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.288, 10.100], loss: 0.576932, mae: 0.376874, mean_q: 4.170762
 93728/100000: episode: 1552, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 37.836, mean reward: 2.910 [2.602, 3.251], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.442, 10.100], loss: 0.863817, mae: 0.393623, mean_q: 4.164725
 93741/100000: episode: 1553, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 41.533, mean reward: 3.195 [2.541, 5.860], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.607, 10.100], loss: 0.188255, mae: 0.438189, mean_q: 4.184613
 93795/100000: episode: 1554, duration: 0.283s, episode steps: 54, steps per second: 191, episode reward: 127.052, mean reward: 2.353 [1.766, 5.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-1.220, 10.100], loss: 0.515486, mae: 0.422609, mean_q: 4.253160
 93803/100000: episode: 1555, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 25.637, mean reward: 3.205 [2.914, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.050, 10.491], loss: 0.152923, mae: 0.416998, mean_q: 4.375481
 93834/100000: episode: 1556, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 122.390, mean reward: 3.948 [2.589, 6.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.164, 10.100], loss: 0.267650, mae: 0.410137, mean_q: 4.247689
 93877/100000: episode: 1557, duration: 0.234s, episode steps: 43, steps per second: 183, episode reward: 101.860, mean reward: 2.369 [1.581, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.536, 10.100], loss: 0.212648, mae: 0.443876, mean_q: 4.261423
 93890/100000: episode: 1558, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 31.114, mean reward: 2.393 [2.024, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.633, 10.100], loss: 1.658435, mae: 0.501090, mean_q: 4.408985
 93903/100000: episode: 1559, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 41.053, mean reward: 3.158 [2.587, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.479, 10.100], loss: 0.178277, mae: 0.417090, mean_q: 4.350406
 93911/100000: episode: 1560, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 26.239, mean reward: 3.280 [2.790, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.440], loss: 0.190555, mae: 0.403708, mean_q: 4.376392
 93965/100000: episode: 1561, duration: 0.296s, episode steps: 54, steps per second: 183, episode reward: 104.340, mean reward: 1.932 [1.466, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.659, 10.141], loss: 0.424148, mae: 0.415074, mean_q: 4.279030
 93996/100000: episode: 1562, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 100.809, mean reward: 3.252 [2.087, 7.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.377, 10.100], loss: 0.159970, mae: 0.389713, mean_q: 4.279188
 94039/100000: episode: 1563, duration: 0.237s, episode steps: 43, steps per second: 182, episode reward: 109.553, mean reward: 2.548 [1.583, 4.717], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.312, 10.100], loss: 0.161431, mae: 0.376920, mean_q: 4.308391
 94052/100000: episode: 1564, duration: 0.069s, episode steps: 13, steps per second: 188, episode reward: 39.975, mean reward: 3.075 [2.475, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.399, 10.100], loss: 1.631367, mae: 0.475077, mean_q: 4.417345
 94062/100000: episode: 1565, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 26.766, mean reward: 2.677 [2.355, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.380, 10.100], loss: 0.149329, mae: 0.383039, mean_q: 4.144423
 94085/100000: episode: 1566, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 70.392, mean reward: 3.061 [2.311, 4.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.202, 10.100], loss: 0.623618, mae: 0.453469, mean_q: 4.348712
 94095/100000: episode: 1567, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 29.332, mean reward: 2.933 [2.568, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.804, 10.100], loss: 0.192751, mae: 0.419190, mean_q: 4.326150
 94114/100000: episode: 1568, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 59.842, mean reward: 3.150 [2.524, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.312, 10.100], loss: 0.153094, mae: 0.373770, mean_q: 4.314838
 94168/100000: episode: 1569, duration: 0.283s, episode steps: 54, steps per second: 191, episode reward: 109.510, mean reward: 2.028 [1.632, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.868 [-1.464, 10.100], loss: 0.556526, mae: 0.419034, mean_q: 4.373013
 94178/100000: episode: 1570, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 27.654, mean reward: 2.765 [2.327, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.335, 10.100], loss: 2.038641, mae: 0.561102, mean_q: 4.530673
 94188/100000: episode: 1571, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 29.693, mean reward: 2.969 [2.596, 3.219], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.169, 10.100], loss: 0.475810, mae: 0.491950, mean_q: 4.356695
 94211/100000: episode: 1572, duration: 0.127s, episode steps: 23, steps per second: 180, episode reward: 70.858, mean reward: 3.081 [1.835, 4.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.779, 10.100], loss: 0.192017, mae: 0.429174, mean_q: 4.371829
 94254/100000: episode: 1573, duration: 0.244s, episode steps: 43, steps per second: 176, episode reward: 101.544, mean reward: 2.361 [1.737, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.458, 10.100], loss: 0.196572, mae: 0.380419, mean_q: 4.389871
 94277/100000: episode: 1574, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 74.877, mean reward: 3.256 [2.218, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.342, 10.100], loss: 0.125954, mae: 0.343846, mean_q: 4.380627
 94296/100000: episode: 1575, duration: 0.103s, episode steps: 19, steps per second: 185, episode reward: 59.584, mean reward: 3.136 [2.305, 4.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.399, 10.100], loss: 0.119337, mae: 0.348030, mean_q: 4.382966
 94319/100000: episode: 1576, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 67.569, mean reward: 2.938 [1.950, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.231, 10.100], loss: 0.136616, mae: 0.354916, mean_q: 4.314211
 94373/100000: episode: 1577, duration: 0.306s, episode steps: 54, steps per second: 177, episode reward: 133.747, mean reward: 2.477 [1.880, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.858 [-0.623, 10.100], loss: 0.171669, mae: 0.357004, mean_q: 4.401832
 94381/100000: episode: 1578, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 30.540, mean reward: 3.817 [2.536, 6.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.505, 10.421], loss: 0.083797, mae: 0.294694, mean_q: 4.245541
 94412/100000: episode: 1579, duration: 0.178s, episode steps: 31, steps per second: 174, episode reward: 88.798, mean reward: 2.864 [2.351, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.261, 10.100], loss: 0.133048, mae: 0.368697, mean_q: 4.438322
 94422/100000: episode: 1580, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 26.202, mean reward: 2.620 [2.092, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.236, 10.100], loss: 0.103816, mae: 0.319475, mean_q: 4.266560
 94453/100000: episode: 1581, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 83.442, mean reward: 2.692 [2.022, 3.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.513, 10.100], loss: 0.339153, mae: 0.416097, mean_q: 4.426982
 94463/100000: episode: 1582, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 29.441, mean reward: 2.944 [2.350, 5.237], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.119, 10.100], loss: 0.114455, mae: 0.340172, mean_q: 4.408735
 94517/100000: episode: 1583, duration: 0.288s, episode steps: 54, steps per second: 187, episode reward: 112.580, mean reward: 2.085 [1.517, 3.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.178, 10.176], loss: 0.340676, mae: 0.413939, mean_q: 4.374867
 94530/100000: episode: 1584, duration: 0.072s, episode steps: 13, steps per second: 179, episode reward: 38.296, mean reward: 2.946 [2.370, 3.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.128, 10.100], loss: 0.139631, mae: 0.369645, mean_q: 4.388661
 94538/100000: episode: 1585, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 34.867, mean reward: 4.358 [3.267, 5.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.463], loss: 0.124098, mae: 0.374288, mean_q: 4.474271
 94548/100000: episode: 1586, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 25.843, mean reward: 2.584 [2.264, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.194, 10.100], loss: 0.159052, mae: 0.368579, mean_q: 4.358387
 94556/100000: episode: 1587, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 18.895, mean reward: 2.362 [1.953, 3.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.106, 10.387], loss: 0.112736, mae: 0.336536, mean_q: 4.437788
 94599/100000: episode: 1588, duration: 0.226s, episode steps: 43, steps per second: 190, episode reward: 102.859, mean reward: 2.392 [1.656, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.659, 10.259], loss: 0.136327, mae: 0.369900, mean_q: 4.406776
 94617/100000: episode: 1589, duration: 0.097s, episode steps: 18, steps per second: 187, episode reward: 55.404, mean reward: 3.078 [2.275, 5.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.576, 10.402], loss: 0.672144, mae: 0.425598, mean_q: 4.459462
 94640/100000: episode: 1590, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 65.832, mean reward: 2.862 [2.173, 4.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.004, 10.100], loss: 0.141676, mae: 0.379909, mean_q: 4.468166
 94671/100000: episode: 1591, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 104.942, mean reward: 3.385 [2.202, 5.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-1.195, 10.100], loss: 0.139402, mae: 0.371675, mean_q: 4.357696
 94725/100000: episode: 1592, duration: 0.296s, episode steps: 54, steps per second: 182, episode reward: 117.497, mean reward: 2.176 [1.523, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.824, 10.100], loss: 0.137771, mae: 0.381235, mean_q: 4.457536
 94735/100000: episode: 1593, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 25.337, mean reward: 2.534 [2.161, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.278, 10.100], loss: 0.120739, mae: 0.349365, mean_q: 4.454907
 94754/100000: episode: 1594, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 50.717, mean reward: 2.669 [2.301, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.319, 10.100], loss: 0.132655, mae: 0.371662, mean_q: 4.451381
 94764/100000: episode: 1595, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 28.201, mean reward: 2.820 [2.391, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.427, 10.100], loss: 0.168699, mae: 0.375423, mean_q: 4.358079
 94807/100000: episode: 1596, duration: 0.247s, episode steps: 43, steps per second: 174, episode reward: 214.996, mean reward: 5.000 [2.812, 13.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.340, 10.100], loss: 0.175355, mae: 0.381066, mean_q: 4.512313
 94850/100000: episode: 1597, duration: 0.240s, episode steps: 43, steps per second: 179, episode reward: 137.990, mean reward: 3.209 [2.241, 5.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.219, 10.100], loss: 0.487478, mae: 0.457355, mean_q: 4.505290
 94868/100000: episode: 1598, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 58.343, mean reward: 3.241 [2.445, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.195, 10.382], loss: 1.212253, mae: 0.449355, mean_q: 4.606134
 94878/100000: episode: 1599, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 27.791, mean reward: 2.779 [2.324, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.287, 10.100], loss: 0.152984, mae: 0.403179, mean_q: 4.535980
 94897/100000: episode: 1600, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 64.608, mean reward: 3.400 [2.717, 5.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.965, 10.100], loss: 1.128950, mae: 0.453363, mean_q: 4.527195
 94907/100000: episode: 1601, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 28.894, mean reward: 2.889 [2.520, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.141, 10.100], loss: 0.175048, mae: 0.411504, mean_q: 4.552896
 94961/100000: episode: 1602, duration: 0.294s, episode steps: 54, steps per second: 184, episode reward: 127.108, mean reward: 2.354 [1.680, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.988, 10.100], loss: 1.082671, mae: 0.497610, mean_q: 4.605107
[Info] 2-TH LEVEL FOUND: 8.278634071350098, Considering 10/90 traces
 94979/100000: episode: 1603, duration: 4.341s, episode steps: 18, steps per second: 4, episode reward: 53.534, mean reward: 2.974 [2.130, 4.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.392], loss: 0.175629, mae: 0.445967, mean_q: 4.681387
 95006/100000: episode: 1604, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 111.618, mean reward: 4.134 [2.766, 9.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.374, 10.100], loss: 0.207532, mae: 0.425072, mean_q: 4.639953
 95024/100000: episode: 1605, duration: 0.098s, episode steps: 18, steps per second: 183, episode reward: 96.098, mean reward: 5.339 [4.143, 8.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.342, 10.100], loss: 0.303122, mae: 0.441138, mean_q: 4.525876
 95051/100000: episode: 1606, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 105.866, mean reward: 3.921 [1.818, 8.829], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.067, 10.100], loss: 0.264699, mae: 0.438967, mean_q: 4.628164
 95069/100000: episode: 1607, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 74.914, mean reward: 4.162 [2.671, 5.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-1.152, 10.100], loss: 0.193060, mae: 0.428781, mean_q: 4.609030
 95084/100000: episode: 1608, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 52.402, mean reward: 3.493 [2.764, 4.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.440, 10.100], loss: 0.222784, mae: 0.404363, mean_q: 4.576610
 95125/100000: episode: 1609, duration: 0.220s, episode steps: 41, steps per second: 187, episode reward: 102.847, mean reward: 2.508 [1.464, 5.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.177, 10.100], loss: 0.234020, mae: 0.413167, mean_q: 4.663568
 95148/100000: episode: 1610, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 196.756, mean reward: 8.555 [2.925, 29.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-1.327, 10.100], loss: 0.300599, mae: 0.458920, mean_q: 4.599209
 95190/100000: episode: 1611, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 109.369, mean reward: 2.604 [1.513, 4.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-0.713, 10.100], loss: 0.471801, mae: 0.471245, mean_q: 4.738482
 95231/100000: episode: 1612, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 105.657, mean reward: 2.577 [1.702, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-1.722, 10.100], loss: 0.467544, mae: 0.458558, mean_q: 4.736502
 95273/100000: episode: 1613, duration: 0.237s, episode steps: 42, steps per second: 177, episode reward: 119.775, mean reward: 2.852 [1.892, 4.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.278, 10.100], loss: 0.481890, mae: 0.509557, mean_q: 4.788762
 95296/100000: episode: 1614, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 177.711, mean reward: 7.727 [3.201, 22.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.247, 10.100], loss: 0.227969, mae: 0.428279, mean_q: 4.739010
 95311/100000: episode: 1615, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 37.356, mean reward: 2.490 [2.084, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.318, 10.100], loss: 0.316886, mae: 0.450438, mean_q: 4.573449
 95326/100000: episode: 1616, duration: 0.093s, episode steps: 15, steps per second: 161, episode reward: 54.222, mean reward: 3.615 [2.525, 5.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.335, 10.100], loss: 0.781918, mae: 0.520917, mean_q: 4.821284
 95367/100000: episode: 1617, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 134.890, mean reward: 3.290 [1.855, 6.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.534, 10.100], loss: 0.296482, mae: 0.458127, mean_q: 4.829823
 95382/100000: episode: 1618, duration: 0.089s, episode steps: 15, steps per second: 168, episode reward: 57.809, mean reward: 3.854 [2.962, 5.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.415, 10.100], loss: 0.423953, mae: 0.470303, mean_q: 4.897411
 95400/100000: episode: 1619, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 82.807, mean reward: 4.600 [2.271, 11.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.503, 10.100], loss: 0.816054, mae: 0.537703, mean_q: 4.825178
 95415/100000: episode: 1620, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 51.583, mean reward: 3.439 [2.398, 4.639], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.158, 10.100], loss: 0.297839, mae: 0.471293, mean_q: 4.801236
 95430/100000: episode: 1621, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 56.500, mean reward: 3.767 [3.208, 4.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.223, 10.100], loss: 0.240576, mae: 0.444645, mean_q: 4.737094
 95453/100000: episode: 1622, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 100.709, mean reward: 4.379 [2.442, 11.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-2.047, 10.100], loss: 0.388103, mae: 0.482579, mean_q: 4.943031
 95468/100000: episode: 1623, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 57.268, mean reward: 3.818 [3.067, 4.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.425, 10.100], loss: 0.157437, mae: 0.412138, mean_q: 4.868764
 95483/100000: episode: 1624, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 50.850, mean reward: 3.390 [2.569, 5.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.257, 10.100], loss: 0.493448, mae: 0.547513, mean_q: 4.938893
 95498/100000: episode: 1625, duration: 0.085s, episode steps: 15, steps per second: 177, episode reward: 43.627, mean reward: 2.908 [2.416, 3.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.710, 10.100], loss: 0.846751, mae: 0.544105, mean_q: 4.924632
 95539/100000: episode: 1626, duration: 0.225s, episode steps: 41, steps per second: 183, episode reward: 112.354, mean reward: 2.740 [1.591, 9.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.964 [-2.008, 10.100], loss: 0.323888, mae: 0.470735, mean_q: 4.944390
 95562/100000: episode: 1627, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 237.739, mean reward: 10.336 [3.694, 30.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.448, 10.100], loss: 0.509590, mae: 0.519499, mean_q: 4.925832
 95603/100000: episode: 1628, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 117.123, mean reward: 2.857 [1.978, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.346, 10.100], loss: 0.954605, mae: 0.618271, mean_q: 5.081995
 95644/100000: episode: 1629, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 113.617, mean reward: 2.771 [1.457, 6.316], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.035, 10.100], loss: 0.488056, mae: 0.592249, mean_q: 5.026997
 95686/100000: episode: 1630, duration: 0.228s, episode steps: 42, steps per second: 184, episode reward: 196.637, mean reward: 4.682 [2.797, 8.446], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.556, 10.100], loss: 0.680195, mae: 0.571423, mean_q: 5.022728
 95727/100000: episode: 1631, duration: 0.249s, episode steps: 41, steps per second: 165, episode reward: 188.765, mean reward: 4.604 [1.742, 18.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.600, 10.100], loss: 0.969302, mae: 0.634191, mean_q: 5.105536
 95748/100000: episode: 1632, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 142.917, mean reward: 6.806 [3.835, 19.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.086, 10.100], loss: 0.621325, mae: 0.567820, mean_q: 5.043204
 95763/100000: episode: 1633, duration: 0.097s, episode steps: 15, steps per second: 155, episode reward: 64.756, mean reward: 4.317 [2.920, 7.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.546, 10.100], loss: 1.053925, mae: 0.633932, mean_q: 5.190347
 95778/100000: episode: 1634, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 85.956, mean reward: 5.730 [2.949, 16.741], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.700, 10.100], loss: 1.185939, mae: 0.700531, mean_q: 5.414526
 95819/100000: episode: 1635, duration: 0.230s, episode steps: 41, steps per second: 178, episode reward: 90.102, mean reward: 2.198 [1.510, 5.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.647, 10.100], loss: 1.053755, mae: 0.677020, mean_q: 5.235835
 95842/100000: episode: 1636, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 73.431, mean reward: 3.193 [1.638, 5.952], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.153, 10.100], loss: 1.565617, mae: 0.728324, mean_q: 5.308022
 95883/100000: episode: 1637, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 112.712, mean reward: 2.749 [1.929, 6.728], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-1.919, 10.100], loss: 0.787170, mae: 0.613219, mean_q: 5.246992
 95924/100000: episode: 1638, duration: 0.213s, episode steps: 41, steps per second: 192, episode reward: 108.035, mean reward: 2.635 [1.645, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.226, 10.100], loss: 0.950965, mae: 0.638903, mean_q: 5.205403
 95939/100000: episode: 1639, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 51.232, mean reward: 3.415 [2.324, 4.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.283, 10.100], loss: 0.552726, mae: 0.568967, mean_q: 5.181267
 95980/100000: episode: 1640, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 124.132, mean reward: 3.028 [1.934, 5.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.963 [-1.433, 10.100], loss: 1.008096, mae: 0.602722, mean_q: 5.269493
 96007/100000: episode: 1641, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 160.365, mean reward: 5.939 [2.882, 10.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.799, 10.100], loss: 0.423180, mae: 0.558483, mean_q: 5.286060
 96049/100000: episode: 1642, duration: 0.226s, episode steps: 42, steps per second: 186, episode reward: 144.775, mean reward: 3.447 [2.198, 8.361], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.559, 10.100], loss: 1.100687, mae: 0.622958, mean_q: 5.325598
 96064/100000: episode: 1643, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 43.752, mean reward: 2.917 [2.470, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.004, 10.100], loss: 0.558642, mae: 0.643840, mean_q: 5.199688
 96079/100000: episode: 1644, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 49.063, mean reward: 3.271 [2.636, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.461, 10.100], loss: 1.066648, mae: 0.668055, mean_q: 5.412317
 96106/100000: episode: 1645, duration: 0.156s, episode steps: 27, steps per second: 174, episode reward: 118.205, mean reward: 4.378 [2.531, 8.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.279, 10.100], loss: 1.022473, mae: 0.635923, mean_q: 5.300842
 96147/100000: episode: 1646, duration: 0.223s, episode steps: 41, steps per second: 184, episode reward: 164.239, mean reward: 4.006 [1.811, 15.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.221, 10.100], loss: 0.778750, mae: 0.612657, mean_q: 5.418050
 96170/100000: episode: 1647, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 118.813, mean reward: 5.166 [2.909, 8.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.356, 10.100], loss: 0.865814, mae: 0.658871, mean_q: 5.424921
 96212/100000: episode: 1648, duration: 0.235s, episode steps: 42, steps per second: 179, episode reward: 112.918, mean reward: 2.689 [1.673, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.291, 10.100], loss: 1.230449, mae: 0.679597, mean_q: 5.361796
 96227/100000: episode: 1649, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 47.830, mean reward: 3.189 [1.809, 5.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.100], loss: 1.513733, mae: 0.716065, mean_q: 5.491407
 96250/100000: episode: 1650, duration: 0.130s, episode steps: 23, steps per second: 178, episode reward: 100.806, mean reward: 4.383 [2.988, 8.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.649, 10.100], loss: 1.471827, mae: 0.678266, mean_q: 5.489802
 96265/100000: episode: 1651, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 41.559, mean reward: 2.771 [2.351, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.460, 10.100], loss: 0.592400, mae: 0.570684, mean_q: 5.360209
 96288/100000: episode: 1652, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 133.740, mean reward: 5.815 [3.280, 10.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.367, 10.100], loss: 0.975834, mae: 0.624866, mean_q: 5.413867
 96329/100000: episode: 1653, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 101.515, mean reward: 2.476 [1.479, 4.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.991 [-0.056, 10.301], loss: 1.009305, mae: 0.705079, mean_q: 5.429561
 96344/100000: episode: 1654, duration: 0.095s, episode steps: 15, steps per second: 157, episode reward: 59.793, mean reward: 3.986 [2.666, 5.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.220, 10.100], loss: 1.075776, mae: 0.699898, mean_q: 5.619687
 96385/100000: episode: 1655, duration: 0.227s, episode steps: 41, steps per second: 180, episode reward: 138.600, mean reward: 3.380 [2.185, 5.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.145, 10.100], loss: 0.851874, mae: 0.666006, mean_q: 5.495295
 96406/100000: episode: 1656, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 90.662, mean reward: 4.317 [3.248, 6.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.610, 10.100], loss: 1.253178, mae: 0.709168, mean_q: 5.627262
 96424/100000: episode: 1657, duration: 0.099s, episode steps: 18, steps per second: 182, episode reward: 61.971, mean reward: 3.443 [2.152, 6.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.582, 10.100], loss: 0.978140, mae: 0.755005, mean_q: 5.631286
 96465/100000: episode: 1658, duration: 0.223s, episode steps: 41, steps per second: 184, episode reward: 148.871, mean reward: 3.631 [2.024, 6.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-1.238, 10.100], loss: 0.416575, mae: 0.588396, mean_q: 5.562122
 96506/100000: episode: 1659, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 109.674, mean reward: 2.675 [1.933, 3.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-1.492, 10.100], loss: 1.555538, mae: 0.727970, mean_q: 5.651399
 96524/100000: episode: 1660, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 78.425, mean reward: 4.357 [2.573, 6.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.346, 10.100], loss: 0.409322, mae: 0.590669, mean_q: 5.690102
 96547/100000: episode: 1661, duration: 0.138s, episode steps: 23, steps per second: 166, episode reward: 99.561, mean reward: 4.329 [2.659, 6.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.230, 10.100], loss: 0.439341, mae: 0.557198, mean_q: 5.578281
 96570/100000: episode: 1662, duration: 0.129s, episode steps: 23, steps per second: 179, episode reward: 104.660, mean reward: 4.550 [2.356, 11.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.366, 10.100], loss: 0.806593, mae: 0.629885, mean_q: 5.629898
 96593/100000: episode: 1663, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 104.270, mean reward: 4.533 [2.677, 8.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.248, 10.100], loss: 1.033289, mae: 0.688057, mean_q: 5.770519
 96620/100000: episode: 1664, duration: 0.159s, episode steps: 27, steps per second: 170, episode reward: 163.140, mean reward: 6.042 [2.896, 12.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.393, 10.100], loss: 0.756319, mae: 0.626334, mean_q: 5.545693
 96635/100000: episode: 1665, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 54.082, mean reward: 3.605 [3.204, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.303, 10.100], loss: 0.944242, mae: 0.730432, mean_q: 5.704857
 96676/100000: episode: 1666, duration: 0.223s, episode steps: 41, steps per second: 184, episode reward: 106.369, mean reward: 2.594 [1.501, 4.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.662, 10.100], loss: 1.283046, mae: 0.702758, mean_q: 5.740405
 96691/100000: episode: 1667, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 50.524, mean reward: 3.368 [2.717, 4.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.268, 10.100], loss: 1.922079, mae: 0.733809, mean_q: 5.827043
 96732/100000: episode: 1668, duration: 0.208s, episode steps: 41, steps per second: 197, episode reward: 136.884, mean reward: 3.339 [2.068, 5.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.916, 10.100], loss: 0.727853, mae: 0.657747, mean_q: 5.664994
 96747/100000: episode: 1669, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 46.713, mean reward: 3.114 [2.542, 4.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.752, 10.100], loss: 0.358896, mae: 0.575591, mean_q: 5.756335
 96788/100000: episode: 1670, duration: 0.239s, episode steps: 41, steps per second: 172, episode reward: 89.832, mean reward: 2.191 [1.545, 4.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.666, 10.100], loss: 0.422703, mae: 0.585493, mean_q: 5.593959
 96803/100000: episode: 1671, duration: 0.101s, episode steps: 15, steps per second: 149, episode reward: 66.688, mean reward: 4.446 [3.011, 8.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.291, 10.100], loss: 0.394574, mae: 0.610239, mean_q: 5.752519
 96824/100000: episode: 1672, duration: 0.109s, episode steps: 21, steps per second: 194, episode reward: 95.754, mean reward: 4.560 [3.082, 7.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.351, 10.100], loss: 1.587995, mae: 0.762029, mean_q: 5.939164
 96839/100000: episode: 1673, duration: 0.096s, episode steps: 15, steps per second: 155, episode reward: 44.695, mean reward: 2.980 [2.349, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.271, 10.100], loss: 1.119178, mae: 0.734482, mean_q: 5.793365
 96857/100000: episode: 1674, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 68.100, mean reward: 3.783 [2.592, 5.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.694, 10.100], loss: 0.962224, mae: 0.694306, mean_q: 5.937198
 96898/100000: episode: 1675, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 135.849, mean reward: 3.313 [2.159, 5.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.238, 10.100], loss: 0.699125, mae: 0.645723, mean_q: 5.818648
 96939/100000: episode: 1676, duration: 0.241s, episode steps: 41, steps per second: 170, episode reward: 166.495, mean reward: 4.061 [2.298, 8.754], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.527, 10.100], loss: 0.728241, mae: 0.678058, mean_q: 5.942620
 96954/100000: episode: 1677, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 57.409, mean reward: 3.827 [2.698, 5.341], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.318, 10.100], loss: 0.769230, mae: 0.612653, mean_q: 5.740089
 96995/100000: episode: 1678, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 107.628, mean reward: 2.625 [2.012, 3.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.421, 10.100], loss: 0.943523, mae: 0.683921, mean_q: 5.825660
 97010/100000: episode: 1679, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 48.797, mean reward: 3.253 [2.066, 7.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.124, 10.100], loss: 1.251358, mae: 0.699962, mean_q: 5.951158
 97028/100000: episode: 1680, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 70.097, mean reward: 3.894 [2.170, 6.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.567, 10.100], loss: 0.481922, mae: 0.595136, mean_q: 5.682243
 97043/100000: episode: 1681, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 52.679, mean reward: 3.512 [2.812, 4.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.337, 10.100], loss: 0.416334, mae: 0.605257, mean_q: 5.875744
 97070/100000: episode: 1682, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 166.271, mean reward: 6.158 [2.851, 17.835], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.120, 10.100], loss: 0.739541, mae: 0.648035, mean_q: 5.831343
 97112/100000: episode: 1683, duration: 0.247s, episode steps: 42, steps per second: 170, episode reward: 91.150, mean reward: 2.170 [1.473, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-1.225, 10.100], loss: 0.680068, mae: 0.681468, mean_q: 5.960221
 97154/100000: episode: 1684, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 100.029, mean reward: 2.382 [1.698, 4.164], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.514, 10.100], loss: 1.051844, mae: 0.749344, mean_q: 5.964613
 97169/100000: episode: 1685, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 88.069, mean reward: 5.871 [3.335, 10.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.299, 10.100], loss: 1.512863, mae: 0.834382, mean_q: 6.017306
 97210/100000: episode: 1686, duration: 0.231s, episode steps: 41, steps per second: 178, episode reward: 131.655, mean reward: 3.211 [1.507, 17.827], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.623, 10.100], loss: 1.482762, mae: 0.896702, mean_q: 6.174607
 97231/100000: episode: 1687, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 88.874, mean reward: 4.232 [2.950, 8.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.233, 10.100], loss: 1.271387, mae: 0.824277, mean_q: 6.200192
 97246/100000: episode: 1688, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 54.952, mean reward: 3.663 [2.905, 4.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.251, 10.100], loss: 0.587912, mae: 0.674939, mean_q: 5.838432
 97264/100000: episode: 1689, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 54.961, mean reward: 3.053 [2.185, 5.817], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.273, 10.100], loss: 0.992023, mae: 0.742173, mean_q: 6.212517
[Info] FALSIFICATION!
[Info] Levels: [5.9335318, 8.278634, 20.124306]
[Info] Cond. Prob: [0.1, 0.1, 0.01]
[Info] Error Prob: 0.00010000000000000002

 97281/100000: episode: 1690, duration: 4.587s, episode steps: 17, steps per second: 4, episode reward: 311.093, mean reward: 18.300 [5.398, 100.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.328, 10.020], loss: 0.791383, mae: 0.707744, mean_q: 5.953281
 97381/100000: episode: 1691, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 210.675, mean reward: 2.107 [1.488, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.218, 10.136], loss: 1.313172, mae: 0.826582, mean_q: 6.067429
 97481/100000: episode: 1692, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 188.651, mean reward: 1.887 [1.443, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.111, 10.160], loss: 2.404332, mae: 0.870317, mean_q: 6.131815
 97581/100000: episode: 1693, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.449, mean reward: 1.934 [1.464, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.858, 10.098], loss: 1.248276, mae: 0.814591, mean_q: 6.209590
 97681/100000: episode: 1694, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 182.341, mean reward: 1.823 [1.458, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.684, 10.183], loss: 2.007320, mae: 0.828229, mean_q: 6.189117
 97781/100000: episode: 1695, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 184.586, mean reward: 1.846 [1.442, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.718, 10.147], loss: 4.447111, mae: 1.001782, mean_q: 6.093882
 97881/100000: episode: 1696, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 189.693, mean reward: 1.897 [1.464, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.018, 10.163], loss: 1.613588, mae: 0.781984, mean_q: 6.100279
 97981/100000: episode: 1697, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 195.012, mean reward: 1.950 [1.493, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.551, 10.262], loss: 3.941417, mae: 0.976847, mean_q: 6.079304
 98081/100000: episode: 1698, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 215.962, mean reward: 2.160 [1.474, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.006, 10.304], loss: 2.751471, mae: 0.888562, mean_q: 6.083707
 98181/100000: episode: 1699, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 203.800, mean reward: 2.038 [1.448, 3.962], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.471, 10.098], loss: 2.000238, mae: 0.757434, mean_q: 6.017271
 98281/100000: episode: 1700, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 205.371, mean reward: 2.054 [1.454, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.256, 10.098], loss: 4.162235, mae: 1.002501, mean_q: 5.989162
 98381/100000: episode: 1701, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 215.896, mean reward: 2.159 [1.515, 4.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.908, 10.098], loss: 2.191420, mae: 0.872202, mean_q: 5.975065
 98481/100000: episode: 1702, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 182.989, mean reward: 1.830 [1.454, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.612, 10.103], loss: 1.987546, mae: 0.806602, mean_q: 6.018322
 98581/100000: episode: 1703, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 196.384, mean reward: 1.964 [1.495, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.632, 10.098], loss: 1.931970, mae: 0.786269, mean_q: 5.973811
 98681/100000: episode: 1704, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 185.814, mean reward: 1.858 [1.445, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.342, 10.100], loss: 1.379596, mae: 0.711492, mean_q: 5.785188
 98781/100000: episode: 1705, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 200.101, mean reward: 2.001 [1.463, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.415, 10.137], loss: 4.321988, mae: 0.923749, mean_q: 5.852689
 98881/100000: episode: 1706, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 202.630, mean reward: 2.026 [1.514, 2.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.883, 10.181], loss: 1.658455, mae: 0.722791, mean_q: 5.771948
 98981/100000: episode: 1707, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 206.229, mean reward: 2.062 [1.475, 4.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.388, 10.191], loss: 1.624073, mae: 0.778581, mean_q: 5.708732
 99081/100000: episode: 1708, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.365, mean reward: 1.954 [1.463, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.409, 10.148], loss: 0.945074, mae: 0.698115, mean_q: 5.714041
 99181/100000: episode: 1709, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 186.511, mean reward: 1.865 [1.455, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.058, 10.244], loss: 1.891790, mae: 0.728448, mean_q: 5.587514
 99281/100000: episode: 1710, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 200.320, mean reward: 2.003 [1.486, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.266, 10.248], loss: 1.600896, mae: 0.742550, mean_q: 5.664703
 99381/100000: episode: 1711, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 182.526, mean reward: 1.825 [1.438, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.500, 10.307], loss: 1.126340, mae: 0.687133, mean_q: 5.612739
 99481/100000: episode: 1712, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 182.728, mean reward: 1.827 [1.460, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.168, 10.184], loss: 0.687465, mae: 0.624358, mean_q: 5.530811
 99581/100000: episode: 1713, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.249, mean reward: 1.882 [1.508, 2.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.510, 10.197], loss: 0.999599, mae: 0.653422, mean_q: 5.568085
 99681/100000: episode: 1714, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 216.853, mean reward: 2.169 [1.454, 4.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.143, 10.098], loss: 2.389192, mae: 0.790356, mean_q: 5.590788
 99781/100000: episode: 1715, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 182.722, mean reward: 1.827 [1.452, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.658, 10.140], loss: 0.631724, mae: 0.577195, mean_q: 5.364221
 99881/100000: episode: 1716, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 202.109, mean reward: 2.021 [1.447, 4.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.502, 10.156], loss: 1.794034, mae: 0.734004, mean_q: 5.528028
 99981/100000: episode: 1717, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 198.840, mean reward: 1.988 [1.473, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.817, 10.099], loss: 2.487760, mae: 0.746245, mean_q: 5.440258
done, took 612.248 seconds
[Info] End Importance Splitting. Falsification occurred 6 times.
